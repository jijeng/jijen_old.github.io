<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="Stay Hungry, Stay Foolish">
<meta property="og:type" content="website">
<meta property="og:title" content="Jijeng&#39;s blog">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="Jijeng&#39;s blog">
<meta property="og:description" content="Stay Hungry, Stay Foolish">
<meta property="og:locale" content="default">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Jijeng&#39;s blog">
<meta name="twitter:description" content="Stay Hungry, Stay Foolish">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/"/>





  <title>Jijeng's blog</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Jijeng's blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="Searching..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/13/Introduction-to-Ensemble/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jijeng Jia">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jijeng's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/06/13/Introduction-to-Ensemble/" itemprop="url">Introduction_to_Ensemble</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-06-13T10:35:17+08:00">
                2018-06-13
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>虽然在<a href="https://jijeng.github.io/2018/06/05/Titanic-Challenge/" target="_blank" rel="noopener">Titanic Challenge</a>博客中介绍了模型融合(Ensemble)，但是其是以特征提取为主旋律，所以这篇博客将结合实战从最基本的Ensemble到比较复杂的Ensemble,最后安利一个用于模型融合的package.</p>
<p>在文中可能出现一些概念，如果不太熟悉，可以先去文末找找看，如果我没有补充，那么请自行Google吧。为了不影响阅读的流畅性，我可能在其中不会涉及很多概念解释，比如说ROC曲线和AUC值。<br>好，我们进入正文。</p>
<h2 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h2><p>本文书写过程中沿用参考博客(<a href="https://www.dataquest.io/blog/introduction-to-ensembles/" target="_blank" rel="noopener">Introduction to Python Ensembles</a>)的数据集。可以去<a href="https://github.com/fivethirtyeight/data/tree/master/science-giving" target="_blank" rel="noopener">这里</a>下载，当然推荐使用原作者处理之后的数据集，you can find <a href="https://www.dataquest.io/blog/large_files/gen_data.py" target="_blank" rel="noopener">here</a>。<br>简单介绍一下这个数据集：<br>Federal Election Commission这个组织收集了2007到2016年的donations记录，最后得出 <a href="https://fivethirtyeight.com/features/when-scientists-donate-to-politicians-its-usually-to-democrats/" target="_blank" rel="noopener">When Scientists Donate To Politicians, It’s Usually To Democrats</a>这样的结论。原文(The dataset is from the Federal Election Commission, filtered to donations form the following occupations from 2007 to 2016)</p>
<p>好了，不想在数据集这里花太多时间，即使你上面不很明白，下文依然也是可以学习的，因为下文并没有进行很多的特征分析的内容，更多的是模型融合。当然你如果能够看懂，可以欣赏一下这个上述结论的有趣之处。</p>
<p>Give me codes:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import pandas as pd</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"># import data</span><br><span class="line"># Always good to set a seed for reproducibility</span><br><span class="line">SEED = 222</span><br><span class="line">np.random.seed(SEED)</span><br><span class="line">df = pd.read_csv(&apos;input.csv&apos;)</span><br><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line">from sklearn.metrics import roc_auc_score</span><br><span class="line"></span><br><span class="line">def get_train_test(test_size= 0.95):</span><br><span class="line">    y =1*(df.cand_pty_affiliation ==&apos;REP&apos;)    </span><br><span class="line">    X = df.drop([&quot;cand_pty_affiliation&quot;], axis=1)</span><br><span class="line">    X = pd.get_dummies(X, sparse=True)</span><br><span class="line">    X.drop(X.columns[X.std() == 0], axis=1, inplace=True)</span><br><span class="line">    return train_test_split(X, y, test_size=test_size, random_state=SEED)</span><br><span class="line">xtrain, xtest, ytrain, ytest = get_train_test()</span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure></p>
<p><img src="http://p9fyz2xe9.bkt.clouddn.com/18-6-12/47340483.jpg" alt="avatar"><br>简单看一下数据长什么样子，虽然有人可能不太懂。</p>
<h2 id="Begin-with-ensemble"><a href="#Begin-with-ensemble" class="headerlink" title="Begin with ensemble"></a>Begin with ensemble</h2><p>之前的<a href="https://jijeng.github.io/2018/06/05/%E6%A8%A1%E5%9E%8B%E8%9E%8D%E5%90%88/" target="_blank" rel="noopener">博客</a>主要从ensemble分类的角度阐述，现在从概念的角度阐述。<br>有时候我觉得英文说得很清楚，所以就不翻译成中文了，求不被打。<br>Ensemble: combining predictions from several models averages out idiosyncratic(怪异的) errors and yield better overall predictions.<br>简单的说，就是可以防止过拟合。当过拟合时，边界曲线就回去过分考虑某一个或某一些少数的点，这时候 ensemble通过某种combine机制，然后得到一个泛化性能比较好的边界曲线，也就是比较好的模型。</p>
<h3 id="Decision-Tree"><a href="#Decision-Tree" class="headerlink" title="Decision Tree"></a>Decision Tree</h3><p>首先我们从 decision tree开始。<br>A decision tree, which is a tree of <strong>if-then</strong> rules<br>The deeper the tree, the more complex the patterns it can capture, but the more prone to overfitting it will be. Because of this, we will need an alternative way of building complex models of decision trees, and an ensemble of different decision trees is one such way.<br>我们先使用 depth =1<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">from IPython.display import Image</span><br><span class="line">from sklearn.metrics import roc_auc_score</span><br><span class="line">from sklearn.tree import DecisionTreeClassifier</span><br><span class="line"></span><br><span class="line">t1 = DecisionTreeClassifier(max_depth=1, random_state=SEED)</span><br><span class="line">t1.fit(xtrain, ytrain)</span><br><span class="line">p = t1.predict_proba(xtest)[:, 1]</span><br><span class="line">print(&quot;Decision tree ROC-AUC score: %.3f&quot; % roc_auc_score(ytest, p))</span><br></pre></td></tr></table></figure></p>
<p>Decision tree ROC-AUC score: 0.672</p>
<p>发现结果不太理想，加深depth.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">t2 =DecisionTreeClassifier(max_depth=3, random_state=SEED)</span><br><span class="line">t2.fit(xtrain, ytrain)</span><br><span class="line">p =t2.predict_proba(xtest)[:, 1]</span><br><span class="line">print(&quot;Decision tree ROC-AUC score: %.3f&quot; % roc_auc_score(ytest, p))</span><br></pre></td></tr></table></figure></p>
<p>Decision tree ROC-AUC score: 0.751</p>
<p>由于我们最后要ensemble，而这要求我们要构造有差异但每个不是那么差的模型。首先我们考虑到使用不同的数据集。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">xtrain_slim =xtrain.drop([&apos;transaction_amt&apos;], axis=1)</span><br><span class="line">xtest_slim =xtest.drop([&apos;transaction_amt&apos;], axis=1)</span><br><span class="line"></span><br><span class="line">t3=DecisionTreeClassifier(max_depth=3, random_state=SEED)</span><br><span class="line">t3.fit(xtrain_slim, ytrain)</span><br><span class="line">p =t3.predict_proba(xtest_slim)[:,1]</span><br><span class="line">print(&apos;Decision tree ROC-AUC score:&#123;&#125;&apos;.format(roc_auc_score(ytest, p)))</span><br></pre></td></tr></table></figure></p>
<p>Decision tree ROC-AUC score:0.7403182587884118<br>通过corr()来检验两者的相关性(差异性)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">p1 =t2.predict_proba(xtest)[:,1]</span><br><span class="line">p2 =t3.predict_proba(xtest_slim)[:,1]</span><br><span class="line">pd.DataFrame(&#123;&apos;full_data&apos;:p1, &apos;red_data&apos;:p2&#125;).corr()</span><br></pre></td></tr></table></figure></p>
<p><img src="http://p9fyz2xe9.bkt.clouddn.com/18-6-12/93565225.jpg" alt="avatar"><br>发现有一定的相关性，但是还是可以容忍的。于是开始融合。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">p1 = t2.predict_proba(xtest)[:, 1]</span><br><span class="line">p2 = t3.predict_proba(xtest_slim)[:, 1]</span><br><span class="line">p = np.mean([p1, p2], axis=0)</span><br><span class="line">print(&quot;Average of decision tree ROC-AUC score: %.3f&quot; % roc_auc_score(ytest, p))</span><br></pre></td></tr></table></figure></p>
<p>Average of decision tree ROC-AUC score: 0.783<br>我们发现：两个旗鼓相当（0.74 0.73）的可以得到一个更好的结果，可以减少决策失误  的平均是0.78。</p>
<p>需要注意的是我们在构造第二个模型时候通过drop()丢掉一个feature，反而得到一个更好的模型。于是乎，我们想通过使用不同的子集（不同的特征）构造不同的模型，是不是能得到更好的模型？</p>
<h3 id="Random-Forest-Bagging"><a href="#Random-Forest-Bagging" class="headerlink" title="Random Forest(Bagging)"></a>Random Forest(Bagging)</h3><p>A fast approach that works well in practice is to randomly select a subset of features, fit one decision tree on each draw and average their predictions.This process is known as bootstrapped averaging (often abbreviated bagging), and when applied to decision trees, the resultant model is a Random Forest. (我觉得原作者比我说的清楚，借用了)<br>我的理解，在上面小节中我们使用的是Decision Tree,在实际应用中发现有差异的多个子树的效果要更好一些。而实现这个途径快速的方法就是 Random Forest。Random 在这里表示任意几个子树(特征)，然后这些Tree组成了Forest。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.ensemble import RandomForestClassifier</span><br><span class="line">rf =RandomForestClassifier(</span><br><span class="line">    n_estimators=10,</span><br><span class="line">    max_features= 3,</span><br><span class="line">    random_state=SEED</span><br><span class="line">)</span><br><span class="line">rf.fit(xtrain, ytrain)</span><br><span class="line">p =rf.predict_proba(xtest)[:, 1]</span><br><span class="line">print(&apos;Average of decision tree ROC-AUC score:&#123;&#125;&apos;.format(roc_auc_score(ytest, p)))</span><br></pre></td></tr></table></figure></p>
<p>Average of decision tree ROC-AUC score:0.844018408542404<br>这就是叫做 质的飞跃 从那个0.783-&gt; 0.844(将近6个百分点，好吧，有点神经质了…)<br>From nobody to somebody, we are on something..</p>
<h3 id="Ensemble-of-various-models"><a href="#Ensemble-of-various-models" class="headerlink" title="Ensemble of various models"></a>Ensemble of various models</h3><p>可以看出上述模型中，最后的模型（Random Forest）的子模型(Decision Tree)。但是子模型并不是局限树这一种结构，我们更多的选择：linear models, kernel-based models, non-parametric models, neural networks or even other ensembles!</p>
<p>为了避免代码的冗余构造了以下的helper function.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"># A host of Scikit-learn models</span><br><span class="line">from sklearn.svm import SVC, LinearSVC</span><br><span class="line">from sklearn.naive_bayes import GaussianNB</span><br><span class="line">from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier</span><br><span class="line">from sklearn.linear_model import LogisticRegression</span><br><span class="line">from sklearn.neighbors import KNeighborsClassifier</span><br><span class="line">from sklearn.neural_network import MLPClassifier</span><br><span class="line">from sklearn.kernel_approximation import Nystroem</span><br><span class="line">from sklearn.kernel_approximation import RBFSampler</span><br><span class="line">from sklearn.pipeline import make_pipeline</span><br><span class="line"></span><br><span class="line">def get_models():</span><br><span class="line">    &quot;&quot;&quot;Generate a library of base learners.&quot;&quot;&quot;</span><br><span class="line">    nb = GaussianNB()</span><br><span class="line">    svc = SVC(C=100, probability=True)</span><br><span class="line">    # C越大边界越复杂，会导致过拟合</span><br><span class="line">    knn = KNeighborsClassifier(n_neighbors=3)</span><br><span class="line">    # KNN算法寻找训练数据中的K个最近的数据，它使用指向最多的那个类别来作为预测的输出。</span><br><span class="line">    lr = LogisticRegression(C=100, random_state=SEED)</span><br><span class="line">    # 对于这个 c 能知道的就是正则化系数 smaller values specify stronger regularization.</span><br><span class="line">    nn = MLPClassifier((80, 10), early_stopping=False, random_state=SEED)</span><br><span class="line">    gb = GradientBoostingClassifier(n_estimators=100, random_state=SEED)</span><br><span class="line">    # 子模型的数量，默认是100, gbc通常 robust to over-fitting, so a large number results in better performance</span><br><span class="line">    rf = RandomForestClassifier(n_estimators=10, max_features=3, random_state=SEED)</span><br><span class="line">    models = &#123;&apos;svm&apos;: svc,</span><br><span class="line">              &apos;knn&apos;: knn,</span><br><span class="line">              &apos;naive bayes&apos;: nb,</span><br><span class="line">              &apos;mlp-nn&apos;: nn,</span><br><span class="line">              &apos;random forest&apos;: rf,</span><br><span class="line">              &apos;gbm&apos;: gb,</span><br><span class="line">              &apos;logistic&apos;: lr,</span><br><span class="line">              &#125;</span><br><span class="line"></span><br><span class="line">    return models</span><br><span class="line">    </span><br><span class="line">def train_predict(model_list):</span><br><span class="line">    P =np.zeros((ytest.shape[0], len(model_list)))</span><br><span class="line">    P =pd.DataFrame(P)</span><br><span class="line">    print(&apos;Fitting models&apos;)</span><br><span class="line">    cols =list()</span><br><span class="line">    for i, (name, m) in enumerate(models.items()):</span><br><span class="line">        print(&quot;%s...&quot; % name, end=&quot; &quot;, flush=False)</span><br><span class="line">        m.fit(xtrain, ytrain)</span><br><span class="line">        P.iloc[:, i] =m.predict_proba(xtest)[:, 1]</span><br><span class="line">        cols.append(name)</span><br><span class="line">        print(&apos;Done&apos;)</span><br><span class="line">    P.columns =cols</span><br><span class="line">    print(&apos;Done.\n&apos;)</span><br><span class="line">    return P</span><br><span class="line">    </span><br><span class="line">def score_models(P, y):</span><br><span class="line">    &quot;&quot;&quot;Score model in predication DF&quot;&quot;&quot;</span><br><span class="line">    print(&apos;Scoring models&apos;)</span><br><span class="line">    for m in P.columns:</span><br><span class="line">        score =roc_auc_score(y, P.loc[:, m])</span><br><span class="line">        print(&quot;%-26s: %.3f&quot; % (m, score))</span><br><span class="line">    print(&apos;Done, \n&apos;)</span><br></pre></td></tr></table></figure></p>
<p>let’s go…<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">models =get_models()</span><br><span class="line">P =train_predict(models)</span><br><span class="line">score_models(P, ytest)</span><br></pre></td></tr></table></figure></p>
<p><img src="http://p9fyz2xe9.bkt.clouddn.com/18-6-13/12164157.jpg" alt="avatar"><br>This is our base line.<br>Gradient Boosting Machine(GBM) 果然名不虚传, does best<br>我们来分析一下模型之间的相关性，原作者使用的mlens package(You can install it with: pip install mlens)，我这里用的是seaborn(install it with: pip install seaborn).<br>在检查相关性(Pearson相关性:衡量两个数据集合的线性相关性)时候，我们使用的是经过处理的相关性。具体说来可以称之为 error correlation,详细见代码。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># look at error correlations  is more promising, errors are significantly correlated </span><br><span class="line">import seaborn as sns</span><br><span class="line">plt.subplots(figsize=(10,8))  </span><br><span class="line">corrmat = P.apply(lambda pred: 1*(pred &gt;= 0.5) - ytest.values).corr()</span><br><span class="line">sns.set(font_scale=1.5)  </span><br><span class="line">hm = sns.heatmap(corrmat, cbar=True, annot=True, square=True, fmt=&apos;.2f&apos;, annot_kws=&#123;&apos;size&apos;: 10&#125;)  </span><br><span class="line">plt.title(&apos;Pearson Correlation of Features&apos;, y=1.05, size=15)  </span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://p9fyz2xe9.bkt.clouddn.com/18-6-13/15404991.jpg" alt="avatar"><br>预测值和真实值之间的差异称之为error，查看error的 Pearson correlation，效果更加明显。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(&quot;Ensemble ROC-AUC score: %.3f&quot; % roc_auc_score(ytest, P.mean(axis=1)))</span><br></pre></td></tr></table></figure></p>
<p>Ensemble ROC-AUC score: 0.884<br>结果高于每一个单独的模型，但是不是那么明显。</p>
<h3 id="Visualize-Curve-ROC-helper-function"><a href="#Visualize-Curve-ROC-helper-function" class="headerlink" title="Visualize Curve ROC(helper function)"></a>Visualize Curve ROC(helper function)</h3><p>我们注意到之前使用的所有的结果的评价标准都是 roc_auc_score，但是并没有提及这是什么。当然在正文中也不打算解释，如果不是很清楚，可以查看本文最后<strong>补充概念</strong>.简单来说AUC可以用来衡量”二分问题”的泛化能力，是一种评价指标。我们这里想说的是 visualize Curve ROC,可视化。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"># a helper function for roc_curve</span><br><span class="line">from sklearn.metrics import roc_curve</span><br><span class="line">def plot_roc_curve(ytest, P_base_learners, P_ensemble, labels, ens_label):</span><br><span class="line">    &quot;&quot;&quot;Plot the roc curve for base learners and ensemble.&quot;&quot;&quot;</span><br><span class="line">    plt.figure(figsize=(10, 8))</span><br><span class="line">    plt.plot([0, 1], [0, 1], &apos;k--&apos;)</span><br><span class="line">    cm = [plt.cm.rainbow(i) for i in np.linspace(0, 1.0, P_base_learners.shape[1] + 1)]</span><br><span class="line">      </span><br><span class="line">    for i in range(P_base_learners.shape[1]):</span><br><span class="line">        p = P_base_learners[:, i]</span><br><span class="line">        fpr, tpr, _ = roc_curve(ytest, p)</span><br><span class="line">        plt.plot(fpr, tpr, label=labels[i], c=cm[i + 1])</span><br><span class="line"></span><br><span class="line">    fpr, tpr, _ = roc_curve(ytest, P_ensemble)</span><br><span class="line">    plt.plot(fpr, tpr, label=ens_label, c=cm[0])</span><br><span class="line">        </span><br><span class="line">    plt.xlabel(&apos;False positive rate&apos;)</span><br><span class="line">    plt.ylabel(&apos;True positive rate&apos;)</span><br><span class="line">    plt.title(&apos;ROC curve&apos;)</span><br><span class="line">    plt.legend(frameon=False)</span><br><span class="line">    plt.show()</span><br><span class="line">plot_roc_curve(ytest, P.values, P.mean(axis=1), list(P.columns), &quot;ensemble&quot;)</span><br></pre></td></tr></table></figure></p>
<p><img src="http://p9fyz2xe9.bkt.clouddn.com/18-6-12/52483658.jpg" alt="avatar"><br>我们顺便把刚才的模型可视化，发现ensemble的AUC是最大的，意味这泛化性能是最好，这也是符合我们的认知的 。</p>
<h3 id="Beyond-ensembles-as-a-simple-average"><a href="#Beyond-ensembles-as-a-simple-average" class="headerlink" title="Beyond ensembles as a simple average"></a>Beyond ensembles as a simple average</h3><p>我们回到主线上，在上一个模型中我们提及最后的 ensemble的结果是好于每个单独的模型，但是没有那么突出。根据ROC曲线我们也可以看出，有的模型(KNN)在这里表现的没有那么好，我们在想是不是由于因为这个而拉底了最后的结果，当然这只是猜测，于是我们找到了 try的方向。<br>可能第一直觉去掉这个模型再进行融合，在这个实验表明该策略最后的结果0.883，相比与0.884，你懂得，并没有变好。<br>我们还有一种策略:learn a sensible set of weights to use when averaging predictions.让模型自己去学习如何调整各个模型之间的比例。</p>
<h3 id="Learning-to-combine-predications"><a href="#Learning-to-combine-predications" class="headerlink" title="Learning to combine predications"></a>Learning to combine predications</h3><p>为了让模型自学习各个之间的预测比例，我们引入了 meta learner(meta是元，理解为最基础的) to learn how to best combine these predictions.</p>
<p>除此之外，我们将使用不同的数据集，像Random Forest使用不同的数据子集(不同的特征组成的数据集)。于是我们需要a method for splitting the training data between the base learners and the meta learner.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">base_learners =get_models()</span><br><span class="line">meta_learner = GradientBoostingClassifier(</span><br><span class="line">    n_estimators=1000,</span><br><span class="line">    loss=&quot;exponential&quot;,</span><br><span class="line">    max_features=4,</span><br><span class="line">    max_depth=3,</span><br><span class="line">    subsample=0.5,</span><br><span class="line">    learning_rate=0.005,     </span><br><span class="line">    random_state=SEED</span><br><span class="line">)</span><br></pre></td></tr></table></figure></p>
<p>使用最强模型GBM作为 meta learner并定义好 base_learners.<br>To keep things simple, we split the full training set into a training and prediction set of the base learners. This method is sometimes referred to as <strong>Blending</strong>. Unfortunately, the terminology differs between communities, so it’s not always easy to know what type of cross-validation the ensemble is using.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># sefine a procedure for generating train and test sets</span><br><span class="line">xtrain_base , xpred_base, ytrain_base, ypred_base =train_test_split(xtrain, ytrain, test_size=0.5, random_state=SEED)</span><br><span class="line"></span><br><span class="line">def train_base_learners(base_learners, inp, out, verbose =True):</span><br><span class="line">    if verbose: print(&apos;Fitting models&apos;)</span><br><span class="line">    for i, (name, m) in enumerate(base_learners.items()):</span><br><span class="line">        if verbose: print(&quot;%s...&quot; % name, end=&quot; &quot;, flush=False)</span><br><span class="line">        m.fit(inp, out)</span><br><span class="line">        if verbose: print(&apos;Done.&apos;)</span><br><span class="line">train_base_learners(base_learners, xtrain_base, ytrain_base)</span><br></pre></td></tr></table></figure></p>
<p>(注意我们只是使用了50%的data去train, test_size =0.5)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">def predict_base_learners(pred_base_learners, inp, verbose=True):</span><br><span class="line">    &quot;&quot;&quot;Generate a prediction matrix.&quot;&quot;&quot;</span><br><span class="line">    P = np.zeros((inp.shape[0], len(pred_base_learners)))</span><br><span class="line"></span><br><span class="line">    if verbose: print(&quot;Generating base learner predictions.&quot;)</span><br><span class="line">    for i, (name, m) in enumerate(pred_base_learners.items()):</span><br><span class="line">        if verbose: print(&quot;%s...&quot; % name, end=&quot; &quot;, flush=False)</span><br><span class="line">        p = m.predict_proba(inp)</span><br><span class="line">        # With two classes, need only predictions for one class</span><br><span class="line">        P[:, i] = p[:, 1]</span><br><span class="line">        if verbose: print(&quot;done&quot;)</span><br><span class="line"></span><br><span class="line">    return P</span><br><span class="line">P_base = predict_base_learners(base_learners, xpred_base)</span><br></pre></td></tr></table></figure></p>
<p>现在我们得到了base_learners的predications，接下来我们应该使用的是这个流程，<strong>在base learners的基础上</strong>（类似两层结构了 meta learner 学习的如何搭配这些base learner使得最后的结果 predications最大）进行训练，而不是训练原来的数据集。<br><img src="http://p9fyz2xe9.bkt.clouddn.com/18-6-13/82384876.jpg" alt="avatar"><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">meta_learner.fit(P_base, ypred_base)</span><br></pre></td></tr></table></figure></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">#meta_learner.fit(P_base, ypred_base)</span><br><span class="line">def ensemble_predict(base_learners, meta_learner, inp, verbose=True):</span><br><span class="line">    &quot;&quot;&quot;Generate predictions from the ensemble.&quot;&quot;&quot;</span><br><span class="line">    P_pred = predict_base_learners(base_learners, inp, verbose=verbose)</span><br><span class="line">    return P_pred, meta_learner.predict_proba(P_pred)[:, 1]</span><br><span class="line">P_pred, p = ensemble_predict(base_learners, meta_learner, xtest)</span><br><span class="line">print(&quot;\nEnsemble ROC-AUC score: %.3f&quot; % roc_auc_score(ytest, p))</span><br></pre></td></tr></table></figure>
<p><img src="http://p9fyz2xe9.bkt.clouddn.com/18-6-13/11434494.jpg" alt="avatar"><br>最后的结果是0.881 相比与之前最好的0.884（使用相同的数据集，没有进行 meta_learner的操作）<br>这是因为我们在划分数据集的使用只是使用了0.5的数据集，而前者的模型使用了全部的train sets。<br>有人不免疑问：为什么不使用全部的data？我的理解是划分xtrain_base , xpred_base, ytrain_base, ypred_base使用的是 train_test_split()，总是需要设定一个数值的，即使train_size =0.01，也是没有用到全部的datas.</p>
<h3 id="Training-with-cross-validation"><a href="#Training-with-cross-validation" class="headerlink" title="Training with cross-validation"></a>Training with cross-validation</h3><p>我们使用cross-validation 来缓解上面那个问题。<br>During cross-validated training of the base learners, a copy of each base learner is fitted on K−1 folds, and predict the left-out fold. This process is iterated until every fold has been predicted. The more folds we specify, the less data is being left out in each training pass. This makes cross-validated predictions less noisy and a better reflection of performance during test time. The cost is significantly increased training time. Fitting an <strong>ensemble</strong> with <strong>cross-validation</strong> is often referred to as <strong>stacking</strong>, while the ensemble itself is known as the Super Learner.</p>
<p>To understand how cross-validation works, we can think of it as an outer loop over our previous ensemble. The outer loop iterates over K distinct test folds, with the remaining data used for training. The inner loop trains the base learners and generate predictions for the held-out data. Here’s a simple stacking implementation:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.base import clone</span><br><span class="line"></span><br><span class="line">def stacking(base_learners, meta_learner, X, y, generator):</span><br><span class="line">    &quot;&quot;&quot;Simple training routine for stacking.&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    # Train final base learners for test time</span><br><span class="line">    print(&quot;Fitting final base learners...&quot;, end=&quot;&quot;)</span><br><span class="line">    train_base_learners(base_learners, X, y, verbose=False)</span><br><span class="line">    print(&quot;done&quot;)</span><br><span class="line"></span><br><span class="line">    # Generate predictions for training meta learners</span><br><span class="line">    # Outer loop:</span><br><span class="line">    print(&quot;Generating cross-validated predictions...&quot;)</span><br><span class="line">    cv_preds, cv_y = [], []</span><br><span class="line">    for i, (train_idx, test_idx) in enumerate(generator.split(X)):</span><br><span class="line"></span><br><span class="line">        fold_xtrain, fold_ytrain = X[train_idx, :], y[train_idx]</span><br><span class="line">        fold_xtest, fold_ytest = X[test_idx, :], y[test_idx]</span><br><span class="line"></span><br><span class="line">        # Inner loop: step 4 and 5</span><br><span class="line">        fold_base_learners = &#123;name: clone(model)</span><br><span class="line">                              for name, model in base_learners.items()&#125;</span><br><span class="line">        train_base_learners(</span><br><span class="line">            fold_base_learners, fold_xtrain, fold_ytrain, verbose=False)</span><br><span class="line"></span><br><span class="line">        fold_P_base = predict_base_learners(</span><br><span class="line">            fold_base_learners, fold_xtest, verbose=False)</span><br><span class="line"></span><br><span class="line">        cv_preds.append(fold_P_base)</span><br><span class="line">        cv_y.append(fold_ytest)</span><br><span class="line">        print(&quot;Fold %i done&quot; % (i + 1))</span><br><span class="line"></span><br><span class="line">    print(&quot;CV-predictions done&quot;)</span><br><span class="line">    </span><br><span class="line">    # Be careful to get rows in the right order</span><br><span class="line">    cv_preds = np.vstack(cv_preds)</span><br><span class="line">    cv_y = np.hstack(cv_y)</span><br><span class="line"></span><br><span class="line">    # Train meta learner</span><br><span class="line">    print(&quot;Fitting meta learner...&quot;, end=&quot;&quot;)</span><br><span class="line">    meta_learner.fit(cv_preds, cv_y)</span><br><span class="line">    print(&quot;done&quot;)</span><br><span class="line">    return base_learners, meta_learner</span><br></pre></td></tr></table></figure></p>
<p>尤其在cv_preds和cv_y的维度问题上，注意小心。<br>The basic difference between blending and stacking is therefore that stacking allows both base learners and the meta learner to train on the full data set. Using 2-fold cross-validation, we can measure the difference this makes in our case:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.model_selection import KFold</span><br><span class="line"># Train with stacking</span><br><span class="line">cv_base_learners, cv_meta_learner = stacking(</span><br><span class="line">    get_models(), clone(meta_learner), xtrain.values, ytrain.values, KFold(2))</span><br><span class="line"></span><br><span class="line">P_pred, p = ensemble_predict(cv_base_learners, cv_meta_learner, xtest, verbose=False)</span><br><span class="line">print(&quot;\nEnsemble ROC-AUC score: %.3f&quot; % roc_auc_score(ytest, p))</span><br></pre></td></tr></table></figure></p>
<p>Ensemble ROC-AUC score: 0.889<br>这是目前为止最好的结果了。<br>Stacking yields a sizeable increase in performance: in fact, it gives us our best score so far. This outcome is typical for small and medium-sized data sets, where the effect of blending can be severe. As the data set size increases, blending and stacking performs similarly.</p>
<h2 id="Use-packages"><a href="#Use-packages" class="headerlink" title="Use packages"></a>Use packages</h2><p>快要接近尾声了，在文章的开始，我们提及要安利一个resemble好用的package. So, it’s now.<br>Here, we use Ensemble and build our previous generalized ensemble, but now using 10-fold cross-validation:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">from mlens.ensemble import SuperLearner</span><br><span class="line"></span><br><span class="line"># Instantiate the ensemble with 10 folds</span><br><span class="line">sl = SuperLearner(</span><br><span class="line">    folds=10,</span><br><span class="line">    random_state=SEED,</span><br><span class="line">    verbose=2,</span><br><span class="line">    backend=&quot;multiprocessing&quot;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"># Add the base learners and the meta learner</span><br><span class="line">sl.add(list(base_learners.values()), proba=True) </span><br><span class="line">sl.add_meta(meta_learner, proba=True)</span><br><span class="line"></span><br><span class="line"># Train the ensemble</span><br><span class="line">sl.fit(xtrain, ytrain)</span><br><span class="line"></span><br><span class="line"># Predict the test set</span><br><span class="line">p_sl = sl.predict_proba(xtest)</span><br><span class="line"></span><br><span class="line">print(&quot;\nSuper Learner ROC-AUC score: %.3f&quot; % roc_auc_score(ytest, p_sl[:, 1]))</span><br></pre></td></tr></table></figure></p>
<p><img src="http://p9fyz2xe9.bkt.clouddn.com/18-6-13/78451107.jpg" alt="avatar"><br>So simple!<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_roc_curve(ytest, p.reshape(-1, 1), P.mean(axis=1), [&quot;Simple average&quot;], &quot;Super Learner&quot;)</span><br></pre></td></tr></table></figure></p>
<p><img src="http://p9fyz2xe9.bkt.clouddn.com/18-6-13/52534893.jpg" alt="avatar"></p>
<p>发现super learner(meta learner)和 basic learner的mean的结果已经不相上下了。super learner得到了很好的训练。</p>
<h2 id="补充概念"><a href="#补充概念" class="headerlink" title="补充概念"></a>补充概念</h2><ul>
<li><p>ROC曲线和AUC值<br>ROC全称是“受试者工作特征”（Receiver Operating Characteristic）。ROC曲线的面积就是AUC（Area Under the Curve）。AUC用于衡量“二分类问题”机器学习算法性能（泛化能力）。说道这里不得不提及就是经常使用的符号，TP(True Positive), FP(False, Positive), TN(True Negative),FN(False Negative)。他们是根据原来真实数据和预测类别进行的排列组合（当然这是针对二分问题）。好丑的图片啊…<br><img src="http://p9fyz2xe9.bkt.clouddn.com/18-6-12/79026872.jpg" alt="avatar"><br>接着我们计算TRP(True Positive Radio)，FRP(False Positive Ratio)用于描述ROC曲线，分别表示该曲线的Y轴，X轴。<br>TPR=TP/(TP+FN)<br>FPR=FP/(FP+TN)<br>最后就形成了类似这样的图像(来源于上述的训练模型)<br><img src="http://p9fyz2xe9.bkt.clouddn.com/18-6-12/52483658.jpg" alt="avatar"><br>我们希望的结果是TRU越大（接近1），FRU越小（接近0）。AUC的值是ROC所覆盖的面积，当AUC越大时候，分类器的效果越好。从途中可以看出模型(ensemble)的面积是最大的，分类效果也是最好的。<br>关于该图像还有一点，如果你的曲线拟合对角线（图中虚线），那么相当于随机猜测。<br>在机器学习中关于这方面经常涉及到的还有precision, recall两个概念，下面是计算公式。<br><img src="http://p9fyz2xe9.bkt.clouddn.com/18-6-12/56890358.jpg" alt="avatar"><br>其中n= TP+FP<br><img src="http://p9fyz2xe9.bkt.clouddn.com/18-6-12/29323029.jpg" alt="avatar"><br>总结：图中的x，y轴的计算和 precision, recall不是一个概念，虽然 recall 和y 轴在计算上是相同的。precision  是针对于预测数据来说的，而x轴，y轴(recall)的计算某种意义上是针对原来真实数据而言的。所以我们在训练模型过程可以追求 precision 和 recall的双高(即图像的左上角)。x，y轴的计算是针对于原来真实数据来说的。</p>
</li>
<li><p>majority/ soft/ hard voting<br>an ensemble that averages classifier predictions is known as a <strong>majority voting</strong> classifier. When an ensembles averages based on probabilities (as above), we refer to it as <strong>soft voting</strong>, averaging final class label predictions is known as <strong>hard voting</strong>.</p>
</li>
<li><p>Pearson相关性<br><img src="http://p9fyz2xe9.bkt.clouddn.com/18-6-13/73757528.jpg" alt="avatar"><br>协方差除以各自的标准差</p>
</li>
<li><p>GBC参数<br>这些参数中，类似于Adaboost，我们把重要参数分为两类，第一类是Boosting框架的重要参数，第二类是弱学习器即CART回归树的重要参数。<br>n_estimators: 也就是弱学习器的最大迭代次数，或者说最大的弱学习器的个数。<br>learning_rate: 即每个弱学习器的权重缩减系数ν，也称作步长<br>对于分类模型，有对数似然损失函数”deviance”和指数损失函数”exponential”两者输入选择。默认是对数似然损失函数”deviance”。在原理篇中对这些分类损失函数有详细的介绍。一般来说，推荐使用默认的”deviance”。它对二元分离和多元分类各自都有比较好的优化。而指数损失函数等于把我们带到了Adaboost算法。<br>对于回归模型，有均方差”ls”, 绝对损失”lad”, Huber损失”huber”和分位数损失“quantile”。默认是均方差”ls”。一般来说，如果数据的噪音点不多，用默认的均方差”ls”比较好。如果是噪音点较多，则推荐用抗噪音的损失函数”huber”。而如果我们需要对训练集进行分段预测的时候，则采用“quantile”。<br>max_features:可以使用很多种类型的值，默认是”None”,意味着划分时考虑所有的特征数.<br>subsample: 选择小于1的比例可以减少方差，即防止过拟合，但是会增加样本拟合的偏差，因此取值不能太低。推荐在[0.5, 0.8]之间，默认是1.0，即不使用子采样。</p>
</li>
</ul>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://www.cnblogs.com/pinard/p/6143927.html" target="_blank" rel="noopener">GBC参数设置</a><br><a href="https://www.cnblogs.com/gatherstars/p/6084696.html" target="_blank" rel="noopener">ROC曲线和AUC值</a><br><a href="https://www.dataquest.io/blog/introduction-to-ensembles/" target="_blank" rel="noopener">Introduction to Python Ensembles</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/05/对抗性生成网络/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jijeng Jia">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jijeng's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/06/05/对抗性生成网络/" itemprop="url">对抗性生成网络实验对比</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-06-05T22:27:25+08:00">
                2018-06-05
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Deep-learning/" itemprop="url" rel="index">
                    <span itemprop="name">Deep learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>GAN模型是典型的隐式无监督生成模型，建模过程中没有利用到数据的语义标签。但在实际应用中生成模型的可控性至关重要，根据标签生成对可控有样本更具实际应用价值。图像生成模型被广泛应用于数据增强、风格转换和数据补全领域，需要可控且语义完备的生成模型。<br>条件GAN模型在GAN模型建模思路的基础上，将语义标签加入了建模过程，将无监督生成模型转变为有监督的条件生成模型。条件GAN模型具体包括Conditional GAN模型、Semi-GAN模型和AC-GAN模型。</p>
<h3 id="实验模型介绍"><a href="#实验模型介绍" class="headerlink" title="实验模型介绍"></a>实验模型介绍</h3><p>（1）Conditional GAN模型<br>GAN模型中判别器D对输入的数据样本的来源进行判别，是典型的判别模型流程。如果在GAN框架中加入有监督信息来辅助训练，如图像的类别信息来辅助判别器D进行判别，则会帮助生成更加真实的样本。其中最初的尝试方式是Conditional GAN模型，结构如图所示。<br><img src="http://p9fyz2xe9.bkt.clouddn.com/18-6-1/89027900.jpg" alt="avatar"><br>Conditional GAN模型在生成器G和判别器D输入中都加入了标签信息，试图让生成器G学习到从数据标签y到样本x的映射，让判别器D学习对样本x和标签y的组合进行判别。<br>与GAN模型相比，Conditional GAN模型增加了标签信息的输入将模型转变为条件生成模型，在一定程度上提高了模型的稳定性。Conditional GAN模型训练过程中，判别器D对样本x和标签y的类别组合进行训练，并没有输入样本x和标签y的类别错误组合进行训练，因此模型并没有学习样本x和标签y的联合分布。<br>（2）Semi-GAN模型<br>Conditional GAN模型利用了标签信息进行建模，但没有对标签语义的信息进行表征，导致模型能够学到的信息有限。在生成模型过程中，如果判别器D能够明确指出生成样本的类别错误，则可为生成器G提供更加精确的梯度信息，最终能生成更加真实的样本。<br>Semi-GAN基于此思路进行改进，具体结构如图9所示。<br><img src="http://p9fyz2xe9.bkt.clouddn.com/18-6-1/89762825.jpg" alt="avatar"><br>Semi-GAN模型在Conditional GAN模型的基础上，对判别器D的分类输出进行细化加入了半监督学习过程。<br>（3）AC-GAN模型<br>与Conditional GAN模型相比，Semi-GAN模型中判别器D能够判别真实样本的来源，增强了判别器D的判别能力。但研究表明过强的判别信息会影响生产样本的质量[39]，具体原因为Semi-GAN模型的建模过程为半监督分类过程，目标优化函数为无监督分类和有监督分类目标函数之和。如公式(2.10)和公式(2.11)所示，若判别器D的监督分类信息过强，则会削弱判别器D对样本来源的判别能力。<br>Conditional GAN模型能够生成制定类别的样本，Semi-GAN模型能够判别样本的类别信息。AC-GAN模型将以上两个模型思路进行整合，得到够进行条件生成的生成器G，和能够判别样本类别和来源信息的判别器D。AC-GAN模型的结构如图10所示。<br><img src="http://p9fyz2xe9.bkt.clouddn.com/18-6-1/23586830.jpg" alt="avatar"><br>AC-GAN模型在Conditional GAN的基础上，让判别器D在判别样本来源的同时，让样本进行分类。此时的判别器D的输出分为样本来源信息LS和样本分类LC信息。</p>
<h3 id="不同模型比较"><a href="#不同模型比较" class="headerlink" title="不同模型比较"></a>不同模型比较</h3><p>下面用表格的方式对比在实验中使用的模型的目标函数(ps,图画比较丑，之后再修改)</p>
<table>
<thead>
<tr>
<th style="text-align:center"><em>Name</em></th>
<th style="text-align:center"><em>Paper Link</em></th>
<th style="text-align:left"><em>Value Function</em></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><strong>GAN</strong></td>
<td style="text-align:center"><a href="https://arxiv.org/abs/1406.2661" target="_blank" rel="noopener">Arxiv</a></td>
<td style="text-align:left"><img src="http://p9fyz2xe9.bkt.clouddn.com/18-6-1/55901553.jpg" alt="avatar"></td>
</tr>
<tr>
<td style="text-align:center"><strong>DCGAN</strong></td>
<td style="text-align:center"><a href="https://arxiv.org/abs/1511.06434" target="_blank" rel="noopener">Arxiv</a></td>
<td style="text-align:left"><img src="http://p9fyz2xe9.bkt.clouddn.com/18-6-1/90451211.jpg" alt="avatar"></td>
</tr>
<tr>
<td style="text-align:center"><strong>Semi-GAN</strong></td>
<td style="text-align:center"><a href="https://arxiv.org/abs/1606.01583" target="_blank" rel="noopener">Arxiv</a></td>
<td style="text-align:left">和GAN 模型相同</td>
</tr>
<tr>
<td style="text-align:center"><strong>CGAN</strong></td>
<td style="text-align:center"><a href="https://arxiv.org/abs/1411.1784" target="_blank" rel="noopener">Arxiv</a></td>
<td style="text-align:left"><img src="http://p9fyz2xe9.bkt.clouddn.com/18-6-1/78212709.jpg" alt="avatar"></td>
</tr>
<tr>
<td style="text-align:center"><strong>ACGAN</strong></td>
<td style="text-align:center"><a href="https://arxiv.org/abs/1610.09585" target="_blank" rel="noopener">Arxiv</a></td>
<td style="text-align:left"><img src="http://p9fyz2xe9.bkt.clouddn.com/18-6-1/84966180.jpg" alt="avatar"></td>
</tr>
<tr>
<td style="text-align:center"><strong>our model</strong></td>
<td style="text-align:center"><a href="https://arxiv.org/abs/1705.07215" target="_blank" rel="noopener">Arxiv</a></td>
<td style="text-align:left"><img src="http://p9fyz2xe9.bkt.clouddn.com/18-6-1/48822906.jpg" alt="avatar"></td>
</tr>
</tbody>
</table>
<h3 id="数据集介绍"><a href="#数据集介绍" class="headerlink" title="数据集介绍"></a>数据集介绍</h3><p><img src="http://p9fyz2xe9.bkt.clouddn.com/18-6-1/70807095.jpg" alt="avatar"><br>在常用于图像生成的图像数据集中，大部分数据的标签类型为离散类型。其中MNIST和Fashion-MNIST为常用的灰度图像数据，每类样本分布较为独立，常用于进行图像分类和样本生成的实验；SVHN和CIFAR10为彩色数据集图像像素分布较为复杂，其中CIFAR10常用来检验分类网络性能的评价数据集；CelebA为大规模的人脸识别和属性分类数据集，每幅人脸图像包括40个属性标签；ImageNet为图像分类和识别数据集，数据集类别分布比较复杂具体包括自然图像和人为图像。UnityEyes为人眼视觉合成数据集，数据集标签包括瞳孔标签和视觉方向标签，其中视觉方向标签为连续的语言标签。<br>常见的离散标签图像数据集的样例:<br><img src="http://p9fyz2xe9.bkt.clouddn.com/18-6-1/73164393.jpg" alt="avatar"></p>
<h3 id="我们的模型"><a href="#我们的模型" class="headerlink" title="我们的模型"></a>我们的模型</h3><p>在原始GAN模型中，目标函数定义为生成器G和判别器D的博弈过程，定义V(G;D)为模型的目标函数，由生成器G和判别器D组成。语义匹配目标函数FMloss，由公式(4.6)和公式(4.8)定义。基于语义匹配的条件生成网络模型的生成器G和判别器D的目标函数分别为：<br><img src="http://p9fyz2xe9.bkt.clouddn.com/18-6-1/48822906.jpg" alt="avatar"><br>如上式，LS为样本来源，LC为分类结果。判别器D目标为最大化LC+ LS + FMloss，其试图对输入样本进行分类，并通过来源和语义匹配区分生成样本和原始样本。生成器G的目的是最大化LC − LS −FMloss，其试图通过样本分类结果、样本来源和语义匹配结果来欺骗判别器D。LS来源损失与原始GAN模型相同，LC为语义标签分类损失，在类别分类中使用交叉信息熵，在数值回归中则使用均方差回归。</p>
<h3 id="生成结果对比"><a href="#生成结果对比" class="headerlink" title="生成结果对比"></a>生成结果对比</h3><p>MNIST数据集生成结果<br><img src="http://p9fyz2xe9.bkt.clouddn.com/18-6-1/60239019.jpg" alt="avatar"><br>Fashion-MNIST数据集生成结果<br><img src="http://p9fyz2xe9.bkt.clouddn.com/18-6-1/25827392.jpg" alt="avatar"><br>SVHN数据集生成结果<br><img src="http://p9fyz2xe9.bkt.clouddn.com/18-6-1/12463336.jpg" alt="avatar"><br>CIFAR10数据集生成结果<br><img src="http://p9fyz2xe9.bkt.clouddn.com/18-6-1/62008706.jpg" alt="avatar"><br>CelebA数据集生成结果<br><img src="http://p9fyz2xe9.bkt.clouddn.com/18-6-1/12335202.jpg" alt="avatar"><br>UnityEyes数据集生成结果<br><img src="http://p9fyz2xe9.bkt.clouddn.com/18-6-1/34168567.jpg" alt="avatar"></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/05/Titanic-Challenge/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jijeng Jia">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jijeng's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/06/05/Titanic-Challenge/" itemprop="url">Titanic Challenge</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-06-05T21:40:46+08:00">
                2018-06-05
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/竞赛/" itemprop="url" rel="index">
                    <span itemprop="name">竞赛</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>用此博客记录自己解决 Kaggle Titanic challenge过程中的个人总结。</p>
<h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>说道Titanic(泰坦尼克号)，最熟悉莫过于Titanic(1997 film),这部由著名导演詹姆斯·卡梅隆执导，莱昂纳多·迪卡普里奥、凯特·温斯莱特领衔主演的电影，经久不衰…但是我们今天的画风不是这样的…</p>
<p>我们今天解决的问题是以该事件问背景，但是没有那么浪漫。<br>关于这个案例的介绍网上有很多内容，为了避免累赘，在这里就不进行详述。总的要求：预测在这个事件中乘客是否死亡。<br>附上对于train sets和 test sets中数据的介绍。<br><img src="http://p9fyz2xe9.bkt.clouddn.com/18-6-5/47360135.jpg" alt="avatar"><br>更多详细的内容参看：<a href="https://www.kaggle.com/c/titanic" target="_blank" rel="noopener">https://www.kaggle.com/c/titanic</a></p>
<h2 id="数据分析"><a href="#数据分析" class="headerlink" title="数据分析"></a>数据分析</h2><p>顺滑过渡到第二阶段，数据分析，对于竞赛而言，我感觉对于数据的认识的重要性完全不亚于模型的重要性。之后我们将再次提到这句话。<br>我将结合代码进行数据分析。</p>
<ul>
<li>pandas 原生数据分析函数<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line">train =pd.read_csv(&apos;data/train.csv&apos;)</span><br><span class="line">test =pd.read_csv(&apos;data/test.csv&apos;)</span><br><span class="line">train.describe(include=&apos;all&apos;)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p><img src="http://p9fyz2xe9.bkt.clouddn.com/18-6-5/96019771.jpg" alt="vatar"><br>除了上面 train.describe()，下面这两个也是比较常用的<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train.describe()</span><br><span class="line">train.columns</span><br></pre></td></tr></table></figure></p>
<p>因为总体的数据量比较少，所以我们选择把train set 和test set连接起来进行数据分析和处理。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">combined2 = pd.concat([train_data, test_data], axis=0)</span><br></pre></td></tr></table></figure></p>
<h3 id="数据质量分析"><a href="#数据质量分析" class="headerlink" title="数据质量分析"></a>数据质量分析</h3><ul>
<li>缺省值 </li>
</ul>
<p>对于缺省值，常用的手段就是填充，但是针对不同的数据有不同的填充手段，有的是均值填充，有的是默认值填充还有的是根据现有数据训练一个 regression进行拟合(这种情况出现在缺省的数据比较重要，对于结果的预测有比较强的相关性的时候)。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">combined2.Embarked.fillna(&apos;S&apos;, inplace=True)</span><br></pre></td></tr></table></figure>
<p>Embardked(上船港口)不是那么能表现出和结果(survival)相关的变量，我么可以直接采用某个默认值进行填充。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">combined2.Fare.fillna(np.median(combined2.Fare[combined2.Fare.notnull()]), inplace=True)</span><br></pre></td></tr></table></figure>
<p>Fare(船票)我们选择使用均值填充</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">classers = [&apos;Fare&apos;,&apos;Parch&apos;,&apos;Pclass&apos;,&apos;SibSp&apos;,&apos;TitleCat&apos;,&apos;CabinCat&apos;,&apos;Sex_female&apos;,&apos;Sex_male&apos;, &apos;EmbarkedCat&apos;, &apos;FamilySize&apos;, &apos;NameLength&apos;, &apos;FamilyId&apos;]</span><br><span class="line">age_et = ExtraTreesRegressor(n_estimators=200)</span><br><span class="line">X_train = full_data.loc[full_data.Age.notnull(),classers]</span><br><span class="line">Y_train = full_data.loc[full_data.Age.notnull(),[&apos;Age&apos;]]</span><br><span class="line">X_test = full_data.loc[full_data.Age.isnull(),classers]</span><br><span class="line">age_et.fit(X_train,np.ravel(Y_train))</span><br><span class="line">age_preds = age_et.predict(X_test)</span><br><span class="line">full_data.loc[full_data.Age.isnull(),[&apos;Age&apos;]] = age_preds</span><br></pre></td></tr></table></figure>
<p>因为在特征提取看来 age 是一个比较重要的属性（下文中使用age来进一步计算性别特征，而性别特征对于survival 是重要的因素），所以需要通过 fit来进行填充 null 值。</p>
<ul>
<li>异常值<br>异常值的检测<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">combined2.boxplot()</span><br><span class="line">plt.ylim(0, 1000)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p><img src="http://p9fyz2xe9.bkt.clouddn.com/18-6-5/40620519.jpg" alt="avatar"></p>
<p>异常值处理<br>大多数情况下我们都采取忽视，但是有时候异常值中却跟结果有比较强的相关性，比如说该题目分数在0.9的一位大神在博客中使用的特征包含名字长度。这个在我一开始的特征提取中确实没有太在意名字长度也可以当作一种和结果(servival or dead)相关的特征。</p>
<ul>
<li>重复值<br>重复值的检测<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train[train.duplicated()==True]</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>如果运行结果为空，那么就是没有重复值,如果有重复值，一般使用下面类似的代码都是可以去除掉的。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.drop_duplicates()</span><br></pre></td></tr></table></figure></p>
<h3 id="分布特征分析"><a href="#分布特征分析" class="headerlink" title="分布特征分析"></a>分布特征分析</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">#分布分析</span><br><span class="line">fig ,ax =plt.subplots(2,2, figsize=(8,6))</span><br><span class="line">sns.countplot(&apos;Embarked&apos;, data =train, ax =ax[0,0])</span><br><span class="line">sns.countplot(&apos;Pclass&apos;, data =train, ax =ax[0,1])</span><br><span class="line">sns.violinplot(&apos;Survived&apos;, &apos;Age&apos;, data= train, ax =ax[1,0]).set(ylim =(-10, 80))</span><br><span class="line">sns.countplot(x =&apos;Survived&apos;, data= train, ax =ax[1,1])</span><br><span class="line">plt.tight_layout()</span><br></pre></td></tr></table></figure>
<p><img src="http://p9fyz2xe9.bkt.clouddn.com/18-6-5/87343336.jpg" alt="avatar"><br>运行需要导入 seaborn<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">import seaborn as sns</span><br></pre></td></tr></table></figure></p>
<p>这这里安利一个数据可视化工具-seaborn。回正题 counplot()可以直观的看出单个数据的特征，但是我们更加关心的是数据和数据之间的关系，更准确的是数据和预测数据(survival )之间的关系。所以…<br>我们进行相关性分析<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">plt.subplots(figsize=(10,8))  </span><br><span class="line">corrmat = train[train.columns[1:]].corr()</span><br><span class="line">sns.set(font_scale=1.5)  </span><br><span class="line">hm = sns.heatmap(corrmat, cbar=True, annot=True, square=True, fmt=&apos;.2f&apos;, annot_kws=&#123;&apos;size&apos;: 10&#125;)  </span><br><span class="line">plt.title(&apos;Pearson Correlation of Features&apos;, y=1.05, size=15)  </span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p>
<p><img src="http://p9fyz2xe9.bkt.clouddn.com/18-6-5/90618337.jpg" alt="avatar"></p>
<p>从图中可以看出，在原始数据集特征中(为什么这么说，嗯，这意味着我们下文还要进行 generate new features),Fare特征是和 survival最相关的。这从数据角度这你船票价钱越高，你生存的几率就越大(三观尽毁)。嗯，这是符合经济社会的运行规律的。我们在分析 Pearson Correlation的时候，关注的是数值的绝对值，如果是正值，表示正相关；如果是负值，表示负相关。</p>
<p>如果细心的小伙伴发现，这个并没有把所有的变量的相关性表示出来，是的，下文我将给出一个加强版的。</p>
<h2 id="特征工程"><a href="#特征工程" class="headerlink" title="特征工程"></a>特征工程</h2><p>当我们对数据有了一个初步的认识，这时候就可以进行特征工程了。网上流传很广的一句话”数据决定了机器学习的上限，而算法只是尽可能逼近这个上限”，我深有体会。因为之前进行特征提取，然后在kaggle的submission score是0.73205,经过模型融合然后达到了0.78947，提高了5个百分点。当自己在思考数据特征重新进行特征提取的时候，最后的score 是0.82296.这都是以10个百分点的提高啊。所以这句话很有道理，我试图找到这句话的出处，以表示我对于版权的尊重，但是科学上网能力有限，没有找见，也许这就是群众的智慧吧。<br><img src="http://p9fyz2xe9.bkt.clouddn.com/18-6-5/68525093.jpg" alt="avatar"><br>图为我在kaggle 的submission 和相应的score</p>
<p>但是我想强调的是特征提取是个很难有模板化的东西，这得看个人对于这个问题的理解和对于数据的理解，对于数据异常值的处理。并且还想说的是这个一个迭代的过程，不是一步到位的。<br>当初步构造好自己特征之后可以使用图形化工具进行简单的分析一下。(下图是我第一次构造的特征工程的图形化)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">def correlation_heatmap(df):  </span><br><span class="line">    _ , ax = plt.subplots(figsize =(14, 12))  </span><br><span class="line">    colormap = sns.diverging_palette(220, 10, as_cmap = True)  </span><br><span class="line">      </span><br><span class="line">    _ = sns.heatmap(  </span><br><span class="line">        df.corr(),   </span><br><span class="line">        cmap = colormap,  </span><br><span class="line">        square=True,   </span><br><span class="line">        cbar_kws=&#123;&apos;shrink&apos;:.9 &#125;,   </span><br><span class="line">        ax=ax,  </span><br><span class="line">        annot=True,   </span><br><span class="line">        linewidths=0.1,vmax=1.0, linecolor=&apos;white&apos;,  </span><br><span class="line">        annot_kws=&#123;&apos;fontsize&apos;:12 &#125;  </span><br><span class="line">    )  </span><br><span class="line">      </span><br><span class="line">    plt.title(&apos;Pearson Correlation of Features&apos;, y=1.05, size=15)  </span><br><span class="line">train2 =train1.drop([&apos;NullCabin&apos;], axis =1)</span><br><span class="line">correlation_heatmap(train2)</span><br></pre></td></tr></table></figure></p>
<p><img src="http://p9fyz2xe9.bkt.clouddn.com/18-6-5/34929686.jpg" alt="avatar"></p>
<p>上图是第一次特征分析的结果(kaggle score: 0.73205),survival和Pclass和Fare是有较强的正相关性。</p>
<p><img src="http://p9fyz2xe9.bkt.clouddn.com/18-6-5/72110289.jpg" alt="avatar"><br>该图是第二次特征分析分析结果（kaggle score:0.82296,survival与male_adult(-0.56)、sex_male(-0.54)负相关,和female_adult(0.54)、sex_female(0.52)正相关。你的survival的概率和你的性别和年龄有关，如果你是成年女子，那么你很大的概率不会死亡(像Rose那样)；如果你是成年男子，那么你有很大概率体现英伦的绅士风度，主动(Jack那样)或者被选择死亡。瞬间想起了Titanic电影中Jack和Rose 的场景，好感人啊!!!</p>
<h2 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h2><p>发现写了这么久，还没有开始训练模型。加快脚步…下面的内容以第一次训练模型为例。在建立基本模型之前我们需要先引入评价函数，以评价不同模型性能的好坏。</p>
<p>通过均值和方差来评价模型性能的优劣<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from sklearn import cross_validation  </span><br><span class="line">def rmsl(clf):  </span><br><span class="line">    s = cross_validation.cross_val_score(clf, X_train, y_train, cv=5)  </span><br><span class="line">    return (s.mean(),s.std())</span><br></pre></td></tr></table></figure></p>
<p>建立基本模型<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">from sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis,gaussian_process</span><br><span class="line"></span><br><span class="line">NLA =[</span><br><span class="line">    #ensemble  methods</span><br><span class="line">    ensemble.AdaBoostClassifier(),</span><br><span class="line">    ensemble.BaggingRegressor(),</span><br><span class="line">    ensemble.GradientBoostingClassifier(),</span><br><span class="line">    ensemble.RandomForestClassifier(n_estimators=60),</span><br><span class="line">    </span><br><span class="line">    # Gaussian process</span><br><span class="line">    gaussian_process.GaussianProcessClassifier(),</span><br><span class="line">    # LM</span><br><span class="line">    linear_model.LogisticRegression(C=1.0, penalty=&apos;l1&apos;),</span><br><span class="line">    #Navies Bayes</span><br><span class="line">    naive_bayes.GaussianNB(),</span><br><span class="line">    # Nearest Neighbor</span><br><span class="line">    neighbors.KNeighborsClassifier(n_neighbors=3),</span><br><span class="line">    # Svm</span><br><span class="line">    svm.SVC(probability=True),</span><br><span class="line">    svm.LinearSVC(),</span><br><span class="line">    #Tree</span><br><span class="line">    tree.DecisionTreeClassifier(),</span><br><span class="line">    tree.ExtraTreeClassifier()    </span><br><span class="line">]</span><br><span class="line">#create table to compare MLA</span><br><span class="line">MLA_columns = [&apos;MLA Name&apos;, &apos;MLA Parameters&apos;,&apos;MLA Train Accuracy Mean&apos;, &apos;MLA Test Accuracy Mean&apos;, &apos;MLA Test Accuracy Min&apos; ,&apos;MLA Time&apos;]  </span><br><span class="line">MLA_compare = pd.DataFrame(columns = MLA_columns) </span><br><span class="line"></span><br><span class="line">row_index = 0  </span><br><span class="line">for alg in MLA:  </span><br><span class="line">    #set name and parameters  </span><br><span class="line">    MLA_compare.loc[row_index, &apos;MLA Name&apos;] = alg.__class__.__name__  </span><br><span class="line">    MLA_compare.loc[row_index, &apos;MLA Parameters&apos;] = str(alg.get_params())  </span><br><span class="line">      </span><br><span class="line">    #score model with cross validation:   </span><br><span class="line">    cv_results = model_selection.cross_validate(alg, X_train, y_train, cv  =5,return_train_score=True)  </span><br><span class="line">    MLA_compare.loc[row_index, &apos;MLA Time&apos;] = cv_results[&apos;fit_time&apos;].mean()  </span><br><span class="line">    MLA_compare.loc[row_index, &apos;MLA Train Accuracy Mean&apos;] = cv_results[&apos;train_score&apos;].mean()  </span><br><span class="line">    MLA_compare.loc[row_index, &apos;MLA Test Accuracy Mean&apos;] = cv_results[&apos;test_score&apos;].mean()     </span><br><span class="line">    MLA_compare.loc[row_index, &apos;MLA Test Accuracy Min&apos;] = cv_results[&apos;test_score&apos;].min()   #let&apos;s know the worst that can happen!  </span><br><span class="line">  </span><br><span class="line">    row_index+=1</span><br><span class="line"></span><br><span class="line">MLA_compare.sort_values(by = [&apos;MLA Test Accuracy Mean&apos;], ascending = False, inplace = True)</span><br></pre></td></tr></table></figure></p>
<p><img src="http://p9fyz2xe9.bkt.clouddn.com/18-6-5/48291250.jpg" alt="avatar"><br>当我们发现某个模型效果比较好的时候，我们仍然可以进一步调参。但是这种调参并不是每次会得到better result,有时候只是一个decent result。调参是个技术活。以上图中的 DecisionTreeClassifier为例进行调参。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">param_grid = &#123;&apos;criterion&apos;: [&apos;gini&apos;, &apos;entropy&apos;],   </span><br><span class="line">              &apos;splitter&apos;: [&apos;best&apos;, &apos;random&apos;],  </span><br><span class="line">              &apos;max_depth&apos;: [None, 2,4,6,8,10],  </span><br><span class="line">              &apos;min_samples_split&apos;: [5,10,15,20,25],  </span><br><span class="line">              &apos;max_features&apos;: [None, &apos;auto&apos;, &apos;sqrt&apos;, &apos;log2&apos;]  </span><br><span class="line">             &#125;</span><br><span class="line">tune_model = model_selection.GridSearchCV(tree.DecisionTreeClassifier(), param_grid=param_grid, scoring = &apos;accuracy&apos;, cv = 5)  </span><br><span class="line">cv_results = model_selection.cross_validate(tune_model, X_train, y_train, cv  = 5)</span><br></pre></td></tr></table></figure></p>
<p>使用的是sklearn 中的model_selection 模块，进行GridSearch，其实只是把调参过程自动化程序化。得到的结果是<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">0.863068608315 #train mean</span><br><span class="line">0.79803322024 # test mean</span><br><span class="line">0.77094972067 #test min</span><br></pre></td></tr></table></figure></p>
<p>我们通过比对发现这个结果和上图的结果是稍微变差的。<br>可视化显示各个算法的效率：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sns.barplot(x=&apos;MLA Test Accuracy Mean&apos;, y = &apos;MLA Name&apos;, data = MLA_compare, color = &apos;m&apos;)  </span><br><span class="line">plt.title(&apos;Machine Learning Algorithm Accuracy Score \n&apos;)  </span><br><span class="line">plt.xlabel(&apos;Accuracy Score (%)&apos;)  </span><br><span class="line">plt.ylabel(&apos;Algorithm&apos;)</span><br></pre></td></tr></table></figure></p>
<p><img src="http://p9fyz2xe9.bkt.clouddn.com/18-6-5/17534343.jpg" alt="avatar"><br>对比之后我们选取几个效果比较“好”的模型，然后进行下一步的模型融合。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">MLA_best = [  </span><br><span class="line">    #Ensemble Methods  </span><br><span class="line">    ensemble.AdaBoostClassifier(), # 0.76076  </span><br><span class="line">    ensemble.BaggingClassifier(), # 0.72248  </span><br><span class="line">    ensemble.GradientBoostingClassifier(), # 0.73684  </span><br><span class="line">    ensemble.RandomForestClassifier(n_estimators = 60), # 0.72727  </span><br><span class="line">    #GLM  </span><br><span class="line">    linear_model.LogisticRegression(C=1.0, penalty=&apos;l1&apos;, tol=1e-6), # 0.77990  </span><br><span class="line">    linear_model.RidgeClassifierCV(), # 0.77033  </span><br><span class="line">    linear_model.LogisticRegressionCV()  #0.77033  </span><br><span class="line">]  </span><br><span class="line">row_index = 0  </span><br><span class="line">for alg in MLA_best:  </span><br><span class="line">    algname = alg.__class__.__name__  </span><br><span class="line">    alg.fit(X_train, y_train)  </span><br><span class="line">    predictions = alg.predict(X_test)  </span><br><span class="line">    result = pd.DataFrame(&#123;&apos;PassengerId&apos;:test[&apos;PassengerId&apos;].as_matrix(), &apos;Survived&apos;:predictions.astype(np.int32)&#125;)  </span><br><span class="line">    result.to_csv(algname+&quot;.csv&quot;, index=False)  # save the results</span><br><span class="line">    row_index+=1</span><br></pre></td></tr></table></figure></p>
<h3 id="模型融合"><a href="#模型融合" class="headerlink" title="模型融合"></a>模型融合</h3><p>简单的说模型融合就是通过多个decent模型的结果通过某种方式的结合，产生了比原来单个模型better的结果。关于模型融合的详细内容，请移步另一篇文章<a href="https://jijeng.github.io/2018/06/05/模型融合/" target="_blank" rel="noopener">模型融合(Ensemble learning)</a><br>我们这里以stacking(二层)为例说明模型融合。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">ntrain = train.shape[0] #891  </span><br><span class="line">ntest = test.shape[0]   #418  </span><br><span class="line">SEED = 0 # for reproducibility  </span><br><span class="line">NFOLDS = 5 # set folds for out-of-fold prediction  </span><br><span class="line">kf =model_selection.KFold(n_splits=NFOLDS, random_state=SEED)</span><br><span class="line"># 封装算法基本操作  </span><br><span class="line">class SklearnHelper(object):  </span><br><span class="line">    def __init__(self, clf, seed=0, params=None):  </span><br><span class="line">        params[&apos;random_state&apos;] = seed  </span><br><span class="line">        self.clf = clf(**params)  </span><br><span class="line">  </span><br><span class="line">    def train(self, x_train, y_train):  </span><br><span class="line">        self.clf.fit(x_train, y_train)  </span><br><span class="line">  </span><br><span class="line">    def predict(self, x):  </span><br><span class="line">        return self.clf.predict(x)  </span><br><span class="line">      </span><br><span class="line">    def fit(self,x,y):  </span><br><span class="line">        return self.clf.fit(x,y)  </span><br><span class="line">      </span><br><span class="line">    def feature_importances(self,x,y):  </span><br><span class="line">        print(self.clf.fit(x,y).feature_importances_)  </span><br><span class="line">        return self.clf.fit(x,y).feature_importances_</span><br></pre></td></tr></table></figure></p>
<p>下面是定义五折交叉验证的方法，默认是三折。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">def get_oof(clf, x_train, y_train, x_test):  </span><br><span class="line">    oof_train = np.zeros((ntrain,))  </span><br><span class="line">    oof_test = np.zeros((ntest,))  </span><br><span class="line">    oof_test_skf = np.empty((NFOLDS, ntest))  </span><br><span class="line">  </span><br><span class="line">    for i, (train_index, test_index) in enumerate(kf.split(x_train)):  </span><br><span class="line">        x_tr = x_train[train_index]  </span><br><span class="line">        y_tr = y_train[train_index]  </span><br><span class="line">        x_te = x_train[test_index]  </span><br><span class="line">  </span><br><span class="line">        clf.train(x_tr, y_tr)  </span><br><span class="line">  </span><br><span class="line">        oof_train[test_index] = clf.predict(x_te)  </span><br><span class="line">        oof_test_skf[i, :] = clf.predict(x_test)  </span><br><span class="line">  </span><br><span class="line">    oof_test[:] = oof_test_skf.mean(axis=0)  </span><br><span class="line">    return  oof_train.reshape(-1, 1), oof_test.reshape(-1, 1) # 想让z变成只有一列，行数不知道多少</span><br></pre></td></tr></table></figure></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier,AdaBoostClassifier, GradientBoostingClassifier</span><br><span class="line"># 定义四个不同的弱分类器的参数值  </span><br><span class="line">  </span><br><span class="line"># Random Forest parameters  </span><br><span class="line">rf_params = &#123;  </span><br><span class="line">    &apos;n_jobs&apos;: -1,&apos;n_estimators&apos;: 500,&apos;warm_start&apos;: True, &apos;max_depth&apos;: 6,&apos;min_samples_leaf&apos;: 2,  </span><br><span class="line">     &apos;max_features&apos; : &apos;sqrt&apos;,&apos;verbose&apos;: 0#&apos;max_features&apos;: 0.2,    </span><br><span class="line">&#125;  </span><br><span class="line"># Extra Trees Parameters  </span><br><span class="line">et_params = &#123;  </span><br><span class="line">    &apos;n_jobs&apos;: -1,&apos;n_estimators&apos;:500,&apos;max_depth&apos;: 8,&apos;min_samples_leaf&apos;: 2,&apos;verbose&apos;: 0  </span><br><span class="line">    #&apos;max_features&apos;: 0.5,      </span><br><span class="line">&#125;  </span><br><span class="line"># AdaBoost parameters  </span><br><span class="line">ada_params = &#123;  </span><br><span class="line">    &apos;n_estimators&apos;: 500,&apos;learning_rate&apos; : 0.75  </span><br><span class="line">&#125;  </span><br><span class="line"># Gradient Boosting parameters  </span><br><span class="line">gb_params = &#123;  </span><br><span class="line">    &apos;n_estimators&apos;: 500,&apos;max_depth&apos;: 5,&apos;min_samples_leaf&apos;: 2, &apos;verbose&apos;: 0  </span><br><span class="line">     #&apos;max_features&apos;: 0.2,     </span><br><span class="line">&#125;  </span><br><span class="line"># Support Vector Classifier parameters   </span><br><span class="line"># svc_params = &#123;  </span><br><span class="line">#     &apos;kernel&apos; : &apos;linear&apos;,&apos;C&apos; : 0.025   </span><br><span class="line">#     &#125;  </span><br><span class="line">  </span><br><span class="line"># 创建四个若分类器模型  </span><br><span class="line">rf = SklearnHelper(clf=RandomForestClassifier, seed=SEED, params=rf_params)  </span><br><span class="line">et = SklearnHelper(clf=ExtraTreesClassifier, seed=SEED, params=et_params)  </span><br><span class="line">ada = SklearnHelper(clf=AdaBoostClassifier, seed=SEED, params=ada_params)  </span><br><span class="line">gb = SklearnHelper(clf=GradientBoostingClassifier, seed=SEED, params=gb_params)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">#X_train =X_train.values</span><br><span class="line">#X_test =X_test.values</span><br><span class="line"># 使用五折交叉方法分别计算出使用不同算法的预测结果，这些结果将用于Stacking的第二层预测  </span><br><span class="line">et_oof_train, et_oof_test = get_oof(et, X_train, y_train, X_test) # Extra Trees  </span><br><span class="line">rf_oof_train, rf_oof_test = get_oof(rf,X_train, y_train, X_test) # Random Forest  </span><br><span class="line">ada_oof_train, ada_oof_test = get_oof(ada, X_train, y_train, X_test) # AdaBoost   </span><br><span class="line">gb_oof_train, gb_oof_test = get_oof(gb,X_train, y_train, X_test) # Gradient Boost  </span><br><span class="line">  </span><br><span class="line">rf_feature = rf.feature_importances(X_train,y_train)  </span><br><span class="line">et_feature = et.feature_importances(X_train, y_train)  </span><br><span class="line">ada_feature = ada.feature_importances(X_train, y_train)  </span><br><span class="line">gb_feature = gb.feature_importances(X_train,y_train) </span><br><span class="line"></span><br><span class="line">feature_dataframe = pd.DataFrame( &#123;&apos;features&apos;: cols,  </span><br><span class="line">     &apos;Random Forest feature importances&apos;: rf_feature,  </span><br><span class="line">     &apos;Extra Trees  feature importances&apos;: et_feature,  </span><br><span class="line">      &apos;AdaBoost feature importances&apos;: ada_feature,  </span><br><span class="line">                                     </span><br><span class="line">     &apos;Gradient Boost feature importances&apos;: gb_feature  </span><br><span class="line">    &#125;)</span><br></pre></td></tr></table></figure>
<p>接下来以第一层为为基础训练第二层<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">base_predictions_train = pd.DataFrame( &#123;  </span><br><span class="line">    &apos;RandomForest&apos;: rf_oof_train.ravel(),# # ravel函数在降维时默认是行序优先  </span><br><span class="line">     &apos;ExtraTrees&apos;: et_oof_train.ravel(),  </span><br><span class="line">     &apos;AdaBoost&apos;: ada_oof_train.ravel(),  </span><br><span class="line">      &apos;GradientBoost&apos;: gb_oof_train.ravel()  </span><br><span class="line">    &#125;)  </span><br><span class="line">X_train2 = np.concatenate(( et_oof_train, rf_oof_train, ada_oof_train, gb_oof_train), axis=1)  </span><br><span class="line">X_test2 = np.concatenate(( et_oof_test, rf_oof_test, ada_oof_test, gb_oof_test), axis=1)</span><br></pre></td></tr></table></figure></p>
<p>使用XGBoost训练第二层的数据。关于XGBoost为什么是有效的和相关的概念，请移步<a href="https://jijeng.github.io/2018/06/05/XGBoost/" target="_blank" rel="noopener">XGBoost</a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"># XGboost </span><br><span class="line">import xgboost as xgb</span><br><span class="line">gbm = xgb.XGBClassifier(  </span><br><span class="line">    #learning_rate = 0.02,  </span><br><span class="line"> n_estimators= 2000,  </span><br><span class="line"> max_depth= 4,  </span><br><span class="line"> min_child_weight= 2,  </span><br><span class="line"> #gamma=1,  </span><br><span class="line"> gamma=0.9,                          </span><br><span class="line"> subsample=0.8,  </span><br><span class="line"> colsample_bytree=0.8,  </span><br><span class="line"> objective= &apos;binary:logistic&apos;,  </span><br><span class="line"> nthread= -1,  </span><br><span class="line"> scale_pos_weight=1).fit(X_train2, y_train)  </span><br><span class="line">predictions = gbm.predict(X_test2)</span><br></pre></td></tr></table></figure></p>
<p>最后产生结果文件 StackingSubmission.csv<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">StackingSubmission = pd.DataFrame(&#123;&apos;PassengerId&apos;:test.PassengerId, &apos;Survived&apos;: predictions &#125;)  </span><br><span class="line">StackingSubmission.to_csv(&quot;StackingSubmission.csv&quot;, index=False) # 0.78947</span><br></pre></td></tr></table></figure></p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>本文在效果可视化中借鉴<a href="https://blog.csdn.net/u012063773/article/details/79347499" target="_blank" rel="noopener">该博客</a><br>特征提取参看kaggle多位大神，在这里就谢过…</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/05/模型融合/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jijeng Jia">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jijeng's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/06/05/模型融合/" itemprop="url">模型融合</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-06-05T21:37:44+08:00">
                2018-06-05
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/竞赛/" itemprop="url" rel="index">
                    <span itemprop="name">竞赛</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>多个有差异性的模型融合可以提高整体的性能。它能同时降低最终模型的bias 和variance，从而在提高竞赛分数的同时降低overfitting 的风险。</p>
<h3 id="从结果文件中融合"><a href="#从结果文件中融合" class="headerlink" title="从结果文件中融合"></a>从结果文件中融合</h3><p>这种做法不需要重新训练模型，融合竞赛提交的结果文件就可以，简单便捷。</p>
<ul>
<li>Voting<br>投票制：少数服从多数。如一个分类问题，多个模型的投票（当然可以设置权重，若没有就是平均投票），最终投票数最多的类就是被预测的类。<br>对于加权表决融合，性能表现较差的模型（权值比较低）只能通过和其他模型保持一致增强自己的说服力。对于结果取平均融合，在不同的评估准则上也能获得不错的效果在于：取均值常常能减少过拟合的现象。如图所示：<br><img src="http://p9fyz2xe9.bkt.clouddn.com/18-6-4/45850387.jpg" alt="avatar"><br>如果单个模型过拟合产生了绿色的边缘，这时候去平均这种策略使得决策边界变成黑色的边缘，这样的效果更好。机器学习的目的并不是让模型记住训练数据，而是具有更好的泛化性。</li>
<li>Ranking<br>Rank的思想其实和Averaging一致，但Rank是把排名做平均，对于<strong>AUC指标</strong>比较有效。</li>
</ul>
<h3 id="训练模型融合"><a href="#训练模型融合" class="headerlink" title="训练模型融合"></a>训练模型融合</h3><ul>
<li>Bagging:<br>使用训练数据的不同随机子集来训练每个 Base Model，最后进行每个 Base Model 权重相同的 Vote。也即 Random Forest 的原理。</li>
<li>Boosting:<br>Boosting是一种迭代的方法，每一次训练会更关心上一次被分错的样本，比如改变被错分的样本的权重的Adaboost方法。也即 Gradient Boosting，Adaboost 的原理。比 Bagging 效果好，但更容易 Overfit。 </li>
<li>Blending<br>用不相交的数据训练不同的 Base Model，将它们的输出取（加权）平均。实现简单，但对训练数据利用少了。</li>
<li><p>Stackinig(以二层为例)<br>在网上为数不多的关于stacking的内容中，相信你已经看过这张图片：<br><img src="http://p9fyz2xe9.bkt.clouddn.com/18-6-4/80388486.jpg" alt="avatar"><br>PS:这不是原图，是在原图的基础上经过修改（把最上面标题的model 1，2,3,4,5 修改为model 1,1,1,1,1。因为这个一个model 的不同阶段，不是多个模型）</p>
<p>  想比较而言我更加喜欢下面这张图片，因为它把stacking 的不同阶段表达的更加清楚，尤其是经过model 1之后，model 2是在model 1的基础上进行训练的。<br><img src="http://p9fyz2xe9.bkt.clouddn.com/18-6-4/76702885.jpg" alt="avatar"><br>  对于第二阶段使用的样本集合，上图使用的是第一阶段的结果数据集，当然还有一种方式，如下图所示。<br><img src="http://p9fyz2xe9.bkt.clouddn.com/18-6-4/68508635.jpg" alt="avatar"><br>  在该图中上一阶段的结果(prob 1-N)列和原始数据集组成新的特征向量，训练第二阶段模型。</p>
</li>
</ul>
<h3 id="名词解释"><a href="#名词解释" class="headerlink" title="名词解释"></a>名词解释</h3><ul>
<li>cross validation交叉验证<br>当评估不同的参数设置，对算法表现的影响时，仍然存在则过拟合的风险。因为在调整参数，优化测试集的算法表现时，测试集的信息已经泄漏进模型中了。</li>
</ul>
<p>为了解决这个问题，需要一部分数据作为验证集(validation set)。 这样，用训练集(Train set)的数据训练模型；用验证集对模型参数调剂，如上述程序中的C值；最后，算法的评价在测试集(Test set)上完成。但是当数据有原来的两份化成三份之后，降低了寻数据量；另外算法的表现依赖于三个数据集的划分。<br>解决上述两个问题的常见方法: cross validation. 测试集仍然单独划分出来，但是 validation set不用单独划分。将训练集划分为k个小的数据集，称之为k-fold CV。对每个fold进行下列过程： </p>
<ol>
<li>用其他k-1 folds 作为training sets，训练模型</li>
<li>模型的结果用剩下的一个 fold进行评价<br>模型的性能用上述循环中的 k-fold 交叉验证集的平均值表现，这样的做法增加了计算量，但是提高数据的利用效率。</li>
</ol>
<h3 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h3><p><a href="https://blog.csdn.net/u013395516/article/details/79745063" target="_blank" rel="noopener">https://blog.csdn.net/u013395516/article/details/79745063</a><br><a href="https://blog.csdn.net/u012969412/article/details/76636336" target="_blank" rel="noopener">https://blog.csdn.net/u012969412/article/details/76636336</a><br><a href="https://blog.csdn.net/u012604810/article/details/77579782" target="_blank" rel="noopener">https://blog.csdn.net/u012604810/article/details/77579782</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/05/XGBoost/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jijeng Jia">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jijeng's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/06/05/XGBoost/" itemprop="url">XGBoost</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-06-05T21:33:28+08:00">
                2018-06-05
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/竞赛/" itemprop="url" rel="index">
                    <span itemprop="name">竞赛</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="Introduction-to-XGBoost"><a href="#Introduction-to-XGBoost" class="headerlink" title="Introduction to XGBoost"></a>Introduction to XGBoost</h3><p>XGBoost is short for “Extreme Gradient Boosting”, where the term “Gradient Boosting” is proposed in the paper Greedy Function Approximation: A Gradient Boosting Machine, by Friedman. XGBoost is based on this original model. And most of the content is based on the websit(<a href="http://xgboost.readthedocs.io/en/latest/model.html" target="_blank" rel="noopener">http://xgboost.readthedocs.io/en/latest/model.html</a>)</p>
<h3 id="element-of-Supervised-learning"><a href="#element-of-Supervised-learning" class="headerlink" title="element of Supervised learning"></a>element of Supervised learning</h3><p>XGBoost is used for supervised learning problems, where we use the training data (with multiple features) xi to predict a target variable yi.</p>
<h3 id="Objective-Function-Training-Loss-Regularization"><a href="#Objective-Function-Training-Loss-Regularization" class="headerlink" title="Objective Function: Training Loss + Regularization"></a>Objective Function: Training Loss + Regularization</h3><p>A very important fact about objective functions is they <strong>must always</strong> contain two parts: training loss and regularization.<br><img src="http://p9fyz2xe9.bkt.clouddn.com/18-6-2/76591754.jpg" alt="avatar"><br>For example, a commomly used training loss is mean squared error.<br><img src="http://p9fyz2xe9.bkt.clouddn.com/18-6-2/20584283.jpg" alt="avatar"><br>Another commonly used loss function is logistic loss for logistic regression<br><img src="http://p9fyz2xe9.bkt.clouddn.com/18-6-2/43560274.jpg" alt="avatar"></p>
<p>The <strong>regularization term</strong> is what people usually forget to add. The regularization term controls the complexity of the model, which helps us to avoid overfitting.<br>The general principle is we want both a simple and predictive model. The tradeoff between the two is also referred as bias-variance tradeoff in machine learning.</p>
<h3 id="Tree-Ensemble"><a href="#Tree-Ensemble" class="headerlink" title="Tree Ensemble"></a>Tree Ensemble</h3><p>The tree ensemble model is a set of classification and regression trees(CART). Here is a simple example of a CART that classifies whether someone will like computer games.<br><img src="http://p9fyz2xe9.bkt.clouddn.com/18-6-2/81320459.jpg" alt="avatar"><br>A CART is a bit different from decision trees, where the leaf only contains decision values. In CART, a real score is associated with each of the leaves, which gives us richer interpretations that go beyond classification. This also makes the unified optimization step easier, as we will see in a later part of this tutorial.</p>
<p>Usually, a single tree is not strong enough to be used in practice. What is actually used is the so-called tree ensemble model, which sums the prediction of multiple trees together.<br><img src="http://p9fyz2xe9.bkt.clouddn.com/18-6-2/43017405.jpg" alt="avatar"><br>Here is an example of a tree ensemble of two trees. The prediction scores of each individual tree are summed up to get the final score. If you look at the example, an important fact is that the two trees try to complement each other. </p>
<p>Now here comes the question, what is the model for random forests? It is exactly tree ensembles! So random forests and boosted trees are not different in terms of model, the difference is how we train them. This means if you write a predictive service of tree ensembles, you only need to write one of them and they should directly work for both random forests and boosted trees. One example of why elements of supervised learning rock.</p>
<h3 id="Tree-Boosting"><a href="#Tree-Boosting" class="headerlink" title="Tree Boosting"></a>Tree Boosting</h3><p>After introducing the model, let us begin with the real training part. How should we learn the trees? The answer is, as is always for all supervised learning models: define an objective function, and optimize it!</p>
<p>Assume we have the following objective function (remember it always needs to contain training loss and regularization)</p>
<h3 id="Additive-Training"><a href="#Additive-Training" class="headerlink" title="Additive Training"></a>Additive Training</h3><p>First thing we want to ask is what are the parameters of trees? You can find that what we need to learn are those functions fi, with each containing the structure of the tree and the leaf scores. This is much harder than traditional optimization problem where you can take the gradient and go. It is not easy to train all the trees at once. Instead, we use an additive strategy: fix what we have learned, and add one new tree at a time. </p>
<p>It remains to ask, which tree do we want at each step? A natural thing is to add the one that optimizes our objective.</p>
<h3 id="Training-model"><a href="#Training-model" class="headerlink" title="Training model"></a>Training model</h3><p>The XGBoost model for classification is called <strong>XGBClassifier</strong>(regression is called XGBRegressor). We can create and and fit it to our training dataset. Models are fit using the scikit-learn API and the <strong>model.fit()</strong> function.</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/05/28/等概率生成器/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jijeng Jia">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jijeng's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/05/28/等概率生成器/" itemprop="url">等概率生成器</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-05-28T22:20:24+08:00">
                2018-05-28
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/算法/" itemprop="url" rel="index">
                    <span itemprop="name">算法</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>给定一个可以等概率生成1-3的rand3()函数生成器，求解可以随机等概率生成1-7的rand7()函数生成器。</p>
<h3 id="解题思路"><a href="#解题思路" class="headerlink" title="解题思路"></a>解题思路</h3><p>使用rand3()函数生成1-9的数字，然后丢弃8,9两种可能。</p>
<h3 id="实现代码-python3"><a href="#实现代码-python3" class="headerlink" title="实现代码(python3)"></a>实现代码(python3)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">random7</span><span class="params">(self)</span>:</span></span><br><span class="line">        x =<span class="number">8</span></span><br><span class="line">        <span class="keyword">while</span> x&gt;<span class="number">7</span>:</span><br><span class="line">            x =self.random3() + (self.random3()<span class="number">-1</span>)*<span class="number">3</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">random3</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> </span><br><span class="line"><span class="keyword">if</span> __name__ ==<span class="string">"__main__"</span>:</span><br><span class="line">    print(Solution().random7())</span><br></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/05/28/机器学习优化方法/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jijeng Jia">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jijeng's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/05/28/机器学习优化方法/" itemprop="url">机器学习优化方法</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-05-28T22:10:28+08:00">
                2018-05-28
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>机器学习的优化方法有很多，这里主要解释的是 Gradient Descent 和 Newton’s Method.</p>
<ul>
<li>梯度下降（gradient  descent）</li>
</ul>
<p>梯度下降法是最早最简单，也是最为常用的最优化方法。梯度下降法实现简单，当目标函数是凸函数时，梯度下降法的解是全局解。接下来衍生的的子类:<br>批量梯度下降法（Batch Gradient Descent，BGD）<br>随机梯度下降（Stochastic Gradient Descent，SGD）</p>
<ul>
<li>牛顿法和拟牛顿法（Newton’s method &amp; Quasi-Newton Methods）</li>
</ul>
<p>牛顿法最大的特点就在于它的收敛速度很快。个人感觉牛顿法只是在在每次迭代的时候进行了两次运算，和梯度下降在总的运算次数上并没有很大的差别，为什么会收敛速度更快呢？<br><img src="http://p9fyz2xe9.bkt.clouddn.com/18-5-28/6594984.jpg" alt="avatar"></p>
<ul>
<li>名词解释</li>
</ul>
<p>凸优化:对凸优化的问题我们在基础数学上面已经有了很多解决方法，例如可以将凸优化问题Lagerange做对偶化，然后用Newton、梯度下降算法求解。<br>凸集合:<br><img src="http://p9fyz2xe9.bkt.clouddn.com/18-5-28/24366718.jpg" alt="avatar"></p>
<p>凸函数：<br><img src="http://p9fyz2xe9.bkt.clouddn.com/18-6-5/93259333.jpg" alt="avatar"><br>Jacobian矩阵和Hessian矩阵：在向量分析中, 雅可比矩阵是一阶偏导数以一定方式排列成的矩阵, 其行列式称为雅可比行列式。<br>Hessian矩阵：<br><img src="http://p9fyz2xe9.bkt.clouddn.com/18-5-28/65836119.jpg" alt="avatar"></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/05/28/求解平方根/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jijeng Jia">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jijeng's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/05/28/求解平方根/" itemprop="url">求解平方根</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-05-28T22:03:59+08:00">
                2018-05-28
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/算法/" itemprop="url" rel="index">
                    <span itemprop="name">算法</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>求解平方根的算法主要有两种：二分法(binary search) 和牛顿迭代法(Newton’s Method)</p>
<h3 id="Just-give-me-codes…"><a href="#Just-give-me-codes…" class="headerlink" title="Just give me codes…"></a>Just give me codes…</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> math <span class="keyword">import</span> sqrt</span><br><span class="line"><span class="comment"># you needn't import sqrt, I do that just for comparing the results of different methods</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="comment"># in-built function</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">my_sqrt</span><span class="params">(self, n)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> sqrt(n)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sqrt_binary</span><span class="params">(self, n)</span>:</span></span><br><span class="line">        low, high =<span class="number">0</span>,n</span><br><span class="line">        mid =(low+high)/<span class="number">2</span></span><br><span class="line">        <span class="keyword">while</span> abs(mid*mid- n)&gt; <span class="number">1e-9</span>:</span><br><span class="line">            <span class="keyword">if</span> mid*mid&gt;n:</span><br><span class="line">                high =mid</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                low =mid</span><br><span class="line">            mid =(low+high)/<span class="number">2</span></span><br><span class="line">        <span class="keyword">return</span> mid</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># Newton method: math</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">newton_method</span><span class="params">(self, n)</span>:</span></span><br><span class="line">        k =<span class="number">1</span></span><br><span class="line">        <span class="keyword">while</span> abs(k*k-n) &gt;<span class="number">1e-9</span>:</span><br><span class="line">            k =(k+n/k)/<span class="number">2</span></span><br><span class="line">        <span class="keyword">return</span> k</span><br><span class="line">        </span><br><span class="line"><span class="keyword">if</span> __name__ ==<span class="string">"__main__"</span>:</span><br><span class="line">    print(Solution().my_sqrt(<span class="number">2</span>))</span><br><span class="line">    print(Solution().sqrt_binary(<span class="number">2</span>))</span><br><span class="line">    print(Solution().newton_method(<span class="number">2</span>))</span><br></pre></td></tr></table></figure>
<h3 id="运行结果"><a href="#运行结果" class="headerlink" title="运行结果"></a>运行结果</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1.4142135623730951 #built-in function</span><br><span class="line">1.4142135623842478 # binary search</span><br><span class="line">1.4142135623746899 # Newton method</span><br></pre></td></tr></table></figure>
<p>从结果中看，Newton method比 binary sqrt更加接近系统自带的sqrt function. 并且从数学上可以证明 Newton method 比 binary sqrt需要更少的迭代次数。</p>
<h3 id="附录："><a href="#附录：" class="headerlink" title="附录："></a>附录：</h3><p>牛顿迭代法是求方程根的重要方法之一，其最大优点是在方程f(x) = 0的单根附近具有<strong>平方收敛</strong>，而且该法还可以用来求方程的重根,复根。<br>牛顿迭代法结论其实就是取泰勒级数前两项等于0求得的,泰勒公式表示为<br><img src="http://p9fyz2xe9.bkt.clouddn.com/18-5-28/68882814.jpg" alt="avatar"><br>更简练的写法为：<br><img src="http://p9fyz2xe9.bkt.clouddn.com/18-5-28/64348458.jpg" alt="avatar"><br>通过 Newton methond逼近方程的解的过程可以表示为下图所以：<br><img src="http://p9fyz2xe9.bkt.clouddn.com/18-5-28/9966097.jpg" alt="avatar"><br>迭代公式的推导过程：</p>
<h3 id="referrences"><a href="#referrences" class="headerlink" title="referrences:"></a>referrences:</h3><p><a href="https://blog.csdn.net/ycf74514/article/details/48996383" target="_blank" rel="noopener">https://blog.csdn.net/ycf74514/article/details/48996383</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  


          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/avatar.jpeg"
                alt="Jijeng Jia" />
            
              <p class="site-author-name" itemprop="name">Jijeng Jia</p>
              <p class="site-description motion-element" itemprop="description">Stay Hungry, Stay Foolish</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">8</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">4</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">13</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/jijeng" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:jia1509309698@163.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Jijeng Jia</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
