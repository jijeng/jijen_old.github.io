<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="Stay Hungry, Stay Foolish">
<meta property="og:type" content="website">
<meta property="og:title" content="Jijeng&#39;s blog">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="Jijeng&#39;s blog">
<meta property="og:description" content="Stay Hungry, Stay Foolish">
<meta property="og:locale" content="default">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Jijeng&#39;s blog">
<meta name="twitter:description" content="Stay Hungry, Stay Foolish">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/"/>





  <title>Jijeng's blog</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Jijeng's blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="Searching..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/06/09/lr/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jijeng Jia">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jijeng's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/06/09/lr/" itemprop="url">逻辑回归概念</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-06-09T12:08:16+08:00">
                2019-06-09
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/06/09/lr/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2019/06/09/lr/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>逻辑回归从线性回归引申而来，对回归的结果进行 logistic 函数运算，将范围限制在[0,1]区间，并更改损失函数为二值交叉熵损失，使其可用于2分类问题(通过得到的概率值与阈值比较进行分类)。</p>
<h2 id="损失函数："><a href="#损失函数：" class="headerlink" title="损失函数："></a>损失函数：</h2><p>$$<br>\begin{align}<br>  f(x) = a + b \<br>       = c + d  \<br>\end{align}<br>$$<br>$$<br>\begin{equation}<br>D(x) = \begin{cases}<br>1, &amp; \text{if } x \in \mathbb{Q}; \<br>0, &amp; \text{if } x \in<br>     \mathbb{R}\setminus\mathbb{Q}.<br>\end{cases}<br>\end{equation}<br>$$<br>如果用平方误差（MSE）作为逻辑回归的损失函数,那么函数曲线将是跳跃式的,非凸的(non-convex),原因是logistic函数将数据范围限制在[0,1]区间,而真实标签值非0即1.最小化 MSE 损失容易陷入局部极小点.逻辑回归损失是如下的分情况的凸函数(单个x与y的损失):<br>$$<br>Cost( h_{\theta}(x), y )=<br>\begin{cases}<br>    -\log(h_{\theta}(x)),  &amp; y =1 \<br>    -\log(1- h_{\theta}(x)),  &amp; y =0 \<br>end{cases}<br>$$<br>为什么LR模型的损失函数是交叉熵,而线性回归模型的损失函数却是最小二乘呢？能否随意确定一个损失函数作为目标呢？<br>模型的损失函数由各自的响应变量y的概率分布决定，对于线性回归模型，其输出是连续值，所以我们对于该问题假设y服从正态分布；相对的，LR模型一般用来解决二分类问题，所以其输出是0/1，故而我们假设其输出服从伯努利分布；而进一步地，两者的损失函数都是通过极大似然估计推导的来的，所以模型的损失函数并非随意确定。<br>分类模型与回归模型之间有种种联系,比如 SVM 模型可以看作逻辑回归加L2正则项, 并使用了不同的损失函数.<br>$$<br>p(x) =<br>\begin{cases}<br>  p, &amp; x = 1 \<br>  1 - p, &amp; x = 0 \<br>\end{cases}<br>$$</p>
<p>为什么不使用回归模型来做分类?<br>这是一种不好的做法, 因为阈值不好确定, 随着数据集的变动, 阈值也需要有较大变化.</p>
<p><strong>正则项：</strong></p>
<ul>
<li>L2 解决过拟合</li>
<li>L1 解决数据稀疏性 </li>
</ul>
<p><strong> L1和L2正则先验分别服从什么分布</strong><br>从信息论的角度看，向系统加入了正确先验这个信息，肯定会提高系统的性能。两者的差别感性的理解？L1是拉普拉斯分布，L2是高斯分布。<br>拉普拉斯分布：<br>$$<br>f ( x | \mu , b ) = \frac { 1 } { 2 b } e ^ { - \frac { | x - \mu | } { b } }<br>$$<br>高斯分布：<br>$$<br>f \left( x | \mu , \sigma ^ { 2 } \right) = \frac { 1 } { \sqrt { 2 \pi \sigma ^ { 2 } } } e ^ { - \frac { ( x - \mu ) ^ { 2 } } { 2 \sigma ^ { 2 } } }<br>$$</p>
<h2 id="逻辑回归的特点："><a href="#逻辑回归的特点：" class="headerlink" title="逻辑回归的特点："></a>逻辑回归的特点：</h2><ul>
<li><p>优点：<br>LR 能以概率的形式输出结果,而非只是 0,1 判定， 可以做 ranking model；<br>LR 的可解释性强,可控度高；<br>训练快</p>
</li>
<li><p>缺点：<br>容易欠拟合，一般准确度不太高<br>只能处理两分类问题. (可以应用多个逻辑回归实现多分类,类似SVM的方式; 另外对于父子类别同时分类的情况,使用逻辑回归要比Softmax等方式效果好)</p>
</li>
</ul>
<p>“海量离散特征+简单模型” 同“少量连续特征+复杂模型”的权衡。既可以离散化用线性模型，也可以用连续特征加深度学习</p>
<p><strong> 为什么对特征进行离散化</strong></p>
<p>在工业界，很少直接将连续值作为逻辑回归模型的特征输入，而是将连续特征离散化为一系列0、1特征(one-hot编码)交给逻辑回归模型，这样做的优势有以下几点<br>离散特征的增加和减少都很容易，易于模型的快速迭代；<br>稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展；<br>离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄&gt;30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰；<br>单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合<br>离散化后可以进行特征交叉</p>
<p>具体的推导过程，可以参见纸质版打印。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/06/08/svm-all-you-need/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jijeng Jia">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jijeng's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/06/08/svm-all-you-need/" itemprop="url">SVM All You Need to Know</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-06-08T11:37:54+08:00">
                2019-06-08
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/06/08/svm-all-you-need/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2019/06/08/svm-all-you-need/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="SVM-理论"><a href="#SVM-理论" class="headerlink" title="SVM 理论"></a>SVM 理论</h2><p>支持向量机分为三个部分，线性可分支持向量机、线性支持向量机、非线性支持向量机。</p>
<h3 id="SVM-原理"><a href="#SVM-原理" class="headerlink" title="SVM 原理"></a>SVM 原理</h3><p>SVM 是一种二类分类模型。它的基本模型是在特征空间中寻找间隔最大化的分离超平面的线性分类器。</p>
<p>当训练样本线性可分时，通过硬间隔最大化，学习一个线性分类器，即线性可分支持向量机；<br>当训练数据近似线性可分时，引入松弛变量，通过软间隔最大化，学习一个线性分类器，即线性支持向量机；<br>当训练数据线性不可分时，通过使用核技巧及软间隔最大化，学习非线性支持向量机。<br>以上各种情况下的数学推到应当掌握，硬间隔最大化（几何间隔）、学习的对偶问题、软间隔最大化（引入松弛变量）、非线性支持向量机（核技巧）。</p>
<h3 id="SVM-为什么采用间隔最大化"><a href="#SVM-为什么采用间隔最大化" class="headerlink" title="SVM 为什么采用间隔最大化"></a>SVM 为什么采用间隔最大化</h3><p>当训练数据线性可分时，存在无穷个分离超平面可以将两类数据正确分开。感知机利用误分类最小策略，求得分离超平面，不过此时的解有无穷多个。线性可分支持向量机利用间隔最大化求得最优分离超平面，这时，解是唯一的。另一方面，此时的分隔超平面所产生的分类结果是最鲁棒的，对未知实例的泛化能力最强。可以借此机会阐述一下几何间隔以及函数间隔的关系。</p>
<h3 id="为什么要将求解-SVM-的原始问题转换为其对偶问题"><a href="#为什么要将求解-SVM-的原始问题转换为其对偶问题" class="headerlink" title="为什么要将求解 SVM 的原始问题转换为其对偶问题"></a>为什么要将求解 SVM 的原始问题转换为其对偶问题</h3><p>一是对偶问题往往更易求解，当我们寻找约束存在时的最优点的时候，约束的存在虽然减小了需要搜寻的范围，但是却使问题变得更加复杂。为了使问题变得易于处理，我们的方法是把目标函数和约束全部融入一个新的函数，即拉格朗日函数，再通过这个函数来寻找最优点。二是可以自然引入核函数，进而推广到非线性分类问题。</p>
<h3 id="为什么-SVM-要引入核函数"><a href="#为什么-SVM-要引入核函数" class="headerlink" title="为什么 SVM 要引入核函数"></a>为什么 SVM 要引入核函数</h3><p>当样本在原始空间线性不可分时，可将样本从原始空间映射到一个更高维的特征空间，使得样本在这个特征空间内线性可分。而引入这样的映射后，所要求解的对偶问题的求解中，无需求解真正的映射函数，而只需要知道其核函数。核函数的定义：K(x,y)=&lt;ϕ(x),ϕ(y)&gt;，即在特征空间的内积等于它们在原始样本空间中通过核函数 K 计算的结果。一方面数据变成了高维空间中线性可分的数据，另一方面不需要求解具体的映射函数，只需要给定具体的核函数即可，这样使得求解的难度大大降低。<br><img src="https://ws1.sinaimg.cn/large/e9a223b5ly1g3tlf561hbj20em07bmxe.jpg" alt=""></p>
<h3 id="为什么SVM对缺失数据敏感"><a href="#为什么SVM对缺失数据敏感" class="headerlink" title="为什么SVM对缺失数据敏感"></a>为什么SVM对缺失数据敏感</h3><p>这里说的缺失数据是指缺失某些特征数据，向量数据不完整。SVM 没有处理缺失值的策略。而 SVM 希望样本在特征空间中线性可分，所以特征空间的好坏对SVM的性能很重要。缺失特征数据将影响训练结果的好坏。</p>
<h3 id="SVM-核函数之间的区别"><a href="#SVM-核函数之间的区别" class="headerlink" title="SVM 核函数之间的区别"></a>SVM 核函数之间的区别</h3><p>一般选择线性核和高斯核，也就是线性核与 RBF 核。 线性核：主要用于线性可分的情形，参数少，速度快，对于一般数据，分类效果已经很理想了。 RBF 核：主要用于线性不可分的情形，参数多，分类结果非常依赖于参数。有很多人是通过训练数据的交叉验证来寻找合适的参数，不过这个过程比较耗时。 如果 Feature 的数量很大，跟样本数量差不多，这时候选用线性核的 SVM。 如果 Feature 的数量比较小，样本数量一般，不算大也不算小，选用高斯核的 SVM。</p>
<p>以上是几个问题在面试中遇到 SVM 算法时，几乎是必问的问题，另外，大家一定要做到自己可以推导集合间隔、函数间隔以及对偶函数，并且理解对偶函数的引入对计算带来的优势。</p>
<h3 id="支持向量"><a href="#支持向量" class="headerlink" title="支持向量"></a>支持向量</h3><p><img src="https://ws1.sinaimg.cn/large/e9a223b5ly1g3tlp6gqqzj20af07pweh.jpg" alt=""></p>
<p>如图所示，上面只有三个点与求解的优化问题有关，它们就叫做支持向量。</p>
<h2 id="SVM公式推导-线性可分条件下"><a href="#SVM公式推导-线性可分条件下" class="headerlink" title="SVM公式推导(线性可分条件下)"></a>SVM公式推导(线性可分条件下)</h2><p>假定样本空间如下<br>$$<br>{ ( x _ { 1 } , y _ { 1 } ) , ( x _ { 2 } , y _ { 2 } ) , \ldots , ( x _ { N } , y _ { N } ) }<br>$$</p>
<p>共有N个向量，其中$x_k$是一个特征向量而不是一个单一数值。</p>
<ol>
<li>这是一个二分类问题，所以$y=+1 $或者$ y=−1$。那么我们就可以得到</li>
</ol>
<p><img src="https://ws1.sinaimg.cn/large/e9a223b5ly1g3tmhb22yaj20i6042dfn.jpg" alt=""></p>
<p>那么，我们可以得到<br>$$<br>y _ { i } \cdot ( w ^ { T } x _ { i } + b ) \geq 1 , i = 1,2 , \ldots , N<br>$$</p>
<ol start="2">
<li><p>因为我们现在只讨论线性可分情况下的支持向量机，那么在这个样本空间中一定存在一个超平面可以将样本集按照y的值分割城两个部分，这个超平面可以表示为<br>$$<br>w ^ { T } x + b = 0<br>$$</p>
</li>
<li><p>根据这个超平面的表达式以及第一步推到中我们得到的结果，可以得到这个样本集中任意一个样本点距离超平面的距离：<br>$$<br>\gamma = \frac { | w ^ { T } x + b | } { | w | } \geq \frac { 1 } { | w | }<br>$$<br>由此，我们还可以进一步得到整个margin的宽度：<br>$$<br>\gamma = \frac { 2 } { | w | }<br>$$</p>
</li>
<li><p>由此，根据第一步和第三步的结果，我们可以得到最基本的目标函数：</p>
</li>
</ol>
<p>$$<br>\arg \max _ { w , b } \frac { 2 } { | w | }<br>$$<br>$$<br>\text {s.t. } y _ { i } ( w ^ { T } x _ { i } + b ) \geq 1 , i = 1,2 , \ldots , N<br>$$</p>
<ol start="5">
<li>我们还可以对这个目标函数进一步做变化：</li>
</ol>
<p>$$<br>\arg \min _ { w , b } \frac { 1 } { 2 } | w | ^ { 2 }<br>$$<br>$$<br>\text {s.t. } y _ { i } ( w ^ { T } x _ { i } + b ) \geq 1 , i = 1,2 , \ldots , N<br>$$</p>
<ol start="6">
<li><p>我们无法继续直接进行计算了，因此引入拉格朗日乘子<br>$$<br>L ( w , b , \alpha ) = \frac { 1 } { 2 } | w | ^ { 2 } + \sum _ { i } \alpha _ { i } [ 1 - y _ { i } ( w ^ { T } x _ { i } + b ) ]<br>$$</p>
</li>
<li><p>对w和b分别求L的偏导，并令其偏导数等于0：<br>$$<br>\frac { \partial L } { \partial w } = w - \sum _ { i } \alpha _ { i } y _ { i } x _ { i } = 0 arrow w = \sum _ { i } \alpha _ { i } y _ { i } x _ { i }<br>$$<br>$$<br>\frac { \partial L } { \partial b } = \sum _ { i } \alpha _ { i } y _ { i } = 0<br>$$</p>
</li>
<li><p>将第七步得到的w和b代入L函数</p>
</li>
</ol>
<p><img src="https://ws1.sinaimg.cn/large/e9a223b5ly1g3tmfcsgk1j20y60gg74q.jpg" alt=""></p>
<ol start="9">
<li><p>至此，我们的目标函数已经变成了<br>$$<br>\arg \max _ { \alpha } ( \sum _ { i } \alpha _ { i } - \frac { 1 } { 2 } \sum _ { i } \sum _ { j } \alpha _ { i } \alpha _ { j } y _ { i } y _ { j } x _ { i } ^ { T } x _ { j } )<br>$$<br>$$<br>\text { s.t. } \sum _ { i } \alpha _ { i } y _ { i } = 0<br>$$<br>$$<br>\alpha _ { i } \geq 0 , i = 1,2 , \ldots , N<br>$$</p>
</li>
<li><p>用数值方法解出α以后，我们就可以得到</p>
</li>
</ol>
<p>$$<br>w^ { * } = \sum _ { i } \alpha _ { i } ^ { * } y _ { i } x _ { i }<br>$$</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/06/01/nlp-papers-reading-sentence-embedding/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jijeng Jia">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jijeng's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/06/01/nlp-papers-reading-sentence-embedding/" itemprop="url">NLP  Papers Reading-Sentence Embedding</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-06-01T16:33:51+08:00">
                2019-06-01
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index">
                    <span itemprop="name">NLP</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/06/01/nlp-papers-reading-sentence-embedding/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2019/06/01/nlp-papers-reading-sentence-embedding/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="Why-Consider-Sentence-Embedding"><a href="#Why-Consider-Sentence-Embedding" class="headerlink" title="Why Consider Sentence Embedding?"></a>Why Consider Sentence Embedding?</h2><p>One simple way you could do this is by generating a word embedding for each word in a sentence, adding up all the embeddings and divide by the number of words in the sentence to get an “average” embedding for the sentence.</p>
<p>Alternatively, you could use a more advanced method which attempts to add a weighting function to word embeddings which down-weights common words. This latter approach is known as Smooth Inverse Frequency (SIF).</p>
<p>These methods can be used as a successful proxy for sentence embeddings. However, this “success” depends on the dataset being used and the task you want to execute. So for some tasks these methods could be good enough</p>
<p>However, there are a number of issues with any of these types of approaches:</p>
<ul>
<li>They ignore word ordering. So your product is easy to use, I do not need any help is identical to I do need help, your product is not easy to use. This is obviously problematic.</li>
<li>It’s difficult to capture the semantic meaning of a sentence. The word crash can be used in multiple contexts, e.g. I crashed a party, the stock market crashed, or I crashed my car. It’s difficult to capture this change of context in a word embedding.</li>
<li>Sentence length becomes problematic. With sentences we can chain them together to create a long sentence without saying very much, The Philadelphia Eagles won the Super Bowl, The Washington Post reported that the Philadelphia Eagles won the Super Bowl, The politician claimed it was fake news when the Washington Post reported that the Philadelphia Eagles won the Super Bowl, and so on. All these sentences are essentially saying the same thing but if we just use word embeddings, it can be difficult to discover if they are similar.</li>
<li>They introduce extra complexity. When using word embeddings as a proxy for sentence embeddings we often need to take extra steps in conjunction with the base model. For example, we need to remove stop words, get averages, measure sentence length and so on.</li>
</ul>
<p>sentence embedding的引用场景：<br>Similar approaches can be used to go beyond representations and semantic search, to document classification and understanding and eventually document summarizing or generation. </p>
<h2 id="Words-Embed"><a href="#Words-Embed" class="headerlink" title="Words Embed"></a>Words Embed</h2><h3 id="平均词向量与TFIDF加权平均词向量"><a href="#平均词向量与TFIDF加权平均词向量" class="headerlink" title="平均词向量与TFIDF加权平均词向量"></a>平均词向量与TFIDF加权平均词向量</h3><h3 id="SIF加权平均词向量"><a href="#SIF加权平均词向量" class="headerlink" title="SIF加权平均词向量"></a>SIF加权平均词向量</h3><p>来自论文 A simple but tough-to-beat baseline for sentence embeddings，更多信息可以参考<a href="https://jijeng.github.io/2019/05/27/paper-reading-nlp/" target="_blank" rel="noopener">这里</a>。</p>
<h3 id="利用n-grams-embedding"><a href="#利用n-grams-embedding" class="headerlink" title="利用n-grams embedding"></a>利用n-grams embedding</h3><p><a href="https://jijeng.github.io/2019/03/25/fastText-faiss/" target="_blank" rel="noopener">fasttext</a> 介绍。</p>
<h3 id="DAN"><a href="#DAN" class="headerlink" title="DAN"></a>DAN</h3><p>（Deep Unordered Composition Rivals Syntactic Methods for Text Classification）<br>其实DAN(Deep Averaging Networks)应该属于Bag of Words类的算法。因为比较特殊，单独列出来。 它是在对所有词语取平均后，在上面加上几层神经网络。特殊的地方在于它在sentiment analysis中表现也不错，这在BOW类方法中比较罕见。</p>
<p>文中提出了DAN(Deep average network)，说白了就是对于一个句子或者一个段落，把每个单词的embedding进行平均求和，得到第一层固定维度的向量，然后在套几层全连接神经网络。<br>本质来讲，这个模型没有考虑单词之间的顺序，not在第一个位置和在最后一个位置对于DAN来讲输入都是一样的，所以自然conver不住这种情况。这是模型本身的问题，没有办法改进，除非换模型，比如textcnn就能很好的解决这种情况<br>对于否定词敏感，比如but,not等，常常判断为negative。<br>训练速度快，且结果较好，和Syntactic Composition性能差不多，但是消耗的计算资源少<br>作为有监督学习任务来讲，可以试一试。但是由于全连接层，无法进行无监督学习。相反，NBOW可以无监督学习，比如文本相似度计算等。当然。对于DAN而言，可以通过迁移学习，预训练好全连接参数，实现无监督学习</p>
<p><img src="http://ww1.sinaimg.cn/large/e9a223b5ly1g3lqb2p5otj20bg08djt6.jpg" alt=""></p>
<p>总结一下：对简单的任务来说，用简单的网络结构进行处理基本就够了，但是对比较复杂的任务，还是依然需要更复杂的网络结构来学习sentence representation的。</p>
<h2 id="Unsupervised-Sentence-Embed"><a href="#Unsupervised-Sentence-Embed" class="headerlink" title="Unsupervised Sentence Embed"></a>Unsupervised Sentence Embed</h2><h3 id="基于Encoder-decoder的Skip-Thought-Vectors"><a href="#基于Encoder-decoder的Skip-Thought-Vectors" class="headerlink" title="基于Encoder-decoder的Skip-Thought Vectors"></a>基于Encoder-decoder的Skip-Thought Vectors</h3><p>Continuing the tour of older papers that started with our ResNet blog post, we now take on Skip-Thought Vectors by Kiros et al. Their goal was to come up with a useful embedding for sentences that was not tuned for a single task and did not require labeled data to train. They took inspiration from Word2Vec skip-gram (you can find my explanation of that algorithm here) and attempt to extend it to sentences.</p>
<p>Changing a single word has had almost no effect on the meaning of that sentence. To account for these word level changes, the skip-thought model needs to be able to handle a large variety of words, some of which were not present in the training sentences. The authors solve this by using a pre-trained continuous bag-of-words (CBOW) Word2Vec model and learning a translation from the Word2Vec vectors to the word vectors in their sentences. Below are shown the nearest neighbor words after the vocabulary expansion using query words that do not appear in the training vocabulary:</p>
<p>论文描述了一种通用、分布式句子编码器的无监督学习方法。使用从书籍中提取的连续文本，训练了一个编码器-解码器模型，借鉴了word2vec中skip-gram模型，通过一句话来预测这句话的上一句和下一句。<br>语义和语法属性一致的句子被映射到相似的向量表示。接着引入一个简单的词汇扩展方法来编码不再训练集内的单词，令词汇量扩展到一百万词。<br>本文的模型被称为skip-thoughts，生成的向量称为skip-thought vector。模型采用了当下流行的端到端框架，通过搜集了大量的小说作为训练数据集，将得到的模型中encoder部分作为feature extractor，可以给任意句子生成vector。</p>
<p>skip-thought模型结构借助了skip-gram的思想。在skip-gram中，是以中心词来预测上下文的词；在skip-thought同样是利用中心句子来预测上下文的句子。</p>
<p><img src="http://ww1.sinaimg.cn/large/e9a223b5ly1g3kgkdu4ivj20t7049glk.jpg" alt=""><br>论文采用了GRU-RNN作为encoder和decoder，encoder部分的最后一个词的hidden<br>state作为decoder的输入来生成词。这里用的是最简单的网络结构，并没有考虑复杂的多层网络、双向网络等提升效果。decoder部分也只是一个考虑了encoder last hidden state的语言模型，并无其他特殊之处，只是有两个decoder，是一个one maps two的情况，但计算方法一样。模型中的目标函数也是两个部分，一个来自于预测下一句，一个来自于预测上一句。<br><img src="http://ww1.sinaimg.cn/large/e9a223b5ly1g3khqremrtj20k001zwet.jpg" alt=""><br>我们将构造一个类似于自编码器的序列到序列结构，但是它与自编码器有两个主要的区别。第一，我们有两个 LSTM 输出层：一个用于之前的句子，一个用于下一个句子；第二，我们会在输出 LSTM 中使用教师强迫（teacher forcing）。这意味着我们不仅仅给输出 LSTM 提供了之前的隐藏状态，还提供了实际的前一个单词（可在上图和输出最后一行中查看输入）。</p>
<p>看上去，Skip-thought和Skip-gram挺象。唯一的遗憾是Skip-thought的decoder那部分，它是作为language modeling来处理的.</p>
<p><img src="http://ww1.sinaimg.cn/large/e9a223b5ly1g3kjhda478j20hz09p3yp.jpg" alt=""></p>
<p>从这里的讲解知道这个是不存在 ”正负“样本的， 这个的损失函数是 正确的上下句和生成的上下句之间的reconstruction error。<br><img src="http://ww1.sinaimg.cn/large/e9a223b5ly1g3kk2h1usnj218g0hvn0n.jpg" alt=""></p>
<p>The end product of Skip-Thoughts is the Encoder. The Decoders are thrown away after training. The trained encoder can then be used to generate fixed length representations of sentences which can be used for several downstream tasks such as sentiment classification, semantic similarity, etc.</p>
<p>The encoder utilises a word embedding layer that serves as a look up table. This converts each word in the input sentence to its corresponding word embedding, effectively converting the input sentence into a sequence of word embeddings. This embedding layer is also shared with both of the decoders. The model is then trained to minimise the reconstruction error of the previous and next sentences using the resulting embedding h(i) generated from sentence s(i) after it is passed through the encoder. Back propagating the reconstruction error from the decoder allows the encoder to learn the best representation of the input sentence while capturing the relation between itself and the surrounding sentences.<br>Skip-Thoughts is designed to be a sentence encoder and the result is that the decoders are actually discarded after the training process. The encoder along with the word embedding layer is used as a feature extractor able to encode new sentences that are fed through it. Using cosine similarity on the resulting encoded sentence embeddings, provides a powerful semantic similarity mechanism, where you can measure how closely two sentences relate in terms of meaning as well as syntax.</p>
<p><strong>Encoder Network:</strong> The encoder is typically a GRU-RNN which generates a fixed length vector representation h(i) for each sentence S(i) in the input. The encoded representation h(i) is obtained by passing final hidden state of the GRU cell (i.e. after it has seen the entire sentence) to multiple dense layers.</p>
<p><strong>Decoder Network:</strong> The decoder network takes this vector representation h(i) as input and tries to generate two sentences — S(i-1) and S(i+1), which could occur before and after the input sentence respectively. Separate decoders are implemented for generation of previous and next sentences, both being GRU-RNNs. The vector representation h(i) acts as the initial hidden state for the GRUs of the decoder networks.</p>
<p><strong>词汇扩展</strong></p>
<p>作者在训练完过后用在Google News dataset上预训练的模型对Vocabulary进行了词汇扩展主要是为了弥补我们的 Decoder 模型中词汇不足的问题。具体的做法就是：<br><img src="http://ww1.sinaimg.cn/large/e9a223b5ly1g3lpn0w28jj2184068wex.jpg" alt=""><br>(from <a href="https://www.cnblogs.com/jiangxinyang/p/9638991.html" target="_blank" rel="noopener">https://www.cnblogs.com/jiangxinyang/p/9638991.html</a>)</p>
<p>该思路借鉴于Tomas Mikolov的一篇文章Exploiting Similarities among Languages for Machine Translation中解决机器翻译missing words问题的思路，对训练集产生的词汇表V(RNN)进行了扩展，具体的思路可参考Mikolov的文章，达到的效果是建立了大数据集下V(Word2Vec)和本文V(RNN)之间的映射，V(Word2Vec)的规模远远大于V(RNN)，论文中V(RNN)包括了20000个词，V(Word2Vec)包括了930000多个词，成功地解决了这一问题，使得本文提出的无监督模型有大的应用价值。</p>
<p><strong>评价观点</strong><br>这个方法只是适用于长文本，要求是至少有两个衔接的句子，思想和skip-gram 比较相近。<br><img src="http://ww1.sinaimg.cn/large/e9a223b5ly1g3lprunwuyj212u0guq4v.jpg" alt=""></p>
<p><strong>源码</strong><br><a href="https://github.com/tensorflow/models/tree/master/research/skip_thoughts" target="_blank" rel="noopener">google 的实现</a><br><a href="https://github.com/ryankiros/skip-thoughts" target="_blank" rel="noopener">作者的实现</a><br><a href="https://papers.nips.cc/paper/5950-skip-thought-vectors.pdf" target="_blank" rel="noopener">论文</a></p>
<h3 id="Quick-Thought-vectors"><a href="#Quick-Thought-vectors" class="headerlink" title="Quick-Thought vectors"></a>Quick-Thought vectors</h3><p>2018年发表的论文An efficient framework for learning sentence representations提出了一种简单且有效的框架用于学习句子表示。和常规的编码解码类模型（如skip-thoughts和SDAE）不同的是，本文采用一种分类器的方式学习句子表示。具体地，模型的输入为一个句子$s$以及一个候选句子集合$S_{cand}$，其中$S_{cand}$包含一个句子$s_{ctxt}$是$s$的上下文句子（也就是$s $)的前一个句子或后一个句子）以及其他不是$s$上下文的句子。模型通过对$s$以及$S_{cand}$中的每个句子进行编码，然后输入到一个分类器中，让分类器选出$S_{cand}$中的哪个句子是$s_{ctxt}$。实验设置候选句子集合大小为3，即$S_{cand}​$包含1个上下文句子和两个无关句子。模型结构如下：</p>
<p>模型有如下两个细节需要注意：<br>模型使用的分类器（得分函数）$c$非常简单，是两个向量内积，即$c(u, v)=u^Tv$，计算$s$的embedding与所有$S_{cand}$中的句子向量内积得分后，输入到softmax层进行分类。使用简单分类器是为了引导模型着重训练句子编码器，因为我们的目的是为了得到好的句子向量表示而不是好的分类器。<br><img src="http://ww1.sinaimg.cn/large/e9a223b5ly1g3lq4allnfj20gk0ji0th.jpg" alt=""><br>虽然某些监督任务模型如文本蕴含模型是参数共享的，$s$的编码器参数和候选句子编码器参数是不同的（不共享），因为句子表示学习往往是在大规模语料上进行训练，不必担心参数学习不充分的问题。测试时，给定待编码句子$s$，通过该模型得到的句子表示是两种编码器的连结 $[ f ( s ) ;g ( s ) ]$。</p>
<p><img src="http://ww1.sinaimg.cn/large/e9a223b5ly1g3klfpv0g9j20k0083go0.jpg" alt=""><br>看上去，Skip-thought和Skip-gram挺象。唯一的遗憾是Skip-thought的decoder那部分，它是作为language modeling来处理的。而Skip-gram则是利用一个classifier预测周围的词(通过hierarchical softmax 或者negative sampling）。QT针对这个问题，对decoder部分做了大的调整，它直接把decoder拿掉，取而代之的是一个classifier。这个classifier负责预测哪些句子才是context sentences。</p>
<p>QT的classifier取代了Skip-thought的Decoder。这样做的好处是运行的速度大大提升了，用判别问题取代了生成式问题。有趣的是，虽然QT出现的比Skip-thought更晚，但是方法更简单，也更加接近Word2Vec算法。<br>QT是一种新的state-of-art的算法。它不光效果好，而且训练时间要远小于其他算法。在算法方法上和效果上，都可称为是句子表征界的Word2Vec一般的存在。和前面几篇介绍的不同算法放在一起比较，同样都是为了找到好的句子表征，它们采取了不同的路径：InferSent在寻找NLP领域的ImageNet, 它的成功更像是在寻找数据集和任务上的成功，当然它成功的找到了SNLI; Concatenated p-means在寻找NLP领域的convolutional filter, 即怎样才能更好的提炼出句子级别的特征，它找到了p-means操作，以及利用了不同的embeddings; QT则是直接在算法层面上，寻找句子级别的Word2Vec, 算法上的改进让它受益。我们看到不同的方法在不同的方向上都作出了努力和取得了成效，很难讲哪种努力会更有效或者更有潜力。唯一唯一可以肯定的是，从应用层面上来讲，合适的才是最好的。</p>
<h2 id="Supervised-Sentence-Embed"><a href="#Supervised-Sentence-Embed" class="headerlink" title="Supervised Sentence Embed"></a>Supervised Sentence Embed</h2><h3 id="InferSent"><a href="#InferSent" class="headerlink" title="InferSent"></a>InferSent</h3><p>来自论文Supervised Learning of Universal Sentence Representations from Natural Language Inference Data，更多信息参考 <a href="https://jijeng.github.io/2019/05/27/paper-reading-nlp/" target="_blank" rel="noopener">这里</a></p>
<h2 id="Multi-task-learning-Sentence-Embed"><a href="#Multi-task-learning-Sentence-Embed" class="headerlink" title="Multi-task learning Sentence Embed"></a>Multi-task learning Sentence Embed</h2><h3 id="Universal-Sentence-Encoder"><a href="#Universal-Sentence-Encoder" class="headerlink" title="Universal Sentence Encoder"></a>Universal Sentence Encoder</h3><p>来自论文 Universal Sentence Encoder，更多信息参考 <a href="https://jijeng.github.io/2019/05/27/paper-reading-nlp/" target="_blank" rel="noopener">Universal Sentence Encoder</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/05/27/paper-reading-nlp/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jijeng Jia">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jijeng's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/05/27/paper-reading-nlp/" itemprop="url">NLP Papers Reading- BERT</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-05-27T14:13:14+08:00">
                2019-05-27
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index">
                    <span itemprop="name">NLP</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/05/27/paper-reading-nlp/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2019/05/27/paper-reading-nlp/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>nlp 论文阅读笔记, 随时 update…</p>
<h2 id="attention-is-all-you-need"><a href="#attention-is-all-you-need" class="headerlink" title="attention is all you need"></a>attention is all you need</h2><p>Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence.</p>
<p>An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The ouput is computed as a weighted sum of the values, where the weight assigned to each value is computed cy a compatibility function of the query with the corresponding key.</p>
<p>中文的理解：<br>深度学习里的Attentionmodel其实模拟的是人脑的注意力模型，举个例子来说，当我们观赏一幅画时，虽然我们可以看到整幅画的全貌，但是在我们深入仔细地观察时，其实眼睛聚焦的就只有很小的一块，这个时候人的大脑主要关注在这一小块图案上，也就是说这个时候人脑对整幅图的关注并不是均衡的，是有一定的权重区分的。这就是深度学习里的AttentionModel的核心思想。所谓注意力机制，就是说在生成每个词的时候，对不同的输入词给予不同的关注权重。通过注意力机制，我们将输入句子编码为一个向量序列，并自适应地选择这些向量的一个子集，同时对译文进行译码，例如where are you——&gt;你在哪？现在我们在翻译“你”的时候给”you”更多的权重，那么就可以有效的解决对齐问题。</p>
<p>Background:</p>
<p>主要是面临的三个问题。<br><img src="http://ww1.sinaimg.cn/large/e9a223b5ly1g3env74i1dj20rs0f8dge.jpg" alt=""></p>
<p>Model:<br><img src="http://ww1.sinaimg.cn/large/e9a223b5ly1g3enz4w1tmj21bu0r0q6v.jpg" alt=""></p>
<p>Encoder: encoder由6个相同的层堆叠而成，每个层有两个子层。第一个子层是多头自我注意力机制(multi-head self-attention mechanism)，第二层是简单的位置的全连接前馈网络(position-wise fully connected feed-forward network)。在两个子层中会使用一个残差连接，接着进行层标准化(layer normalization)。也就是说每一个子层的输出都是LayerNorm(x + sublayer(x))。网络输入是三个相同的向量q, k和v，是word embedding和position embedding相加得到的结果。为了方便进行残差连接，我们需要子层的输出和输入都是相同的维度。</p>
<p>Decoder: decoder也是由N（N=6）个完全相同的Layer组成，decoder中的Layer由encoder的Layer中插入一个Multi-Head Attention + Add&amp;Norm组成。输出的embedding与输出的position embedding求和做为decoder的输入，经过一个Multi-HeadAttention + Add&amp;Norm（（MA-1）层，MA-1层的输出做为下一Multi-Head Attention + Add&amp;Norm（MA-2）的query（Q）输入，MA-2层的Key和Value输入（从图中看，应该是encoder中第i（i = 1,2,3,4,5,6）层的输出对于decoder中第i（i = 1,2,3,4，5,6）层的输入）。MA-2层的输出输入到一个前馈层（FF），经过AN操作后，经过一个线性+softmax变换得到最后目标输出的概率。<br> 对于decoder中的第一个多头注意力子层，需要添加masking，确保预测位置i的时候仅仅依赖于位置小于i的输出。<br> 层与层之间使用的Position-wise feed forward network。</p>
<p> 从整体上来看，Transformer依旧是一个“Sequence to Sequence”框架，拥有Encoder和Decoder两部分：</p>
<p><img src="http://ww1.sinaimg.cn/large/e9a223b5ly1g3eo6o8obxj20l00d6wee.jpg" alt=""></p>
<p>Transformer的Encoder其实有6层，Decoder也有6层，从Encoder的角度，低层的Encoder是表层的词法信息，逐步向上进行抽象之后，在上层将表示抽象语义信息。Encoder部分还在最上层连了几条线到每个Decoder的部分，这是为了在Decoder中进行Attention操作，Decoder的网络中和Encoder也有信息传递和交互的。最后一个特点是Decoder和Encoder画的大小是一样的，因为它们层的维度大小是一样的。</p>
<p><img src="http://ww1.sinaimg.cn/large/e9a223b5ly1g3eo7eiu73j20rs0i342d.jpg" alt=""></p>
<p>也就是说encoder的输出，会和每一层的decoder进行结合。<br>Encoder和Decoder的内部结构：<br><img src="http://ww1.sinaimg.cn/large/e9a223b5ly1g3enq6lqouj20uq0eoq4u.jpg" alt=""></p>
<p>模型的特点：<br>Positional embedding；（位置嵌入向量——其实类似word2vec，处理的语序的信息）。<br>multi-head attention; (多头注意力机制——点乘注意力的升级版本， 这个就类似ensemble的思想，不同的子空间的attention 进行融合）<br>Position-wise Feed-Forward Networks（位置全链接前馈网络——MLP变形）</p>
<p>有两种常用的注意力函数，一种是加法注意力(additive attention)，另外一种是点乘注意力(dot-productattention)，论文所采用的就是点乘注意力，这种注意力机制对于加法注意力而言，更快，同时更节省空间。</p>
<p>加法注意力<br>还是以传统的RNN的seq2seq问题为例子，加性注意力是最经典的注意力机制，它使用了有一个隐藏层的前馈网络（全连接）来计算注意力分配：<br><img src="http://ww1.sinaimg.cn/large/e9a223b5ly1g3ep38emyqj20bl03o3zj.jpg" alt=""></p>
<p>公式:<br>$$<br>\alpha _ { i j } = \frac { \exp \left( e _ { i j } \right) } { \sum _ { k = 1 } ^ { L } e _ { i k } }<br>$$</p>
<p>Scaled Dot-Product<br>这篇论文计算query和key相似度使用了dot-product attention，即query和key进行点乘（内积）来计算相似度。<br><img src="http://ww1.sinaimg.cn/large/e9a223b5ly1g3ftfz3uxij206e088aa2.jpg" alt=""></p>
<p>Multi-Head Attention:</p>
<p>在实际中为了并行计算，可以在一组queries上计算注意力函数，将多个query堆叠成Q，同理keys和values也被堆叠成K和V，通过下面的公式来计算矩阵输出:<br>self-attention 模型就是自己对自己求attention，即𝑄=𝐾=𝑉<br>$$<br>\text { Attention } ( Q , K , V ) = \operatorname { softmax } \left( \frac { Q K ^ { T } } { \sqrt { d _ { k } } } \right) V<br>$$<br>之所以用内积除以维度的开方，论文给出的解释是：假设Q和K都是独立的随机变量，满足均值为0，方差为1，则点乘后结果均值为0，方差为dk。也即方差会随维度dk的增大而增大，而大的方差导致极小的梯度(我认为大方差导致有的输出单元a（a是softmax的一个输出）很小，softmax反向传播梯度就很小（梯度和a有关））。为了避免这种大方差带来的训练问题，论文中用内积除以维度的开方，使之变为均值为0，方差为1。</p>
<p>除了计算一个单独的注意力函数，论文提出对queries，keys和values做h次不同的投影, 然后都经过Scaled Dot-Product Attention，将结果拼接在一起，最后通过一个线性映射输出，通过多头注意力，模型能够获得不同子空间下的位置信息。如下图所示，公式如下:<br>$$<br>\text {MultiHead} ( Q , K , V ) =Concat(head_1, head_2, …, head_h)  W ^ { o }$$</p>
<p>Self-Attention<br>那么首先要明白什么是Attention。从语言学的角度，它是表示词与词之间的关联关系，像下图所示，这是一个Self-Attention的示意，它这个it会和其他位置的词发生关系，颜色越深的是说关系越紧密，从中图中看到它很正确的关联到了animal它实际指代的一个词。<br><img src="http://ww1.sinaimg.cn/large/e9a223b5ly1g3eobewbqoj20c50bhq4y.jpg" alt=""><br>从机器学习的角度，这个Attention是神经网络隐层之间一个相似度的表示，什么是Self-Attention？就是表示句子内部词与词之间的关联关系，就像这里的it到animal，可以用于指代消解等问题。</p>
<p>Positional Encoding<br>Position Embedding，也就是“位置向量”，将每个位置编号，然后每个编号对应一个向量，通过结合位置向量和词向量，就给每个词都引入了一定的位置信息，这样Attention就可以分辨出不同位置的词了。</p>
<p>Residual connection和layer-normalization<br>（这两个操作主要是应对 深度网络而提出的）<br>对于学习CV的人估计对这个结构一点也不陌生，Residual connection是对于较为深层的神经网络有比较好的作用，比如网络层很深时，数值的传播随着weight不断的减弱，Residual connection是从输入的部分，就是图中虚线的部分，实际连到它输出层的部分，把输入的信息原封不动copy到输出的部分，减少信息的损失。</p>
<p><img src="http://ww1.sinaimg.cn/large/e9a223b5ly1g3eof674adj20ii0dfgoe.jpg" alt=""><br>layer-normalization这种归一化层是为了防止在某些层中由于某些位置过大或者过小导致数值过大或过小，对神经网络梯度回传时有训练的问题，保证训练的稳定性，这是神经网络设计比较常用的case。</p>
<p>结论：<br>self-attention层的好处是能够一步到位捕捉到全局的联系，解决了长距离依赖，因为它直接把序列两两比较（代价是计算量变为 O(n2)，当然由于是纯矩阵运算，这个计算量相当也不是很严重），而且最重要的是可以进行并行计算。<br>相比之下，RNN 需要一步步递推才能捕捉到，并且对于长距离依赖很难捕捉。而 CNN 则需要通过层叠来扩大感受野，这是 Attention 层的明显优势。</p>
<h2 id="A-simple-but-tough-to-beat-baseline-for-sentence-embeddings"><a href="#A-simple-but-tough-to-beat-baseline-for-sentence-embeddings" class="headerlink" title="A simple but tough-to-beat baseline for sentence embeddings"></a>A simple but tough-to-beat baseline for sentence embeddings</h2><p>这种motivation 还是很值得好好看的，验证论文的好坏是可以通过看最后的效果/ 结果是否按照 motivation 那样的。  “relative weights” 是针对 word2vec 中的效果改进的，从motivation 的角度没有考虑到 melo or BERT 中的 context . 当然论文的创新点在于 无监督学习方法，当别人都在转向有监督和多任务的时候，在缩短运行时间的同时，最后的效果和神经网络旗鼓相当。</p>
<p>Taking the average of the word embeddings in a sentence tends to give too much weight to words that are quite irrelevant, semantically speaking. Smooth Inverse Frequency tries to solve this problem in two ways:</p>
<ul>
<li>Weighting: like our tf-idf baseline above, SIF takes the weighted average of the word embeddings in the sentence. Every word embedding is weighted by a/(a + p(w)), where a is a parameter that is typically set to 0.001 and p(w) is the estimated frequency of the word in a reference corpus.</li>
<li>Common component removal: next, SIF computes the principal component of the resulting embeddings for a set of sentences. It then subtracts from these sentence embeddings their projections on their first principal component. This should remove variation related to frequency and syntax that is less relevant semantically.<br>As a result, SIF downgrades unimportant words such as but, just, etc., and keeps the information that contributes most to the semantics of the sentence.</li>
</ul>
<p>本文是用无监督方法做句子级别的 embedding，用的是一个十分简单但却又很有效的传统方法，这在神经网络泛滥的年代算是一股清流了。<br>这张图上的信息还是很多的，所以好好归纳整理一下。<br>尽管长期以来句子的无监督表示学习是主流，最近几个月（2017年末/2018年初），我们看到了许多非常有趣的工作，显示了向监督学习和多任务学习转向的趋势。</p>
<p><img src="http://ww1.sinaimg.cn/large/e9a223b5ly1g3dvg2o3wwj20qo0grdgp.jpg" alt=""></p>
<ul>
<li>强力/迅速的基线：FastText、词袋（Bag-of-Words）</li>
<li>当前最先进模型：ELMo、Skip-Thoughts、Quick-Thoughts、 InferSent、MILA/MSR的General Purpose Sentence Representations、Google的Universal Sentence Encoder</li>
</ul>
<p>关于nlp 中的word embedding 是可以有 phrases, sentences, and paragraphs 三个不同类别的 embedding，所以还是挺好的。</p>
<p>近五年来提出了大量词嵌入方法。其中最常用的模型是word2vec和GloVe，这两个模型都是基于分布假说（distributional hypothesis）的无监督方法。（根据分布假说，出现在相同上下文中的单词倾向于具有相似的含义）。<br>尽管有一些工作通过并入语义或语法知识等增强这些无监督方法，纯无监督方法在2017-2018年期间取得了有趣的进展，其中最重大的是FastText（word2vec的扩展）和ELMo（当前最先进的上下文词向量）。</p>
<p>在ELMo中，嵌入基于一个双层的双向语言模型（biLM）的内部状态计算，ELMo也是因此得名的：Embeddings from Language Models（来自语言模型的嵌入）。<br>ELMo的特性：<br>ELMo的输入是字符而不是单词。这使得它可以利用子字（sub-word）单元为词汇表以外的单词计算有意义的表示（和FastText类似）。<br>ELMo是biLM的多层激活的连接（concatenation）。语言模型的不同层编码了单词的不同信息。连接所有层使得ELMo可以组合多种词表示，以提升下游任务的表现。</p>
<p>普适句嵌入<br>词袋方法<br>这一领域的一般共识是，直接平均一个句子的词向量这一简单方法（所谓词袋方法），为许多下游任务提供了强力的基线。<br>Arora等去年在ICLR发表的论文A Simple but Tough-to-Beat Baseline for Sentence Embeddings提供了一个很好的算法：选择一种流行的词嵌入，编码句子为词向量的线性加权组合，然后进行相同成分移除（根据首要主成分移除向量投影）。这一通用方法具有深刻而强大的理论动机，基于在语篇向量上随机行走以生成文本的生成式模型。</p>
<p><img src="http://ww1.sinaimg.cn/large/e9a223b5ly1g3dw277xiyj20pi0c10ww.jpg" alt=""></p>
<p>（有人实践）这个方法在短文本上效果更好，在语料不足的时候效果不能保证。这种模型没有考虑词顺序（也可以说只能理解词意思，但是不能理解语义），而深度网络模型是可以考虑语义的。可能再相似度问题上可以取得比较好的效果，但是在文本分类，情感分类上效果一般。</p>
<blockquote>
<p>思考：从直觉上理解, 短文本上的 word2vec、SIF 这种没有 handle 语序的模型得到的效果就已经足够的好，对于中长文本（句子、段落等）elmo 和BERT 这种模型的效果是更加的。</p>
</blockquote>
<p>程序的运行只需要十几分钟，与神经网络的效果旗鼓相当。</p>
<h2 id="Supervised-Learning-of-Universal-Sentence-Representations-from-Natural-Language-Inference-Data"><a href="#Supervised-Learning-of-Universal-Sentence-Representations-from-Natural-Language-Inference-Data" class="headerlink" title="Supervised Learning of Universal Sentence Representations from Natural Language Inference Data"></a>Supervised Learning of Universal Sentence Representations from Natural Language Inference Data</h2><p>文章成功的找到了NLP领域的ImageNet — SNLI (Stanford Natural Language Inference dataset), 并且试验了不同的深度学习模型，最终确定bi-LSTM max pooled 为最佳模型。</p>
<table>
<thead>
<tr>
<th>域</th>
<th>数据</th>
<th>任务</th>
<th>模型(编码器)</th>
</tr>
</thead>
<tbody>
<tr>
<td>CV</td>
<td>ImageNet</td>
<td>image classification</td>
<td>Le-Net, VGG-Net, Google-Net, ResNet, DenseNet</td>
</tr>
<tr>
<td>NLP</td>
<td>SNLI</td>
<td>NLI</td>
<td>?</td>
</tr>
</tbody>
</table>
<p>基于监督学习方法学习sentence embeddings可以归纳为两个步骤：<br>第一步选择监督训练数据，设计相应的包含句子编码器Encoder的模型框架；<br>第二步选择（设计）具体的句子编码器，包括DAN、基于LSTM、基于CNN和Transformer等。</p>
<p>数据集：</p>
<p>本文采用的是Stanford Natural Language Inference Datasets，简称SNLI （NLP领域的ImageNet ）。SNLI包含570K个人类产生的句子对，每个句子对都已经做好了标签，标签总共分为三类：蕴含、矛盾和中立（Entailment、contradiction and neutral）。下面是这些数据集的一个例子：</p>
<p><img src="http://ww1.sinaimg.cn/large/e9a223b5ly1g3eqwkv9tlj20kd097gm4.jpg" alt=""><br>从上图可以看出，每个句子对为（text, hypothesis）,中间的judgments为它们的标签。可以看到标签是综合了5个专家的意见，根据少数服从多数的原则得到的。</p>
<p>7种不同的architectures： </p>
<ol>
<li>standard recurrent encoders with LSTM ，取最后一个隐状态</li>
<li>standard recurrent encoders with GRU ，取最后一个隐状态<br>上述两种是基础的recurrent encoder，在句子建模中通常将网络中的最后一个隐藏状态作为sentence representation； </li>
<li>conncatenation of last hidden states of forward and backward GRU<br>这种方法是将单向的网络变成了双向的网络，然后用将前向和后向的最后一个状态进行连接，得到句子向量； </li>
<li>Bi-directional LSTMs (BiLSTM) with mean pooling </li>
<li>Bi-directional LSTMs (BiLSTM) with max pooling<br>这两种方法使用了双向LSTM结合一个pooling层的方法来获取句子表示，具体公式如下： </li>
<li>self-attentive network<br>这个网络在双向LSTM的基础上加入了attention机制，具体网络结构如下： </li>
<li>hierarchical convolutional networks </li>
</ol>
<p>Now that we have discussed the various sentence encoding architectures used in the paper, let’s go through the part of the network which takes these sentence embeddings and predicts the output label.</p>
<p><img src="http://ww1.sinaimg.cn/large/e9a223b5ly1g3eqyoxmknj20ey0d640u.jpg" alt=""><br>After the sentence vectors are fed as input to this model, 3 matching methods are applied to extract relations between the text, u and hypothesis, v –</p>
<ul>
<li>concatenation of the two representations (u, v)</li>
<li>element-wise product u * v</li>
<li>and, absolute element-wise difference |u – v |</li>
</ul>
<p>The resulting vector captures information from both the text, u and the hypothesis, v, and is fed into a 3-class classifier consisting of multiple fully connected layers followed by a softmax layer.</p>
<h2 id="Universal-Sentence-Encoder"><a href="#Universal-Sentence-Encoder" class="headerlink" title="Universal Sentence Encoder"></a>Universal Sentence Encoder</h2><p>这篇文章基于InferSent， 也是想找到一个universal encoder。不同之处在于文章把InferSent的bi-lstm换成了DAN（或者Transformer)，而使用DAN这样“简单”的encoder的效果竟然相当好（尤其是时间和内存消耗和其他算法比小很多。）</p>
<p>The Google Sentence Encoder is Google’s answer to Facebook’s InferSent. It comes in two forms:</p>
<ul>
<li>an advanced model that takes the element-wise sum of the context-aware word representations produced by the encoding subgraph of a Transformer model.</li>
<li>a simpler Deep Averaging Network (DAN) where input embeddings for words and bigrams are averaged together and passed through a feed-forward deep neural network.<br>The Transformer-based model tends to give better results, but at the time of writing, only the DAN-based encoder was available. In contrast to InferSent, the Google Sentence Encoder was trained on a combination of unsupervised data (in a skip-thought-like task) and supervised data (the SNLI corpus).</li>
</ul>
<p>DAN<br>其实DAN(Deep Averaging Networks)应该属于Bag of Words类的算法。因为比较特殊，单独列出来。 它是在对所有词语取平均后，在上面加上几层神经网络。特殊的地方在于它在sentiment analysis中表现也不错，这在BOW类方法中比较罕见。</p>
<p><img src="http://ww1.sinaimg.cn/large/e9a223b5ly1g3esxw2nilj20dh08ujth.jpg" alt=""></p>
<table>
<thead>
<tr>
<th>新方法</th>
<th>类型</th>
<th>基于的旧算法</th>
<th>贡献</th>
</tr>
</thead>
<tbody>
<tr>
<td>SIF</td>
<td>无监督</td>
<td>BOW</td>
<td>一个简单而有效的baseline算法</td>
</tr>
<tr>
<td>InferSent</td>
<td>监督</td>
<td>NA</td>
<td>找到了NLP领域的ImageNet – SNLI， 并给出了一个state-of-art 算法</td>
</tr>
<tr>
<td>P-mean</td>
<td>无监督</td>
<td>BOW</td>
<td>比SIF更简单且有效的一个算法且适用于cross-lingual</td>
</tr>
<tr>
<td>Universal-sentence-encoder</td>
<td>监督</td>
<td>InferSent</td>
<td>更加简单的encoder</td>
</tr>
</tbody>
</table>
<p>文章共提出两种基于不同网络架构的Universal Sentence Encoder：Transformer and Deep Averaging Network (DAN).<br>Our two encoders have different design goals. One based on the transformer architecture targets high accuracy at the cost of greater model complexity and resource consumption. The other targets efficient inference with slightly reduced accuracy.</p>
<h2 id="deep-contextualized-word-representations"><a href="#deep-contextualized-word-representations" class="headerlink" title="deep contextualized word representations"></a>deep contextualized word representations</h2><p>introduction:</p>
<p>这种embedding -context 必要性的介绍，感觉是有更好，没有也是能够理解的。<br>Why do we need contextualized representations?<br>As an illustrative example, take the following two sentences:</p>
<blockquote>
<p>“The bank on the other end of the street was robbed”<br>“We had a picnic on the bank of the river”</p>
</blockquote>
<p>Both sentences use the word “bank”, but the meaning of the word differs completely between them. This phenomenon where two identical words change meaning depending on the context is known as “polysemy“, and has been an issue in the NLP deep learning community ever since word embeddings really took off. Most current neural networks are bad at handling polysemy because they use a single vector to represent the meaning of the word “bank”, regardless of the context. In reality, the vector representing any word should change depending on the words around it.</p>
<p>什么是一个好的词向量：<br>ELMo能够学习到词汇用法的复杂性，比如语法、语义。<br>ELMo能够学习不同上下文情况下的词汇多义性。</p>
<p>之前的做法的缺点是对于每一个单词都有唯一的一个embedding表示, 而对于多义词显然这种做法不符合直觉, 而单词的意思又和上下文相关, ELMo的做法是我们只预训练language model, 而word embedding是通过输入的句子实时输出的, 这样单词的意思就是上下文相关的了, 这样就很大程度上缓解了歧义的发生.</p>
<p>这种算法的特点是：每一个word representation都是整个输入语句的函数。具体做法就是先在大语料上以language model为目标训练出bidirectional LSTM模型，然后利用LSTM产生词语的表征。ELMo故而得名(Embeddings from Language Models)。为了应用在下游的NLP任务中，一般先利用下游任务的语料库(注意这里忽略掉label)进行language model的微调,这种微调相当于一种domain transfer; 然后才利用label的信息进行supervised learning。</p>
<p>（这个描述跟 cv 是惊人的相似）<br>ELMo表征是“深”的，就是说它们是biLM的所有层的内部表征的函数。这样做的好处是能够产生丰富的词语表征。高层的LSTM的状态可以捕捉词语意义中和语境相关的那方面的特征(比如可以用来做语义的消歧)，而低层的LSTM可以找到语法方面的特征(比如可以做词性标注)。如果把它们结合在一起，在下游的NLP任务中会体现优势。</p>
<p>Salient features<br>ELMo representations are:</p>
<ul>
<li>Contextual: The representation for each word depends on the entire context in which it is used.</li>
<li>Deep: The word representations combine all layers of a deep pre-trained neural network.</li>
<li>Character based: ELMo representations are purely character based, allowing the network to use morphological clues to form robust representations for out-of-vocabulary tokens unseen in training.</li>
</ul>
<p>related work:</p>
<p>针对传统词向量是固定的，与上下文语境无关的缺点，先前的工作多通过两种方式来解决：<br> (1) 通过引入字符级(subword)信息丰富词向量表达；<br> (2) 学习每个单词不同含义的独立向量；<br> ELMo也利用了字符卷积（Character-Convolutions）引入字符级信息，并同时结合了深度双向语言模型的各层隐状态来丰富词向量表达。</p>
<p>P.s.：基于字符的模型不仅能够通过引入字符级信息丰富词向量表达，也能够在很大程度上解决NLP领域的OOV（Out-Of-Vocabulary）问题。</p>
<p>ELMo用到上文提到的双向的language model, 给定N个tokens (t1, t2,…,tN), language model通过给定前面的k-1个位置的token序列计算第k个token的出现的概率:<br>$$<br>p \left( t _ { 1 } , t _ { 2 } , \ldots , t _ { N } \right) = \prod _ { k = 1 } ^ { N } p \left( t _ { k } | t _ { 1 } , t _ { 2 } , \ldots , t _ { k - 1 } \right)<br>$$<br>后向的计算方法与前向相似:</p>
<p>$$<br>p \left( t _ { 1 } , t _ { 2 } , \ldots , t _ { N } \right) = \prod _ { k = 1 } ^ { N } p \left( t _ { k } | t _ { k + 1 } , t _ { k + 2 } , \ldots , t _ { N } \right)<br>$$<br>biLM训练过程中的目标就是最大化:<br>$$<br>\sum _ { k = 1 } ^ { N } \left( \log p \left( t _ { k } | t _ { 1 } , \ldots , t _ { k - 1 } ; \Theta _ { x } , \vec { \Theta } _ { L S T M } , \Theta _ { s } \right) + \log p \left( t _ { k } | t _ { k + 1 } , \ldots , t _ { N } ; \Theta _ { x } , \overline { \Theta } _ { L S T M } , \Theta _ { s } \right) \right)<br>$$</p>
<p>ELMo对于每个token $t_k$, 通过一个L层的biLM计算出2L+1个表示:<br>$$<br>R_{ k } = \left{ x _ { k } ^ { L M } , \vec { h } _ { k , j } ^ { L M } , h _ { k , j } ^ { L M } | j = 1 , \ldots , L \right} = \left{ h _ { k , j } ^ { L M } | j = 0 , \ldots , L \right}<br>$$<br>其中$h _ { k , 0 } ^ { L M }$是对token进行直接编码的结果(这里是字符通过CNN编码), $h _ { k , j } ^ { L M } = \left[ \vec { h } _ { k , j } ^ { L M } ; \overline { h } _ { k , j } \right]$ 是每个biLSTM层输出的结果. 在实验中还发现不同层的biLM的输出的token表示对于不同的任务效果不同.</p>
<p>应用中将ELMo中所有层的输出R压缩为单个向量, ELMok=E(Rk;Θϵ), 最简单的压缩方法是取最上层的结果做为token的表示:$E \left( R _ { k } \right) = h _ { k , L } ^ { L M }$ 更通用的做法是通过一些参数来联合所有层的信息:<br>$$E L M o _ { k } ^ { t a s k } = E \left( R _ { k } ; \Theta ^ { t a s k } \right) = \gamma ^ { t a s k } \sum _ { j = 0 } ^ { L } s _ { j } ^ { t a s k } h _ { k , j } ^ { L M }$$</p>
<p>其中$s_j$是一个softmax出来的结果, $γ$是一个任务相关的scale参数, 我试了平均每个层的信息和学出来$s_j$发现学习出来的效果会好很多. 文中提到$γ$在不同任务中取不同的值效果会有较大的差异, 需要注意, 在SQuAD中设置为0.01取得的效果要好于设置为1时.</p>
<p>ELMo: Context Matters</p>
<p>Instead of using a fixed embedding for each word, ELMo looks at the entire sentence before assigning each word in it an embedding. It uses a bi-directional LSTM trained on a specific task to be able to create those embeddings.<br><img src="http://ww1.sinaimg.cn/large/e9a223b5ly1g3fub67ulaj20u20h3wfx.jpg" alt=""></p>
<p>ELMo provided a significant step towards pre-training in the context of NLP. The ELMo LSTM would be trained on a massive dataset in the language of our dataset, and then we can use it as a component in other models that need to handle language.</p>
<p>What’s ELMo’s secret?</p>
<p>ELMo gained its language understanding from being trained to predict the next word in a sequence of words - a task called Language Modeling. This is convenient because we have vast amounts of text data that such a model can learn from without needing labels.</p>
<p><img src="http://ww1.sinaimg.cn/large/e9a223b5ly1g3fubpu9ibj20li0m1q5b.jpg" alt=""></p>
<p>We can see the hidden state of each unrolled-LSTM step peaking out from behind ELMo’s head. Those come in handy in the embedding proecss after this pre-training is done.</p>
<p>ELMo actually goes a step further and trains a bi-directional LSTM – so that its language model doesn’t only have a sense of the next word, but also the previous word.<br><img src="http://ww1.sinaimg.cn/large/e9a223b5ly1g3fuc693mrj211z0hbjur.jpg" alt=""><br>ELMo comes up with the contextualized embedding through grouping together the hidden states (and initial embedding) in a certain way (concatenation followed by weighted summation).<br><img src="http://ww1.sinaimg.cn/large/e9a223b5ly1g3fuci4xodj213j0kitcq.jpg" alt=""></p>
<p>lstm-based language model<br>In case you are unfamiliar with language models, a language model is simply a model that can predict how “likely” a certain sequence of words is to be a real piece of text. This is generally done by training a model to take a part of sentence (say, the first n words) and predict the next word – or more precisely, output the probability of each word in the vocabulary being the next word (In this blog post, we’ll focus on LSTM-based language models which are the focus of this paper). </p>
<p>One trick that this paper uses is to train a language model with reversed sentences that the authors call the “backward” language model.<br>这种模型：上一个模型的输出到下一个模型输入<br>Furthermore, instead of using a single-layer LSTM, this paper uses a stacked, multi-layer LSTM. Whereas a single-layer LSTM would take the sequence of words as input, a multi-layer LSTM trains multiple LSTMs to take the output sequence of the LSTM in the previous layer as input (of course, the first layer takes the sequence of words as input). This is best illustrated in the following illustration:<br><img src="http://ww1.sinaimg.cn/large/e9a223b5ly1g3flokoc0mj20ab07474a.jpg" alt=""></p>
<p>最后的embedding 是是将不同的层 combination起来，这个系数是通过学习出来的。<br>In ELMo, the part that is task specific is the combination of the task-agnostic representations. The weight is learned for each task and normalized using the softmax function. The parameter $\gamma$ is a task-dependent value that allows for scaling the entire vector, which is important during optimization.</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/05/27/Dynamic-Programming-Examples/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jijeng Jia">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jijeng's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/05/27/Dynamic-Programming-Examples/" itemprop="url">Dynamic Programming Examples</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-05-27T14:10:41+08:00">
                2019-05-27
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/算法/" itemprop="url" rel="index">
                    <span itemprop="name">算法</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/05/27/Dynamic-Programming-Examples/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2019/05/27/Dynamic-Programming-Examples/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>Dynamic Programming is a method for solving a complex problem by breaking it down into a collection of simpler subproblems, solving each of those subproblems just once, and storing their solutions using a memory-based data structure (array, map, etc.). So the next time the same sub-problem occurs, instead of recomputing its solution, one simply looks up the previously computed solution, thereby saving computation time.</p>
<p><strong><em>knapsack problem:</em></strong></p>
<p><a href="https://leetcode.com/problems/partition-equal-subset-sum/" target="_blank" rel="noopener">https://leetcode.com/problems/partition-equal-subset-sum/</a></p>
<p>Given a non-empty array containing only positive integers, find if the array can be partitioned into two subsets such that the sum of elements in both subsets is equal.</p>
<p>Actually, this is a 0/1 knapsack problem, for each number, we can pick it or not. Let us assume dp[i][j] means whether the specific sum j can be gotten from the first i numbers. If we can pick such a series of numbers from 0-i whose sum is j, dp[i][j] is true, otherwise it is false.<br>Base case: dp[0][0] is true; (zero number consists of sum 0 is true)<br>Transition function: For each number, if we don’t pick it, dp[i][j] = dp[i-1][j], which means if the first i-1 elements has made it to j, dp[i][j] would also make it to j (we can just ignore nums[i]). If we pick nums[i]. dp[i][j] = dp[i-1][j-nums[i]], which represents that j is composed of the current value nums[i] and the remaining composed of other previous numbers. Thus, the transition function is dp[i][j] = dp[i-1][j] || dp[i-1][j-nums[i]]</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">canPartition</span><span class="params">(self, nums)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :type nums: List[int]</span></span><br><span class="line"><span class="string">        :rtype: bool</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        n =len(nums)</span><br><span class="line">        s = sum(nums)</span><br><span class="line">        <span class="keyword">if</span> s &amp;<span class="number">1</span> ==<span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">False</span></span><br><span class="line">        dp =[<span class="number">0</span> <span class="keyword">for</span> _ <span class="keyword">in</span> range(s+<span class="number">1</span>)]</span><br><span class="line">        dp[<span class="number">0</span>] =<span class="number">1</span></span><br><span class="line">        <span class="comment">#import ipdb</span></span><br><span class="line">        <span class="keyword">for</span> num <span class="keyword">in</span> nums:</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(s, <span class="number">-1</span>, <span class="number">-1</span>):</span><br><span class="line">                <span class="comment"># ipdb.set_trace()</span></span><br><span class="line">                </span><br><span class="line">                <span class="keyword">if</span> dp[i]:</span><br><span class="line">                    dp[i+num] =<span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> dp[s//<span class="number">2</span>]:</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">True</span></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">False</span></span><br></pre></td></tr></table></figure>
<p><strong><em>longest commone substring</em></strong></p>
<p>Given two strings ‘X’ and ‘Y’, find the length of the longest common substring.</p>
<blockquote>
<p>Input : X = “GeeksforGeeks”, y = “GeeksQuiz”<br>Output : 5</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Given two strings ‘X’ and ‘Y’, find the length of the longest common substring.</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">minDistance</span><span class="params">(self, word1, word2)</span>:</span></span><br><span class="line"></span><br><span class="line">        m  =len(word1)</span><br><span class="line">        n =len(word2)</span><br><span class="line">        <span class="comment">#dp=[ [None] for _ in range(n+1) for _ in range(m+1)]</span></span><br><span class="line">        dp = [[<span class="keyword">None</span>] *(n +<span class="number">1</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> range(m+<span class="number">1</span>) ]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(m+<span class="number">1</span>):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(n+<span class="number">1</span>):</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> i ==<span class="number">0</span> <span class="keyword">or</span> j ==<span class="number">0</span>:</span><br><span class="line">                    dp[i][j] =<span class="number">0</span></span><br><span class="line">                <span class="comment"># 这个是python 中语法决定的 word1[len(word1)] 是访问不到的，这个访问是从0开始的，所以只能是这样的</span></span><br><span class="line">                <span class="keyword">elif</span> word1[i<span class="number">-1</span>] == word2[j<span class="number">-1</span>]:</span><br><span class="line">                    dp[i][j] =dp[i<span class="number">-1</span>][j<span class="number">-1</span>] +<span class="number">1</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    dp[i][j] =max(dp[i<span class="number">-1</span>][j], dp[i][j<span class="number">-1</span>])</span><br><span class="line">        <span class="keyword">return</span> dp[m][n]</span><br><span class="line"></span><br><span class="line">solution =Solution()</span><br><span class="line">word1 =<span class="string">'abcdaf'</span></span><br><span class="line">word2 =<span class="string">'acbcf'</span></span><br><span class="line">result =solution.minDistance(word1, word2)</span><br><span class="line">print(result)</span><br></pre></td></tr></table></figure>
<p><strong><em>matrix chain multiplication</em></strong></p>
<p>Given a sequence of matrices, find the most efficient way to multiply these matrices together. The problem is not actually to perform the multiplications, but merely to decide in which order to perform the multiplications.</p>
<p><img src="http://ww1.sinaimg.cn/large/e9a223b5ly1g3ft7nakbnj20lq0laads.jpg" alt=""><br>dp 的思想就是借助之前的 subquestion 的结果，然后计算更大的question，这个的核心在于减少了重复子单元的计算。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="comment"># 就是这个 l 和1 是很不容易分清楚的，所以这个是慎重使用的 </span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">MatrixChainOrder</span><span class="params">(list1, len1)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    :param list1:  list of matrix style</span></span><br><span class="line"><span class="string">    :param l:  len1gth of list</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    dp = [[<span class="number">0</span> <span class="keyword">for</span> _ <span class="keyword">in</span> range(len1)] <span class="keyword">for</span> _ <span class="keyword">in</span> range(len1)]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, len1):</span><br><span class="line">        dp[i][i] = <span class="number">0</span></span><br><span class="line">    <span class="comment"># l 个matrix 连成的意思</span></span><br><span class="line">    <span class="keyword">for</span> ll <span class="keyword">in</span> range(<span class="number">2</span>, len1):</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, len1 - ll + <span class="number">1</span>):</span><br><span class="line">            j = i + ll - <span class="number">1</span></span><br><span class="line">            dp[i][j] = sys.maxint</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> k <span class="keyword">in</span> range(i, j):</span><br><span class="line">                tmp = dp[i][k] + dp[k + <span class="number">1</span>][j] + list1[i - <span class="number">1</span>] * list1[k] * list1[j]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> tmp &lt; dp[i][j]:</span><br><span class="line">                    dp[i][j] = tmp</span><br><span class="line">    <span class="keyword">return</span> dp[<span class="number">1</span>][len1 - <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Driver program to test above function</span></span><br><span class="line">arr = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]</span><br><span class="line">arr1 = [<span class="number">2</span>, <span class="number">3</span>, <span class="number">6</span>, <span class="number">4</span>, <span class="number">5</span>]</span><br><span class="line">size = len(arr)</span><br><span class="line">size1 = len(arr1)</span><br><span class="line">print(<span class="string">"Minimum number of multiplications is "</span> +</span><br><span class="line">      str(MatrixChainOrder(arr, size)))</span><br><span class="line">print(<span class="string">"Minimum number of multiplications is "</span> +</span><br><span class="line">      str(MatrixChainOrder(arr1, size1)))</span><br></pre></td></tr></table></figure>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/05/22/beyond-word-embedding/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jijeng Jia">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jijeng's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/05/22/beyond-word-embedding/" itemprop="url">Beyond  Word Embedding</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-05-22T15:23:37+08:00">
                2019-05-22
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index">
                    <span itemprop="name">NLP</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/05/22/beyond-word-embedding/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2019/05/22/beyond-word-embedding/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="Traditional-Word-Vectors"><a href="#Traditional-Word-Vectors" class="headerlink" title="Traditional Word Vectors"></a>Traditional Word Vectors</h2><p>Before diving directly into Word2Vec it’s worth while to do a brief overview of some of the traditional methods that pre-date neural embeddings.</p>
<p>这个是用来描述文章的，有一个大的dict，然后一片文章是如何进行表示、<br><strong>Bag of Words</strong> or BoW vector representations are the most common used traditional vector representation. Each word or n-gram is linked to a vector index and marked as 0 or 1 depending on whether it occurs in a given document.</p>
<p>An example of a one hot bag of words representation for documents with one word.</p>
<p><img src="http://ww1.sinaimg.cn/large/e9a223b5ly1g3a0alr102j20l104jt9t.jpg" alt=""><br>局限性: 一方面只是一种counter，没有考虑语义信息；另一方面有些 words 是明显的 relevant than others.<br>BoW representations are often used in methods of document classification where the frequency of each word, bi-word or tri-word is a useful feature for training classifiers. One challenge with bag of word representations is that they don’t encode any information with regards to the meaning of a given word.<br>In BoW word occurrences are evenly weighted independently of how frequently or what context they occur. However in most NLP tasks some words are more relevant than others.</p>
<p>这个是可以认识是对于 bag of words “relevant” 上的改进：使得 选择的words 更加的 “representative” 文章的调性。<br>TF-IDF, short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word or n-gram is to a document in a collection or corpus. They provide some weighting to a given word based on the context it occurs.The tf–idf value increases proportionally to the number of times a word appears in a document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently than others.</p>
<p>但是对于 bag of words 中“没有体现语义” 的缺陷还是没有 deal with。<br>However even though tf-idf BoW representations provide weights to different words they are unable to capture the word meaning.</p>
<p>这个名字只是因为有定义而存在的名字（网络模型 or 深度网络的出现就是为了 handle 语义信息）<br><strong>Distributional Embeddings</strong> enable word vectors to encapsulate contextual context. Each embedding vector is represented based on the mutual information it has with other words in a given corpus.<br>重点就是这种方式是要 predict a target word from context words，一定是要能够体现语境的。<br><strong>Predictive models</strong> learn their vectors in order to improve their predictive ability of a loss such as the loss of predicting the vector for a target word from the vectors of the surrounding context words.<br>word2vec 是一种思想，有两种CBOW 和skip-gram 两种实现。<br><strong>Word2Vec</strong> is a predictive embedding model. There are two main Word2Vec architectures that are used to produce a distributed representation of words:</p>
<p>CBOW中的 context words 没有体现 words order，但是训练速度快。<br>Continuous bag-of-words (CBOW) — The order of context words does not influence prediction (bag-of-words assumption).</p>
<p>skip-gram  weights nearbt context words heavily 效果相对于 cbow 更加， 但训练速度相对比较慢。<br>Continuous skip-gram weighs nearby context words more heavily than more distant context words. While order still is not captured each of the context vectors are weighed and compared independently vs CBOW which weighs against the average context.</p>
<p>CBOW is faster while skip-gram is slower but does a better job for infrequent words.</p>
<p><strong>glove</strong> （g lou v）<br>这个不属于 word2vec，其只是考虑了 local context，没有考虑 global context<br>Both CBOW and Skip-Grams are “predictive” models, in that they only take local contexts into account.  word2vec does not take advantage of global context.<br>(细节 能看懂就看)<br> GloVe embeddings by contrast leverage the same intuition behind the co-occurrence matrix (共生矩阵) used distributional embeddings, but uses neural methods to decompose the co-occurrence matrix into more expressive and dense word vectors.<br>效果：和word2vec 相比没有  definitively better results<br>While GloVe vectors are faster to train, neither GloVe or Word2Vec has been shown to provide definitively better results rather they should both be evaluated for a given dataset.</p>
<p><strong>fasttext</strong><br>这个主要是 each word + n-gram within each word， 最后的效果是好于 word2vec 的。<br>FastText, builds on Word2Vec by learning vector representations for each word and the n-grams found within each word. The values of the representations are then averaged into one vector at each training step. While this adds a lot of additional computation to training it enables word embeddings to encode sub-word information. FastText vectors have been shown to be more accurate than Word2Vec vectors by a number of different measures.</p>
<h2 id="overview-of-Neural-NLP-Architectures"><a href="#overview-of-Neural-NLP-Architectures" class="headerlink" title="overview of Neural NLP Architectures"></a>overview of Neural NLP Architectures</h2><p>Deep Feed Forward Networks</p>
<p>1D CNNs</p>
<p>RNNs (LSTM/GRU)</p>
<p>encoder- decoder 结构</p>
<p>attention and copy mechanisms<br>这个是 attention 机制提出的背景：解决 句子中的长依赖；contextual impact (specific words may carry more importance at different steps)<br>While in theory they can capture long term dependencies they tend to struggle modeling longer sequences, this is still an open problem.</p>
<p>One cause for sub-optimal performance standard RNN encoder-decoder models for sequence to sequence tasks such as NER or translation is that they weight the impact each input vector evenly on each output vector when in reality specific words in the input sequence may carry more importance at different time steps.<br>Attention mechanisms provide a means of weighting the contextual impact of each input vector on each output prediction of the RNN. These mechanisms are responsible for much of the current or near current state of the art in Natural language processing.</p>
<p><strong>attention</strong></p>
<p>In sum, algorithms can allocate attention, and they can learn how to do so, by adjusting the weights they assign to various inputs. Imagine a heat map over a photo. The heat is attention.</p>
<p>这个是要引出来 context word embeddings.<br>One of the limits of traditional word vectors is that they presume that a word’s meaning is relatively stable across sentences.<br>并不是物理上的二维关系能够表示词语之间的 relationship，有时候是需要高纬空间进行表示的。<br>In fact, the strongest relationships binding a given word to the rest of the sentence may be with words quite distant from it.<br>从 credit assignment的角度阐述了 neural networks 就是 allocating importance to input features。<br>The fundamental task of all neural networks is credit assignment. Credit assignment is allocating importance to input features through the weights of the neural network’s model. Learning is the process by which neural networks figure out which input features correlate highly with the outcomes the net tries to predict, and their learnings are embodied in the adjusted quantities of the weights that result in accurate decisions about the data they’re exposed to.<br>这个是传统的 LSTM （encoder -decoder） 模型，问题在于当句子过长（比如说大于20 words）之后，encoder 是无法 memory 之前的所有 words，所以效果就会变得差一些。</p>
<p><img src="http://ww1.sinaimg.cn/large/e9a223b5ly1g3a478w59aj21hc0ixmzq.jpg" alt=""></p>
<p>但是 attention 就是模仿了人翻译过程，一段作为一个单位，然后进行翻译。这样就可以持续保证较高中确率的输出。<br>In neural networks, attention primarily serves as a memory-access mechanism.<br><img src="http://ww1.sinaimg.cn/large/e9a223b5ly1g3a47iu166j20ck072t90.jpg" alt=""><br> 每次的输出都是关注不同的地方，但是至于哪里更加重要，这个交给了 feedback mechanism 反向传播。下面的图片十分清晰的展示了 在翻译的过程中 “focus” 是不断地变化的。<br>Above, a model highlights which pixels it is focusing on as it predicts the underlined word in the respective captions. Below, a language model highlights the words from one language, French, that were relevant as it produced the English words in the translation. As you can see, attention provides us with a route to interpretability. We can render attention as a heat map over input data such as words and pixels, and thus communicate to human operators how a neural network made a decision. (This could be the basis of a feedback mechanism whereby those humans tell the network to pay attention to certain features and not others.)<br> (This could be the basis of a feedback mechanism whereby those humans tell the network to pay attention to certain features and not others.)<br> <img src="http://ww1.sinaimg.cn/large/e9a223b5ly1g3a487h7rej2078079mx1.jpg" alt=""></p>
<p>Additionally in Machine Reading Comprehension and Summarization systems RNNs often tend to generate results, that while on first glance look structurally correct are in reality hallucinated or incorrect. One mechanism that helps mitigate some of these issues is the Copy Mechanism.<br>copy mechanism 简单说来就是 word embedding or raw text.<br>The copy mechanism is an additional layer applied during decoding that decides whether it is better to generate the next word from the source sentence or from the general embedding vocabulary.</p>
<p><strong>reading comprehension and summary</strong></p>
<p>上面是说的在 machine translation，下面说的是 阅读理解 和 summary领域。<br>Additionally in Machine Reading Comprehension and Summarization systems RNNs often tend to generate results, that while on first glance look structurally correct are in reality hallucinated or incorrect. One mechanism that helps mitigate some of these issues is the Copy Mechanism.<br>copy mechanism 简单说来就是 decide word embedding from model or raw text.<br>The copy mechanism is an additional layer applied during decoding that decides whether it is better to generate the next word from the source sentence or from the general embedding vocabulary.</p>
<p>Taming Recurrent Neural Networks for Better Summarization<br>有两种不同的 summarization:<br>Two types of summarization：Extractive （You might think of these approaches as like a highlighter.） Abstractive（By the same analogy, these approaches are like a pen.）<br>The great majority of existing approaches to automatic summarization are extractive – mostly because it is much easier to select text than it is to generate text from scratch.<br>但是一个问题在于，只是使用 extrative way 可能得到相同的words，<br>Problem 1: The summaries sometimes reproduce factual details inaccurately (e.g. Germany beat Argentina 3-2). This is especially common for rare or out-of-vocabulary words such as 2-0.<br>Problem 2: The summaries sometimes repeat themselves (e.g. Germany beat Germany beat Germany beat…)<br>Easier Copying with Pointer-Generator Networks。这个跟 attention 不是很相关，简单说就是In this way, the pointer-generator network is a best of both worlds, combining both extraction (pointing) and abstraction (generating).</p>
<p>To tackle Problem 2 (repetitive summaries), we use a technique called coverage. The idea is that we use the attention distribution to keep track of what’s been covered so far, and penalize the network for attending to same parts again.</p>
<p><strong>elmo (e l  mo)</strong><br>elmo 产生一个 embedding 是根据 context 产生的。<br>ELMo is a model generates embeddings for a word based on the context it appears thus generating slightly different embeddings for each of its occurrence.<br>（感觉理解一个概念都是 根据其 for example 进行理解的）<br>For example, the word “play” in the sentence above using standard word embeddings encodes multiple meanings such as the verb to play or in the case of the sentence a theatre production. In standard word embeddings such as Glove, Fast Text or Word2Vec each instance of the word play would have the same representation.</p>
<p>参考blog:<br><a href="https://towardsdatascience.com/beyond-word-embeddings-part-2-word-vectors-nlp-modeling-from-bow-to-bert-4ebd4711d0ec" target="_blank" rel="noopener">https://towardsdatascience.com/beyond-word-embeddings-part-2-word-vectors-nlp-modeling-from-bow-to-bert-4ebd4711d0ec</a></p>
<p><a href="http://www.abigailsee.com/2017/04/16/taming-rnns-for-better-summarization.html" target="_blank" rel="noopener">http://www.abigailsee.com/2017/04/16/taming-rnns-for-better-summarization.html</a></p>
<p><a href="https://skymind.ai/wiki/word2vec" target="_blank" rel="noopener">https://skymind.ai/wiki/word2vec</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/05/22/Hyperparameter-optimization-for-machine-learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jijeng Jia">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jijeng's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/05/22/Hyperparameter-optimization-for-machine-learning/" itemprop="url">Hyper-parameter Optimization for Machine Learning</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-05-22T15:18:44+08:00">
                2019-05-22
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/05/22/Hyperparameter-optimization-for-machine-learning/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2019/05/22/Hyperparameter-optimization-for-machine-learning/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>Following are four common methods of hyperparameter optimization for machine learning in order of increasing efficiency:</p>
<ul>
<li>Manual</li>
<li>Grid search</li>
<li>Random search</li>
<li>Bayesian model-based optimization</li>
</ul>
<h2 id="Random-Search"><a href="#Random-Search" class="headerlink" title="Random Search"></a>Random Search</h2><p>First we will implement a common technique for hyperparameter optimization: random search. Each iteration, we choose a random set of model hyperparameters from a search space. Empirically, random search is very effective, returning nearly as good results as grid search with a significant reduction in time spent searching. However, it is still an uninformed method in the sense that it does not use past evaluations of the objective function to inform the choices it makes for the next evaluation.</p>
<p>Random search uses the following four parts, which also are used in Bayesian hyperparameter optimization:</p>
<ul>
<li>Domain: values over which to search</li>
<li>Optimization algorithm: pick the next values at random! (yes this qualifies as an algorithm)</li>
<li>Objective function to minimize: in this case our metric is cross validation ROC AUC</li>
<li>Results history that tracks the hyperparameters tried and the cross validation metric</li>
</ul>
<p>Random search can be implemented in the Scikit-Learn library using RandomizedSearchCV, however, because we are using Early Stopping (to determine the optimal number of estimators), we will have to implement the method ourselves (more practice!). This is pretty straightforward, and many of the ideas in random search will transfer over to Bayesian hyperparameter optimization.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Load libraries</span></span><br><span class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> uniform</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> linear_model, datasets</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> RandomizedSearchCV</span><br><span class="line"></span><br><span class="line"><span class="comment"># data and model</span></span><br><span class="line"><span class="comment"># Load data</span></span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line">X = iris.data</span><br><span class="line">y = iris.target</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create logistic regression</span></span><br><span class="line">logistic = linear_model.LogisticRegression()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create Hyperparameter Search Space</span></span><br><span class="line"><span class="comment"># Create regularization penalty space</span></span><br><span class="line"><span class="comment"># 如果比较少，那么久枚举出来</span></span><br><span class="line">penalty = [<span class="string">'l1'</span>, <span class="string">'l2'</span>]</span><br><span class="line"><span class="comment"># 如果是有规律的连续的，就使用这种方式列举出来</span></span><br><span class="line"><span class="comment"># Create regularization hyperparameter distribution using uniform distribution</span></span><br><span class="line">C = uniform(loc=<span class="number">0</span>, scale=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create hyperparameter options</span></span><br><span class="line">hyperparameters = dict(C=C, penalty=penalty)</span><br><span class="line"></span><br><span class="line"><span class="comment"># cv: cross validation, This approach involves randomly dividing the set of observations into k groups, or folds, of approximately equal size. The first fold is treated as a validation set, and the method is fit on the remaining k − 1 folds.</span></span><br><span class="line"><span class="comment"># Create randomized search 5-fold cross validation and 100 iterations</span></span><br><span class="line">clf = RandomizedSearchCV(logistic, hyperparameters, random_state=<span class="number">1</span>, n_iter=<span class="number">100</span>, cv=<span class="number">5</span>, verbose=<span class="number">0</span>, n_jobs=<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Fit randomized search</span></span><br><span class="line">best_model = clf.fit(X, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># View best hyperparameters</span></span><br><span class="line"><span class="comment"># 注意这种获取best params 的方式</span></span><br><span class="line">print(<span class="string">'Best Penalty:'</span>, best_model.best_estimator_.get_params()[<span class="string">'penalty'</span>])</span><br><span class="line">print(<span class="string">'Best C:'</span>, best_model.best_estimator_.get_params()[<span class="string">'C'</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Predict target vector</span></span><br><span class="line">best_model.predict(X)</span><br></pre></td></tr></table></figure>
<p><strong>grid search and random search</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> time <span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> randint <span class="keyword">as</span> sp_randint</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> RandomizedSearchCV</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_digits</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"></span><br><span class="line"><span class="comment"># get some data</span></span><br><span class="line">digits = load_digits()</span><br><span class="line">X, y = digits.data, digits.target</span><br><span class="line"></span><br><span class="line"><span class="comment"># build a classifier</span></span><br><span class="line">clf = RandomForestClassifier(n_estimators=<span class="number">20</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Utility function to report best scores</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">report</span><span class="params">(results, n_top=<span class="number">3</span>)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, n_top + <span class="number">1</span>):</span><br><span class="line">        candidates = np.flatnonzero(results[<span class="string">'rank_test_score'</span>] == i)</span><br><span class="line">        <span class="keyword">for</span> candidate <span class="keyword">in</span> candidates:</span><br><span class="line">            print(<span class="string">"Model with rank: &#123;0&#125;"</span>.format(i))</span><br><span class="line">            print(<span class="string">"Mean validation score: &#123;0:.3f&#125; (std: &#123;1:.3f&#125;)"</span>.format(</span><br><span class="line">                  results[<span class="string">'mean_test_score'</span>][candidate],</span><br><span class="line">                  results[<span class="string">'std_test_score'</span>][candidate]))</span><br><span class="line">            print(<span class="string">"Parameters: &#123;0&#125;"</span>.format(results[<span class="string">'params'</span>][candidate]))</span><br><span class="line">            print(<span class="string">""</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># specify parameters and distributions to sample from</span></span><br><span class="line">param_dist = &#123;<span class="string">"max_depth"</span>: [<span class="number">3</span>, <span class="keyword">None</span>],</span><br><span class="line">              <span class="string">"max_features"</span>: sp_randint(<span class="number">1</span>, <span class="number">11</span>),</span><br><span class="line">              <span class="string">"min_samples_split"</span>: sp_randint(<span class="number">2</span>, <span class="number">11</span>),</span><br><span class="line">              <span class="string">"bootstrap"</span>: [<span class="keyword">True</span>, <span class="keyword">False</span>],</span><br><span class="line">              <span class="string">"criterion"</span>: [<span class="string">"gini"</span>, <span class="string">"entropy"</span>]&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># run randomized search</span></span><br><span class="line">n_iter_search = <span class="number">20</span></span><br><span class="line">random_search = RandomizedSearchCV(clf, param_distributions=param_dist,</span><br><span class="line">                                   n_iter=n_iter_search, cv=<span class="number">5</span>, iid=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line">start = time()</span><br><span class="line">random_search.fit(X, y)</span><br><span class="line">print(<span class="string">"RandomizedSearchCV took %.2f seconds for %d candidates"</span></span><br><span class="line">      <span class="string">" parameter settings."</span> % ((time() - start), n_iter_search))</span><br><span class="line">report(random_search.cv_results_)</span><br><span class="line"></span><br><span class="line"><span class="comment"># use a full grid over all parameters</span></span><br><span class="line">param_grid = &#123;<span class="string">"max_depth"</span>: [<span class="number">3</span>, <span class="keyword">None</span>],</span><br><span class="line">              <span class="string">"max_features"</span>: [<span class="number">1</span>, <span class="number">3</span>, <span class="number">10</span>],</span><br><span class="line">              <span class="string">"min_samples_split"</span>: [<span class="number">2</span>, <span class="number">3</span>, <span class="number">10</span>],</span><br><span class="line">              <span class="string">"bootstrap"</span>: [<span class="keyword">True</span>, <span class="keyword">False</span>],</span><br><span class="line">              <span class="string">"criterion"</span>: [<span class="string">"gini"</span>, <span class="string">"entropy"</span>]&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># run grid search</span></span><br><span class="line">grid_search = GridSearchCV(clf, param_grid=param_grid, cv=<span class="number">5</span>, iid=<span class="keyword">False</span>)</span><br><span class="line">start = time()</span><br><span class="line">grid_search.fit(X, y)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"GridSearchCV took %.2f seconds for %d candidate parameter settings."</span></span><br><span class="line">      % (time() - start, len(grid_search.cv_results_[<span class="string">'params'</span>])))</span><br><span class="line">report(grid_search.cv_results_)</span><br></pre></td></tr></table></figure>
<p><strong>Random search without in-built function:</strong><br>write it yourself, more information.<br><a href="https://github.com/WillKoehrsen/hyperparameter-optimization/blob/master/Bayesian%20Hyperparameter%20Optimization%20of%20Gradient%20Boosting%20Machine.ipynb" target="_blank" rel="noopener">https://github.com/WillKoehrsen/hyperparameter-optimization/blob/master/Bayesian%20Hyperparameter%20Optimization%20of%20Gradient%20Boosting%20Machine.ipynb</a></p>
<h2 id="Bayesian-Hyperparameter-Optimization"><a href="#Bayesian-Hyperparameter-Optimization" class="headerlink" title="Bayesian Hyperparameter Optimization"></a>Bayesian Hyperparameter Optimization</h2><p>The one-sentence summary of Bayesian hyperparameter optimization is: build a probability model of the objective function and use it to select the most promising hyperparameters to evaluate in the true objective function.</p>
<p>The basic idea is: spend a little more time selecting the next hyperparameters in order to make fewer calls to the objective function. </p>
<p>In the case of hyperparameter optimization, the objective function is the validation error of a machine learning model using a set of hyperparameters. The aim is to find the hyperparameters that yield the lowest error on the validation set in the hope that these results generalize to the testing set. Evaluating the objective function is expensive because it requires training the machine learning model with a specific set of hyperparameters. Ideally, we want a method that can explore the search space while also limiting evaluations of poor hyperparameter choices. Bayesian hyperparameter tuning uses a continually updated probability model to “concentrate” on promising hyperparameters by reasoning from past results.</p>
<p>有很多基于这种思想的实现，hyperopt 只是其中一种<br>There are several Bayesian optimization libraries in Python which differ in the algorithm for the surrogate of the objective function. In this article, we will work with Hyperopt, which uses the Tree Parzen Estimator (TPE) Other Python libraries include Spearmint (Gaussian Process surrogate) and SMAC (Random Forest Regression). </p>
<p>There are four parts to a Bayesian Optimization problem:</p>
<ul>
<li>Objective Function: what we want to minimize, in this case the validation error of a machine learning model with respect to the hyperparameters （原来model 中的 objective function）</li>
<li>Domain Space: hyperparameter values to search over （调参空间）</li>
<li>Optimization algorithm: method for constructing the surrogate model and choosing the next hyperparameter values to evaluate （loss 和调参空间的 新的关系）</li>
<li>Result history: stored outcomes from evaluations of the objective function consisting of the hyperparameters and validation loss （result 没有什么好说的）</li>
</ul>
<p>其中的 Bayesian Hyperparameter Optimization using Hyperopt是可以好好学习的。 data scientists  这种东西更加贴近于 data scientist 真的。<br>参考一：<br><a href="https://github.com/WillKoehrsen/hyperparameter-optimization/blob/master/Bayesian%20Hyperparameter%20Optimization%20of%20Gradient%20Boosting%20Machine.ipynb" target="_blank" rel="noopener">https://github.com/WillKoehrsen/hyperparameter-optimization/blob/master/Bayesian%20Hyperparameter%20Optimization%20of%20Gradient%20Boosting%20Machine.ipynb</a></p>
<p>参考二：<br><a href="https://github.com/WillKoehrsen/hyperparameter-optimization/blob/master/Introduction%20to%20Bayesian%20Optimization%20with%20Hyperopt.ipynb" target="_blank" rel="noopener">https://github.com/WillKoehrsen/hyperparameter-optimization/blob/master/Introduction%20to%20Bayesian%20Optimization%20with%20Hyperopt.ipynb</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/05/21/Introduction-to-Natural-Language-Processing-for-Text/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jijeng Jia">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jijeng's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/05/21/Introduction-to-Natural-Language-Processing-for-Text/" itemprop="url">Introduction to Natural Language Processing for Text</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-05-21T10:05:39+08:00">
                2019-05-21
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index">
                    <span itemprop="name">NLP</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/05/21/Introduction-to-Natural-Language-Processing-for-Text/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2019/05/21/Introduction-to-Natural-Language-Processing-for-Text/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>Natural Language Processing is used to apply machine learning algorithms to text and speech. For example, we can use it to create systems like speech recognition, document summarization, machine translation, spam detection, named entity recognition, question answering, autocomplete, predictive typing and so on.</p>
<p>NLTK (Natural Language Toolkit) is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to many corpora and lexical resources. Also, it contains a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning. Best of all, NLTK is a free, open source, community-driven project.</p>
<p>In this article, we’ll cover the following topics.<br>这些功能都是可以使用nltk 进行实现的。<br><strong>text Lemmatization</strong> 比如，单词“cars”词形还原后的单词为“car”，单词“ate”词形还原后的单词为“eat”。</p>
<p><strong>Sentence Tokenization</strong><br>段落成句。<br>Sentence tokenization (also called sentence segmentation) is the problem of dividing a string of written language into its component sentences. The idea here looks very simple. In English and some other languages, we can split apart the sentences whenever we see a punctuation mark.（标点符号）</p>
<p><strong>Word Tokenization</strong><br>句子成词，颗粒度变得更小。<br>Word tokenization (also called word segmentation) is the problem of dividing a string of written language into its component words. In English and many other languages using some form of Latin alphabet, space is a good approximation of a word divider.</p>
<p><strong>Text Lemmatization and Stemming</strong><br>这种操作如果被认为是一种 normalization，那么一个优点就是加快了运行的速度。从不同的形式到统一的形式，这可以认为减少了变量。感觉这个更加涉及语法，语法树之类的东西。<br>For grammatical reasons, documents can contain different forms of a word such as drive, drives, driving. Also, sometimes we have related words with a similar meaning, such as nation, national, nationality.</p>
<p>Stemming and lemmatization are special cases of normalization. However, they are different from each other.</p>
<blockquote>
<p>Stemming usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes.<br>Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma.</p>
</blockquote>
<p><strong>Stop Words</strong><br>因为 stop words往往是带了 noise rather than useful information，所以这个是要去掉的。<br>Stop words are words which are filtered out before or after processing of text. When applying machine learning to text, these words can add a lot of noise. That’s why we want to remove these irrelevant words.</p>
<p>stop words dictionary 可以理解成一种过滤词表，是可以根据应用的不同，然后 change的。<br>Stop words usually refer to the most common words such as “and”, “the”, “a” in a language, but there is no single universal list of stopwords. The list of the stop words can change depending on your application.</p>
<p>在存储 stopword 的时候使用 set rather than list 主要原因是  much faster than search operations in a set.<br>You might wonder why we convert our list into a set. Set is an abstract data type that can store unique values, without any particular order. The search operation in a set is much faster than the search operation in a list. For a small number of words, there is no big difference, but if you have a large number of words it’s highly recommended to use the set type.</p>
<p><strong>Regex</strong><br>A kind of search pattern. A regular expression, regex, or regexp is a sequence of characters that define a search pattern. Let’s see some basics.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">. - match any character <span class="keyword">except</span> newline</span><br><span class="line">\w - match word</span><br><span class="line">\d - match digit</span><br><span class="line">\s - match whitespace</span><br><span class="line">\W - match <span class="keyword">not</span> word</span><br><span class="line">\D - match <span class="keyword">not</span> digit</span><br><span class="line">\S - match <span class="keyword">not</span> whitespace</span><br><span class="line">[abc] - match any of a, b, <span class="keyword">or</span> c</span><br><span class="line">[^abc] - <span class="keyword">not</span> match a, b, <span class="keyword">or</span> c</span><br><span class="line">[a-g] - match a character between a &amp; g</span><br></pre></td></tr></table></figure>
<p>这个解释说明了为什么在正则表达式 中使用 r”” 作为一种前缀。因为正则表达是中 ”\“ 的使用和 python 中的”\” 使用有冲突。简而言之，如果加上了 r”” 那么这个就是一种完全的 正则表达式的语法了。</p>
<blockquote>
<p>Regular expressions use the backslash character (‘\’) to indicate special forms or to allow special characters to be used without invoking their special meaning. This collides with Python’s usage of the same character for the same purpose in string literals; for example, to match a literal backslash, one might have to write ‘\\‘ as the pattern string, because the regular expression must be \, and each backslash must be expressed as \ inside a regular Python string literal.<br>The solution is to use Python’s raw string notation for regular expression patterns; backslashes are not handled in any special way in a string literal prefixed with ‘r’. So r”\n” is a two-character string containing ‘\’ and ‘n’, while “\n” is a one-character string containing a newline. Usually, patterns will be expressed in Python code using this raw string notation.</p>
</blockquote>
<p>An example,</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line">sentence = <span class="string">"The development of snowboarding was inspired by skateboarding, sledding, surfing and skiing."</span></span><br><span class="line">pattern = <span class="string">r"[^\w]"</span></span><br><span class="line">print(re.sub(pattern, <span class="string">" "</span>, sentence))</span><br></pre></td></tr></table></figure>
<p><strong>Bag of words</strong></p>
<p><img src="http://ww1.sinaimg.cn/large/e9a223b5ly1g38517f1imj20du0e4aa4.jpg" alt=""><br>Machine learning algorithms cannot work with raw text directly, we need to convert the text into vectors of numbers. This is called feature extraction.<br>The bag-of-words model is a popular and simple feature extraction technique used when we work with text. It describes the occurrence of each word within a document.</p>
<p>这个是 bag  of words的”特点“： order or structure of words 没有体现出来。<br>Any information about the order or structure of words is discarded. That’s why it’s called a bag of words. This model is trying to understand whether a known word occurs in a document, but don’t know where is that word in the document.</p>
<p><strong>The intuition is that similar documents have similar contents</strong>. Also, from a content, we can learn something about the meaning of the document.</p>
<p>To use this model, we need to:</p>
<ul>
<li>Design a vocabulary of known words (also called tokens)</li>
<li>Choose a measure of the presence of known words</li>
</ul>
<p>1) 最简单的方式是 “occurrence” ，如果出现了 标为1 否则标为0；<br>这种是最为简单的 bag of words 最的方式，这四个是一一对应的。注意体会。</p>
<p><img src="http://ww1.sinaimg.cn/large/e9a223b5ly1g38p4a4jq9j20va08it8z.jpg" alt=""><br><img src="http://ww1.sinaimg.cn/large/e9a223b5ly1g38p4hhhltj20o607uq3f.jpg" alt=""></p>
<p>The complexity of the bag-of-words model comes in deciding how to design the vocabulary of known words (tokens) and how to score the presence of known words.</p>
<p>bag of words 中使用 “occurrence” 的方式的缺点：稀疏矩阵（当dict 很大的时候，文章的 representation中有相当成分的0）。</p>
<p>In some cases, we can have a huge amount of data and in this cases, the length of the vector that represents a document might be thousands or millions of elements. Furthermore, each document may contain only a few of the known words in the vocabulary.<br>Therefore the vector representations will have a lot of zeros. These vectors which have a lot of zeros are called sparse vectors. They require more memory and computational resources.<br>We can decrease the number of the known words when using a bag-of-words model to decrease the required memory and computational resources. We can use the text cleaning techniques we’ve already seen in this article before we create our bag-of-words model:</p>
<p>减少 dictionary size 的方式。</p>
<blockquote>
<p>Ignoring punctuation<br>Removing the stop words from our documents<br>Reducing the words to their base form (Text Lemmatization and Stemming)<br>Fixing misspelled words</p>
</blockquote>
<p>n-gram 的思想是很广泛：通过 sequence of words，这个是可以增加文本的表达力的。<br>An n-gram is a sequence of a number of items (words, letter, numbers, digits, etc.). In the context of text corpora, n-grams typically refer to a sequence of words. A unigram is one word, a bigram is a sequence of two words, a trigram is a sequence of three words etc.</p>
<p>关于如何去 score the presence  of word： 这里是有三种方式的。<br>We saw one very simple approach - the binary approach (1 for presence, 0 for absence).<br>Some additional scoring methods are:<br>2) Counts. Count the number of times each word appears in a document.<br>3) Frequencies. Calculate the frequency that each word appears in document out of all the words in the document.</p>
<p>TF-IDF 这个语境 是相对于 frequency 而言的，关键词是不一定有 频率所决定，而一些 rarer or domain-specific words 可能是更加常见的。<br>One problem with scoring word frequency is that the most frequent words in the document start to have the highest scores. These frequent words may not contain as much “informational gain” to the model compared with some rarer and domain-specific words. One approach to fix that problem is to penalize words that are frequent across all the documents. This approach is called TF-IDF.</p>
<p>TF-IDF 的关键在于体现了“语料库”。<br>TF-IDF, short for term frequency-inverse document frequency is a statistical measure used to evaluate the importance of a word to a document in a collection or corpus.</p>
<p><strong>参考资料</strong><br><a href="https://towardsdatascience.com/introduction-to-natural-language-processing-for-text-df845750fb63" target="_blank" rel="noopener">https://towardsdatascience.com/introduction-to-natural-language-processing-for-text-df845750fb63</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/05/17/剑指offer-其他/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jijeng Jia">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jijeng's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/05/17/剑指offer-其他/" itemprop="url">剑指offer-其他</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-05-17T20:41:38+08:00">
                2019-05-17
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/算法/" itemprop="url" rel="index">
                    <span itemprop="name">算法</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/05/17/剑指offer-其他/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2019/05/17/剑指offer-其他/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>这是剑指offer 系列四部曲中的最后一部，因为有些算法题目类别数量太少就汇总到了”其他“, 比如位运算、正则匹配等。第一部关于<a href="https://jijeng.github.io/2019/05/17/%E5%89%91%E6%8C%87Offer-%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%92%8C%E6%95%B0%E7%BB%84/" target="_blank" rel="noopener">字符串和数组</a>，第二部是<a href="https://jijeng.github.io/2019/05/17/%E5%89%91%E6%8C%87Offer-%E6%A0%88-%E9%98%9F%E5%88%97-%E9%93%BE%E8%A1%A8%E5%92%8C%E6%A0%91/" target="_blank" rel="noopener">栈、队列、链表和树</a>， 第三部<a href="https://jijeng.github.io/2019/05/17/%E5%89%91%E6%8C%87offer-%E9%80%92%E5%BD%92-%E5%9B%9E%E6%BA%AF%E5%92%8C%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/" target="_blank" rel="noopener">递归、回溯和动态规划</a>。</p>
<ul>
<li>二进制中1的个数</li>
</ul>
<blockquote>
<p>输入一个整数，输出该数二进制表示中1的个数。其中负数用补码表示。</p>
</blockquote>
<p>Tips：首先处理正数, bitwise and operation， 很简单。对于负数，需要转换成 正数然后进行处理，math。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">NumberOf1</span><span class="params">(self, n)</span>:</span></span><br><span class="line">        <span class="comment"># write code here</span></span><br><span class="line">        <span class="keyword">if</span> n == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> n &gt; <span class="number">0</span>:</span><br><span class="line">            counts = self.number_of_positive(n)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            n = abs(n) - <span class="number">1</span></span><br><span class="line">            counts = <span class="number">32</span> - self.number_of_positive(n)</span><br><span class="line">        <span class="keyword">return</span> counts</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">number_of_positive</span><span class="params">(self, n)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> n == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        counts = <span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> n:</span><br><span class="line">            counts += (n &amp; <span class="number">1</span>)</span><br><span class="line">            n = n &gt;&gt; <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> counts</span><br></pre></td></tr></table></figure>
<ul>
<li>数值的整数次方</li>
</ul>
<blockquote>
<p>给定一个double类型的浮点数base和int类型的整数exponent。求base的exponent次方。</p>
</blockquote>
<p>Tips:  次方使用乘法来进行累乘</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    这个就是边界条件比较多而已，需要分别判断 base 和 exponent 的正负</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">Power</span><span class="params">(self, base, exponent)</span>:</span></span><br><span class="line">        <span class="comment"># write code here</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> base == <span class="number">0</span> <span class="keyword">and</span> exponent != <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        <span class="keyword">if</span> base != <span class="number">0</span> <span class="keyword">and</span> exponent == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">        flag = <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> base &lt;= <span class="number">0</span> <span class="keyword">and</span> (exponent % <span class="number">2</span> == <span class="number">1</span>):</span><br><span class="line">            flag = <span class="number">-1</span></span><br><span class="line">        base = abs(base)</span><br><span class="line">        result = <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> exponent &gt; <span class="number">0</span>:</span><br><span class="line">            reverse = <span class="number">0</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            reverse = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        exponent = abs(exponent)</span><br><span class="line">        <span class="keyword">if</span> exponent % <span class="number">2</span> == <span class="number">0</span>:</span><br><span class="line">            result = base * base</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(exponent // <span class="number">2</span> - <span class="number">1</span>):</span><br><span class="line">                result = result * result</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            result = base * base</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(exponent // <span class="number">2</span> - <span class="number">1</span>):</span><br><span class="line">                result = result * result</span><br><span class="line">            result = result * base</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> reverse:</span><br><span class="line">            result = <span class="number">1.0</span> / result</span><br><span class="line">        <span class="keyword">return</span> result * flag</span><br></pre></td></tr></table></figure>
<ul>
<li>最小的K个数</li>
</ul>
<blockquote>
<p>输入n个整数，找出其中最小的K个数。例如输入4,5,1,6,2,7,3,8这8个数字，则最小的4个数字是1,2,3,4,。</p>
</blockquote>
<p>Tips： 这个有点投机取巧了，使用了 “heapq” 的库函数。这个题目跟 第 K个 smallest 是有差别的，快排中的 partition 是找到了 一个数字在最后排序结果中的位置。对于有”累加“前 K个数字还是要使用常规的排序。比较好的就是堆排序。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="comment"># 想说的是既然是使用这种开源的库函数 那么就记住这种函数名字</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">GetLeastNumbers_Solution</span><span class="params">(self, tinput, k)</span>:</span></span><br><span class="line">        <span class="comment"># write code here</span></span><br><span class="line">        <span class="keyword">if</span> len(tinput) &lt; k:</span><br><span class="line">            <span class="keyword">return</span> []</span><br><span class="line">        <span class="keyword">import</span> heapq</span><br><span class="line">        res = heapq.nsmallest(k, tinput)</span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<blockquote>
<p>The function partition puts the numbers smaller than nums[left] to its left and then returns the new index of nums[left]. The returned index is actually telling us how small nums[left] ranks in nums. </p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">findKthLargest</span><span class="params">(self, nums, k)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :type nums: List[int]</span></span><br><span class="line"><span class="string">        :type k: int</span></span><br><span class="line"><span class="string">        :rtype: int</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        left, right = <span class="number">0</span>, len(nums) - <span class="number">1</span></span><br><span class="line">        <span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line">            pos = self.partition(nums, left, right)</span><br><span class="line">            <span class="comment"># 这个在排序的时候，是把大的数字放到前面，而前面是pos 是从0 开始的，</span></span><br><span class="line">            <span class="comment"># 所以这里是 k-1</span></span><br><span class="line">            <span class="keyword">if</span> pos == k - <span class="number">1</span>:</span><br><span class="line">                <span class="keyword">return</span> nums[pos]</span><br><span class="line">            <span class="comment"># 左边的并不足以构成k 个， 那么在右边</span></span><br><span class="line">            <span class="keyword">elif</span> pos &lt; k - <span class="number">1</span>:</span><br><span class="line">                left = pos + <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                right = pos - <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">partition</span><span class="params">(self, nums, left, right)</span>:</span></span><br><span class="line">        <span class="comment"># choose nums[left] as pivot</span></span><br><span class="line">        pivot = nums[left]</span><br><span class="line">        <span class="comment"># p1, p2就类似 working 中的left right</span></span><br><span class="line">        p1, p2 = left + <span class="number">1</span>, right</span><br><span class="line">        <span class="keyword">while</span> p1 &lt;= p2:</span><br><span class="line">            <span class="keyword">if</span> nums[p1] &lt; pivot <span class="keyword">and</span> nums[p2] &gt; pivot:</span><br><span class="line">                nums[p1], nums[p2] = nums[p2], nums[p1]</span><br><span class="line">                p1, p2 = p1 + <span class="number">1</span>, p2 - <span class="number">1</span></span><br><span class="line">            <span class="keyword">elif</span> nums[p1] &gt;= pivot:</span><br><span class="line">                p1 += <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>: <span class="comment">#nums[p2] &lt;= pivot:</span></span><br><span class="line">                p2 -=<span class="number">1</span></span><br><span class="line"></span><br><span class="line">        nums[left], nums[p2] = nums[p2], nums[left]</span><br><span class="line">        <span class="keyword">return</span> p2</span><br></pre></td></tr></table></figure>
<ul>
<li>整数中1出现的次数（从1到n整数中1出现的次数）</li>
</ul>
<blockquote>
<p>求出1~13的整数中1出现的次数,并算出100~1300的整数中1出现的次数？为此他特别数了一下1~13中包含1的数字有1、10、11、12、13因此共出现6次,但是对于后面问题他就没辙了。ACMer希望你们帮帮他,并把问题更加普遍化,可以很快的求出任意非负整数区间中1出现的次数（从1 到 n 中1出现的次数）。</p>
</blockquote>
<p>Tips：math, 计数原理，按位统计该位为1时可能包含的数字总数.由低位向高位依次遍历数字n的每一位curn。记当前位数为c，curn左边（高位）的数字片段为highn，cur右边（低位）的数字片段为lown，lowc = 10 ^ c</p>
<ul>
<li>若curn = 0，则高位范围为0 ~ highn - 1，低位0 ~ lowc - 1</li>
<li>若curn = 1，则高位范围为0 ~ highn - 1，低位0 ~ lowc - 1；或者 高位为highn， 低位0 ~ lown</li>
<li>若curn ＞ 1，则高位范围为0 ~ highn， 低位为0 ~ lowc - 1</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="comment"># 数字的基本结构分成  weights +working_num + n%base 这三个部分</span></span><br><span class="line">    <span class="comment"># 然后一个while 循环是处理一个数字</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">NumberOf1Between1AndN_Solution</span><span class="params">(self, n)</span>:</span></span><br><span class="line">        <span class="comment"># write code here</span></span><br><span class="line">        <span class="keyword">if</span> n &lt; <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        num = n</span><br><span class="line">        counts = <span class="number">0</span></span><br><span class="line">        base = <span class="number">1</span></span><br><span class="line">        <span class="keyword">while</span> num:</span><br><span class="line">            weights = num % <span class="number">10</span></span><br><span class="line">            num = num // <span class="number">10</span></span><br><span class="line">            counts += base * num</span><br><span class="line">            <span class="keyword">if</span> weights == <span class="number">1</span>:</span><br><span class="line">                counts += (n % base) + <span class="number">1</span></span><br><span class="line">            <span class="keyword">elif</span> weights &gt; <span class="number">1</span>:</span><br><span class="line">                counts += base</span><br><span class="line">            base *= <span class="number">10</span></span><br><span class="line">        <span class="keyword">return</span> counts </span><br><span class="line">```  </span><br><span class="line"></span><br><span class="line">- 把数组排成最小的数</span><br><span class="line"></span><br><span class="line">&gt; 输入一个正整数数组，把数组里所有数字拼接起来排成一个数，打印能拼接出的所有数字中最小的一个。例如输入数组&#123;<span class="number">3</span>，<span class="number">32</span>，<span class="number">321</span>&#125;，则打印出这三个数字能排成的最小数字为<span class="number">321323</span>。</span><br><span class="line"></span><br><span class="line">Tips：使用sorted() 函数， string 类型的排序 和 int 类型的排序是一样的，在python 里面来说。</span><br><span class="line"></span><br><span class="line">```python</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">PrintMinNumber</span><span class="params">(self, numbers)</span>:</span></span><br><span class="line">        <span class="comment"># write code here</span></span><br><span class="line">        sorted_list = sorted(numbers, cmp=<span class="keyword">lambda</span> a, b: cmp(str(a) + str(b), str(b) + str(a)))</span><br><span class="line">        <span class="comment"># 这个时候已经排好序，然后只要一个个连接起来就行了</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">''</span>.join(map(str, sorted_list))</span><br></pre></td></tr></table></figure>
<ul>
<li>丑数</li>
</ul>
<blockquote>
<p>把只包含质因子2、3和5的数称作丑数（Ugly Number）。例如6、8都是丑数，但14不是，因为它包含质因子7。 习惯上我们把1当做是第一个丑数。求按从小到大的顺序的第N个丑数。</p>
</blockquote>
<p>Tips：之后的丑数肯定是2，3或5 的倍数，分别单独计数，然后选择最小的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="comment"># 在进行 append 操作的时候去重，</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">GetUglyNumber_Solution</span><span class="params">(self, index)</span>:</span></span><br><span class="line">        <span class="comment"># write code here</span></span><br><span class="line">        <span class="keyword">if</span> index &lt;<span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        list1 = [<span class="number">1</span>]</span><br><span class="line">        <span class="comment"># 意味着只能是 append() 操作了</span></span><br><span class="line">        i, j, k = <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> len(list1) &lt; index:</span><br><span class="line">            num = min(list1[i] * <span class="number">2</span>, list1[j] * <span class="number">3</span>, list1[k] * <span class="number">5</span>)</span><br><span class="line">            <span class="keyword">if</span> num &gt; list1[<span class="number">-1</span>]:</span><br><span class="line">                list1.append(num)</span><br><span class="line">    </span><br><span class="line">            <span class="keyword">if</span> num == list1[i] * <span class="number">2</span>:</span><br><span class="line">                i += <span class="number">1</span></span><br><span class="line">            <span class="keyword">elif</span> num == list1[j] * <span class="number">3</span>:</span><br><span class="line">                j += <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                k += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> list1[<span class="number">-1</span>]</span><br></pre></td></tr></table></figure>
<ul>
<li>正则表达式匹配</li>
</ul>
<blockquote>
<p>请实现一个函数用来匹配包括’.’和’<em>‘的正则表达式。模式中的字符’.’表示任意一个字符，而’</em>‘表示它前面的字符可以出现任意次（包含0次）。 在本题中，匹配是指字符串的所有字符匹配整个模式。例如，字符串”aaa”与模式”a.a”和”ab<em>ac</em>a”匹配，但是与”aa.a”和”ab*a”均不匹配</p>
</blockquote>
<p>Tips:  dp 问题。转换方程 dp[i][j] i 表示 string 的index j表示 pattern 的index， dp[i][j] ==dp[i-1][j-1] or dp[i][j] =dp[i][j-2] or dp[i][j-1] 。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="comment"># s, pattern都是字符串</span></span><br><span class="line">    <span class="comment"># https://www.youtube.com/watch?v=l3hda49XcDE 心中一定要有这个表格, a[i][j] 这个更像是一种指针</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">match</span><span class="params">(self, s, pattern)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> len(s) == <span class="number">0</span> <span class="keyword">and</span> len(pattern) == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">True</span></span><br><span class="line">        dp = [[<span class="keyword">False</span> <span class="keyword">for</span> _ <span class="keyword">in</span> range(len(pattern) + <span class="number">1</span>)] <span class="keyword">for</span> _ <span class="keyword">in</span> range(len(s) + <span class="number">1</span>)]</span><br><span class="line">        dp[<span class="number">0</span>][<span class="number">0</span>] = <span class="keyword">True</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">1</span>, len(pattern) + <span class="number">1</span>):</span><br><span class="line">            <span class="keyword">if</span> pattern[j - <span class="number">1</span>] == <span class="string">"*"</span>:</span><br><span class="line">                dp[<span class="number">0</span>][j] = dp[<span class="number">0</span>][j - <span class="number">2</span>]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, len(s) + <span class="number">1</span>):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">1</span>, len(pattern) + <span class="number">1</span>):</span><br><span class="line">                <span class="keyword">if</span> pattern[j - <span class="number">1</span>] == s[i - <span class="number">1</span>] <span class="keyword">or</span> pattern[j - <span class="number">1</span>] == <span class="string">"."</span>:</span><br><span class="line">                    dp[i][j] = dp[i - <span class="number">1</span>][j - <span class="number">1</span>]</span><br><span class="line">    </span><br><span class="line">                <span class="keyword">elif</span> pattern[j - <span class="number">1</span>] == <span class="string">"*"</span>:</span><br><span class="line">    </span><br><span class="line">                    dp[i][j] = dp[i][j - <span class="number">2</span>]</span><br><span class="line">                    <span class="keyword">if</span> s[i - <span class="number">1</span>] == pattern[j - <span class="number">2</span>] <span class="keyword">or</span> pattern[j - <span class="number">2</span>] == <span class="string">"."</span>:</span><br><span class="line">                        dp[i][j] = dp[i][j] <span class="keyword">or</span> dp[i - <span class="number">1</span>][j]</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    dp[i][j] = <span class="keyword">False</span></span><br><span class="line">        <span class="keyword">return</span> dp[len(s)][len(pattern)]</span><br></pre></td></tr></table></figure>
<ul>
<li>数据流中的中位数</li>
</ul>
<blockquote>
<p>如何得到一个数据流中的中位数？如果从数据流中读出奇数个数值，那么中位数就是所有数值排序之后位于中间的数值。如果从数据流中读出偶数个数值，那么中位数就是所有数值排序之后中间两个数的平均值。我们使用Insert()方法读取数据流，使用GetMedian()方法获取当前读取数据的中位数。</p>
</blockquote>
<p>Tips：主要是体现了数据流，要求能够 insert 元素，然后基于当前的状态去 getmedian() ，是动态的，而不是静态的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    对于数据流 这个应该是第二次接触了，需要使用一个全局变量</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># 虽然知道这个使用 堆的思想是更优的，搜索时间可以O（1）， 堆的调整是 O(log n)</span></span><br><span class="line">    <span class="comment"># 但是没有什么很好的教程，所以我也没有学会啊</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.list1 = []</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">Insert</span><span class="params">(self, num)</span>:</span></span><br><span class="line">        self.list1.append(num)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">GetMedian</span><span class="params">(self, ch)</span>:</span></span><br><span class="line">        length = len(self.list1)</span><br><span class="line">        <span class="comment"># 我记得有一个更加快一些</span></span><br><span class="line">        self.list1 = sorted(self.list1)</span><br><span class="line">        <span class="keyword">if</span> length % <span class="number">2</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> (self.list1[length // <span class="number">2</span>] + self.list1[length // <span class="number">2</span> - <span class="number">1</span>]) / <span class="number">2.0</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> self.list1[length // <span class="number">2</span>]</span><br></pre></td></tr></table></figure>
<ul>
<li>滑动窗口的最大值</li>
</ul>
<blockquote>
<p>给定一个数组和滑动窗口的大小，找出所有滑动窗口里数值的最大值。例如，如果输入数组{2,3,4,2,6,2,5,1}及滑动窗口的大小3，那么一共存在6个滑动窗口，他们的最大值分别为{4,4,6,6,6,5}； 针对数组{2,3,4,2,6,2,5,1}的滑动窗口有以下6个： {[2,3,4],2,6,2,5,1}， {2,[3,4,2],6,2,5,1}， {2,3,[4,2,6],2,5,1}， {2,3,4,[2,6,2],5,1}， {2,3,4,2,[6,2,5],1}， {2,3,4,2,6,[2,5,1]}。</p>
</blockquote>
<p>Tips：使用max(list1) 这样的操作是可行的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="comment"># 最简单的模拟滑动窗口 的过程</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">maxInWindows</span><span class="params">(self, num, size)</span>:</span></span><br><span class="line">        slip = []</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> num <span class="keyword">or</span> len(num) &lt; size <span class="keyword">or</span> size == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(num) - size + <span class="number">1</span>):</span><br><span class="line">            slip.append(max(num[i:i + size]))</span><br><span class="line">        <span class="keyword">return</span> slip</span><br></pre></td></tr></table></figure>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/05/17/剑指offer-递归-回溯和动态规划/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jijeng Jia">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jijeng's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/05/17/剑指offer-递归-回溯和动态规划/" itemprop="url">剑指offer-递归、回溯和动态规划</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-05-17T20:40:33+08:00">
                2019-05-17
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/算法/" itemprop="url" rel="index">
                    <span itemprop="name">算法</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/05/17/剑指offer-递归-回溯和动态规划/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2019/05/17/剑指offer-递归-回溯和动态规划/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>这是剑指offer 系列四部曲中的第三部：递归、回溯和动态规划。第一部关于<a href="https://jijeng.github.io/2019/05/17/%E5%89%91%E6%8C%87Offer-%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%92%8C%E6%95%B0%E7%BB%84/" target="_blank" rel="noopener">字符串和数组</a>，第二部是<a href="https://jijeng.github.io/2019/05/17/%E5%89%91%E6%8C%87Offer-%E6%A0%88-%E9%98%9F%E5%88%97-%E9%93%BE%E8%A1%A8%E5%92%8C%E6%A0%91/" target="_blank" rel="noopener">栈、队列、链表和树</a>， 最后一部分在<a href="https://jijeng.github.io/2019/05/17/%E5%89%91%E6%8C%87offer-%E5%85%B6%E4%BB%96/" target="_blank" rel="noopener">这里</a>。</p>
<ul>
<li>斐波那契数列</li>
</ul>
<blockquote>
<p>大家都知道斐波那契数列，现在要求输入一个整数n，请你输出斐波那契数列的第n项（从0开始，第0项为0）。n&lt;=39</p>
</blockquote>
<p>Tips: 简单的递归，可以转换成循环。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="comment"># python 中list 的初始化，最开始的是从0 开始，所以是需要多进行一个初始化的</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">Fibonacci</span><span class="params">(self, n)</span>:</span></span><br><span class="line">        <span class="comment"># write code here</span></span><br><span class="line">        <span class="keyword">if</span> n == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        <span class="keyword">if</span> n == <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">        arr = [<span class="number">0</span>] * (n + <span class="number">1</span>)</span><br><span class="line">        arr[<span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">        arr[<span class="number">1</span>] = <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>, n + <span class="number">1</span>):</span><br><span class="line">            arr[i] = arr[i - <span class="number">1</span>] + arr[i - <span class="number">2</span>]</span><br><span class="line">        <span class="keyword">return</span> arr[n]</span><br></pre></td></tr></table></figure>
<ul>
<li>跳台阶</li>
</ul>
<blockquote>
<p>一只青蛙一次可以跳上1级台阶，也可以跳上2级。求该青蛙跳上一个n级的台阶总共有多少种跳法（先后次序不同算不同的结果）。</p>
</blockquote>
<p>Tips： 同上。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">jumpFloor</span><span class="params">(self, number)</span>:</span></span><br><span class="line">        <span class="comment"># write code here</span></span><br><span class="line">        <span class="keyword">if</span> number == <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> number == <span class="number">2</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">2</span></span><br><span class="line"></span><br><span class="line">        arr = [<span class="number">0</span>] * (number + <span class="number">1</span>)</span><br><span class="line">        arr[<span class="number">1</span>] = <span class="number">1</span></span><br><span class="line">        arr[<span class="number">2</span>] = <span class="number">2</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">3</span>, number + <span class="number">1</span>):</span><br><span class="line">            arr[i] = arr[i - <span class="number">1</span>] + arr[i - <span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> arr[number]</span><br></pre></td></tr></table></figure>
<ul>
<li>跳台阶2</li>
</ul>
<blockquote>
<p>一只青蛙一次可以跳上1级台阶，也可以跳上2级……它也可以跳上n级。求该青蛙跳上一个n级的台阶总共有多少种跳法。</p>
</blockquote>
<p>Tips：同上。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    在使用 for循环的时候，注意 range() 这种取值，究竟是使用 range() 作为次数的计量；</span></span><br><span class="line"><span class="string">    还是要使用range 中的index 。两者是不相同的操作，尤其是对于前后的取值。</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">jumpFloorII</span><span class="params">(self, number)</span>:</span></span><br><span class="line">        <span class="comment"># write code here</span></span><br><span class="line">        <span class="keyword">if</span> number == <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">        nums = <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(number - <span class="number">1</span>):</span><br><span class="line">            nums = nums * <span class="number">2</span></span><br><span class="line">        <span class="keyword">return</span> nums</span><br></pre></td></tr></table></figure>
<ul>
<li>矩形覆盖</li>
</ul>
<blockquote>
<p>我们可以用2*1的小矩形横着或者竖着去覆盖更大的矩形。请问用n个2*1的小矩形无重叠地覆盖一个2*n的大矩形，总共有多少种方法？</p>
</blockquote>
<p>Tips: math, 找出递归方程。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">既然结果只是最后一个解，那么就没有必要保存中间变量，所以只是，</span></span><br><span class="line"><span class="string">所以空间复杂度从O（n） -&gt; O(1) ，这个是超级nice的</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">rectCover</span><span class="params">(self, number)</span>:</span></span><br><span class="line">        <span class="comment"># write code here</span></span><br><span class="line">        <span class="keyword">if</span> number &lt;= <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        <span class="keyword">if</span> number &lt;= <span class="number">2</span>:</span><br><span class="line">            <span class="keyword">return</span> number</span><br><span class="line">        <span class="comment"># 只是两个变量罢了</span></span><br><span class="line">        a, b = <span class="number">1</span>, <span class="number">2</span></span><br><span class="line">        <span class="keyword">while</span> number &gt; <span class="number">2</span>:</span><br><span class="line">            a, b = b, a + b</span><br><span class="line">            number -= <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> b</span><br></pre></td></tr></table></figure>
<ul>
<li>机器人的运动范围</li>
</ul>
<blockquote>
<p>地上有一个m行和n列的方格。一个机器人从坐标0,0的格子开始移动，每一次只能向左，右，上，下四个方向移动一格，但是不能进入行坐标和列坐标的数位之和大于k的格子。 例如，当k为18时，机器人能够进入方格（35,37），因为3+5+3+7 = 18。但是，它不能进入方格（35,38），因为3+5+3+8 = 19。请问该机器人能够达到多少个格子？</p>
</blockquote>
<p>Tips：递归，转移方程不难，在上下左右四个方向进行尝试，需要判断的条件比较多，比如是否访问过，数位之和等一些条件。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">movingCount</span><span class="params">(self, threshold, rows, cols)</span>:</span></span><br><span class="line">        <span class="comment"># visited 不一定是二维的，只要是能够"自圆其说" 就行。</span></span><br><span class="line">        visited = [<span class="keyword">False</span>] * (rows * cols)</span><br><span class="line">        count = self.movingCountCore(threshold, rows, cols, <span class="number">0</span>, <span class="number">0</span>, visited)</span><br><span class="line">        <span class="keyword">return</span> count</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">movingCountCore</span><span class="params">(self, threshold, rows, cols, row, col, visited)</span>:</span></span><br><span class="line">        count = <span class="number">0</span></span><br><span class="line">        <span class="comment"># 就是这个访问记录是需要进行变化的， 如果是false ，然后访问之后 是需要设置为 true的</span></span><br><span class="line">        <span class="keyword">if</span> self.check(threshold, rows, cols, row, col, visited):</span><br><span class="line">            visited[row * cols + col] = <span class="keyword">True</span></span><br><span class="line">            count = <span class="number">1</span> + self.movingCountCore(threshold, rows, cols, row, col - <span class="number">1</span>, visited) + \</span><br><span class="line">                    self.movingCountCore(threshold, rows, cols, row, col + <span class="number">1</span>, visited) + \</span><br><span class="line">                    self.movingCountCore(threshold, rows, cols, row + <span class="number">1</span>, col, visited) + \</span><br><span class="line">                    self.movingCountCore(threshold, rows, cols, row - <span class="number">1</span>, col, visited)</span><br><span class="line">        <span class="keyword">return</span> count</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">check</span><span class="params">(self, threshold, rows, cols, row, col, visited)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> row &gt;= <span class="number">0</span> <span class="keyword">and</span> row &lt; rows <span class="keyword">and</span> col &gt;= <span class="number">0</span> <span class="keyword">and</span> col &lt; cols <span class="keyword">and</span> self.judge(threshold, row, col) <span class="keyword">and</span> <span class="keyword">not</span> visited[row * cols + col]:</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">True</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">judge</span><span class="params">(self, threshold, i, j)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> sum(map(int, str(i) + str(j))) &lt;= threshold:</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">True</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">False</span></span><br><span class="line">```            </span><br><span class="line"></span><br><span class="line">- 矩阵中的路径</span><br><span class="line"></span><br><span class="line">&gt; 请设计一个函数，用来判断在一个矩阵中是否存在一条包含某字符串所有字符的路径。路径可以从矩阵中的任意一个格子开始，每一步可以在矩阵中向左，向右，向上，向下移动一个格子。如果一条路径经过了矩阵中的某一个格子，则之后不能再次进入这个格子。 例如 a b c e s f c s a d e e 这样的<span class="number">3</span> X <span class="number">4</span> 矩阵中包含一条字符串<span class="string">"bcced"</span>的路径，但是矩阵中不包含<span class="string">"abcb"</span>路径，因为字符串的第一个字符b占据了矩阵中的第一行第二个格子之后，路径不能再次进入该格子。</span><br><span class="line"></span><br><span class="line">Tips： 在二维数组中每个点上都进行尝试，每个点上同样是上下左右进行尝试，返回符合条件的。</span><br><span class="line"></span><br><span class="line">```python</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="comment"># 递归 这个是 true or false 判断类型的。</span></span><br><span class="line">    <span class="comment"># 思路：先是 rows* cols 这样的全部遍历</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">hasPath</span><span class="params">(self, matrix, rows, cols, path)</span>:</span></span><br><span class="line">        <span class="comment"># 如果使用 [ for _in range(rows) ] for _ in range(cols) ， 这个是有结构的 rows* cols</span></span><br><span class="line">        assist = [<span class="keyword">True</span>] * rows * cols</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(rows):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(cols):</span><br><span class="line">                <span class="keyword">if</span> self.rightPath(matrix, rows, cols, i, j, path, assist):</span><br><span class="line">                    <span class="keyword">return</span> <span class="keyword">True</span></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">False</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">rightPath</span><span class="params">(self, matrix, rows, cols, i, j, path, assist)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> path:</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">True</span></span><br><span class="line">        index = i * cols + j</span><br><span class="line">        <span class="keyword">if</span> i &lt; <span class="number">0</span> <span class="keyword">or</span> i &gt;= rows <span class="keyword">or</span> j &lt; <span class="number">0</span> <span class="keyword">or</span> j &gt;= cols <span class="keyword">or</span> matrix[index] != path[<span class="number">0</span>] <span class="keyword">or</span> assist[index] == <span class="keyword">False</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">False</span></span><br><span class="line">        assist[index] = <span class="keyword">False</span></span><br><span class="line">        <span class="keyword">if</span> (self.rightPath(matrix, rows, cols, i + <span class="number">1</span>, j, path[<span class="number">1</span>:], assist) <span class="keyword">or</span></span><br><span class="line">                self.rightPath(matrix, rows, cols, i - <span class="number">1</span>, j, path[<span class="number">1</span>:], assist) <span class="keyword">or</span></span><br><span class="line">                self.rightPath(matrix, rows, cols, i, j - <span class="number">1</span>, path[<span class="number">1</span>:], assist) <span class="keyword">or</span></span><br><span class="line">                self.rightPath(matrix, rows, cols, i, j + <span class="number">1</span>, path[<span class="number">1</span>:], assist)):</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">True</span></span><br><span class="line">        assist[index] = <span class="keyword">True</span></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">False</span></span><br></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/5/">5</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/avatar.jpeg"
                alt="Jijeng Jia" />
            
              <p class="site-author-name" itemprop="name">Jijeng Jia</p>
              <p class="site-description motion-element" itemprop="description">Stay Hungry, Stay Foolish</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">45</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">7</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">30</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/jijeng" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:jia1509309698@163.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Jijeng Jia</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>

<div>
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
Total  <span id="busuanzi_value_site_pv"></span> views
You got  <span id="busuanzi_value_site_uv"></span> visitors
</div>




        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      
    </span>
  
</div>








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  

    
      <script id="dsq-count-scr" src="https://https-jijeng-github-io.disqus.com/count.js" async></script>
    

    

  




	





  














  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
