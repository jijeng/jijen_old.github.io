<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="Stay Hungry, Stay Foolish">
<meta property="og:type" content="website">
<meta property="og:title" content="Jijeng&#39;s blog">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="Jijeng&#39;s blog">
<meta property="og:description" content="Stay Hungry, Stay Foolish">
<meta property="og:locale" content="default">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Jijeng&#39;s blog">
<meta name="twitter:description" content="Stay Hungry, Stay Foolish">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/"/>





  <title>Jijeng's blog</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Jijeng's blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="Searching..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/04/18/mode-collapse-in-gan/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jijeng Jia">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jijeng's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/04/18/mode-collapse-in-gan/" itemprop="url">Mode Collapse in GANs</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-04-18T21:42:01+08:00">
                2019-04-18
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Deep-learning/" itemprop="url" rel="index">
                    <span itemprop="name">Deep learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>Mode collapse, a failure case for GANs where the generator generate a limited diversity of samples, regardless of the input.</p>
<p>But what causes the mode collapse? There are four reasons for that.</p>
<p><strong>The objective of GANs</strong></p>
<p>The generator, generates new data, while the discriminator evaluates it for authenticity but not for the diversity of generated instances.</p>
<p>the generator can win by producing a polynomial number of training examples. And a low capacity discriminator cannot detect this process, thus, it cannot guide the generator to approximate the target distribution. Even if a high discriminator identifies and assigns the collapse part a low probability, then the generator will simply move from its collapsed output to focus on another fixed output point.</p>
<p><strong>Generator</strong></p>
<p>No matter the objective function is, if it only considers individual samples (without looking forward or backward) then the generator is not directly incentivised to produce diverse examples.</p>
<p>From [1], standard GAN training corresponds exactly to updating the generator parameters using only the first term in this gradient because of a fixed discriminator during GAN training. Therefore, in standard GAN training, each generator updata step is a partial collapse towards a delta function.</p>
<p>$$<br>\frac { \mathrm { d } f _ { K } \left( \theta _ { G } , \theta _ { D } \right) } { \mathrm { d } \theta _ { G } } = \frac { \partial f \left( \theta _ { G } , \theta _ { D } ^ { K } \left( \theta _ { G } , \theta _ { D } \right) \right) } { \partial \theta _ { G } } + \frac { \partial f \left( \theta _ { G } , \theta _ { D } ^ { K } \left( \theta _ { G } , \theta _ { D } \right) \right) } { \partial \theta _ { D } ^ { K } \left( \theta _ { G } , \theta _ { D } \right) } \frac { \mathrm { d } \theta _ { D } ^ { K } \left( \theta _ { G } , \theta _ { D } \right) } { \mathrm { d } \theta _ { G } }<br>$$</p>
<p>Some methods have been proposed. Multiple generators and weight-sharing generators are developed to capture more modes of the distribution. </p>
<p><strong>Discriminator</strong></p>
<p>The mode collapse is often explained as gradient exploding of discriminator, which comes from the imbalance between the discriminator and the generator. For example, the technique of TTUR could help discriminator to keep its optimality. But some researchers believe that this is a desirable goal since a good discriminator can give good feedback and ignore the fact.</p>
<p>In addition, the discriminator process each example independently, the generator depends on generator, thus  no mechanism to tell the outputs of the generator to become more similar to each other. </p>
<p>The idea from [2], that we could use minibatch discrimination to help generator give better feedback</p>
<p>A straightforward approach to handle multimodality is to take random noise vectors along with the conditional contexts as inputs, where the contexts determine the main content and noise vectors are responsible for variations.<br>The noise vectors are ignored or of minor impacts, since cGANs pay more attention to learn from the highdimensional and structured conditional contexts. </p>
<p><strong>Another question</strong></p>
<p>Mode collapse may happen only partially?<br>since training is stochastic progress, the input of generator network will vary and the sample drawn from the real distribution will also vary</p>
<p>But sometimes mode collapse is not all bad news. In style transfer using GAN, we are happy to convert one image to just a good one, rather than finding all variants. Indeed, the specialization in the partial mode collapse sometimes creates higher quality images.</p>
<h2 id="referrences"><a href="#referrences" class="headerlink" title="referrences:"></a>referrences:</h2><p>[1]. Section 2.4 of <a href="https://arxiv.org/abs/1611.02163" target="_blank" rel="noopener">Unrolled Generative Adversarial Networks</a><br>[2]. Section 3.2 of <a href="https://arxiv.org/abs/1606.03498" target="_blank" rel="noopener">Improved Techniques for Training GANs</a><br>[3]. <a href="https://arxiv.org/abs/1903.05628" target="_blank" rel="noopener">Mode Seeking Generative Adversarial Networks for Diverse Image Synthesis</a><br>[4]. <a href="https://arxiv.org/abs/1902.03984" target="_blank" rel="noopener">Improving Generalization and Stability of Generative Adversarial Networks</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/04/17/a-not-so-gentle-introduction-to-hyperparameters-tuning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jijeng Jia">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jijeng's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/04/17/a-not-so-gentle-introduction-to-hyperparameters-tuning/" itemprop="url">A Not-So-Gentle Introduction to Hyperparameters Tuning</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-04-17T21:14:15+08:00">
                2019-04-17
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Deep-learning/" itemprop="url" rel="index">
                    <span itemprop="name">Deep learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>Setting the hyper-parameters seems like a black art that requires years of experience to acquire. Currently, there are no simple and easy ways to set hyper-parameters- specifically, batch size, learning rate, momentum, and weight decay. A grid search or random search maybe sounds like a good idea. In this blog, I’d like to share you my idea from reading papers and my projects.</p>
<h2 id="Hyper-parameters"><a href="#Hyper-parameters" class="headerlink" title="Hyper-parameters"></a>Hyper-parameters</h2><h3 id="Batch-Size"><a href="#Batch-Size" class="headerlink" title="Batch Size"></a>Batch Size</h3><p>Learning rate is maybe the most important hyper-parameters, but we choose batch size firstly because large batch size needs a large learning rate in most circumstances.</p>
<p>A general principle is: use as a large batch size as possible to fit your CPU memory or/both GPU memory. There are several reasons:</p>
<ul>
<li>larger batch sizes permit the use of larger learning rates</li>
<li>A constant number of iterations favors larger batch sizes</li>
</ul>
<p>However, small batch sizes add regularization while large batch sizes add less. So utilize it while balancing the proper amount of regularization.</p>
<h3 id="Learning-Rate"><a href="#Learning-Rate" class="headerlink" title="Learning Rate"></a>Learning Rate</h3><p>We will introduce the idea from [Cyclical Learning Rates for Training Neural Networks][1]: Cyclical Learning Rates.</p>
<p>Instead of monotonically decreasing the learning rate, this method lets the learning rate cyclically vary between reasonable boundary values. The essence of this learning rate policy comes from the observation that increasing the learning rate might have a short term negative effect and yet achieve a longer term beneficial effect. This observation leads to the idea of letting the learning rate vary within a range of values rather than adopting a stepwise fixed or exponentially decreasing value. That is, one sets minimum and maximum boundaries and the learning rate cyclically varies between these bounds.<br><img src="http://ww1.sinaimg.cn/large/e9a223b5ly1g25wjj6cu9j20p80gcwh4.jpg" alt=""><br><small>From Cyclical Learning Rates for Training Neural Networks </small></p>
<p>An intuitive understanding of why CLR methods work comes from considering the loss function topology. Dauphin et al. argue that the difficulty in minimizing the loss arises from saddle points rather than poor local minima. Saddle points have small gradients that slow the learning process. However, increasing the learning rate allows for more rapid traversal of saddle point plateaus.</p>
<p>But the question is that how can we find the Minimum bound and Maximum bound. There is a simple way to estimate the reasonable minimum and maximum boundary values with one training run of the network for a few epochs. It is a “LR range test”; run your model for several epochs while letting the learning rate increase linearly between low and high LR values. For example, set both the step size and max iter to the same number of iterations. In this case, the learning rate will increase linearly from the minimum value to the maximum value during this short run. Next, plot the accuracy versus learning rate. Note the learning rate value when the accuracy starts to increase and when the accuracy slows, becomes ragged, or starts to fall. These two learning rates are good choices for bounds; that is, set base-lr to the first value and set max-lr to the latter value.</p>
<h3 id="Momentum"><a href="#Momentum" class="headerlink" title="Momentum"></a>Momentum</h3><p>Since learning rate is regarded as the most important hyper-parameter to tune then momentum is also important. Like learning rates, it is valuable to set momentum as large as possible without causing instabilities during training.</p>
<p>The large learning rate can deal with local minimum but works fail when it comes to saddle point where momentum comes to rescue.</p>
<p>The local minimum is like the following picture.<br><img src="http://ww1.sinaimg.cn/large/e9a223b5ly1g25x29cb1dj20y00iogs2.jpg" alt=""><br>In mathematics, a saddle point or minimax point is a point on the surface of the graph of a function <strong>where the slopes (derivatives) in orthogonal directions are all zero</strong> (a critical point), but which is not a local extremum of the function.</p>
<p>Your first step from the very top would likely take you down, but then you’d be on a flat rice terrace. The gradient would be zero, and you’d have nowhere to go. To remedy this, we employ momentum - the algorithm remembers its last step and adds some psroportion of it to the current step. This way, even if the algorithm is stuck in a flat region, or a small local minimum, it can get out and continue towards the true minimum.</p>
<p>In summary: when performing gradient descent, learning rate measures how much the current situation affects the next step, while momentum measures how much past steps affect the next step.</p>
<h3 id="Weights-Decay"><a href="#Weights-Decay" class="headerlink" title="Weights Decay"></a>Weights Decay</h3><p>When training neural networks, it is common to use “weight decay,” where after each update, the weights are multiplied by a factor slightly less than 1. This prevents the weights from growing too large and can be seen as gradient descent on a quadratic regularization term.</p>
<p>But why?</p>
<p>Large weights might correlate with certain patterns in the input data (x), this means that the model almost hard codes certain values. This then makes our training data fit well but our test data fit less well.</p>
<p>The idea of weight decay is simple: to prevent overfitting, every time we update a weight $w$ with the gradient $∇J$ in respect to $w$, we also subtract from it $λ∙w$. This gives the weights a tendency to decay towards zero, hence the name. L2 is a type of weights decay.<br>$$<br>J ( W ; X , y ) + \frac { 1 } { 2 } \lambda \cdot | W | ^ { 2 }<br>$$</p>
<p>But weights decay is not necessarily true for all gradient-base algorithms and was recently shown to not be the case for adaptive gradient algorithms, such as Adam.</p>
<p>In addition, weight decay is not the only regularization technique. In the past few years, some other approaches have been introduced such as Dropout, Bagging, Early Stop, and Parameter Sharing which work very well in NNs.</p>
<h2 id="Takeoff"><a href="#Takeoff" class="headerlink" title="Takeoff"></a>Takeoff</h2><ol>
<li>Batch Size</li>
</ol>
<p>Use as a large batch size as possible to fit your memory</p>
<ol start="2">
<li>Learning Rate</li>
</ol>
<p>Perform a learning rate range test to identify a “large” learning rate.</p>
<ol start="3">
<li>Momentum</li>
</ol>
<p>Test with short runs of momentum values 0.99, 0.97, 0.95, and 0.9 to get the best value for momentum.</p>
<p>If using the 1-cycle learning rate schedule, it is better to use a cyclical momentum (CM) that starts at this maximum momentum value and decreases with increasing learning rate to a value of 0.8 or 0.85.</p>
<ol start="4">
<li>Weights Decay</li>
</ol>
<p>A grid search to determine the proper magnitude but usually does not require more than one significant figure accuracy.<br>A more complex dataset requires less regularization so test smaller weight decay values, such as $10^{−4} $, $10^{−5} $, $10^{−6} $, 0.<br>A shallow architecture requires more regularization so test larger weight decay values, such as $10^{−2} $, $10^{−3} $, $10^{−4} $.</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>[1]. <a href="https://arxiv.org/abs/1506.01186" target="_blank" rel="noopener">Cyclical Learning Rates for Training Neural Networks</a><br>[2]. <a href="https://arxiv.org/abs/1803.09820" target="_blank" rel="noopener">A disciplined approach to neural network hyper-parameters</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/04/17/weights-initialization/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jijeng Jia">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jijeng's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/04/17/weights-initialization/" itemprop="url">Weights Initialization</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-04-17T21:04:44+08:00">
                2019-04-17
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Deep-learning/" itemprop="url" rel="index">
                    <span itemprop="name">Deep learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>Training a neural network consists of four steps: initialize weights and biases, forward propagation, compute the loss function and backward propagation. This blog mainly focuses on the first part: weights initialization.</p>
<p>After completing this tutorial, you will know:</p>
<ul>
<li><p>Four main types of weights initialization</p>
</li>
<li><p>How to choose between Xavier initialization and He initialization</p>
</li>
</ul>
<h2 id="Types-of-Weights-Initialization"><a href="#Types-of-Weights-Initialization" class="headerlink" title="Types of Weights Initialization"></a>Types of Weights Initialization</h2><ol>
<li>Initializing weights with zero</li>
</ol>
<p>When you set all weights in a neural network to zero, the derivative with respect to loss function is the same for every $ w$ in the same layer, thus all the weights have the same values in the subsequent iteration, which makes your model equivalent to a linear model.</p>
<ol start="2">
<li>Initializing weights randomly</li>
</ol>
<p>You can get weights like this (Python):</p>
<pre><code>w =np.random.randn(layer_size[l],layer_size[l-1])
</code></pre><p>The weighs follows standard normal distribution while it can potentially lead to two issues: vanishing gradients and exploding gradients.</p>
<p>If the weights start too small, then the signal shrinks as it passes through each layer until it’s too small to be useful.</p>
<p>If the weights start too large, then the signal grows as it passes through each layer until it’s too massive to be useful (big value in sigmoid function).</p>
<p>Thus there are two necessary conditions to consider:</p>
<ul>
<li><p>The values of each activation layer won’t be zero</p>
</li>
<li><p>The values of each activation layer won’t go into the area of saturation</p>
</li>
</ul>
<ol start="3">
<li>Xavier/Glorot Initialization</li>
</ol>
<p>For deep networks, we can use a heuristic to initialize the weights depending on the non-linear activation function. This applies to Xavier and He initialization.</p>
<p>Xavier/Glorot Initialization initializes the weights in your network by drawing them from a distribution with zero mean and a specific variance.<br>$$ { var } ( w _ { i } ) = \frac { 1 } { layer_{l-1}}$$</p>
<pre><code>w=np.random.randn(layer_size[l],layer_size[l-1])*np.sqrt(1/layer_size[l-1])
</code></pre><p>In practice, it works better for layers with sigmoid or tanh function. </p>
<ol start="4">
<li>He Initialization</li>
</ol>
<p>Using RELU or Leaky RELU is relatively robust to the vanishing/ exploding gradient issues compared with sigmoid function especially for networks that are not too deep. And it the case of Leaky RELU, it never has zero gradients. For RELU, we multiply the randomly generated values of $w$ by: </p>
<p>$$\sqrt { \frac { 2 } { layer _ { [ l - 1 ] } } }$$</p>
<pre><code>w=np.random.randn(layer_size[l],layer_size[l-1])*np.sqrt(2/layer_size[l-1])
</code></pre><p>Sometimes, we combine the idea of Xavier initialization and He initializaiton so the variance becomes the following:</p>
<p>$$\sqrt { \frac { 2 } { layer _ { [ l - 1 ] } + \operatorname { layer } _ { [ l ] } } }$$</p>
<pre><code>w=np.random.randn(layer_size[l],layer_size[l-1])*np.sqrt(2/(layer_size[l-1]+layer_size[l]))
</code></pre><p>The idea behind this is that we set the weights neither too much bigger than 1 nor too much less than 1 so the gradients do not vanish or explode too quickly.</p>
<h2 id="Takeoff"><a href="#Takeoff" class="headerlink" title="Takeoff"></a>Takeoff</h2><p>In summary, the main difference in machine learning is the following:</p>
<ul>
<li><p>He initialization works better for layers with ReLu(s) activation.</p>
</li>
<li><p>Xavier initialization works better for layers with sigmoid activation.</p>
</li>
</ul>
<h2 id="Referrence"><a href="#Referrence" class="headerlink" title="Referrence:"></a>Referrence:</h2><p><a href="https://arxiv.org/abs/1502.01852" target="_blank" rel="noopener">He initialization</a></p>
<p><a href="http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf" target="_blank" rel="noopener">Xavier initialization</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/04/09/cyclegan-stylegan/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jijeng Jia">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jijeng's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/04/09/cyclegan-stylegan/" itemprop="url">CycleGAN & StyleGAN</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-04-09T18:48:30+08:00">
                2019-04-09
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Deep-learning/" itemprop="url" rel="index">
                    <span itemprop="name">Deep learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>In the past few yeas, GANs have been used in lots of different applications such as generating synthetic data, style transfer, super-resolution and text2image generation. But we donn’t aim to give a overview of what GANs are made for. There are many great and detailed blogs for your understanding.</p>
<p>If you have fun about transfer learning, especially style transfer using GANs, this post might interest you!</p>
<h2 id="What-this-post-is-about"><a href="#What-this-post-is-about" class="headerlink" title="What this post is about"></a>What this post is about</h2><ul>
<li>Main ideas of CycleGAN</li>
<li>Keypoints in StyleGAN</li>
</ul>
<h2 id="A-Gentle-Introduction-of-GANs"><a href="#A-Gentle-Introduction-of-GANs" class="headerlink" title="A Gentle Introduction of GANs"></a>A Gentle Introduction of GANs</h2><p>We assume the reader has some prior experience with neural networks. In addition, if you are familiar with GANs you can <a href="#jump">skip</a> this section.</p>
<p>The famous minimax objective function can be formulated as following:<br>$$<br>\min _ { \theta _ { g } } \max _ { \theta _ { d } } \left[ \mathbb { E } _ { x \sim p _ { d a t a } } \log D _ { \theta _ { d } } ( x ) + \mathbb { E } _ { z \sim p ( z ) } \log \left( 1 - D _ { \theta _ { d } } \left( G _ { \theta _ { g } } ( z ) \right) \right) \right)<br>$$<br>But in practical, the loss function cannot work very well. So we have alternative objective function:</p>
<ol>
<li>Gradient ascent on discriminator</li>
</ol>
<p>$$<br>\max _ { \theta _ { d } } \left[ \mathbb { E } _ { x \sim p _ { d a t a } } \log D _ { \theta _ { d } } ( x ) + \mathbb { E } _ { z \sim p ( z ) } \log \left( 1 - D _ { \theta _ { d } } \left( G _ { \theta _ { g } } ( z ) \right) \right) \right]<br>$$</p>
<ol start="2">
<li>Gradient ascent on generator<br>$$<br>\max _ { \theta _ { g } } \mathbb { E } _ { z \sim p ( z ) } \log \left( D _ { \theta _ { d } } \left( G _ { \theta _ { g } } ( z ) \right) \right)<br>$$<br>The reasoning behind this can be found in original <a href="https://arxiv.org/abs/1406.2661" target="_blank" rel="noopener">paper</a>. Simplily speaking, we can get higher gradient signal for bad samples, which works much better in practice.<br><img src="http://ww1.sinaimg.cn/large/e9a223b5ly1g1w9einh3uj20sk0dkn1c.jpg" alt=""><center><span id="jump">From Stanford CS231 Lecture 13 — Generative Models </span></center>

</li>
</ol>
<h2 id="Main-ideas-of-CycleGAN"><a href="#Main-ideas-of-CycleGAN" class="headerlink" title="Main ideas of CycleGAN"></a>Main ideas of CycleGAN</h2><p>CycleGAN was introduced in 2017 out of Berkeley, <a href="https://arxiv.org/abs/1703.10593" target="_blank" rel="noopener">Unpaired Image-to-Image Translation Using Cycle-Coonsistent Adversarial Networks</a>.  This task is performed on unpaired data. Recent methods such as Pix2Pix depend on the availability of training examples where the samee data is availabel in both domains. However, CycleGAN is able to learning such pair information without one-to-one mapping between training data in source and target domains.<br><img src="http://ww1.sinaimg.cn/large/e9a223b5ly1g1wb7eq678j20vg0kidpn.jpg" alt=""></p>
<h2 id="Network-Architecture"><a href="#Network-Architecture" class="headerlink" title="Network Architecture"></a>Network Architecture</h2><p>We build three networks.</p>
<ul>
<li>A generator $F$ to convert image $y$ to image $ \hat{x}$</li>
<li>A generator $G$ to convert image $\hat{x}$ to image $ \hat{y}$</li>
<li>A discriminator $D$ to idenfify real image or generated picture<br>Simplified version of CycleGAN architecture can be showed in the following.<br><img src="http://ww1.sinaimg.cn/large/e9a223b5ly1g1wbibdfe1j20i60c3wf0.jpg" alt=""><br>The function $F$ and $G$ are generator network, which consists of encoder, transformer and decoder. Encoder is extracting the features from an image which is done by convolution networks. Each convolution layer leads to extraction of progressively higher level features. We would like to transform the feature emebdding of an image from domain $X$ to that of domain $Y$. So for this, authors have used 6 layers of resnet blocks. Resnet block is a neural network layer which consists of two convolutiona layers when a residue of input ia added to the output. This is done to ensure peoperties of input of previous layers are available for later layers as well.Resnet block can be summarized in following image<br><img src="http://ww1.sinaimg.cn/large/e9a223b5ly1g1wet610p7j20km0ceabs.jpg" alt=""><br>The decoder transfer embedding from $y$ back to original embedding $x$.</li>
</ul>
<h2 id="Loss-function"><a href="#Loss-function" class="headerlink" title="Loss function"></a>Loss function</h2><p>There are two types of losses in CycleGAN. Besides adversarial loss, we have another loss named reconstruction cost.<br>Adversarial loss is similary to original GAN.<br>$$<br>\operatorname { Loss } _ { a d v } \left( F , D _ { x } , Y \right) = \frac { 1 } { m } \sum _ { i = 1 } ^ { m } \left( 1 - D _ { x } \left( F \left( y _ { i } \right) \right) \right) ^ { 2 }<br>$$<br>$$<br>\operatorname { Loss } _ { a d v } \left( G , D _ { y } , X \right) = \frac { 1 } { m } \sum _ { i = 1 } ^ { m } \left( 1 - D _ { y } \left( G \left( x _ { i } \right) \right) \right) ^ { 2 }<br>$$<br>However, the adversarial loss alone is not sufficient to produce good looking images, which can not enfore that the input and output are recognizably the same. The cycle consistency loss addresses this issue. It relies on the expectation that if you convert an image to the other domain and back again, and then you should get back something similar to what you put in. It enforces that $F ( G ( x ) ) \approx x$ and $G ( F ( y ) ) \approx y$.<br>$$<br>\operatorname { Loss } _ { c y c } ( G , F , X , Y ) = \frac { 1 } { m } \sum _ { i = 1 } ^ { m } \left[ F \left( G \left( x _ { i } \right) \right) - x _ { i } \right] + \left[ G \left( F \left( y _ { i } \right) \right) - y _ { i } \right]<br>$$<br>We can get the full objective function by putting these two together.<br>$$<br>\mathcal { L } \left( G , F , D _ { x} , D _ { y } \right) = \mathcal { L } _ { \text { GAN } } \left( G , D _ { y } , X , Y \right) + \mathcal { L } _ { \text { GAN } } \left( F , D _ { x } , Y , X \right) + \lambda \mathcal { L } _ { \text { cyc } } ( G , F )<br>$$</p>
<h2 id="Keypoints-of-StyleGAN"><a href="#Keypoints-of-StyleGAN" class="headerlink" title="Keypoints of StyleGAN"></a>Keypoints of StyleGAN</h2><p>The StyleGAN offeras an upgrade version of ProGAN’s image generator, with a focus on the generator. </p>
<p>ProGAN generates high-quality images but, in most models, its ability to control specific features of the generated image is very limited. In other word, the features are entangled and therefore attempting to tweak the input, even a bit, usually affects multiple features at the same time. A good illustrations would be following pictures.<br><img src="http://ww1.sinaimg.cn/large/e9a223b5ly1g1wfimzvg2g20ps0en4qq.gif" alt=""><br>Compared with first version (ProGAN), the new generator includes several additions to ProGAN’s generators.</p>
<h2 id="Mapping-Network"><a href="#Mapping-Network" class="headerlink" title="Mapping Network"></a>Mapping Network</h2><p>The mapping network’s goal in to encode the input vector into an intermediate vector whose different elements control different visual features, which consists of 8 fully connected layers and its output $w$ is of the same size as the input.<br><img src="http://ww1.sinaimg.cn/large/e9a223b5ly1g1wfvtdeh3j20nn0b9gma.jpg" alt=""></p>
<h2 id="Style-Modules-AdaIN"><a href="#Style-Modules-AdaIN" class="headerlink" title="Style Modules (AdaIN)"></a>Style Modules (AdaIN)</h2><p>The AdaIn (Adaptive Instance Normalization) module transfers the encoded information $w$, created by the mapping network, into the generated image.<br><img src="http://ww1.sinaimg.cn/large/e9a223b5ly1g1wg3c5awfj20ca09q3z6.jpg" alt=""></p>
<h2 id="Removing-traditional-input"><a href="#Removing-traditional-input" class="headerlink" title="Removing traditional input"></a>Removing traditional input</h2><p>Since the encoded information $w$ from mapping network was used into generator image, the traditional random input can be omitted and replaced by constant values.</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/04/09/python-for-beginners/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jijeng Jia">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jijeng's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/04/09/python-for-beginners/" itemprop="url">Python for Beginners</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-04-09T18:47:46+08:00">
                2019-04-09
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Python/" itemprop="url" rel="index">
                    <span itemprop="name">Python</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="module"><a href="#module" class="headerlink" title="module"></a>module</h3><p>python 文件可以当做主文件进行运行或者当做函数的集合进行调用。如果是前者一般是需要包含”__name__ ==”__main__”。对于后者就是在其他的python文件中进行调用。</p>
<pre><code>import my_module # python文件
from my_module import my_object 
</code></pre><h3 id="packages"><a href="#packages" class="headerlink" title="packages"></a>packages</h3><pre><code>from packageroot.packagefolder.mod import my_object
</code></pre><p>Note: Ensure each directory within your package import contains a file __init__.py</p>
<h3 id="pythonpath"><a href="#pythonpath" class="headerlink" title="pythonpath"></a>pythonpath</h3><p>python2 和python3 使用不同的解释器，导致在一些函数命名和计算上有一些差别，最好在文件的开头标明使用的解释器。</p>
<h3 id="while-for"><a href="#while-for" class="headerlink" title="while for"></a>while for</h3><p>while : provide a condition and run the loop until the condition is not met.</p>
<p>loop for a number of specific times; loop over items or characters of a string.</p>
<p>examples:</p>
<pre><code>[Variable] AggregateFunction([Value] for [item] in [collection])
x =[1, 2,3, 4, 5]
y =[ 2*a for a in x if a%2 ==0]
y &gt;&gt; [4, 8]
</code></pre><p>或者可以使用这样更加简洁的语句：</p>
<pre><code>lambda arguments : expressio
fun1 = lambda a,b,c : a+b+c
print(fun1(5,6,2))
</code></pre><p>来个比较复杂的例子</p>
<pre><code>nums =[1,2,3,4,5]
letters =[&apos;a&apos;, &apos;b&apos;, &apos;c&apos;,&apos;d&apos;,&apos;e&apos;]
nums_letters =[[n, l] for n in nums for l in letters ]
nums_letters
</code></pre><h3 id="break-continue-pass"><a href="#break-continue-pass" class="headerlink" title="break continue pass"></a>break continue pass</h3><p>The break, continue, and pass statements in Python will allow you to use for loops and while loops more effectively in your code.</p>
<pre><code>number = 0
for number in range(10):
   number = number + 1

   if number == 5:
      pass    # pass here

   print(&apos;Number is &apos; + str(number))

print(&apos;Out of loop&apos;)
</code></pre><p>The pass statement occurring after the if conditional statement is telling the program to continue to run the loop and ignore the fact that the variable number evaluates as equivalent to 5 during one of its iterations.</p>
<p>可以用用作新的 if的测试。<br>The pass statement can create minimal classes, or act as a placeholder when working on new code and thinking on an algorithmic level before hammering out details.</p>
<h3 id="yield-return"><a href="#yield-return" class="headerlink" title="yield return"></a>yield return</h3><p>经常被用来作为生成器。</p>
<blockquote>
<p>when you call a normal function with a return statement the function is terminated whenever it encounters a return statement. In a function with a yield statement the state of the function is ‘saved’ from the last call and can be picked up the next time you call a generator function.</p>
</blockquote>
<p>for examples</p>
<pre><code>gen_exp =(x **2 for x in range(10) if x %2 ==0)
for x in gen_exp:
  print(x)

def my_gen():
  for x in range(5):
    yield x

gen1 =my_gen()
next(gen1)

def my_generator1():
  yield 1
  yield 2
  yield 3

my_gen =my_generator1()
next(my_gen)
</code></pre><h3 id="recursion"><a href="#recursion" class="headerlink" title="recursion"></a>recursion</h3><p>A function calling itself is known as recursion.</p>
<h3 id="list-tuples-dictionary"><a href="#list-tuples-dictionary" class="headerlink" title="list, tuples, dictionary"></a>list, tuples, dictionary</h3><p>在python 中是使用频繁的data structure，这个是属于 collection 类别，里面放的是element<br>to add/update/ delete an item of a colletion ( list)</p>
<pre><code>my_list.append(&apos;C&apos;) #adds at the end
my_list[1] = &apos;D&apos; #update
my_list.pop(1) # removes

mylist.pop() # 默认就是类似 栈的结构，就是pop 出来最后一个
mylist.pop(0) # 当然也可以根据index 指定特定的 pop(delete) 的element
or 
del mylist[1:2] # 通过指定 index range 然后进行del
mylist.sort() # 支持 sorting 然后是从小到大, 这个sort是一种操作，inplace 的操作
</code></pre><p>tuples<br>tuples store a sequence of objects, the object can be of any typle. Tuples are faster than lists.</p>
<p>dictionary:<br>It stores key/value pair objects.</p>
<pre><code>my_dict =dict()
my_dict[&apos;key&apos;] =&apos;value&apos;
or 
my_dict ={&apos;key&apos;: &apos;value&apos;, ...}

for key in my_dict:
# do something
if &apos;some key&apos; in my_dict:
# do something
</code></pre><h3 id="Iterators"><a href="#Iterators" class="headerlink" title="Iterators"></a>Iterators</h3><pre><code>class yrange:
  def __init__(self, n):
    self.i =0
    self.n =n
  # 这个表明是一个 iterator，make an object iterable
  def __iter__(self):
    return self
  # 这个next 函数就被当做是 class的属性，可以被外部调用的，
  def next(self):
    if self.i &lt; self.n:
      i =self.i
      self.i +=1
      return i
    else:
      raise StopIteration()
</code></pre><h3 id="shallow-vs-deep-copy"><a href="#shallow-vs-deep-copy" class="headerlink" title="shallow vs deep copy"></a>shallow vs deep copy</h3><p>python3 中：<br>对于简单的数据类型，像int ，string，这种 copy() 和copy.deepcopy() 这两者都是相同的，copy 都是一种映射，都是相当于”值“ 上的引用；</p>
<pre><code>aa =2
bb =aa
print(id(aa), id(bb)) # 相同
bb =3
print(id(aa), id(bb)) # 不同，因为把3 这个值重新复制给了变量bb
</code></pre><p>对于复杂的数据类型，使用deepcopy() 的时候，本来就是会重新拷贝一份到内存中。在python3 中copy() 和deepcopy() 这个是没有什么区别的。</p>
<pre><code>list1 =[&apos;a&apos;, &apos;b&apos;]
list2 =list1 # 这个是引用，所以和list1 是相同的
list3 =copy.copy(list1)  # 这个id 和list1 不同
list4 =copy.deepcopy(list1)# 这个id 和list1 不同 
print(id(list1), id(list2), id(list3), id(list4))
</code></pre><h3 id="object-oriented-design"><a href="#object-oriented-design" class="headerlink" title="object oriented design"></a>object oriented design</h3><pre><code>class ParentClass:
 def my_function(self):
   print &apos;I am here&apos;

class SubClass1(ParentClass): 
class SubClass2(ParentClass): 
</code></pre><p>对于多继承的支持 （接口）</p>
<pre><code>class A(B,C):  #A implments B and C
</code></pre><p>如果想要call parent class function then you can dp:</p>
<pre><code>super(A, self).funcion_name()
</code></pre><h3 id="garbage-collection"><a href="#garbage-collection" class="headerlink" title="garbage collection"></a>garbage collection</h3><p>all the objects  in python are stored in a heap space. Python has an in-built garbage collection mechanism.</p>
<blockquote>
<p>In computer science, a heap is a specialized tree-based data structure which is essentially an almost complete[1] tree that satisfies the heap property: if P is a parent node of C, then the key(the value) of P is either greater than or equal to (in a max heap) or less than or equal to (in a min heap) the key of C.</p>
</blockquote>
<h3 id="try…catch"><a href="#try…catch" class="headerlink" title="try…catch"></a>try…catch</h3><pre><code># raise exceptions
try:
  raise TyeError
except:
  print(&apos;exception&apos;)

# catching exceptions
try:
  do_something()
except:
  print(&apos;exception&apos;)
# try/ catch /finally
try:
  do_something()
except TypeError:
  print(&apos;exception&apos;)
finally:
  close_connections()
</code></pre>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/04/06/The-evaluation-of-sentence-similarity/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jijeng Jia">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jijeng's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/04/06/The-evaluation-of-sentence-similarity/" itemprop="url">The Evaluation of Sentence Similarity</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-04-06T15:11:01+08:00">
                2019-04-06
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index">
                    <span itemprop="name">NLP</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>I am trying to write my first english blog based on two reasons: First, the data set used in this blog is english; Second, I’d like to expand my reach and attract more audiences, although I should admit that nobody cares.</p>
<h2 id="Data"><a href="#Data" class="headerlink" title="Data"></a>Data</h2><p>Initially I want to use chinese corpus, but I cannot find a proper one. The data should sound like this one:</p>
<blockquote>
<p>word1    word2    similarity score<br>阿拉伯人    阿拉伯    7.2<br>畜产    农业    5.6<br>垂涎    崇敬    3.4<br>次序    秩序    4.7<br>定心丸    药品    4.3<br>房租    价格    5.2<br>翡翠    宝石    6.7<br>高科技    技术    7.5<br>购入    购买    8.5<br>观音    菩萨    8.2<br>归并    合并    7.7</p>
</blockquote>
<!-- come from https://biendata.com/ccf_tcci2018/datasets/tcci_tag/5 -->
<p>not like this:</p>
<blockquote>
<p>为何我无法申请开通花呗信用卡收款    支付宝开通信用卡花呗收款不符合条件怎么回事    1<br>花呗分期付款会影响使用吗    花呗分期有什么影响吗    0<br>为什么我花呗没有临时额度    花呗没有临时额度怎么可以负    0<br>能不能开花呗老兄    花呗逾期了还能开通    0<br>我的怎么开通花呗收钱    这个花呗是个什么啥？我没开通 我怎么有账单    0<br>蚂蚁借呗可以停掉么    蚂蚁借呗为什么给我关掉了    0<br>我想把花呗功能关了    我去饭店吃饭，能用花呗支付吗    0<br>为什么我借呗开通了又关闭了    为什么借呗存在风险    0<br>支付宝被冻了花呗要怎么还    支付功能冻结了，花呗还不了怎么办    1</p>
</blockquote>
<p>If you can find the dataset where ‘similarity score’ is double, please donot hesitate to <a href="mailto:jiajizhengbuaa@gmail.com" target="_blank" rel="noopener">email me.</a></p>
<p>So, the choice has to be enlgish corpus. The dataset used in this experiment are <a href="http://ixa2.si.ehu.es/stswiki/index.php/STSbenchmark" target="_blank" rel="noopener">STSbenchmark</a> and SICK data. The SICK data contains 10,000 sentence paris labeled with semantic relatedness and entailment relation.<br><img src="http://ww1.sinaimg.cn/large/e9a223b5ly1g1sqxx3vl0j219e07yaam.jpg" alt=""></p>
<h2 id="Similarity-Methods"><a href="#Similarity-Methods" class="headerlink" title="Similarity Methods"></a>Similarity Methods</h2><h3 id="Baseline"><a href="#Baseline" class="headerlink" title="Baseline"></a>Baseline</h3><p>As the baseline, we just take the embedding of the words in sentence, and compute the average, weighted by frequency of each word.</p>
<pre><code>def run_avg_benchmark(sentences1, sentences2, model=None, use_stoplist=False, doc_freqs=None):
    if doc_freqs is not None:
        N = doc_freqs[&quot;NUM_DOCS&quot;]

    sims = []
    for (sent1, sent2) in zip(sentences1, sentences2):

        tokens1 = sent1.tokens_without_stop if use_stoplist else sent1.tokens
        tokens2 = sent2.tokens_without_stop if use_stoplist else sent2.tokens

        tokens1 = [token for token in tokens1 if token in model]
        tokens2 = [token for token in tokens2 if token in model]

        if len(tokens1) == 0 or len(tokens2) == 0:
            sims.append(0)
            continue

        tokfreqs1 = Counter(tokens1)
        tokfreqs2 = Counter(tokens2)

        weights1 = [tokfreqs1[token] * math.log(N / (doc_freqs.get(token, 0) + 1))
                    for token in tokfreqs1] if doc_freqs else None
        weights2 = [tokfreqs2[token] * math.log(N / (doc_freqs.get(token, 0) + 1))
                    for token in tokfreqs2] if doc_freqs else None

        embedding1 = np.average([model[token] for token in tokfreqs1], axis=0, weights=weights1).reshape(1, -1)
        embedding2 = np.average([model[token] for token in tokfreqs2], axis=0, weights=weights2).reshape(1, -1)

        sim = cosine_similarity(embedding1, embedding2)[0][0]
        sims.append(sim)

    return sims
</code></pre><h3 id="Smooth-Inverse-Frequency"><a href="#Smooth-Inverse-Frequency" class="headerlink" title="Smooth Inverse Frequency"></a>Smooth Inverse Frequency</h3><p>The baseline, like we did before, is very simple and crude of computing sentence embedding. Word frequency cannot reliably reflect its importance to sentence, semantically speaking. Smooth Inverse Frequency (SIF) tries to solve thhis problem.</p>
<p>SIF is very similar to the weighted average we used before, with the difference that it’s weighted by this formular.<br>$$<br>\operatorname { SIF } ( w ) = \frac { a } { ( a + p ( w ) )}<br>$$<br>where $a$ is a hyper-parameter (set to 0.001 by default) and $ p(w)$ is the estimated word frequency in the corpus. </p>
<p>Next, we need to perform common component removal: subtract from the sentence embeddingh obtained above the first principal component of the matrix. This corrects for the influence of high-frequency words that have syntactic or dicourse function, such as ‘but’, ‘and’, etc. You can find more information from <a href="https://openreview.net/pdf?id=SyK00v5xx" target="_blank" rel="noopener">this paper</a>.</p>
<pre><code>def remove_first_principal_component(X):
    svd = TruncatedSVD(n_components=1, n_iter=7, random_state=0)
    svd.fit(X)
    pc = svd.components_
    XX = X - X.dot(pc.transpose()) * pc
    return XX


def run_sif_benchmark(sentences1, sentences2, model, freqs={}, use_stoplist=False, a=0.001):
    total_freq = sum(freqs.values())

    embeddings = []

    # SIF requires us to first collect all sentence embeddings and then perform
    # common component analysis.
    for (sent1, sent2) in zip(sentences1, sentences2):
        tokens1 = sent1.tokens_without_stop if use_stoplist else sent1.tokens
        tokens2 = sent2.tokens_without_stop if use_stoplist else sent2.tokens

        tokens1 = [token for token in tokens1 if token in model]
        tokens2 = [token for token in tokens2 if token in model]

        weights1 = [a / (a + freqs.get(token, 0) / total_freq) for token in tokens1]
        weights2 = [a / (a + freqs.get(token, 0) / total_freq) for token in tokens2]

        embedding1 = np.average([model[token] for token in tokens1], axis=0, weights=weights1)
        embedding2 = np.average([model[token] for token in tokens2], axis=0, weights=weights2)

        embeddings.append(embedding1)
        embeddings.append(embedding2)

    embeddings = remove_first_principal_component(np.array(embeddings))
    sims = [cosine_similarity(embeddings[idx * 2].reshape(1, -1),
                              embeddings[idx * 2 + 1].reshape(1, -1))[0][0]
            for idx in range(int(len(embeddings) / 2))]

    return sims
</code></pre><h3 id="Google-Sentence-Encoder"><a href="#Google-Sentence-Encoder" class="headerlink" title="Google Sentence Encoder"></a>Google Sentence Encoder</h3><p><a href="https://github.com/facebookresearch/InferSent" target="_blank" rel="noopener">InferSent</a> is a pre-trained encoder that produces sentence embedding, which opensourced by Facebook. <a href="https://tfhub.dev/google/universal-sentence-encoder/1" target="_blank" rel="noopener">The Google Sentence Encoder</a> is Google’s answer to Facebook’s InferSent. In contrast to InferSent, the Google Sentence Encoder was trained on a combination of unsupervised data and supervised data (SNLI corpus), which tends to give better results.</p>
<p>The codes can be used in <a href="https://colab.research.google.com/notebooks/welcome.ipynb#recent=true" target="_blank" rel="noopener">Google Jupyter Notebook</a></p>
<pre><code>import tensorflow_hub as hub

tf.logging.set_verbosity(tf.logging.ERROR)
embed = hub.Module(&quot;https://tfhub.dev/google/universal-sentence-encoder/1&quot;)

def run_gse_benchmark(sentences1, sentences2):
    sts_input1 = tf.placeholder(tf.string, shape=(None))
    sts_input2 = tf.placeholder(tf.string, shape=(None))

    sts_encode1 = tf.nn.l2_normalize(embed(sts_input1))
    sts_encode2 = tf.nn.l2_normalize(embed(sts_input2))

    sim_scores = tf.reduce_sum(tf.multiply(sts_encode1, sts_encode2), axis=1)

    with tf.Session() as session:
        session.run(tf.global_variables_initializer())
        session.run(tf.tables_initializer())

        [gse_sims] = session.run(
            [sim_scores],
            feed_dict={
                sts_input1: [sent1.raw for sent1 in sentences1],
                sts_input2: [sent2.raw for sent2 in sentences2]
            })
    return gse_sims
</code></pre><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><pre><code>def run_experiment(df, benchmarks):
    sentences1 = [Sentence(s) for s in df[&apos;sent_1&apos;]]
    sentences2 = [Sentence(s) for s in df[&apos;sent_2&apos;]]

    pearson_cors, spearman_cors = [], []
    for label, method in benchmarks:
        sims = method(sentences1, sentences2)
        pearson_correlation = scipy.stats.pearsonr(sims, df[&apos;sim&apos;])[0]
        print(label, pearson_correlation)
        pearson_cors.append(pearson_correlation)
        spearman_correlation = scipy.stats.spearmanr(sims, df[&apos;sim&apos;])[0]
        spearman_cors.append(spearman_correlation)

    return pearson_cors, spearman_cors
</code></pre><p>Helper function:</p>
<pre><code>import functools as ft

benchmarks = [
    (&quot;AVG-GLOVE&quot;, ft.partial(run_avg_benchmark, model=glove, use_stoplist=False)),
    (&quot;AVG-GLOVE-STOP&quot;, ft.partial(run_avg_benchmark, model=glove, use_stoplist=True)),
    (&quot;AVG-GLOVE-TFIDF&quot;, ft.partial(run_avg_benchmark, model=glove, use_stoplist=False, doc_freqs=doc_frequencies)),
    (&quot;AVG-GLOVE-TFIDF-STOP&quot;, ft.partial(run_avg_benchmark, model=glove, use_stoplist=True, doc_freqs=doc_frequencies)),
    (&quot;SIF-W2V&quot;, ft.partial(run_sif_benchmark, freqs=frequencies, model=word2vec, use_stoplist=False)),
    (&quot;SIF-GLOVE&quot;, ft.partial(run_sif_benchmark, freqs=frequencies, model=glove, use_stoplist=False)),

]
</code></pre><h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h2><pre><code>import matplotlib.pyplot as plt
plt.rcParams[&apos;figure.figsize&apos;] = (20,13)
spearman[[&apos;AVG-GLOVE&apos;, &apos;AVG-GLOVE-STOP&apos;,&apos;AVG-GLOVE-TFIDF&apos;, &apos;AVG-GLOVE-TFIDF-STOP&apos;,&apos;GSE&apos;]].plot(kind=&quot;bar&quot;).legend(loc=&quot;lower left&quot;)
</code></pre><p><strong>Take Off</strong></p>
<ul>
<li>Smooth Inverse Frequency methods are better than baseline, no matter with word2vec or Glove embeddings.</li>
<li>Glove Sentence Encoder has the similar performance as Smooth Inverse Frequency.</li>
<li>Using tf-idf weights does not help and using a stoplist looks like a reasonable choice.</li>
</ul>
<p>Pearson Correlation<br><img src="http://ww1.sinaimg.cn/large/e9a223b5ly1g1swpqj9d0j20ps0h5t8z.jpg" alt=""><br>Spearman Correlation<br><img src="http://ww1.sinaimg.cn/large/e9a223b5ly1g1swpqj9d0j20ps0h5t8z.jpg" alt=""></p>
<p>Full codes can be found in <a href="https://github.com/jijeng/sentence-similarity" target="_blank" rel="noopener">here</a>.</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/26/深度网络中的碎碎念/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jijeng Jia">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jijeng's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/26/深度网络中的碎碎念/" itemprop="url">深度网络中的碎碎念</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-03-26T10:28:14+08:00">
                2019-03-26
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Deep-learning/" itemprop="url" rel="index">
                    <span itemprop="name">Deep learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>主要介绍了常见的网络中权重的初始化，激活函数和优化器。</p>
<h2 id="Weights-Initialization"><a href="#Weights-Initialization" class="headerlink" title="Weights Initialization"></a>Weights Initialization</h2><p>weights 的初始化在网络的训练起到重要的作用，初始化的好坏能够直接影响到网络是否可以正常收敛。<br>这里的初始化都是指的是weights初始化。bias 这个变量就是在企图去描述真实的分布，通过引入随机性来表示这个是具有 推广性的。</p>
<blockquote>
<p>Here’s another trick — before squishing our scalar value (called an activation) into the sigmoid function, we can add a little number called the bias, b. There will be two biases, one for each user and one for each movie.</p>
</blockquote>
<h3 id="Random-Initialization"><a href="#Random-Initialization" class="headerlink" title="Random Initialization"></a>Random Initialization</h3><p>最基础的即使 bias 使用 zero initialization ，然后 weights 使用 random initialzation。这种方法的缺陷在于梯度消失。就是你的weights 如果很大或者很小的时候，再加上如果使用了sigmoid 那么很容易出现上述的现象。</p>
<blockquote>
<p>a) If weights are initialized with very high values the term np.dot(W,X)+bbecomes significantly higher and if an activation function like sigmoid() is applied, the function maps its value near to 1 where slope of gradient changes slowly and learning takes a lot of time.<br>b) If weights are initialized with low values it gets mapped to 0, where the case is same as above.</p>
</blockquote>
<h3 id="He-Initialization"><a href="#He-Initialization" class="headerlink" title="He Initialization"></a>He Initialization</h3><p>$$W^{[l]} =np.random.randn(size_l, size_l -1)* np.sqrt(2/size_l -1) $$<br>这个是使用 relu 或者说 leaky relu 配合使用的。</p>
<p>所以上述的初始化从数学的角度去理解： 从(0,1) 标准正太分布 转换成了 (0, np.sqrt(2/ size_l -1)) 这样的分布，就是你网络结构是越宽，那么这个方差就是越小的，最后的结果是越集中的，就越集中的 均值 u 左右。从图像的角度看，方差越大，图像越矮胖；方差越小，图像越瘦高。</p>
<h3 id="Xavier-initialization"><a href="#Xavier-initialization" class="headerlink" title="Xavier initialization"></a>Xavier initialization</h3><p>这个是使用tanh() 作为 activation function的。<br>$$W^{[l]} =np.random.randn(size_l, size_l -1)* np.sqrt( 1/size_l -1) $$<br>总的思想原则：<br>They set the weights neither too much bigger that 1, nor too much less than 1.<br>就是本来就是在 (0,1) 标准正太分布出来，然后进行了进一步的约束条件。</p>
<h2 id="Activation-function"><a href="#Activation-function" class="headerlink" title="Activation function"></a>Activation function</h2><p>总的说可以分为线性和非线性的激活函数， activation function 的作用 就是对于网络的输出 说yes or no. It maps the resulting values in between 0 to 1 or -1 to 1 etc.</p>
<h3 id="Sigmoid-function-Logistic-Activation"><a href="#Sigmoid-function-Logistic-Activation" class="headerlink" title="Sigmoid function (Logistic Activation)"></a>Sigmoid function (Logistic Activation)</h3><p>the only reason why we use sigmoid is because it exists between 0 to 1. 这个非常有利于 predict probability. 因为自然映射到 0 是不存在 然后1 是存在。而当多分类的时候，使用softmax。</p>
<h3 id="Tanh-function"><a href="#Tanh-function" class="headerlink" title="Tanh function"></a>Tanh function</h3><blockquote>
<p>The advantage is that the negative inputs will be mapped strongly negative and the zero inputs will be mapped near zero in the tanh graph.</p>
</blockquote>
<h3 id="Relu-Rectified-Linear-Unit-Activation"><a href="#Relu-Rectified-Linear-Unit-Activation" class="headerlink" title="Relu (Rectified Linear Unit) Activation"></a>Relu (Rectified Linear Unit) Activation</h3><p>本质上是分段函数。 range: [0, infinity]. The function and its derivative both are monotonic.</p>
<h3 id="Leaky-Relu"><a href="#Leaky-Relu" class="headerlink" title="Leaky Relu"></a>Leaky Relu</h3><p>每当一个伟大的东西产出，总会伴随着比较明显的错误，然后紧跟着就是有一个  rectified(improved) 这种版本。这个相比之前就是修正了 当输入是负数的时候，怎么办的问题。</p>
<h3 id="Softmax"><a href="#Softmax" class="headerlink" title="Softmax"></a>Softmax</h3><p>这个需要从数学的角度去理解，数学公式就是下面。有一个真实的例子： 如果一张”4” 的图片，输出一个网络，最后是softmax 激活函数，那么最后得到的 “4” 的概率是最大的，但是也是有其他的可能性存在的，这个就是softmax 的效果。 最主要的功能是 convert number into probabilities. 这种效果，不像sigmoid 那样有很明确的数学公式。</p>
<p>$$\sigma ( z ) _ { j } = \frac { e ^ { z _ { j } } } { \sum _ { k = 1 } ^ { K } e ^ { z _ { k } } }$$</p>
<blockquote>
<p>Softmax function, a wonderfulactivation function that turns numbers aka logits into probabilities that sum to one.<br>Remember the takeaway is: the essential goal of softmax is to turn numbers into probabilities.</p>
</blockquote>
<h2 id="Optimizer"><a href="#Optimizer" class="headerlink" title="Optimizer"></a>Optimizer</h2><h3 id="Gradient-Descent"><a href="#Gradient-Descent" class="headerlink" title="Gradient Descent"></a>Gradient Descent</h3><p>最基本的应该是 gradient descent，这个算是开山鼻祖。给出了每次梯度变化的方向</p>
<h3 id="Momentum"><a href="#Momentum" class="headerlink" title="Momentum"></a>Momentum</h3><p>个人更喜欢将其翻译成动量，这个gradient 考虑之前的gradient 的方向和大小，有点像动量的的含义。不仅考虑了current step, 而且accumulates gradient of the past steps.</p>
<blockquote>
<p> A very popular technique that is used along with SGD is called Momentum. Instead of using only the gradient of the current step to guide the search, momentum also accumulates the gradient of the past steps to determine the direction to go. </p>
</blockquote>
<p>好处在于:</p>
<blockquote>
<p>most recent is weighted than the less recent ones<br>the weightage of the most recent previous gradients is more than the less recent ones.<br>for example:</p>
</blockquote>
<p><img src="http://ww1.sinaimg.cn/large/e9a223b5ly1g1fyli4wtkj20nk04ldfr.jpg" alt=""></p>
<h3 id="RMSprop"><a href="#RMSprop" class="headerlink" title="RMSprop"></a>RMSprop</h3><p>RMSprop, or Root Mean Square Propogation has an interesting history. RMSProp also tries to dampen the oscillations, but in a different way than momentum. </p>
<p><img src="http://ww1.sinaimg.cn/large/e9a223b5ly1g1fz79596rj20fu0boaad.jpg" alt=""></p>
<h3 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h3><p>这种套路总是一样的，总是有一个中和者，关键是如何整合到一起。</p>
<blockquote>
<p>Adam or Adaptive Moment Optimization  algorithms combines the heuristics of both Momentum and RMSProp.</p>
</blockquote>
<p><img src="http://ww1.sinaimg.cn/large/e9a223b5ly1g1fzc05tnrj20cy08u74e.jpg" alt=""></p>
<p>The hyperparameter beta1 is generally kept around 0.9 while beta_2 is kept at 0.99. Epsilon is chosen to be 1e-10 generally.</p>
<p>对于公式的解释，Eq 1 and Eq 2是come from RMSprop,  Eq 3 comes from Adam。最后一步是 update 操作，条理清楚，没有问题。(公式中的v 表示导数，g 表示导数e 不太清楚)<br>Here, we compute the exponential average of the gradient as well as the squares of the gradient for each parameters (Eq 1, and Eq 2). To decide our learning step, we multiply our learning rate by average of the gradient (as was the case with momentum) and divide it by the root mean square of the exponential average of square of gradients (as was the case with momentum) in equation 3. Then, we add the update.)</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/26/siamese-network/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jijeng Jia">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jijeng's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/26/siamese-network/" itemprop="url">siamese network</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-03-26T10:26:03+08:00">
                2019-03-26
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Deep-learning/" itemprop="url" rel="index">
                    <span itemprop="name">Deep learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>主要是介绍自己论文中的网络结构：siamese network。</p>
<p>但凡优化，无非两条路：在好的基础上更快，在快的基础上效果更好。 Siamese network训练速度快，所以只是需要其在训练效果上更好。</p>
<p>先来分析一下为什么训练速度快。那么不得不分析该网络结构。整个网络的输入是 (img1, img2, y) 这样的三元组，img 表示图片，y表示label。图片可以是同一类别的图片，也可以是不同类别的，y表示两张图片之间的相似程度，y的取值 (0,1)，0表示相似（同一类别），1 表示不相似（不同类别）。并且y 是double 类型，属于0-1 区间任意的数字。网路结构最后的输出是 0-1 区间的任意数字。通常是以0.5 作为分界线，如果小于0.5 那么认为两种图片是属于同一类别，或者说更相似；反之也成立。<br>重要的一点是中间的weights 实现了权值共享，这样可以加快网络的训练速度。<br><img src="http://ww1.sinaimg.cn/large/e9a223b5ly1g1fwyu5fnsj20z90cq75v.jpg" alt=""></p>
<h3 id="loss-function"><a href="#loss-function" class="headerlink" title="loss function"></a>loss function</h3><p>这个是属于经典的 contrastive loss function。当y 接近于0的时候，右半部分消失，这个是表示两张图片很是相似，然后就不断使得 欧氏距离减少；当y 接近于1的时候，左半部分消失，这个时候两张图片很不相似，然后右边就是 hinge loss。参数m 作为一种margin 是是可以调节，我的实验中 m 取1.<br>总的思想：就是使得相近的图像距离相近，不想近的图像距离变远。</p>
<p>$L ( W , ( Y , X _ { 1 } , X _ { 2 } ) ) = ( 1 - Y ) \frac { 1 } { 2 } ( D _ { W } ) ^ { 2 } + ( Y ) \frac { 1 } { 2 } { \max ( 0 , m - D _ { W } ) } ^ { 2 }$</p>
<h3 id="Spectral-Normalization"><a href="#Spectral-Normalization" class="headerlink" title="Spectral Normalization"></a>Spectral Normalization</h3><p>图像输入到网络之前使用正则化，然后输入到激活函数之前也是使用正则化，所以这种效果也是扩展到 weights，直接对 weights 进行正则化使其符合  Lipschitz 约束，避免使用大的gradients。在GAN 网络中的 discriminator 或者 generator 都发现了其可以稳定训练的过程。在实验中，我们扩大了了这种使用范围，把其应用到所有的网络的layer上。</p>
<h3 id="self-attention-mechanism"><a href="#self-attention-mechanism" class="headerlink" title="self-attention mechanism"></a>self-attention mechanism</h3><p>Attention 机制自从 <a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank" rel="noopener">“Attention Is All You Need”</a> 开始火爆，并且实验的效果也是很好的，然后在图像领域也开始尝试使用 attention 机制来解决长依赖的问题。应用到图像领域主要是 explore spatial locality information, 说白了就是细节的信息。</p>
<blockquote>
<p>If we look at the DCGAN model, we see that regular GANs are heavily based on convolution operations, which use a local receptive field (convolutional kernel) to learn representations. Simple features like edges and corners are learned in the first few layers. Also, ConvNets are able to use these simple representations to learn more complex ones. However, long-range dependency might be hard to learn. Long-range dependency (long-term dependency) is from RNN, which we can say anything larger than trigram as a long term dependency. Thus, most of the image content does not exhibit elaborated shape such as sky or the ocean looks fine. The task of creating geometrically complex forms, such as four-legged animals, is far more challenging. This is where attention comes into play.</p>
</blockquote>
<p>而 self-attention 中QKV 三个部分是相同的，对于这种处理方法和Res_block 还是有点相似的。<br><img src="http://ww1.sinaimg.cn/large/e9a223b5ly1g1fx354ispj210k0fsaae.jpg" alt=""></p>
<h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><p>训练数据集使用是 Cifar-10，记录了训练过程中 acc 和loss 的变化情况。除了训练的效果比较好外，训练速度也是非常快的，可以清楚的看到model acc 在接近25 epoches的时候就开始收敛。<br><img src="http://ww1.sinaimg.cn/large/e9a223b5ly1g1fx3rncspj20hi0d20t9.jpg" alt=""></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/25/fastText-faiss/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jijeng Jia">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jijeng's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/25/fastText-faiss/" itemprop="url">fastText & faiss</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-03-25T21:13:52+08:00">
                2019-03-25
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index">
                    <span itemprop="name">NLP</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="fastText"><a href="#fastText" class="headerlink" title="fastText"></a>fastText</h2><p>fastText结合了自然语言处理和机器学习中最成功的理念。这些包括了使用词袋以及n-gram袋表征语句，还有使用子字(subword)信息，并通过隐藏表征在类别间共享信息。另外采用了一个softmax层级(利用了类别不均衡分布的优势)来加速运算过程。</p>
<p>fastText 主要是用来解决 word representations和 sentence classification. 有趣的是前者是无监督的学习方式，后者是有监督的学习方式。分别主要来自 ”Enriching Word Vectors with Subword Information“ 和 “Bag of Tricks for Efficient Text Classification” 两篇论文。并且使用的是 shallow neural network 而不是深度网络。</p>
<blockquote>
<p>FastText is a library created by the Facebook Research Team for efficient learning of word representations and sentence classification.</p>
</blockquote>
<p>Take off:<br>fastText 方法包含三部分：模型架构、层次 Softmax 和 N-gram 特征。<br>fasttext 有两个用处： text classification 和 word embedding 。<br>使用场景：大型数据，高效计算</p>
<p>下面进行细说：</p>
<h3 id="模型架构"><a href="#模型架构" class="headerlink" title="模型架构"></a>模型架构</h3><p>这个是总的框架图。<br><img src="http://ww1.sinaimg.cn/large/e9a223b5ly1g1f8ph67dkj21280yuk1n.jpg" alt=""><br>&lt;:center&gt;<sub><sup>抱歉哈 这个引用找不见了，如果有侵权，please email me..</sup></sub><center><br>分为两个部分介绍这个网络结构：<br>从input -&gt; hidden:<br>输入层输入的是一个已经分词后短文本。短文本中每个词的词向量是由该短文本的one-hot矩阵乘以一个初始化的矩阵w得到的。（原理图：下图是fasttext 运行的时候，这个分词是再处理成单词和n-gram 组成的特征，这个是不需要我们进行显性的操作的）<br><img src="http://ww1.sinaimg.cn/large/e9a223b5ly1g1f8x600o5j21440lsjz0.jpg" alt=""><br>从 hidden -&gt; output：<br><img src="http://ww1.sinaimg.cn/large/e9a223b5ly1g1f8xzkf3yj213y0lsti8.jpg" alt=""><br>插播一句，我们经常使用的预训练模型中的weights 是从input-&gt; hidden。</center></p>
<h3 id="层次Softmax"><a href="#层次Softmax" class="headerlink" title="层次Softmax"></a>层次Softmax</h3><p>从名字上就知道这个是基于softmax的改进版本，主要是运算上的改进。具体说来：利用了类别（class）不均衡这个事实（一些类别出现次数比其他的更多），通过使用 Huffman 算法建立用于表征类别的树形结构。fastText 模型使用了层次 Softmax 技巧。层次 Softmax 技巧建立在Huffman的基础上，对标签进行编码，能够极大地缩小模型预测目标的数量。</p>
<p>这个是softmax 的原始的计算公式：<br><img src="http://ww1.sinaimg.cn/large/e9a223b5ly1g1fbkxph28j20ia0560sm.jpg" alt=""><br>采用二叉树的结构之后，时间上优化不少。$O ( N) \rightarrow O \left( \log _ { 2 } N \right)$。见下图。<br><img src="http://ww1.sinaimg.cn/large/e9a223b5ly1g1fboz6w71j20n20bedh7.jpg" alt=""></p>
<p><center><sub><sup>抱歉哈 这个引用找不见了，如果有侵权，please email me..</sup></sub><center><br>和之前的神经网络模型相比，这里的huffmax树的所有内部节点就类似之前神经网络隐藏层的神经元。其中，根节点的词向量对应我们的投影后的词向量，而所有的叶子节点就类似于之前神经网络softmax输出层的神经元。叶子节点的个数就是词汇表的大小. </center></center></p>
<p>和之前的相比，从隐藏层到输出层的softmax映射不是一下就完成的，而是沿着huffman树一步步完成的，因此这种softmax取名为”Hierarchical softmax”. </p>
<h3 id="N-gram-特征"><a href="#N-gram-特征" class="headerlink" title="N-gram 特征"></a>N-gram 特征</h3><p>N-gram是基于这样的思想：某个词的出现依赖于其他若干个词，第二个特点是我们获得的信息越多，预测越准确。我想说，我们每个人的大脑中都有一个N-gram模型，而且是在不断完善和训练的。我们的见识与经历，都在丰富着我们的阅历，增强着我们的联想能力。</p>
<p> N-gram模型是一种语言模型（Language Model，LM），语言模型是一个基于概率的判别模型，它的输入是一句话（单词的顺序序列），输出是这句话的概率，即这些单词的联合概率（joint probability）。 </p>
<p>这样的作用，使用n-gram来给文本添加额外的特征获得关于局部词顺序的部分信息。<br>举个栗子：对于句子：“我 喜欢 喝 咖啡”, 如果不考虑顺序，那么就是每个词，“我”，“喜欢”，“喝”，“咖啡”这五个单词的word embedding求平均。如果考虑2-gram, 那么除了以上五个词，还有“我喜欢”，“喜欢喝”，“喝咖啡”等词。“我喜欢”，“喜欢喝”，“喝咖啡”这三个词就作为这个句子的文本特征。<br>我们经常见到的场景：输入法的预选词汇。就是可以通过这种方式实现的。</p>
<p>当然使用了更多的特征意味着造成了效率下降，于是该作者提出了两种解决方法：<br>过滤掉低词频；使用词粒度代替字粒度。<br>比如说海慧寺使用上面那个句子”我喜欢喝咖啡“，如果使用子粒度的2-gram，那么产生的特征是“我喜”，“喜欢”，“欢喝”，“喝咖”，“咖啡”。如果使用词粒度为2-gram，那么产生的特征是“我喜欢”，“喜欢喝”，“喝咖啡”。</p>
<h3 id="Negative-Sampling"><a href="#Negative-Sampling" class="headerlink" title="Negative Sampling"></a>Negative Sampling</h3><p>这个技术不是 fastText 首创的，但是本着总结知识点的，也就放在这里了。<br>CBOW / Skip-gram模型 （这个论文中）提出了两种方法，一种是Hierarchical Softmax，另一种是Negative Sampling。<br>论文中提出的两种方法都是用来提高计算效率的，下面说一下负采样。</p>
<p>在训练神经网络时，每当接受一个训练样本，然后调整所有神经单元权重参数，来使神经网络预测更加准确。换句话说，每个训练样本都将会调整所有神经网络中的参数。而negative sampling 每次让一个训练样本仅仅更新一小部分的权重参数，从而降低梯度下降过程中的计算量。如果 vocabulary 大小为1万时， 当输入样本 ( “fox”, “quick”) 到神经网络时， “ fox” 经过 one-hot 编码，在输出层我们期望对应 “quick” 单词的那个神经元结点输出 1，其余 9999 个都应该输出0。在这里，这9999个我们期望输出为0的神经元结点所对应的单词我们为 negative word. negative sampling 的想法也很直接，将随机选择一小部分的 negative words，比如选 10个 negative words 来更新对应的权重参数。</p>
<h3 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h3><p>第一个应用场景：词向量。<br>fastText作为训练词向量认为可以有两种模式，一种是根据周围词语预测中心词汇的CBOW （continuous bag-of-words）模型，另一种是根据中心词汇预测上下文的 skip-gram 模型。</p>
<blockquote>
<p>./fasttext – It is used to invoke the FastText library.<br> skipgram/cbow – It is where you specify whether skipgram or cbow is to be used to create the word representations.<br> -input – This is the name of the parameter which specifies the following word to be used as the name of the file used for training. This argument should be used as is.<br> data.txt – a sample text file over which we wish to train the skipgram or cbow model. Change this name to the name of the text file you have.<br> -output – This is the name of the parameter which specifies the following word to be used as the name of the model being created. This argument is to be used as is.<br> model – This is the name of the model created.<br>Running the above command will create two files named model.bin and model.vec. model.bin contains the model parameters, dictionary and the hyperparameters and can be used to compute word vectors. model.vec is a text file that contains the word vectors for one word per line.</p>
</blockquote>
<p>最后生成有两个文件，一个 xxx.bin 文件，一个是 xxx.vec 文件，前者是预训练模型，后者是词向量。<br>这两个可能是最重要的格式了。</p>
<blockquote>
<p>The most important parameters of the model are its dimension and the range of size for the subwords. </p>
</blockquote>
<p>常见的代码格式：</p>
<pre><code>./fasttext skipgram -input data/fil9 -output result/fil9 -minn 2 -maxn 5 -dim 300
</code></pre><p>跑偏一下说一下shell的小技巧。<br>使用echo 或者 &lt; 这样进行单个词或者多个词的词向量的查询。</p>
<blockquote>
<p>./fasttext print-word-vectors model.bin &lt; queries.txt<br>echo “word” | ./fasttext print-word-vectors model.bin</p>
</blockquote>
<p>Finding simialr words:</p>
<pre><code>./fasttext nn model.bin
</code></pre><p>第二个应用场景：文本分类。</p>
<blockquote>
<p>Sentiment analysis and email classification are classic examples of text classification</p>
</blockquote>
<p>在训练数据集中label 默认是使用 “__label__” 进行表示的，当然也是可以进行自定义的。</p>
<pre><code>./fasttext supervised -input train.ft.txt -output model_kaggle -label  __label__ -lr 0.5
就是进行predict的时候，有时候并不是很能想起来只是predict top 3 这样的东西。
# Predicting on the test dataset
./fasttext predict model_kaggle.bin test.ft.txt
# Predicting the top 3 labels
./fasttext predict model_kaggle.bin test.ft.txt 3
</code></pre><h2 id="faiss"><a href="#faiss" class="headerlink" title="faiss"></a>faiss</h2><p>用途：相似度检测和稠密向量的聚类。</p>
<blockquote>
<p>Faiss is a library for efficient similarity search and clustering of dense vectors.</p>
</blockquote>
<p>之前的实习经历主要是用faiss 处理文本的representation，但是这个是有偏差的，凡是能够打成词向量，都是可以使用faiss 进行计算的，当然这词向量需要满足：相近内容在相近的空间。</p>
<blockquote>
<p>Once the vectors are extracted by learning machinery (from images, videos, text documents, and elsewhere), they’re ready to feed into the similarity search library.</p>
</blockquote>
<h3 id="faiss的实现过程"><a href="#faiss的实现过程" class="headerlink" title="faiss的实现过程"></a>faiss的实现过程</h3><p>首先使用 index对于向量进行预处理，然后选择不同的模式。</p>
<p>牺牲了一些精确性来使得运行速度更快。</p>
<blockquote>
<p>Similarity search can be made orders of magnitude faster if we’re willing to trade some accuracy; that is, deviate a bit from the reference result. For example, it may not matter much if the first and second results of an image similarity search are swapped, since they’re probably both correct results for a given query. Accelerating the search involves some pre-processing of the data set, an operation that we call indexing.</p>
</blockquote>
<p>向量的比较有两种metric：一种是L2 一种是基于consine (点乘)进行检索。前者是求解最小的值，后者是通过inner——product 求解maximum. 并且是支持GPU的，在原来CPU上建立的index，然后很好的迁移到 GPU上。</p>
<p>在涉及index使用考虑速度，acc和内存大小三个不同的维度。然后不同的index 是有不同的侧重的。<br><img src="http://ww1.sinaimg.cn/large/e9a223b5ly1g1faf2twaaj20ft0e3ae0.jpg" alt=""></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/25/NLP中的碎碎念/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jijeng Jia">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jijeng's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/25/NLP中的碎碎念/" itemprop="url">NLP中的碎碎念</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-03-25T21:11:40+08:00">
                2019-03-25
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index">
                    <span itemprop="name">NLP</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>主要介绍关键词提取并整理一下NLP相关的基础知识点。</p>
<h2 id="关键词提取"><a href="#关键词提取" class="headerlink" title="关键词提取"></a>关键词提取</h2><h3 id="TF-IDF"><a href="#TF-IDF" class="headerlink" title="TF-IDF"></a>TF-IDF</h3><p>这个是可以参看之前自己写的一个<a href="https://jijeng.github.io/2018/08/23/%E6%96%87%E6%9C%AC%E7%9B%B8%E4%BC%BC%E5%BA%A6%E6%AF%94%E8%BE%83%E5%9F%BA%E6%9C%AC%E7%9F%A5%E8%AF%86/" target="_blank" rel="noopener">博客</a></p>
<h3 id="卡方分布"><a href="#卡方分布" class="headerlink" title="卡方分布"></a>卡方分布</h3><p>卡方检验是以χ2分布为基础的一种常用假设检验方法，它的无效假设H0是：观察频数与期望频数没有差别。该检验的基本思想是：首先假设H0成立，基于此前提计算出χ2值，它表示观察值与理论值之间的偏离程度。<br>而实际应用到特征选择中的时候，我们不需要知道自由度，不要知道卡方分布，我们只需要根据算出来的χ2 进行排序就好了，越大我们就越喜欢！挑选最大的一堆，于是就完成了利用卡方检验来进行特征提取。<br>卡方分布的缺点：<br>没有考虑词频</p>
<h3 id="Textrank"><a href="#Textrank" class="headerlink" title="Textrank"></a>Textrank</h3><p>有一个与之很像的概念 pageRanking，最开始是用来计算网页的重要性。Textrank 主要用来提取文章的关键词，然后比较适合长文本。</p>
<h2 id="CBOW和skip-gram"><a href="#CBOW和skip-gram" class="headerlink" title="CBOW和skip-gram"></a>CBOW和skip-gram</h2><p>举一个简单的小例子说明 CBOW和skip-gram的区别：skip-gram 是根据中心词汇然后预测上下文词汇，这个不是一下子输入上下文词汇的，而是一个过程，中心词汇和上下文词汇1 ，中心词汇和上下文词汇2，这样的进行输入。 cbow 和其的区别，在于简单相加了上下文词汇作为一个整体，然后和中心词汇进行输入，所以最后是这样的结果。</p>
<p>使用skip gram训练的时间更长，但是对于出现频率不高的词汇，效果比较好。但CBOW的训练速度是相对来说比较快一些。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/4/">4</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/avatar.jpeg"
                alt="Jijeng Jia" />
            
              <p class="site-author-name" itemprop="name">Jijeng Jia</p>
              <p class="site-description motion-element" itemprop="description">Stay Hungry, Stay Foolish</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">32</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">7</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">26</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/jijeng" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:jia1509309698@163.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Jijeng Jia</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
