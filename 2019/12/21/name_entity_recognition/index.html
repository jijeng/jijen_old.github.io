<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="基于中文简历的命名实体识别项目（持续更新中）">
<meta property="og:type" content="article">
<meta property="og:title" content="基于中文简历的命名实体识别">
<meta property="og:url" content="http://yoursite.com/2019/12/21/name_entity_recognition/index.html">
<meta property="og:site_name" content="Jijeng&#39;s blog">
<meta property="og:description" content="基于中文简历的命名实体识别项目（持续更新中）">
<meta property="og:image" content="https://s2.ax1x.com/2019/12/31/l19XvT.jpg">
<meta property="article:published_time" content="2019-12-21T09:42:46.000Z">
<meta property="article:modified_time" content="2020-01-01T05:03:31.998Z">
<meta property="article:author" content="Jijeng Jia">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://s2.ax1x.com/2019/12/31/l19XvT.jpg">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2019/12/21/name_entity_recognition/"/>







<script>
	(function(){
		if('hexo149'){
			if (prompt('请输入文章密码','') !== 'hexo149'){
				alert('密码错误！');
				history.back();
			}
		}
	})();
</script>

  <title>基于中文简历的命名实体识别 | Jijeng's blog</title>
  








<meta name="generator" content="Hexo 4.2.0"></head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Jijeng's blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/%20" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/%20" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/%20" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/%20" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/12/21/name_entity_recognition/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jijeng Jia">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jijeng's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">基于中文简历的命名实体识别</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-12-21T17:42:46+08:00">
                2019-12-21
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">Post modified&#58;</span>
              
              <time title="Post modified" itemprop="dateModified" datetime="2020-01-01T13:03:31+08:00">
                2020-01-01
              </time>
            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NOT-FOR-YOU/" itemprop="url" rel="index">
                    <span itemprop="name">NOT_FOR_YOU</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/12/21/name_entity_recognition/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2019/12/21/name_entity_recognition/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="page-pv"><i class="fa fa-file-o"></i>
            <span class="busuanzi-value" id="busuanzi_value_page_pv" ></span>
            </span>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>基于中文简历的命名实体识别项目（持续更新中）</p>
<a id="more"></a>



<p>命名实体识别是一个极具挑战性的任务。首先，在大多数语言和领域中，训练数据非常少；其次，对可以成为实体的单词类型的限制很小，因此很难从小样本数据集训练出一个泛化性强的模型。本项目尝试了多种模型算法（如HMM, CRF，Bi-LSTM + CRF）来处理中文命名实体识别的问题。数据集来自 <code>Chinese NER using Lattice LSTM</code>（ACL 2018）中收集到的简历数据集。</p>
<h2 id="数据"><a href="#数据" class="headerlink" title="数据"></a>数据</h2><p>总共的数据集有 4775（大概5000条数据），按照以下的 8：1：1 分成训练集、验证集和测试集。</p>
<table>
<thead>
<tr>
<th>数据集分类</th>
<th align="center">数量</th>
</tr>
</thead>
<tbody><tr>
<td>train set</td>
<td align="center">3821</td>
</tr>
<tr>
<td>dev set</td>
<td align="center">477</td>
</tr>
<tr>
<td>test set</td>
<td align="center">477</td>
</tr>
</tbody></table>
<p>数据的格式如下，它的每一行由一个字及其对应的标注组成，标注集采用BIOES，句子之间用一个空行隔开。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">美	B-LOC</span><br><span class="line">国	E-LOC</span><br><span class="line">的	O</span><br><span class="line">华	B-PER</span><br><span class="line">莱	I-PER</span><br><span class="line">士	E-PER</span><br><span class="line"></span><br><span class="line">我	O</span><br><span class="line">跟	O</span><br><span class="line">他	O</span><br><span class="line">谈	O</span><br><span class="line">笑	O</span><br><span class="line">风	O</span><br><span class="line">生	O </span><br><span class="line"></span><br><span class="line">汉     S</span><br><span class="line"></span><br><span class="line">党 B-TITLE</span><br><span class="line">员 E-TITLE</span><br></pre></td></tr></table></figure>

<p>训练数据<br>词典大小总数 1792，tag 总数28</p>
<p>一份简历信息可以划分成借个中长句子。每一条就是一个训练数据集。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[&#39;高&#39;, &#39;勇&#39;, &#39;：&#39;, &#39;男&#39;, &#39;，&#39;, &#39;中&#39;, &#39;国&#39;, &#39;国&#39;, &#39;籍&#39;, &#39;，&#39;, &#39;无&#39;, &#39;境&#39;, &#39;外&#39;, &#39;居&#39;, &#39;留&#39;, &#39;权&#39;, &#39;，&#39;]</span><br><span class="line">&#39;高勇：男，中国国籍，无境外居留权，&#39;</span><br><span class="line"></span><br><span class="line">[&#39;1&#39;, &#39;9&#39;, &#39;6&#39;, &#39;6&#39;, &#39;年&#39;, &#39;出&#39;, &#39;生&#39;, &#39;，&#39;, &#39;汉&#39;, &#39;族&#39;, &#39;，&#39;, &#39;中&#39;, &#39;共&#39;, &#39;党&#39;, &#39;员&#39;, &#39;，&#39;, &#39;本&#39;, &#39;科&#39;, &#39;学&#39;, &#39;历&#39;, &#39;，&#39;, &#39;工&#39;, &#39;程&#39;, &#39;师&#39;, &#39;、&#39;, &#39;美&#39;, &#39;国&#39;, &#39;项&#39;, &#39;目&#39;, &#39;管&#39;, &#39;理&#39;, &#39;协&#39;, &#39;会&#39;, &#39;注&#39;, &#39;册&#39;, &#39;会&#39;, &#39;员&#39;, &#39;（&#39;, &#39;P&#39;, &#39;M&#39;, &#39;I&#39;, &#39;M&#39;, &#39;e&#39;, &#39;m&#39;, &#39;b&#39;, &#39;e&#39;, &#39;r&#39;, &#39;）&#39;, &#39;、&#39;, &#39;注&#39;, &#39;册&#39;, &#39;项&#39;, &#39;目&#39;, &#39;管&#39;, &#39;理&#39;, &#39;专&#39;, &#39;家&#39;, &#39;（&#39;, &#39;P&#39;, &#39;M&#39;, &#39;P&#39;, &#39;）&#39;, &#39;、&#39;, &#39;项&#39;, &#39;目&#39;, &#39;经&#39;, &#39;理&#39;, &#39;。&#39;]</span><br><span class="line">&#39;1966年出生，汉族，中共党员，本科学历，工程师、美国项目管理协会注册会员（PMIMember）、注册项目管理专家（PMP）、项目经理。&#39;</span><br><span class="line"></span><br><span class="line">[&#39;2&#39;, &#39;0&#39;, &#39;0&#39;, &#39;7&#39;, &#39;年&#39;, &#39;1&#39;, &#39;0&#39;, &#39;月&#39;, &#39;至&#39;, &#39;今&#39;, &#39;任&#39;, &#39;人&#39;, &#39;和&#39;, &#39;投&#39;, &#39;资&#39;, &#39;董&#39;, &#39;事&#39;, &#39;；&#39;]</span><br><span class="line">&#39;2007年10月至今任人和投资董事；&#39;</span><br><span class="line"></span><br><span class="line">[&#39;2&#39;, &#39;0&#39;, &#39;0&#39;, &#39;7&#39;, &#39;年&#39;, &#39;1&#39;, &#39;2&#39;, &#39;月&#39;, &#39;至&#39;, &#39;2&#39;, &#39;0&#39;, &#39;1&#39;, &#39;3&#39;, &#39;年&#39;, &#39;2&#39;, &#39;月&#39;, &#39;任&#39;, &#39;公&#39;, &#39;司&#39;, &#39;董&#39;, &#39;事&#39;, &#39;、&#39;, &#39;董&#39;, &#39;事&#39;, &#39;会&#39;, &#39;秘&#39;, &#39;书&#39;, &#39;、&#39;, &#39;综&#39;, &#39;合&#39;, &#39;管&#39;, &#39;理&#39;, &#39;部&#39;, &#39;部&#39;, &#39;长&#39;, &#39;；&#39;]</span><br><span class="line">&#39;2007年12月至2013年2月任公司董事、董事会秘书、综合管理部部长；&#39;</span><br><span class="line"></span><br><span class="line">[&#39;2&#39;, &#39;0&#39;, &#39;1&#39;, &#39;3&#39;, &#39;年&#39;, &#39;2&#39;, &#39;月&#39;, &#39;至&#39;, &#39;今&#39;, &#39;任&#39;, &#39;山&#39;, &#39;东&#39;, &#39;三&#39;, &#39;维&#39;, &#39;石&#39;, &#39;化&#39;, &#39;工&#39;, &#39;程&#39;, &#39;股&#39;, &#39;份&#39;, &#39;有&#39;, &#39;限&#39;, &#39;公&#39;, &#39;司&#39;, &#39;董&#39;, &#39;事&#39;, &#39;、&#39;, &#39;董&#39;, &#39;事&#39;, &#39;会&#39;, &#39;秘&#39;, &#39;书&#39;, &#39;、&#39;, &#39;副&#39;, &#39;总&#39;, &#39;经&#39;, &#39;理&#39;, &#39;。&#39;]</span><br><span class="line">&#39;2013年2月至今任山东三维石化工程股份有限公司董事、董事会秘书、副总经理。&#39;</span><br></pre></td></tr></table></figure>


<p>特征向量（对于一条训练数据集而言）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[&#123;&#39;w&#39;: &#39;高&#39;, &#39;w-1&#39;: &#39;&lt;s&gt;&#39;, &#39;w+1&#39;: &#39;勇&#39;, &#39;w-1:w&#39;: &#39;&lt;s&gt;高&#39;, &#39;w:w+1&#39;: &#39;高勇&#39;, &#39;bias&#39;: 1&#125;, &#123;&#39;w&#39;: &#39;勇&#39;, &#39;w-1&#39;: &#39;高&#39;, &#39;w+1&#39;: &#39;：&#39;, &#39;w-1:w&#39;: &#39;高勇&#39;, &#39;w:w+1&#39;: &#39;勇：&#39;, &#39;bias&#39;: 1&#125;, &#123;&#39;w&#39;: &#39;：&#39;, &#39;w-1&#39;: &#39;勇&#39;, &#39;w+1&#39;: &#39;男&#39;, &#39;w-1:w&#39;: &#39;勇：&#39;, &#39;w:w+1&#39;: &#39;：男&#39;, &#39;bias&#39;: 1&#125;, &#123;&#39;w&#39;: &#39;男&#39;, &#39;w-1&#39;: &#39;：&#39;, &#39;w+1&#39;: &#39;，&#39;, &#39;w-1:w&#39;: &#39;：男&#39;, &#39;w:w+1&#39;: &#39;男，&#39;, &#39;bias&#39;: 1&#125;, &#123;&#39;w&#39;: &#39;，&#39;, &#39;w-1&#39;: &#39;男&#39;, &#39;w+1&#39;: &#39;中&#39;, &#39;w-1:w&#39;: &#39;男，&#39;, &#39;w:w+1&#39;: &#39;，中&#39;, &#39;bias&#39;: 1&#125;, &#123;&#39;w&#39;: &#39;中&#39;, &#39;w-1&#39;: &#39;，&#39;, &#39;w+1&#39;: &#39;国&#39;, &#39;w-1:w&#39;: &#39;，中&#39;, &#39;w:w+1&#39;: &#39;中国&#39;, &#39;bias&#39;: 1&#125;, &#123;&#39;w&#39;: &#39;国&#39;, &#39;w-1&#39;: &#39;中&#39;, &#39;w+1&#39;: &#39;国&#39;, &#39;w-1:w&#39;: &#39;中国&#39;, &#39;w:w+1&#39;: &#39;国国&#39;, &#39;bias&#39;: 1&#125;, &#123;&#39;w&#39;: &#39;国&#39;, &#39;w-1&#39;: &#39;国&#39;, &#39;w+1&#39;: &#39;籍&#39;, &#39;w-1:w&#39;: &#39;国国&#39;, &#39;w:w+1&#39;: &#39;国籍&#39;, &#39;bias&#39;: 1&#125;, &#123;&#39;w&#39;: &#39;籍&#39;, &#39;w-1&#39;: &#39;国&#39;, &#39;w+1&#39;: &#39;，&#39;, &#39;w-1:w&#39;: &#39;国籍&#39;, &#39;w:w+1&#39;: &#39;籍，&#39;, &#39;bias&#39;: 1&#125;, &#123;&#39;w&#39;: &#39;，&#39;, &#39;w-1&#39;: &#39;籍&#39;, &#39;w+1&#39;: &#39;无&#39;, &#39;w-1:w&#39;: &#39;籍，&#39;, &#39;w:w+1&#39;: &#39;，无&#39;, &#39;bias&#39;: 1&#125;, &#123;&#39;w&#39;: &#39;无&#39;, &#39;w-1&#39;: &#39;，&#39;, &#39;w+1&#39;: &#39;境&#39;, &#39;w-1:w&#39;: &#39;，无&#39;, &#39;w:w+1&#39;: &#39;无境&#39;, &#39;bias&#39;: 1&#125;, &#123;&#39;w&#39;: &#39;境&#39;, &#39;w-1&#39;: &#39;无&#39;, &#39;w+1&#39;: &#39;外&#39;, &#39;w-1:w&#39;: &#39;无境&#39;, &#39;w:w+1&#39;: &#39;境外&#39;, &#39;bias&#39;: 1&#125;, &#123;&#39;w&#39;: &#39;外&#39;, &#39;w-1&#39;: &#39;境&#39;, &#39;w+1&#39;: &#39;居&#39;, &#39;w-1:w&#39;: &#39;境外&#39;, &#39;w:w+1&#39;: &#39;外居&#39;, &#39;bias&#39;: 1&#125;, &#123;&#39;w&#39;: &#39;居&#39;, &#39;w-1&#39;: &#39;外&#39;, &#39;w+1&#39;: &#39;留&#39;, &#39;w-1:w&#39;: &#39;外居&#39;, &#39;w:w+1&#39;: &#39;居留&#39;, &#39;bias&#39;: 1&#125;, &#123;&#39;w&#39;: &#39;留&#39;, &#39;w-1&#39;: &#39;居&#39;, &#39;w+1&#39;: &#39;权&#39;, &#39;w-1:w&#39;: &#39;居留&#39;, &#39;w:w+1&#39;: &#39;留权&#39;, &#39;bias&#39;: 1&#125;, &#123;&#39;w&#39;: &#39;权&#39;, &#39;w-1&#39;: &#39;留&#39;, &#39;w+1&#39;: &#39;，&#39;, &#39;w-1:w&#39;: &#39;留权&#39;, &#39;w:w+1&#39;: &#39;权，&#39;, &#39;bias&#39;: 1&#125;, &#123;&#39;w&#39;: &#39;，&#39;, &#39;w-1&#39;: &#39;权&#39;, &#39;w+1&#39;: &#39;&lt;&#x2F;s&gt;&#39;, &#39;w-1:w&#39;: &#39;权，&#39;, &#39;w:w+1&#39;: &#39;，&lt;&#x2F;s&gt;&#39;, &#39;bias&#39;: 1&#125;]</span><br></pre></td></tr></table></figure>



<p>一元语法（unigram）、二元语法（bigram）、三元语法（trigram）和多元词法（n-gram），指的是文本中连续出现的n 个词语。对于英文来说 gram 越大，那么上下文信息越是明显，但是训练的时间成本相对变大；对于中文来说，使用unigram 就可以（左右各一个）</p>
<p>BFGS 算法是一种拟牛顿算法。</p>
<p>为什么使用BiLSTM，有人认为这种倒叙的是没有意义的。但是在实际工程中，BilSTM 的效果十有八九是好于LSTM，所以一般时候都是这样使用的。双向LSTM 的结构也很简单，就是两个单向LSTM 分别沿着正序和反序进行传播，然后将最后输出拼接在一起。注意该层输出的size 是 <code>2 * hidden_size</code> 。</p>
<h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p>（1）数据预处理阶段：</p>
<p>输入是一个list，返回一个dictionary，其中key 是list 内容，val 是相应的index，所以完成了 char2index 的功能。实现比较巧妙 <code>maps[e] =len(maps)</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_map</span><span class="params">(lists)</span>:</span></span><br><span class="line">    maps = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> list_ <span class="keyword">in</span> lists:</span><br><span class="line">        <span class="keyword">for</span> e <span class="keyword">in</span> list_:</span><br><span class="line">            <span class="keyword">if</span> e <span class="keyword">not</span> <span class="keyword">in</span> maps:</span><br><span class="line">                maps[e] = len(maps)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> maps</span><br></pre></td></tr></table></figure>

<p>只有在训练数据集上需要建立词典索引和tag 索引，在验证集和测试集上是不需要建立索引的。</p>
<p>（2）HMM 模型</p>
<p>train 阶段</p>
<p>HMM 的训练就是根据训练语料对模型参数进行估计：我们有观测序列和对应的状态序列，然后使用极大似然的方法估计hmm 模型的参数。无论是转移概率矩阵还是观测概率矩阵，都是做的一个统计的工作。以观测矩阵为例</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 估计观测概率矩阵</span></span><br><span class="line"><span class="keyword">for</span> tag_list, word_list <span class="keyword">in</span> zip(tag_lists, word_lists):</span><br><span class="line">    <span class="keyword">assert</span> len(tag_list) == len(word_list)</span><br><span class="line">    <span class="keyword">for</span> tag, word <span class="keyword">in</span> zip(tag_list, word_list):</span><br><span class="line">        tag_id = tag2id[tag]</span><br><span class="line">        word_id = word2id[word]</span><br><span class="line">        self.B[tag_id][word_id] += <span class="number">1</span></span><br><span class="line">self.B[self.B == <span class="number">0.</span>] = <span class="number">1e-10</span></span><br><span class="line">self.B = self.B / self.B.sum(dim=<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<p>问题：当某个元素没有出现过的时候，该位置为0。解决方案是，将0替换成很小的数字，比如说 $e ^{-10}$。</p>
<p>decoding阶段</p>
<p>使用维特比算法，给定观测序列求解状态序列，这个是就是生成标注的过程。维特比算法实际上使用动态规划解hmm模型预测问题，基于dp 求解概率最大路径。</p>
<p>维特比算法（viterbi algorithm）使用的是动态规划的思想解决HMM 和CRF 中的预测问题。即在HMM模型中寻找最有可能的隐状态的路径。使用一句话概括为：在每一时刻，计算当前时刻是由上一个时刻中的每种隐状态导致的最大概率，并且记录这个最大概率是从前一时刻哪个隐状态转移过来的，最后回溯最大概率，即为路径。</p>
<p>DP 递推方程</p>
<p>\begin{equation}<br>\delta_{t}(j)=\max \left[\delta_{t-1}(i) \times a_{i j} \times b_{j}\left(o_{t}\right)\right]<br>\end{equation}</p>
<ul>
<li>$\delta_{t}(j)$ ： $t$ 时刻沿着一条状态路径 $q_1$, $q_2$,  $q_t$， 当 $t$ 时刻处于$j$状态，产生$o_1$, $o_2$，… $o_t$的最大的概率</li>
<li>$a_{ij}$ ：从状态$i$ 到状态 $j$ 的转移概率</li>
<li>$b_j(o_t)$ ：从状态 $j$ 到输出$o_t$ 的概率</li>
</ul>
<p>时间复杂度 $O(TN^2)$，其中$T$ 表示时刻， $N$表示有多少种隐状态， $N^2$表示隐状态的组合。</p>
<p>问题1： 当$T$很长的时候，连乘容易下溢。解决方案：使用对数概率，这样相乘变成了相加。<br>问题2：如果某个词不再字典中，那么假设其为均匀分布。<br>问题3：最优路径的计算。正向传播的时候，记录当前时刻最大概率是由上一时刻哪个隐状态转换过来的，然后回溯求解最大值。</p>
<p>（3）CRF 模型</p>
<p>对于中文的特征函数写的比较简单。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">word2features</span><span class="params">(sent, i)</span>:</span></span><br><span class="line">    <span class="string">"""抽取单个字的特征"""</span></span><br><span class="line">    word = sent[i]</span><br><span class="line">    prev_word = <span class="string">"&lt;s&gt;"</span> <span class="keyword">if</span> i == <span class="number">0</span> <span class="keyword">else</span> sent[i<span class="number">-1</span>]</span><br><span class="line">    next_word = <span class="string">"&lt;/s&gt;"</span> <span class="keyword">if</span> i == (len(sent)<span class="number">-1</span>) <span class="keyword">else</span> sent[i+<span class="number">1</span>]</span><br><span class="line">    <span class="comment"># 使用的特征：</span></span><br><span class="line">    <span class="comment"># 前一个词，当前词，后一个词，</span></span><br><span class="line">    <span class="comment"># 前一个词+当前词， 当前词+后一个词</span></span><br><span class="line">    features = &#123;</span><br><span class="line">        <span class="string">'w'</span>: word,</span><br><span class="line">        <span class="string">'w-1'</span>: prev_word,</span><br><span class="line">        <span class="string">'w+1'</span>: next_word,</span><br><span class="line">        <span class="string">'w-1:w'</span>: prev_word+word,</span><br><span class="line">        <span class="string">'w:w+1'</span>: word+next_word,</span><br><span class="line">        <span class="string">'bias'</span>: <span class="number">1</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> features</span><br></pre></td></tr></table></figure>




<p>（4）Bi-LSTM</p>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BiLSTM</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, emb_size, hidden_size, out_size)</span>:</span></span><br><span class="line">       <span class="string">"""初始化参数：</span></span><br><span class="line"><span class="string">           vocab_size:字典的大小</span></span><br><span class="line"><span class="string">           emb_size:词向量的维数</span></span><br><span class="line"><span class="string">           hidden_size：隐向量的维数</span></span><br><span class="line"><span class="string">           out_size:标注的种类</span></span><br><span class="line"><span class="string">       """</span></span><br><span class="line">       super(BiLSTM, self).__init__()</span><br><span class="line">       self.embedding = nn.Embedding(vocab_size, emb_size)</span><br><span class="line">       self.bilstm = nn.LSTM(emb_size, hidden_size,</span><br><span class="line">                             batch_first=<span class="literal">True</span>,</span><br><span class="line">                             bidirectional=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">       self.lin = nn.Linear(<span class="number">2</span>*hidden_size, out_size)</span><br><span class="line"></span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, sents_tensor, lengths)</span>:</span></span><br><span class="line">       emb = self.embedding(sents_tensor)  <span class="comment"># [B, L, emb_size] # 分词之后是经过一个embedding，而不是one -hot</span></span><br><span class="line"></span><br><span class="line">       packed = pack_padded_sequence(emb, lengths, batch_first=<span class="literal">True</span>) </span><br><span class="line">       <span class="comment"># 这个操作有点怪呀， 在进入bilstm之前是需要把padding pack，然后出了lstm 之后需要把packed </span></span><br><span class="line">       rnn_out, _ = self.bilstm(packed) <span class="comment"># embedding 给padding</span></span><br><span class="line">       <span class="comment"># rnn_out:[B, L, hidden_size*2] </span></span><br><span class="line">      <span class="comment"># 这种操作可能的原因是减少 lstm 中的参数量吧</span></span><br><span class="line">       rnn_out, _ = pad_packed_sequence(rnn_out, batch_first=<span class="literal">True</span>)</span><br><span class="line">       scores = self.lin(rnn_out)  <span class="comment"># [B, L, out_size]</span></span><br><span class="line">       <span class="keyword">return</span> scores</span><br><span class="line"></span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">(self, sents_tensor, lengths, _)</span>:</span></span><br><span class="line">       <span class="string">"""第三个参数不会用到，加它是为了与BiLSTM_CRF保持同样的接口"""</span></span><br><span class="line">       logits = self.forward(sents_tensor, lengths)  <span class="comment"># [B, L, out_size]</span></span><br><span class="line">       _, batch_tagids = torch.max(logits, dim=<span class="number">2</span>)</span><br><span class="line">       <span class="keyword">return</span> batch_tagids</span><br></pre></td></tr></table></figure>


<p>文本的数据预处理无非是将文本转换成模型可以理解的数字，在训练lstm 的时候需要加上四个特殊的token：</p>
<ul>
<li>&lt; PAD&gt;: 补全字符。</li>
<li>&lt; EOS&gt;: 解码器端的句子结束标识符。</li>
<li>&lt; UNK&gt;: 低频词或者一些未遇到过的词等。</li>
<li>&lt; GO&gt;: 解码器端的句子起始标识符。</li>
</ul>
<p>这种基于 <code>len()</code> 的实现实在是太骚了</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># LSTM模型训练的时候需要在word2id和tag2id加入PAD和UNK</span></span><br><span class="line"><span class="comment"># 如果是加了CRF的lstm还要加入&lt;start&gt;和&lt;end&gt; (解码的时候需要用到)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">extend_maps</span><span class="params">(word2id, tag2id, for_crf=True)</span>:</span></span><br><span class="line">    word2id[<span class="string">'&lt;unk&gt;'</span>] = len(word2id)</span><br><span class="line">    word2id[<span class="string">'&lt;pad&gt;'</span>] = len(word2id)</span><br><span class="line">    tag2id[<span class="string">'&lt;unk&gt;'</span>] = len(tag2id)</span><br><span class="line">    tag2id[<span class="string">'&lt;pad&gt;'</span>] = len(tag2id)</span><br><span class="line">    <span class="comment"># 如果是加了CRF的bilstm  那么还要加入&lt;start&gt; 和 &lt;end&gt;token</span></span><br><span class="line">    <span class="keyword">if</span> for_crf:</span><br><span class="line">        word2id[<span class="string">'&lt;start&gt;'</span>] = len(word2id)</span><br><span class="line">        word2id[<span class="string">'&lt;end&gt;'</span>] = len(word2id)</span><br><span class="line">        tag2id[<span class="string">'&lt;start&gt;'</span>] = len(tag2id)</span><br><span class="line">        tag2id[<span class="string">'&lt;end&gt;'</span>] = len(tag2id)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> word2id, tag2id</span><br></pre></td></tr></table></figure>


<p>如果一个类中只有函数的（如 <code>utils.py</code>）,那么使用 <code>from utils import fun1</code>  就可以；如果某个类中包含类<code>crf.py</code>, 在引用的时候，使用 <code>from crf import CRF</code>的语句。</p>
<p>（5）Bi-LSTM +CRF模型</p>
<img src="https://s2.ax1x.com/2019/12/31/l19XvT.jpg" width="80%" height="80%">

<p>为了使转移分数矩阵更具鲁棒性，我们加上START 和 END两类标签。START代表一个句子的开始（不是句子的第一个单词），END代表一个句子的结束。对于这种写法是非常有条理的：train，evaluation 和test 数据集是需要经过不同的处理，然后可以写一个函数，引入不同的参数，表示不同的处理。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">prepocess_data_for_lstmcrf</span><span class="params">(word_lists, tag_lists, test=False)</span>:</span></span><br><span class="line">    <span class="keyword">assert</span> len(word_lists) == len(tag_lists)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(word_lists)):</span><br><span class="line">        word_lists[i].append(<span class="string">"&lt;end&gt;"</span>) <span class="comment"># 对于训练数据集</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> test:  <span class="comment"># 如果是测试数据，就不需要加end token了</span></span><br><span class="line">            tag_lists[i].append(<span class="string">"&lt;end&gt;"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> word_lists, tag_lists</span><br></pre></td></tr></table></figure>



<p>如果是<code>bi-lstm</code> 模型，因为相当于是一个语言模型，<br>通过softmax 可以获得输出序列的概率 $p(y| X)$，然后取对数，这个时候的损失函数就定义为 $- \log p(y|X)$，使用最大似然估计求解。到了 <code>bi-lstm + CRF</code>中是可以使用各种损失函数的。</p>
<p>LSTM without CRF 的损失函数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cal_loss</span><span class="params">(logits, targets, tag2id)</span>:</span></span><br><span class="line">    <span class="string">"""计算损失</span></span><br><span class="line"><span class="string">    参数:</span></span><br><span class="line"><span class="string">        logits: [B, L, out_size]</span></span><br><span class="line"><span class="string">        targets: [B, L]</span></span><br><span class="line"><span class="string">        lengths: [B]</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    PAD = tag2id.get(<span class="string">'&lt;pad&gt;'</span>)</span><br><span class="line">    <span class="keyword">assert</span> PAD <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    mask = (targets != PAD)  <span class="comment"># [B, L]</span></span><br><span class="line">    targets = targets[mask]</span><br><span class="line">    out_size = logits.size(<span class="number">2</span>)</span><br><span class="line">    logits = logits.masked_select(</span><br><span class="line">        mask.unsqueeze(<span class="number">2</span>).expand(<span class="number">-1</span>, <span class="number">-1</span>, out_size)</span><br><span class="line">    ).contiguous().view(<span class="number">-1</span>, out_size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span> logits.size(<span class="number">0</span>) == targets.size(<span class="number">0</span>)</span><br><span class="line">    loss = F.cross_entropy(logits, targets)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure>


<p>LSTM with CRF的损失函数。相比于上一个损失函数，该损失函数增加了对于<em>生成label之间的约束</em>。具体如下：</p>
<p>输入数据 $X$ 表示如下：</p>
<p>\begin{equation}<br>\mathbf{X}=\left(\mathbf{x} _{1}, \mathbf{x} _{2}, \dots, \mathbf{x} _{n}\right)<br>\end{equation}</p>
<p>\begin{equation}<br>\mathbf{y}=\left(y_{1}, y_{2}, \ldots, y_{n}\right)<br>\end{equation}</p>
<blockquote>
<p>we consider $P$ to be the matrix of scores output by the bidirectional LSTM network. P is of size $n × k$, where k is the number of distinct tags, and $P_{i,j} corresponds to the score of the $j$ th tag of the $i$th word in a sentence. For a sequence of predictions</p>
</blockquote>
<p>其中分数 score 定义为:<br>\begin{equation}<br>s(\mathbf{X}, \mathbf{y})=\sum_{i=0}^{n} A_{y_{i}, y_{i+1}}+\sum_{i=1}^{n} P_{i, y_{i}}<br>\end{equation}</p>
<p>$A$ 是转移矩阵，$A_{i, j}$表示从tags $i$ 到 tags$j$的转移。</p>
<p>对结果使用softmax 可以得到：</p>
<p>\begin{equation}<br>p(\mathbf{y} | \mathbf{X})=\frac{e^{s(\mathbf{X}, \mathbf{y})}}{\sum_{\widetilde{\mathbf{y}} \in \mathbf{Y}_{\mathbf{X}}} e^{s(\mathbf{X}, \widetilde{\mathbf{y}})}}<br>\end{equation}</p>
<p>在训练过程中，经常将结果取对数。最终的tag 用 $y^*$表示为：</p>
<p>\begin{equation}<br>\mathbf{y}^{*}=\underset{\tilde{\mathbf{y}} \in \mathbf{Y}_{\mathbf{X}}}{\operatorname{argmax}} s(\mathbf{X}, \widetilde{\mathbf{y}})<br>\end{equation}</p>
<div><div class="fold_hider"><div class="close hider_title">开/合</div></div><div class="fold">
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cal_lstm_crf_loss</span><span class="params">(crf_scores, targets, tag2id)</span>:</span></span><br><span class="line">    <span class="string">"""计算双向LSTM-CRF模型的损失</span></span><br><span class="line"><span class="string">    该损失函数的计算可以参考:https://arxiv.org/pdf/1603.01360.pdf</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    pad_id = tag2id.get(<span class="string">'&lt;pad&gt;'</span>)</span><br><span class="line">    start_id = tag2id.get(<span class="string">'&lt;start&gt;'</span>)</span><br><span class="line">    end_id = tag2id.get(<span class="string">'&lt;end&gt;'</span>)</span><br><span class="line"></span><br><span class="line">    device = crf_scores.device</span><br><span class="line"></span><br><span class="line">    <span class="comment"># targets:[B, L] crf_scores:[B, L, T, T]</span></span><br><span class="line">    batch_size, max_len = targets.size()</span><br><span class="line">    target_size = len(tag2id)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># mask = 1 - ((targets == pad_id) + (targets == end_id))  # [B, L]</span></span><br><span class="line">    mask = (targets != pad_id)</span><br><span class="line">    lengths = mask.sum(dim=<span class="number">1</span>)</span><br><span class="line">    targets = indexed(targets, target_size, start_id)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># # 计算Golden scores方法１</span></span><br><span class="line">    <span class="comment"># import pdb</span></span><br><span class="line">    <span class="comment"># pdb.set_trace()</span></span><br><span class="line">    targets = targets.masked_select(mask)  <span class="comment"># [real_L]</span></span><br><span class="line"></span><br><span class="line">    flatten_scores = crf_scores.masked_select(</span><br><span class="line">        mask.view(batch_size, max_len, <span class="number">1</span>, <span class="number">1</span>).expand_as(crf_scores)</span><br><span class="line">    ).view(<span class="number">-1</span>, target_size*target_size).contiguous()</span><br><span class="line"></span><br><span class="line">    golden_scores = flatten_scores.gather(</span><br><span class="line">        dim=<span class="number">1</span>, index=targets.unsqueeze(<span class="number">1</span>)).sum()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算golden_scores方法２：利用pack_padded_sequence函数</span></span><br><span class="line">    <span class="comment"># targets[targets == end_id] = pad_id</span></span><br><span class="line">    <span class="comment"># scores_at_targets = torch.gather(</span></span><br><span class="line">    <span class="comment">#     crf_scores.view(batch_size, max_len, -1), 2, targets.unsqueeze(2)).squeeze(2)</span></span><br><span class="line">    <span class="comment"># scores_at_targets, _ = pack_padded_sequence(</span></span><br><span class="line">    <span class="comment">#     scores_at_targets, lengths-1, batch_first=True</span></span><br><span class="line">    <span class="comment"># )</span></span><br><span class="line">    <span class="comment"># golden_scores = scores_at_targets.sum()</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算all path scores</span></span><br><span class="line">    <span class="comment"># scores_upto_t[i, j]表示第i个句子的第t个词被标注为j标记的所有t时刻事前的所有子路径的分数之和</span></span><br><span class="line">    scores_upto_t = torch.zeros(batch_size, target_size).to(device) <span class="comment"># 凡是数据类型是 tensor 的，都是可以在后面加上 to(device) 的</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(max_len):</span><br><span class="line">        <span class="comment"># 当前时刻 有效的batch_size（因为有些序列比较短)</span></span><br><span class="line">        batch_size_t = (lengths &gt; t).sum().item()</span><br><span class="line">        <span class="keyword">if</span> t == <span class="number">0</span>:</span><br><span class="line">            scores_upto_t[:batch_size_t] = crf_scores[:batch_size_t,</span><br><span class="line">                                                      t, start_id, :]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># We add scores at current timestep to scores accumulated up to previous</span></span><br><span class="line">            <span class="comment"># timestep, and log-sum-exp Remember, the cur_tag of the previous</span></span><br><span class="line">            <span class="comment"># timestep is the prev_tag of this timestep</span></span><br><span class="line">            <span class="comment"># So, broadcast prev. timestep's cur_tag scores</span></span><br><span class="line">            <span class="comment"># along cur. timestep's cur_tag dimension</span></span><br><span class="line">            scores_upto_t[:batch_size_t] = torch.logsumexp(</span><br><span class="line">                crf_scores[:batch_size_t, t, :, :] +</span><br><span class="line">                scores_upto_t[:batch_size_t].unsqueeze(<span class="number">2</span>),</span><br><span class="line">                dim=<span class="number">1</span></span><br><span class="line">            )</span><br><span class="line">    all_path_scores = scores_upto_t[:, end_id].sum()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练大约两个epoch loss变成负数，从数学的角度上来说，loss = -logP</span></span><br><span class="line">    loss = (all_path_scores - golden_scores) / batch_size</span><br><span class="line">    <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure>

</div></div>


<p>如果数据集比较小，那么是可以通过切分的方式获得一个 <code>batch size</code> 的数据的。其中的预先排序感觉有点意思，这样保证一个batch 中长度基本上是保持一致的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self, word_lists, tag_lists,</span></span></span><br><span class="line"><span class="function"><span class="params">          dev_word_lists, dev_tag_lists,</span></span></span><br><span class="line"><span class="function"><span class="params">          word2id, tag2id)</span>:</span></span><br><span class="line">    <span class="comment"># 对数据集按照长度进行排序</span></span><br><span class="line">    word_lists, tag_lists, _ = sort_by_lengths(word_lists, tag_lists)</span><br><span class="line">    dev_word_lists, dev_tag_lists, _ = sort_by_lengths(</span><br><span class="line">        dev_word_lists, dev_tag_lists)</span><br><span class="line"></span><br><span class="line">    B = self.batch_size</span><br><span class="line">    <span class="keyword">for</span> e <span class="keyword">in</span> range(<span class="number">1</span>, self.epoches+<span class="number">1</span>):</span><br><span class="line">        self.step = <span class="number">0</span></span><br><span class="line">        losses = <span class="number">0.</span></span><br><span class="line">        <span class="keyword">for</span> ind <span class="keyword">in</span> range(<span class="number">0</span>, len(word_lists), B): <span class="comment"># 非常直接得到了一个batch 的数据集</span></span><br><span class="line">            batch_sents = word_lists[ind:ind+B]</span><br><span class="line">            batch_tags = tag_lists[ind:ind+B]</span><br><span class="line">            losses += self.train_step(batch_sents,</span><br><span class="line">                                      batch_tags, word2id, tag2id)</span><br><span class="line">            <span class="keyword">if</span> self.step % TrainingConfig.print_step == <span class="number">0</span>:</span><br><span class="line">                total_step = (len(word_lists) // B + <span class="number">1</span>)</span><br><span class="line">                print(<span class="string">"Epoch &#123;&#125;, step/total_step: &#123;&#125;/&#123;&#125; &#123;:.2f&#125;% Loss:&#123;:.4f&#125;"</span>.format(</span><br><span class="line">                    e, self.step, total_step,</span><br><span class="line">                    <span class="number">100.</span> * self.step / total_step,</span><br><span class="line">                    losses / self.print_step</span><br><span class="line">                ))</span><br><span class="line">                losses = <span class="number">0.</span></span><br><span class="line">        <span class="comment"># 每轮结束测试在验证集上的性能，保存最好的一个</span></span><br><span class="line">        val_loss = self.validate(</span><br><span class="line">            dev_word_lists, dev_tag_lists, word2id, tag2id)</span><br><span class="line">        print(<span class="string">"Epoch &#123;&#125;, Val Loss:&#123;:.4f&#125;"</span>.format(e, val_loss))</span><br></pre></td></tr></table></figure>






<p>所以LSTM-CRF最大的特点就是：由LSTM提供特征，而且特征是有参数的，是可以学习的！因此它可能根据不同问题学到各种合适的底层特征；而CRF的特征是人工定义出来的，不可变的，我们最多改改这个特征的参数。</p>
<p>（6） ensemble多个模型 </p>
<p>这个ensemble机制也是比较简单，就是众人投票机制。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ensemble_evaluate</span><span class="params">(results, targets, remove_O=False)</span>:</span></span><br><span class="line">    <span class="string">"""ensemble多个模型"""</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(results)):</span><br><span class="line">        results[i] = flatten_lists(results[i])</span><br><span class="line"></span><br><span class="line">    pred_tags = []</span><br><span class="line">    <span class="keyword">for</span> result <span class="keyword">in</span> zip(*results):</span><br><span class="line">        ensemble_tag = Counter(result).most_common(<span class="number">1</span>)[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line">        pred_tags.append(ensemble_tag)</span><br><span class="line"></span><br><span class="line">    targets = flatten_lists(targets)</span><br><span class="line">    <span class="keyword">assert</span> len(pred_tags) == len(targets)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"Ensemble 四个模型的结果如下："</span>)</span><br><span class="line">    metrics = Metrics(targets, pred_tags, remove_O=remove_O)</span><br><span class="line">    metrics.report_scores()</span><br><span class="line">    metrics.report_confusion_matrix()</span><br></pre></td></tr></table></figure>



<p>其他函数。</p>
<p>下面有两个函数，第一个函数是能够递归的处理多层（三层及以上）的嵌套，第二个函数只能处理两层的嵌套。本质是需要采取递归的思路。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">t=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,[<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,[<span class="number">5</span>,<span class="number">4</span>,<span class="number">3</span>],<span class="number">5</span>],<span class="number">1</span>,<span class="number">2</span>,[<span class="number">4</span>,<span class="number">5</span>],<span class="number">7</span>,<span class="number">4</span>,[<span class="number">6</span>,<span class="number">34</span>]]</span><br><span class="line">res =[]</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">flatten_list</span><span class="params">(list1)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> list1:</span><br><span class="line">        <span class="keyword">if</span> type(item) ==list:</span><br><span class="line">            flatten_list(item)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            res.append(item)</span><br><span class="line"></span><br><span class="line">flatten_list(t)</span><br><span class="line">print(res)</span><br><span class="line">            </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">flatten_lists</span><span class="params">(lists)</span>:</span></span><br><span class="line">    flatten_list = []</span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> lists:</span><br><span class="line">        <span class="keyword">if</span> type(l) == list:</span><br><span class="line">            flatten_list += l</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            flatten_list.append(l)</span><br><span class="line">    <span class="keyword">return</span> flatten_list</span><br><span class="line">print(flatten_lists(t))</span><br></pre></td></tr></table></figure>



<p>（7）评估模型</p>
<p>主要是针对precision， recall 和F1 的实现问题。按照行输出混淆矩阵。</p>
<div><div class="fold_hider"><div class="close hider_title">开/合</div></div><div class="fold">
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Metrics</span><span class="params">(object)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, golden_tags, predict_tags, remove_0 =False)</span>:</span></span><br><span class="line">        </span><br><span class="line">        self.golden_tags = flattent_list(golen_tags)</span><br><span class="line">        self.predict_tags =flattent_list(predict_tags)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> remove_0:</span><br><span class="line">            self._remove_0tags()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 辅助性的计算变量</span></span><br><span class="line">        self.tagset = set(self.golden_tags)</span><br><span class="line">        <span class="comment"># true positive </span></span><br><span class="line">        self.correct_tags_number = self.count_correct_tags()</span><br><span class="line">        </span><br><span class="line">        self.predict_tags_counter =Counter(self.predict_tags)</span><br><span class="line">        self.golden_tags_counter =Counter(self.golden_tags)</span><br><span class="line">        self.precision_scores =self.cal_precision()</span><br><span class="line">        self.recall_scores =self.cal_recall()</span><br><span class="line">        </span><br><span class="line">        self.f1_scores =self.cal_f1()</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">cal_precision</span><span class="params">(self)</span>:</span></span><br><span class="line">        </span><br><span class="line">        precision_scores =&#123;&#125;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> tag <span class="keyword">in</span> self.tagset:</span><br><span class="line">            precision_scores[tag] =self.correct_tags_number.get(tag, <span class="number">0</span>) /self.predict_tags_counter[tag]</span><br><span class="line">        <span class="keyword">return</span> precision_scores</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">cal_recall</span><span class="params">(self)</span>:</span></span><br><span class="line">        </span><br><span class="line">        recall_scores =&#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> tag <span class="keyword">in</span> self.tagset:</span><br><span class="line">            recall_scores[tag] =self.correct_tags_number.get(tag, <span class="number">0</span>) /self.golden_tags_counter[tag]</span><br><span class="line">        <span class="keyword">return</span> recall_scores</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">cal_f1</span><span class="params">(self)</span>:</span></span><br><span class="line">        f1_scores =&#123;&#125;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> tag <span class="keyword">in</span> self.tagset:</span><br><span class="line">            p, r =self.precision_scores[tag], self.recall_scores[tag]</span><br><span class="line">            f1_scores[tag] =<span class="number">2</span>*p*r /(p+r +<span class="number">1e-10</span>)</span><br><span class="line">        <span class="keyword">return</span> f1_scores</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">report_confusion_matrix</span><span class="params">(self)</span>:</span></span><br><span class="line">        </span><br><span class="line">        print(<span class="string">"Confusion Matrix:"</span>)</span><br><span class="line">        </span><br><span class="line">        tag_list =list(self.tagset)</span><br><span class="line">        tags_size =len(tag_list)</span><br><span class="line">        </span><br><span class="line">        matrix=[] <span class="comment"># 最后的结果就是一个二维矩阵</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(tags_size):</span><br><span class="line">            matrix.append([<span class="number">0</span>] * tags_size)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> golden_tag, predict_tag <span class="keyword">in</span> zip(self.golden_tags, self.predict_tags):</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                row =tag_list.index(golden_tag)</span><br><span class="line">                col =tag_list.index(predict_tag)</span><br><span class="line">                </span><br><span class="line">                matrix[row][col] +=<span class="number">1</span></span><br><span class="line">            <span class="keyword">except</span> ValueError: <span class="comment"># 极小的概率发生： 出现在gold tag 中，但是没有出现在predic tag中</span></span><br><span class="line">                <span class="keyword">continue</span> </span><br><span class="line">        </span><br><span class="line">        row_format_ =<span class="string">'&#123;:&gt;7&#125; '</span>*(tags_size+<span class="number">1</span>)</span><br><span class="line">        print(row_format_(<span class="string">""</span>, * tags_list))</span><br><span class="line">        <span class="comment"># 写得比较好的是以行为单位进行输出，这样至少看起来是非常美观的</span></span><br><span class="line">        <span class="keyword">for</span> i , row <span class="keyword">in</span> enumerate(matrix):</span><br><span class="line">            print(row_format_.format(tags_list[i], *row))</span><br><span class="line">            </span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_cal_weighted_average</span><span class="params">(self)</span>:</span></span><br><span class="line">        weighted_average =&#123;&#125;</span><br><span class="line">        total =len(self.golden_tags)</span><br><span class="line">        </span><br><span class="line">        weighted_average[<span class="string">'precision'</span>] =<span class="number">0.</span></span><br><span class="line">        weighted_average[<span class="string">'recall'</span>] =<span class="number">0.</span></span><br><span class="line">        weighted_average[<span class="string">'f1'</span>] =<span class="number">0.</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> tag <span class="keyword">in</span> self.tagset:</span><br><span class="line">            size =self.golden_tags_counter[tag]</span><br><span class="line">            weighted_average[<span class="string">'precision'</span> ] += self.precision_scores[tag] * size</span><br><span class="line">            weighted_average[<span class="string">'recall'</span>] += self.recall_scores[tag] *size</span><br><span class="line">            weighted_average[<span class="string">'f1'</span>] +=self.f1_score[tag] *size</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 这个加权平均和中的权重是 size长度</span></span><br><span class="line">        <span class="keyword">for</span> metric <span class="keyword">in</span> weighted_average.keys():</span><br><span class="line">            weighted_average[metric] /= total</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> weighted_average</span><br></pre></td></tr></table></figure>

</div></div>


<p>（8）常见的参数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TrainingConfig</span><span class="params">(object)</span>:</span></span><br><span class="line">    batch_size = <span class="number">64</span></span><br><span class="line">    <span class="comment"># 学习速率</span></span><br><span class="line">    lr = <span class="number">0.001</span></span><br><span class="line">    <span class="comment">#epoches = 30</span></span><br><span class="line">    epoches = <span class="number">5</span></span><br><span class="line">    print_step = <span class="number">5</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LSTMConfig</span><span class="params">(object)</span>:</span></span><br><span class="line">    emb_size = <span class="number">128</span>  <span class="comment"># 词向量的维数</span></span><br><span class="line">    hidden_size = <span class="number">128</span>  <span class="comment"># lstm隐向量的维数</span></span><br><span class="line">    </span><br><span class="line"><span class="comment"># 多说一句，最后out size 的维度是 len(tags)， 因为模型是为了给定一个字，然后predict 其 tag，所以这个是没有问题的； 实际上问的embedding size ，那么是想要考虑 word embedding的大小， 是 128</span></span><br><span class="line">out_size =</span><br></pre></td></tr></table></figure>



<p>使用DropOut 提高了最后的结果</p>
<blockquote>
<p>Initial experiments showed that character-level embeddings did not improve our overall performance when used in conjunction with pre-trained word representations. To encourage the model to depend on both representations, we use dropout training, applying a dropout mask to the final embedding layer just before the input to the bi-directional LSTM. We observe a significant improvement in our model’s performance after using dropout.</p>
</blockquote>
<p>（8）手写viterbi 算法</p>
<div><div class="fold_hider"><div class="close hider_title">开/合</div></div><div class="fold">
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HMM</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, N, M)</span>:</span></span><br><span class="line">        <span class="string">"""Args:</span></span><br><span class="line"><span class="string">            N: 状态数，这里对应存在的标注的种类</span></span><br><span class="line"><span class="string">            M: 观测数，这里对应有多少不同的字</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.N = N</span><br><span class="line">        self.M = M</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 状态转移概率矩阵 A[i][j]表示从i状态转移到j状态的概率</span></span><br><span class="line">        self.A = torch.zeros(N, N)</span><br><span class="line">        <span class="comment"># 观测概率矩阵, B[i][j]表示i状态下生成j观测的概率</span></span><br><span class="line">        self.B = torch.zeros(N, M)</span><br><span class="line">        <span class="comment"># 初始状态概率  Pi[i]表示初始时刻为状态i的概率</span></span><br><span class="line">        self.Pi = torch.zeros(N)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self, word_lists, tag_lists, word2id, tag2id)</span>:</span></span><br><span class="line">        <span class="string">"""HMM的训练，即根据训练语料对模型参数进行估计,</span></span><br><span class="line"><span class="string">           因为我们有观测序列以及其对应的状态序列，所以我们</span></span><br><span class="line"><span class="string">           可以使用极大似然估计的方法来估计隐马尔可夫模型的参数</span></span><br><span class="line"><span class="string">        参数:</span></span><br><span class="line"><span class="string">            word_lists: 列表，其中每个元素由字组成的列表，如 ['担','任','科','员']</span></span><br><span class="line"><span class="string">            tag_lists: 列表，其中每个元素是由对应的标注组成的列表，如 ['O','O','B-TITLE', 'E-TITLE']</span></span><br><span class="line"><span class="string">            word2id: 将字映射为ID</span></span><br><span class="line"><span class="string">            tag2id: 字典，将标注映射为ID</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">assert</span> len(tag_lists) == len(word_lists)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 估计转移概率矩阵</span></span><br><span class="line">        <span class="keyword">for</span> tag_list <span class="keyword">in</span> tag_lists:</span><br><span class="line">            seq_len = len(tag_list)</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(seq_len - <span class="number">1</span>):</span><br><span class="line">                current_tagid = tag2id[tag_list[i]]</span><br><span class="line">                next_tagid = tag2id[tag_list[i+<span class="number">1</span>]]</span><br><span class="line">                self.A[current_tagid][next_tagid] += <span class="number">1</span></span><br><span class="line">        <span class="comment"># 问题：如果某元素没有出现过，该位置为0，这在后续的计算中是不允许的</span></span><br><span class="line">        <span class="comment"># 解决方法：我们将等于0的概率加上很小的数</span></span><br><span class="line">        self.A[self.A == <span class="number">0.</span>] = <span class="number">1e-10</span></span><br><span class="line">        self.A = self.A / self.A.sum(dim=<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 估计观测概率矩阵</span></span><br><span class="line">        <span class="keyword">for</span> tag_list, word_list <span class="keyword">in</span> zip(tag_lists, word_lists):</span><br><span class="line">            <span class="keyword">assert</span> len(tag_list) == len(word_list)</span><br><span class="line">            <span class="keyword">for</span> tag, word <span class="keyword">in</span> zip(tag_list, word_list):</span><br><span class="line">                tag_id = tag2id[tag]</span><br><span class="line">                word_id = word2id[word]</span><br><span class="line">                self.B[tag_id][word_id] += <span class="number">1</span></span><br><span class="line">        self.B[self.B == <span class="number">0.</span>] = <span class="number">1e-10</span></span><br><span class="line">        self.B = self.B / self.B.sum(dim=<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 估计初始状态概率</span></span><br><span class="line">        <span class="keyword">for</span> tag_list <span class="keyword">in</span> tag_lists:</span><br><span class="line">            init_tagid = tag2id[tag_list[<span class="number">0</span>]]</span><br><span class="line">            self.Pi[init_tagid] += <span class="number">1</span></span><br><span class="line">        self.Pi[self.Pi == <span class="number">0.</span>] = <span class="number">1e-10</span></span><br><span class="line">        self.Pi = self.Pi / self.Pi.sum()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">(self, word_lists, word2id, tag2id)</span>:</span></span><br><span class="line">        pred_tag_lists = []</span><br><span class="line">        <span class="keyword">for</span> word_list <span class="keyword">in</span> word_lists:</span><br><span class="line">            pred_tag_list = self.decoding(word_list, word2id, tag2id)</span><br><span class="line">            pred_tag_lists.append(pred_tag_list)</span><br><span class="line">        <span class="keyword">return</span> pred_tag_lists</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">decoding</span><span class="params">(self, word_list, word2id, tag2id)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        使用维特比算法对给定观测序列求状态序列， 这里就是对字组成的序列,求其对应的标注。</span></span><br><span class="line"><span class="string">        维特比算法实际是用动态规划解隐马尔可夫模型预测问题，即用动态规划求概率最大路径（最优路径）</span></span><br><span class="line"><span class="string">        这时一条路径对应着一个状态序列</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># 问题:整条链很长的情况下，十分多的小概率相乘，最后可能造成下溢</span></span><br><span class="line">        <span class="comment"># 解决办法：采用对数概率，这样源空间中的很小概率，就被映射到对数空间的大的负数</span></span><br><span class="line">        <span class="comment">#  同时相乘操作也变成简单的相加操作</span></span><br><span class="line">        A = torch.log(self.A)</span><br><span class="line">        B = torch.log(self.B)</span><br><span class="line">        Pi = torch.log(self.Pi)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 初始化 维比特矩阵viterbi 它的维度为[状态数, 序列长度]</span></span><br><span class="line">        <span class="comment"># 其中viterbi[i, j]表示标注序列的第j个标注为i的所有单个序列(i_1, i_2, ..i_j)出现的概率最大值</span></span><br><span class="line">        seq_len = len(word_list)</span><br><span class="line">        viterbi = torch.zeros(self.N, seq_len)</span><br><span class="line">        <span class="comment"># backpointer是跟viterbi一样大小的矩阵</span></span><br><span class="line">        <span class="comment"># backpointer[i, j]存储的是 标注序列的第j个标注为i时，第j-1个标注的id</span></span><br><span class="line">        <span class="comment"># 等解码的时候，我们用backpointer进行回溯，以求出最优路径</span></span><br><span class="line">        backpointer = torch.zeros(self.N, seq_len).long()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># self.Pi[i] 表示第一个字的标记为i的概率</span></span><br><span class="line">        <span class="comment"># Bt[word_id]表示字为word_id的时候，对应各个标记的概率</span></span><br><span class="line">        <span class="comment"># self.A.t()[tag_id]表示各个状态转移到tag_id对应的概率</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 所以第一步为</span></span><br><span class="line">        start_wordid = word2id.get(word_list[<span class="number">0</span>], <span class="literal">None</span>)</span><br><span class="line">        Bt = B.t()</span><br><span class="line">        <span class="keyword">if</span> start_wordid <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># 如果字不再字典里，则假设状态的概率分布是均匀的</span></span><br><span class="line">            bt = torch.log(torch.ones(self.N) / self.N)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            bt = Bt[start_wordid]</span><br><span class="line">        viterbi[:, <span class="number">0</span>] = Pi + bt</span><br><span class="line">        backpointer[:, <span class="number">0</span>] = <span class="number">-1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 递推公式：</span></span><br><span class="line">        <span class="comment"># viterbi[tag_id, step] = max(viterbi[:, step-1]* self.A.t()[tag_id] * Bt[word])</span></span><br><span class="line">        <span class="comment"># 其中word是step时刻对应的字</span></span><br><span class="line">        <span class="comment"># 由上述递推公式求后续各步</span></span><br><span class="line">        <span class="keyword">for</span> step <span class="keyword">in</span> range(<span class="number">1</span>, seq_len):</span><br><span class="line">            wordid = word2id.get(word_list[step], <span class="literal">None</span>)</span><br><span class="line">            <span class="comment"># 处理字不在字典中的情况</span></span><br><span class="line">            <span class="comment"># bt是在t时刻字为wordid时，状态的概率分布</span></span><br><span class="line">            <span class="keyword">if</span> wordid <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                <span class="comment"># 如果字不再字典里，则假设状态的概率分布是均匀的</span></span><br><span class="line">                bt = torch.log(torch.ones(self.N) / self.N)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                bt = Bt[wordid]  <span class="comment"># 否则从观测概率矩阵中取bt</span></span><br><span class="line">            <span class="keyword">for</span> tag_id <span class="keyword">in</span> range(len(tag2id)):</span><br><span class="line">                max_prob, max_id = torch.max(</span><br><span class="line">                    viterbi[:, step<span class="number">-1</span>] + A[:, tag_id],</span><br><span class="line">                    dim=<span class="number">0</span></span><br><span class="line">                )</span><br><span class="line">                viterbi[tag_id, step] = max_prob + bt[tag_id]</span><br><span class="line">                backpointer[tag_id, step] = max_id</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 终止， t=seq_len 即 viterbi[:, seq_len]中的最大概率，就是最优路径的概率</span></span><br><span class="line">        best_path_prob, best_path_pointer = torch.max(</span><br><span class="line">            viterbi[:, seq_len<span class="number">-1</span>], dim=<span class="number">0</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 回溯，求最优路径</span></span><br><span class="line">        best_path_pointer = best_path_pointer.item()</span><br><span class="line">        best_path = [best_path_pointer]</span><br><span class="line">        <span class="keyword">for</span> back_step <span class="keyword">in</span> range(seq_len<span class="number">-1</span>, <span class="number">0</span>, <span class="number">-1</span>):</span><br><span class="line">            best_path_pointer = backpointer[best_path_pointer, back_step]</span><br><span class="line">            best_path_pointer = best_path_pointer.item()</span><br><span class="line">            best_path.append(best_path_pointer)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将tag_id组成的序列转化为tag</span></span><br><span class="line">        <span class="keyword">assert</span> len(best_path) == len(word_list)</span><br><span class="line">        id2tag = dict((id_, tag) <span class="keyword">for</span> tag, id_ <span class="keyword">in</span> tag2id.items())</span><br><span class="line">        tag_list = [id2tag[id_] <span class="keyword">for</span> id_ <span class="keyword">in</span> reversed(best_path)]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> tag_list</span><br></pre></td></tr></table></figure>

</div></div>




<h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p>下面结果是不同类别加权平均和得来的。</p>
<table>
<thead>
<tr>
<th></th>
<th>hmm</th>
<th>crf</th>
<th>bi-lstm</th>
<th>bilstm+crf</th>
<th>ensemble</th>
</tr>
</thead>
<tbody><tr>
<td>precision</td>
<td>91.49%</td>
<td>95.43%</td>
<td>95.37%</td>
<td>95.74%</td>
<td>95.69%</td>
</tr>
<tr>
<td>recall</td>
<td>91.22%</td>
<td>95.43%</td>
<td>95.32%</td>
<td>95.72%</td>
<td>95.65%</td>
</tr>
<tr>
<td>F1</td>
<td>91.30%</td>
<td>95.42%</td>
<td>95.32%</td>
<td>95.70%</td>
<td>95.64%</td>
</tr>
</tbody></table>
<p>可以看出<code>bilstm+crf</code> 得到了最好的结果 $F1=95.70%$，当使用 ensemble时候，并不见得能够得到更好的结果，尤其是当子模型有较大的重合度且模型效果不是很好的时候。</p>
<p>数据特点：</p>
<p>（1）IOBES标注方式更加细化，相应的label 预测要求更加高。<br>作者提到了两种tagging scheme，一种是IOB标注形式的，这种形式标注集为{B、I、O}，B表示命名实体的开头词，I表示命名实体非开始的词，O表示非命名实体词。另一个中是IOBES标注形式，标注集合为{B、I、E、O、S}，添加的E表示命名实体结尾词，S表示单个词的命名实体。</p>
<p>优化点：</p>
<p>（1）CRF 模型中自定义更加丰富的特征函数，相对于HMM而言，能够表示上下文。<br>（2）损失函数是交叉熵。ilstm+crf 中参考的是16年一篇论文中计算$y_{pred}$，但是那篇论文中的数据集是针对外文的（英文，德文）等，主要思想是加上了 $A_{i,j}$ 转换约束<br>CRF： 通过上述的双向LSTM获得整个句子的表示后，一个简单有效的标记方法就是独立地为每个单词输出标签。尽管这种方法在简单任务如POS上很成功，但是它在输出标签有强相关性时有很大的局限性。NER就是这样，输出标签是有强相关性的，比如I-PER后不能接B-LOC。因此，使用条件随机场CRF来解决这个问题。</p>
<p>展望：</p>
<p>（1）基于少量的标注数据进行NER 也是研究的热点。一种常见的思路是使用海量无标注数据训练一个语言模型，然后使用这个训练好的语言模型获取当前要标注词的语言模型向量，然后将这些向量加入到原始的双向LSTM-RNN 的模型中。实验结果表明，加入语言模型向量可以提高NER 的效果。</p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/12/17/graph_algorithms/" rel="next" title="图相关算法">
                <i class="fa fa-chevron-left"></i> 图相关算法
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/12/24/vpn/" rel="prev" title="科学上网指北 (VPN)">
                科学上网指北 (VPN) <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="https://disqus.com/?ref_noscript" target="_blank" rel="noopener">comments powered by Disqus.</a>
        </noscript>
      </div>
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/avatar.jpeg"
                alt="Jijeng Jia" />
            
              <p class="site-author-name" itemprop="name">Jijeng Jia</p>
              <p class="site-description motion-element" itemprop="description">Solving Problems by Coding</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/%20%7C%7C%20archive">
              
                  <span class="site-state-item-count">139</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">7</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">85</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/jijeng" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:jia1509309698@163.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          

          <script type="text/javascript" src="//rf.revolvermaps.com/0/0/5.js?i=5kk0u0mm50w&amp;m=0&amp;c=54ff00&amp;cr1=ff0000" async="async"></script>


        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#数据"><span class="nav-number">1.</span> <span class="nav-text">数据</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#代码"><span class="nav-number">2.</span> <span class="nav-text">代码</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#实验结果"><span class="nav-number">3.</span> <span class="nav-text">实验结果</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Jijeng Jia</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>

<div>
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
Total  <span id="busuanzi_value_site_pv"></span> views
You got  <span id="busuanzi_value_site_uv"></span> visitors
</div>




        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      
    </span>
  
</div>








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  

    
      <script id="dsq-count-scr" src="https://https-jijeng-github-io.disqus.com/count.js" async></script>
    

    
      <script type="text/javascript">
        var disqus_config = function () {
          this.page.url = 'http://yoursite.com/2019/12/21/name_entity_recognition/';
          this.page.identifier = '2019/12/21/name_entity_recognition/';
          this.page.title = '基于中文简历的命名实体识别';
        };
        var d = document, s = d.createElement('script');
        s.src = 'https://https-jijeng-github-io.disqus.com/embed.js';
        s.setAttribute('data-timestamp', '' + +new Date());
        (d.head || d.body).appendChild(s);
      </script>
    

  




	





  















  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
