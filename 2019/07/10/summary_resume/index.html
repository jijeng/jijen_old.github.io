<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="something">
<meta property="og:type" content="article">
<meta property="og:title" content="Summary">
<meta property="og:url" content="http://yoursite.com/2019/07/10/summary_resume/index.html">
<meta property="og:site_name" content="Jijeng&#39;s blog">
<meta property="og:description" content="something">
<meta property="og:locale" content="default">
<meta property="og:image" content="https://ftp.bmp.ovh/imgs/2019/12/c349ba9fc920bc70.png">
<meta property="og:image" content="https://ftp.bmp.ovh/imgs/2019/09/3baa1391fe380171.png">
<meta property="og:updated_time" content="2019-12-22T11:35:36.135Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Summary">
<meta name="twitter:description" content="something">
<meta name="twitter:image" content="https://ftp.bmp.ovh/imgs/2019/12/c349ba9fc920bc70.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2019/07/10/summary_resume/"/>







<script>
	(function(){
		if('hexo149'){
			if (prompt('请输入文章密码','') !== 'hexo149'){
				alert('密码错误！');
				history.back();
			}
		}
	})();
</script>

  <title>Summary | Jijeng's blog</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Jijeng's blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/07/10/summary_resume/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jijeng Jia">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jijeng's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Summary</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-07-10T17:10:11+08:00">
                2019-07-10
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">Post modified&#58;</span>
              
              <time title="Post modified" itemprop="dateModified" datetime="2019-12-22T19:35:36+08:00">
                2019-12-22
              </time>
            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NOT-FOR-YOU/" itemprop="url" rel="index">
                    <span itemprop="name">NOT_FOR_YOU</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/07/10/summary_resume/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2019/07/10/summary_resume/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="page-pv"><i class="fa fa-file-o"></i>
            <span class="busuanzi-value" id="busuanzi_value_page_pv" ></span>
            </span>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>something</p>
<a id="more"></a>
<h3 id="总的大纲"><a href="#总的大纲" class="headerlink" title="总的大纲"></a>总的大纲</h3><ol>
<li>面试的时候还是尽量控制一下，放慢语速，别着急，想清楚了再好好说；(已经找不上工作了，别慌)</li>
<li>表现欲要控制好，不要得意忘形，点到为止，不然容易挖更多的坑。</li>
<li>这次有一点压力面的，遇到这种情况，心态积极开放一些，反正我是全程笑嘻嘻，谦虚请教的态度，做的不好就承认，虚心接受，质疑的地方也要敢于拿实力证明，毕竟还年轻，有很大的进步空间。（就是在心态上一定要是平和的，不卑不亢）</li>
</ol>
<h3 id="爱奇艺实习经历"><a href="#爱奇艺实习经历" class="headerlink" title="爱奇艺实习经历"></a>爱奇艺实习经历</h3><p>面试官好，我叫贾继征，来自北京航空航天大学计算机学院。在研究生期间…<br>总的来说之前分别在腾讯和爱奇艺实习过一段时间，分别做短文本相似度计算和文本分类、然后以第一作者身份分别发表了一篇C 类会议和EI 会议，一个涉及的nlp 文本分类问题，一个是深度学习领域中的gan 网络。做过一些关于机器学习项目和NLP项目，nlp 项目主要是处理短文本，句子的相似度计算和文章的判重，完成了一个类似faker 的开源工具。</p>
<p>从后往前说，<br>爱奇艺实习经历：<br>实习内容的背景<br>使用nlp 和机器学习知识对推荐中query的识别，实际上做到后来不只是是低俗内容的识别，一开始是用于软色情的判断，后来模型加大了识别的范围，凡是一切不适合出现在推荐中的query，都需要进行判断。比如暴力，不符合积极的正面宣传内容都会进行识别。</p>
<p>如何实现？</p>
<p>训练数据集300万，有标注， 分别使用 1 0 表示低俗和非低俗数据，人工标注，能有95%的准确率；两个测试数据集分别是1万（从一个月中的query 抽选出来的数据，一个是原始的数据，一个是经过规则筛选的测试数据集） 。训练数据集中正负样本1 ：1.5，但在测试集中使用的是一个月中的数据，所以正负样本比较高，达到了18：1。正常query 是低俗query 的18倍。</p>
<p>模型有两部分组成。一部分是基于transformer 的语义抽取，一部分是基于xgboost 进行的特征提取，训练过程中优化两个loss。<br> 为什么使用 xgboost 机器学习进行特征提取呢？因为这里使用到了其他维度的信息，而这些信息是有助于对query 进行判别的。分成三部分：<br>使用 fasttext 获得query 的n-gram 特征，常规word embedding<br>query 和channel 的特征，对于色情这个维度，影视channel 和知识类 channel是有不同的影响的<br>使用DSSM（获得是query 和item 之间点击的信息，得到的模型中也是计算相似度的）<br>这些特征都是 一个transformer 没有办法获得的</p>
<p>然后使用xgboost +LR 模型作为这一部分的网络结构。</p>
<p>另一部分相对来说就比较简单，使用 transformer作为一个语义识别的模型。<br>然后得到的两个loss，在训练过程中一块进行训练。</p>
<p>线下版本因为正负样本比较均匀，直接使用AUC 作为评价指标，auc 很高了，能够达到0.95左右了，有点过拟合了。在某种意义上就是想要过拟合，这可以很好的处理一些case 的问题，但是模型的优化的目标不是这样的，是希望在测试数据集上效果好的。AUC是 0.89。</p>
<p>线上因为正负样本的问题，使用F1 作为评价指标，使用字向量和拼音向量之后又 4%的提升。（0.79-0.83 ）</p>
<p>难点<br>对于query 的理解应该从字面意思、内涵和搜索意图进行全方位的理解。字面可以使用规则进行判断，但是一些词语（比如韩国办公室沙发，公公和儿媳妇）这种很难使用单一的规则进行判断。应该根据语义信息进行判断。<br>query 中还有一点，可能出现拼音混合汉字，错别字等现象。这种现象的存在意味着不能使用传统的分词工具进行处理，因为这个是分不准的。我们的处理方案是”字向量“+ ”拼音向量“用来补充原来的词向量。</p>
<p>线上版本是有要求的，是要保证precision在 0.9为底线的情况下，然后去提高recall，抱有的是宁缺毋滥的观念，绝对不能是推荐出来不好的query。</p>
<p>可能存在的问题：<br>数据标准 定的标准是95%，虽然人工标注这种精度已经很高，但是还是有error的，并且这种error 在短期内是不太可能有大的提升的。<br>线上和线下正负样本比不一致，也是误差来源之一<br>模型依赖于别的团队的channel embedding和DSSM embedding，也可能是一个误差的来源。自己训练的fasttext 的word embedding<br>precision (0.92)很高，recall(0.6~7) 的样子，说明正样本的特征并不明显，可能的一种解释是中性词，歧义词，存在二义的词语比较多（当然这种假设是可以验证的，可以看一下false positive rate ) 预测成了假的）。所以可以进一步去挖掘正样本（低俗词汇）的特征；这种特征就是上面说的可能是query 的语义和搜索意图，而不单单是quer 本身的意思。<br>后期的改进：</p>
<p>后期打算，加上万能的bert，然后三个loss 进行一块训练，应该是会得到更好的query 的表示。bert 是一个预训练模型。</p>
<p>正负样本是一个相对的概念，对于该研究问题，那么低俗词汇就是正样本，负样本就是正常的词汇。</p>
<p>Bad case 分析：<br>主要分成三类</p>
<ol>
<li>本身label 就有问题， 比如case “爱死亡与机器人”(这个应该是电影) 被打成1，然而预测是0；”口疮咋治疗”  被label 成1，然而预测是0.</li>
<li>二分类错误。比如” ”武则天用的什么安慰器”” 被label 成1，然而预测为0；</li>
<li>Query 本身不太好分类的。比如” 女人腿肥怎么办” 被label 成0，但是预测为1；”吃吐了” 被label 成1，但是预测为0.</li>
</ol>
<p>我觉得可以对bad case再进行一下划分，不是所有的bad case 都需要处理的。具体而言，第二种情况是应该处理的。第一种情况是打标签的准确度问题，第三种情况是需要看具体语境了，并不是一种很明显地可以划分为低俗与否的问题。<br>应该着重的看看一下第二种情况。</p>
<p>改进的地方：<br>有一些中性词，比如”爱死亡与机器人“ 是一个中性词，但是在label 的时候就被标记错误了，所以对于这类词语应该怎么办？</p>
<h3 id="腾讯新闻的实习经历"><a href="#腾讯新闻的实习经历" class="headerlink" title="腾讯新闻的实习经历"></a>腾讯新闻的实习经历</h3><ol>
<li>绝对提高和相对提高<br>假如之前的水平是a，然后之后的水平是b，那么绝对提高就是 (b -a); 相对提高 (b -a) /a *100\% .</li>
</ol>
<p>CTR最开始是多少？这个时间有点长了，具体忘记了，但前后模型是有增长效果的。</p>
<ol start="2">
<li><p>faiss 的介绍<br>首先使用 index对于向量进行预处理，然后选择不同的模式. 主要讲的是三种模式，一个维度是简单模式，适合在小数据上进行计算 欧氏距离；一个维度是加快检索速度，这种模式下是需要提前的train，其基本的思路对向量进行聚类，当然文中说的是 “细胞”，建立倒排索引，然后检索的时候，搜索这个“细胞”内 和周围的“细胞” 的id 的集合，就可以返回前 K 个最相近的结果；最后一个维度是减少内存的使用，上面两种都是使用的完整的向量，这个模式下是使用的压缩向量，可以使用PCA 进行实现，当然这个模式下得到的结果也是近似解。还有两种计算的上的优化，对于向量进行分段计算，这种可以实现并行，并且支持任务在GPU 上进行运算。（一般的情况是文档id 作为索引，文档的内容作为记录；倒排索引，是文章内容关键字作为索引，文档id作为记录）</p>
</li>
<li><p>什么是 AB test?<br>AB test 测试强调的是在同一时间维度对相似属性分组用户的测试，时间的统一性有效的规避了因为时间、季节因素带来的影响，属性的相似性（vv）则使得地域、性别、年龄等其他因素对统计的影响降为最低。对于测试的流量不宜太大，应该逐步增大流量，同时如果太小又有随机性的干扰。时间上保证是完整的一周，包括工作日和周末。AB test 也是一个迭代的过程。AB test 有两种情况，一种是界面 UI的修改，一种是后台算法的修改。在对产品进行A/B测试时，我们可以为同一个优化目标（例如优化购买转化率）制定两个方案（比如两个页面），让一部分用户使用A 方案，同时另一部分用户使用 B 方案，统计并对比不同方案的转化率、点击量、留存率等指标，以判断不同方案的优劣并进行决策，从而提升转化率。ab test 存在缺点，如果只是单纯的选择用户，那么可能存在观看视频习惯不同对最后结论产生不利的影响。所以要尽可能的选择属性相同的用户，挑选出活跃用户（vv达到某一个值的用户）。分别施以模型A和B，才能验证模型A的效果。还要一种思路，如何在不分组的情况下，进行测试呢？给用户两个算法召回的结果（等概率出现），然后记录用户的行为数据，然后判断a b 两种方案的优劣。这种思路就排除了用户本身带来的误差。<br>summary：<br>同一个时间维度是容易保证的，但是属性的相似性比较难保证。这里有两种策略，一种是选择属性相同的用户，比如说挑选出北京是中活跃的用户，然后分成两组进行测试；一种是在一个地区的用户分别等概率的提供两种算法，最后看效果（比如说ctr ，留存，用户时长等）。</p>
</li>
</ol>
<ol start="3">
<li><p>句子向量的训练过程？<br>word embedding 表示的句子的缺点：<br>没有词序；没有上下文；依赖前期的处理<br>SIF 这种句子向量 （无监督，非网络结构）</p>
</li>
<li><p>如何进行 sentence 训练?<br>无监督模型，通过skip-though 举例说明：<br>skip-gram  根据中心词汇预测上下文<br>skip-thought  根据中心句 预测上下句<br>encoder -decoder<br>encoder 就是一个 特征提取，然后又两个decoder， 是语言模型 ，分别对应着 上一句和下一句<br>统计语言模型是一个单词序列上的概率分布，对于一个给定长度为m的序列，它可以为整个序列产生一个概率 $P(w_1,w_2,…,w_m) $ 。</p>
</li>
</ol>
<p><strong>词汇扩展</strong><br>使用 word2vec 得到的词向量 来补充 encoder 过程中 该问题的词向量，要求前者的规模是远远大于后者的。<br>通过这种方法补充 encoder 过程中的词汇。</p>
<p><strong>数据集</strong><br>数据集中的句子是有衔接关系的，论文中的数据集是 google news dataset。要求有一定的逻辑的文章。</p>
<p>有监督模型：<br>infersent</p>
<p>综述类型，找到了 nlp 领域的imagenet 和 inception network。网络结构模型是 使用一个encoder 得到向量表示，然后记性 u 相加 和点乘 三种操作，得到 fully-connection 最后是softmax。encoder 是 bi-lstm 并且使用 max pooling 进行网络结构的优化。数据集是 句子对组成的数据集，标注信息是数据间的关系（蕴含，对立和中性）数据量 570k。</p>
<p>具体实验对比的是 RNN lstm gru 然后搭配着 单向or 双向 max pooling or average pooling 进行做的实验。</p>
<p>universal sentence encoder</p>
<p>上面那个是 facebook 团队出的，然后这个是 google 出的，主要的区别在于使用 transformer 代替bi-lstm 作为encoder。 训练的时候，既进行了有监督的学习，又进行了无监督的训练。</p>
<ol start="5">
<li>图像处理和nlp 的差别：<br>离散  or 连续； 定长 or 非定长；稠密 or 稀疏。</li>
</ol>
<ol start="6">
<li>对于 bert transformer 的介绍？<br>bert 是集大成者，前面还有很多重要的研究成果，比如attention，self-attention 和transformer。直接从 transformer 说起，大的结构还是 encoder、decoder 的结构。encoder 是6 个两层结构， decoder 也是 6个三层结构。<br>transformer 中有几个技术点是需要讲解： layer normalization （主要 vs batch normalization）、position embedding （在encoder 中是结合了 word embedding 一块作为输入的）、masked attention（当predict t 时刻的word的时候，只能看到  t 之前的情况，将 t 之后的word 进行了 mask）、multi-head attention（类似 图像中的多个 filter ）、self-attention 就不多说了<br>从 transformer 到 bert 又是一个比较复杂的模型组合的工作。</li>
</ol>
<p>6.1 关于bert 和lstm 的比较<br>（解答这种问题需要有一个切入点，那么就是 lstm 的缺点）<br>lstm 是无法解决长依赖的问题，lstm 使用输入门，遗忘门和输出门可以缓解一定程度上的长依赖的问题，但是当句子更长的时候，那么也是无能为力的。 bert 是一个集大成这，其中大量使用了 transformer的结构，而transformer 结构是基于 attention机制，所以可以解决长依赖的问题。<br>除此之外，bert是可以在 GPU 上进行fine tune，那么意味着计算效率是比较高的，可以并行运算。当然还有其他的，比如 bert 是一个预训练模型，可以在特定的任务上进行fine tune 得到比较好的结果，比如我论文中的模型就是基于 bert 进行fine tune ，然后不需要怎么训练，效果就已经很好了。</p>
<p>效果好并且速度比较快，这个快是指的 fine-tune 的效果，不是从头开始训练的时间。（一般都是从 速度 和效果两方面评价模型的好坏）效果主要体现在表示语义，上下文信息；给定it 能够知道指代的关系</p>
<p>(能多说点就多说点，因为一个面试官的面试时间有限的，那么是需要尽可能在这个时间段里面让其少问问题，自己尽量把这个充实起来)</p>
<p>介绍transformer的时候，但是在这两层中间还有一层attention层，帮助当前节点获取到当前需要关注的重点内容。</p>
<p>正弦余弦函数用来编码绝对的位置信息，也是可以用来表示位置之间的相对的距离。</p>
<p>batch normalization layer normalization, instance normalization, group normalization</p>
<p>两种mask 技术： padding mask（输入数据） 和sequence mask（decoder）</p>
<p>decoder有三层结构 multi-head attention+ encoder-decoder attention 和前向传播<br>公式需要掌握：</p>
<p>$$<br>\text { Attention }(Q, K, V)=\operatorname{softmax}\left(\frac{Q K^{T}}{\sqrt{d_{k}}}\right) V<br>$$</p>
<p>lstm 相对于 rnn 是能够处理一部分的长依赖的，但是当句子更加的长的时候，效果也是不太好的；并且计算量比较大。</p>
<p>elmo 中使用了 n-gram（sub-words）的思想</p>
<p>bert 的两个训练任务，一个是随机遮蔽掉一个词，利用上下文进行预测；一个是预测下一个句子。处理句内关系和句间关系。第一个任务：随机选择15%的token，大多数的token 是被 mask 特殊字符取代；一部分是随机被其他单词取代；一部分是保留原词。目的是为了模型对上下文进行表示，减少训练集和test 集合中的数据分布的不统一。第二个任务：输入两个句子A和B，预测B 是否为A 的下一个句子，50% 是下一个句子，剩余的都不是。<br>目标是最小化两种策略的组合函数。</p>
<p>输入： token embedding词向量（肯定是subword的，这个都是标配）+ segment embedding区分两个句子 + position embedding表示位置信息。</p>
<p>更大的数据集：使用了bookcorpus 和维基百科中的数据</p>
<p>改进的空间：<br>从字模型到词模型，科大讯飞和哈工大有做这方面的研究<br>probability这个词被切分成”pro”、”#babi”和”#lity”3个WordPiece。有可能出现的一种随机Mask是把”#babi” Mask住，但是”pro”和”#lity”没有被Mask。这样的预测任务就变得容易了，因为在”pro”和”#lity”之间基本上只能是”#babi”了。这样它只需要记住一些词(WordPiece的序列)就可以完成这个任务，而不是根据上下文的语义关系来预测出来的。类似的中文的词”模型”也可能被Mask部分(其实用”琵琶”的例子可能更好，因为这两个字只能一起出现而不能单独出现)，这也会让预测变得容易。<br>为了解决这个问题，很自然的想法就是词作为一个整体要么都Mask要么都不Mask，这就是所谓的Whole Word Masking。</p>
<p>对于transformer 或者bert 本身的模型优化，这个模型还是有点复杂。</p>
<p>速度上计算的优势， bert 是基于transformer，而transformer是基于self-attention，self-attention 是可以连接任意两个结点，所以可以并行的运算。</p>
<p>bert 的</p>
<p>关于bert 和transformer中的输入（input）？</p>
<p>在 word2vec 中使用 one-hot 方式进行表示，vector 的长度就是词汇表 vocabulary 的长度。但是在transformer中并不这样做，因为这种方式过于稀疏。如果是在pytorch 中，经常使用 nn.Embedding 来做或者使用 one-hot 和权重矩阵 $W$ 相乘得到。nn.Embedding 包含一个权重矩阵 W，对应的 shape 为 ( num_embeddings，embedding_dim )。num_embeddings 指的是词汇量，即想要翻译的 vocabulary 的长度。embedding_dim 指的是想用多长的 vector 来表达一个词，可以任意选择，比如64，128，256，512等。在 Transformer 论文中选择的是512(即 d_model =512)。<br>处理 nn.Embedding 权重矩阵有两种选择：</p>
<ul>
<li>使用 pre-trained 的 embeddings 并固化，这种情况下实际就是一个 lookup table。</li>
<li>对其进行随机初始化(当然也可以选择 pre-trained 的结果)，但设为 trainable。这样在 training 过程中不断地对 embeddings 进行改进。<br>Transformer 选择的是后者。但是这个是没有位置信息的，所以加入了position embedding。</li>
</ul>
<p><a href="https://zhuanlan.zhihu.com/p/80986272" target="_blank" rel="noopener">10分钟带你深入理解Transformer原理及实现</a></p>
<ol start="7">
<li><p>关键词提取技术<br>卡方分布：卡方检验是以χ2分布为基础的一种常用假设检验方法。该检验的基本思想是：首先假设$H_0$（比如说某个特征和label 无关）成立，基于此前提计算出χ2值，它表示观察值与理论值之间的偏离程度。卡方分布中的参数F（自由度，（行数 - 1） * （列数 - 1）），有了自由度那么就可以得到对应表中的卡方值，然后得到原假设成立的概率。但是在实际应用中，只需要将最后的将最后的结果进行排序，选择最大的前K，就是最后的结果。缺点： 考虑的是出现与否，不是频率；没有考虑语意信息。</p>
</li>
<li><p>多线程，多进行<br>python 中的实现， 多线程是 ， 使用threading， 处理的是io 响应；多进程是Concurrency， 使用multiprocessing 包，处理的是多核cpu的操作。并发和并行，并发只是用户感觉，通过快速切换，保存现场实现；并行是真正的同一时刻运行着不同的程序。<br>对于io 绑定的任务，使用多进程可以提高性能；使用多进程也可以提高性能，但是开销往往比多线程高。比如说访问多个网站，使用多线程进行爬取网页的任务的效率是高于单个线程的效率。（大部分和io 绑定的程序 都是在等待输入输出，无所事事。比如由于网络延迟，导致来自网络 数据库 文件所花费的时间是远远大于 cpu 处理时间，那么这个时候使用多线程就比较nice）<br>对于cpu 绑定的任务，使用多线程往往可以降低性能，使用多进程可以提高性能。比如说就算 1w 以内的所有的质数，因为现在的cpu 都是多核的，可以调用多个处理模块，但是当生成的进程数量多于cpu 的处理能力，那么就观察到了性能的下降，因为这个时候使用更多的工作来换取cpu 内核内外的进程。</p>
</li>
</ol>
<ol start="9">
<li>新词发现<br>这里有三个阈值（都是越大越好）：<br>第一是最小互信息，因为互信息越大说明相关度越大，将n-gram分好的词计算互信息，如果低于阈值，则说明不能成词。<br>第二是最小熵值，因为熵也是越大说明周边词越丰富，计算其左熵和右熵的最小值，如果最小值低于阈值，则说明不能成词。<br>第三个是最少出现次数，为什么有这个数呢？假设前后两个词是完全相关的，出现400次，总共8000词，那么互信息=log((400/8000)/(400/8000)<em>(400/8000))，约掉之后剩下log(8000/400)。但是一个词如果从头到尾出现了一次，但是并不是单词，则互信息为=log((1/8000)/(1/8000)</em>(1/8000))=log(8000/1)，那么它的互信息会更大。取最少出现次数也会出现问题，就是一些低频率的词不能发现。<a href="https://jijeng.github.io/2019/03/25/NLP%E4%B8%AD%E7%9A%84%E7%A2%8E%E7%A2%8E%E5%BF%B5/" target="_blank" rel="noopener">有更加详细的公式</a></li>
</ol>
<ol start="10">
<li>ner 数据集</li>
</ol>
<p>从github 中有标注的数据集，5万的数据集，一条是20个字左右；人名，地名，机构名、专有名词和其他。在实习的时候使用旁边组的结果，但是后来有做个一个类似的项目，使用的是隐马尔科夫模型进行实体命名识别，正在做的有两个项目，一个是基于 bert + crf 的项目，一个是机器翻译 。</p>
<h3 id="文章去重"><a href="#文章去重" class="headerlink" title="文章去重"></a>文章去重</h3><p>参加的一个课题项目，针对某个特定的行业，比如房地产、互联网，基于大数据，对网页或者文章进行分析，使用数据挖掘的手段，捕捉热点，然后为企业或者个人提供事件或者品牌的情感分析、品牌分析。这个是大的项目背景。</p>
<p>我参与的是在于对网页或者网页的去重工作，因为之后的文章分析的指标计算，比如流行度，都是比较依赖于数据的准确性。数据的来源：新闻（搜狐网、新浪网）电子报纸，app 新闻，分成不同类别的新闻，比如科技、教育、金融和体育。每条数据由标题、站点、url、发布时间、内容、阅读量、评论量等组成，数据量是百万级别的。其中内容部分是由500 到2000不等的字数组成。步骤的话，先是文本的预处理，提取关键词，然后使用simhash 或者minhash 进行计算，重复文章的集合，最后去掉重复的文章。</p>
<p>优化：多线程，字典（有一些品牌类的，人名类，需要进行人工的添加、新词发现，网络词汇）</p>
<p>如何去考察最后去重的效果，一般来说，simhash 中的距离函数是汉明距离，这个在3 以内，都是可以认为是相似的文章。当然对于定量的评价的话，有一个标注的小的测试集，然后可以在上面进行 baseline的验证。</p>
<h3 id="Home-credit-default-risk"><a href="#Home-credit-default-risk" class="headerlink" title="Home credit default risk"></a>Home credit default risk</h3><p>问题描述：Home Credit收集了申请人当前的申请表、其他机构的信用数据和之前的申请记录；用这些信息预测申请人违约概率，是一个风控模型。<br>数据方面：原始训练集（30万*122），正负样本不均衡；进行如下操作（1）数据清洗（2）数据整合（3）数据转换（4）数据降维；<br>模型方面：基于LR模型和XGBoost模型，两个模型进行STACK；<br>训练方面：对数据10折交叉验证训练，使用Random Search调参；<br>评价指标：AUC为0.798。排名：Top 2%（110/3337）。</p>
<ol>
<li>数据方面<br>(1) 数据清洗<br>(2) 特征工程<br>1). 原始特征<br>包括原始特征和通过简单地额操作（加减乘除/分箱）构造的新特征。比如刻度收入和贷款总额的比率，客户家庭人均收入，客户年龄段，客户收入段<br>2). 统计特征<br>除了申请表之外，其他的表是以月为单位的数据，可以对时间序列进行聚合求解汇总统计量（mean, median, std,max, min, sum）<br>3). 时序特征<br>只对时间序列做统计会丢掉很多信息。从实际情况出发，客户近期的申请情况和信用情况相对比之前的更加重要，所以可以按照窗口划分：离本次申请最近的1一个月/2个月/三个月或者和6个月的信用情况。<br>4). 交互特征<br>对Top10的特征进行交互操作，比如说乘积、比值等，尝试是否能够交互出更加优秀的特征。<br><a href="https://jijeng.github.io/2019/03/25/Data-Pre-processing/" target="_blank" rel="noopener">Data Pre-processing 学习笔记</a> 已经总结得比较好了，可以多分析一下。</li>
</ol>
<p>如何处理正负样本不均匀的情况？<br>参考这里：(Unbalanced Datasets Problems)[<a href="https://jijeng.github.io/2019/06/01/unbalanced-datasets/]" target="_blank" rel="noopener">https://jijeng.github.io/2019/06/01/unbalanced-datasets/]</a></p>
<p>如何处理样本极端不均匀的情况？ 进行正负采样之后需要有什么操作吗，得到的是部分的数据的分布，而不是完整的？</p>
<ol start="2">
<li>模型方面</li>
</ol>
<p>xgboost<br>lr</p>
<p>GBDT<br>DT中常见的有三种算法：CART算法、ID3和C4.5算法。其中CART算法使用在GBDT 中，既可以处理回归问题也可以处理分类问题，回归问题使用均方差作为损失函数，对于二分类使用gini 系数作为损失函数。树在选择分裂点的时候，先是枚举特征字段，然后是枚举单个特征中可以分裂的点，然后计算gini系数，选择gini系数最小的那个作为分裂点。</p>
<p>Boosting 算法在每次迭代的过程中，从弱学习器通过弥补之前的学习器的不足，变成一个强学习器。著名的代表是adaboost 和gbdt。对于gbdt，如果是回归问题并且使用平方误差作为损失函数，那么每次都是在上一步的结果的残差进行训练，然后对数据进行分类和回归，是不断降低偏差来提高最后分类器的精度。当选择指数函数作为损失函数时候，gbdt 就是adaboost。</p>
<blockquote>
<p>Bagging allows multiple similar models with high variance are averaged to decrease variance. Boosting builds multiple incremental models to decrease the bias, while keeping variance small.<br>Bagging中的单个模型方差高，然后使用多个模型来降低总体的方差；Boosting 降低的是偏差，方差能够保持比较小。</p>
</blockquote>
<p>Stacking模型融合</p>
<p><img src="https://ftp.bmp.ovh/imgs/2019/12/c349ba9fc920bc70.png" alt=""></p>
<ul>
<li>We split the training data into K-folds just like K-fold cross-validation.</li>
<li>A base model is fitted on the K-1 parts and predictions are made for Kth part.</li>
<li>We do for each part of the training data.</li>
<li>The base model is then fitted on the whole train data set to calculate its performance on the test set.</li>
<li>We repeat the last 3 steps for other base models.</li>
<li>Predictions from the train set are used as features for the second level model.</li>
<li>Second level model is used to make a prediction on the test set.</li>
</ul>
<ol start="3">
<li>训练和验证</li>
</ol>
<p>使用10 折交叉验证划分数据集，然后进行训练。</p>
<ol start="4">
<li>模型融合</li>
</ol>
<p>bagging 是相对独立的训练同质强学习器， 通过有放回的采样，得到的一系列的弱学习器，然后通过组合得到一个强学习器。该方法是可以并行，通过减少偏差提高最后的效果，其中的代表是随机森林。<br>boosting是串行的同质学习器，在上一个模型的基础上，不断减少方差提高效果，只能串行，其中的代表是gbdt。实现是xgboost。</p>
<p>对于机器学习中的特征提取，可以从业务和统计学通用方法两方面入手。首先尽可能的从业务本身，提取更多可解释、稳定的特征。这个项目是一个小的风控模型，其中有一个特征是消费水平比上收入水平，一般来说消费水平对应的是人的消费观，主观意愿多花钱与否；收入水平是客观存在的，如果高收入人群，一般来说消费水平比较高。并且近几个月的消费水平和 长期的消费水平对于风控模型有着不同影响程度，一般来说近期的行为对模型影响更大。所以需要尽可能地懂数据，然后从这些维度提取可解释的特征。</p>
<p>人工调参是技术活，需要对模型和数据有个比价长的认识，一开始的时候常常使用Grid search 和Random search</p>
<p>这个也是可以展开的：<br>domain： 所有的超参数及其值 的dictionary（键值对）<br>Optimization algorithm:  如何去选择下一组 超参数。<br>目标函数：这个是模型中的目标函数<br>results history： 目标函数和 一组超参数的对应关系</p>
<p>关于 results history 的使用就是 random search 和基于贝叶斯方式搜索的区别，前者没有使用这种对应的关系，后者有利用这种关系。 贝叶斯，p(目标函数 | 一组超参数)。</p>
<p><strong>特征工程</strong></p>
<ol>
<li>特征来源一部分是业务场景，一部分是常规操作。常规操作包括aggregation聚合操作（主要针对子表），特征离散化，组合特征。业务特征，就是根据不同的场景，构造在该场景下重要字段。当然还有一些骚操作，使用机器模型进行特征的构造和选择，优点是work，缺点是可解释性差，比如使用xgboost +LR 模型，前者就是一种特征提取的功能，最后叶子结点的输出，知道其是重要的，但是不知道其含义是什么。</li>
<li>连续特征离散化的<strong>好处</strong>： (1)  增加了模型的非线性，提升了模型表达能力 (2) 离散化特征对异常数据具有很强的鲁棒性。<strong>常用的选取离散点的方法</strong>：(1) 等距离离散 (2) 等样本离散 (3) 画图 (4) 根据实际场景，比如对于年龄的划分</li>
<li>特征组合( 1)基本特征的非线性组合  (2) 特征之间的差和乘积商，mean,variance，std 统计学特征</li>
<li>特征选择（降维）的<strong>方法</strong>: (1) 特征本身 （如果缺省值比较大或者数据的波动比较小） (2) 特征之间的关系（特征之间有较强的相关性，可以使用PCA进行降维） (3 ) 特征和最后target 的关系 (feature  importance, 卡房分布， pearson 相关系数) (4) 很多常见的机器学习模型都是一种特征选择的方式，比如xgboost</li>
<li>连续特征离散化，离散特征one-hot 化（这个都是为了LR 使用方便，是不是很押韵）</li>
</ol>
<p>数据挖掘中的小的trick</p>
<p>这个从某种角度上使用的之前的申请的记录，这个特征是非常强的。识别到user_id（根据生日，职业等信息），在训练集和测试集中发现有8000多个user 是有两个行记录，100多个user 是有3行记录，那么如果之前的申请目标是1，那么这次申请90%也是1. </p>
<h3 id="pygen-项目"><a href="#pygen-项目" class="headerlink" title="pygen 项目"></a>pygen 项目</h3><p>faker 中生成的信息是单列的，个人信息之间是没有联系的，所以想要用在机器学习训练的时候比较难。<br>关键技术：<br>中文名字有很强的性别属性。例如名字中带有“杰”“志”“宏”等字的一般为男性，带有“琬”“佩”“梅”等字的一般为女性。当然也有一些比较中性的字，例如“文”“安”“清”等，比较难猜测性别。</p>
<h3 id="gan-论文"><a href="#gan-论文" class="headerlink" title="gan 论文"></a>gan 论文</h3><p>D 网络在判别的时候，是从逼真程度上进行判别的，并不是从生成图像多样性上进行判别的，并且是无法从多样性上进行判别的，因为输入到D 网络中的是一个样本或者说是一个 batch的样本，是很难得到生成图像的多样性这个角度的。</p>
<p>同理 G 网络也只是考虑单个的生成图像，对于G 网络图像的多样性是没有考虑在内的。</p>
<p>有时候也会在训练过程中也会利用这种性质，比如说只是为了得到一张或者几张非常逼真好看的图像，而不会很在乎最后是否生成了多样性的图像。</p>
<p>孪生网络最初是用来做指纹的验证，然后扩展到 cv 和nlp 中都有用来做相似度方面的计算。在论文中的是用来作为生成图像之间相似度的计算，该网络中两个 CNN 用于提取特征，然后基于 欧式距离进行计算，输出label 是0-1 之间的数字， 0表示最相近，1表示不相近。损失函数使用的是 对比损失函数，有点类似交叉熵的感觉，分段函数， y =0 的时候是平方损失函数，y =1 的时候，是合页损失函数。</p>
<p>主要的是做了以下的优化：<br>label 浮点化，Label Smoothing :这种距离的计算是通过聚类实现的，在同一个簇中或者相邻簇中的距离小，向着0 靠近；在不同的簇之间的相距大，不超过1.</p>
<p>网络结构上有两点优化：<br>对于对抗生成网络的D 网路的权重进行正则化，具体来说是 谱归一化，使其符合lipschitz 约束，其中超参数 K 取1.即 D(x) -D(y) 的欧式距离是不大于 x-y 的欧式距离的。</p>
<p>加入了 self -attention机制。</p>
<h3 id="XGboost-和GBDT-比较"><a href="#XGboost-和GBDT-比较" class="headerlink" title="XGboost 和GBDT 比较"></a>XGboost 和GBDT 比较</h3><p>最后的效果提升：<br>数据方面</p>
<ol>
<li>Ignoring sparse inputs<br>这个是处理缺省值（或者 0）的手段：两者在split 分裂点的时候，都是先不处理数值 0；然后找到分裂点之后，把0 放到哪边造成的loss 下降的比较大，然后就放到哪边。<br><img src="https://ftp.bmp.ovh/imgs/2019/09/3baa1391fe380171.png" alt=""></li>
</ol>
<p>模型方面</p>
<ol>
<li><p>带深度限制的Leaf-wise的叶子生长策略<br>最开始的时候xgboost 中叶子生长方式是 level-wise 的生长策略，实际上这是一种比较非常低效的算法，因为同一层的很多叶子分裂增益较低，没有必要进行搜索和分类。这种不加区分的对待同一层叶子带来的是没有必要的开销。leaf-wise 是 lightGBM 上使用的一种算法，在分裂次数相同的情况下，可以降低更多的误差，得到更好的精度。leaf-wise 的缺点可能会过拟合，所以加上了深度的限制，在保证高效率的同时防止过拟合。</p>
</li>
<li><p>loss function<br>相对比 GBDT，目标函数加入了正则项，因为树结构的模型是很容易过拟合大，加入正则项可以减少模型的复杂度，增加模型的泛化能力。</p>
</li>
<li>shrinkage 和 column subsampling<br>提出了两种防止过拟合的方法：衰减因子和采样。前者应用于上层树，后者应用于特征，选择部分特征进行建树。XGBoost利用梯度优化模型算法, 样本是不放回的，想象一个样本连续重复抽出,梯度来回踏步，这显然不利于收敛。但是，XGBoost支持子采样, 也就是每轮计算可以不使用全部样本。</li>
</ol>
<p>计算效率的提升：</p>
<ol>
<li>find the best split<br>xgboost 是基于决策树，那么如何快速发现最后的分裂点。一般做法使用遍历所有的样本和所有的特征，时间复杂度是 $( n_{data}n_{features})$。xgboost 采用了一种优化手段，引入一个超参数 number of bins, 时间复杂度优化到 $n_{data}n_{bins}$。当你的 bins 的数量越大，那么进度是越高的，这是一种 trade-off。</li>
<li>支持并行化<br>虽然层次树的建立是串行的，但是在一个结点选择候选结点的时候，是可以使用多进程并行运算的。</li>
<li>loss function是二阶求导（taylor 二阶展开式）</li>
</ol>
<p>泰勒公式一句话描述：就是用多项式函数去逼近光滑函数。通用式子如下，<br>$$<br>f(x) =\sum_{n=0}^{N} \frac{f^{(n)}(0)}{n !} x^{n}<br>$$<br>有常见的两种写法，一种是 $f(x)$ 在 $x_0$处的基本形式，<br>$$<br>\begin{split}<br>f(x) &amp;=\sum_{n=0}^{\infty} \frac{f^{(n)}(x_{0})}{n !}(x-x_{0})^{n} \\<br> &amp;=f(x_{0})+f^{1}(x_{0})(x-x_{0})+\frac{f^{2}(x_{0})}{2}(x-x_{0})^{2}+\cdots+\frac{f^{(n)}(x_{0})}{n !}(x-x_{0})^{n}<br>\end{split}<br>$$<br>还有一种常见的写法， $x^{t +1} = x^t + \Delta x$， 将 $f (x^{t+1})$ 在 $x^t$ 处进行泰勒展开<br>$$<br>f\left(x^{t+1}\right)=f\left(x^{t}\right)+f^{1}\left(x^{t}\right) \Delta x+\frac{f^{2}\left(x^{t}\right)}{2} \Delta x^{2}+\cdots<br>$$</p>
<p><a href="https://www.hrwhisper.me/machine-learning-xgboost/" target="_blank" rel="noopener">machine-learning-xgboost</a> 中的模型学习部分有公式推导，好好看。</p>
<h3 id="fasttext-amp-faiss"><a href="#fasttext-amp-faiss" class="headerlink" title="fasttext &amp; faiss"></a>fasttext &amp; faiss</h3><p>fasttex 中有有两部分： 无监督的学习(subword)词向量的训练 和有监督分类(text classification)任务，使用的是三层网络结构：输入层，隐藏层和输出层。</p>
<ol>
<li>使用二叉树的结构，时间复杂度从 $O(N) $ 优化到了$log_2(N)$，当使用huffman 树的时候，这种效果更加明显。层次softmax 不是fasttext 的首创，它的改进之处在实现的时候基于 huffman 树而不是普通的二叉树， 属于运算上的优化。利用了类别不均衡的特点，类别多的路径短，整体上的时间效率会提高。</li>
<li>N-gram 一种是基于character-level 对于不常见单词的扩充，解决的是OOV问题；一种是word-level，考虑的是词语周边的信息，加入了context 的信息，local context 的信息。</li>
<li>negative sampling 是解决最后softmax 层中，不更新所有的negative words，只是更新少部分单词，根据词频选择negative words，并且这种词频是经过约束，主要是使得低频词语也有出现的机会。</li>
<li>调参分为字典相关的参数和训练相关参数</li>
<li>fasttext 的和之前 CBOW的区别：网络结构中的输入层，CBOW是经过one-hot的上下文单词，而fasttext 是单词+ n-gram 的特征，在解决OOV效果比较好；另外在最后的输出层，基于huffman 树实现了层次softmax，对于类别不均衡的训练集来说，训练时间会变得更短。</li>
<li>fasttext 的缺点，使用文本分类的时候，当类别比较多的时候提升效果比较明显，否则是容易过拟合的。</li>
</ol>
<p>fasttext 和 word2vec 的区别?</p>
<ul>
<li>CBOW的输入是目标单词的上下文，fastText的输入是多个单词及其n-gram特征，这些特征用来表示单个文档；</li>
<li>CBOW的输入单词被onehot编码过，fastText的输入特征是被embedding过；（EMBEDDING_DIM表示经过embedding层输出，每个词被分布式表示的向量的维度，这里设置为100。比如对于“达观”这个词，会被一个长度为100的类似于[ 0.97860014, 5.93589592, 0.22342691, -3.83102846, -0.23053935, …]的实值向量来表示；）</li>
<li>word2vec是一个无监督算法，而fasttext是一个有监督算法。CBOW的输出是目标词汇，fastText的输出是文档对应的类标。</li>
</ul>
<p>word2vec 中 的训练trick（这个也不是 word2vec 的首创）：</p>
<ul>
<li>hierarchical softmax 本质是把 N 分类问题变成 log(N)次二分类</li>
<li>negative sampling 本质是预测总体类别的一个子集</li>
</ul>
<p>在某些文本分类任务中类别很多，计算线性分类器的复杂度高。为了改善运行时间，fastText 模型使用了层次 Softmax 技巧。层次 Softmax 技巧建立在哈弗曼编码的基础上，对标签进行编码，能够极大地缩小模型预测目标的数量。</p>
<p>n-gram 是有两个维度的：字符n-gram 是缓解 oov 问题； 字维度是可以加入部分的上下文信息。</p>
<p>fasttext 的优势：</p>
<ol>
<li>适合大型数据+高效的训练速度：能够训练模型“在使用标准多核CPU的情况下10分钟内处理超过10亿个词汇”</li>
<li>fastText专注于文本分类，在许多标准问题上实现当下最好的表现（例如文本倾向性分析或标签预测）。</li>
</ol>
<p>faiss 是稠密向量之间计算距离的开源工具。</p>
<ol>
<li>faiss 三种模式或者说索引。一种简单模式在小的数据集上计算欧式距离；一种加快检索的速度，使用聚类算法，检索的时候只是检索id 所在的簇和周围的簇，不过这个过程是需要预训练的；一种是减少内存的时候，如果是求解近似解，那么不必存储完整的向量，使用pca 降维。还有比较通用的加快速度的方式，比如分段计算和使用gpu 进行计算。</li>
<li>关于k-means中选择聚类簇k的个数的算法：尝试法。如果增大k，发现并不能使得指标明显的下降，这个时候就达到了阈值。指标：一个簇内所有的点到簇类中心的距离的总和。</li>
<li>knn 和k-means 的区别，前者是有监督的分类算法，根据测试点周围k 个点的类别信息判断该点的信息；k-means 是无监督算法，属于聚类中的一种。</li>
</ol>
<h3 id="使用什么库函数"><a href="#使用什么库函数" class="headerlink" title="使用什么库函数"></a>使用什么库函数</h3><p>机器学习中经常使用 numpy，sklearn，pandas，xgboost</p>
<p>NLP和深度学习中中使用jieba，NLTK，tensorflow</p>
<h3 id="如何评价自己"><a href="#如何评价自己" class="headerlink" title="如何评价自己"></a>如何评价自己</h3><p>缺点：</p>
<p>（说一个真实的缺点，这个缺点必须是正在改善的缺点，最好能够成为优点的那种。）<br>有时候自己可能过于专注于某一点，把太多的时间花费在某一方面。实习和在学校是不太一样的，公司更加看重成果，最后做出来的东西的效果，所以和实验室中整天整个月做一件事情是不太一样的，可能需要根据公司业务的需求去调整工作的重心。实习期间，自己也是在努力调整这种状态。</p>
<p>优点：</p>
<p>动手能力强，对于一个新东西，上手比较快，在ai 大多数的领域，还是从美国硅谷哪里来的，所以英语能力比较重要，而自己的英语水平比较好，看英文论文调研方便比较有优势。快速学习能力比较强，所以能够比较快的接受新鲜的事物，上手比较快。</p>
<p>这些任务中，你的角色是什么？ 这个问题常常被面试官这样问道“这个是你一个人做的吗？</p>
<p>感觉既要体现自己的独立做事的能力又要有合作精神。实习工作基本上都是独立完成的，项目是和实验室的人合作完成的，我是主要负责人。</p>
<h3 id="需要单独复习的文章"><a href="#需要单独复习的文章" class="headerlink" title="需要单独复习的文章"></a>需要单独复习的文章</h3><p><a href="https://jijeng.github.io/2019/06/21/overfit/" target="_blank" rel="noopener">如何处理 overfit</a></p>
<h3 id="最后的发问？"><a href="#最后的发问？" class="headerlink" title="最后的发问？"></a>最后的发问？</h3><ol>
<li>你们所在的小组主要是什么业务呢？</li>
<li>如果有机会来这里工作，那么主要的工作内容是什么？</li>
<li>面试结果大概什么时候出？</li>
<li>如果感觉面试的比较好，那么问一下可以简单的评价我这次面试吗，因为您也面试了不少的同学了吧。</li>
</ol>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/07/05/logistics2softmax/" rel="next" title="logistics 和softmax 的公式推导">
                <i class="fa fa-chevron-left"></i> logistics 和softmax 的公式推导
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/07/12/gbdt/" rel="prev" title="A Gentle Introduction of GBDT">
                A Gentle Introduction of GBDT <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
      </div>
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/avatar.jpeg"
                alt="Jijeng Jia" />
            
              <p class="site-author-name" itemprop="name">Jijeng Jia</p>
              <p class="site-description motion-element" itemprop="description">Stay Hungry, Stay Foolish</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">107</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">6</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">79</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/jijeng" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:jia1509309698@163.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          

          <script type="text/javascript" src="//rf.revolvermaps.com/0/0/5.js?i=5kk0u0mm50w&amp;m=0&amp;c=54ff00&amp;cr1=ff0000" async="async"></script>


  <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
  <script src="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
  <script src="//cdn.bootcss.com/blueimp-md5/1.1.0/js/md5.min.js"></script>
  <script>
      var gitalk = new Gitalk({
        clientID: '8c6403951ee3eab4e420',
        clientSecret: 'd842e48ca0c28ec41200f973ba52f96ba975b441',
        repo: 'jijeng.github.io',
        owner: 'jia1509309698@163.com',
        admin: 'jia1509309698@163.com',
        id: md5(location.pathname),
        distractionFreeMode: 'true'
      });
      var div = document.createElement('div');
      div.setAttribute("id", "gitalk_comments");
      div.setAttribute("class", "post-nav");
      var bro = document.getElementById('posts').getElementsByTagName('article');
      bro = bro[0].getElementsByClassName('post-block');
      bro = bro[0].getElementsByTagName('footer');
      bro = bro[0];
      bro.appendChild(div);
      gitalk.render('gitalk_comments');
  </script>

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#总的大纲"><span class="nav-number">1.</span> <span class="nav-text">总的大纲</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#爱奇艺实习经历"><span class="nav-number">2.</span> <span class="nav-text">爱奇艺实习经历</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#腾讯新闻的实习经历"><span class="nav-number">3.</span> <span class="nav-text">腾讯新闻的实习经历</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#文章去重"><span class="nav-number">4.</span> <span class="nav-text">文章去重</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Home-credit-default-risk"><span class="nav-number">5.</span> <span class="nav-text">Home credit default risk</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#pygen-项目"><span class="nav-number">6.</span> <span class="nav-text">pygen 项目</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#gan-论文"><span class="nav-number">7.</span> <span class="nav-text">gan 论文</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#XGboost-和GBDT-比较"><span class="nav-number">8.</span> <span class="nav-text">XGboost 和GBDT 比较</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#fasttext-amp-faiss"><span class="nav-number">9.</span> <span class="nav-text">fasttext &amp; faiss</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#使用什么库函数"><span class="nav-number">10.</span> <span class="nav-text">使用什么库函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#如何评价自己"><span class="nav-number">11.</span> <span class="nav-text">如何评价自己</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#需要单独复习的文章"><span class="nav-number">12.</span> <span class="nav-text">需要单独复习的文章</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#最后的发问？"><span class="nav-number">13.</span> <span class="nav-text">最后的发问？</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Jijeng Jia</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>

<div>
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
Total  <span id="busuanzi_value_site_pv"></span> views
You got  <span id="busuanzi_value_site_uv"></span> visitors
</div>




        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      
    </span>
  
</div>








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  

    
      <script id="dsq-count-scr" src="https://https-jijeng-github-io.disqus.com/count.js" async></script>
    

    
      <script type="text/javascript">
        var disqus_config = function () {
          this.page.url = 'http://yoursite.com/2019/07/10/summary_resume/';
          this.page.identifier = '2019/07/10/summary_resume/';
          this.page.title = 'Summary';
        };
        var d = document, s = d.createElement('script');
        s.src = 'https://https-jijeng-github-io.disqus.com/embed.js';
        s.setAttribute('data-timestamp', '' + +new Date());
        (d.head || d.body).appendChild(s);
      </script>
    

  




	





  














  <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
  <script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>
   <script type="text/javascript">
        var gitalk = new Gitalk({
          clientID: '',
          clientSecret: '',
          repo: 'jijeng.github.io',
          owner: '',
          admin: [''],
          id: location.pathname,
          distractionFreeMode: ''
        })
        gitalk.render('gitalk-container')           
       </script>


  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
