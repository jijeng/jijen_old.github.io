<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="优化,loss function,Activation Function," />










<meta name="description" content="主要介绍 activation function， loss function 等等概念和分类。">
<meta name="keywords" content="优化,loss function,Activation Function">
<meta property="og:type" content="article">
<meta property="og:title" content="Loss Activation and Optimisation Function">
<meta property="og:url" content="http://yoursite.com/2018/07/07/Loss-Activation-and-Optimisation-Function/index.html">
<meta property="og:site_name" content="Jijeng&#39;s blog">
<meta property="og:description" content="主要介绍 activation function， loss function 等等概念和分类。">
<meta property="og:locale" content="default">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/e9a223b5ly1g450tpr64tj20df083wee.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/e9a223b5ly1g450uxt3ykj20df083a9y.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/e9a223b5ly1g450wzrr2gj20df0830sl.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/e9a223b5ly1g450y6qpe0j20df083745.jpg">
<meta property="og:image" content="https://i.loli.net/2019/07/15/5d2c371ec62f690070.png">
<meta property="og:image" content="https://i.loli.net/2019/07/15/5d2c371ee553533628.png">
<meta property="og:image" content="https://i.loli.net/2019/07/15/5d2c39ac3289176404.png">
<meta property="og:image" content="https://i.bmp.ovh/imgs/2019/07/1b14b874a9323ede.png">
<meta property="og:image" content="https://i.bmp.ovh/imgs/2019/07/b181fa529616d69a.png">
<meta property="og:image" content="https://i.bmp.ovh/imgs/2019/07/1f6fc8f2b951d570.png">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/e9a223b5ly1g452awgfddj20fg0b1t91.jpg">
<meta property="og:updated_time" content="2019-07-15T09:37:46.005Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Loss Activation and Optimisation Function">
<meta name="twitter:description" content="主要介绍 activation function， loss function 等等概念和分类。">
<meta name="twitter:image" content="http://ww1.sinaimg.cn/large/e9a223b5ly1g450tpr64tj20df083wee.jpg">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2018/07/07/Loss-Activation-and-Optimisation-Function/"/>





  <title>Loss Activation and Optimisation Function | Jijeng's blog</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Jijeng's blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/07/07/Loss-Activation-and-Optimisation-Function/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jijeng Jia">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jijeng's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Loss Activation and Optimisation Function</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-07-07T13:37:05+08:00">
                2018-07-07
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">Post modified&#58;</span>
              
              <time title="Post modified" itemprop="dateModified" datetime="2019-07-15T17:37:46+08:00">
                2019-07-15
              </time>
            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/07/07/Loss-Activation-and-Optimisation-Function/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2018/07/07/Loss-Activation-and-Optimisation-Function/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="page-pv"><i class="fa fa-file-o"></i>
            <span class="busuanzi-value" id="busuanzi_value_page_pv" ></span>
            </span>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>主要介绍 activation function， loss function 等等概念和分类。</p>
<a id="more"></a>
<h2 id="Activation-Function"><a href="#Activation-Function" class="headerlink" title="Activation Function"></a>Activation Function</h2><h3 id="What"><a href="#What" class="headerlink" title="What?"></a>What?</h3><blockquote>
<p>It’s just a thing (node) that you add to the output end of any neural network. It is also known as <strong>Transfer Function</strong>. It can also be attached in between two Neural Networks.</p>
</blockquote>
<p>$$<br>Output  =  activation function  \left( x _ { 1 } w _ { 1 } + x _ { 2 } w _ { 2 } + \cdots + x _ { n } w _ { n } + b i a s \right)<br>$$</p>
<p>A weighted sum is computed as:<br>$$<br>x _ { 1 } w _ { 1 } + x _ { 2 } w _ { 2 } + \cdots + x _ { n } w _ { n }<br>$$<br>Then,  the computed value is fed into the activation function, which then prepares an output.<br>$$<br>activation function \left( x _ { 1 } w _ { 1 } + x _ { 2 } w _ { 2 } + \cdots + x _ { n } w _ { n } + b i a s \right)<br>$$</p>
<blockquote>
<p>Think of the activation function as a mathematical operation that normalises the input and produces an output. The output is then passed forward onto the neurons on the subsequent layer.</p>
</blockquote>
<p>作用：增加非线性</p>
<p>The thresholds are pre-defined numerical values in the function. This very nature of the activation functions can add non-linearity to the output. </p>
<h3 id="Activation-Function-Types"><a href="#Activation-Function-Types" class="headerlink" title="Activation Function Types"></a>Activation Function Types</h3><p><strong> Linear Activation Function:</strong></p>
<p>$$<br> output  = k * x<br>$$<br>where $k$ is a scalar value, as an instance 2, and $x$ is the input.</p>
<p><img src="http://ww1.sinaimg.cn/large/e9a223b5ly1g450tpr64tj20df083wee.jpg" alt=""></p>
<p><strong>Sigmoid or Logistic Activation Function </strong></p>
<p>The sigmoid activation function is “S” shaped. It can add non-linearity to the output and returns a binary value of 0 or 1.</p>
<p>$$<br>Output  = \frac { 1 } { 1 + e ^ { - x } }<br>$$</p>
<p><img src="http://ww1.sinaimg.cn/large/e9a223b5ly1g450uxt3ykj20df083a9y.jpg" alt=""><br>这个函数有一个很好的导数形式，在反向传播的时候，效果比较明显。</p>
<p><strong>Tanh Activation Function</strong></p>
<p>Tanh is an extension of the sigmoid activation function. Hence Tanh can be used to add non-linearity to the output. The output is within the range of -1 to 1. Tanh function shifts the result of the sigmoid activation function:</p>
<p>$$<br>\text { Output } = \frac { 2 } { 1 + e ^ { - 2 x } } - 1<br>$$</p>
<p><img src="http://ww1.sinaimg.cn/large/e9a223b5ly1g450wzrr2gj20df0830sl.jpg" alt=""></p>
<p><strong>Rectified Linear Unit Activation Function (RELU)</strong></p>
<p>RELU is one of the most used activation functions. It is preferred to use RELU in the hidden layer. The concept is very straight forward. It also adds non-linearity to the output. However the result can range from 0 to infinity.</p>
<p>$$<br> Output  = \max ( 0 , x )<br>$$<br><img src="http://ww1.sinaimg.cn/large/e9a223b5ly1g450y6qpe0j20df083745.jpg" alt=""><br>这个是很高的评价了。<br>If you are unsure of which activation function you want to use then use RELU.</p>
<p><strong>Softmax Activation Function </strong></p>
<p>Softmax is an extension of the Sigmoid activation function. Softmax function adds non-linearity to the output, however it is mainly used for classification examples where multiple classes of results can be computed.</p>
<p>$$<br>Output  = \frac { e ^ { x } } { \operatorname { sum } \left( e ^ { x } \right) }<br>$$</p>
<p>这个一般使用在最后，作为多分类的结束。</p>
<h2 id="Loss-Function-Error-Function"><a href="#Loss-Function-Error-Function" class="headerlink" title="Loss Function(Error Function)"></a>Loss Function(Error Function)</h2><p>机器学习中所有的算法都需要最大化或最小化一个函数，这个函数被称为“目标函数”。其中，我们一般把最小化的一类函数，称为“损失函数”。它能根据预测结果，衡量出模型预测能力的好坏。</p>
<p>损失函数 (Loss function) 是用来衡量模型的预测值 $f(x)$ 和真实值 $Y$ 的不一样的程度，通常使用 $L (Y, f(x))$ 来进行表示，损失函数越小，模型的鲁棒性越强。</p>
<p>分类：分类问题的损失函数和回归问题的损失函数。</p>
<table>
<thead>
<tr>
<th>分类损失函数</th>
<th>回归损失函数</th>
</tr>
</thead>
<tbody>
<tr>
<td>log loss (交叉熵)</td>
<td>mean square error</td>
</tr>
<tr>
<td>KL divergence / relative entropy</td>
<td>Huber loss</td>
</tr>
<tr>
<td>指数损失</td>
<td></td>
</tr>
<tr>
<td>合页损失</td>
</tr>
</tbody>
</table>
<p><strong> 平方损失函数</strong></p>
<p>在线性回归中，它假设样本和噪声都服从高斯分布（为什么假设成高斯分布呢？其实这里隐藏了一个小知识点，就是中心极限定理），最后通过极大似然估计MLE可以推导出最小二乘式子，即平方损失函数可以通过线性回归在假设样本是高斯分布的条件下推导得到。</p>
<p>$$<br>S E = \sum _ { i = 1 } ^ { n } \left( y _ { i } - y _ { i } ^ { p } \right) ^ { 2 }<br>$$</p>
<p>为什么选择欧式距离作为误差的度量？</p>
<ul>
<li>简单，计算方便</li>
<li>欧式距离是一种很好的相似度衡量标准</li>
<li>在不同的表示域变换之后，特征的性质能够保持不变。</li>
</ul>
<p>在实际应用中，通常会使用 均方差作为一种衡量指标，就是在上面的公式中除以 N.</p>
<p><strong> 交叉熵损失函数</strong></p>
<p>二分类问题的交叉熵 loss 主要是有两种形式。第一种是输出的label 是 {0, 1}，也是最为常见的。<br>$$<br>Loss= - [ y \log \hat { y } + ( 1 - y ) \log ( 1 - \hat { y } ) ]<br>$$</p>
<p>这个公式可以先从一个分段函数推导，然后从最大似然的角度出发，预测类别的概率是:<br>$$<br>P ( y | x ) = \hat { y } ^ { y } \cdot ( 1 - \hat { y } ) ^ { ( 1 - y ) }<br>$$<br>然后取对数：<br>$$<br>log(loss)= - [ y \log \hat { y } + ( 1 - y ) \log ( 1 - \hat { y } ) ]<br>$$</p>
<p>当 $y =1$ 的时候，<br>$$<br>L = - \log \hat { y }<br>$$<br>因为，<br>$$<br>\hat { y } = \frac { 1 } { 1 + e ^ { - s } }<br>$$<br>所以得到<br>$$<br>L = \log \left( 1 + e ^ { - s } \right)<br>$$<br>loss 的曲线如下图所示：</p>
<p><img src="https://i.loli.net/2019/07/15/5d2c371ec62f690070.png" alt="11-1.png"><br>从图中明显能够看出，s 越大于零，L 越小，函数的变化趋势也完全符合实际需要的情况。（y =1 是这个时候的目标）</p>
<p>当 $y =0$ 的时候：<br>同理可以得到，<br>$$<br>log(loss)= \log \left( 1 + e ^ { s } \right)<br>$$</p>
<p><img src="https://i.loli.net/2019/07/15/5d2c371ee553533628.png" alt="14.png"></p>
<p>从图中明显能够看出，s 越小于零，L 越小，函数的变化趋势也完全符合实际需要的情况。</p>
<p>第二种情况是基于输出label 表示方式 {-1, 1}，这个时候的loss  表达式为:</p>
<p>$$<br>loss = \log \left( 1 + e ^ { - y s } \right)<br>$$</p>
<p>这个时候只不过使用 $ys$ 代替上面的s ，实际上的分析还是一样的。其中 y 表示真实的标签，s 表示 sigmoid 中的s，见下面的公式。<br>$$<br>g ( s ) = \frac { 1 } { 1 + e ^ { - s } }<br>$$<br>当 ys &gt;0 的时候，表示的预测正确，否则是预测错误。</p>
<p>交叉熵损失函数经常使用sigmoid 函数作为激活函数，因为这个可以完美解决平方损失函数中权重更新比较慢点的情况。</p>
<p><strong> Hinge Loss</strong></p>
<p>Hinge Loss，又称合页损失，其表达式如下：</p>
<p>$$<br>Loss = \max ( 0,1 - y s )<br>$$<br>图像如下：</p>
<p><img src="https://i.loli.net/2019/07/15/5d2c39ac3289176404.png" alt="20.png"></p>
<p>如同合起来的书，所以称之为 合页损失。显然，只有当 ys &lt; 1 时，Loss 才大于零；对于 ys &gt; 1 的情况，Loss 始终为零。Hinge Loss 一般多用于支持向量机（SVM）中，体现了 SVM 距离最大化的思想。 而且，当 Loss 大于零时，是线性函数，便于梯度下降算法求导。Hinge Loss 的另一个优点是使得 ys &gt; 0 的样本损失皆为 0，由此带来了稀疏解，使得 SVM 仅通过少量的支持向量就能确定最终超平面。</p>
<p><strong> log 对数损失函数 </strong></p>
<p>在逻辑回归的推导中，它假设样本服从伯努利分布（0-1分布），然后求得满足该分布的似然函数，接着取对数求极值等等。</p>
<p><strong> 指数损失函数 </strong></p>
<p>公式如下：<br>$$<br>loss = e ^ { - y s }<br>$$</p>
<p>曲线如下：<br><img src="https://i.bmp.ovh/imgs/2019/07/1b14b874a9323ede.png" alt=""></p>
<p>Exponential Loss 与交叉熵 Loss 类似，但它是指数下降的，因此梯度较其它 Loss 来说，更大一些。 Exponential Loss 一般多用于AdaBoost 中。因为使用 Exponential Loss 能比较方便地利用加法模型推导出 AdaBoost算法。</p>
<p>$$<br>L = - \log \frac { e ^ { s } } { \sum _ { j = 1 } ^ { C } e ^ { s _ { j } } } = - s + \log \sum _ { j = 1 } ^ { C } e ^ { s _ { j } }<br>$$</p>
<p>softmax loss 的曲线如下图所示：</p>
<p><img src="https://i.bmp.ovh/imgs/2019/07/b181fa529616d69a.png" alt=""></p>
<p>上图中，当 s &lt;&lt; 0 时，Softmax 近似线性；当 s&gt;&gt;0 时，Softmax 趋向于零。Softmax 同样受异常点的干扰较小，多用于神经网络多分类问题中。</p>
<p>若我们把 ys 的坐标范围取得更大一些，上面 5 种 Loss 的差别会更大一些，如图：</p>
<p><img src="https://i.bmp.ovh/imgs/2019/07/1f6fc8f2b951d570.png" alt=""></p>
<p>显然，这时候 Exponential Loss 会远远大于其它 Loss。从训练的角度来看，模型会更加偏向于惩罚较大的点，赋予其更大的权重。如果样本中存在离群点，Exponential Loss 会给离群点赋予更高的权重，但却可能是以牺牲其他正常数据点的预测效果为代价，可能会降低模型的整体性能，使得模型不够健壮（robust）。</p>
<p>相比 Exponential Loss，其它四个 Loss，包括 Softmax Loss，都对离群点有较好的“容忍性”，受异常点的干扰较小，模型较为健壮。</p>
<p><strong> Softmax loss </strong></p>
<p>对于多分类问题，可以使用 softmax loss。</p>
<p>其中，C 为类别个数，小写字母 s 是正确类别对应的 Softmax 输入，大写字母 S 是正确类别对应的 Softmax 输出。</p>
<p>由于 log 运算符不会影响函数的单调性，我们对 S 进行 log 操作。另外，我们希望 log(S) 越大越好，即正确类别对应的相对概率越大越好，那么就可以对 log(S) 前面加个负号，来表示损失函数：</p>
<p>如何选择损失函数？</p>
<p>对于异常点的处理是一个维度，比如L1 损失函数处理异常点更加稳定，相对L2 损失函数。</p>
<p>what?</p>
<p>衡量模型好坏的 function，如果模型表现好，那么loss 应该是小；如果模型表现不好，那么loss 应该是大的。</p>
<blockquote>
<p>At its core, a loss function is incredibly simple: it’s a method of evaluating how well your algorithm models your dataset. If your predictions are totally off, your loss function will output a higher number. If they’re pretty good, it’ll output a lower number. As you change pieces of your algorithm to try and improve your model, your loss function will tell you if you’re getting anywhere.</p>
</blockquote>
<p><strong> Log Loss (Cross Entropy Loss)</strong></p>
<p>Cross-entropy loss, or log loss, measures the performance of a classification model whose output is a probability value between 0 and 1. Cross-entropy loss increases as the predicted probability diverges from the actual label. So predicting a probability of .012 when the actual observation label is 1 would be bad and result in a high loss value. A perfect model would have a log loss of 0.</p>
<p><img src="http://ww1.sinaimg.cn/large/e9a223b5ly1g452awgfddj20fg0b1t91.jpg" alt=""></p>
<p>The graph above shows the range of possible loss values given a true observation (isDog = 1). As the predicted probability approaches 1, log loss slowly decreases. As the predicted probability decreases, however, the log loss increases rapidly. Log loss penalizes both types of errors, but especially those predictions that are confident and wrong!</p>
<p>Cross-entropy and log loss are slightly different depending on context, but in machine learning when calculating error rates between 0 and 1 they resolve to the same thing.</p>
<p>In binary classification, where the number of classes M equals 2, cross-entropy can be calculated as:</p>
<p>$$<br>- ( y \log ( y ) + ( 1 - y ) \log ( 1 - y ) )<br>$$</p>
<p>If $ M&gt;$2 (i.e. multiclass classification), we calculate a separate loss for each class label per observation and sum the result.</p>
<p>$$<br>- \sum _ { c = 1 } ^ { M } y _ { o , c } \log \left( y _ { o , c } \right)<br>$$</p>
<blockquote>
<p>M - number of classes (dog, cat, fish)<br>log - the natural log<br>y - binary indicator (0 or 1) if class label c is the correct classification for observation o</p>
</blockquote>
<p>想要表达的是 log loss 是从 Likelihood Loss，改进过来的，有没有发现最大似然的痕迹。<br>log loss 表达式如下：<br>$$<br>\begin{split}<br>P(Y | X)  &amp;= P(X_1 | Y)  \times P(X_2 | Y)  \times  \dots \times P(X_n | Y)  \times P(Y) = P(Y) \prod_{i}^{n} P(X_i | Y) \\<br>&amp;\Rightarrow log(P(Y | X)) = log(\prod_{i}^{n} P(X_i | Y) \Rightarrow \sum_{i}^{n} log(P(X_i | Y))<br>\end{split}<br>$$<br>交叉熵表达式：<br>$$CE(\hat{y}, y) = - \sum_{i=1}^{n} y_i log(\hat{y}) + (1 - y_i) log(1 - \hat{y})$$</p>
<p><strong> Mean Squared Error</strong></p>
<p>在回归中使用</p>
<blockquote>
<p>Mean Squared Error (MSE), or quadratic, loss function is widely used in linear regression as the performance measure, and the method of minimizing MSE is called Ordinary Least Squares (OSL)。</p>
</blockquote>
<p>To calculate MSE, you take the difference between your predictions and the ground truth, square it, and average it out across the whole dataset.</p>
<p>$$<br>Loss  = \frac { 1 } { n } \sum _ { i = 1 } ^ { n } \left( y ^ { ( i ) } - \hat { y } ^ { ( i ) } \right) ^ { 2 }<br>$$</p>
<p><strong>Mean Absolute Error</strong></p>
<p>Mean Absolute Error (MAE) is a quantity used to measure how close forecasts or predictions are to the eventual outcomes, which is computed by</p>
<p>$$<br>Loss = \frac { 1 } { n } \sum _ { i = 1 } ^ { n } \left| y ^ { ( i ) } - \hat { y } ^ { ( i ) } \right|<br>$$</p>
<p>where $| .|$ denotes the absolute value. Albeit, both MSE and MAE are used in predictive modeling, there are several differences between them. MSE has nice mathematical properties which makes it easier to compute the gradient. However, MAE requires more complicated tools such as linear programming to compute the gradient. Because of the square, large errors have relatively greater influence on MSE than do the smaller error. Therefore, MAE is more robust to outliers since it does not make use of square. On the other hand, MSE is more useful if concerning about large errors whose consequences are much bigger than equivalent smaller ones. MSE also corresponds to maximizing the likelihood of Gaussian random variables.</p>
<p><strong> L2</strong></p>
<p>这两个loss function 在<a href="https://jijeng.github.io/2018/07/21/differences-between-l1-and-l2-as-loss-function-and-regularization/" target="_blank" rel="noopener">这里</a>介绍过，所以本博客中简单说一下。</p>
<p>L2 loss function is the square of the L2 norm of the difference between actual value and predicted value. It is mathematically similar to MSE, only do not have division by<br>n, it is computed by</p>
<p>$$<br> Loss = \sum _ { i = 1 } ^ { n } \left( y ^ { ( i ) } - \hat { y } ^ { ( i ) } \right) ^ { 2 }<br>$$</p>
<p><strong>Kullback Leibler (KL) Divergence</strong><br>（计算的是两个分布的问题）<br>KL Divergence, also known as relative entropy, information divergence/gain, is a measure of how one probability distribution diverges from a second expected probability distribution. KL divergence loss function is computed by<br>$$<br>D _ { K L } ( p | q ) = \sum _ { x } p ( x ) \log \frac { p ( x ) } { q ( x ) }<br>$$</p>
<p>交叉熵的定义：<br>$$<br>H ( p , q ) = - \sum _ { x } p ( x ) \log q ( x )<br>$$<br>两者的关系推导，<br>$$<br>\begin{split}<br>D _ { K L } ( p | q ) &amp;= \sum _ { x } p ( x ) \log \frac { p ( x ) } { q ( x ) } \\<br>&amp;= \sum _ { x } ( p ( x ) \log p ( x ) - p ( x ) \log q ( x ) ) \\<br>&amp;= - H ( p ) - \sum _ { x } p ( x ) \log q ( x ) \\<br>&amp;= - H ( p ) + H ( p , q )<br>\end{split}<br>$$<br>所以说， cross entropy 也是可以写成这样：<br>$$<br>H ( p , q ) = D _ { K L } ( p | q ) + H ( p )<br>$$</p>
<p><strong>logistic loss 和 cross entropy的关系</strong></p>
<p>当 $ p \in { y , 1 - y }$,  $q \in { \hat { y } , 1 - \hat { y } }$ ，cross entropy 可以写成 logistic  loss:</p>
<p>$$<br>H ( p , q ) = - \sum _ { x } p ( x ) \log q ( x ) =  - y \log \hat { y } - ( 1 - y ) \log ( 1 - \hat { y } )<br>$$</p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/优化/" rel="tag"># 优化</a>
          
            <a href="/tags/loss-function/" rel="tag"># loss function</a>
          
            <a href="/tags/Activation-Function/" rel="tag"># Activation Function</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/06/29/特征工程相关概念/" rel="next" title="特征工程">
                <i class="fa fa-chevron-left"></i> 特征工程
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/07/21/那些年的算法题目（二）/" rel="prev" title="那些年的算法题目（二）">
                那些年的算法题目（二） <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
      </div>
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/avatar.jpeg"
                alt="Jijeng Jia" />
            
              <p class="site-author-name" itemprop="name">Jijeng Jia</p>
              <p class="site-description motion-element" itemprop="description">Stay Hungry, Stay Foolish</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">56</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">7</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">31</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/jijeng" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:jia1509309698@163.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          

          <script type="text/javascript" src="//rf.revolvermaps.com/0/0/5.js?i=5kk0u0mm50w&amp;m=0&amp;c=54ff00&amp;cr1=ff0000" async="async"></script>

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Activation-Function"><span class="nav-number">1.</span> <span class="nav-text">Activation Function</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#What"><span class="nav-number">1.1.</span> <span class="nav-text">What?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Activation-Function-Types"><span class="nav-number">1.2.</span> <span class="nav-text">Activation Function Types</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Loss-Function-Error-Function"><span class="nav-number">2.</span> <span class="nav-text">Loss Function(Error Function)</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Jijeng Jia</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>

<div>
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
Total  <span id="busuanzi_value_site_pv"></span> views
You got  <span id="busuanzi_value_site_uv"></span> visitors
</div>




        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      
    </span>
  
</div>








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  

    
      <script id="dsq-count-scr" src="https://https-jijeng-github-io.disqus.com/count.js" async></script>
    

    
      <script type="text/javascript">
        var disqus_config = function () {
          this.page.url = 'http://yoursite.com/2018/07/07/Loss-Activation-and-Optimisation-Function/';
          this.page.identifier = '2018/07/07/Loss-Activation-and-Optimisation-Function/';
          this.page.title = 'Loss Activation and Optimisation Function';
        };
        var d = document, s = d.createElement('script');
        s.src = 'https://https-jijeng-github-io.disqus.com/embed.js';
        s.setAttribute('data-timestamp', '' + +new Date());
        (d.head || d.body).appendChild(s);
      </script>
    

  




	





  














  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
