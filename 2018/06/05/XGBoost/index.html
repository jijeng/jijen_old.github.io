<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="xgboost原理," />










<meta name="description" content="介绍 xgboost 原理, xgboost+ lr 模型和 决策树。">
<meta name="keywords" content="xgboost原理">
<meta property="og:type" content="article">
<meta property="og:title" content="XGBoost">
<meta property="og:url" content="http://yoursite.com/2018/06/05/XGBoost/index.html">
<meta property="og:site_name" content="Jijeng&#39;s blog">
<meta property="og:description" content="介绍 xgboost 原理, xgboost+ lr 模型和 决策树。">
<meta property="og:locale" content="default">
<meta property="og:image" content="https://s2.ax1x.com/2019/11/22/M7O8fO.png">
<meta property="og:image" content="https://s2.ax1x.com/2019/10/04/uBgJoQ.png">
<meta property="og:image" content="https://i.loli.net/2019/10/04/NbFc5hSjsD6VHzY.png">
<meta property="og:image" content="https://ftp.bmp.ovh/imgs/2019/11/0e7f7da922071d57.png">
<meta property="og:image" content="https://upload.cc/i1/2019/11/21/GOAZyS.png">
<meta property="og:updated_time" content="2019-11-23T04:10:59.056Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="XGBoost">
<meta name="twitter:description" content="介绍 xgboost 原理, xgboost+ lr 模型和 决策树。">
<meta name="twitter:image" content="https://s2.ax1x.com/2019/11/22/M7O8fO.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2018/06/05/XGBoost/"/>







<script>
	(function(){
		if(''){
			if (prompt('请输入文章密码','') !== ''){
				alert('密码错误！');
				history.back();
			}
		}
	})();
</script>

  <title>XGBoost | Jijeng's blog</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Jijeng's blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/05/XGBoost/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jijeng Jia">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jijeng's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">XGBoost</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-06-05T21:33:28+08:00">
                2018-06-05
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">Post modified&#58;</span>
              
              <time title="Post modified" itemprop="dateModified" datetime="2019-11-23T12:10:59+08:00">
                2019-11-23
              </time>
            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/06/05/XGBoost/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2018/06/05/XGBoost/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="page-pv"><i class="fa fa-file-o"></i>
            <span class="busuanzi-value" id="busuanzi_value_page_pv" ></span>
            </span>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>介绍 xgboost 原理, xgboost+ lr 模型和 决策树。</p>
<a id="more"></a>
<h2 id="提升树简介"><a href="#提升树简介" class="headerlink" title="提升树简介"></a>提升树简介</h2><h3 id="监督学习的要素"><a href="#监督学习的要素" class="headerlink" title="监督学习的要素"></a>监督学习的要素</h3><ol>
<li>model and parameters</li>
</ol>
<blockquote>
<p>The parameters are the undetermined part that we need to learn from data. In linear regression problems, the parameters are the coefficients $\theta$. </p>
</blockquote>
<p>对于监督学习中，学习的是模型的参数 $\theta$。不论是回归还是分类模型，基本假设是参数表示数据的分布。</p>
<ol start="2">
<li>目标函数= 训练loss function + 正则项<br>$$<br>obj(\theta) =L(\theta) + \Omega(\theta)<br>$$</li>
</ol>
<blockquote>
<p><code>training loss</code> measures how well model fit on training data, while <code>regularization</code>, measures complexity of model<br>其中 $L(\theta)$ 是loss function，表示模型的预测性； $\Omega(\theta) $表示模型的复杂度。 这个的选择就是bias-variance tradeoff 的问题。</p>
</blockquote>
<p>为什么要simple models?</p>
<blockquote>
<p>simple models tends to have smaller variance in future predictions, making prediction stable。<br>一般来说简单的参数，得到的结果是more stable。</p>
</blockquote>
<h3 id="决策树集合（Decision-Tree-Ensembles）"><a href="#决策树集合（Decision-Tree-Ensembles）" class="headerlink" title="决策树集合（Decision Tree Ensembles）"></a>决策树集合（Decision Tree Ensembles）</h3><blockquote>
<p>To begin with, let us first learn about the model choice of XGBoost: decision tree ensembles. The tree ensemble model consists of a set of classification and regression trees (CART).</p>
</blockquote>
<p>xgboost 就是一系列决策树的集合。这些决策树是由一系列的 CART 树组成的。</p>
<blockquote>
<p> A CART is a bit different from decision trees, in which the leaf only contains decision values. In CART, a real score is associated with each of the leaves, which gives us richer interpretations that go beyond classification. This also allows for a principled, unified approach to optimization, as we will see in a later part of this tutorial.</p>
</blockquote>
<p>通常情况下，一棵树是不足以进行预测，所以使用的是决策树的集合。对于某一个样本的结果是多个树的总和。</p>
<p>决策树就是分段函数，所以一个决策树是可以使用 $f(x)$ 进行表示的。<br>$$<br>\hat{y} _{i}=\sum _{k=1} ^{K} f _{k}(x _{i}), f _{k} \in \mathcal{F}<br>$$<br>其中 $K$ 是树的个数（函数的个数）, $f $ 表示某个函数, $ \mathcal{F}$所有的CARTs 可能的总的空间。</p>
<p>于是新的目标函数可以：</p>
<p>$$<br>obj(\theta) =\sum _{i} ^{n} l(y _{i}, \hat{y} _{i})+\sum _{k=1} ^{K} \Omega(f _{k})<br>$$</p>
<blockquote>
<p>Now here comes a trick question: what is the model used in random forests? Tree ensembles! So random forests and boosted trees are really the same models; the difference arises from how we train them. </p>
</blockquote>
<p>但是一个问题出现了，boosted trees 和 random forest 都是相同的模型，其区别在于如何训练。</p>
<h3 id="提升树"><a href="#提升树" class="headerlink" title="提升树"></a>提升树</h3><p>这个小结讲的是如何进行训练的问题。第一小节中陈述的是监督学习中通用模型是通过梯度下降进行训练的，在这个过程中学习到了参数。树模型不能使用梯度下降进行训练，需要把树看做整体。</p>
<ol>
<li>加法学习</li>
</ol>
<blockquote>
<p>It is intractable to learn all the trees at once. Instead, we use an additive strategy: fix what we have learned, and add one new tree at a time.</p>
</blockquote>
<p>它是一种启发式学习，优化的目标不是全部的树。而是每次在上一步的基础上进行优化，每次增加一棵树。</p>
<p>但是还有一个问题，那么如何去选择哪个树呢？当然是使得最后loss 下降的那颗树。</p>
<p>$$<br>\begin{split}<br>obj ^{(t)} &amp;= \sum _{i=1} ^{n} l(y _{i}, \hat{y} _{i} ^{(t)})+\sum _{i=1} ^{t} \Omega(f _{i}) \\<br>&amp; =\sum _{i=1} ^{n} l(y _{i}, \hat{y} _{i} ^{(t-1)}+f _{t}(x _{i}))+\Omega(f _{t})+\text { constant }<br>\end{split}<br>$$</p>
<p>这个时候，使用均方误差作为损失函数，可以得到：</p>
<p>\begin{split}<br>obj _{(t)}  &amp; =\sum _{i=1} ^{n} (y _{i}, \hat{y} _{i} ^{(t-1)}+f _{t}(x _{i})) ^2+ \sum _{i=1} ^t \Omega(f _{i}) \\<br>&amp;=\sum _{i=1} ^{n}[2(\hat{y} _{i} ^{(t-1)}-y _{i}) f _{t}(x _{i})+f _{t}(x _{i}) ^{2}]+\Omega(f _{t})+\text { constant }<br>\end{split}</p>
<p>当使用均方误差的时候，很nice 得到了比较简洁的形式（残差形式），但是如果是其他的损失函数，就没有这种组合成 $f(x), f^2(x)$ 这种形式，所以这个时候使用到的是talor 展开式。所以更加通用的形式如下：<br>$$<br>\mathrm{obj} ^{(t)}=\sum _{i=1} ^{n}[l(y _{i}, \hat{y} _{i} ^{(t-1)})+g _{i} f _{t}(x _{i})+\frac{1}{2} h _{i} f _{t} ^{2}(x _{i})]+\Omega(f _{t})+\mathrm{constant}<br>$$</p>
<p>其中 $g_i$和 $h_i$ 分别定义为如下形式：</p>
<p>$$<br>\begin{split}<br>g _{i} &amp;=\partial _{\hat{y} _{i} ^{(t-1)}} l(y _{i}, \hat{y} _{i} ^{(t-1)}) \\<br>h _{i} &amp;=\partial _{\hat{y} _{i} ^{(t-1)}} ^{2} l(y _{i}, \hat{y} _{i} ^{(t-1)})<br>\end{split}<br>$$</p>
<p>最后去掉所有的常量，在 $t$时刻的目标函数是如下形式：<br>$$<br>\sum _{i=1} ^{n}[g _{i} f _{t}(x _{i})+\frac{1}{2} h _{i} f _{t} ^{2}(x _{i})]+\Omega(f _{t})<br>$$</p>
<p>有了上述通用的形式，那么就可以定义任意的损失函数，只要该损失函数有一阶导数和二阶导数即可， so convinent。</p>
<ol start="2">
<li>模型的复杂度</li>
</ol>
<blockquote>
<p>this is not the only possible definition<br>对于模型的复杂度有多种定义方式，下面的这种是一种常见的并且在实际应用中效果很好的。</p>
</blockquote>
<p>$$<br>\Omega(f)=\gamma T+\frac{1}{2} \lambda \sum_{j=1}^{T} w_{j} ^{2}<br>$$</p>
<p>其中$T$ 表示<code>number of leaves</code>， $w$表示<code>L2 norm of leaf scores</code></p>
<blockquote>
<p>Simpler model means less overfitting and lower variance.</p>
</blockquote>
<p>减少方差和过拟合</p>
<ol start="3">
<li>The Structure Score</li>
</ol>
<p>$$<br>\begin{split}<br>obj ^{(t)} \approx \sum _{i=1} ^{n}[g _{i} w _{q(x _{i})}+\frac{1}{2} h _{i} w _{q(x _{i})} ^{2}]+\gamma T+\frac{1}{2} \lambda \sum _{j=1} ^{T} w _{j} ^{2}<br>=\sum _{j=1} ^{T}[(\sum _{i \in I _{j}} g _{i}) w _{j}+\frac{1}{2}(\sum _{i \in I _{j}} h _{i}+\lambda) w _{j} ^{2}]+\gamma T<br>\end{split}<br>$$</p>
<p>公式出现 $\approx$， 这个是因为使用一阶导数和二阶导数去近似地代替原始的函数，这个在计算的时候比较方便~。这个时候可以看成是一个二次方程，那么就可以求解最值了。</p>
<ol start="4">
<li>学习树的结构 （additive training）</li>
</ol>
<blockquote>
<p>We can not use methods such as SGD, to find f (since they are trees, instead of just numerical vectors)<br>需要使用 <code>Additive Training (boosting)</code>， 以树为单位进行加法和减法。</p>
</blockquote>
<p>$$<br>\text {Gain}=\frac{1}{2}[\frac{G _{L}  ^{2}}{H _{L}+\lambda}+\frac{G _{R}  ^{2}}{H _{R}+\lambda}-\frac{(G _{L}+G _{R})  ^{2}}{H _{L}+H _{R}+\lambda}]-\gamma<br>$$</p>
<p>这个时候令 $G _{j}=\sum _{i \in I _{j}} g _{i}$ ， $H _{j}=\sum _{i \in I _{i}} h _{i}$， 所以可以化简成以下的形式：<br>$$<br>\mathrm{obj} ^{(t)}=\sum _{j=1} ^{T}[G _{j} w _{j}+\frac{1}{2}(H _{j}+\lambda) w _{j} ^{2}]+\gamma T<br>$$</p>
<p>二次方程是可以求解到最值的，那么可以得到：</p>
<p>$$<br>\begin{split}<br>w _{j} ^{<em>} &amp;=-\frac{G _{j}}{H _{j}+\lambda} \\<br>obj ^{</em>} &amp;=-\frac{1}{2} \sum _{j=1} ^{T} \frac{G _{j} ^{2}}{H _{j}+\lambda}+\gamma T<br>\end{split}<br>$$<br>进行分裂时的策略， 如果分裂的左右子树是能够有信息的增加，那么就进行分裂，否则的话不进行分裂，如果式子前半部小于 $\gamma$的话，那么就进行剪枝。</p>
<p>$$<br>obj=-\sum _{j} \frac{G _{j} ^{2}}{H _{j}+\lambda}+3 \gamma<br>$$</p>
<blockquote>
<p>This score is like the impurity measure in a decision tree, except that it also takes the model complexity into account.</p>
</blockquote>
<p>其中，最后的数值越小，那么结构也是越好。（从这个角度出发是不是和基尼系数相似）</p>
<ol start="5">
<li>分裂点的寻找</li>
</ol>
<ul>
<li><p>For each node, enumerate over all features</p>
<ul>
<li>for each feature, sorted the instances by feature value</li>
<li>use a linear scan to decide the best split along that feature</li>
<li>take the best split solution along all the features</li>
</ul>
</li>
<li><p>对于每一个结点，都遍历所有的特征</p>
<ul>
<li>对于每一个特征，样本按照特征的值进行排序</li>
<li>使用线性扫描的方式，决定最好的分裂点</li>
<li>在所有的特征中，按照最优的分裂点进行分裂</li>
</ul>
</li>
</ul>
<p>时间复杂度分析</p>
<ul>
<li>Time Complexity growing a tree of depth$ K$<ul>
<li>It is $O(n d K log n) $: or each level, need $O(n log n) $time to sort, there are $d$ features, and we need to do it for $K$ level</li>
<li>This can be further optimized (e.g. use approximation or caching the sorted features)</li>
<li>Can scale to very large dataset</li>
</ul>
</li>
</ul>
<p>（1） Basic Exact Greedy Algorithm</p>
<p>暴力解法就是exact greedy algorithm。做法：在所有可能的分裂点上遍历所有的features 的值。效果虽然好，但是时间效率不高。<br>（2）approximate algorithm<br>exact greedy algorithm的缺点除了时间成本，还有内存上方面的考虑。当数据无法一次性读入内存，那么就不能使用。approximate algorithm：</p>
<pre><code>- 根据feature 的分布选择候选分裂点
- 根据分裂点划分连续数值的feature，分成不同的桶
- 根据桶find 最好的结果（遍历成本就小了）
</code></pre><ol start="5">
<li>sparsity-aware split finding</li>
</ol>
<p>有三种来源：</p>
<ul>
<li>missing values in the data</li>
<li>frequent zero entries in the statistics </li>
<li>artifacts of feature engineering such as one-hot encoding</li>
</ul>
<p>类别变量是第三种情况：</p>
<blockquote>
<p>Actually it is not necessary to handle categorical separately</p>
<ul>
<li>We can encode the categorical variables into numerical vector using one-hot encoding. Allocate a #categorical length vector</li>
<li>The vector will be sparse if there are lots of categories, the learning algorithm is preferred to handle sparse data</li>
</ul>
</blockquote>
<p>把类别变量进行one-hot 转换成数值类型，那么最后就可以使用树的模型进行计算；算法比较擅长处理稀疏数据</p>
<ol start="6">
<li>Shrinkage and Column Subsampling</li>
</ol>
<blockquote>
<p>Shrinkage scales newly added weights by a factor η after each step of tree boosting. Similar to a learning rate in stochastic optimization, shrinkage reduces the influence of each individual tree and leaves space for future trees to improve the model.</p>
</blockquote>
<p>Shrinkage 的思想和 stochastic optimizer中类似，减弱了当前的结点的影响力，leave space for future or before trees</p>
<blockquote>
<p>According to user feedback, using column sub-sampling prevents over-fitting even more so than the traditional row sub-sampling (which is also supported). The usage of column sub-samples also speeds up computations of the parallel algorithm described later.</p>
</blockquote>
<p>对于行（数据的数量）采样的思想早就有了，xgboost 中实现了列采样，不仅有利于加快计算速度（并行），也可以缓解过拟合（the idea from random forest）</p>
<ol start="7">
<li>学习的过程</li>
</ol>
<p>在监督学习中，不断减去 $\frac{\partial f(x)}{\partial x}$， 可以得到 $min_xf(x)$。同理不断减去 $\frac{\partial l}{\partial F}$，这样就得到了 $min_FL(F)$。只不过在决策树中使用的是一阶梯度，但是在xgboost 中使用的二阶梯度。</p>
<h3 id="优势和不足"><a href="#优势和不足" class="headerlink" title="优势和不足"></a>优势和不足</h3><p>Advantages</p>
<blockquote>
<ol>
<li>Invariant to scale of feature, need less data preprocessing, But it never means we don’t need to clean and transform features</li>
</ol>
</blockquote>
<p>不需要进行减均值除方差的操作，但是基本的数据清洗还是要有的</p>
<blockquote>
<ol start="2">
<li>Naturally support categorical feature<br>自动处理类别特征，不需要手动one-hot</li>
</ol>
</blockquote>
<p>Disadvantages</p>
<blockquote>
<ol>
<li>Overfitting<br>过拟合</li>
</ol>
</blockquote>
<p>疑问？<br><img src="https://s2.ax1x.com/2019/11/22/M7O8fO.png" alt="M7O8fO.png"><br>图中叶子节点上的数值是如何得到，有什么计算方法吗？</p>
<h3 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h3><p><a href="https://xgboost.readthedocs.io/en/latest/tutorials/model.html#introduction-to-boosted-trees" target="_blank" rel="noopener">Introduction to Boosted Trees</a><br><a href="https://arxiv.org/abs/1603.02754" target="_blank" rel="noopener">XGBoost: A Scalable Tree Boosting System</a></p>
<h2 id="xgboost-lr模型"><a href="#xgboost-lr模型" class="headerlink" title="xgboost +lr模型"></a>xgboost +lr模型</h2><p>简单地说，就是把gbdt的输出，作为logistic regression的输入，最后得到一个logistic regression模型。</p>
<p>例如，gbdt里有3棵树T1,T2,T3，每棵树的叶节点个数为4，第i个树的第j个叶节点是Li,j。</p>
<p>当gdbt训练完成之后，样本X1在第一棵树中被分到了第3个叶节点上，也就是L1,3，那么这个样本在T1上的向量表达为(0,0,1,0)。</p>
<p>样本X1在T2被分到了L2,1，那么X1在T2上的向量表达为(1,0,0,0)<br>样本X1在T3被分到了L3,4，那么X1在T3上的向量表达为(0,0,0,1)<br>那么X1在整个gbdt上的向量表达为</p>
<p>(0,0,1,0,1,0,0,0,0,0,0,1)<br>所以每个样本都会被表示为一个长度为12的0-1向量，其中有3个数值是1。</p>
<p>然后这类向量就是LR模型的输入数据。</p>
<ol>
<li>GBDT + LR 是什么<br>本质上GBDT+LR是一种具有stacking思想的二分类器模型，所以可以用来解决二分类问题。这个方法出自于Facebook 2014年的论文 Practical Lessons from Predicting Clicks on Ads at Facebook 。</li>
</ol>
<p>GBDT+LR 使用最广泛的场景是CTR点击率预估，即预测当给用户推送的广告会不会被用户点击。</p>
<p><a href="https://imgchr.com/i/uBgJoQ" target="_blank" rel="noopener"><img src="https://s2.ax1x.com/2019/10/04/uBgJoQ.png" alt="uBgJoQ.png"></a></p>
<p>离线部分</p>
<ol>
<li>数据收集：主要收集和业务相关的数据，通常会有专门的同事在app位置进行埋点，拿到业务数据</li>
<li>预处理：对埋点拿到的业务数据进行去脏去重；</li>
<li>构造数据集：经过预处理的业务数据，构造数据集，在切分<strong>训练、测试、验证集</strong>时应该合理根据业务逻辑来进行切分；</li>
<li>特征工程：对原始数据进行基本的特征处理，包括去除相关性大的特征，<strong>离散变量one-hot，连续特征离散化</strong>等等;</li>
<li>模型选择：选择合理的机器学习模型来完成相应工作，原则是先从简入深，先找到baseline，然后逐步优化；</li>
<li>超参选择：利用gridsearch、randomsearch或者hyperopt来进行超参选择，选择在离线数据集中性能最好的超参组合；</li>
<li>在线A/B Test：选择优化过后的模型和原先模型（如baseline）进行A/B Test，若性能有提升则替换原先模型；</li>
</ol>
<p>在线部分</p>
<ol>
<li>Cache &amp; Logic：设定简单过滤规则，过滤异常数据；</li>
<li>模型更新：当Cache &amp; Logic 收集到合适大小数据时，对模型进行pretrain+finetuning，若在测试集上比原始模型性能高，则更新model server的模型参数；</li>
<li>Model Server：接受数据请求，返回预测结果；</li>
</ol>
<p>这个可以参考 <a href="https://www.cnblogs.com/wkang/p/9657032.html" target="_blank" rel="noopener">GBDT+LR算法解析及Python实现</a></p>
<p>过程</p>
<blockquote>
<p>用已有特征训练 GBDT 模型，然后利用 GBDT 模型学习到的树来构造新特征，最后把这些新特征加入原有特征一起训练模型。构造的新特征向量是取值 0/1 的，向量的每个元素对应于 GBDT 模型中树的叶子结点。当一个样本点通过某棵树最终落在这棵树的一个叶子结点上，那么在新特征向量中这个叶子结点对应的元素值为 1，而这棵树的其他叶子结点对应的元素值为 0。新特征向量的长度等于 GBDT 模型里所有树包含的叶子结点数之和。</p>
</blockquote>
<p>XGBoost + LR 融合方式原理很简单。先用数据训练一个 XGBoost 模型，然后将训练数据中的实例给 XGBoost 模型得到实例的叶子节点，然后将叶子节点当做特征训练一个 LR 模型。得到的是一种 transformer 之后的特征，不能完全取代之前的特征工程。</p>
<p>此过程需注意： sklearn 或者 xgboost 输出的结果都是叶子节点的 index，所以需要自己动手去做 onehot 编码，然后交给 lr 训练，onehot 可以在 sklearn 的预处理包中调用即可。下面进入正题。</p>
<p>论文中 GBDT 的参数，树的数量最多 500 颗（500 以上就没有提升了），每棵树的节点不多于 12。</p>
<p>当树的颗数增大时，叶子结点数也相应成倍数的增加，造成lr模型训练时间拉长，同时也需要更大的惩罚项来适应；</p>
<p>结论：当xgb训练充分时，lr直接利用xgb叶子结点的编码特征在合适的惩罚系数下可以训练得到和xgb一样的甚至更好的效果（不显著）；</p>
<p><img src="https://i.loli.net/2019/10/04/NbFc5hSjsD6VHzY.png" alt="1.png"></p>
<p>顺便来讲，RF也是多棵树，但从效果上有实践证明不如GBDT。且GBDT前面的树，特征分裂主要体现对多数样本有区分度的特征；后面的树，主要体现的是经过前N颗树，残差仍然较大的少数样本。优先选用在整体上有区分度的特征，再选用针对少数样本有区分度的特征，思路更加合理，这应该也是用GBDT的原因。</p>
<p>现在，我们思考这样一个问题，Logistic Regression是一个线性分类器，也就是说会忽略掉特征与特征之间的关联信息，那么是否可以采用构建新的交叉特征这一特征组合方式从而提高模型的效果？</p>
<p>其次，我们已经在2.3小节中了解到GBDT很有可能构造出的新训练数据是高维的稀疏矩阵，而Logistic Regression使用高维稀疏矩阵进行训练，会直接导致计算量过大，特征权值更新缓慢的问题。</p>
<p>针对上面可能出现的问题，可以翻看我之前的文章：FM算法解析及Python实现 ，使用FM算法代替LR，这样就解决了Logistic Regression的模型表达效果及高维稀疏矩阵的训练开销较大的问题。然而，这样就意味着可以高枕无忧了吗？当然不是，因为采用FM对本来已经是高维稀疏矩阵做完特征交叉后，新的特征维度会更加多，并且由于元素非0即1，新的特征数据可能也会更加稀疏，那么怎么办？</p>
<p>所以，我们需要再次回到GBDT构造新训练数据这里。当GBDT构造完新的训练样本后，我们要做的是对每一个特征做与输出之间的特征重要度评估并筛选出重要程度较高的部分特征，这样，GBDT构造的高维的稀疏矩阵就会减少一部分特征，也就是说得到的稀疏矩阵不再那么高维了。之后，对这些筛选后得到的重要度较高的特征再做FM算法构造交叉项，进而引入非线性特征，继而完成最终分类器的训练数据的构造及模型的训练。</p>
<h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><ol>
<li>介绍GBDT 中防止过拟合的方法：</li>
</ol>
<ul>
<li>限制树的高度</li>
<li>采样（训练每一棵树的时候，只使用一部分样本）</li>
<li>列采样（训练每一个树的时候，只使用一部分特征。这是xgboost 中的创新，将随机森林的思想引入了GBDT 中）</li>
<li>shrinkage（衰减稀疏，惩罚系数）</li>
<li>early stop（在深度学习中，early stop 的指标是根据某个指标，但是在树的模型中，early stop是根据要不要加上某个树，注意效果的类比）</li>
</ul>
<ol start="2">
<li>启发式学习<blockquote>
<p>启发式算法（heuristic algorithm)是相对于最优化算法提出的。启发式算法可以这样定义：一个基于直观或经验构造的算法，在可接受的花费（指计算时间和空间）下给出待解决组合优化问题每一个实例的一个可行解，该可行解与最优解的偏离程度一般不能被预计。现阶段，启发式算法以仿自然体算法为主，主要有蚁群算法、模拟退火法、神经网络等。</p>
</blockquote>
</li>
</ol>
<ol start="3">
<li>图解常见的集成学习</li>
</ol>
<p>bagging 的特点：</p>
<ul>
<li>Weak learners are independent.</li>
<li>Bagging can be easily parallelized.</li>
</ul>
<p><img src="https://ftp.bmp.ovh/imgs/2019/11/0e7f7da922071d57.png" alt=""></p>
<p>adaboost的特点：</p>
<p><img src="https://upload.cc/i1/2019/11/21/GOAZyS.png" alt="img"></p>
<blockquote>
<p>Guarantee: sum of unnormalized weights is monotonously decreasing. Sum of unnormalized weights measures the error.</p>
</blockquote>
<p>通过重采样，重新分配权重来处理bad 样本。（这点不是很理解哈？）</p>
<p>Gradient Boosting 的特点：</p>
<p>[img]<a href="https://upload.cc/i1/2019/11/21/EVuaCp.png[/img]" target="_blank" rel="noopener">https://upload.cc/i1/2019/11/21/EVuaCp.png[/img]</a></p>
<blockquote>
<p>Guarantee: sum of residuals is monotonously decreasing. Residuals are highly related to loss.</p>
</blockquote>
<p>通过减少残差的方式减少误差。</p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/xgboost原理/" rel="tag"># xgboost原理</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/05/28/等概率生成器/" rel="next" title="等概率生成器">
                <i class="fa fa-chevron-left"></i> 等概率生成器
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/06/05/对抗性生成网络/" rel="prev" title="对抗生成网络实验对比">
                对抗生成网络实验对比 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
      </div>
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/avatar.jpeg"
                alt="Jijeng Jia" />
            
              <p class="site-author-name" itemprop="name">Jijeng Jia</p>
              <p class="site-description motion-element" itemprop="description">Stay Hungry, Stay Foolish</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">107</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">6</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">79</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/jijeng" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:jia1509309698@163.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          

          <script type="text/javascript" src="//rf.revolvermaps.com/0/0/5.js?i=5kk0u0mm50w&amp;m=0&amp;c=54ff00&amp;cr1=ff0000" async="async"></script>


  <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
  <script src="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
  <script src="//cdn.bootcss.com/blueimp-md5/1.1.0/js/md5.min.js"></script>
  <script>
      var gitalk = new Gitalk({
        clientID: '8c6403951ee3eab4e420',
        clientSecret: 'd842e48ca0c28ec41200f973ba52f96ba975b441',
        repo: 'jijeng.github.io',
        owner: 'jia1509309698@163.com',
        admin: 'jia1509309698@163.com',
        id: md5(location.pathname),
        distractionFreeMode: 'true'
      });
      var div = document.createElement('div');
      div.setAttribute("id", "gitalk_comments");
      div.setAttribute("class", "post-nav");
      var bro = document.getElementById('posts').getElementsByTagName('article');
      bro = bro[0].getElementsByClassName('post-block');
      bro = bro[0].getElementsByTagName('footer');
      bro = bro[0];
      bro.appendChild(div);
      gitalk.render('gitalk_comments');
  </script>

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#提升树简介"><span class="nav-number">1.</span> <span class="nav-text">提升树简介</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#监督学习的要素"><span class="nav-number">1.1.</span> <span class="nav-text">监督学习的要素</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#决策树集合（Decision-Tree-Ensembles）"><span class="nav-number">1.2.</span> <span class="nav-text">决策树集合（Decision Tree Ensembles）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#提升树"><span class="nav-number">1.3.</span> <span class="nav-text">提升树</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#优势和不足"><span class="nav-number">1.4.</span> <span class="nav-text">优势和不足</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#参考文献"><span class="nav-number">1.5.</span> <span class="nav-text">参考文献</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#xgboost-lr模型"><span class="nav-number">2.</span> <span class="nav-text">xgboost +lr模型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#其他"><span class="nav-number">3.</span> <span class="nav-text">其他</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Jijeng Jia</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>

<div>
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
Total  <span id="busuanzi_value_site_pv"></span> views
You got  <span id="busuanzi_value_site_uv"></span> visitors
</div>




        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      
    </span>
  
</div>








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  

    
      <script id="dsq-count-scr" src="https://https-jijeng-github-io.disqus.com/count.js" async></script>
    

    
      <script type="text/javascript">
        var disqus_config = function () {
          this.page.url = 'http://yoursite.com/2018/06/05/XGBoost/';
          this.page.identifier = '2018/06/05/XGBoost/';
          this.page.title = 'XGBoost';
        };
        var d = document, s = d.createElement('script');
        s.src = 'https://https-jijeng-github-io.disqus.com/embed.js';
        s.setAttribute('data-timestamp', '' + +new Date());
        (d.head || d.body).appendChild(s);
      </script>
    

  




	





  














  <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
  <script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>
   <script type="text/javascript">
        var gitalk = new Gitalk({
          clientID: '',
          clientSecret: '',
          repo: 'jijeng.github.io',
          owner: '',
          admin: [''],
          id: location.pathname,
          distractionFreeMode: ''
        })
        gitalk.render('gitalk-container')           
       </script>


  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
