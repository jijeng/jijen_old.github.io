<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="pca,svd,t-sne," />










<meta name="description" content="关于特征工程的概念，特征离散化、特征生成、组合特征、特征选取的方法。">
<meta name="keywords" content="pca,svd,t-sne">
<meta property="og:type" content="article">
<meta property="og:title" content="特征工程">
<meta property="og:url" content="http://yoursite.com/2018/06/29/特征工程相关概念/index.html">
<meta property="og:site_name" content="Jijeng&#39;s blog">
<meta property="og:description" content="关于特征工程的概念，特征离散化、特征生成、组合特征、特征选取的方法。">
<meta property="og:locale" content="default">
<meta property="og:image" content="https://ftp.bmp.ovh/imgs/2019/09/d993b830928805b8.png">
<meta property="og:updated_time" content="2019-11-13T06:34:23.677Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="特征工程">
<meta name="twitter:description" content="关于特征工程的概念，特征离散化、特征生成、组合特征、特征选取的方法。">
<meta name="twitter:image" content="https://ftp.bmp.ovh/imgs/2019/09/d993b830928805b8.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2018/06/29/特征工程相关概念/"/>







<script>
	(function(){
		if(''){
			if (prompt('请输入文章密码','') !== ''){
				alert('密码错误！');
				history.back();
			}
		}
	})();
</script>

  <title>特征工程 | Jijeng's blog</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Jijeng's blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/29/特征工程相关概念/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jijeng Jia">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jijeng's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">特征工程</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-06-29T22:28:25+08:00">
                2018-06-29
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">Post modified&#58;</span>
              
              <time title="Post modified" itemprop="dateModified" datetime="2019-11-13T14:34:23+08:00">
                2019-11-13
              </time>
            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/06/29/特征工程相关概念/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2018/06/29/特征工程相关概念/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="page-pv"><i class="fa fa-file-o"></i>
            <span class="busuanzi-value" id="busuanzi_value_page_pv" ></span>
            </span>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>关于特征工程的概念，特征离散化、特征生成、组合特征、特征选取的方法。</p>
<a id="more"></a>
<h2 id="特征生成"><a href="#特征生成" class="headerlink" title="特征生成"></a>特征生成</h2><p>机器学习的一大任务在于手工 create 特征。特征主要来源有两个，一个统计学知识，一个该场景下的特征。前者属于常规操作，后者需要有对业务领域比较熟悉的了解。后者举个例子，比如在衡量个人信用的时候， days_employed / days_birth 就是一个属于金融（保险）领域的较为熟悉，才能理解的一个特征, 这种百分比，占比的思想还是非常常见的。前者同样举个栗子，对于子表（传统机器学习还是有很多来自数据库的信息的）常用的操作是 aggregation 操作（min max sum variance mean）等聚合操作，然后和主表进行连接。一般来说子表是可以广泛的使用 aggregation 操作，但是对于主表来说，这个就取决于信息是什么，特征是什么内容，聚合函数的本质在于”总结“，就是你操作的变量是否有必要这样做，看一下数据是不是流水账。</p>
<h2 id="特征离散化"><a href="#特征离散化" class="headerlink" title="特征离散化"></a>特征离散化</h2><p>连续化特征就是一些不可枚举的有理数。那么什么是离散化特征呢？ 离散化特征就是可枚举的特征。离散化的作用是把数据变成可计算状态。而特征工程就是从原始字段中根据业务提取出对模型有效的特征出来。</p>
<p>在线性模型下(w.x)，w已经确定的情况下，x的某个特征的值是20，或者30，w.x的值相差是很大的，但是往往20岁的人跟30岁的人对同一个广告的兴趣差距不会那么大。连续特征离散化的基本假设，是默认连续特征不同区间的取值对结果的贡献是不一样的。离散化和连续化最大的区别是，对一个字段做连续化后的结果就还只是一个特征，而离散化后的这一列有多少个key(字段可能的值)就会抽取出多少个特征。当经过离散化之后，特征各有各的权重，彼此之间就没有关系了。</p>
<p>模型是使用离散特征还是连续特征, <strong>其实是一个“海量离散特征+简单模型” 同 “少量连续特征+复杂模型”的权衡</strong>。既可以离散化用线性模型，也可以用连续特征加深度学习。</p>
<p>常用的选取离散点的方法：等距离离散，等样本离散、画图观察趋势和决策树模型(天生就可以对连续特征分段)。</p>
<p>离散化：变量分箱（在风控模型中是这样进行阐述的），是对于连续变量的离散化的一种称呼。分箱的方式，一般有等距离分段，等深分段（先确定分段数量，然后令每个分段中的数据数量大致相等）和最优分段。</p>
<p>在工业界，很少直接将连续值作为特征喂给逻辑回归模型，而是将连续特征离散化为一系列0、1特征交给逻辑回归模型，这样做的优势有以下几点：</p>
<ul>
<li>单变量离散化为N个后，每个变量有单独的权重，在激活函数的作用下相当于为<strong>模型增加了非线性</strong>，能够提升模型表达能力，加大拟合。</li>
<li><strong>离散化后的特征对异常数据有很强的鲁棒性</strong>：比如一个特征是年龄&gt;30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰，因为特征值的异常会导致权重也就是w的值也会异常。</li>
<li>一定有同学担心特征过多会导致运算缓慢，但是LR是线性模型，我们在内部计算的时候是向量化计算，而不是循环迭代。稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展。</li>
</ul>
<p>所以不用担心像GBDT算法那样，特征多了就跑不动了(我们都说GBDT不能用离散特征不是因为它处理不了离散特征，而是因为离散化特征后会产生特别多的特征，决策树的叶子节点过多，遍历的时候太慢了)。 所以海量离散特征＋LR是业内常见的一个做法。而少量连续特征+复杂模型是另外一种做法，例如GBDT。</p>
<p>当然也可以通过深度学习来进行特征选择：目前这种手段正在随着深度学习的流行而成为一种手段，尤其是在计算机视觉领域，原因是深度学习具有自动学习特征的能力，这也是深度学习又叫unsupervised feature learning的原因。从深度学习模型中选择某一神经层的特征后就可以用来进行最终目标模型的训练了。</p>
<p>参考文献:<br><a href="https://blog.csdn.net/lujiandong1/article/details/52412123" target="_blank" rel="noopener">https://blog.csdn.net/lujiandong1/article/details/52412123</a></p>
<h2 id="组合特征"><a href="#组合特征" class="headerlink" title="组合特征"></a>组合特征</h2><p>先是离散化，然后是特征组合。<br>交叉从理论上而言是为了引入特征之间的交互，也即为了引入非线性。<br>LR(逻辑回归）分类算法:因为线性函数的表达能力有限，所以我们引入激活函数就是给LR增加非线性关系。能让一条直线变成曲线。这样可以拟合出更好的效果。（也由此才后了后来说的过拟合问题而引入了正则化超参数）</p>
<p>LR模型之所以很受欢迎，主要是因为LR模型本质是对数线性模型，实现简单，易于并行，大规模扩展方便，迭代速度快，同时使用的特征比较好解释，预测输出在0与1之间契合概率模型。（模型的可解释性举例，比如A-B的权重比较大，A代表用户，B代表物品，那么可以认为A是对B比较感兴趣的）但是，线性模型对于非线性关系缺乏准确刻画，特征组合正好可以加入非线性表达，增强模型的表达能力。另外，广告LR中，基本特征可以认为是用于全局建模，组合特征更加精细，是个性化建模，因为在这种大规模离散LR中，单对全局建模会对部分用户有偏，对每一用户建模又数据不足易过拟合同时带来模型数量爆炸，所以基本特征+组合特征兼顾了全局和个性化。</p>
<p>从统计的角度解释，基本特征仅仅是真实特征分布在低维的映射，不足以描述真实的分布，加入特征在高维空间拟合真实分布，使得预测更加准确。</p>
<p>寻找高级特征最常用的方法有：<br>若干项特征加和： 我们假设你希望根据每日销售额得到一周销售额的特征。你可以将最近的7天的销售额相加得到。<br>若干项特征之差： 假设你已经拥有每周销售额以及每月销售额两项特征，可以求一周前一月内的销售额。<br>若干项特征乘积： 假设你有商品价格和商品销量的特征，那么就可以得到销售额的特征。<br>若干项特征除商： 假设你有每个用户的销售额和购买的商品件数，那么就是得到该用户平均每件商品的销售额。</p>
<h2 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h2><p>真正测试一个模型的不是简单与否，更重要在于它在预测新的情况时表现如何。小权重意味着网络的行为不会因为我们随意更改了一些输入而改变太多。</p>
<p>这里是需要使用latex 补充一下公式的。公式是可以参考着上面进行书写。</p>
<p><a href="https://testerhome.com/topics/10811" target="_blank" rel="noopener">https://testerhome.com/topics/10811</a></p>
<p>这是我们加了正则化之后的成本函数，可以看我们后面加入了正则化 λ 的表达式来完善成本函数。为什么加入λ能够减轻过拟合呢？直观一点的解释是设置的λ值越大，那么参数w的值就会被压缩的越小(在梯度下降中, 每次迭代的步长，也就是这个公式w=w - 学习率*成本函数对w的导数， 现在由于成本函数增加了正则项，使得J和w变得数值相关了)。 假设λ设置的足够大，那么w会无限的趋近于0. 把多隐藏层的单元的权重设置为0以后，那么基本上就是消除掉了这些单元的作用，而使得网络模型得到简化，就像下面的图一样。由于正则化的设置，消除了一些隐藏单元的作用。而使得整个模型越来越接近于线性化，也就是从下图中的过拟合往欠拟合偏转。当然我们有一个适合的λ的值，能让我们的拟合状态达到最佳。所以我们在训练模型的时候，往往都会有一个Ｌ２正则项的超参数需要我们设置。</p>
<h2 id="feature-selection"><a href="#feature-selection" class="headerlink" title="feature selection"></a>feature selection</h2><p>定义：</p>
<p>feature selection 的过程就是dimension reduction的过程。就是说由较多的数据集 映射到 较少的数据集，这种方式就叫做降维。</p>
<blockquote>
<p>Using dimensionality reduction techniques, of course. You can use this concept to reduce the number of features in your dataset without having to lose much information and keep (or improve) the model’s performance.</p>
</blockquote>
<p>为什么? （必要性分析）</p>
<p>时间角度，空间（内存）角度。减少冗余信息，就减少了模型去拟合”噪声“数据的可能性。</p>
<p>常见的有以下几种方式：</p>
<p><strong> 可以归结成几类：特征本身（数据缺省值比较大，数据的波动性比较小），特征和特征之间（特征具有较高的相关性，使用PCA 进行降维），特征和最后的target的关系（机器学习模型的 feature importance，卡方分布检测特征和target 的重要性，Pearson 相关系数：-1 表示负相关，0 表示不相关，1表示正相关）。</strong></p>
<p>可以划分成三类：</p>
<p>一、独立于模型的特征选择（没有在入模时候进行的特征选择）或者叫做 filter：</p>
<ol>
<li>移除低方差的特征 (Removing features with low variance)</li>
</ol>
<p>当特征值都是离散型变量的时候这种方法才能用，如果是连续型变量，就需要将连续变量离散化之后才能用。</p>
<ol start="2">
<li>单变量特征选择 (Univariate feature selection)</li>
</ol>
<p>对于分类问题(y离散)，可采用：卡方检验，f_classif, mutual_info_classif，互信息<br>对于回归问题(y连续)，可采用：皮尔森相关系数，f_regression, mutual_info_regression，最大信息系数</p>
<p>这里说的是特征选择，但是上面说的都是针对“特征重要性” 这点展开的，但是一个特征入模是一个复杂的过程，需要考虑的因素很多。比如：变量的预测能力，变量之间的相关性，变量的简单性（容易生成和使用），变量的强壮性（不容易被绕过），变量在业务上的可解释性（被挑战时可以解释的通）等等。当然，其中最主要和最直接的衡量标准是变量的预测能力。</p>
<p>尤其是当你使用LR 这类简单的模型的时候，是需要重点的在特征上下功夫的，因为模型是线性的，比较简单，引入特征，加入非线性，然后才能更好的表达实际问题。</p>
<p>二、基于模型选择的特征排序</p>
<p>有些机器学习方法本身就具有对特征进行打分的机制，或者很容易将其运用到特征选择任务中，例如回归模型，SVM，决策树，随机森林等等。这种方法的思路是直接使用你要用的机器学习算法，针对 每个单独的特征 和 响应变量建立预测模型。假如 特征 和 响应变量 之间的关系是非线性的，可以用基于树的方法(决策树、随机森林)、或者 扩展的线性模型 等。</p>
<p>三、无监督的模型选择</p>
<p>聚类，可以从降维的角度理解。可以在机器学习算法中的importance 不是很大，容易被忽略的特征。</p>
<h3 id="Principal-Component-Analysis-PCA"><a href="#Principal-Component-Analysis-PCA" class="headerlink" title="Principal Component Analysis (PCA)"></a>Principal Component Analysis (PCA)</h3><p>为什么要进行降维数据的处理？</p>
<p>可以从 计算和存储的角度分析，还可以从模型和数据的角度分析。</p>
<p>这个属于一句话的的思想：PCA的问题其实是一个基的变换，使得变换后的数据有着最大的方差。</p>
<p>pca 的假设：<br>这个说得很清楚，higher dimensional space is mapped to lower dimension, 然后这种在lower dimensional 中数据的variance应该是保持max的。<br>It works on a condition that while the data in a higher dimensional space is mapped to data in a lower dimension space, the variance of the data in the lower dimensional space should be maximum.</p>
<p>就是需要理解这个主成分是不断的生成的，在前者的基础之上生成的。<br>below are some of the key points you should know about PCA before proceeding further:</p>
<ul>
<li>A principal component is a linear combination of the original variables</li>
<li>Principal components are extracted in such a way that the first principal component explains maximum variance in the dataset</li>
<li>Second principal component tries to explain the remaining variance in the dataset and is uncorrelated to the first principal component</li>
<li><p>Third principal component tries to explain the variance which is not explained by the first two principal components and so on</p>
<p>The first component is the most important one, followed by the second, then the third, and so on.</p>
</li>
</ul>
<p>如何去理解PCA 降维？<br>PCA 通过线性变换将原始数据变换成一组各维度线性无关的表示，可以用于提取数据的主要特征分量，将用于高纬数据的降维。我们期待的是降维的同时将信息的损失尽量降低。</p>
<p>矩阵相乘的意义：将右边矩阵中的每一列列向量变换到左边矩阵中每一行行向量为基所表示的向量空间中去。更加抽象的说，一个矩阵可以表示一种线性变换。</p>
<p>我们希望投影后投影值尽可能分散，而这种分散程序，可以用数学中的方差进行表示。</p>
<p>PCA 算法步骤</p>
<p>假设原始数据按列组成n行m列矩阵X</p>
<ol>
<li>将X的每一行（代表一个属性字段）进行零均值化，即减去这一行的均值</li>
<li>求出协方差矩阵$C=\frac{1}{m}XX^\mathsf{T}$</li>
<li>求出协方差矩阵的特征值及对应的特征向量</li>
<li>将特征向量按对应特征值大小从上到下按行排列成矩阵，取前k行组成矩阵P</li>
<li>$Y=PX$即为降维到k维后的数据</li>
</ol>
<h3 id="SVD"><a href="#SVD" class="headerlink" title="SVD"></a>SVD</h3><p>SVD decomposes the original variables into three constituent matrices. It is essentially used to remove redundant features from the dataset. It uses the concept of Eigenvalues and Eigenvectors to determine those three matrices. We will not go into the mathematics of it due to the scope of this article, but let’s stick to our plan, i.e. reducing the dimensions in our dataset.</p>
<p>$$<br>\operatorname { Data } _ { m \times n } = U _ { m \times m } \Sigma _ { m \times n } V _ { n \times n } ^ { T }<br>$$<br>SVD将原始的数据集矩阵Data分解成三个矩阵：U、Sigma、VT，如果原始矩阵是m行n列，那么U、Sigma和VT分别就是m行m列、m行n列、n行n列。比较值得一提的是矩阵Sigma，该矩阵只有对角元素，其他元素均为0，有一个惯例是：Sigma的对角元素是从大到小排列的。这些对角元素就称为奇异值.</p>
<p>PCA 是方阵是 $ m^2 $ 操作，那么SVD 是 $mn$ 就是更加广泛的矩阵操作。</p>
<p>特征分解只能分解方阵，奇异值分解可以分解任意矩阵，pca中的特征分解通常会使用svd。（方阵是一种特殊的矩阵，当行数和列数相同的时候就叫做方阵）</p>
<h3 id="投影也是一种降维手段"><a href="#投影也是一种降维手段" class="headerlink" title="投影也是一种降维手段"></a>投影也是一种降维手段</h3><p>这种思想真的是服气，虽然我也不是很懂，但是思想是很好的<br>By projecting one vector onto the other, dimensionality can be reduced.</p>
<p>当投影到另一个平面的时候，原来的平面维度就消失了，所以只剩下了投影面的维度。</p>
<h3 id="T-sne"><a href="#T-sne" class="headerlink" title="T-sne"></a>T-sne</h3><p>就是指出 t-sne 这个可以非线性降维。有local approaches 和 global approaches 两种方式，我大概的理解就是：类之间还能尽可能的远离，类内保持差异性。（不知道对不对）<br>So far we have learned that PCA is a good choice for dimensionality reduction and visualization for datasets with a large number of variables. But what if we could use something more advanced? What if we can easily search for patterns in a non-linear way? t-SNE is one such technique. There are mainly two types of approaches we can use to map the data points:<br>Local approaches :  They maps nearby points on the manifold to nearby points in the low dimensional representation.<br>Global approaches : They attempt to preserve geometry at all scales, i.e. mapping nearby points on manifold to nearby points in low dimensional representation as well as far away points to far away points.</p>
<h3 id="LDA（有监督）"><a href="#LDA（有监督）" class="headerlink" title="LDA（有监督）"></a>LDA（有监督）</h3><p>和上面最大的区别在于 LDA 是有监督的，<br>LDA试图让不同类别样本之间的距离最大，同时让相同类别样本之间的距离最小。简单来说LDA是为了使降维后的数据点尽可能的可分。</p>
<p>上图中国提供了两种投影方式，哪一种能更好的满足我们的标准呢？从直观上可以看出，右图要比左图的投影效果好，因为右图的黑色数据和蓝色数据各个较为集中，且类别之间的距离明显。左图则在边界处数据混杂。以上就是LDA的主要思想了，当然在实际应用中，我们的数据是多个类别的，我们的原始数据一般也是超过二维的，投影后的也一般不是直线，而是一个低维的超平面</p>
<p><img src="https://ftp.bmp.ovh/imgs/2019/09/d993b830928805b8.png" alt=""></p>
<p>这个是关于该算法的一个<a href="https://www.cnblogs.com/pinard/p/6244265.html" target="_blank" rel="noopener">讲解</a>， 如果对于步骤感兴趣的话。</p>
<h3 id="PCA-vs-LDA"><a href="#PCA-vs-LDA" class="headerlink" title="PCA vs. LDA"></a>PCA vs. LDA</h3><p>LDA用于降维，和PCA有很多相同，也有很多不同的地方，因此值得好好的比较一下两者的降维异同点。</p>
<p>首先我们看看相同点：<br>　　　　1）两者均可以对数据进行降维。<br>　　　　2）两者在降维时均使用了矩阵特征分解的思想。<br>　　　　3）两者都假设数据符合高斯分布。<br>我们接着看看不同点：<br>　　　　1）LDA是有监督的降维方法，而PCA是无监督的降维方法<br>　　　　2）LDA降维最多降到类别数k-1的维数，而PCA没有这个限制。<br>　　　　3）LDA除了可以用于降维，还可以用于分类。<br>　　　　4）LDA选择分类性能最好的投影方向，而PCA选择样本点投影具有最大方差的方向。</p>
<p>当类别特别多的时候，每个类中的样本就越少，此时更加适合使用PCA而不是LDA。</p>
<h3 id="t-SNE"><a href="#t-SNE" class="headerlink" title="t-SNE"></a>t-SNE</h3><p>PCA和 t-SNE的background：</p>
<p>我们有一堆数据并且想要知道这些数据 “looks like”， 然后建立了一个”map”，使这些数据投影到了二维或者三维平面上。但是PCA 是线性 mapping，</p>
<blockquote>
<p>PCA learns a linear mapping, which is very restrictive. PCA focuses on preserving large pairwise distances.<br>对于 t-SNE 的定义<br>compute pairwise similarities between data with normalized Gaussian kernel </p>
</blockquote>
<p>计算步骤</p>
<ol>
<li>在高纬计算两点 with normalized Gaussian kernel(<a href="http://hyperphysics.phy-astr.gsu.edu/hbase/Math/gaufcn.html" target="_blank" rel="noopener">高斯分布</a><br>$$<br>p_{i,j}=\frac{\exp (-|x_{i}-x_{j}|^{2} / 2 \sigma^{2})}{\sum_{k} \sum_{l \neq k} \exp (-|x_{k}-x_{l}|^{2} / 2 \sigma^{2})}<br>$$</li>
<li>在低维计算两点（使用 normalized Student-t similarities in the t-SNE map）<br>$$<br>q_{i,j}=\frac{(1+\overline{|} y_{i}-y_{j} |^{2})^{-1}}{\sum_{k} \sum_{l \neq k}(1+|y_{k}-y_{l}|^{2})^{-1}}<br>$$</li>
<li>计算KL 散度（尽可能的最小化两者之间的差异， kl尽可能的小）<br>$$<br>K L(P | Q)=\sum_{i} \sum_{j \neq i} p_{i j} \log \frac{p_{i j}}{q_{i j}}<br>$$</li>
</ol>
<p>为什么选择 KL divergence?</p>
<p>The KL divergence preserves local data structure： 如果在高维度 p中相差大，但是在低维q 中相差小，那么kl 的值非常大。如果再高纬度p 相差小，但是在低纬度相差大，那么kl 的值就非常小。所以说尽可能保存了 local information。</p>
<p>降维必然带来信息损失，这里更加保存的是局部信息，那么必然损失的全局信息。而 t 分布能够放大这种密度，因为t 分布更加长尾。</p>
<p>补充知识点（t分布和卡方分布）</p>
<ol>
<li>关于 $t$ 分布， 假设 $X \sim N(0, 1) $ ， $Y \sim  \chi^{2}(n)$ ，那么 $ Z=\frac{X}{\sqrt{Y / n}}  $ 的分布就被称为自由度为 $n$ 的 $t$ 分布， 记做 $Z \sim t(n)$。 但自由度$n$ 越大，那么越是接近正太分布。一般来说 $t$ 分布比 正太分布更加长尾。<br>如果是正太分布，那么就可以使用中心极限定理，但是 $t$ 分布就不可以了。$t$ 分布也被称为 学生t 分布。</li>
<li>若 $k$个随机变量 $Z_1, Z_2, … Z_n$是相互独立，符合标准正态分布的随机变量（数学期望为0、方差为1），则随机变量 $Z$的平方和<br>$$<br>X=\sum_{i=1}^{k} Z_{i}^{2}<br>$$<br>被称为自由度为 $k$ 的卡方分布， 记做 $X \sim \chi^{2}(k)$。</li>
</ol>
<p>t-sne 时间复杂度 $n^2$</p>
<blockquote>
<p>naive implementations are quadratic in the number of data points</p>
</blockquote>
<p><strong>可以被用来做Do</strong></p>
<ol>
<li>Use t-SNE to get some <strong>qualitative</strong> hypotheses on what your features capture 这个是定性分析</li>
<li>在输入和输出形式上可以 more creative</li>
</ol>
<p><strong>不可以被用来 Don’t</strong></p>
<ol>
<li>不是证明你的理论的工具</li>
<li>assign meaning to distance across empty space（注意这个是局部的特征，not 全局的信息）</li>
<li>think that t-sne will help you find outlier, or assign meaning to point densities in cluster</li>
<li>forget the scale (perplexity) matters<br>（ you can think of perplexity as the “effective” number of nearest neighbors， 所以说当这个 perplexity 越是接近原始一个簇的neighbors的个数，那么这个分类的效果是）</li>
<li>forget that t-SNE 是解决一个 non-convex objective: there are local mimima</li>
<li>local minima generally split a natural cluster into multiple parts （就是如何有这个效果，那么不要惊讶，有可能出现多个不同的parts）</li>
<li>forget that low-dimentional metric  spaces cannot caputure non-metric similarities</li>
</ol>
<p><strong>How input similarities in t-SNE are actually computed</strong></p>
<ol>
<li>compute conditional similarities<br>$$<br>p_{j |i}=\frac{\exp (-|x_{i}-x_{j}|^{2} / 2 \sigma_{i}^{2})}{\sum_{j^{\prime} \neq i} \exp (-|x_{i}-x_{j^{\prime}}|^{2} / 2 \sigma_{i}^{2})}<br>$$<br>perform a binary search over $\sigma_{i}$ to obtain a target perplexity</li>
</ol>
<p>( 从这里得到启发，如果某个指标是不对称的，那么 symmetrize 就是可以得到一个对称的指标， 想想从kl divergence 到JS divergence这个过程)</p>
<ol start="2">
<li>Symmetrize the conditions<br>$$<br>p_{i |j}=\frac{p_{j | i}+p_{i | j}}{2 N}<br>$$</li>
</ol>
<p><strong>Conclusion</strong></p>
<p>t-SNE is a valuable tool in generating hypotheses and understanding, but does not produce conclusive evidence. </p>
<p>来自<a href="http://deeplearning.csail.mit.edu/slide_cvpr2018/laurens_cvpr18tutorial.pdf" target="_blank" rel="noopener">do’s and Don’ts of using t-SNE to Understand Vision Models</a>的阅读笔记。<br><a href="http://frankchen.xyz/2018/01/30/Understanding-TSNE/" target="_blank" rel="noopener">理解TSNE算法</a></p>
<h2 id="杂货铺"><a href="#杂货铺" class="headerlink" title="杂货铺"></a>杂货铺</h2><p><strong> 对于encodding 的理解</strong>，从字符转成数字，并且尽可能的保留原来的信息。接触的有三种，一种 label encoding，适合类别信息只有两类。如果大于两类那么就使用one-hot。第三种就是万物可以embedding，使用神经网络的思想。</p>
<p>特征工程分为特征构造 (feature creation or construction)和 特征选择（feature selection） 。</p>
<p>归一化（去中心，方差归一）是属于特征(预)处理:把特征值压缩成0~1的区间。</p>
<p>One-hot（也叫One-of-k）的方法把每个无序特征转化为一个数值向量。比如一个无序特征color有三种取值：red，green，blue。那么可以用一个长度为3的向量来表示它，向量中的各个值分别对应于red，green，blue。如：这种方法在NLP里用的很多，就是所谓的词向量模型。变换后的向量长度对于词典长度，每个词对应于向量中的一个元素。</p>
<p>过拟合和欠拟合问题：</p>
<p>可以从数据和模型两方面考虑。</p>
<table>
<thead>
<tr>
<th></th>
<th>过拟合</th>
<th>欠拟合</th>
</tr>
</thead>
<tbody>
<tr>
<td>数据</td>
<td></td>
<td>收集更多的数据 或者 数据增强</td>
</tr>
<tr>
<td>模型</td>
<td>模型简单化 (dropout, normalizaion )，目标函数加上L1 or L2 ，不同模型取平均</td>
<td>模型变得复杂（如果树的结构，那么树个数增加），loss function</td>
</tr>
<tr>
<td>训练</td>
<td>early stop, 随机采样 (加入了随机性)</td>
<td>多训练一下看看效果</td>
</tr>
</tbody>
</table>
<h2 id="复习总结"><a href="#复习总结" class="headerlink" title="复习总结"></a>复习总结</h2><ol>
<li>特征来源一部分是业务场景，一部分是常规操作。常规操作包括aggregation聚合操作（主要针对子表），特征离散化，组合特征。业务特征，就是根据不同的场景，构造在该场景下重要字段。当然还有一些骚操作，使用机器模型进行特征的构造和选择，优点是work，缺点是可解释性差，比如使用xgboost +LR 模型，前者就是一种特征提取的功能，最后叶子结点的输出，知道其是重要的，但是不知道其含义是什么。</li>
<li>连续特征离散化的<strong>好处</strong>： (1)  增加了模型的非线性，提升了模型表达能力 (2) 离散化特征对异常数据具有很强的鲁棒性。<strong>常用的选取离散点的方法</strong>：(1) 等距离离散 (2) 等样本离散 (3) 画图 (4) 根据实际场景，比如对于年龄的划分</li>
<li>特征组合( 1)基本特征的非线性组合  (2) 特征之间的差和乘积商，mean,variance，std 统计学特征</li>
<li>特征选择（降维）的<strong>方法</strong>: (1) 特征本身 （如果缺省值比较大或者数据的波动比较小） (2) 特征之间的关系（特征之间有较强的相关性，可以使用PCA进行降维） (3 ) 特征和最后target 的关系 (feature  importance, 卡房分布， pearson 相关系数) (4) 很多常见的机器学习模型都是一种特征选择的方式，比如xgboost</li>
<li>LR 是广义的线性模型，内部是矩阵运算，速度相当的快。随着工业界计算能力的提升，使用xgboost 的范围是越来越大，不再局限于LR 模型。</li>
<li>正则化是使得模型不再变得那么复杂，具体是通过减少目标函数中loss 中weights 的相对大小。</li>
<li>embedding 的方式 (1) label embedding (2) one-hot (3) 神经网络，万物embedding</li>
</ol>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/pca/" rel="tag"># pca</a>
          
            <a href="/tags/svd/" rel="tag"># svd</a>
          
            <a href="/tags/t-sne/" rel="tag"># t-sne</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/06/29/常见的排序算法总结/" rel="next" title="常见的排序算法总结">
                <i class="fa fa-chevron-left"></i> 常见的排序算法总结
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/07/07/Loss-Activation-and-Optimisation-Function/" rel="prev" title="Activation Function and Loss Function">
                Activation Function and Loss Function <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
      </div>
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/avatar.jpeg"
                alt="Jijeng Jia" />
            
              <p class="site-author-name" itemprop="name">Jijeng Jia</p>
              <p class="site-description motion-element" itemprop="description">Stay Hungry, Stay Foolish</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">95</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">6</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">57</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/jijeng" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:jia1509309698@163.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          

          <script type="text/javascript" src="//rf.revolvermaps.com/0/0/5.js?i=5kk0u0mm50w&amp;m=0&amp;c=54ff00&amp;cr1=ff0000" async="async"></script>


  <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
  <script src="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
  <script src="//cdn.bootcss.com/blueimp-md5/1.1.0/js/md5.min.js"></script>
  <script>
      var gitalk = new Gitalk({
        clientID: '8c6403951ee3eab4e420',
        clientSecret: 'd842e48ca0c28ec41200f973ba52f96ba975b441',
        repo: 'jijeng.github.io',
        owner: 'jia1509309698@163.com',
        admin: 'jia1509309698@163.com',
        id: md5(location.pathname),
        distractionFreeMode: 'true'
      });
      var div = document.createElement('div');
      div.setAttribute("id", "gitalk_comments");
      div.setAttribute("class", "post-nav");
      var bro = document.getElementById('posts').getElementsByTagName('article');
      bro = bro[0].getElementsByClassName('post-block');
      bro = bro[0].getElementsByTagName('footer');
      bro = bro[0];
      bro.appendChild(div);
      gitalk.render('gitalk_comments');
  </script>

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#特征生成"><span class="nav-number">1.</span> <span class="nav-text">特征生成</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#特征离散化"><span class="nav-number">2.</span> <span class="nav-text">特征离散化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#组合特征"><span class="nav-number">3.</span> <span class="nav-text">组合特征</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#正则化"><span class="nav-number">4.</span> <span class="nav-text">正则化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#feature-selection"><span class="nav-number">5.</span> <span class="nav-text">feature selection</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Principal-Component-Analysis-PCA"><span class="nav-number">5.1.</span> <span class="nav-text">Principal Component Analysis (PCA)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SVD"><span class="nav-number">5.2.</span> <span class="nav-text">SVD</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#投影也是一种降维手段"><span class="nav-number">5.3.</span> <span class="nav-text">投影也是一种降维手段</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#T-sne"><span class="nav-number">5.4.</span> <span class="nav-text">T-sne</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#LDA（有监督）"><span class="nav-number">5.5.</span> <span class="nav-text">LDA（有监督）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#PCA-vs-LDA"><span class="nav-number">5.6.</span> <span class="nav-text">PCA vs. LDA</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#t-SNE"><span class="nav-number">5.7.</span> <span class="nav-text">t-SNE</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#杂货铺"><span class="nav-number">6.</span> <span class="nav-text">杂货铺</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#复习总结"><span class="nav-number">7.</span> <span class="nav-text">复习总结</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Jijeng Jia</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>

<div>
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
Total  <span id="busuanzi_value_site_pv"></span> views
You got  <span id="busuanzi_value_site_uv"></span> visitors
</div>




        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      
    </span>
  
</div>








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  

    
      <script id="dsq-count-scr" src="https://https-jijeng-github-io.disqus.com/count.js" async></script>
    

    
      <script type="text/javascript">
        var disqus_config = function () {
          this.page.url = 'http://yoursite.com/2018/06/29/特征工程相关概念/';
          this.page.identifier = '2018/06/29/特征工程相关概念/';
          this.page.title = '特征工程';
        };
        var d = document, s = d.createElement('script');
        s.src = 'https://https-jijeng-github-io.disqus.com/embed.js';
        s.setAttribute('data-timestamp', '' + +new Date());
        (d.head || d.body).appendChild(s);
      </script>
    

  




	





  














  <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
  <script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>
   <script type="text/javascript">
        var gitalk = new Gitalk({
          clientID: '',
          clientSecret: '',
          repo: 'jijeng.github.io',
          owner: '',
          admin: [''],
          id: location.pathname,
          distractionFreeMode: ''
        })
        gitalk.render('gitalk-container')           
       </script>


  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
