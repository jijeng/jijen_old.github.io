<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[CycleGAN & StyleGAN]]></title>
    <url>%2F2019%2F04%2F09%2Fcyclegan-stylegan%2F</url>
    <content type="text"><![CDATA[In the past few yeas, GANs have been used in lots of different applications such as generating synthetic data, style transfer, super-resolution and text2image generation. But we donn’t aim to give a overview of what GANs are made for. There are many great and detailed blogs for your understanding. If you have fun about transfer learning, especially style transfer using GANs, this post might interest you! What this post is about Main ideas of CycleGAN Keypoints in StyleGAN A Gentle Introduction of GANsWe assume the reader has some prior experience with neural networks. In addition, if you are familiar with GANs you can skip this section. The famous minimax objective function can be formulated as following:$$\min _ { \theta _ { g } } \max _ { \theta _ { d } } \left[ \mathbb { E } _ { x \sim p _ { d a t a } } \log D _ { \theta _ { d } } ( x ) + \mathbb { E } _ { z \sim p ( z ) } \log \left( 1 - D _ { \theta _ { d } } \left( G _ { \theta _ { g } } ( z ) \right) \right) \right)$$But in practical, the loss function cannot work very well. So we have alternative objective function: Gradient ascent on discriminator $$\max _ { \theta _ { d } } \left[ \mathbb { E } _ { x \sim p _ { d a t a } } \log D _ { \theta _ { d } } ( x ) + \mathbb { E } _ { z \sim p ( z ) } \log \left( 1 - D _ { \theta _ { d } } \left( G _ { \theta _ { g } } ( z ) \right) \right) \right]$$ Gradient ascent on generator$$\max _ { \theta _ { g } } \mathbb { E } _ { z \sim p ( z ) } \log \left( D _ { \theta _ { d } } \left( G _ { \theta _ { g } } ( z ) \right) \right)$$The reasoning behind this can be found in original paper. Simplily speaking, we can get higher gradient signal for bad samples, which works much better in practice.From Stanford CS231 Lecture 13 — Generative Models Main ideas of CycleGANCycleGAN was introduced in 2017 out of Berkeley, Unpaired Image-to-Image Translation Using Cycle-Coonsistent Adversarial Networks. This task is performed on unpaired data. Recent methods such as Pix2Pix depend on the availability of training examples where the samee data is availabel in both domains. However, CycleGAN is able to learning such pair information without one-to-one mapping between training data in source and target domains. Network ArchitectureWe build three networks. A generator $F$ to convert image $y$ to image $ \hat{x}$ A generator $G$ to convert image $\hat{x}$ to image $ \hat{y}$ A discriminator $D$ to idenfify real image or generated pictureSimplified version of CycleGAN architecture can be showed in the following.The function $F$ and $G$ are generator network, which consists of encoder, transformer and decoder. Encoder is extracting the features from an image which is done by convolution networks. Each convolution layer leads to extraction of progressively higher level features. We would like to transform the feature emebdding of an image from domain $X$ to that of domain $Y$. So for this, authors have used 6 layers of resnet blocks. Resnet block is a neural network layer which consists of two convolutiona layers when a residue of input ia added to the output. This is done to ensure peoperties of input of previous layers are available for later layers as well.Resnet block can be summarized in following imageThe decoder transfer embedding from $y$ back to original embedding $x$. Loss functionThere are two types of losses in CycleGAN. Besides adversarial loss, we have another loss named reconstruction cost.Adversarial loss is similary to original GAN.$$\operatorname { Loss } _ { a d v } \left( F , D _ { x } , Y \right) = \frac { 1 } { m } \sum _ { i = 1 } ^ { m } \left( 1 - D _ { x } \left( F \left( y _ { i } \right) \right) \right) ^ { 2 }$$$$\operatorname { Loss } _ { a d v } \left( G , D _ { y } , X \right) = \frac { 1 } { m } \sum _ { i = 1 } ^ { m } \left( 1 - D _ { y } \left( G \left( x _ { i } \right) \right) \right) ^ { 2 }$$However, the adversarial loss alone is not sufficient to produce good looking images, which can not enfore that the input and output are recognizably the same. The cycle consistency loss addresses this issue. It relies on the expectation that if you convert an image to the other domain and back again, and then you should get back something similar to what you put in. It enforces that $F ( G ( x ) ) \approx x$ and $G ( F ( y ) ) \approx y$.$$\operatorname { Loss } _ { c y c } ( G , F , X , Y ) = \frac { 1 } { m } \sum _ { i = 1 } ^ { m } \left[ F \left( G \left( x _ { i } \right) \right) - x _ { i } \right] + \left[ G \left( F \left( y _ { i } \right) \right) - y _ { i } \right]$$We can get the full objective function by putting these two together.$$\mathcal { L } \left( G , F , D _ { x} , D _ { y } \right) = \mathcal { L } _ { \text { GAN } } \left( G , D _ { y } , X , Y \right) + \mathcal { L } _ { \text { GAN } } \left( F , D _ { x } , Y , X \right) + \lambda \mathcal { L } _ { \text { cyc } } ( G , F )$$ Keypoints of StyleGANThe StyleGAN offeras an upgrade version of ProGAN’s image generator, with a focus on the generator. ProGAN generates high-quality images but, in most models, its ability to control specific features of the generated image is very limited. In other word, the features are entangled and therefore attempting to tweak the input, even a bit, usually affects multiple features at the same time. A good illustrations would be following pictures.Compared with first version (ProGAN), the new generator includes several additions to ProGAN’s generators. Mapping NetworkThe mapping network’s goal in to encode the input vector into an intermediate vector whose different elements control different visual features, which consists of 8 fully connected layers and its output $w$ is of the same size as the input. Style Modules (AdaIN)The AdaIn (Adaptive Instance Normalization) module transfers the encoded information $w$, created by the mapping network, into the generated image. Removing traditional inputSince the encoded information $w$ from mapping network was used into generator image, the traditional random input can be omitted and replaced by constant values.]]></content>
      <categories>
        <category>Deep learning</category>
      </categories>
      <tags>
        <tag>GANs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python for Beginners]]></title>
    <url>%2F2019%2F04%2F09%2Fpython-for-beginners%2F</url>
    <content type="text"><![CDATA[modulepython 文件可以当做主文件进行运行或者当做函数的集合进行调用。如果是前者一般是需要包含”__name__ ==”__main__”。对于后者就是在其他的python文件中进行调用。 import my_module # python文件 from my_module import my_object packagesfrom packageroot.packagefolder.mod import my_object Note: Ensure each directory within your package import contains a file __init__.py pythonpathpython2 和python3 使用不同的解释器，导致在一些函数命名和计算上有一些差别，最好在文件的开头标明使用的解释器。 while forwhile : provide a condition and run the loop until the condition is not met. loop for a number of specific times; loop over items or characters of a string. examples: [Variable] AggregateFunction([Value] for [item] in [collection]) x =[1, 2,3, 4, 5] y =[ 2*a for a in x if a%2 ==0] y &gt;&gt; [4, 8] 或者可以使用这样更加简洁的语句： lambda arguments : expressio fun1 = lambda a,b,c : a+b+c print(fun1(5,6,2)) 来个比较复杂的例子 nums =[1,2,3,4,5] letters =[&apos;a&apos;, &apos;b&apos;, &apos;c&apos;,&apos;d&apos;,&apos;e&apos;] nums_letters =[[n, l] for n in nums for l in letters ] nums_letters break continue passThe break, continue, and pass statements in Python will allow you to use for loops and while loops more effectively in your code. number = 0 for number in range(10): number = number + 1 if number == 5: pass # pass here print(&apos;Number is &apos; + str(number)) print(&apos;Out of loop&apos;) The pass statement occurring after the if conditional statement is telling the program to continue to run the loop and ignore the fact that the variable number evaluates as equivalent to 5 during one of its iterations. 可以用用作新的 if的测试。The pass statement can create minimal classes, or act as a placeholder when working on new code and thinking on an algorithmic level before hammering out details. yield return经常被用来作为生成器。 when you call a normal function with a return statement the function is terminated whenever it encounters a return statement. In a function with a yield statement the state of the function is ‘saved’ from the last call and can be picked up the next time you call a generator function. for examples gen_exp =(x **2 for x in range(10) if x %2 ==0) for x in gen_exp: print(x) def my_gen(): for x in range(5): yield x gen1 =my_gen() next(gen1) def my_generator1(): yield 1 yield 2 yield 3 my_gen =my_generator1() next(my_gen) recursionA function calling itself is known as recursion. list, tuples, dictionary在python 中是使用频繁的data structure，这个是属于 collection 类别，里面放的是elementto add/update/ delete an item of a colletion ( list) my_list.append(&apos;C&apos;) #adds at the end my_list[1] = &apos;D&apos; #update my_list.pop(1) # removes mylist.pop() # 默认就是类似 栈的结构，就是pop 出来最后一个 mylist.pop(0) # 当然也可以根据index 指定特定的 pop(delete) 的element or del mylist[1:2] # 通过指定 index range 然后进行del mylist.sort() # 支持 sorting 然后是从小到大, 这个sort是一种操作，inplace 的操作 tuplestuples store a sequence of objects, the object can be of any typle. Tuples are faster than lists. dictionary:It stores key/value pair objects. my_dict =dict() my_dict[&apos;key&apos;] =&apos;value&apos; or my_dict ={&apos;key&apos;: &apos;value&apos;, ...} for key in my_dict: # do something if &apos;some key&apos; in my_dict: # do something Iteratorsclass yrange: def __init__(self, n): self.i =0 self.n =n # 这个表明是一个 iterator，make an object iterable def __iter__(self): return self # 这个next 函数就被当做是 class的属性，可以被外部调用的， def next(self): if self.i &lt; self.n: i =self.i self.i +=1 return i else: raise StopIteration() shallow vs deep copypython3 中：对于简单的数据类型，像int ，string，这种 copy() 和copy.deepcopy() 这两者都是相同的，copy 都是一种映射，都是相当于”值“ 上的引用； aa =2 bb =aa print(id(aa), id(bb)) # 相同 bb =3 print(id(aa), id(bb)) # 不同，因为把3 这个值重新复制给了变量bb 对于复杂的数据类型，使用deepcopy() 的时候，本来就是会重新拷贝一份到内存中。在python3 中copy() 和deepcopy() 这个是没有什么区别的。 list1 =[&apos;a&apos;, &apos;b&apos;] list2 =list1 # 这个是引用，所以和list1 是相同的 list3 =copy.copy(list1) # 这个id 和list1 不同 list4 =copy.deepcopy(list1)# 这个id 和list1 不同 print(id(list1), id(list2), id(list3), id(list4)) object oriented designclass ParentClass: def my_function(self): print &apos;I am here&apos; class SubClass1(ParentClass): class SubClass2(ParentClass): 对于多继承的支持 （接口） class A(B,C): #A implments B and C 如果想要call parent class function then you can dp: super(A, self).funcion_name() garbage collectionall the objects in python are stored in a heap space. Python has an in-built garbage collection mechanism. In computer science, a heap is a specialized tree-based data structure which is essentially an almost complete[1] tree that satisfies the heap property: if P is a parent node of C, then the key(the value) of P is either greater than or equal to (in a max heap) or less than or equal to (in a min heap) the key of C. try…catch# raise exceptions try: raise TyeError except: print(&apos;exception&apos;) # catching exceptions try: do_something() except: print(&apos;exception&apos;) # try/ catch /finally try: do_something() except TypeError: print(&apos;exception&apos;) finally: close_connections()]]></content>
      <categories>
        <category>Python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[The-evaluation-of-sentence-similarity]]></title>
    <url>%2F2019%2F04%2F06%2FThe-evaluation-of-sentence-similarity%2F</url>
    <content type="text"><![CDATA[I am trying to write my first english blog based on two reasons: First, the data set used in this blog is english; Second, I’d like to expand my reach and attract more audiences, although I should admit that nobody cares. DataInitially I want to use chinese corpus, but I cannot find a proper one. The data should sound like this one: word1 word2 similarity score阿拉伯人 阿拉伯 7.2畜产 农业 5.6垂涎 崇敬 3.4次序 秩序 4.7定心丸 药品 4.3房租 价格 5.2翡翠 宝石 6.7高科技 技术 7.5购入 购买 8.5观音 菩萨 8.2归并 合并 7.7 not like this: 为何我无法申请开通花呗信用卡收款 支付宝开通信用卡花呗收款不符合条件怎么回事 1花呗分期付款会影响使用吗 花呗分期有什么影响吗 0为什么我花呗没有临时额度 花呗没有临时额度怎么可以负 0能不能开花呗老兄 花呗逾期了还能开通 0我的怎么开通花呗收钱 这个花呗是个什么啥？我没开通 我怎么有账单 0蚂蚁借呗可以停掉么 蚂蚁借呗为什么给我关掉了 0我想把花呗功能关了 我去饭店吃饭，能用花呗支付吗 0为什么我借呗开通了又关闭了 为什么借呗存在风险 0支付宝被冻了花呗要怎么还 支付功能冻结了，花呗还不了怎么办 1 If you can find the dataset where ‘similarity score’ is double, please donot hesitate to email me. So, the choice has to be enlgish corpus. The dataset used in this experiment are STSbenchmark and SICK data. The SICK data contains 10,000 sentence paris labeled with semantic relatedness and entailment relation. Similarity MethodsBaselineAs the baseline, we just take the embedding of the words in sentence, and compute the average, weighted by frequency of each word. def run_avg_benchmark(sentences1, sentences2, model=None, use_stoplist=False, doc_freqs=None): if doc_freqs is not None: N = doc_freqs[&quot;NUM_DOCS&quot;] sims = [] for (sent1, sent2) in zip(sentences1, sentences2): tokens1 = sent1.tokens_without_stop if use_stoplist else sent1.tokens tokens2 = sent2.tokens_without_stop if use_stoplist else sent2.tokens tokens1 = [token for token in tokens1 if token in model] tokens2 = [token for token in tokens2 if token in model] if len(tokens1) == 0 or len(tokens2) == 0: sims.append(0) continue tokfreqs1 = Counter(tokens1) tokfreqs2 = Counter(tokens2) weights1 = [tokfreqs1[token] * math.log(N / (doc_freqs.get(token, 0) + 1)) for token in tokfreqs1] if doc_freqs else None weights2 = [tokfreqs2[token] * math.log(N / (doc_freqs.get(token, 0) + 1)) for token in tokfreqs2] if doc_freqs else None embedding1 = np.average([model[token] for token in tokfreqs1], axis=0, weights=weights1).reshape(1, -1) embedding2 = np.average([model[token] for token in tokfreqs2], axis=0, weights=weights2).reshape(1, -1) sim = cosine_similarity(embedding1, embedding2)[0][0] sims.append(sim) return sims Smooth Inverse FrequencyThe baseline, like we did before, is very simple and crude of computing sentence embedding. Word frequency cannot reliably reflect its importance to sentence, semantically speaking. Smooth Inverse Frequency (SIF) tries to solve thhis problem. SIF is very similar to the weighted average we used before, with the difference that it’s weighted by this formular.$$\operatorname { SIF } ( w ) = \frac { a } { ( a + p ( w ) )}$$where $a$ is a hyper-parameter (set to 0.001 by default) and $ p(w)$ is the estimated word frequency in the corpus. Next, we need to perform common component removal: subtract from the sentence embeddingh obtained above the first principal component of the matrix. This corrects for the influence of high-frequency words that have syntactic or dicourse function, such as ‘but’, ‘and’, etc. You can find more information from this paper. def remove_first_principal_component(X): svd = TruncatedSVD(n_components=1, n_iter=7, random_state=0) svd.fit(X) pc = svd.components_ XX = X - X.dot(pc.transpose()) * pc return XX def run_sif_benchmark(sentences1, sentences2, model, freqs={}, use_stoplist=False, a=0.001): total_freq = sum(freqs.values()) embeddings = [] # SIF requires us to first collect all sentence embeddings and then perform # common component analysis. for (sent1, sent2) in zip(sentences1, sentences2): tokens1 = sent1.tokens_without_stop if use_stoplist else sent1.tokens tokens2 = sent2.tokens_without_stop if use_stoplist else sent2.tokens tokens1 = [token for token in tokens1 if token in model] tokens2 = [token for token in tokens2 if token in model] weights1 = [a / (a + freqs.get(token, 0) / total_freq) for token in tokens1] weights2 = [a / (a + freqs.get(token, 0) / total_freq) for token in tokens2] embedding1 = np.average([model[token] for token in tokens1], axis=0, weights=weights1) embedding2 = np.average([model[token] for token in tokens2], axis=0, weights=weights2) embeddings.append(embedding1) embeddings.append(embedding2) embeddings = remove_first_principal_component(np.array(embeddings)) sims = [cosine_similarity(embeddings[idx * 2].reshape(1, -1), embeddings[idx * 2 + 1].reshape(1, -1))[0][0] for idx in range(int(len(embeddings) / 2))] return sims Google Sentence EncoderInferSent is a pre-trained encoder that produces sentence embedding, which opensourced by Facebook. The Google Sentence Encoder is Google’s answer to Facebook’s InferSent. In contrast to InferSent, the Google Sentence Encoder was trained on a combination of unsupervised data and supervised data (SNLI corpus), which tends to give better results. The codes can be used in Google Jupyter Notebook import tensorflow_hub as hub tf.logging.set_verbosity(tf.logging.ERROR) embed = hub.Module(&quot;https://tfhub.dev/google/universal-sentence-encoder/1&quot;) def run_gse_benchmark(sentences1, sentences2): sts_input1 = tf.placeholder(tf.string, shape=(None)) sts_input2 = tf.placeholder(tf.string, shape=(None)) sts_encode1 = tf.nn.l2_normalize(embed(sts_input1)) sts_encode2 = tf.nn.l2_normalize(embed(sts_input2)) sim_scores = tf.reduce_sum(tf.multiply(sts_encode1, sts_encode2), axis=1) with tf.Session() as session: session.run(tf.global_variables_initializer()) session.run(tf.tables_initializer()) [gse_sims] = session.run( [sim_scores], feed_dict={ sts_input1: [sent1.raw for sent1 in sentences1], sts_input2: [sent2.raw for sent2 in sentences2] }) return gse_sims Experimentsdef run_experiment(df, benchmarks): sentences1 = [Sentence(s) for s in df[&apos;sent_1&apos;]] sentences2 = [Sentence(s) for s in df[&apos;sent_2&apos;]] pearson_cors, spearman_cors = [], [] for label, method in benchmarks: sims = method(sentences1, sentences2) pearson_correlation = scipy.stats.pearsonr(sims, df[&apos;sim&apos;])[0] print(label, pearson_correlation) pearson_cors.append(pearson_correlation) spearman_correlation = scipy.stats.spearmanr(sims, df[&apos;sim&apos;])[0] spearman_cors.append(spearman_correlation) return pearson_cors, spearman_cors Helper function: import functools as ft benchmarks = [ (&quot;AVG-GLOVE&quot;, ft.partial(run_avg_benchmark, model=glove, use_stoplist=False)), (&quot;AVG-GLOVE-STOP&quot;, ft.partial(run_avg_benchmark, model=glove, use_stoplist=True)), (&quot;AVG-GLOVE-TFIDF&quot;, ft.partial(run_avg_benchmark, model=glove, use_stoplist=False, doc_freqs=doc_frequencies)), (&quot;AVG-GLOVE-TFIDF-STOP&quot;, ft.partial(run_avg_benchmark, model=glove, use_stoplist=True, doc_freqs=doc_frequencies)), (&quot;SIF-W2V&quot;, ft.partial(run_sif_benchmark, freqs=frequencies, model=word2vec, use_stoplist=False)), (&quot;SIF-GLOVE&quot;, ft.partial(run_sif_benchmark, freqs=frequencies, model=glove, use_stoplist=False)), ] Resultsimport matplotlib.pyplot as plt plt.rcParams[&apos;figure.figsize&apos;] = (20,13) spearman[[&apos;AVG-GLOVE&apos;, &apos;AVG-GLOVE-STOP&apos;,&apos;AVG-GLOVE-TFIDF&apos;, &apos;AVG-GLOVE-TFIDF-STOP&apos;,&apos;GSE&apos;]].plot(kind=&quot;bar&quot;).legend(loc=&quot;lower left&quot;) Take Off Smooth Inverse Frequency methods are better than baseline, no matter with word2vec or Glove embeddings. Glove Sentence Encoder has the similar performance as Smooth Inverse Frequency. Using tf-idf weights does not help and using a stoplist looks like a reasonable choice. Pearson CorrelationSpearman Correlation Full codes can be found in here.]]></content>
      <categories>
        <category>NLP</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[深度网络中的碎碎念]]></title>
    <url>%2F2019%2F03%2F26%2F%E6%B7%B1%E5%BA%A6%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E7%A2%8E%E7%A2%8E%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[主要介绍了常见的网络中权重的初始化，激活函数和优化器。 Weights Initializationweights 的初始化在网络的训练起到重要的作用，初始化的好坏能够直接影响到网络是否可以正常收敛。这里的初始化都是指的是weights初始化。bias 这个变量就是在企图去描述真实的分布，通过引入随机性来表示这个是具有 推广性的。 Here’s another trick — before squishing our scalar value (called an activation) into the sigmoid function, we can add a little number called the bias, b. There will be two biases, one for each user and one for each movie. Random Initialization最基础的即使 bias 使用 zero initialization ，然后 weights 使用 random initialzation。这种方法的缺陷在于梯度消失。就是你的weights 如果很大或者很小的时候，再加上如果使用了sigmoid 那么很容易出现上述的现象。 a) If weights are initialized with very high values the term np.dot(W,X)+bbecomes significantly higher and if an activation function like sigmoid() is applied, the function maps its value near to 1 where slope of gradient changes slowly and learning takes a lot of time.b) If weights are initialized with low values it gets mapped to 0, where the case is same as above. He Initialization$$W^{[l]} =np.random.randn(size_l, size_l -1)* np.sqrt(2/size_l -1) $$这个是使用 relu 或者说 leaky relu 配合使用的。 所以上述的初始化从数学的角度去理解： 从(0,1) 标准正太分布 转换成了 (0, np.sqrt(2/ size_l -1)) 这样的分布，就是你网络结构是越宽，那么这个方差就是越小的，最后的结果是越集中的，就越集中的 均值 u 左右。从图像的角度看，方差越大，图像越矮胖；方差越小，图像越瘦高。 Xavier initialization这个是使用tanh() 作为 activation function的。$$W^{[l]} =np.random.randn(size_l, size_l -1)* np.sqrt( 1/size_l -1) $$总的思想原则：They set the weights neither too much bigger that 1, nor too much less than 1.就是本来就是在 (0,1) 标准正太分布出来，然后进行了进一步的约束条件。 Activation function总的说可以分为线性和非线性的激活函数， activation function 的作用 就是对于网络的输出 说yes or no. It maps the resulting values in between 0 to 1 or -1 to 1 etc. Sigmoid function (Logistic Activation)the only reason why we use sigmoid is because it exists between 0 to 1. 这个非常有利于 predict probability. 因为自然映射到 0 是不存在 然后1 是存在。而当多分类的时候，使用softmax。 Tanh function The advantage is that the negative inputs will be mapped strongly negative and the zero inputs will be mapped near zero in the tanh graph. Relu (Rectified Linear Unit) Activation本质上是分段函数。 range: [0, infinity]. The function and its derivative both are monotonic. Leaky Relu每当一个伟大的东西产出，总会伴随着比较明显的错误，然后紧跟着就是有一个 rectified(improved) 这种版本。这个相比之前就是修正了 当输入是负数的时候，怎么办的问题。 Softmax这个需要从数学的角度去理解，数学公式就是下面。有一个真实的例子： 如果一张”4” 的图片，输出一个网络，最后是softmax 激活函数，那么最后得到的 “4” 的概率是最大的，但是也是有其他的可能性存在的，这个就是softmax 的效果。 最主要的功能是 convert number into probabilities. 这种效果，不像sigmoid 那样有很明确的数学公式。 $$\sigma ( z ) _ { j } = \frac { e ^ { z _ { j } } } { \sum _ { k = 1 } ^ { K } e ^ { z _ { k } } }$$ Softmax function, a wonderfulactivation function that turns numbers aka logits into probabilities that sum to one.Remember the takeaway is: the essential goal of softmax is to turn numbers into probabilities. OptimizerGradient Descent最基本的应该是 gradient descent，这个算是开山鼻祖。给出了每次梯度变化的方向 Momentum个人更喜欢将其翻译成动量，这个gradient 考虑之前的gradient 的方向和大小，有点像动量的的含义。不仅考虑了current step, 而且accumulates gradient of the past steps. A very popular technique that is used along with SGD is called Momentum. Instead of using only the gradient of the current step to guide the search, momentum also accumulates the gradient of the past steps to determine the direction to go. 好处在于: most recent is weighted than the less recent onesthe weightage of the most recent previous gradients is more than the less recent ones.for example: RMSpropRMSprop, or Root Mean Square Propogation has an interesting history. RMSProp also tries to dampen the oscillations, but in a different way than momentum. Adam这种套路总是一样的，总是有一个中和者，关键是如何整合到一起。 Adam or Adaptive Moment Optimization algorithms combines the heuristics of both Momentum and RMSProp. The hyperparameter beta1 is generally kept around 0.9 while beta_2 is kept at 0.99. Epsilon is chosen to be 1e-10 generally. 对于公式的解释，Eq 1 and Eq 2是come from RMSprop, Eq 3 comes from Adam。最后一步是 update 操作，条理清楚，没有问题。(公式中的v 表示导数，g 表示导数e 不太清楚)Here, we compute the exponential average of the gradient as well as the squares of the gradient for each parameters (Eq 1, and Eq 2). To decide our learning step, we multiply our learning rate by average of the gradient (as was the case with momentum) and divide it by the root mean square of the exponential average of square of gradients (as was the case with momentum) in equation 3. Then, we add the update.)]]></content>
      <categories>
        <category>Deep learning</category>
      </categories>
      <tags>
        <tag>Loss Function</tag>
        <tag>Activation Function</tag>
        <tag>Optimizer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[siamese network]]></title>
    <url>%2F2019%2F03%2F26%2Fsiamese-network%2F</url>
    <content type="text"><![CDATA[主要是介绍自己论文中的网络结构：siamese network。 但凡优化，无非两条路：在好的基础上更快，在快的基础上效果更好。 Siamese network训练速度快，所以只是需要其在训练效果上更好。 先来分析一下为什么训练速度快。那么不得不分析该网络结构。整个网络的输入是 (img1, img2, y) 这样的三元组，img 表示图片，y表示label。图片可以是同一类别的图片，也可以是不同类别的，y表示两张图片之间的相似程度，y的取值 (0,1)，0表示相似（同一类别），1 表示不相似（不同类别）。并且y 是double 类型，属于0-1 区间任意的数字。网路结构最后的输出是 0-1 区间的任意数字。通常是以0.5 作为分界线，如果小于0.5 那么认为两种图片是属于同一类别，或者说更相似；反之也成立。重要的一点是中间的weights 实现了权值共享，这样可以加快网络的训练速度。 loss function这个是属于经典的 contrastive loss function。当y 接近于0的时候，右半部分消失，这个是表示两张图片很是相似，然后就不断使得 欧氏距离减少；当y 接近于1的时候，左半部分消失，这个时候两张图片很不相似，然后右边就是 hinge loss。参数m 作为一种margin 是是可以调节，我的实验中 m 取1.总的思想：就是使得相近的图像距离相近，不想近的图像距离变远。 $L ( W , ( Y , X _ { 1 } , X _ { 2 } ) ) = ( 1 - Y ) \frac { 1 } { 2 } ( D _ { W } ) ^ { 2 } + ( Y ) \frac { 1 } { 2 } { \max ( 0 , m - D _ { W } ) } ^ { 2 }$ Spectral Normalization图像输入到网络之前使用正则化，然后输入到激活函数之前也是使用正则化，所以这种效果也是扩展到 weights，直接对 weights 进行正则化使其符合 Lipschitz 约束，避免使用大的gradients。在GAN 网络中的 discriminator 或者 generator 都发现了其可以稳定训练的过程。在实验中，我们扩大了了这种使用范围，把其应用到所有的网络的layer上。 self-attention mechanismAttention 机制自从 “Attention Is All You Need” 开始火爆，并且实验的效果也是很好的，然后在图像领域也开始尝试使用 attention 机制来解决长依赖的问题。应用到图像领域主要是 explore spatial locality information, 说白了就是细节的信息。 If we look at the DCGAN model, we see that regular GANs are heavily based on convolution operations, which use a local receptive field (convolutional kernel) to learn representations. Simple features like edges and corners are learned in the first few layers. Also, ConvNets are able to use these simple representations to learn more complex ones. However, long-range dependency might be hard to learn. Long-range dependency (long-term dependency) is from RNN, which we can say anything larger than trigram as a long term dependency. Thus, most of the image content does not exhibit elaborated shape such as sky or the ocean looks fine. The task of creating geometrically complex forms, such as four-legged animals, is far more challenging. This is where attention comes into play. 而 self-attention 中QKV 三个部分是相同的，对于这种处理方法和Res_block 还是有点相似的。 结果训练数据集使用是 Cifar-10，记录了训练过程中 acc 和loss 的变化情况。除了训练的效果比较好外，训练速度也是非常快的，可以清楚的看到model acc 在接近25 epoches的时候就开始收敛。]]></content>
      <categories>
        <category>Deep learning</category>
      </categories>
      <tags>
        <tag>siamese network</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[fastText & faiss]]></title>
    <url>%2F2019%2F03%2F25%2FfastText-faiss%2F</url>
    <content type="text"><![CDATA[fastTextfastText结合了自然语言处理和机器学习中最成功的理念。这些包括了使用词袋以及n-gram袋表征语句，还有使用子字(subword)信息，并通过隐藏表征在类别间共享信息。另外采用了一个softmax层级(利用了类别不均衡分布的优势)来加速运算过程。 fastText 主要是用来解决 word representations和 sentence classification. 有趣的是前者是无监督的学习方式，后者是有监督的学习方式。分别主要来自 ”Enriching Word Vectors with Subword Information“ 和 “Bag of Tricks for Efficient Text Classification” 两篇论文。并且使用的是 shallow neural network 而不是深度网络。 FastText is a library created by the Facebook Research Team for efficient learning of word representations and sentence classification. Take off:fastText 方法包含三部分：模型架构、层次 Softmax 和 N-gram 特征。fasttext 有两个用处： text classification 和 word embedding 。使用场景：大型数据，高效计算 下面进行细说： 模型架构这个是总的框架图。&lt;:center&gt;抱歉哈 这个引用找不见了，如果有侵权，please email me..分为两个部分介绍这个网络结构：从input -&gt; hidden:输入层输入的是一个已经分词后短文本。短文本中每个词的词向量是由该短文本的one-hot矩阵乘以一个初始化的矩阵w得到的。（原理图：下图是fasttext 运行的时候，这个分词是再处理成单词和n-gram 组成的特征，这个是不需要我们进行显性的操作的）从 hidden -&gt; output：插播一句，我们经常使用的预训练模型中的weights 是从input-&gt; hidden。 层次Softmax从名字上就知道这个是基于softmax的改进版本，主要是运算上的改进。具体说来：利用了类别（class）不均衡这个事实（一些类别出现次数比其他的更多），通过使用 Huffman 算法建立用于表征类别的树形结构。fastText 模型使用了层次 Softmax 技巧。层次 Softmax 技巧建立在Huffman的基础上，对标签进行编码，能够极大地缩小模型预测目标的数量。 这个是softmax 的原始的计算公式：采用二叉树的结构之后，时间上优化不少。$O ( N) \rightarrow O \left( \log _ { 2 } N \right)$。见下图。 抱歉哈 这个引用找不见了，如果有侵权，please email me..和之前的神经网络模型相比，这里的huffmax树的所有内部节点就类似之前神经网络隐藏层的神经元。其中，根节点的词向量对应我们的投影后的词向量，而所有的叶子节点就类似于之前神经网络softmax输出层的神经元。叶子节点的个数就是词汇表的大小. 和之前的相比，从隐藏层到输出层的softmax映射不是一下就完成的，而是沿着huffman树一步步完成的，因此这种softmax取名为”Hierarchical softmax”. N-gram 特征N-gram是基于这样的思想：某个词的出现依赖于其他若干个词，第二个特点是我们获得的信息越多，预测越准确。我想说，我们每个人的大脑中都有一个N-gram模型，而且是在不断完善和训练的。我们的见识与经历，都在丰富着我们的阅历，增强着我们的联想能力。 N-gram模型是一种语言模型（Language Model，LM），语言模型是一个基于概率的判别模型，它的输入是一句话（单词的顺序序列），输出是这句话的概率，即这些单词的联合概率（joint probability）。 这样的作用，使用n-gram来给文本添加额外的特征获得关于局部词顺序的部分信息。举个栗子：对于句子：“我 喜欢 喝 咖啡”, 如果不考虑顺序，那么就是每个词，“我”，“喜欢”，“喝”，“咖啡”这五个单词的word embedding求平均。如果考虑2-gram, 那么除了以上五个词，还有“我喜欢”，“喜欢喝”，“喝咖啡”等词。“我喜欢”，“喜欢喝”，“喝咖啡”这三个词就作为这个句子的文本特征。我们经常见到的场景：输入法的预选词汇。就是可以通过这种方式实现的。 当然使用了更多的特征意味着造成了效率下降，于是该作者提出了两种解决方法：过滤掉低词频；使用词粒度代替字粒度。比如说海慧寺使用上面那个句子”我喜欢喝咖啡“，如果使用子粒度的2-gram，那么产生的特征是“我喜”，“喜欢”，“欢喝”，“喝咖”，“咖啡”。如果使用词粒度为2-gram，那么产生的特征是“我喜欢”，“喜欢喝”，“喝咖啡”。 Negative Sampling这个技术不是 fastText 首创的，但是本着总结知识点的，也就放在这里了。CBOW / Skip-gram模型 （这个论文中）提出了两种方法，一种是Hierarchical Softmax，另一种是Negative Sampling。论文中提出的两种方法都是用来提高计算效率的，下面说一下负采样。 在训练神经网络时，每当接受一个训练样本，然后调整所有神经单元权重参数，来使神经网络预测更加准确。换句话说，每个训练样本都将会调整所有神经网络中的参数。而negative sampling 每次让一个训练样本仅仅更新一小部分的权重参数，从而降低梯度下降过程中的计算量。如果 vocabulary 大小为1万时， 当输入样本 ( “fox”, “quick”) 到神经网络时， “ fox” 经过 one-hot 编码，在输出层我们期望对应 “quick” 单词的那个神经元结点输出 1，其余 9999 个都应该输出0。在这里，这9999个我们期望输出为0的神经元结点所对应的单词我们为 negative word. negative sampling 的想法也很直接，将随机选择一小部分的 negative words，比如选 10个 negative words 来更新对应的权重参数。 使用第一个应用场景：词向量。fastText作为训练词向量认为可以有两种模式，一种是根据周围词语预测中心词汇的CBOW （continuous bag-of-words）模型，另一种是根据中心词汇预测上下文的 skip-gram 模型。 ./fasttext – It is used to invoke the FastText library. skipgram/cbow – It is where you specify whether skipgram or cbow is to be used to create the word representations. -input – This is the name of the parameter which specifies the following word to be used as the name of the file used for training. This argument should be used as is. data.txt – a sample text file over which we wish to train the skipgram or cbow model. Change this name to the name of the text file you have. -output – This is the name of the parameter which specifies the following word to be used as the name of the model being created. This argument is to be used as is. model – This is the name of the model created.Running the above command will create two files named model.bin and model.vec. model.bin contains the model parameters, dictionary and the hyperparameters and can be used to compute word vectors. model.vec is a text file that contains the word vectors for one word per line. 最后生成有两个文件，一个 xxx.bin 文件，一个是 xxx.vec 文件，前者是预训练模型，后者是词向量。这两个可能是最重要的格式了。 The most important parameters of the model are its dimension and the range of size for the subwords. 常见的代码格式： ./fasttext skipgram -input data/fil9 -output result/fil9 -minn 2 -maxn 5 -dim 300 跑偏一下说一下shell的小技巧。使用echo 或者 &lt; 这样进行单个词或者多个词的词向量的查询。 ./fasttext print-word-vectors model.bin &lt; queries.txtecho “word” | ./fasttext print-word-vectors model.bin Finding simialr words: ./fasttext nn model.bin 第二个应用场景：文本分类。 Sentiment analysis and email classification are classic examples of text classification 在训练数据集中label 默认是使用 “__label__” 进行表示的，当然也是可以进行自定义的。 ./fasttext supervised -input train.ft.txt -output model_kaggle -label __label__ -lr 0.5 就是进行predict的时候，有时候并不是很能想起来只是predict top 3 这样的东西。 # Predicting on the test dataset ./fasttext predict model_kaggle.bin test.ft.txt # Predicting the top 3 labels ./fasttext predict model_kaggle.bin test.ft.txt 3 faiss用途：相似度检测和稠密向量的聚类。 Faiss is a library for efficient similarity search and clustering of dense vectors. 之前的实习经历主要是用faiss 处理文本的representation，但是这个是有偏差的，凡是能够打成词向量，都是可以使用faiss 进行计算的，当然这词向量需要满足：相近内容在相近的空间。 Once the vectors are extracted by learning machinery (from images, videos, text documents, and elsewhere), they’re ready to feed into the similarity search library. faiss的实现过程首先使用 index对于向量进行预处理，然后选择不同的模式。 牺牲了一些精确性来使得运行速度更快。 Similarity search can be made orders of magnitude faster if we’re willing to trade some accuracy; that is, deviate a bit from the reference result. For example, it may not matter much if the first and second results of an image similarity search are swapped, since they’re probably both correct results for a given query. Accelerating the search involves some pre-processing of the data set, an operation that we call indexing. 向量的比较有两种metric：一种是L2 一种是基于consine (点乘)进行检索。前者是求解最小的值，后者是通过inner——product 求解maximum. 并且是支持GPU的，在原来CPU上建立的index，然后很好的迁移到 GPU上。 在涉及index使用考虑速度，acc和内存大小三个不同的维度。然后不同的index 是有不同的侧重的。]]></content>
      <categories>
        <category>NLP</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[NLP中的碎碎念]]></title>
    <url>%2F2019%2F03%2F25%2FNLP%E4%B8%AD%E7%9A%84%E7%A2%8E%E7%A2%8E%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[主要介绍关键词提取并整理一下NLP相关的基础知识点。 关键词提取TF-IDF这个是可以参看之前自己写的一个博客 卡方分布卡方检验是以χ2分布为基础的一种常用假设检验方法，它的无效假设H0是：观察频数与期望频数没有差别。该检验的基本思想是：首先假设H0成立，基于此前提计算出χ2值，它表示观察值与理论值之间的偏离程度。而实际应用到特征选择中的时候，我们不需要知道自由度，不要知道卡方分布，我们只需要根据算出来的χ2 进行排序就好了，越大我们就越喜欢！挑选最大的一堆，于是就完成了利用卡方检验来进行特征提取。卡方分布的缺点：没有考虑词频 Textrank有一个与之很像的概念 pageRanking，最开始是用来计算网页的重要性。Textrank 主要用来提取文章的关键词，然后比较适合长文本。 CBOW和skip-gram举一个简单的小例子说明 CBOW和skip-gram的区别：skip-gram 是根据中心词汇然后预测上下文词汇，这个不是一下子输入上下文词汇的，而是一个过程，中心词汇和上下文词汇1 ，中心词汇和上下文词汇2，这样的进行输入。 cbow 和其的区别，在于简单相加了上下文词汇作为一个整体，然后和中心词汇进行输入，所以最后是这样的结果。 使用skip gram训练的时间更长，但是对于出现频率不高的词汇，效果比较好。但CBOW的训练速度是相对来说比较快一些。]]></content>
      <categories>
        <category>NLP</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Python中的多线程和多进程]]></title>
    <url>%2F2019%2F03%2F25%2FPython%E4%B8%AD%E7%9A%84%E5%A4%9A%E7%BA%BF%E7%A8%8B%E5%92%8C%E5%A4%9A%E8%BF%9B%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[多线程和多进程问题是可以对应到 并发 （cncurrency）和并行(parallelism)上的。 并发，就是一个单核cpu同时开始了多个任务，但是这个任务并不是同时独立进行的，而是通过cpu的不断切换，保存现场，然后重启这样的快速的切换，给用户的感觉是并发，但是实际上是cpu的计算能力受到了限制，用户体验比较好一些。如果在多核cpu （比如我的mac 是一cpu 6核）这样的话完全是可以达到并行的，这个是真正的独立操作(parallelism)，对应的是多进程的。对应python 中的实现多线程是使用threading，处理的是io 响应；多进程是Concurrency，使用multiprocessing包，处理的是多核cpu的操作。 Take off: 如果处理io 响应，那么使用多线程；如果是计算，那么使用多进程。 So, before we go deeper into the multiprocessing module, it’s worthwhile ensuring you know the advantages of using multiprocessing over multithreading. The general rule of thumb is that, if you are trying to improve the performance of CPU-bound tasks, multiprocessing is what you want to use. However, if your particular task is Input/Output bound, then you’ll generally want to use multithreading to improve the performance of your applications. 下面是多线程的demo。 import multiprocessing as mp def my_func(x): print(mp.current_process()) return x ** x def main(): pool = mp.Pool(mp.cpu_count()) # 这个还是很好的 pool 这个的个数和你的cpu count 是保持一致的 result = pool.map(my_func, [4, 2, 3, 5, 3, 2, 1, 2]) result_set_2 = pool.map(my_func, [4, 6, 5, 4, 6, 3, 23, 4, 6]) print(result) print(result_set_2) if __name__ == &quot;__main__&quot;: main() 这个是多进程的demo。 import threading class Worker(threading.Thread): # Our workers constructor, note the super() method which is vital if we want this # to function properly def __init__(self): super(Worker, self).__init__() def run(self): for i in range(10): print(i) def main(): thread1 = Worker() thread1.start() thread2 = Worker() thread2.start() thread3 = Worker() thread3.start() thread4 = Worker() thread4.start() if __name__ == &quot;__main__&quot;: main()]]></content>
      <categories>
        <category>Python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Data Pre-processing 学习笔记]]></title>
    <url>%2F2019%2F03%2F25%2FData-Pre-processing%2F</url>
    <content type="text"><![CDATA[Data Cleaning:这个步骤主要处理 missing values 和 noisy data (outlier).对于missing values 通常有一下处理手段： ignore the tuple; fill in the missing value manually use a global constant to fill in the missing value use the attribute mean to fill in the missing value (均值) use the most probable value to fill in the missing value (mode 众数) 有时候就是根据某几个特征然后弄一个简单的回归模型，根据模型进行predict关于这几种方法如何去选择，我如果说 “it depends”，那么其他人不认为这是一个具有说服力的答案，他们更像知道 it depends what, and when and why to use specific method? 我认为应该是根据缺省值程度和重要性进行经验性的选择，这也去就是 empirical study吧。 接着是 noisy data (outlier)，我的观点是首先得认识到这个是错误的数据，不是真实的数据来源，可能是来自人为的笔误 或者仪器记录的问题，这个是需要修改的。可以使用聚类 (clustering) 进行noisy data 的检测，找到之后这个就类似 missing value了，可以采取以上的手段进行操作，应该注意到的这个 noisy data 所占比例不会很高，否则就成了主要的数据分布了。 Data Integration:处理数据库数据，经常是需要处理子表信息的，那么必然存在着主表，而子表系信息往往是主表信息的某一方面的细化。所以有必要将两者连接起来。 Data Transformation:In data transformation, the data are transformed or consolidated into forms appropriate for mining.这里想要澄清的是很多相同的内容都可以用不同的方式表达，并且可以放在数据处理的不同阶段，并且这种工作不是一次性完成的，而是迭代的 until you run out your patience and time.首先我接触的最常见的就是 discrete variables -&gt; continuous variables. 当然对于 discrete variables，基于树结构的机器学习模型是可以处理的，这里想说的是有这种方式。这种 transformation 常见的处理方式: one-hot 或者 label encoding. 如果按照 data transformation的预设，那么 normalization 就也属于该模块的内容。 不论是在 machine learning 还是在 图像处理的时候，对于原始的数据经常采取 normalization. 一方面这个是可以预防梯度消失 或者 gradient exploding, 如果你采用了 SIgmoid的激活函数的话。另一方面我认为更加重要的原因是将 不同的数据放在了同一个尺度下，如果你采取了 normalization之后。实现这种normalization 经常采用的是以下三种方式：min-max normalization: $x ^ { \prime } = \frac { x - \min ( x ) } { \max ( x ) - \min ( x ) }$mean normalization:$x ^ { \prime } = \frac { x - \text { average } ( x ) } { \max ( x ) - \min ( x ) }$standardization: $x ^ { \prime } = \frac { x - \overline { x } } { \sigma }$ Data Reduction:一般来说很少提及到到 data reducation的必要性，如果非要给出原因，那么可以从时间和空间的角度进行考虑。更加需要关注的是如何做的问题。 我的理解reducation 可以从两个维度进行考虑，假设一个 matrics A 是 m*n，这个是一个二维的矩阵，那么可以从 行列两方面入手。映射到机器学习中一般这样描述 从dimension 和 data两个角度去描述，分别称之为 dimension reduction 和 data compression. 前者指的是特征的选取，后者是数据size的减少。dimension reduction: where irrelevant, weakly relevant, or redundant attributes or dimensions may be detected and removed.data compression: PCA 线性降维 to reduce the data set size. 这个是针对某一个特征展开的。 机器学习中的特征工程是有一定技巧可言，其中我觉得最为有趣的是: generation or you can call it abstraction. 对于特征的泛的提取才是对于问题本身或者特征的理解，这不仅需要积累，更需要对于该问题领域的专业知识， that’s all.举个栗子，在 “Home Credit Default Risk” (kaggle 竞赛)中，原始的训练数据有信贷金额和客户的年收入，这个时候 “credit_income_percent” 就是类似这种性质的提取特征。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Dimension Reduction]]></title>
    <url>%2F2019%2F03%2F25%2FDimension-Reduction%2F</url>
    <content type="text"><![CDATA[对于dimension reduction最近有了新的理解:广义上将降维就是使用更少的数据 (bits) 却保存了尽可能多的信息。You can use this concept to reduce the number of features in your dataset without having to lose much information and keep (or improve) the model’s performance. 不必纠结于采用降维的必要性，直接进入 techniques of dimension reduction. low variances这个是针对一个特征内部的，如果一个特征的数据本身没有什么变化，那么这个类似就是一种“死”数据。 high correlation filter用来判别特征 x 和最后的 target之间的相关性 principal component analysis (PCA)our old good friend. 如果你提降维，但是你不知道PCA，那么就说不过去。该方法的基本思路：一个基（向量空间）的变换，使得变换后的数据有着最大的方差。It works on a condition that while the data in a higher dimensional space is mapped to data in a lower dimension space, the variance of the data in the lower dimensional space should be maximum.下面是PCA的一些特点： A principal component is a linear combination of the original variablesPrincipal components are extracted in such a way that the first principal component explains maximum variance in the datasetSecond principal component tries to explain the remaining variance in the dataset and is uncorrelated to the first principal componentThird principal component tries to explain the variance which is not explained by the first two principal components and so on 主成分是不断生成的，在前者基础之上生成的。 The first component is the most important one, followed by the second, then the third, and so on. Singular Value Decomposition (SVD)翻译成中文感觉还是挺别扭的，奇异值分解。关于奇异值，特征值这些数学概念打算另外写一个主题，wait a moment. 简单理解PCA 是针对方阵 (mm), SVD是针对矩阵(m n)，所以后者是具有更大的适用范围。 Independent Component Analysis (ICA)这个是在面试的时候被问道的一种降维方法。抓住独立向量应该就没有问题。 Independent Component Analysis (ICA) is based on information-theory and is also one of the most widely used dimensionality reduction techniques. The major difference between PCA and ICA is that PCA looks for uncorrelated factors while ICA looks for independent factors. 基本假设： This algorithm assumes that the given variables are linear mixtures of some unknown latent variables. It also assumes that these latent variables are mutually independent, i.e., they are not dependent on other variables and hence they are called the independent components of the observed data. ICA 和PCA的异同：从线性代数的角度去理解，PCA和ICA都是要找到一组基，这组基张成一个特征空间，数据的处理就都需要映射到新空间中去。ICA相比与PCA更能刻画变量的随机统计特性，且能抑制高斯噪声。 T-SNE就是指出 t-SNE 这个可以非线性降维。有local approaches 和 global approaches 两种方式，我大概的理解就是：类之间还能尽可能的远离，类内保持差异性。这个使用场景是在可视化中，经常会看见将数据或者 So far we have learned that PCA is a good choice for dimensionality reduction and visualization for datasets with a large number of variables. But what if we could use something more advanced? What if we can easily search for patterns in a non-linear way? t-SNE is one such technique. There are mainly two types of approaches we can use to map the data points:Local approaches : They maps nearby points on the manifold to nearby points in the low dimensional representation.Global approaches : They attempt to preserve geometry at all scales, i.e. mapping nearby points on manifold to nearby points in low dimensional representation as well as far away points to far away points. 下面介绍两种不是那么“常规”，但是也符合”dimension reduction” 定义的方式。 projectionBy projecting one vector onto the other, dimensionality can be reduced. autoencoder网络结构通常有 encoder和decoder两部分组成，那么encoder 就作为 information abstraction,而 decoder作为一种重新映射。从这个角度NLP中的词向量也是可以是一种降维手段。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Linear Algebra in ML]]></title>
    <url>%2F2019%2F03%2F25%2FLinear-Algebra-in-ML%2F</url>
    <content type="text"><![CDATA[我觉得到 ML 中的一个难点：就是由原来简单的 linear equations 直接过渡到了 matrics and vectors。这个过程是没有人跟你说的。网络结构可以当作是一个complicated 并且是无法表示的函数，于是很多使用者把它当作黑匣子，关心于输入和输出，中间过程 don’t care. 变量（特征个数）和解的关系多变量和最后target的关系是可以使用 matrices 进行表示的，这就是一种数学公式化。 Broadly speaking, in linear algebra data is represented in the form of linear equations. These linear equations are in turn represented in the form of matrices and vectors. 先直观的感受一下变量和图形（可视化）的关系。两个变量组成的equations 是两条线的相交情况。而三个变量在空间中有三种情况： 相交，平行，不在一个平面上。三个变量组成的equations 是三个面的相交情况。有四种情况 (try hard to figure it out)：No intersection at all.Planes intersect in a line.They can intersect in a plane.All the three planes intersect at a point. 当到达4 dims 的时候，it’s impossible to visulize it. terms in related to matrix这些词汇 (terms) 经常在文献中出现，需要对于其含义有个比较好的认识。Order of matrix – If a matrix has 3 rows and 4 columns, order of the matrix is 34 i.e. rowcolumn. (翻译成 矩阵的阶)Square matrix – The matrix in which the number of rows is equal to the number of columns.Diagonal matrix – A matrix with all the non-diagonal elements equal to 0 is called a diagonal matrix.Upper triangular matrix – Square matrix with all the elements below diagonal equal to 0.Lower triangular matrix – Square matrix with all the elements above the diagonal equal to 0.Scalar matrix – Square matrix with all the diagonal elements equal to some constant k.Identity matrix – Square matrix with all the diagonal elements equal to 1 and all the non-diagonal elements equal to 0.Column matrix – The matrix which consists of only 1 column. Sometimes, it is used to represent a vector.Row matrix – A matrix consisting only of row.Trace – It is the sum of all the diagonal elements of a square matrix.Rank of a matrix – Rank of a matrix is equal to the maximum number of linearly independent row vectors in a matrix.Determinant of a matrix - 矩阵的行列式转置 -在图形 matrix中还是很常见的。$$\mathrm { A } _ { \mathrm { ij } } ^ { \mathrm{T}} = \mathrm { A } _ { \mathrm { ji } }$$ 这个矩阵乘法和元素相称的区别，后者是element-wise 进行的。可以从另外一个角度去列及矩阵相称： This operation on a vector is called linear transformation. 就是后面的vector 映射到了前面的矩阵空间。 特征值和奇异值着两个是分别对应着PCA 和SVD。Eigenvalues and Eigenvectors如公式所示，特征值和特征向量的乘积就是方阵和特征向量的乘积，原先的方阵是可以降维表示成特征向量和特征值的。$ A x = \lambda x $ 对于奇异值分解，最常见的就是这种表达：$A = U \Sigma V ^ { T }$特征值分解和奇异值分解都是给一个矩阵(线性变换)找一组特殊的基，特征值分解找到了特征向量这组基，在这组基下该线性变换只有缩放效果。而奇异值分解则是找到另一组基，这组基下线性变换的旋转、缩放、投影三种功能独立地展示出来了。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Back to my blog]]></title>
    <url>%2F2019%2F03%2F07%2FBack-to-my-blog%2F</url>
    <content type="text"><![CDATA[直到某一天发现个人网站中的图片都显示不出来了，查了一下才发现之前的图床不能用了（点名批评七牛），果断弃之，换了个大厂子产品。证明一下图片是能出来的。ps：之前的图片有时间再整理到新的平台上。]]></content>
      <categories>
        <category>人间不值得</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[基于simhash的文本相似度比较]]></title>
    <url>%2F2018%2F08%2F23%2F%E5%9F%BA%E4%BA%8Esimhash%E7%9A%84%E6%96%87%E6%9C%AC%E7%9B%B8%E4%BC%BC%E5%BA%A6%E6%AF%94%E8%BE%83%2F</url>
    <content type="text"><![CDATA[我只是想说我还没有放弃这个网站… 本文主要记录使用simhash比较中文文本的相似度问题。先说一下文本特征，数据属于中文文本，每篇文章的字数大于500,小于2000,基本上属于大文本。步骤如下： 基于tf-idf提取文本的关键词。如果这些关键词在之后的比较中是相同的，那么认为对应的文章也是相同。简而言之，这些提取的关键词可以看做原文章的”代表”. 根据关键字计算simhash编码，然后使用hamming distance进行比较两者的不同。如果对于上述概念比较模糊，建议首先阅读该篇博客。 顺滑过渡到代码实现：123456789101112131415# 常规导包import sys,codecsimport pandas as pdimport numpy as npimport jieba.possegimport jieba.analysefrom sklearn import feature_extractionfrom sklearn.feature_extraction.text import TfidfTransformerfrom sklearn.feature_extraction.text import CountVectorizer# 数据集的路径path =&quot;../tianmao2.csv&quot;names =[&apos;where&apos;, &apos;time&apos;, &apos;title&apos;, &apos;url&apos;, &apos;contents&apos;]data =pd.read_csv(path, delimiter=&apos;\t&apos;, names= names, nrows=200)data[&apos;id&apos;] =data.index+1data.head() 我们使用title和contents 组合作为原始处理的数据，我们认为该数据能够就是文章的内容。1stopkey = [w.strip() for w in codecs.open(&apos;../keyword_extraction/data/stopWord.txt&apos;, &apos;r&apos;).readlines()] 该stop words是中文停用词，就是常见的”的 了”。常见的有百度停用词表、哈尔滨工业大学停用词表以及中科院的停用词表。这里使用的是中科院的停用词。12345678def dataPrepos(text, stopkey): l = [] pos = [&apos;n&apos;, &apos;nz&apos;, &apos;v&apos;, &apos;vd&apos;, &apos;vn&apos;, &apos;l&apos;, &apos;a&apos;, &apos;d&apos;] # 定义选取的词性 seg = jieba.posseg.cut(text) # 分词 for i in seg: if i.word not in stopkey and i.flag in pos: # 去停用词 + 词性筛选 l.append(i.word) return l 我们选择名词作为主要的分析对象。12345678idList, titleList, abstractList = data[&apos;id&apos;], data[&apos;title&apos;], data[&apos;contents&apos;]corpus = [] # 将所有文档输出到一个list中，一行就是一个文档# 这个 虽然使用 &quot; &quot; 进行分割 但是实际上还是一个打的listfor index in range(len(idList)): text = &apos;%s。%s&apos; % (titleList[index], abstractList[index]) # 拼接标题和摘要 text = dataPrepos(text, stopkey) # 文本预处理 text = &quot; &quot;.join(text) # 连接成字符串，空格分隔 corpus.append(text) 这里的corus 是将所有的经过预处理文档作为当前计算 idf 的语料库。123456789vectorizer = CountVectorizer()X = vectorizer.fit_transform(corpus) # 词频矩阵,a[i][j]:表示j词在第i个文本中的词频# 2、统计每个词的tf-idf权值transformer = TfidfTransformer()tfidf = transformer.fit_transform(X)# 3、获取词袋模型中的关键词word = vectorizer.get_feature_names()# 4、获取tf-idf矩阵，a[i][j]表示j词在i篇文本中的tf-idf权重weight = tfidf.toarray() 使用sklearn 内置的函数计算tf-idf。1234567891011121314151617181920212223242526272829topK = 10ids, titles, keys, weights = [], [], [], []for i in range(len(weight)): print(&quot;-------这里输出第&quot;, i + 1, &quot;篇文本的词语tf-idf------&quot;) ids.append(idList[i]) titles.append(titleList[i]) df_word, df_weight = [], [] # 当前文章的所有词汇列表、词汇对应权重列表 for j in range(len(word)): # print(word[j],weight[i][j]) df_word.append(word[j]) df_weight.append(weight[i][j]) df_word = pd.DataFrame(df_word, columns=[&apos;word&apos;]) df_weight = pd.DataFrame(df_weight, columns=[&apos;weight&apos;]) word_weight = pd.concat([df_word, df_weight], axis=1) # 拼接词汇列表和权重列表 word_weight = word_weight.sort_values(by=&quot;weight&quot;, ascending=False) # 按照权重值降序排列 # 在这里可以查看 k的选取的数值应该是多大， # from ipdb import set_trace # set_trace() keyword = np.array(word_weight[&apos;word&apos;]) # 选择词汇列并转成数组格式 word_split = [keyword[x] for x in range(0, topK)] # 抽取前topK个词汇作为关键词 word_split = &quot; &quot;.join(word_split) keys.append(word_split) wei = np.array(word_weight[&apos;weight&apos;]) wei_split = [str(wei[x]) for x in range(0, topK)] wei_split = &quot; &quot;.join(wei_split) weights.append(wei_split) # 这里的命名 容易混淆result = pd.DataFrame(&#123;&quot;id&quot;: ids, &quot;title&quot;: titles, &quot;key&quot;: keys, &apos;weight&apos;: weights&#125;, columns=[&apos;id&apos;, &apos;title&apos;, &apos;key&apos;, &apos;weight&apos;]) 选择前10个频率最高的词语作为该篇文章的代表，当然这个参数是可以调整，需要根据具体的问题和结果进行调整。1result.head() 最后的效果如上。至此我们第一步的提取文章的关键词就已经做完。下面进行相似度的比较。 1234import jiebaimport jieba.analyseimport pandas as pd#日常导包 数据和上述的一样，所以就不截图了。123datasets =pd.read_csv(&quot;../tianmao2-tf-idf.csv&quot;)tokens =datasets[&apos;key&apos;]weights =datasets[&apos;weight&apos;] 提取关键词和对应的权重。123456789101112print(tokens[0], len(tokens[0]))print(weights[0], len(weights[0]))tokens0 =tokens[0].split()weights0 =weights[0].split()len(tokens0)len(weights0)tokens1 =tokens[1].split()weights1 =weights[1].split()import astweights0 =[ ast.literal_eval(i) for i in weights0]weights1 =[ ast.literal_eval(i) for i in weights1] 构造测试用例。因为权重是字符串，所以简单处理转成整数。 12dict0 =dict(zip(tokens0, weights0))dict1 =dict(zip(tokens1, weights1)) 定义一个Simhash，提供对文档的数值映射和文档间相似度计算的功能.123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081class Simhash(object): # 初始化函数 def __init__(self, weights_dict, tokens=&apos;&apos;, hashbits=64): self.hashbits = hashbits self.hash = self.simhash_function(tokens, weights_dict) # toString函数 # 不懂这个 self.hash ，凡是带有self 的函数都是可以类变量，所以这个就是返回的 self.hash这个变量 #凡是使用__str__ 这种类型的函数 都是重写 原来的函数 def __str__(self): return str(self.hash) &quot;&quot;&quot; ord() 函数是 chr() 函数（对于8位的ASCII字符串）或 unichr() 函数（对于Unicode对象）的配对函数，它以一个字符（长度为1的字符串）作为参数，返回对应的 ASCII 数值，或者 Unicode 数值 &quot;&quot;&quot; # 给每一个单词生成对应的hash值 # 这个操作搞懂之后一定很简洁， 但是现在很难理解，因为不是字符串，而是位操作 def _string_hash(self, source): if source == &apos;&apos;: return 0 else: x = ord(source[0]) &lt;&lt; 7 # &lt;&lt; 表示 乘以2^7 ; &gt;&gt; 表示除以 ; ** 表示次方的意思 # ^ : 按位异或 (二进制进行异或)； &amp; 按位进行与 操作 # 左移位操作也是可以理解为 2^x 的操作，因为存储是二进制，这样左移一位 表示×2 一次 m = 1000003 mask = 2 ** self.hashbits - 1 for c in source: x = ((x * m) ^ ord(c)) &amp; mask x ^= len(source) if x == -1: x = -2 return x # 生成simhash值 def simhash_function(self, tokens, weights_dict): v = [0] * self.hashbits # 这种使用 &#123;&#125; dictionary 然后强行得到item 再进行遍历也是牛逼 for key, t in &#123;x: self._string_hash(x) for x in tokens&#125;.items(): for i in range(self.hashbits): bitmask = 1 &lt;&lt; i if t &amp; bitmask: v[i] += weights_dict[key] else: v[i] -= weights_dict[key] fingerprint = 0 for i in range(self.hashbits): if v[i] &gt;= 0: fingerprint += 1 &lt;&lt; i return fingerprint # 求文档间的海明距离 def hamming_distance(self, other): x = (self.hash ^ other.hash) &amp; ((1 &lt;&lt; self.hashbits) - 1 ) tot = 0 while x : tot += 1 x &amp;= x - 1 return tot #求相似度 # 这个相似度的计算，十分简单，如果两个数接近，那么就是认为相似。越是接近1 越是相似， # 不是原先那种以某一个参数整数 如3 为距离的相似度 def similarity(self, other): a = float(self.hash) b = float(other.hash) if a &gt; b: return b / a else: return a / b if __name__ == &apos;__main__&apos;: hash0 = Simhash(weights_dict=dict0, tokens=tokens0) print(hash0) hash1 = Simhash(weights_dict=dict1, tokens=tokens1) print(hash1) print(hash0.hamming_distance(hash1)) print(hash0.similarity(hash1)) 结果如上。可以看出该例子中使用的两两比较的方式，对于大数据来说，一般可能会用到倒排索引和cpu并行技术。]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>文本相似度</tag>
        <tag>Simhash</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[文本相似度比较基本知识]]></title>
    <url>%2F2018%2F08%2F23%2F%E6%96%87%E6%9C%AC%E7%9B%B8%E4%BC%BC%E5%BA%A6%E6%AF%94%E8%BE%83%E5%9F%BA%E6%9C%AC%E7%9F%A5%E8%AF%86%2F</url>
    <content type="text"><![CDATA[本文服务于该篇博客,主要进行名词解释。 simhash基本概念simhash 也是一种hash，一般的hash 函数映射规则只需要满足以下两个条件： 对很多不同的特征来说，它们对所对应的向量是均匀随机分布的 相同的特征来说对应的向量是唯一简单来说普通的hash映射需要满足随机分布和唯一性两个条件。simhash想要实现的是，如果原来的文本的特征是相似，那么映射之后的编码也是相似。这里使用 hamming distance 进行比较simhash映射之后的距离。根据经验值，对64位的 SimHash值，海明距离在3以内的可认为相似度比较高。编码之后的表示在英文中是 fingerprint(指纹)。simhash最初被google 用于网页去重，当时使用的fingerprint 是64,所以这里沿用了这个传统。64位的签名可以表示多达264个象限，因此只保存所在象限的信息也足够表征一个文档了。更进一步，表示的文档的数字最多是多少？这个应该可以准确计算特征的个数应为如果用三位(01) 表示，那么有8种，那么2^64 这么多种特征，所以16*10^18 这么多。算法步骤第一步：文本预处理得到分词（去重，去除的了stop words）,然后weight权重可以使用分词的frequency 或者tfidf 进行得到第二步：进行hash 映射（可以使用md5这种传统的映射方式，基本的要求就是均匀映射到一个空间，这种映射并不能反映原始样本的相关性）第三步：hash 映射值和weight 进行相乘，如果原来是1 则乘以1，如果是0 则乘以-1，第四步： 列向相加，得到summing weights,进行降维如果是正数那么为1，如果是负数那么为-1然后这个simhash就出来了.有图有真相simhash的局限性：只考虑到文章存在哪些词，没有考虑到词的顺序。不过相应的优点是，可以实现海量文章相似度计算。文章相似度计算忽略词的顺序之后效果更好。所以在处理大文本时候，simhash是有效的，但是在处理小文本，这种效果往往不能被保证。直观上理解，在一片段文章或者段落中，词语出现的顺序还是比较重要的。minhash可以参考该视频和这篇文章。Locality Sensitive HashingLocality Sensitive Hashing(局部敏感哈希)作用就是从海量的数据中挖掘出相似的数据，可以具体应用到文本相似度检测、网页搜索等领域。上面的simhah和minhash 就是该思想的实现。距离函数这里的距离函数都是用来文本相似度。Jaccard相似度简单来说交集除以并集。这个集合中存放的是文章或者段落的关键词。1234567891011def JaccardSim(str_a, str_b): &apos;&apos;&apos; Jaccard相似性系数 计算sa和sb的相似度 len（sa &amp; sb）/ len（sa | sb） &apos;&apos;&apos; seta = splitWords(str_a)[1] setb = splitWords(str_b)[1] sa_sb = 1.0 * len(seta &amp; setb) / len(seta | setb) return sa_sb 可以看到核心代码很简单，经过分词之后，就是seta 和setb 进行的操作。 cosine12345def cos_sim(a, b): a = np.array(a) b = np.array(b) # return &#123;&quot;文本的余弦相似度:&quot;:np.sum(a*b) / (np.sqrt(np.sum(a ** 2)) * np.sqrt(np.sum(b ** 2)))&#125; return np.sum(a * b) / (np.sqrt(np.sum(a ** 2)) * np.sqrt(np.sum(b ** 2))) 将文本的关键词映射成某种高维函数，然后在高维空间中计算两者的相似度。 tf-idf在simhash 中使用 tf-idf作为我们的比较函数。TF-IDF的主要思想就是：如果某个词在一篇文档中出现的频率高，也即TF高；并且在语料库中其他文档中很少出现，即DF的低，也即IDF高，则认为这个词具有很好的类别区分能力。词频(term frequency)有两种计算方式,后者考虑了相对的情况。计算idf(inverse document frequency):TF-IDF 优点是简单快速，比较符合实际。缺点，无法体现词的位置信息，所有的位置都是被认为重要性相同，但是开头结尾，段落的开头和段落的结尾，therefore，so，but这些词语都是没有体现的。 hamming distancehamming distance就是比较01串的不同，按照位进行比较。算法：异或时，只有在两个比较的位不同时其结果是1 ，否则结果为0，两个二进制“异或”后得到1的个数即为海明距离的大小。123456789101112131415161718hashbits =64 # 使用64位进行编码def simhash_function(tokens, weights_dict): v = [0] * hashbits # 这种 &#123;key: value&#125;.item() 的操作也是没有了谁了 for key, t in &#123;x: _string_hash(x) for x in tokens&#125;.items(): for i in range(hashbits): bitmask = 1 &lt;&lt; i if t &amp; bitmask: v[i] += weights_dict[key] else: v[i] -= weights_dict[key] fingerprint = 0 for i in range(hashbits): if v[i] &gt;= 0: fingerprint += 1 &lt;&lt; i return fingerprintfingerprint = simhash_function(tokens, weights) min edit distance123456789101112131415161718192021222324# 最小编辑距离def min_edit_distance(str1, str2): rows =len(str2) +1 cols =len(str1) +1 arr =[[0 for _ in range(cols)] for _ in range(rows)] # 这种简洁的代码也是牛逼 for j in range(cols): arr[0][j] =j for i in range(rows): arr[i][0] =i for i in range(1, rows): for j in range(1, cols): # 因为string 是从0 ，len(str) -1的 if str2[i-1] ==str1[j-1]: arr[i][j] =arr[i-1][j-1] else: # 以后见到这样的式子，就要想到这个二维的数组，因为这个是可以帮助记忆的 arr[i][j] =1 +min(arr[i-1][j-1], arr[i-1][j], arr[i][j-1]) # 右下角就是距离 return arr[rows-1][cols-1]str_a =&quot;abcdef&quot;str_b =&quot;azced&quot;result =min_edit_distance(str_a, str_b)result 具体可以参看该视频讲解。(ps. 如果刷leetcode,也可以参看该视频) 分词在英文中存在天然的空格可以进行分词操作，但是中文的分词就比较复杂了。常用的中文分词开源工具有 jieba和HanLP前者简单易行，容易上手；后者在自然语言处理作为汉语言处理包，可以用于词性标注，命名实体识别等一系列功能。常用的英文分词 corenlp 倒排索引倒排索引使用python在实现上就是一个dictionary 嵌套一个 set(). 一般的索引都是数字或者英文字母映射内容，具体在放到simhash的情景下就是使用文章的序列号对应提取出来的关键词。但是倒排索引就是关键词对应文章的序列号，类似与原来的”值”对应这”键”，所以称之为倒排索引。可以参考这篇文章。]]></content>
      <tags>
        <tag>文本相似度</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[differences-between-l1-and-l2-as-loss-function-and-regularization]]></title>
    <url>%2F2018%2F07%2F21%2Fdifferences-between-l1-and-l2-as-loss-function-and-regularization%2F</url>
    <content type="text"><![CDATA[如果补交作业也算作业的话，那么这篇博文就算做作业。L1 和L2 作为Loss function和 regularization，个人感觉是一个经常容易混淆的概念。但是如果读者觉得很清楚，那么就可以跳过了。本文大量借鉴于该博客，原文是英文，如果读者英文能够handle，建议读原文。 As loss functionloss function or error function 是用来衡量真实y 和生成的f(x) 之间差距的函数。在模型训练中我们一般情况下不断训练模型使得loss function不断下降（如果task要求loss function是增大，这时候一般加上符号或者转换成 1- loss fucntion，最后实现的还是loss function下降）。好的回到L1 loss function和L2 loss function. L1-norm loss function is also known as least absolute deviations(LAD), least absolute errors. It is basically minimizing the sum of the absolute differences between the target value Y and the estimated values (f(x)).L2-norm loss function is also known as least squares error(LSE). It is basically minimizing the sum of the square of the differences between the target value Y and the estimated values (f(x)).主要你从一下三个指标去衡量两者的不同： robustness，stability和是否具有唯一解。wiki 中关于robust 中是这样定义： A learning algorithm that can reduce the chance of fitting noise is called robust。具有更好的泛化性能，不去过度拟合noise. 关于stability) wiki 是这样定义：Stability, also known as algorithmic stability, is a notion in computational learning theory of how a machine learning algorithm is perturbed by small changes to its inputs. 我对于前两个指标的理解：robustness 是对于原始的train 样本中离群点的态度，如果某个模型是robustness的，那么对于该数据集中的离群点是能够抗干扰的。反之则是不具有robustness的。stability是对于原始train 数据的轻微的平移的反应，如果对于某个原始数据的轻微平移，最后的结果没有产生很大的波动，那么该模型就是具有stability, 反之，则不具有stability. 比较L1-norm 和L2-norm在前两个评价指标中的表现：对于第三点，我想在下面进行介绍。因为这点和后面和下面的solution uniqueness是相同的。 As regularization从XGBoost调参指南中我们知道objective function = loss funcion + regularization. 而我们大多数情况下提及的都是loss function,常常忽略了regularization 的作用。所以正确的姿态应该是这样的：The regularization term controls the complexity of the model, which helps us to avoid overfitting.对于模型训练，一开始的想法是尽量的overfitting, 因为就现在不成熟的经验而言，对于overfitting这个问题有很多处理方法，比如卷积深度神经网络中的dropout, LightGBM中的early stop 和随机采样的思想。 这些方法都是可以缓解overfitting，所以可以出现overfitting。相反，如果你的模型是underfitting，那么你就微显尴尬了。好，收回到L1 and L2。 先上公式L1 regularization on least squares:L2 regularization on least squares:The difference between their properties can be promptly summarized as follows: 对于第一点computational efficient的理解：平方比绝对值更容易计算，平方可以求导直接求最值，但是绝对值就无法求导。并且L1 regularization在 non-sparse cases中是 computational inefficient，但是在 sparse(0比较多) cases中是有相应的稀疏算法来进行优化的，所以是computational efficient.对于第二点是否具有sparse solution可以从几何意义的角度解读：The green line(L2-norm) is the unique shortest path, while the red, blue yellow(L1-norm) are all same length for the same route. Built-in feature selection is frequently mentioned as a useful property of the L1-norm, which the L2-norm does not. This is actually a result of the L1-norm, which tends to produces sparse coefficients (explained below). Suppose the model have 100 coefficients but only 10 of them have non-zero coefficients, this is effectively saying that “the other 90 predictors are useless in predicting the target values”. L2-norm produces non-sparse coefficients, so does not have this property. 所以表格中第三点也是顺理成章的了。至此，我们区分了L1-norm vs L2-norm loss function 和L1-regularization vs L2-regularization。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>l1</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LightGBM和XGBoost及其调参]]></title>
    <url>%2F2018%2F07%2F21%2FLightGBM%E5%92%8CXGBoost%E5%8F%8A%E5%85%B6%E8%B0%83%E5%8F%82%2F</url>
    <content type="text"><![CDATA[lightGBM调参(常用参数)Since lightGBM is based on decision tree algorithms, it splits the tree with the best fit whereas boosting algorithms split the tree depth wise or level wise rather than leaf-wise. So when growing on the same leaf in Light GBM, the leaf-wise algorithm can reduce more loss than the level-wise algorithm and hence results in much better accuracy which can rarely be achieved by any of the existing boosting algorithms. Also, it is surprisingly very fast, hence the word ‘Light’.Leaf wise splits lead to increase in complexity and may lead to overfitting and it can be overcome by specifying another parameter max_depth parameter. Advantages of LightGBM faster training speed and higher efficiencyLight GBM use histogram based algorithm i.e it buckets continuous feature values into discrete bins which fasten the training procedure. lower memory usageReplaces continuous values to discrete bins which result in lower memory usage. better accuracy than any other boosting algorithmIt produces much more complex trees by following leaf wise split approach rather than a level-wise approach which is the main factor in achieving higher accuracy. compatibility with large datasetsIt is capable of performing equally good with large datasets with a significant reduction in training time as compared to XGBOOST. parallel learning supported lightGBM调参(常用参数) taskdefault= train, option: train, prediction applicationdefault= regression, option: regression, binary, multiclass, lambdarank(lambdarank application) datatraining data, 这个比较诡异，你需要创建一个lightGBM类型的data num_iterationsdefault =100, 可以设置为的大一些，然后使用early_stopping进行调节。 early_stopping_rounddefault =0, will stop training if one metric of one validation data doesn’t improve in last early_stopping_round rounds. num_leavesdefault =31, number of leaves in a tree devicedefault =cpu, options: gpu, cpu, choose gpu for faster training. max_depthspecify the max depth to which tree will grow, which is very important. feature_fractiondefault =1, specifies the fraction of features to be taken for each iteration. bagging_fractiondefault =1, spefifies the fraction of data to be used for each iteration and it generally used to speed up the training and avoid overfitting. max_binmax number of bins to bucket the feature values.因为模型是基于bin训练的，如果bin 数量越多，得到better accuracy,同时更加容易 overfitting. num_threads labelspecify the label columns. categorical_featurespecify the categorical features num_classdefault =1, used only for multi-class classification referrencewhich-algorithm-takes-the-crown-light-gbm-vs-xgboostLightGBM 如何调参官方文档param_tuning官方文档parameter XGBoost调参Advantage of XGBoost regularizationstandard GBM implementation has no regularization, in fact, XGBoost is also known as ‘regularized boosting’ technique. parallel processingwe know that boosting is sequential process so how can it be parallelized? this link to explore further. high flexibilityXGBoost allow users to define custom optimization objectives and evaluation criteria handling missing valuesvery useful property. XGBoost has an in-built routine to handle missing values. User is required to supply a different value than other observations and pass that as a parameter. XGBoost tries different things as it encounter a missing value on each node and learns which path to take for missing values in future. Tree pruningA GBM would stop splitting a node when it encounters a negative loss in the split. Thus it is more of a greedy algorithm. XGBoost on the other hand make splits upon the max_depth specified and then start pruning the tree backwards and remove splits beyond which there is no positive gain. built-in cross-validationThis is unlike GBM where we have to run a grid-search and only a limited values can be tested. continue on existing model XGBoost Parametersgeneral parametersGeneral Parameters: Guide the overall functioning booster:default =gbtree, can be gbtree, gblinear or dart. 一般使用gbtree. silent:default =0, silent mode is activated if set to 1(no running messages will be printed) nthread:default to maximum of threads. booster parametersBooster Parameters: Guide the individual booster (tree/regression) at each step eta(learning rate):default=0.3, typical final values to be used: 0.01-0.2, using CV to tune min_child_weight:minimum sum of instance weight (hessian) needed in a child. If the tree partition step results in a leaf node with the sum of instance weight less than min_child_weight, then the building process will give up further partitioning. In linear regression mode, this simply corresponds to minimum number of instances needed to be in each node. The larger, the more conservative the algorithm will be.default =1,too high values can lead to under-fitting, it should be tuned using CV. 数值越小越容易过拟合，越大越容易 under-fitting. max_depth:default =6, typical values: 3-10, should be tuned using CV. gamma:default =0, Gamma specifies the minimum loss reduction required to make a split.如果在分裂过程中小于该值，那么就不会继续分裂。 subsample:default =1, typical values: 0.5-1. Denotes the fraction of observations to be randomly samples for each tree. colsample_bytree:default =1, typical values: 0.5-1. colsample_bytree和subsample不同点：colsample_by是特征的随机fraction, subsample是rows的随机fraction。 lambda:default =1, L2 regularization term on weights(analogous to Ridge regression). Though many data scientists don’t use it often, it should be explored to reduce overfitting. alpha:default =0, L1 regularization term on weight (analogous to Lasso regression) scale_pos_weight:default =1, a value greater than 0 should be used in case of high class imbalance as it helps in faster convergence. learning task parametersLearning Task Parameters: Guide the optimization performed objectivebinary: logistic- returns predicated probability(not class)multi: softmax- returns predicated class(not probabilities)multi: softprob- returns predicated probability of each data point belonging to each class. eval_metircdefault according to objective(rmse for regression and error for classification), used for validation data.typical values: rmse(root mean square error), mse(mean absolute error), logloss(negative log-likelihood), error(binary classification error rate, 0.5 threshold), auc(area under the curve) seeddefault =0, used for reproducible results and also for parameter tuning. Control OverfittingThere are in general two ways that you can control overfitting in xgboost. The first way is to directly control model complexity. The second way is to add regularization parameters Referrencecomplete guide parameter tuning xgboost with codes python官方文档 param_tuning官方文档 parameter]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>LightGBM</tag>
        <tag>XGBoost</tag>
        <tag>调参</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[那些年的算法题目（二）]]></title>
    <url>%2F2018%2F07%2F21%2F%E9%82%A3%E4%BA%9B%E5%B9%B4%E7%9A%84%E7%AE%97%E6%B3%95%E9%A2%98%E7%9B%AE%EF%BC%88%E4%BA%8C%EF%BC%89%2F</url>
    <content type="text"><![CDATA[最长01相同子串已知一个长度为N的字符串，只由0和1组成， 求一个最长的子串，要求该子串出0和1出现的次数相等。思路：最简单的方式是先生成字串，然后判断每个字串是否满足0的个数和1的个数相同。这种暴力求解时间复杂度O(n^3),明显是不合理的。下面说一下简单的做法：定义一个数组B[N]，B[i]表示从A[0…i]中 num_of_0 - num_of_1，0的个数与1的个数的差 。那么如果A[i] ~ A[j]是符合条件的子串，一定有 B[i] == B[j]，因为中间的部分0、1个数相等，相减等于0。 代码实现12345678910111213141516171819202122232425def lengest01SubStr(s): &apos;&apos;&apos; 最长0,1 相等的子串长度 &apos;&apos;&apos; count =[0, 0] B =[0]*len(s) dic =&#123;&#125; lengest =0 for i in range(len(s)): count[int(s[i])] +=1 B[i] =count[0] - count[1] if B[i] ==0: lengest +=1 continue if B[i] in dic: lengest =max(lengest, i- dic[B[i]]) else: dic[B[i]] =i return lengesta =&apos;1011010&apos;b =&apos;10110100&apos;print(lengest01SubStr(a))print(lengest01SubStr(b)) 顺时针打印矩阵输入一个矩阵，按照从外向里以顺时针的顺序依次扫印出每一个数字。思路：发现网上有很多使用递归的，但是使用四个循环就可以解决这个问题。找到每次开始的起点，然后按照最上面一行，最右面一列，最小面一行和最左面一行这样的顺序进行打印即可。123456789101112131415161718192021222324252627282930313233343536373839404142def printMatrix(matrix): if matrix ==[[]]: return # 第一次见这样判断空的matrix row =len(matrix) column =len(matrix[0]) # 这里的left, right, up, down 都是真实能够access到数据的 left =0 right =column -1 up =0 down =row -1 res =[] while right &gt;left and up &lt;down: # from left to right for i in range(left, right+1): res.append(matrix[up][i]) # from up to down for i in range(up+1, down+1): res.append(matrix[i][right]) # from right to left for i in range(right-1, left-1, -1): res.append(matrix[down][i]) for i in range(down-1, up, -1): res.append(matrix[i][left]) left +=1 right -=1 up +=1 down -=1 # 最后对于这种特殊情况的处理是容易忘记的 # left one row 这种情况很特殊，只是从左往右遍历 if up ==down and left &lt;right: for i in range(left, right+1): res.append(matrix[up][i]) # left one column 只有可能是从上往下遍历 if left ==right and up &lt;down: for i in range(up, down+1): res.append(matrix[i][left]) if up ==down and left ==right: res.append(matrix[left][up]) return resprint(printMatrix(matrix)) 下面这个版本并没有运行成功，但是中间有个语法点是可以学习的。12345678910111213141516171819202122232425262728293031323334353637def printMatrix(matrix): res =[] # 第一个坐标表示行数，第二个坐标表示列数 # m 表示行数，n 表示列数 m =len(matrix) n = len(matrix[0]) if m ==1 and n ==1: res =[matrix[0][0]] return res else: for o in range(int((min(m,n)+1)/2)): # 不加这个[] 会有语法错误 [res.append(matrix[o][i]) for i in range(o, n-o)] [res.append(matrix[j][n-o-1]) for j in range(o, m-o) if matrix[j][n-o-1] not in res ] # 感觉这个最后的not in 是不合理的，万一这两个数字就是相同呢，试一下 [res.append(matrix[m-o-1][k]) for k in range(n-1, o-1, -1) if matrix[m-o-1][k] not in res] [res.append(matrix[l][o]) for l in range(m-1-o, o-1,-1) if matrix[l][o] not in res] # 这种写法还是少用，可以使用append,就少用这种，可读性不强，容易错 return resdef printMatrix(matrix): res =[] # 第一个坐标表示行数，第二个坐标表示列数 # m 表示行数，n 表示列数 m =len(matrix) n = len(matrix[0]) if m ==1 and n ==1: res =[matrix[0][0]] return res else: for o in range(int((min(m,n)+1)/2)): # 不加这个[] 会有语法错误 [res.append(matrix[o][i]) for i in range(o, n-o)] [res.append(matrix[j][n-o-1]) for j in range(o, m-o) if matrix[j][n-o-1] not in res ] # 感觉这个最后的not in 是不合理的，万一这两个数字就是相同呢，试一下 [res.append(matrix[m-o-1][k]) for k in range(n-1, o-1, -1) if matrix[m-o-1][k] not in res] [res.append(matrix[l][o]) for l in range(m-1-o, o-1,-1) if matrix[l][o] not in res] # 这种写法还是少用，可以使用append,就少用这种，可读性不强，容易错 return res 最短摘要生成思路：两个指针begin和end, 首先将end 指针向后移动至包含所有的字符集q的位置；然后将begin向后移动直至再移动一次将不再包含q，记录该位置长度。然后将begin向后移动一位，end继续向后移动直至包含所有q，周而复始。比较位置长度，然后得出最短距离。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879#include &quot;stdio.h&quot;#include &quot;string.h&quot;#include &quot;assert.h&quot;#define MAX 1024int isMatchAll(const char *str,const char *key,int begin,int end)&#123; int ret =0; char hash[256]; int i =0; int lenK = strlen(key); memset(hash,0,sizeof(hash)); for(i=begin;i&lt;=end;i++) &#123; hash[str[i]]=1; &#125; // 标记string之后，然后再遍历key中的字符是否存在 for(i=0;i&lt;lenK;i++) &#123; if(hash[key[i]]==0) break; &#125; if(i == lenK ) ret =1; return ret;&#125;void find(const char *str,const char *key)&#123; int lenS = strlen(str); int lenK = strlen(key); int begin = 0; int end = 0; int minLength = 0x7FFFFFFF; int mstart = 0; int mend =0; assert(str&amp;&amp;key); for(;;) &#123; while(!isMatchAll(str,key,begin,end)&amp;&amp;end&lt;lenS) &#123; end++; &#125; while(isMatchAll(str,key,begin,end)) &#123; if(end-begin+1 &lt;minLength) &#123; minLength = end-begin+1; mstart =begin; mend =end; &#125; begin++; &#125; if(end&gt;=lenS) break; &#125; printf(&quot;%d\n&quot;,minLength); for(;mstart&lt;=mend;mstart++) printf(&quot;%c&quot;,str[mstart]);&#125; int main()&#123;/** char *str =&quot;hello are you bottom of do the is bot doke astring&quot;; char *key =&quot;abde&quot; */ char str[MAX]; char key[MAX]; gets(str); gets(key); find(str,key); return 0;&#125;]]></content>
      <categories>
        <category>算法</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Loss-Activation-and-Optimisation-Function]]></title>
    <url>%2F2018%2F07%2F07%2FLoss-Activation-and-Optimisation-Function%2F</url>
    <content type="text"><![CDATA[用图说话… Loss Function(Error Function)For accurate predictions, one needs to minimize the calculated error. In a neural network, this is done using back propagation.损失函数是用来估量你模型的预测值f(x)与真实值Y的不一致程度，它是一个非负实值函数,通常使用L(Y,f(x))来表示，损失函数越小，模型的鲁棒性就越好。按照函数种类可以划分一下主要的几个类别。 log损失函数常用于逻辑回归中。 平方损失函数又称为最小二乘法，最优拟合直线应该是使各点到回归直线的距离和最小的直线，即平方和最小。Y-f(X) 表示残差，残差在数理统计中是指实际观察值与估计值（拟合值）之间的差。整个式子表示的是残差的平方和。 指数损失函数Adaboost的目标式子就是指数损失，在给定n个样本的情况下，Adaboost的损失函数为： Hinge损失函数听着名字怪怪的，但是SVM（support vector machine）的损失函数就是这个。从目标函数看来，lr 采用的logistic loss 和 svm 采用的 hinge loss function 思想都是增加对分类影响较大点的权重，减少那些与分类相关不大点的权重。但是两个方法处理的方法不同: LR采用一个sigmod的映射函数，通过这样的非线性映射，大大降低了离分类平面点远的权重 ; SVM 采用的是一个hinge loss function，通过上图可以看到，对于那些离超平面比较远的点，直接设为0了，也就是说直接忽视，只考虑那些对分类平面有影响的点，这些点就是我们经常听到的支持向量。另一方面，支持向量机只考虑局部的边界线附近的点，而逻辑回归考虑全局（远离的点对边界线的确定也起作用，虽然作用会相对小一些）。 按照应用领域可以划分为:Regressive loss functions:They are used in case of regressive problems, that is when the target variable is continuous. Most widely used regressive loss function is Mean Square Error.Other loss functions are:Absolute error — measures the mean absolute value of the element-wise difference between input.Classification loss functions:The output variable in classification problem is usually a probability value f(x), called the score for the input x. Generally, the magnitude of the score represents the confidence of our prediction. The target variable y, is a binary variable, 1 for true and -1 for false.On an example (x,y), the margin is defined as yf(x). The margin is a measure of how correct we are. Most classification losses mainly aim to maximize the margin. Some classification algorithms are: Binary Cross Entropy,Negative Log Likelihood,Margin Classifier and Soft Margin Classifier.Embedding loss functions:It deals with problems where we have to measure whether two inputs are similar or dissimilar. Some examples are: L1 Hinge Error- Calculates the L1 distance between two inputs. Cosine Error- Cosine distance between two inputs. Visualising Loss Functions:We performed the task to reconstruct an image using a type of neural network called Autoencoders. Different results were obtained for the same task by using different Loss Functions, while everything else in the neural network architecture remained constant. Thus, the difference in result represents the properties of the different loss functions employed. A very simple data set, MNIST data set was used for this purpose. Three loss functions were used to reconstruct images. Activation FunctionWhat?It’s just a thing (node) that you add to the output end of any neural network. It is also known as Transfer Function. It can also be attached in between two Neural Networks. Sigmoid or Logistic Activation Function导数比较有特点： Softmax FunctionThe softmax function is a more generalized logistic activation function which is used for multiclass classification.使用softmax和多个logistic的多分类的区别：softmax回归进行的多分类，类与类之间是互斥的，即一个输入只能被归为一类；多个logistic回归进行多分类，输出的类别并不是互斥的，即”苹果”这个词语既属于”水果”类也属于”3C”类别。 Tanh ReLUReLU (Rectified Linear Unit) Activation Functionit is used in almost all the convolutional neural networks or deep learning.在卷积网络和深度网络中经常看到。 Leaky ReLU明显的发现，Leaky ReLU是对于在 X&lt;0时候的改进。Both Leaky and Randomized ReLU functions are monotonic in nature. Also, their derivatives also monotonic in nature. Optimisation AlgorithmsOptimisation Algoritms are used to update weights and biases i.e. the internal parameters of a model to reduce the error. They can be divided into two categories:Back Propogation and Optimisation Function: Error J(w) is a function of internal parameters of model i.e weights and bias. For accurate predictions, one needs to minimize the calculated error. In a neural network, this is done using back propagation. The current error is typically propagated backwards to a previous layer, where it is used to modify the weights and bias in such a way that the error is minimized. The weights are modified using a function called Optimization Function.Optimisation functions usually calculate the gradient i.e. the partial derivative of loss function with respect to weights, and the weights are modified in the opposite direction of the calculated gradient. This cycle is repeated until we reach the minima of loss function. Constant Learning Rate AlgorithmsHere η is called as learning rate which is a hyperparameter that has to be tuned. Choosing a proper learning rate can be difficult.选的小训练速度慢， While a learning rate that is too large can hinder convergence and cause the loss function to fluctuate around the minimum or even to divergeA similar hyperparameter is momentum(动量), which determines the velocity with which learning rate has to be increased as we approach the minima. Adaptive Learning Algorithms(自适应)Adaptive gradient descent algorithms such as Adagrad, Adadelta, RMSprop, Adam, provide an alternative to classical SGD. They have per-paramter learning rate methods, which provide heuristic approach without requiring expensive work in tuning hyperparameters for the learning rate schedule manually. The challenge of using gradient descent is that their hyper parameters have to be defined in advance and they depend heavily on the type of model and problem. Another problem is that the same learning rate is applied to all parameter updates. If we have sparse data, we may want to update the parameters in different extent instead.(假装翻译:gradient descent 的艰难之处在于需要提前 define这种hyper parameters, 并且不同数据的learning rate 应该是不同（学习率应该根据数据的稀疏与否变化)) 说到这我们就多说一些关于自适应算法的内容。We used three first order optimisation functions and studied their effect-Stochastic Gradient Decent, Adagrad and Adam. Gradient Descent calcultes gradient for the whole dataset and updates values in direction opposite to the gradients until we find a local minima. Stochastic Gradient Descent performs a parameter update for each training example unlike normal Gradient Descent which performs only one update. Thus it is much faster. Gradient Decent algorithms can further be improved by tuning important parametes like momentum, learning rate etc. Adagrad is more preferrable for a sparse data set as it makes big updates for infrequent parameters and small updates for frequent parameters. It uses a different learning Rate for every parameter θ at a time step based on the past gradients which were computed for that parameter. Thus we do not need to manually tune the learning rate. Adam stands for Adaptive Moment Estimation. It also calculates different learning rate. Adam works well in practice, is faster, and outperforms other techniques. Stochastic Gradient Decent was much faster than the other algorithms but the results produced were far from optimum. Both, Adagrad and Adam produced better results that SGD, but they were computationally extensive. Adam was slightly faster than Adagrad. Thus, while using a particular optimization function, one has to make a trade off between more computation power and more optimum results. SGD 和 GDD的区别：正如上所说，在∂E/∂wi=∑（h(x)-y）(xi) 的时候∑耗费了大量的时间，特别是在训练集庞大的时候。所以肯定有人会猜想，如果把求和去掉如何，即变为∂E/∂wi=（h(x)-y）(xi)。（只是专注于当前训练训练集合，当前的数据）对于步长η的取值，标准梯度下降的η比随机梯度下降的大。因为标准梯度下降的是使用准确的梯度，理直气壮地走，随机梯度下降使用的是近似的梯度，就得小心翼翼地走，怕一不小心误入歧途南辕北辙了。 事实证明：中文和英文混在一起，排版是难看的。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>优化</tag>
        <tag>Loss Function</tag>
        <tag>Activation Function</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[特征工程相关概念]]></title>
    <url>%2F2018%2F06%2F29%2F%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[特征离散化？连续化特征就是一些不可枚举的有理数。那么什么是离散化特征呢？ 离散化特征就是可枚举的特征。离散化的作用是把数据变成可计算状态。而特征工程就是从原始字段中根据业务提取出对模型有效的特征出来。 在线性模型下(w.x)，w已经确定的情况下，x的某个特征的值是20，或者30，w.x的值相差是很大的，但是往往20岁的人跟30岁的人对同一个广告的兴趣差距不会那么大。连续特征离散化的基本假设，是默认连续特征不同区间的取值对结果的贡献是不一样的。离散化和连续化最大的区别是，对一个字段做连续化后的结果就还只是一个特征，而离散化后的这一列有多少个key(字段可能的值)就会抽取出多少个特征。当经过离散化之后，特征各有各的权重，彼此之间就没有关系了。 模型是使用离散特征还是连续特征,其实是一个“海量离散特征+简单模型” 同 “少量连续特征+复杂模型”的权衡。既可以离散化用线性模型，也可以用连续特征加深度学习。 常用的选取离散点的方法：等距离离散，等样本离散、画图观察趋势和决策树模型(天生就可以对连续特征分段)。 在工业界，很少直接将连续值作为特征喂给逻辑回归模型，而是将连续特征离散化为一系列0、1特征交给逻辑回归模型，这样做的优势有以下几点： 单变量离散化为N个后，每个变量有单独的权重，在激活函数的作用下相当于为模型增加了非线性，能够提升模型表达能力，加大拟合。 离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄&gt;30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰，因为特征值的异常会导致权重也就是w的值也会异常。 一定有同学担心特征过多会导致运算缓慢，但是LR是线性模型，我们在内部计算的时候是向量化计算，而不是循环迭代。稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展。 所以不用担心像GBDT算法那样，特征多了就跑不动了(我们都说GBDT不能用离散特征不是因为它处理不了离散特征，而是因为离散化特征后会产生特别多的特征，决策树的叶子节点过多，遍历的时候太慢了)。 所以海量离散特征＋LR是业内常见的一个做法。而少量连续特征+复杂模型是另外一种做法，例如GBDT。 当然也可以通过深度学习来进行特征选择：目前这种手段正在随着深度学习的流行而成为一种手段，尤其是在计算机视觉领域，原因是深度学习具有自动学习特征的能力，这也是深度学习又叫unsupervised feature learning的原因。从深度学习模型中选择某一神经层的特征后就可以用来进行最终目标模型的训练了。 参考文献:https://blog.csdn.net/lujiandong1/article/details/52412123 组合特征先是离散化，然后是特征组合。交叉从理论上而言是为了引入特征之间的交互，也即为了引入非线性。LR(逻辑回归）分类算法:因为线性函数的表达能力有限，所以我们引入激活函数就是给LR增加非线性关系。能让一条直线变成曲线。这样可以拟合出更好的效果。（也由此才后了后来说的过拟合问题而引入了正则化超参数）LR模型之所以很受欢迎，主要是因为LR模型本质是对数线性模型，实现简单，易于并行，大规模扩展方便，迭代速度快，同时使用的特征比较好解释，预测输出在0与1之间契合概率模型。（模型的可解释性举例，比如A-B的权重比较大，A代表用户，B代表物品，那么可以认为A是对B比较感兴趣的）但是，线性模型对于非线性关系缺乏准确刻画，特征组合正好可以加入非线性表达，增强模型的表达能力。另外，广告LR中，基本特征可以认为是用于全局建模，组合特征更加精细，是个性化建模，因为在这种大规模离散LR中，单对全局建模会对部分用户有偏，对每一用户建模又数据不足易过拟合同时带来模型数量爆炸，所以基本特征+组合特征兼顾了全局和个性化。 从统计的角度解释，基本特征仅仅是真实特征分布在低维的映射，不足以描述真实的分布，加入特征在高维空间拟合真实分布，使得预测更加准确。 正则化真正测试一个模型的不是简单与否，更重要在于它在预测新的情况时表现如何。小权重意味着网络的行为不会因为我们随意更改了一些输入而改变太多。这是我们加了正则化之后的成本函数，可以看我们后面加入了正则化 λ 的表达式来完善成本函数。为什么加入λ能够减轻过拟合呢？直观一点的解释是设置的λ值越大，那么参数w的值就会被压缩的越小(在梯度下降中, 每次迭代的步长，也就是这个公式w=w - 学习率*成本函数对w的导数， 现在由于成本函数增加了正则项，使得J和w变得数值相关了)。 假设λ设置的足够大，那么w会无限的趋近于0. 把多隐藏层的单元的权重设置为0以后，那么基本上就是消除掉了这些单元的作用，而使得网络模型得到简化，就像下面的图一样。由于正则化的设置，消除了一些隐藏单元的作用。而使得整个模型越来越接近于线性化，也就是从下图中的过拟合往欠拟合偏转。当然我们有一个适合的λ的值，能让我们的拟合状态达到最佳。所以我们在训练模型的时候，往往都会有一个Ｌ２正则项的超参数需要我们设置。这是我们的tanh激活函数， 可以看到当z的值越大时，整个函数的非线性就越大，而z的值越小(图中红色加粗部分),函数就越是呈现出线性分布。 所以当我们增加λ的值， w得值就越小，相应的z的值也就越小。因为z = wx + b。 而我们第一次说激活函数的时候就说过神经网络中基本上是不使用线性函数作为激活函数的，因为不论有多少层，多少个单元，线性激活函数会使得所有单元所计算的都呈现线性状态。 杂货铺特征工程可以分为特征处理、Feature Selection（特征选择）、Feature Extraction（特征提取）和Feature construction（特征构造）等阶段。归一化（去中心，方差归一）是属于特征(预)处理:把特征值压缩成0~1的区间。 One-hot（也叫One-of-k）的方法把每个无序特征转化为一个数值向量。比如一个无序特征color有三种取值：red，green，blue。那么可以用一个长度为3的向量来表示它，向量中的各个值分别对应于red，green，blue。如：这种方法在NLP里用的很多，就是所谓的词向量模型。变换后的向量长度对于词典长度，每个词对应于向量中的一个元素。 对于欠拟合: 增加神经网络复杂度，出现欠拟合的原因之一是由于函数的非线性不足，所以用更复杂的网络模型进行训练来加深拟合。对于过拟合：增加数据规模， 出现过拟合的原因之一是数据规模不足而造成的数据分布不均，扩展数据规模能比较好的解决这个问题。当然另一个做法是正则化，我们采取使用正则化来解决过拟合问题，常用的是L2正则，其他的还有L1和 Dropout正则。 很多人会想着既然线性分类器搞不定，那就直接找个非线性的好了，比如高斯核的SVM。我们确实可以通过这种简单换算法的方式解决这个简单的问题。但对于很多实际问题（如广告点击率预测），往往特征非常多，这时候时间约束通常不允许我们使用很复杂的非线性分类器。这也是为什么算法发展这么多年，广告点击率预测最常用的方法还是Logistic Regression (LogisticReg)。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>特征工程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常见的排序算法总结]]></title>
    <url>%2F2018%2F06%2F29%2F%E5%B8%B8%E8%A7%81%E7%9A%84%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[分类和总结 根据待排序的数据大小不同，使得排序过程中所涉及的存储器不同，可分为内部排序和外部排序。 排序关键字可能出现重复，根据重复关键字的排序情况可分为稳定排序和不稳定排序。 对于内部排序，依据不同的排序原则，可分为插入排序、交换(快速)排序、选择排序、归并排序和计数排序。 针对内部排序所需的工作量划分，可分为:简单排序 O(n^2)、先进排序 O(nlogn)和基数排序 O(d*n)。常见算法的性质总结： 排序算法实现默认都是升序… 插入排序(Insert Sort)123456789101112131415def insert_sort(lists): count = len(lists) for i in range(1, count): key =lists[i] j =i-1 while j &gt;= 0: if lists[j] &gt;key: lists[j+1] =lists[j] lists[j] =key j -= 1 return lists# testlists =[1,2,-3, 90,34]print(insert_sort(lists)) 选择排序(Select Sort)为每个位置选择当前元素最小的。选择最小的元素和a[0]交换，选择次最小的和a[1]交换，以此类推。代码实现:12345678910111213def select_sort(lists): count =len(lists) for i in range(0, count): min =i for j in range(i+1, count): if lists[min] &gt;lists[j]: min =j lists[min], lists[i] =lists[i], lists[min] return lists# testlists =[1, 23,45, 0,-1]print(select_sort(lists)) 冒泡排序(Bubble Sort)重复遍历要排序的数列，一次比较两个元素，如果顺序错误就交换位置。1234567891011def bubble_sort(lists): count =len(lists) for i in range(0, count): for j in range(i+1, count): if lists[i]&gt; lists[j]: lists[i], lists[j] =lists[j], lists[i] return lists# testlists =[1,34,45,0,89]print(bubble_sort(lists)) 归并排序(Merge Sort)归并排序，应该是我第一个接触的排序算法。然后在在某次重要面试时候，使用该算法救急。该算法是采用采用分治法(Divide and Conquer)的思想:先使得子序列有序，然后合并子序列。1234567891011121314151617181920212223242526def merge(left, right): i, j =0,0 result =[] while i&lt;len(left) and j &lt;len(right): if left[i] &lt;= right[j]: result.append(left[i]) i +=1 else: result.append(right[j]) j +=1 result += left[i:] result += right[j:] return resultdef merge_sort(lists): if len(lists) &lt;=1: return lists num =int(len(lists)/2) left =merge_sort(lists[:num]) right = merge_sort(lists[num:]) return merge(left, right)# merge_sort 是先切分，然后再整合，quick sort 是两个指针# testlists =[1, 34, 23,45,0,9]print(merge_sort(lists)) 快速排序(Quick Sort)快速排序的思想：任意选择一个key(通常选择a[0])，将比他小的数据放在它的前面，比他大的数字放在它的后面。递归进行。12345678910111213141516171819202122232425def quick_sort(lists, left, right): if left &gt;= right: return lists key =lists[left] low =left high =right while left &lt; right: # 因为你最初key 取得是left，然后从右边找到一个比key小的，然后替换left 的位置 while left &lt;right and lists[right]&gt;=key: right -= 1 lists[left] =lists[right] while left &lt; right and lists[left] &lt;= key: left +=1 lists[right] =lists[left] lists[left] =key # 这里的left 和right 都是可以的，因为从while 中出来之后两者是相同的 # 这个步伐是1,所以只能是一个个变化 quick_sort(lists, low, left-1) quick_sort(lists, left+1, high) # 因为left的位置已经被占了，所以只是划分左边一块，右边一块就是可以的 return lists# testlists =[3,2,45, 100,1,56,56]#lists =[1,2,3,2,2,2,5,4,2]print(quick_sort(lists, 0, len(lists)-1)) 堆排序参看另一篇博客中堆排序。 参考文献算法动图效果排序算法分类]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>排序算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[那些年的算法题目（一）]]></title>
    <url>%2F2018%2F06%2F22%2F%E9%82%A3%E4%BA%9B%E5%B9%B4%E7%9A%84%E7%AE%97%E6%B3%95%E9%A2%98%E7%9B%AE%EF%BC%88%E4%B8%80%EF%BC%89%2F</url>
    <content type="text"><![CDATA[交作业了… 完全二叉树插入问题描述已知一个完全二叉树的结构，现在需要将一个节点插入到这颗完全二叉树的最后，使得它还是一个完全二叉树。第一种解法：如果该树为满二叉树或者左子树不为满二叉树，那么就进入左子树，否则进入右子树，递归进行。 二叉树(Binary Tree)强行补充一下关于二叉树概念的知识。完全二叉树(Complete Binary Tree):若设二叉树的深度为h，除第h层外，其它各层(1～h-1)的结点数都达到最大个数，第h层所有的结点都连续集中在最左边，这就是完全二叉树。满二叉树:树中除了叶子节点，每个节点都有两个子节点。满二叉树是一种特殊的完全二叉树。二叉搜索树(binary search tree):所有非叶子结点至多拥有两个儿子（Left和Right）；所有结点存储一个关键字,非叶子结点的左指针指向小于其关键字的子树，右指针指向大于其关键字的子树。平衡二叉树(AVL树)：它是一颗空树或它的左右两个子树的高度差的绝对值不超过1。哈夫曼树：带权路径长度达到最小的二叉树，也叫做最优二叉树。树的深度和高度：深度是从上往下数；高度是从下往上数 代码实现平滑过渡到本问题的代码实现。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152#include&lt;iostream&gt;using namespace std;typedef struct Node&#123; int value; struct Node *lchild, *rchild;&#125;Tree;int GetLeftDepth(Tree* root)&#123; Tree* pNode =root-&gt;lchild ; int depth =0; while(pNode != NULL) &#123; depth ++; pNode =pNode-&gt;lchild; &#125; return depth;&#125;int GetRightDepth(Tree* root)&#123; Tree* pNode =root-&gt;rchild; int depth =0 ; while(pNode != NULL) &#123; depth ++ ; pNode =pNode-&gt;rchild ; &#125; return depth;&#125;bool IsFullBinaryTree(Tree* root)&#123; return GetLeftDepth(root) == GetRightDepth(root) ;&#125;void insert(Tree* root, Tree * node)&#123; if (IsFullBinaryTree(root) || !IsFullBinaryTree(root-&gt;lchild))&#123; insert(root-&gt;lchild, node); return ; &#125; if (root-&gt;rchild ==NULL)&#123; root-&gt;rchild =node ; return ; &#125; insert(root-&gt;rchild, node) ;&#125;int main()&#123; Node* a = new Node(); a-&gt;value =1;&#125; 第二种思路，如果已知之前树的个数，那么可以使用前序遍历的方式，得到将要插入的的节点的位置，然后插入。123456789101112int insert(Tree *t, int n, struct Node *node);# n表示原来二叉树节点的个数# 前序遍历 void printTree(Tree* root)&#123; if(root ==NULL) &#123; return ; &#125; print(root-&gt;value) printTree(root-&gt;lchild) printTree(root-&gt;rchild)&#125; 第三种思路：因为是完全二叉树，那么插入的点只能在最下一层。于是我们可以去找最下一层的中间点，找到根的左子树的最右下的节点，如果这个点存在，那么说明，最下一层的左边已经填满，递归右子树，否则递归左子树。 参考文献http://www.voidcn.com/article/p-kbxsvnyq-yq.htmlhttps://www.toutiao.com/i6192546626911126017/https://blog.csdn.net/psc0606/article/details/48742239 inplace 去除连续的 0给定一个一维整数数组，不使用额外的空间，本地去掉数组中连续的0。123456789101112131415161718192021222324252627282930#include&lt;iostream&gt;using namespace std;int RemoveDuplicates(int* sortBuffer,int length)&#123; if(sortBuffer == NULL || length == 0) &#123; return false; &#125; int count = 0; for(int i = 1; i &lt; length; i++) &#123; if(sortBuffer[i] ==0 &amp;&amp; 0 == sortBuffer[i-1]) &#123; continue; &#125; else &#123; sortBuffer[count]=sortBuffer[i]; count++; &#125; &#125; return count; &#125;int main()&#123; int length =sizeof(array)/sizeof(int); &#125; 最大连续子数组和已知一个整数二维数组，求最大的子数组和(子数组的定义从左上角(x0,y0) 到右下角(x1,y1)的数组)先考虑一维整数数组的情况。最大连续子序列的DP动态转移方程为 1234567891011121314151617181920212223#include&lt;iostream&gt;using namespace std;int Max(int a, int b)&#123; return a&gt;b ?a:b;&#125;int FindGreatestSubarray(int *arr, int n)&#123; int sum =arr[0]; int max =arr[0]; for(int i =1; i&lt;n; i++)&#123; sum =Max(sum+arr[i], arr[i]); max =Max(sum, max) /** if(sum &gt;=max)&#123; max =sum; &#125; */ &#125; return max;&#125;int main()&#123; return 0;&#125; 本题目的要求是从二位的数组中求解最大的子矩阵。我们可以将其转化成一维数组的问题。如果是二维数组可以压缩为一维数组（我当时也是不懂这里）。如果最大子矩阵和原矩阵等高，就可以这样压缩。12345678910111213141516171819202122232425262728293031323334353637383940414243#include&lt;stdio.h&gt;#include&lt;iostream&gt;#include&lt;vector&gt;using namespace std;#define inf 0x3f3f3f3fint Max(int a, int b)&#123; return a&gt;b? a:b;&#125;// 求解一维数组的最大连续子数列int FindGreatestSubarray(int *arr, int n)&#123; int sum =arr[0]; int max =arr[0]; for(int i =1;i&lt;n;i++)&#123; sum =Max(sum+arr[i], arr[i]) if(sum &gt;=max)&#123; max =sum; &#125; &#125; return max;&#125;int GreatestMatrix(int[][] arr, int rows, int cols)&#123; int maxVal =- inf for(int i =0 ; i &lt;rows; i++)&#123; vector&lt;int&gt; temp(arr[i]); maxVal =Max(maxVal, FindGreatestSubarray(temp)); // 得到第一行的最大和 // 将行的n个元素加到上一行，然后计算最大和 for(int j =i+1; j&lt;rows; j++)&#123; for(int k =0;k&lt;cols ;k++)&#123; temp[k] =arr[j][k]; &#125; // 依次0~k行的最大和 maxVal =Max(maxVal, FindGreatestSubarray(temp)) &#125; &#125;&#125;int main()&#123;&#125; 聚类(clustering )聚类是一种无监督学习(Unsupervised Learning)，该算法基于数据内部的特征寻找样本的自然族群(集群)。通常使用数据可视化来评价结果。应用场景：新闻聚类，文章推荐，细分客户。 最常见的聚类算法就是K均值(K-Means)：以空间中k个点为中心进行聚类，对最靠近他们的对象归类，通过迭代的方法，逐次更新各聚类中心的值，直到得到最好的聚类结果。最后希望达到的目的：聚类中的对象相似度较高，聚类之间的相似度比较低。 相似度计算：不同的算法需要的”相似度”是不一样的，对于空间中的点，我们一般选取欧式距离来进行衡量，认为距离越近，数据之间越相似。 堆排序堆是一种完全二叉树，堆排序是一种树形选择排序，其时间复杂度为O(nlogn)，空间复杂度:对于记录较少的文件不推荐使用，对于较大的文件还是有效的.堆分为大根堆和小根堆。大根堆的要求是每个节点的值都不大于其父节点的值，即A[PARENT[i]] &gt;= A[i]。小根堆的要求是每个节点的值都不小于其父节点的值，即A[PARENT[i]] &lt;= A[i]。 堆栈是计算机的两种最基本的数据结构。堆的特点就是FIFO(first in first out)先进先出，栈的特性正好与堆相反，是属于FILO(first in/last out)先进后出的类型。堆的每次调整交换堆顶和最后一个元素，然后只是调整堆顶和堆顶的左右孩子树的关系。 1234567891011121314151617181920212223242526272829303132333435# heap modifydef MAX_Heapify(heap, HeapSize, root): left =2* root+1 right = left +1 larger =root if left &lt;HeapSize and heap[larger] &lt;heap[left]: larger =left if right &lt; HeapSize and heap[larger] &lt;heap[right]: larger =right # if modify the larger then exchange it if larger != root: heap[larger], heap[root] =heap[root], heap[larger] MAX_Heapify(heap, HeapSize, larger)# Build the heapdef Build_MAX_Heap(heap): HeapSize =len(heap) # from the end to the begin for i in range((HeapSize -2)//2, -1,-1): MAX_Heapify(heap, HeapSize, i)# sort after building the heapdef HeapSort(heap): Build_MAX_Heap(heap) for i in range(len(heap)-1, -1, -1): heap[0], heap[i] =heap[i], heap[0] MAX_Heapify(heap, i, 0) return heapif __name__ ==&quot;__main__&quot;: a =[30, 50, 57, 77, 62, 78, 94, 80, 84] print(a) print(&quot;without sort but with build heap&quot;) Build_MAX_Heap(a) print(a) 参考文献https://blog.csdn.net/minxihou/article/details/51850001https://blog.csdn.net/chibangyuxun/article/details/53018294 KMP(字符串高效查找)假设两个字符串的长度分别是m，n(m&gt;n)，在长度为m中的字符串查找长度为n的字符串，最常见的方式是暴力求解，但是这个常规解法的时间复杂度是O(nm)。KMP通过一个O(n)的预处理，可以使得时间复杂度降为O(n+m).代码实现(视频讲解(科学上网))123456789101112131415161718192021222324def kmp_match(s, p): m, n =len(s) ,len(p) cur =0 table = partial_table(p) while cur &lt;= m-n: for i in range(n): if s[i+cur] != p[i]: cur += max(i -table[i-1], 1) break else: return True return Falsedef partial_table(p): prefix =set() postfix =set() ret =[0] for i in range(1, len(p)): prefix.add(p[:i]) postfix =&#123; p[j:i+1] for j in range(1, i+1)&#125; ret.append(len((prefix &amp; postfix or &#123;&apos;&apos;&#125;).pop())) # &amp;两个set求交集 return retprint(partial_table(&apos;ABCDABD&apos;))print(kmp_match(&quot;BBC ABCDAB ABCDABCDABDE&quot;, &quot;ABCDABD&quot;)) 参考文献https://www.cnblogs.com/fanguangdexiaoyuer/p/8270332.html 二叉树的遍历在python中二叉树的结构:12345class BinNode(): def __init__(self, val): self.value =val self.lchild =None self.rchild =None 先序遍历(preOrder)第一种思路是递归实现，第二种思路借助栈的结构来实现。栈的大小空间为O(h)，h为二叉树高度；时间复杂度为O(n)，n是树的节点的个数。123456789101112131415161718192021# 递归def preOrder(self, root): if root == None: return print(root.val) self.preOrder(root.lchild) self.preOrder(root.rchild)# 借助栈结构def preOrder(self, root): if root == None: return myStack =[] node =root while node or myStack: while node: print(node.val) myStack.append(node) node =node.lchild node =myStack.pop() node =node.rchild 中序遍历(inOrder)递归和非递归两种实现思路。入栈的顺序是一样的，只是改变的遍历(print())的顺序.123456789101112131415161718192021# 递归def inOrder(self, root): if root ==None: return self.inOrder(root.lchild) print(root.val) self.inOrder(root.rchild)# 借助栈结构def inOrder(self, root): if root ==None: return myStack =[] node =root while node or myStack: while node: myStack.append(node) node =node.lchild node = myStack.pop() print(node.val) node =node.rchild 后序遍历(post order)仍然是递归和非递归版本，非递归中使用两个stack,两个stack的后进先出等于一个先进先出。12345678910111213141516171819202122232425# 递归def postOrder(self, root): if root == None: return self.postOrder(root.lchild) self.postOrder(root.rchild) print(root.val)# 借助栈结构def postOrder(self, root): if root ==None: return myStack1 =[] myStack2 =[] node =root myStack1.append(node) while myStack1: node =myStack1.pop() if node.lchild: myStack1.append(node.lchild) if node.rchild: myStack1.append(node.rchild) myStack2.append(node) while myStack2: print(myStack2.pop().val) 层序遍历使用到了队列的思想，先进先出。实际上，用的是Python中list.pop(0).注意默认是list.pop(-1),也就是默认弹出的是最后一个元素。1234567891011121314def levelOrder(self, root): if root ==None: return myQueue =[] node =root myQueue.append(node) while myQueue: # remove and return item at index (default last) node =myQueue.pop(0) print(node.val) if node.lchild != None: myQueue.append(node.lchild) if node.rchild != None: myQueue.append(node.rchild) 参考文献https://blog.yangx.site/2016/07/22/Python-binary-tree-traverse/ 旋转数组找最小(大)值把一个数组最开始的若干个元素搬到数组的末尾，我们称之为数组的旋转。 输入一个非递减排序的数组的一个旋转，输出旋转数组的最小元素。 例如数组{3,4,5,1,2}为{1,2,3,4,5}的一个旋转，该数组的最小值为1。 NOTE：给出的所有元素都大于0，若数组大小为0，请返回0。 暴力求解大多数的问题都是可以暴力求解的–鲁迅。(In China 凡是不知道是谁说，都可以说是鲁迅说的; In US，凡是不知道谁说的，as said by Albert Einstein)因为原来的数组假设是增序，所以如果出现了的某一个元素比上一个元素小，该元素就是这个序列中的最小值。(这个情况具有唯一性吧).时间复杂度O(N)123456789101112def minNumberInRotateArray(rotateArray): arr =rotateArray # just because of laziness if not arr: return 0 if len(arr) ==2: return arr[1] num =arr[0] for i in range(1, len(arr)): if arr[i] &gt;= num: num =arr[i] else: return arr[i] 递归版本旋转数组也是一种有序数组，时间复杂度O(N)…，面试官说改进吧…使用二分法降到O(logN)。12345678910111213def minNumberInRotateArray(rotateArray): arr = rotateArray if not arr: return 0 if len(arr) ==2: return arr[1] mid =int(len(arr) /2) if arr[mid] &gt; arr[0]: return minNumberInRotateArray(arr[mid:]) elif arr[mid] &lt;arr[0]: return minNumberInRotateArray(arr[:mid+1]) else: return minNumberInRotateArray(arr[1:]) 非递归版本递归版本占的内存比较多，改进吧..于是非递归的版本就出来了。需要注意的是该版本的判断比较条件(其中一点是和 arr[right]进行比较)一定要小心，都是小坑…12345678910111213def minNumberInRotateArray(rotateArray): arr =rotateArray left =0 right =len(arr) -1 while left &lt; right: mid = int((left+ right)/2) if arr[mid] &gt;arr[right]: left =mid+1 elif arr[mid] &lt;arr[right]: right =mid else: right -=1 return arr[left] 文章的小标题是求解最小(大)值，上述讲述的都是最小值。如果求解最大值，稍微修改一下特殊情况的判断条件，将返回的index-1 即可。因为最小值的位置是”某一个元素比上一个元素小”，那么 index-1 之后这个元素就是该数组序列中最大的。 参考文献https://blog.csdn.net/u010005281/article/details/79823154 单链表反转单链表的反转有循环迭代和递归两种方法。单链表节点123456class Node(object): def __init__(self): self.value =None self.next =None def __str__(self): return str(self.value) 循环迭代循环迭代需要维持三个变量：pre, head, next。pre是head的pre，next是head的next.(废话)1234567891011def reverse_Linkedlist(head): if not head or not head.next : return head pre =None while head: next = head.next head.next =pre pre = head head =next # 最后一次循环迭代 Head==None，而pre指向了头结点 return pre 递归一开始正常情况下不会执行if判断，利用递归走到链表的末端，new_head的值没有发生改变，为链表的最后一个节点，反转之后就成为了新链表的head。12345678def reverse_Linkedlist(head): if not head or not head.next: return head new_head = reverse_Linkedlist(head.next) # 将当前节点设置为后面节点的后续节点 head.next.next =head head.next =None return new_head 测试12345678910111213141516171819202122232425if __name__ == &quot;__main__&quot;: three = Node() three.value =3 two =Node() two.value =2 two.next =three one =Node() one.value =1 one.next =two head =Node() head.value =0 head.next =one &quot;&quot;&quot; while head: print(head.value, ) head =head.next print(&quot;******&quot;) &quot;&quot;&quot; newhead = reverse_Linkedlist(head) while newhead: print(newhead.value) newhead =newhead.next 参考文献https://foofish.net/linklist-reverse.html]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>递归</tag>
        <tag>迭代</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Introduction_to_Ensemble]]></title>
    <url>%2F2018%2F06%2F13%2FIntroduction-to-Ensemble%2F</url>
    <content type="text"><![CDATA[虽然在Titanic Challenge博客中介绍了模型融合(Ensemble)，但是其是以特征提取为主旋律，所以这篇博客将结合实战从最基本的Ensemble到比较复杂的Ensemble,最后安利一个用于模型融合的package. 在文中可能出现一些概念，如果不太熟悉，可以先去文末找找看，如果我没有补充，那么请自行Google吧。为了不影响阅读的流畅性，我可能在其中不会涉及很多概念解释，比如说ROC曲线和AUC值。好，我们进入正文。 Dataset本文书写过程中沿用参考博客(Introduction to Python Ensembles)的数据集。可以去这里下载，当然推荐使用原作者处理之后的数据集，you can find here。 简单介绍一下这个数据集：Federal Election Commission这个组织收集了2007到2016年的donations记录，最后得出 When Scientists Donate To Politicians, It’s Usually To Democrats这样的结论。 好了，我不想在数据集这里花太多时间，即使你不太明白数据集的具体含义，完全不影响下文的阅读，因为你很快就会发现下文并没有进行很多和原数据集相关的内容，更多的是模型融合。当然你如果能够看懂，可以感受一下上述结论的有趣之处。 Give me codes:1234567891011121314151617181920import numpy as npimport pandas as pdimport matplotlib.pyplot as plt%matplotlib inline# import data# Always good to set a seed for reproducibilitySEED = 222np.random.seed(SEED)df = pd.read_csv(&apos;input.csv&apos;)from sklearn.model_selection import train_test_splitfrom sklearn.metrics import roc_auc_scoredef get_train_test(test_size= 0.95): y =1*(df.cand_pty_affiliation ==&apos;REP&apos;) X = df.drop([&quot;cand_pty_affiliation&quot;], axis=1) X = pd.get_dummies(X, sparse=True) X.drop(X.columns[X.std() == 0], axis=1, inplace=True) return train_test_split(X, y, test_size=test_size, random_state=SEED)xtrain, xtest, ytrain, ytest = get_train_test()df.head() 简单看一下数据长什么样子，虽然有人可能不太懂。 Begin with ensemble之前的博客主要从ensemble分类的角度阐述，现在从概念的角度阐述。Ensemble: combining predictions from several models averages out idiosyncratic(怪异的) errors and yield better overall predictions.(有时候我觉得英文说得很清楚，所以就不翻译成中文了，求不被打。) 简单的说，就是可以防止过拟合。当过拟合时，边界曲线就回去过分考虑某一个或某一些少数的点，这时候 ensemble通过某种combine机制，然后得到一个泛化性能比较好的边界曲线，也就是比较好的模型。 Decision Tree首先我们从 decision tree开始。A decision tree, which is a tree of if-then rules. The deeper the tree, the more complex the patterns it can capture, but the more prone to overfitting it will be. Because of this, we will need an alternative way of building complex models of decision trees, and an ensemble of different decision trees is one such way.我们先使用 depth =112345678from IPython.display import Imagefrom sklearn.metrics import roc_auc_scorefrom sklearn.tree import DecisionTreeClassifiert1 = DecisionTreeClassifier(max_depth=1, random_state=SEED)t1.fit(xtrain, ytrain)p = t1.predict_proba(xtest)[:, 1]print(&quot;Decision tree ROC-AUC score: %.3f&quot; % roc_auc_score(ytest, p)) Decision tree ROC-AUC score: 0.672 发现结果不太理想，加深depth.1234t2 =DecisionTreeClassifier(max_depth=3, random_state=SEED)t2.fit(xtrain, ytrain)p =t2.predict_proba(xtest)[:, 1]print(&quot;Decision tree ROC-AUC score: %.3f&quot; % roc_auc_score(ytest, p)) Decision tree ROC-AUC score: 0.751 由于我们最后要ensemble，而这要求我们要构造有差异但每个不是那么差的模型。首先我们考虑到使用不同的数据集。1234567xtrain_slim =xtrain.drop([&apos;transaction_amt&apos;], axis=1)xtest_slim =xtest.drop([&apos;transaction_amt&apos;], axis=1)t3=DecisionTreeClassifier(max_depth=3, random_state=SEED)t3.fit(xtrain_slim, ytrain)p =t3.predict_proba(xtest_slim)[:,1]print(&apos;Decision tree ROC-AUC score:&#123;&#125;&apos;.format(roc_auc_score(ytest, p))) Decision tree ROC-AUC score:0.7403182587884118通过corr()来检验两者的相关性(差异性)123p1 =t2.predict_proba(xtest)[:,1]p2 =t3.predict_proba(xtest_slim)[:,1]pd.DataFrame(&#123;&apos;full_data&apos;:p1, &apos;red_data&apos;:p2&#125;).corr() 发现有一定的相关性，但是还是可以容忍的。于是开始融合。1234p1 = t2.predict_proba(xtest)[:, 1]p2 = t3.predict_proba(xtest_slim)[:, 1]p = np.mean([p1, p2], axis=0)print(&quot;Average of decision tree ROC-AUC score: %.3f&quot; % roc_auc_score(ytest, p)) Average of decision tree ROC-AUC score: 0.783我们发现：两个旗鼓相当（0.74 0.73）的可以得到一个更好的结果，可以减少决策失误 的平均是0.78。 需要注意的是我们在构造第二个模型时候通过drop()丢掉一个feature，反而得到一个更好的模型。于是乎，我们想通过使用不同的子集（不同的特征）构造不同的模型，是不是能得到更好的模型？ Random Forest(Bagging)A fast approach that works well in practice is to randomly select a subset of features, fit one decision tree on each draw and average their predictions.This process is known as bootstrapped averaging (often abbreviated bagging), and when applied to decision trees, the resultant model is a Random Forest. (我觉得原作者比我说的清楚，借用了)我的理解，在上面小节中我们使用的是Decision Tree,在实际应用中发现有差异的多个子树的效果要更好一些。而实现这个途径快速的方法就是 Random Forest。Random 在这里表示任意几个子树(特征)，然后这些Tree组成了Forest。123456789from sklearn.ensemble import RandomForestClassifierrf =RandomForestClassifier( n_estimators=10, max_features= 3, random_state=SEED)rf.fit(xtrain, ytrain)p =rf.predict_proba(xtest)[:, 1]print(&apos;Average of decision tree ROC-AUC score:&#123;&#125;&apos;.format(roc_auc_score(ytest, p))) Average of decision tree ROC-AUC score:0.844018408542404这就是叫做”质的飞跃”从那个0.783-&gt; 0.844(将近6个百分点，好吧，有点神经质了…)From nobody to somebody, we are on something.. Ensemble of various models可以看出上述模型中，最后的模型（Random Forest）的子模型(Decision Tree)。但是子模型并不是局限树这一种结构，我们更多的选择：linear models, kernel-based models, non-parametric models, neural networks or even other ensembles! 为了避免代码的冗余构造了以下的helper function.123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657# A host of Scikit-learn modelsfrom sklearn.svm import SVC, LinearSVCfrom sklearn.naive_bayes import GaussianNBfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifierfrom sklearn.linear_model import LogisticRegressionfrom sklearn.neighbors import KNeighborsClassifierfrom sklearn.neural_network import MLPClassifierfrom sklearn.kernel_approximation import Nystroemfrom sklearn.kernel_approximation import RBFSamplerfrom sklearn.pipeline import make_pipelinedef get_models(): &quot;&quot;&quot;Generate a library of base learners.&quot;&quot;&quot; nb = GaussianNB() svc = SVC(C=100, probability=True) # C越大边界越复杂，会导致过拟合 knn = KNeighborsClassifier(n_neighbors=3) # KNN算法寻找训练数据中的K个最近的数据，它使用指向最多的那个类别来作为预测的输出。 lr = LogisticRegression(C=100, random_state=SEED) # 对于这个 c 能知道的就是正则化系数 smaller values specify stronger regularization. nn = MLPClassifier((80, 10), early_stopping=False, random_state=SEED) gb = GradientBoostingClassifier(n_estimators=100, random_state=SEED) # 子模型的数量，默认是100, gbc通常 robust to over-fitting, so a large number results in better performance rf = RandomForestClassifier(n_estimators=10, max_features=3, random_state=SEED) models = &#123;&apos;svm&apos;: svc, &apos;knn&apos;: knn, &apos;naive bayes&apos;: nb, &apos;mlp-nn&apos;: nn, &apos;random forest&apos;: rf, &apos;gbm&apos;: gb, &apos;logistic&apos;: lr, &#125; return models def train_predict(model_list): P =np.zeros((ytest.shape[0], len(model_list))) P =pd.DataFrame(P) print(&apos;Fitting models&apos;) cols =list() for i, (name, m) in enumerate(models.items()): print(&quot;%s...&quot; % name, end=&quot; &quot;, flush=False) m.fit(xtrain, ytrain) P.iloc[:, i] =m.predict_proba(xtest)[:, 1] cols.append(name) print(&apos;Done&apos;) P.columns =cols print(&apos;Done.\n&apos;) return P def score_models(P, y): &quot;&quot;&quot;Score model in predication DF&quot;&quot;&quot; print(&apos;Scoring models&apos;) for m in P.columns: score =roc_auc_score(y, P.loc[:, m]) print(&quot;%-26s: %.3f&quot; % (m, score)) print(&apos;Done, \n&apos;) let’s go…123models =get_models()P =train_predict(models)score_models(P, ytest) This is our base line.Gradient Boosting Machine(GBM) 果然名不虚传, does best我们来分析一下模型之间的相关性，原作者使用的mlens package(You can install it with: pip install mlens)，我这里用的是seaborn(install it with: pip install seaborn).在检查相关性(Pearson相关性:衡量两个数据集合的线性相关性)时候，我们使用的是经过处理的相关性。具体说来可以称之为 error correlation,详细见代码。 12345678# look at error correlations is more promising, errors are significantly correlated import seaborn as snsplt.subplots(figsize=(10,8)) corrmat = P.apply(lambda pred: 1*(pred &gt;= 0.5) - ytest.values).corr()sns.set(font_scale=1.5) hm = sns.heatmap(corrmat, cbar=True, annot=True, square=True, fmt=&apos;.2f&apos;, annot_kws=&#123;&apos;size&apos;: 10&#125;) plt.title(&apos;Pearson Correlation of Features&apos;, y=1.05, size=15) plt.show() 预测值和真实值之间的差异称之为error，查看error的 Pearson correlation，效果更加明显。1print(&quot;Ensemble ROC-AUC score: %.3f&quot; % roc_auc_score(ytest, P.mean(axis=1))) Ensemble ROC-AUC score: 0.884结果高于每一个单独的模型，但是不是那么明显。 Visualize Curve ROC(helper function)我们注意到之前使用的所有的结果的评价标准都是 roc_auc_score，但是并没有提及这是什么。当然在正文中也不打算解释，如果不是很清楚，可以查看本文最后补充概念.简单来说AUC可以用来衡量”二分问题”的泛化能力，是一种评价指标。我们这里想说的是 visualize Curve ROC,可视化。12345678910111213141516171819202122# a helper function for roc_curvefrom sklearn.metrics import roc_curvedef plot_roc_curve(ytest, P_base_learners, P_ensemble, labels, ens_label): &quot;&quot;&quot;Plot the roc curve for base learners and ensemble.&quot;&quot;&quot; plt.figure(figsize=(10, 8)) plt.plot([0, 1], [0, 1], &apos;k--&apos;) cm = [plt.cm.rainbow(i) for i in np.linspace(0, 1.0, P_base_learners.shape[1] + 1)] for i in range(P_base_learners.shape[1]): p = P_base_learners[:, i] fpr, tpr, _ = roc_curve(ytest, p) plt.plot(fpr, tpr, label=labels[i], c=cm[i + 1]) fpr, tpr, _ = roc_curve(ytest, P_ensemble) plt.plot(fpr, tpr, label=ens_label, c=cm[0]) plt.xlabel(&apos;False positive rate&apos;) plt.ylabel(&apos;True positive rate&apos;) plt.title(&apos;ROC curve&apos;) plt.legend(frameon=False) plt.show()plot_roc_curve(ytest, P.values, P.mean(axis=1), list(P.columns), &quot;ensemble&quot;) 我们顺便把刚才的模型可视化，发现ensemble的AUC是最大的，意味这泛化性能是最好，这也是符合我们的认知的 。 Beyond ensembles as a simple average我们回到主线上，在上一个模型中我们提及最后的 ensemble的结果是好于每个单独的模型，但是没有那么突出。根据ROC曲线我们也可以看出，有的模型(KNN)在这里表现的没有那么好，我们在想是不是由于因为这个而拉底了最后的结果，当然这只是猜测，于是我们找到了 try的方向。可能第一直觉去掉这个模型再进行融合，在这个实验表明该策略最后的结果0.883，相比与0.884，你懂得，并没有变好。我们还有一种策略:learn a sensible set of weights to use when averaging predictions.让模型自己去学习如何调整各个模型之间的比例。 Learning to combine predications为了让模型自学习各个之间的预测比例，我们引入了 meta learner(meta是元，理解为最基础的) to learn how to best combine these predictions. 除此之外，我们将使用不同的数据集，像Random Forest使用不同的数据子集(不同的特征组成的数据集)。于是我们需要a method for splitting the training data between the base learners and the meta learner.12345678910base_learners =get_models()meta_learner = GradientBoostingClassifier( n_estimators=1000, loss=&quot;exponential&quot;, max_features=4, max_depth=3, subsample=0.5, learning_rate=0.005, random_state=SEED) 使用最强模型GBM作为 meta learner并定义好 base_learners.To keep things simple, we split the full training set into a training and prediction set of the base learners. This method is sometimes referred to as Blending. Unfortunately, the terminology differs between communities, so it’s not always easy to know what type of cross-validation the ensemble is using.12345678910# sefine a procedure for generating train and test setsxtrain_base , xpred_base, ytrain_base, ypred_base =train_test_split(xtrain, ytrain, test_size=0.5, random_state=SEED)def train_base_learners(base_learners, inp, out, verbose =True): if verbose: print(&apos;Fitting models&apos;) for i, (name, m) in enumerate(base_learners.items()): if verbose: print(&quot;%s...&quot; % name, end=&quot; &quot;, flush=False) m.fit(inp, out) if verbose: print(&apos;Done.&apos;)train_base_learners(base_learners, xtrain_base, ytrain_base) (注意我们只是使用了50%的data去train, test_size =0.5)1234567891011121314def predict_base_learners(pred_base_learners, inp, verbose=True): &quot;&quot;&quot;Generate a prediction matrix.&quot;&quot;&quot; P = np.zeros((inp.shape[0], len(pred_base_learners))) if verbose: print(&quot;Generating base learner predictions.&quot;) for i, (name, m) in enumerate(pred_base_learners.items()): if verbose: print(&quot;%s...&quot; % name, end=&quot; &quot;, flush=False) p = m.predict_proba(inp) # With two classes, need only predictions for one class P[:, i] = p[:, 1] if verbose: print(&quot;done&quot;) return PP_base = predict_base_learners(base_learners, xpred_base) 现在我们得到了base_learners的predications，接下来我们应该使用的是这个流程，在base learners的基础上（类似两层结构了 meta learner 学习的如何搭配这些base learner使得最后的结果 predications最大）进行训练，而不是训练原来的数据集。1meta_learner.fit(P_base, ypred_base) 1234567#meta_learner.fit(P_base, ypred_base)def ensemble_predict(base_learners, meta_learner, inp, verbose=True): &quot;&quot;&quot;Generate predictions from the ensemble.&quot;&quot;&quot; P_pred = predict_base_learners(base_learners, inp, verbose=verbose) return P_pred, meta_learner.predict_proba(P_pred)[:, 1]P_pred, p = ensemble_predict(base_learners, meta_learner, xtest)print(&quot;\nEnsemble ROC-AUC score: %.3f&quot; % roc_auc_score(ytest, p)) 最后的结果是0.881 相比与之前最好的0.884（使用相同的数据集，没有进行 meta_learner的操作）这是因为我们在划分数据集的使用只是使用了0.5的数据集，而前者的模型使用了全部的train sets。有人不免疑问：为什么不使用全部的data？我的理解是划分xtrain_base , xpred_base, ytrain_base, ypred_base使用的是 train_test_split()，总是需要设定一个数值的，即使train_size =0.01，也是没有用到全部的datas. Training with cross-validation我们使用cross-validation 来缓解上面那个问题。During cross-validated training of the base learners, a copy of each base learner is fitted on K−1 folds, and predict the left-out fold. This process is iterated until every fold has been predicted. The more folds we specify, the less data is being left out in each training pass. This makes cross-validated predictions less noisy and a better reflection of performance during test time. The cost is significantly increased training time. Fitting an ensemble with cross-validation is often referred to as stacking, while the ensemble itself is known as the Super Learner. To understand how cross-validation works, we can think of it as an outer loop over our previous ensemble. The outer loop iterates over K distinct test folds, with the remaining data used for training. The inner loop trains the base learners and generate predictions for the held-out data. Here’s a simple stacking implementation:12345678910111213141516171819202122232425262728293031323334353637383940414243from sklearn.base import clonedef stacking(base_learners, meta_learner, X, y, generator): &quot;&quot;&quot;Simple training routine for stacking.&quot;&quot;&quot; # Train final base learners for test time print(&quot;Fitting final base learners...&quot;, end=&quot;&quot;) train_base_learners(base_learners, X, y, verbose=False) print(&quot;done&quot;) # Generate predictions for training meta learners # Outer loop: print(&quot;Generating cross-validated predictions...&quot;) cv_preds, cv_y = [], [] for i, (train_idx, test_idx) in enumerate(generator.split(X)): fold_xtrain, fold_ytrain = X[train_idx, :], y[train_idx] fold_xtest, fold_ytest = X[test_idx, :], y[test_idx] # Inner loop: step 4 and 5 fold_base_learners = &#123;name: clone(model) for name, model in base_learners.items()&#125; train_base_learners( fold_base_learners, fold_xtrain, fold_ytrain, verbose=False) fold_P_base = predict_base_learners( fold_base_learners, fold_xtest, verbose=False) cv_preds.append(fold_P_base) cv_y.append(fold_ytest) print(&quot;Fold %i done&quot; % (i + 1)) print(&quot;CV-predictions done&quot;) # Be careful to get rows in the right order cv_preds = np.vstack(cv_preds) cv_y = np.hstack(cv_y) # Train meta learner print(&quot;Fitting meta learner...&quot;, end=&quot;&quot;) meta_learner.fit(cv_preds, cv_y) print(&quot;done&quot;) return base_learners, meta_learner 尤其在cv_preds和cv_y的维度问题上，注意小心。The basic difference between blending and stacking is therefore that stacking allows both base learners and the meta learner to train on the full data set. Using 2-fold cross-validation, we can measure the difference this makes in our case:1234567from sklearn.model_selection import KFold# Train with stackingcv_base_learners, cv_meta_learner = stacking( get_models(), clone(meta_learner), xtrain.values, ytrain.values, KFold(2))P_pred, p = ensemble_predict(cv_base_learners, cv_meta_learner, xtest, verbose=False)print(&quot;\nEnsemble ROC-AUC score: %.3f&quot; % roc_auc_score(ytest, p)) Ensemble ROC-AUC score: 0.889这是目前为止最好的结果了。Stacking yields a sizeable increase in performance: in fact, it gives us our best score so far. This outcome is typical for small and medium-sized data sets, where the effect of blending can be severe. As the data set size increases, blending and stacking performs similarly. Use packages快要接近尾声了，在文章的开始，我们提及要安利一个resemble好用的package. So, it’s now.Here, we use Ensemble and build our previous generalized ensemble, but now using 10-fold cross-validation:123456789101112131415161718192021from mlens.ensemble import SuperLearner# Instantiate the ensemble with 10 foldssl = SuperLearner( folds=10, random_state=SEED, verbose=2, backend=&quot;multiprocessing&quot;)# Add the base learners and the meta learnersl.add(list(base_learners.values()), proba=True) sl.add_meta(meta_learner, proba=True)# Train the ensemblesl.fit(xtrain, ytrain)# Predict the test setp_sl = sl.predict_proba(xtest)print(&quot;\nSuper Learner ROC-AUC score: %.3f&quot; % roc_auc_score(ytest, p_sl[:, 1])) So simple!1plot_roc_curve(ytest, p.reshape(-1, 1), P.mean(axis=1), [&quot;Simple average&quot;], &quot;Super Learner&quot;) 发现super learner(meta learner)和 basic learner的mean的结果已经不相上下了。super learner得到了很好的训练。 补充概念 ROC曲线和AUC值ROC全称是“受试者工作特征”（Receiver Operating Characteristic）。ROC曲线的面积就是AUC（Area Under the Curve）。AUC用于衡量“二分类问题”机器学习算法性能（泛化能力）。说到这里不得不提及就是经常使用的符号，TP(True Positive), FP(False, Positive), TN(True Negative),FN(False Negative)。他们是根据原来真实数据和预测类别进行的排列组合（当然这是针对二分问题）。好丑的图片啊…接着我们计算TRP(True Positive Radio)，FRP(False Positive Ratio)用于描述ROC曲线，分别表示该曲线的Y轴，X轴。TPR=TP/(TP+FN)FPR=FP/(FP+TN)最后就形成了类似这样的图像(来源于上述的训练模型)我们希望的结果是TRU越大（接近1），FRU越小（接近0）。AUC的值是ROC所覆盖的面积，当AUC越大时候，分类器的效果越好。从图中可以看出模型(ensemble)的面积是最大的，分类效果也是最好的。关于该图像还有一点，如果你的曲线拟合对角线（图中虚线），那么相当于随机猜测。在机器学习中关于这方面经常涉及到的还有precision(查准率), recall(查全率)两个概念，下面是计算公式。其中n= TP+FP总结：图中的x，y轴的计算和 precision, recall不是一个概念，虽然 recall 和y 轴在计算上是相同的。precision 是针对于预测数据(predication结果)来说的，而x轴，y轴(recall)的计算某种意义上是针对原来真实数据而言的。所以我们在训练模型过程可以追求 precision 和 recall的双高(即图像的左上角)。这时候引入了F1-measure(F1 =(2PR)/(R+P)).(P: precision, R: recall) majority/ soft/ hard votingan ensemble that averages classifier predictions is known as a majority voting classifier. When an ensembles averages based on probabilities (as above), we refer to it as soft voting, averaging final class label predictions is known as hard voting. Pearson相关性协方差除以各自的标准差 GBC参数这些参数中，类似于Adaboost，我们把重要参数分为两类，第一类是Boosting框架的重要参数，第二类是弱学习器即CART回归树的重要参数。n_estimators: 也就是弱学习器的最大迭代次数，或者说最大的弱学习器的个数。learning_rate: 即每个弱学习器的权重缩减系数ν，也称作步长对于分类模型，有对数似然损失函数”deviance”和指数损失函数”exponential”两者输入选择。默认是对数似然损失函数”deviance”。一般来说，推荐使用默认的”deviance”。它对二元分离和多元分类各自都有比较好的优化。而指数损失函数等于把我们带到了Adaboost算法。对于回归模型，有均方差”ls”, 绝对损失”lad”, Huber损失”huber”和分位数损失“quantile”。默认是均方差”ls”。一般来说，如果数据的噪音点不多，用默认的均方差”ls”比较好。如果是噪音点较多，则推荐用抗噪音的损失函数”huber”。而如果我们需要对训练集进行分段预测的时候，则采用“quantile”。max_features:可以使用很多种类型的值，默认是”None”,意味着划分时考虑所有的特征数.subsample: 选择小于1的比例可以减少方差，即防止过拟合，但是会增加样本拟合的偏差，因此取值不能太低。推荐在[0.5, 0.8]之间，默认是1.0，即不使用子采样。 参考文献GBC参数设置ROC曲线和AUC值Introduction to Python Ensembles]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>模型融合(Ensemble)</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[对抗生成网络实验对比]]></title>
    <url>%2F2018%2F06%2F05%2F%E5%AF%B9%E6%8A%97%E6%80%A7%E7%94%9F%E6%88%90%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[GAN模型是典型的隐式无监督生成模型，建模过程中没有利用到数据的语义标签。但在实际应用中生成模型的可控性至关重要，根据标签生成对可控有样本更具实际应用价值。图像生成模型被广泛应用于数据增强、风格转换和数据补全领域，需要可控且语义完备的生成模型。条件GAN模型在GAN模型建模思路的基础上，将语义标签加入了建模过程，将无监督生成模型转变为有监督的条件生成模型。条件GAN模型具体包括Conditional GAN模型、Semi-GAN模型和AC-GAN模型。 实验模型介绍（1）Conditional GAN模型GAN模型中判别器D对输入的数据样本的来源进行判别，是典型的判别模型流程。如果在GAN框架中加入有监督信息来辅助训练，如图像的类别信息来辅助判别器D进行判别，则会帮助生成更加真实的样本。其中最初的尝试方式是Conditional GAN模型，结构如图所示。Conditional GAN模型在生成器G和判别器D输入中都加入了标签信息，试图让生成器G学习到从数据标签y到样本x的映射，让判别器D学习对样本x和标签y的组合进行判别。与GAN模型相比，Conditional GAN模型增加了标签信息的输入将模型转变为条件生成模型，在一定程度上提高了模型的稳定性。Conditional GAN模型训练过程中，判别器D对样本x和标签y的类别组合进行训练，并没有输入样本x和标签y的类别错误组合进行训练，因此模型并没有学习样本x和标签y的联合分布。（2）Semi-GAN模型Conditional GAN模型利用了标签信息进行建模，但没有对标签语义的信息进行表征，导致模型能够学到的信息有限。在生成模型过程中，如果判别器D能够明确指出生成样本的类别错误，则可为生成器G提供更加精确的梯度信息，最终能生成更加真实的样本。Semi-GAN基于此思路进行改进，具体结构如图9所示。Semi-GAN模型在Conditional GAN模型的基础上，对判别器D的分类输出进行细化加入了半监督学习过程。（3）AC-GAN模型与Conditional GAN模型相比，Semi-GAN模型中判别器D能够判别真实样本的来源，增强了判别器D的判别能力。但研究表明过强的判别信息会影响生产样本的质量，具体原因为Semi-GAN模型的建模过程为半监督分类过程，目标优化函数为无监督分类和有监督分类目标函数之和。若判别器D的监督分类信息过强，则会削弱判别器D对样本来源的判别能力。Conditional GAN模型能够生成指定类别的样本，Semi-GAN模型能够判别样本的类别信息。AC-GAN模型将以上两个模型思路进行整合，得到够进行条件生成的生成器G，和能够判别样本类别和来源信息的判别器D。AC-GAN模型的结构如图10所示。AC-GAN模型在Conditional GAN的基础上，让判别器D在判别样本来源的同时，让样本进行分类。此时的判别器D的输出分为样本来源信息LS和样本分类LC信息。 不同模型比较下面用表格的方式对比在实验中使用的模型的目标函数(ps,图画比较丑，之后再修改) Name Paper Link Value Function GAN Arxiv DCGAN Arxiv Semi-GAN Arxiv 和GAN 模型相同 CGAN Arxiv ACGAN Arxiv our model Arxiv 数据集介绍在常用于图像生成的图像数据集中，大部分数据的标签类型为离散类型。其中MNIST和Fashion-MNIST为常用的灰度图像数据，每类样本分布较为独立，常用于进行图像分类和样本生成的实验；SVHN和CIFAR10为彩色数据集图像像素分布较为复杂，其中CIFAR10常用来检验分类网络性能的评价数据集；CelebA为大规模的人脸识别和属性分类数据集，每幅人脸图像包括40个属性标签；ImageNet为图像分类和识别数据集，数据集类别分布比较复杂具体包括自然图像和人为图像。UnityEyes为人眼视觉合成数据集，数据集标签包括瞳孔标签和视觉方向标签，其中视觉方向标签为连续的语言标签。常见的离散标签图像数据集的样例: 我们的模型在原始GAN模型中，目标函数定义为生成器G和判别器D的博弈过程，定义V(G;D)为模型的目标函数，由生成器G和判别器D组成。语义匹配目标函数FMloss。基于语义匹配的条件生成网络模型的生成器G和判别器D的目标函数分别为：如上式，LS为样本来源，LC为分类结果。判别器D目标为最大化LC+ LS + FMloss，其试图对输入样本进行分类，并通过来源和语义匹配区分生成样本和原始样本。生成器G的目的是最大化LC − LS −FMloss，其试图通过样本分类结果、样本来源和语义匹配结果来欺骗判别器D。LS来源损失与原始GAN模型相同，LC为语义标签分类损失，在类别分类中使用交叉信息熵，在数值回归中则使用均方差回归。 生成结果对比MNIST数据集生成结果Fashion-MNIST数据集生成结果SVHN数据集生成结果CIFAR10数据集生成结果CelebA数据集生成结果UnityEyes数据集生成结果]]></content>
      <categories>
        <category>Deep learning</category>
      </categories>
      <tags>
        <tag>GANs</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Titanic Challenge]]></title>
    <url>%2F2018%2F06%2F05%2FTitanic-Challenge%2F</url>
    <content type="text"><![CDATA[用此博客记录自己解决 Kaggle Titanic challenge过程中的个人总结。 问题描述说道Titanic(泰坦尼克号)，最熟悉莫过于Titanic(1997 film),这部由著名导演詹姆斯·卡梅隆执导，莱昂纳多·迪卡普里奥、凯特·温斯莱特领衔主演的电影，经久不衰…但是我们今天的画风不是这样的… 我们今天解决的问题是以该事件问背景，但是没有那么浪漫。关于这个案例的介绍网上有很多内容，为了避免累赘，在这里就不进行详述。总的要求：预测在这个事件中乘客是否死亡。附上对于train sets和 test sets中数据的介绍。更多详细的内容参看：https://www.kaggle.com/c/titanic 数据分析顺滑过渡到第二阶段，数据分析，对于竞赛而言，我感觉对于数据的认识的重要性完全不亚于模型的重要性。之后我们将再次提到这句话。我将结合代码进行数据分析。 pandas 原生数据分析函数1234import pandas as pdtrain =pd.read_csv(&apos;data/train.csv&apos;)test =pd.read_csv(&apos;data/test.csv&apos;)train.describe(include=&apos;all&apos;) 除了上面 train.describe()，下面这两个也是比较常用的12train.head()train.columns 因为总体的数据量比较少，所以我们选择把train set 和test set连接起来进行数据分析和处理。1combined2 = pd.concat([train_data, test_data], axis=0) 数据质量分析 缺省值对于缺省值，常用的手段就是填充，但是针对不同的数据有不同的填充手段，有的是均值填充，有的是默认值填充还有的是根据现有数据训练一个 regression进行拟合(这种情况出现在缺省的数据比较重要，对于结果的预测有比较强的相关性的时候)。1combined2.Embarked.fillna(&apos;S&apos;, inplace=True) Embardked(上船港口)不是那么能表现出和结果(survival)相关的变量，我么可以直接采用某个默认值进行填充。1combined2.Fare.fillna(np.median(combined2.Fare[combined2.Fare.notnull()]), inplace=True) Fare(船票)我们选择使用均值填充 12345678classers = [&apos;Fare&apos;,&apos;Parch&apos;,&apos;Pclass&apos;,&apos;SibSp&apos;,&apos;TitleCat&apos;,&apos;CabinCat&apos;,&apos;Sex_female&apos;,&apos;Sex_male&apos;, &apos;EmbarkedCat&apos;, &apos;FamilySize&apos;, &apos;NameLength&apos;, &apos;FamilyId&apos;]age_et = ExtraTreesRegressor(n_estimators=200)X_train = full_data.loc[full_data.Age.notnull(),classers]Y_train = full_data.loc[full_data.Age.notnull(),[&apos;Age&apos;]]X_test = full_data.loc[full_data.Age.isnull(),classers]age_et.fit(X_train,np.ravel(Y_train))age_preds = age_et.predict(X_test)full_data.loc[full_data.Age.isnull(),[&apos;Age&apos;]] = age_preds 因为在特征提取看来 age 是一个比较重要的属性（下文中使用age来进一步计算性别特征，而性别特征对于survival 是重要的因素），所以需要通过 fit来进行填充 null 值。 异常值异常值的检测12combined2.boxplot()plt.ylim(0, 1000) 异常值处理大多数情况下我们都采取忽视，但是有时候异常值中却跟结果有比较强的相关性，比如说该题目分数在0.9的一位大神在博客中使用的特征包含名字长度。这个在我一开始的特征提取中确实没有太在意名字长度也可以当作一种和结果(servival or dead)相关的特征。 重复值重复值的检测1train[train.duplicated()==True] 如果运行结果为空，那么就是没有重复值,如果有重复值，一般使用下面类似的代码都是可以去除掉的。1df.drop_duplicates() 分布特征分析1234567#分布分析fig ,ax =plt.subplots(2,2, figsize=(8,6))sns.countplot(&apos;Embarked&apos;, data =train, ax =ax[0,0])sns.countplot(&apos;Pclass&apos;, data =train, ax =ax[0,1])sns.violinplot(&apos;Survived&apos;, &apos;Age&apos;, data= train, ax =ax[1,0]).set(ylim =(-10, 80))sns.countplot(x =&apos;Survived&apos;, data= train, ax =ax[1,1])plt.tight_layout() 运行需要导入 seaborn1import seaborn as sns 这里安利一个数据可视化工具-seaborn。回正题 counplot()可以直观的看出单个数据的特征，但是我们更加关心的是数据和数据之间的关系，更准确的是数据和预测数据(survival )之间的关系。所以,我们进行相关性分析。123456plt.subplots(figsize=(10,8)) corrmat = train[train.columns[1:]].corr()sns.set(font_scale=1.5) hm = sns.heatmap(corrmat, cbar=True, annot=True, square=True, fmt=&apos;.2f&apos;, annot_kws=&#123;&apos;size&apos;: 10&#125;) plt.title(&apos;Pearson Correlation of Features&apos;, y=1.05, size=15) plt.show() 从图中可以看出，在原始数据集特征中(为什么这么说，嗯，这意味着我们下文还要进行 generate new features),Fare特征是和 survival最相关的。这从数据角度这你船票价钱越高，你生存的几率就越大(三观尽毁)。嗯，这是符合经济社会的运行规律的。我们在分析 Pearson Correlation的时候，关注的是数值的绝对值，如果是正值，表示正相关；如果是负值，表示负相关。 如果细心的小伙伴发现，这个并没有把所有的变量的相关性表示出来，是的，下文我将给出一个加强版的。 特征工程当我们对数据有了一个初步的认识，这时候就可以进行特征工程了。网上流传很广的一句话”数据特征决定了机器学习的上限，而算法优化只是尽可能逼近这个上限”，我深有体会。因为之前进行特征提取，然后在kaggle的submission score是0.73205,经过模型融合然后达到了0.78947，提高了5个百分点。当自己在思考数据特征重新进行特征提取的时候，最后的score 是0.82296.这都是以10个百分点的提高啊。所以这句话很有道理，我试图找到这句话的出处，以表示我对于版权的尊重，但是科学上网能力有限，没有找见，也许这句话来自群众的智慧吧。图：我在kaggle 的submission 和相应的score 但是我想强调的是特征提取是个很难有模板化的东西，这得看个人对于这个问题的理解和对于数据的理解，对于数据异常值的处理。并且还想说的是这个一个迭代的过程，不是一步到位的。当初步构造好自己特征之后可以使用图形化工具进行简单的分析一下。(下图是我第一次构造的特征工程的图形化)12345678910111213141516171819def correlation_heatmap(df): _ , ax = plt.subplots(figsize =(14, 12)) colormap = sns.diverging_palette(220, 10, as_cmap = True) _ = sns.heatmap( df.corr(), cmap = colormap, square=True, cbar_kws=&#123;&apos;shrink&apos;:.9 &#125;, ax=ax, annot=True, linewidths=0.1,vmax=1.0, linecolor=&apos;white&apos;, annot_kws=&#123;&apos;fontsize&apos;:12 &#125; ) plt.title(&apos;Pearson Correlation of Features&apos;, y=1.05, size=15) train2 =train1.drop([&apos;NullCabin&apos;], axis =1)correlation_heatmap(train2) 上图是第一次特征分析的结果(kaggle score: 0.73205),survival和Pclass和Fare是有较强的正相关性。 该图是第二次特征分析分析结果（kaggle score:0.82296,survival与male_adult(-0.56)、sex_male(-0.54)负相关,和female_adult(0.54)、sex_female(0.52)正相关。你的survival的概率和你的性别和年龄有关，如果你是成年女子，那么你很大的概率不会死亡(像Rose那样)；如果你是成年男子，那么你有很大概率体现英伦的绅士风度，主动(Jack那样)或者被选择死亡。瞬间想起了Titanic电影中Jack和Rose 的场景，好感人啊!!! 模型训练发现写了这么久，还没有开始训练模型。加快脚步…下面的内容以第一次训练模型为例。在建立基本模型之前我们需要先引入评价函数，以评价不同模型性能的好坏。 通过均值和方差来评价模型性能的优劣1234from sklearn import cross_validation def rmsl(clf): s = cross_validation.cross_val_score(clf, X_train, y_train, cv=5) return (s.mean(),s.std()) 建立基本模型1234567891011121314151617181920212223242526272829303132333435363738394041424344from sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis,gaussian_processNLA =[ #ensemble methods ensemble.AdaBoostClassifier(), ensemble.BaggingRegressor(), ensemble.GradientBoostingClassifier(), ensemble.RandomForestClassifier(n_estimators=60), # Gaussian process gaussian_process.GaussianProcessClassifier(), # LM linear_model.LogisticRegression(C=1.0, penalty=&apos;l1&apos;), #Navies Bayes naive_bayes.GaussianNB(), # Nearest Neighbor neighbors.KNeighborsClassifier(n_neighbors=3), # Svm svm.SVC(probability=True), svm.LinearSVC(), #Tree tree.DecisionTreeClassifier(), tree.ExtraTreeClassifier() ]#create table to compare MLAMLA_columns = [&apos;MLA Name&apos;, &apos;MLA Parameters&apos;,&apos;MLA Train Accuracy Mean&apos;, &apos;MLA Test Accuracy Mean&apos;, &apos;MLA Test Accuracy Min&apos; ,&apos;MLA Time&apos;] MLA_compare = pd.DataFrame(columns = MLA_columns) row_index = 0 for alg in MLA: #set name and parameters MLA_compare.loc[row_index, &apos;MLA Name&apos;] = alg.__class__.__name__ MLA_compare.loc[row_index, &apos;MLA Parameters&apos;] = str(alg.get_params()) #score model with cross validation: cv_results = model_selection.cross_validate(alg, X_train, y_train, cv =5,return_train_score=True) MLA_compare.loc[row_index, &apos;MLA Time&apos;] = cv_results[&apos;fit_time&apos;].mean() MLA_compare.loc[row_index, &apos;MLA Train Accuracy Mean&apos;] = cv_results[&apos;train_score&apos;].mean() MLA_compare.loc[row_index, &apos;MLA Test Accuracy Mean&apos;] = cv_results[&apos;test_score&apos;].mean() MLA_compare.loc[row_index, &apos;MLA Test Accuracy Min&apos;] = cv_results[&apos;test_score&apos;].min() #let&apos;s know the worst that can happen! row_index+=1MLA_compare.sort_values(by = [&apos;MLA Test Accuracy Mean&apos;], ascending = False, inplace = True) 当我们发现某个模型效果比较好的时候，我们仍然可以进一步调参。但是这种调参并不是每次会得到better result,有时候只是一个decent result。调参是个技术活。以上图中的 DecisionTreeClassifier为例进行调参。12345678param_grid = &#123;&apos;criterion&apos;: [&apos;gini&apos;, &apos;entropy&apos;], &apos;splitter&apos;: [&apos;best&apos;, &apos;random&apos;], &apos;max_depth&apos;: [None, 2,4,6,8,10], &apos;min_samples_split&apos;: [5,10,15,20,25], &apos;max_features&apos;: [None, &apos;auto&apos;, &apos;sqrt&apos;, &apos;log2&apos;] &#125;tune_model = model_selection.GridSearchCV(tree.DecisionTreeClassifier(), param_grid=param_grid, scoring = &apos;accuracy&apos;, cv = 5) cv_results = model_selection.cross_validate(tune_model, X_train, y_train, cv = 5) 使用的是sklearn 中的model_selection 模块，进行GridSearch，其实只是把调参过程自动化程序化。得到的结果是1230.863068608315 #train mean0.79803322024 # test mean0.77094972067 #test min 我们通过比对发现这个结果和上图的结果是稍微变差的。可视化显示各个算法的效率：1234sns.barplot(x=&apos;MLA Test Accuracy Mean&apos;, y = &apos;MLA Name&apos;, data = MLA_compare, color = &apos;m&apos;) plt.title(&apos;Machine Learning Algorithm Accuracy Score \n&apos;) plt.xlabel(&apos;Accuracy Score (%)&apos;) plt.ylabel(&apos;Algorithm&apos;) 对比之后我们选取几个效果比较“好”的模型，然后进行下一步的模型融合。12345678910111213141516171819MLA_best = [ #Ensemble Methods ensemble.AdaBoostClassifier(), # 0.76076 ensemble.BaggingClassifier(), # 0.72248 ensemble.GradientBoostingClassifier(), # 0.73684 ensemble.RandomForestClassifier(n_estimators = 60), # 0.72727 #GLM linear_model.LogisticRegression(C=1.0, penalty=&apos;l1&apos;, tol=1e-6), # 0.77990 linear_model.RidgeClassifierCV(), # 0.77033 linear_model.LogisticRegressionCV() #0.77033 ] row_index = 0 for alg in MLA_best: algname = alg.__class__.__name__ alg.fit(X_train, y_train) predictions = alg.predict(X_test) result = pd.DataFrame(&#123;&apos;PassengerId&apos;:test[&apos;PassengerId&apos;].as_matrix(), &apos;Survived&apos;:predictions.astype(np.int32)&#125;) result.to_csv(algname+&quot;.csv&quot;, index=False) # save the results row_index+=1 模型融合简单的说模型融合就是通过多个decent模型的结果通过某种方式的结合，产生了比原来单个模型better的结果。关于模型融合的详细内容，请移步另一篇文章模型融合(Ensemble learning)我们这里以stacking(二层)为例说明模型融合。1234567891011121314151617181920212223ntrain = train.shape[0] #891 ntest = test.shape[0] #418 SEED = 0 # for reproducibility NFOLDS = 5 # set folds for out-of-fold prediction kf =model_selection.KFold(n_splits=NFOLDS, random_state=SEED)# 封装算法基本操作 class SklearnHelper(object): def __init__(self, clf, seed=0, params=None): params[&apos;random_state&apos;] = seed self.clf = clf(**params) def train(self, x_train, y_train): self.clf.fit(x_train, y_train) def predict(self, x): return self.clf.predict(x) def fit(self,x,y): return self.clf.fit(x,y) def feature_importances(self,x,y): print(self.clf.fit(x,y).feature_importances_) return self.clf.fit(x,y).feature_importances_ 下面是定义五折交叉验证的方法，默认是三折。123456789101112131415161718def get_oof(clf, x_train, y_train, x_test): oof_train = np.zeros((ntrain,)) oof_test = np.zeros((ntest,)) oof_test_skf = np.empty((NFOLDS, ntest)) for i, (train_index, test_index) in enumerate(kf.split(x_train)): x_tr = x_train[train_index] y_tr = y_train[train_index] x_te = x_train[test_index] clf.train(x_tr, y_tr) oof_train[test_index] = clf.predict(x_te) oof_test_skf[i, :] = clf.predict(x_test) oof_test[:] = oof_test_skf.mean(axis=0) return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1) # 想让z变成只有一列，行数不知道多少 1234567891011121314151617181920212223242526272829303132from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier,AdaBoostClassifier, GradientBoostingClassifier# 定义四个不同的弱分类器的参数值 # Random Forest parameters rf_params = &#123; &apos;n_jobs&apos;: -1,&apos;n_estimators&apos;: 500,&apos;warm_start&apos;: True, &apos;max_depth&apos;: 6,&apos;min_samples_leaf&apos;: 2, &apos;max_features&apos; : &apos;sqrt&apos;,&apos;verbose&apos;: 0#&apos;max_features&apos;: 0.2, &#125; # Extra Trees Parameters et_params = &#123; &apos;n_jobs&apos;: -1,&apos;n_estimators&apos;:500,&apos;max_depth&apos;: 8,&apos;min_samples_leaf&apos;: 2,&apos;verbose&apos;: 0 #&apos;max_features&apos;: 0.5, &#125; # AdaBoost parameters ada_params = &#123; &apos;n_estimators&apos;: 500,&apos;learning_rate&apos; : 0.75 &#125; # Gradient Boosting parameters gb_params = &#123; &apos;n_estimators&apos;: 500,&apos;max_depth&apos;: 5,&apos;min_samples_leaf&apos;: 2, &apos;verbose&apos;: 0 #&apos;max_features&apos;: 0.2, &#125; # Support Vector Classifier parameters # svc_params = &#123; # &apos;kernel&apos; : &apos;linear&apos;,&apos;C&apos; : 0.025 # &#125; # 创建四个若分类器模型 rf = SklearnHelper(clf=RandomForestClassifier, seed=SEED, params=rf_params) et = SklearnHelper(clf=ExtraTreesClassifier, seed=SEED, params=et_params) ada = SklearnHelper(clf=AdaBoostClassifier, seed=SEED, params=ada_params) gb = SklearnHelper(clf=GradientBoostingClassifier, seed=SEED, params=gb_params) 1234567891011121314151617181920#X_train =X_train.values#X_test =X_test.values# 使用五折交叉方法分别计算出使用不同算法的预测结果，这些结果将用于Stacking的第二层预测 et_oof_train, et_oof_test = get_oof(et, X_train, y_train, X_test) # Extra Trees rf_oof_train, rf_oof_test = get_oof(rf,X_train, y_train, X_test) # Random Forest ada_oof_train, ada_oof_test = get_oof(ada, X_train, y_train, X_test) # AdaBoost gb_oof_train, gb_oof_test = get_oof(gb,X_train, y_train, X_test) # Gradient Boost rf_feature = rf.feature_importances(X_train,y_train) et_feature = et.feature_importances(X_train, y_train) ada_feature = ada.feature_importances(X_train, y_train) gb_feature = gb.feature_importances(X_train,y_train) feature_dataframe = pd.DataFrame( &#123;&apos;features&apos;: cols, &apos;Random Forest feature importances&apos;: rf_feature, &apos;Extra Trees feature importances&apos;: et_feature, &apos;AdaBoost feature importances&apos;: ada_feature, &apos;Gradient Boost feature importances&apos;: gb_feature &#125;) 接下来以第一层为为基础训练第二层12345678base_predictions_train = pd.DataFrame( &#123; &apos;RandomForest&apos;: rf_oof_train.ravel(),# # ravel函数在降维时默认是行序优先 &apos;ExtraTrees&apos;: et_oof_train.ravel(), &apos;AdaBoost&apos;: ada_oof_train.ravel(), &apos;GradientBoost&apos;: gb_oof_train.ravel() &#125;) X_train2 = np.concatenate(( et_oof_train, rf_oof_train, ada_oof_train, gb_oof_train), axis=1) X_test2 = np.concatenate(( et_oof_test, rf_oof_test, ada_oof_test, gb_oof_test), axis=1) 使用XGBoost训练第二层的数据。关于XGBoost为什么是有效的和相关的概念，请移步XGBoost123456789101112131415# XGboost import xgboost as xgbgbm = xgb.XGBClassifier( #learning_rate = 0.02, n_estimators= 2000, max_depth= 4, min_child_weight= 2, #gamma=1, gamma=0.9, subsample=0.8, colsample_bytree=0.8, objective= &apos;binary:logistic&apos;, nthread= -1, scale_pos_weight=1).fit(X_train2, y_train) predictions = gbm.predict(X_test2) 最后产生结果文件 StackingSubmission.csv12StackingSubmission = pd.DataFrame(&#123;&apos;PassengerId&apos;:test.PassengerId, &apos;Survived&apos;: predictions &#125;) StackingSubmission.to_csv(&quot;StackingSubmission.csv&quot;, index=False) # 0.78947 参考文献本文在效果可视化中借鉴该博客特征提取参看kaggle多位大神，在这里就谢过…]]></content>
      <categories>
        <category>竞赛</category>
      </categories>
      <tags>
        <tag>Kaggle</tag>
        <tag>Titanic</tag>
        <tag>泰坦尼克</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[模型融合]]></title>
    <url>%2F2018%2F06%2F05%2F%E6%A8%A1%E5%9E%8B%E8%9E%8D%E5%90%88%2F</url>
    <content type="text"><![CDATA[多个有差异性的模型融合可以提高整体的性能。它能同时降低最终模型的bias 和variance，从而在提高竞赛分数的同时降低overfitting 的风险。 从结果文件中融合这种做法不需要重新训练模型，融合竞赛提交的结果文件就可以，简单便捷。 Voting投票制：少数服从多数。如一个分类问题，多个模型的投票（当然可以设置权重，若没有就是平均投票），最终投票数最多的类就是被预测的类。对于加权表决融合，性能表现较差的模型（权值比较低）只能通过和其他模型保持一致增强自己的说服力。对于结果取平均融合，在不同的评估准则上也能获得不错的效果在于：取均值常常能减少过拟合的现象。如图所示：如果单个模型过拟合产生了绿色的边缘，这时候去平均这种策略使得决策边界变成黑色的边缘，这样的效果更好。机器学习的目的并不是让模型记住训练数据，而是具有更好的泛化性。 RankingRank的思想其实和Averaging一致，但Rank是把排名做平均，对于AUC指标比较有效。 训练模型融合 Bagging:使用训练数据的不同随机子集来训练每个 Base Model，最后进行每个 Base Model 权重相同的 Vote。也即 Random Forest 的原理。 Boosting:Boosting是一种迭代的方法，每一次训练会更关心上一次被分错的样本，比如改变被错分的样本的权重的Adaboost方法。也即 Gradient Boosting，Adaboost 的原理。比 Bagging 效果好，但更容易 Overfit。 Blending用不相交的数据训练不同的 Base Model，将它们的输出取（加权）平均。实现简单，但对训练数据利用少了。 Stackinig(以二层为例)在网上为数不多的关于stacking的内容中，相信你已经看过这张图片：PS:这不是原图，是在原图的基础上经过修改（把最上面标题的model 1，2,3,4,5 修改为model 1,1,1,1,1。因为这个一个model 的不同阶段，不是多个模型） 想比较而言我更加喜欢下面这张图片，因为它把stacking 的不同阶段表达的更加清楚，尤其是经过model 1之后，model 2是在model 1的基础上进行训练的。 对于第二阶段使用的样本集合，上图使用的是第一阶段的结果数据集，当然还有一种方式，如下图所示。 在该图中上一阶段的结果(prob 1-N)列和原始数据集组成新的特征向量，训练第二阶段模型。 名词解释 cross validation交叉验证当评估不同的参数设置，对算法表现的影响时，仍然存在则过拟合的风险。因为在调整参数，优化测试集的算法表现时，测试集的信息已经泄漏进模型中了。为了解决这个问题，需要一部分数据作为验证集(validation set)。 这样，用训练集(Train set)的数据训练模型；用验证集对模型参数调剂，如上述程序中的C值；最后，算法的评价在测试集(Test set)上完成。但是当数据有原来的两份化成三份之后，降低了寻数据量；另外算法的表现依赖于三个数据集的划分。解决上述两个问题的常见方法: cross validation. 测试集仍然单独划分出来，但是 validation set不用单独划分。将训练集划分为k个小的数据集，称之为k-fold CV。对每个fold进行下列过程： 用其他k-1 folds 作为training sets，训练模型 模型的结果用剩下的一个 fold进行评价模型的性能用上述循环中的 k-fold 交叉验证集的平均值表现，这样的做法增加了计算量，但是提高数据的利用效率。 参考文献https://blog.csdn.net/u013395516/article/details/79745063https://blog.csdn.net/u012969412/article/details/76636336https://blog.csdn.net/u012604810/article/details/77579782]]></content>
      <categories>
        <category>竞赛</category>
      </categories>
      <tags>
        <tag>Kaggle</tag>
        <tag>优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[XGBoost]]></title>
    <url>%2F2018%2F06%2F05%2FXGBoost%2F</url>
    <content type="text"><![CDATA[Introduction to XGBoostXGBoost is short for “Extreme Gradient Boosting”, where the term “Gradient Boosting” is proposed in the paper Greedy Function Approximation: A Gradient Boosting Machine, by Friedman. XGBoost is based on this original model. And most of the content is based on the websit(http://xgboost.readthedocs.io/en/latest/model.html) element of Supervised learningXGBoost is used for supervised learning problems, where we use the training data (with multiple features) xi to predict a target variable yi. Objective Function: Training Loss + RegularizationA very important fact about objective functions is they must always contain two parts: training loss and regularization.For example, a commomly used training loss is mean squared error.Another commonly used loss function is logistic loss for logistic regression The regularization term is what people usually forget to add. The regularization term controls the complexity of the model, which helps us to avoid overfitting.The general principle is we want both a simple and predictive model. The tradeoff between the two is also referred as bias-variance tradeoff in machine learning. Tree EnsembleThe tree ensemble model is a set of classification and regression trees(CART). Here is a simple example of a CART that classifies whether someone will like computer games.A CART is a bit different from decision trees, where the leaf only contains decision values. In CART, a real score is associated with each of the leaves, which gives us richer interpretations that go beyond classification. This also makes the unified optimization step easier, as we will see in a later part of this tutorial. Usually, a single tree is not strong enough to be used in practice. What is actually used is the so-called tree ensemble model, which sums the prediction of multiple trees together.Here is an example of a tree ensemble of two trees. The prediction scores of each individual tree are summed up to get the final score. If you look at the example, an important fact is that the two trees try to complement each other. Now here comes the question, what is the model for random forests? It is exactly tree ensembles! So random forests and boosted trees are not different in terms of model, the difference is how we train them. This means if you write a predictive service of tree ensembles, you only need to write one of them and they should directly work for both random forests and boosted trees. One example of why elements of supervised learning rock. Tree BoostingAfter introducing the model, let us begin with the real training part. How should we learn the trees? The answer is, as is always for all supervised learning models: define an objective function, and optimize it! Assume we have the following objective function (remember it always needs to contain training loss and regularization) Additive TrainingFirst thing we want to ask is what are the parameters of trees? You can find that what we need to learn are those functions fi, with each containing the structure of the tree and the leaf scores. This is much harder than traditional optimization problem where you can take the gradient and go. It is not easy to train all the trees at once. Instead, we use an additive strategy: fix what we have learned, and add one new tree at a time. It remains to ask, which tree do we want at each step? A natural thing is to add the one that optimizes our objective. Training modelThe XGBoost model for classification is called XGBClassifier(regression is called XGBRegressor). We can create and and fit it to our training dataset. Models are fit using the scikit-learn API and the model.fit() function.]]></content>
      <categories>
        <category>竞赛</category>
      </categories>
      <tags>
        <tag>XGBoost</tag>
        <tag>模型融合(Ensemble)</tag>
        <tag>Kaggle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[等概率生成器]]></title>
    <url>%2F2018%2F05%2F28%2F%E7%AD%89%E6%A6%82%E7%8E%87%E7%94%9F%E6%88%90%E5%99%A8%2F</url>
    <content type="text"><![CDATA[给定一个可以等概率生成1-3的rand3()函数生成器，求解可以随机等概率生成1-7的rand7()函数生成器。 解题思路使用rand3()函数生成1-9的数字，然后丢弃8,9两种可能。 实现代码12345678910class Solution: def random7(self): x =8 while x&gt;7: x =self.random3() + (self.random3()-1)*3 return x def random3(self): return if __name__ ==&quot;__main__&quot;: print(Solution().random7())]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>等概率生成器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习优化方法]]></title>
    <url>%2F2018%2F05%2F28%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[机器学习的优化方法有很多，这里主要解释的是 Gradient Descent 和 Newton’s Method. 梯度下降（gradient descent） 梯度下降法是最早最简单，也是最为常用的最优化方法。梯度下降法实现简单，当目标函数是凸函数时，梯度下降法的解是全局解。接下来衍生的的子类:批量梯度下降法（Batch Gradient Descent，BGD）随机梯度下降（Stochastic Gradient Descent，SGD） 牛顿法和拟牛顿法（Newton’s method &amp; Quasi-Newton Methods） 牛顿法最大的特点就在于它的收敛速度很快。个人感觉牛顿法只是在在每次迭代的时候进行了两次运算，和梯度下降在总的运算次数上并没有很大的差别，为什么会收敛速度更快呢？ 名词解释 凸优化:对凸优化的问题我们在基础数学上面已经有了很多解决方法，例如可以将凸优化问题Lagerange做对偶化，然后用Newton、梯度下降算法求解。凸集合: 凸函数：Jacobian矩阵和Hessian矩阵：在向量分析中, 雅可比矩阵是一阶偏导数以一定方式排列成的矩阵, 其行列式称为雅可比行列式。Hessian矩阵：]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>优化</tag>
        <tag>Gradient Descent</tag>
        <tag>Newton&#39;s Method</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[求解平方根]]></title>
    <url>%2F2018%2F05%2F28%2F%E6%B1%82%E8%A7%A3%E5%B9%B3%E6%96%B9%E6%A0%B9%2F</url>
    <content type="text"><![CDATA[求解平方根的算法主要有两种：二分法(binary search) 和牛顿迭代法(Newton’s Method) Just give me codes…1234567891011121314151617181920212223242526272829from math import sqrt# you needn't import sqrt, I do that just for comparing the results of different methodsclass Solution: # in-built function def my_sqrt(self, n): return sqrt(n) def sqrt_binary(self, n): low, high =0,n mid =int((low+high)/2) while abs(mid*mid- n)&gt; 1e-9: if mid*mid&gt;n: high =mid else: low =mid mid =(low+high)/2 return mid # Newton method: math def newton_method(self, n): k =1 while abs(k*k-n) &gt;1e-9: k =(k+n/k)/2 return k if __name__ =="__main__": print(Solution().my_sqrt(2)) print(Solution().sqrt_binary(2)) print(Solution().newton_method(2)) 运行结果1231.4142135623730951 #built-in function1.4142135623842478 # binary search1.4142135623746899 # Newton method 从结果中看，Newton method比 binary sqrt更加接近系统自带的sqrt function. 并且从数学上可以证明 Newton method 比 binary sqrt需要更少的迭代次数。 附录：牛顿迭代法是求方程根的重要方法之一，其最大优点是在方程f(x) = 0的单根附近具有平方收敛，而且该法还可以用来求方程的重根,复根。牛顿迭代法结论其实就是取泰勒级数前两项等于0求得的,泰勒公式表示为更简练的写法为：通过 Newton methond逼近方程的解的过程可以表示为下图： referrences:https://blog.csdn.net/ycf74514/article/details/48996383]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>Newton&#39;s Method</tag>
        <tag>求解平方根</tag>
        <tag>binary search</tag>
      </tags>
  </entry>
</search>
