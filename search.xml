<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Similarity Measures]]></title>
    <url>%2F2019%2F06%2F21%2Fsimilarity-measures%2F</url>
    <content type="text"><![CDATA[The similarity measure is the measure of how much alike two data objects are. Similarity measure in a data mining context is a distance with dimensions representing features of the objects. If this distance is small, it will be the high degree of similarity where large distance will be the low degree of similarity. The similarity is subjective and is highly dependent on the domain and application. For example, two fruits are similar because of color or size or taste. Care should be taken when calculating distance across dimensions/features that are unrelated. The relative values of each element must be normalized, or one feature could end up dominating the distance calculation. Similarity are measured in the range 0 to 1 [0,1]. Euclidean distanceEuclidean distance is the most common use of distance. In most cases when people said about distance, they will refer to Euclidean distance. Euclidean distance is also known as simply distance. When data is dense or continuous, this is the best proximity measure. Applications: Where data is continuous or numerical. Also knows as L2 Norm famously. In an n dimensional space between two vectors x and y the formula is simply the square root of the sum of the square distance: $$d \left( \left[ x _ { 1 } , x _ { 2 } , \ldots , x _ { n } \right] , \left[ y _ { 1 } , y _ { 2 } , \ldots , y _ { n } \right] \right) = \sqrt { \sum _ { i = 1 } ^ { n } \left( x _ { i } - y _ { i } \right) ^ { 2 } }$$ 1234567#!/usr/bin/env pythonfrom math import* def euclidean_distance(x,y): return sqrt(sum(pow(a-b,2) for a, b in zip(x, y))) print euclidean_distance([0,3,4,5],[7,6,3,-1]) Manhattan distanceThis Manhattan distance metric is also known as Manhattan length, rectilinear distance, L1 distance or L1 norm, city block distance, Minkowski’s L1 distance, taxi-cab metric, or city block distance. Can be used if the dimensions are continuous or numeric. This distance measure is very similar to Euclidean but it is a sum of the absolute difference of every dimension rather than the sum of squares. It is also known as L1 Norm. $$d = \sum _ { i = 1 } ^ { n } \left| x _ { i } - y _ { i } \right|$$ 123456from math import*def manhattan_distance(x,y): return sum(abs(a-b) for a,b in zip(x,y)) print manhattan_distance([10,20,10],[10,20,20]) Minkowski distanceMinkowski distance is the generalized distance metric. $$\left( \sum _ { i = 1 } ^ { n } \left| x _ { i } - y _ { i } \right| ^ { p } \right) ^ { 1 / p }$$ 123456789101112131415#!/usr/bin/env python from math import*from decimal import Decimal def nth_root(value, n_root): root_value = 1/float(n_root) return round (Decimal(value) ** Decimal(root_value),3) def minkowski_distance(x,y,p_value): return nth_root(sum(pow(abs(a-b),p_value) for a,b in zip(x, y)),p_value) print minkowski_distance([0,3,4,5],[7,6,3,-1],3) Cosine similarityCosine similarity is particularly used in positive space, where the outcome is neatly bounded in [0,1]. One of the reasons for the popularity of cosine similarity is that it is very efficient to evaluate, especially for sparse vectors. Used in identifying document similarity, product recommendations, Information retrieval and works efficiently with high-dimensional sparse data. It is an angle between two data points in the vector space. Given a vector A and B, the cosine distance is the dot product of x and y divided by Euclidean distance. $$\text { similarity } = \cos ( \theta ) = \frac { \mathbf { A } \cdot \mathbf { B } } { | \mathbf { A } | | \mathbf { B } | } = \frac { \sum _ { i = 1 } ^ { n } A _ { i } B _ { i } } { \sqrt { \sum _ { i = 1 } ^ { n } A _ { i } ^ { 2 } } \sqrt { \sum _ { i = 1 } ^ { n } B _ { i } ^ { 2 } } }$$ 1234567891011121314#!/usr/bin/env pythonfrom math import* def square_rooted(x): return round(sqrt(sum([a*a for a in x])),3) def cosine_similarity(x,y): numerator = sum(a*b for a,b in zip(x,y)) denominator = square_rooted(x)*square_rooted(y) return round(numerator/float(denominator),3) print cosine_similarity([3, 45, 7, 2], [2, 54, 13, 15]) Jaccard similarityJaccard Distance measures how close two sets are. It is simply a ratio of the intersection of the sets to the Union. Can be used when the datatypes are categorical. Example: Products purchased/viewed by customers. Typically used in Product recommendation, Clustering customers based on purchase/engagement patterns. Please note Jaccard distance is a dissimilarity metric and Jaccard coefficient, J(A,B) is a similarity metric. $$d _ { J } ( A , B ) = 1 - J ( A , B ) = \frac { | A \cup B | - | A \cap B | } { | A \cup B | }$$ 123456789from math import* def jaccard_similarity(x,y): intersection_cardinality = len(set.intersection(*[set(x), set(y)])) union_cardinality = len(set.union(*[set(x), set(y)])) return intersection_cardinality/float(union_cardinality) print jaccard_similarity([0,1,2,5,6],[0,2,3,5,7,9]) Edit DistanceEdit distance is used when the comparing strings. Ideal use cases would be auto spell check, meta data correction etc. The distance between two strings are smaller if the number of corrections ( insertions or deletions ) needed to perfectly match are smaller.]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>similarity-measures</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ResNet Understanding]]></title>
    <url>%2F2019%2F06%2F21%2Fresnet-understanding%2F</url>
    <content type="text"><![CDATA[本文介绍 ResNet 的背景，想要解决的问题，基本思路和框架 和对于其的一种解读方式。 背景网络的深度是容易出现梯度爆炸和梯度消失，造成网络的不收敛。一些方法已经在很大程度上可以缓解这个问题，比如使用 ReLU激活函数、 良好的权值初始化方法 、还有 intermediate normalization layers(即网络中间的batch normalization)。并且对于网络过程中的过拟合问题，也提出了一些办法如，使用 regularization、权值衰减和dropout方法。 但解决了深度网络收敛问题之后，又出现了另外一个问题。 残差网络要解决的问题一般来说在没有过拟合的情况下，可以逐步增加网络的深度。但在实验中发现了这样的问题。网络退化： 网络越深，训练误差越大。（accuracy开始饱和，原文中这样说的）这种退化并不是由于过拟合造成的，并且在适当深度模型中增加更多的层会导致更多的训练误差 基本思路和结构作者基于增加层如果为恒等映射那么更深层网络不应该比浅层网络产生更高错误率的思想 如图所示左边的是传统的plain networks的结构，右边的是修改为ResNet的结构。改变前目标： 训练 F(x) 逼近 H(x)改变后目标：训练 F(x) 逼近 H(x) -x 即增加一个identity mapping（恒等映射），将原始所需要学的函数H(x)转换成F(x)+x，而作者认为这两种表达的效果相同，但是优化的难度却并不相同，作者假设F(x)的优化 会比H(x)简单的多。这一想法也是源于图像处理中的残差向量编码，通过一个reformulation，将一个问题分解成多个尺度直接的残差问题，能够很好的起到优化训练的效果 在论文中尝试了 skip 2层或者 3层 一种解读方式 残差网络单元其中可以分解成右图的形式，从图中可以看出，残差网络其实是由多种路径组合的一个网络，直白了说，残差网络其实是很多并行子网络的组合，整个残差网络其实相当于一个多人投票系统（Ensembling） 从这可以看出其实ResNet是由大多数中度网络和一小部分浅度网络和深度网络组成的， 说明虽然表面上ResNet网络很深，但是其实起实际作用的网络层数并没有很深我们可以看出大多数的梯度其实都集中在中间的路径上，论文里称为effective path。 (网络越深，也是容易出现梯度消失或者梯度爆炸，这个是没有问题的)ResNet其实就是一个多人投票系统。 ps, 现在深度网络基本上分成两个方向，一个像 resnet 向着”深度“发展，一个向着”宽度“的inception network]]></content>
      <categories>
        <category>Deep learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[LeetCode-Others]]></title>
    <url>%2F2019%2F06%2F16%2Fleetcode-others%2F</url>
    <content type="text"><![CDATA[LeetCode 刷题总结（四），使用Python 实现。该篇题目类型主要包含无法归类到上述三篇的题目。 Reverse Integer Given a 32-bit signed integer, reverse digits of an integer. Input: 120 Output: 21 Input: 123 Output: 321 https://leetcode.com/problems/reverse-integer/ Tips： 这个只是reverse 操作，注意一些细节比如负号和数字 0的处理。对于最大数的表示 pow(2, 31)。 通过 % 求余获得最小位。 12345678910111213141516class Solution(object): def reverse(self, x): """ :type x: int :rtype: int """ result = 0 if x &lt; 0: symbol = -1 x = -x else: symbol = 1 while x: result = result * 10 + x % 10 x /= 10 return 0 if result &gt; pow(2, 31) else result * symbol String to Integer (atoi) Implement atoi which converts a string to an integer. Tips: 涉及到bit 级别数字处理的一般都会用到 res =res 10 + something 这样的东西。对于能够表示的数字的判断 max(-pow(2, 31), min(ressign, pow(2, 31) -1)) 这个还是挺经典的代码的。 https://leetcode.com/problems/string-to-integer-atoi/ 1234567891011121314151617181920212223class Solution(object): def myAtoi(self, str): """ :type str: str :rtype: int """ ls =list(str.strip()) if len(ls) == 0: return 0 sign = -1 if ls[0] == '-' else 1 index=0 # 有一个index 是贯穿始终的 res =0 if ls[index] in ['-', '+']: index +=1 for i in range(index, len(ls)): if ls[i].isdigit(): res =res *10 + ord(ls[i]) -ord('0') else: # case "words and 987" 是不能有 字母的 break return max(-pow(2, 31), min(sign*res, pow(2,31) -1)) Palindrome Number Determine whether an integer is a palindrome. An integer is a palindrome when it reads the same backward as forward. Tips： 回文数。代码写的很巧妙，整体上说是通过 / 和 % 获得数字的头和尾，在实现的时候有若干细节。 1234567891011121314151617181920class Solution(object): def isPalindrome(self, x): """ :type x: int :rtype: bool """ if x &lt; 0: return False ranger = 1 while x / ranger &gt;= 10: ranger *= 10 while x: left = x / ranger right = x % 10 if left != right: return False x = (x % ranger) / 10 ranger /= 100 return True Integer to Roman Roman numerals are represented by seven different symbols: I, V, X, L, C, D and M. Tips： 基于roman 数字规则的转换。代码实现角度 tuple都是要好于 list （内存和速度方面都是），如果你想要存储的是静态的可以遍历的数据，不需要每次进行修改的话，why not 123456789101112131415161718class Solution(object): def intToRoman(self, num): if num &lt;= 0: return "" digits = &#123; "I":1, "V":5, "X":10, "L":50, "C":100, "D":500, "M":1000 &#125; nums = (1000, 900, 500, 400, 100, 90, 50, 40, 10, 9, 5, 4, 1) chs = ("M", "CM", "D", "CD", "C", "XC", "L", "XL", "X", "IX", "V", "IV", "I") len = 13 s = "" while num &gt; 0: for i in range(0, len): if num &gt;= nums[i]: num -= nums[i] s += chs[i] break return s Roman to Integer Roman numerals are represented by seven different symbols: I, V, X, L, C, D and M. Tips： 和上一道题目相似。 1234567891011121314151617181920class Solution(object): def romanToInt(self, s): """ 这个是输入时 string，所以是 index 遍历的在 dict 中进行访问，但是上一个题目是总的 number，是没有办法的 """ digits = &#123; "I":1, "V":5, "X":10, "L":50, "C":100, "D":500, "M":1000 &#125; len_s = len(s) num = 0 # 这个少遍历了一个 ，因为其中有 i+1 的存在 for i in range(0, len_s - 1): cur = digits[s[i]] next_s = digits[s[i + 1]] if cur &gt;= next_s: num += cur else: num -= cur # 处理的是最后一个 num += digits[s[len_s - 1]] return num Letter Combinations of a Phone Number Given a string containing digits from 2-9 inclusive, return all possible letter combinations that the number could represent.A mapping of digit to letters (just like on the telephone buttons) is given below. Note that 1 does not map to any letters. Input: &quot;23&quot; Output: [&quot;ad&quot;, &quot;ae&quot;, &quot;af&quot;, &quot;bd&quot;, &quot;be&quot;, &quot;bf&quot;, &quot;cd&quot;, &quot;ce&quot;, &quot;cf&quot;]. Tips：使用循环的方式表示层级的关系（使用二层循环，第一层表示外围a， 第二层表示 def 等）。 1234567891011121314151617181920212223242526272829303132333435363738class Solution: def letterCombinations(self, digits): """ :type digits: str :rtype: List[str] """ if not digits or digits == "": return [] # 命名很到位 maps =&#123; '1': (), '0': (), '2': ('a', 'b', 'c'), '3': ('d', 'e', 'f'), '4': ('g', 'h', 'i'), '5': ('j', 'k', 'l'), '6': ('m', 'n', 'o'), '7': ('p', 'q', 'r', 's'), '8': ('t', 'u', 'v'), '9': ('w', 'x', 'y', 'z') &#125; results = [""] for digit in digits: tuple1 = maps[digit] tmp =[] if len(tuple1) == 0: continue # 二重循环 for prefix in results: for suffix in tuple1: tmp.append(prefix + suffix) results = tmp return results Divide Two Integers Given two integers dividend and divisor, divide two integers without using multiplication, division and mod operator.Return the quotient after dividing dividend by divisor.The integer division should truncate toward zero. Tips： 不让使用乘除和 mod 操作，只能使用位运算符了。在python 中0 == False 这个在逻辑判断中是等价的（1 ==True）。计算的时候使用 &lt;&lt; （不断的*2）, ,maybe 是二分 12345678910111213141516171819202122class Solution(object): def divide(self, divident, divisor): sign =-1 if divident* divisor&lt;0 else 1 divident, divisor =abs(divident), abs(divisor) ans =0 while divisor &lt;= divident: div =divisor tmp =1 while (div &lt;&lt;1) &lt;= divident: div &lt;&lt;= 1 tmp &lt;&lt;= 1 divident -= div ans += tmp return max(-pow(2, 31), min(ans*sign, pow(2, 31) -1)) Pow(x, n) Implement pow(x, n), which calculates x raised to the power n (xn) Tips： 简单的递归。 1234567891011121314151617181920class Solution(object): # 递归写起来比较好些，但是有时候比较难理解这个运行的过程 def myPow(self, x,n): """ :type x: float :type n: int :rtype: float """ if n ==0: return 1 # 求解pow() 都是正数，如果n &lt;0,那么需要做的是 取导数 elif n &lt;0: return 1.0/self.myPow(x, -n) else: half =self.myPow(x, n&gt;&gt;1) if n%2 ==0: return half *half else: return x *half*half N-Queens The n-queens puzzle is the problem of placing n queens on an n×n chessboard such that no two queens attack each other. Given an integer n, return all distinct solutions to the n-queens puzzle.Each solution contains a distinct board configuration of the n-queens’ placement, where ‘Q’ and ‘.’ both indicate a queen and an empty space respectively. Input: 4Output: [ [“.Q..”, // Solution 1 “…Q”, “Q…”, “..Q.”], [“..Q.”, // Solution 2 “Q…”, “…Q”, “.Q..”]]Explanation: There exist two distinct solutions to the 4-queens puzzle as shown above. Tips： N-皇后，行列对角线是不能出现重复。中规中矩的递归解法，board 是使用的一维向量， 这样去理解 比如board=[1, 3, 0, 2]，这是4皇后问题的一个解，意思是：在第0行，皇后放在第1列；在第1行，皇后放在第3列。 check函数表示 第k 个 皇后是否能够放在第j 个位置。 123456789101112131415161718192021222324252627class Solution(object): def __init__(self): self.board =[] def check(self, k,j): for i in range(k): # 如果之前的皇后已经放到了这个位置，或者两者在一条直线上，这个abs 用的比较牛逼 if self.board[i] ==j or abs(k -i) ==abs(self.board[i] -j): return False return True def dfs(self, depth, valuelist, n, res): if depth ==n: res.append(valuelist) return for i in range(n): if self.check(depth, i): self.board[depth] =i s ='.'*n self.dfs(depth +1, valuelist+[s[:i] +'Q'+s[i+1:]], n,res) def solveNQueens(self, n): self.board =[-1 for i in range(n)] res =[] self.dfs(0, [], n, res) return res N-Queens II The n-queens puzzle is the problem of placing n queens on an n×n chessboard such that no two queens attack each other. Tips: 这个相对于上一个要简单一些，因为最后的结果要的是 counts 而不是 list of path。按照道理讲是不用记录path 的。 123456789101112131415161718192021222324252627282930313233class Solution(object): def __init__(self): self.board =[] self.count =0 def check(self, k,j): """ check if the kth queen can be put in column j :param k: :param j: :return: """ for i in range(k): if self.board[i] ==j or abs(k -i) ==abs(self.board[i] -j): return False return True def dfs(self, depth, valuelist, n, res): if depth ==n: #res.append(valuelist) self.count +=1 return for i in range(n): if self.check(depth, i): self.board[depth] =i s ='.'*n self.dfs(depth +1, valuelist+[s[:i] +'Q'+s[i+1:]], n,res) def totalNQueens(self, n): self.board =[-1 for i in range(n)] res =[] self.dfs(0, [], n, res) return self.count Add Binary Given two binary strings, return their sum (also a binary string).The input strings are both non-empty and contains only characters 1 or 0. Tips：二级制相加，从后往前走，使用 carry 位置记录进位数。 12345678910111213141516171819202122class Solution(object): def addBinary(self, a, b): """ :type a: str :type b: str :rtype: str """ i, j, carry, res =len(a) -1, len(b) -1, 0, '' while i&gt;=0 or j &gt;=0 or carry: if i &gt;=0: carry += int(a[i]) i -=1 if j&gt;=0: carry += int(b[j]) j -=1 res =str(carry%2) +res carry //=2 return res Max Points on a Line Given n points on a 2D plane, find the maximum number of points that lie on the same straight line. Tips：当使用除法的时候，因为精度问题造成的误差，所以这里使用最大的公约数进行化简。使用以下三种方式处理。 Map from (a,b,c,d) representing y=(a/b)x+(c/d) to set of indices of points that are on that line. a/b and c/d are reduced, i.e. a and b are divided by their GCD and so are c and d. Vertical lines are represented by a tuple with 1 element, the x-axis value Single points are represented by a 2-tuple (x, y). 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647import collectionsimport mathdef fraction(x, y): if x &lt; 0: x, y = -x, -y gcd = math.gcd(x, y) return x // gcd, y // gcd class Solution: def maxPoints(self, points): if not points: return 0 if len(points) == 1: return 1 aligned_points = collections.defaultdict(set) duplicates = collections.defaultdict(set) for i, p in enumerate(points): for j, q in enumerate(points[i + 1:], start=i + 1): # x 是否相同 if q[0] == p[0]: if q[1] == p[1]: duplicates[i].add(j) key = tuple(p) else: key = (q[0],) else: a, b = fraction(q[1] - p[1], q[0] - p[0]) # k斜率 c, d = fraction(p[1] * q[0] - q[1] * p[0], q[0] - p[0]) # b 位移 key = (a, b, c, d) #aligned_points[key] = aligned_points[key] or &#123;i, j&#125; # 因为之前定义的是set aligned_points[key] |= &#123;i, j&#125; for p, dups in duplicates.items(): for key in aligned_points: if p in aligned_points[key]: #aligned_points[key] = aligned_points[key] or dups # 这个or 不是选择的意思，是两者都要的意思 aligned_points[key] |= dups max_points = 0 for aliged in aligned_points.values(): max_points = max(max_points, len(aliged)) return max_points]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode-Recursion]]></title>
    <url>%2F2019%2F06%2F16%2Fleetcode-recursion%2F</url>
    <content type="text"><![CDATA[LeetCode 刷题总结（三）， 使用Python 实现。该篇题目类型主要包括：recursion, iteration 和dynamic programming。 Regular Expression Matching Given an input string (s) and a pattern (p), implement regular expression matching with support for ‘.’ and ‘ * ‘ .‘.’ Matches any single character.‘*‘ Matches zero or more of the preceding element. Tips：典型的dp，二维数组是常见的方式。 如果看不懂注释，可以看这里 1234567891011121314151617181920212223242526272829303132333435class Solution(object): """ dp, dp[i][j] means the match status between p[:i] and s[:j] """ def isMatch(self, s, p): dp =[[False]*(len(s)+1) for _ in range(len(p) +1)] dp[0][0]= True # case, of when s is an empty string but p is not, # since each * can eliminate character before it for i in range(2, len(p)+1): dp[i][0] =dp[i-2][0] and p[i-1] =="*" for i in range(1, len(p)+1): for j in range(1, len(s)+1): if p[i-1] =='*': # elimination or propagations dp[i][j] =dp[i-2][j] or dp[i-1][j] # another case, propagations if p[i-2] ==s[j-1] or p[i-2] =='.': # 下面两种写法都是可以 # dp[i][j] = dp[i][j] or dp[i][j-1] dp[i][j] |= dp[i][j-1] else: # 对于and 这个语句就类似 if 语句, 下面两种写法都是可以的 #dp[i][j] =dp[i-1][j-1] and (p[i-1] ==s[j-1] or p[i-1] =='.') if p[i-1] ==s[j-1] or p[i-1] =='.': dp[i][j] =dp[i-1][j-1] return dp[-1][-1] Wildcard Matching Given an input string (s) and a pattern (p), implement wildcard pattern matching with support for ‘?’ and ‘*‘.‘?’ Matches any single character.‘*‘ Matches any sequence of characters (including the empty sequence). Input:s = “aa”p = “a”Output: falseExplanation: “a” does not match the entire string “aa”. Tips: Wildcard 通配符，这个和上一个基本相同啊， 1234567891011121314151617181920212223242526272829303132333435363738394041424344class Solution(object): """ 这个更加简单，while 就是能够搞定的，然后对于 特殊符号特殊判断。使用两个指针进行操作. s[i] ==p[j] 和 p[j] =='?' ，这个是可以放到同一个 if 条件下的。两者是等价的。 关键是 * 的匹配 2，在 p 中出现 * 时，记录 p 中 * 的位置，同时记录此时 s 的位置。 3，从 * 的后面的第一个字符开始匹配。如果匹配失败，返回 s 处，从 s++ 开始重新匹配。 """ def isMatch(self, s, p): """ :type s: str :type p: str :rtype: bool """ j = i = ss = 0; star = -1 # 首先把 string 中的字符比完 while i &lt; len(s): if j &lt; len(p) and (s[i] == p[j] or p[j] == '?'): i += 1; j += 1 continue # star 记录的是 j 的位置，相应的 ss 是记录的i (string) 中的位置 if j &lt; len(p) and p[j] == '*': star = j; j += 1; ss = i; continue # 如果已经有了 star 的出现， 到这里已经说明 star的下一个和 string 中的位置元素不是exact 的匹配 # 所以这里进行了 ss +=1 的操作是为了，相当于把 string 中的char 使用 * 进行了代替 # 好好理解一下 if star != -1: j = star + 1; ss += 1; i = ss continue return False # string 已经比较完了，如果只剩下 * 那么是可以行的，否则是不可行的 while j &lt; len(p) and p[j] == '*': j += 1 if j == len(p): return True return False Valid Parentheses Given a string containing just the characters ‘(‘, ‘)’, ‘{‘, ‘}’, ‘[‘ and ‘]’, determine if the input string is valid. Tips: 必须要使用 len(stack) 进行检测，因为中间的时候也可能 len(stack) 是等于0的，这时候只能是 append() ，不能访问 stack[-1] 123456789101112131415161718192021class Solution(object): def isMatch(self, l, r): return l =='[' and r==']' or l =='(' and r ==')' or l =='&#123;' and r =='&#125;' def isValid(self, s): len_s =len(s) if len_s ==0: return True stack =[] for ch in s: if len(stack) ==0 or not self.isMatch(stack[-1], ch): stack.append(ch) else: stack.pop() return len(stack) ==0 Generate Parentheses Given n pairs of parentheses, write a function to generate all combinations of well-formed parentheses. For example, given n = 3, a solution set is: [ “((()))”, “(()())”, “(())()”, “()(())”, “()()()”] Tips: dfs, left_count 表示是 ‘(‘ 的总数， left_remain 表示 left- right 的差值. 12345678910111213141516171819202122232425class Solution(object): def DogenerateParenthesis(self, n, left_count, left_remain, prefix): if n ==left_count and left_remain ==0: return [prefix] left =[] right =[] if left_count &lt;n: left =self.DogenerateParenthesis(n, left_count+1, left_remain+1, prefix+'(') if left_remain&gt;0: right =self.DogenerateParenthesis(n, left_count, left_remain-1, prefix+')') return left +right def generateParenthesis(self, n): """ :type n: int :rtype: List[str] """ if n ==0: return [] else: list =self.DogenerateParenthesis(n ,0, 0, "") return list Combination Sum Given a set of candidate numbers (candidates) (without duplicates) and a target number (target), find all unique combinations in candidates where the candidate numbers sums to target.The same repeated number may be chosen from candidates unlimited number of times. Tips: 这种找到所有符合题目要求的解，十之八九都是要使用递归。最优解（最值）一般是使用dp，减少子问题的运算。这里给出了 list 和dfs 的结合使用，通过传入 start index来解决是否遍历过的问题。 示意图： 12345678910111213141516171819202122class Solution(object): def dfs(self, candidates, target, start, intermedia, res): # target 这个变量调节了是 继续deeper or return， 每一次都是在变化的。如果 ==0，那么就return 了 if target ==0: res.append(intermedia) return for i in range(start, len(candidates)): if target &lt; candidates[i]: return self.dfs(candidates, target-candidates[i], i, intermedia+[candidates[i]], res) def combinationSum(self, candidates, target): """ :type candidates: List[int] :type target: int :rtype: List[List[int]] """ candidates.sort() res =[] self.dfs(candidates, target, 0, [], res) return res Combination Sum II Given a collection of candidate numbers (candidates) and a target number (target), find all unique combinations in candidates where the candidate numbers sums to target.Each number in candidates may only be used once in the combination. Tips： 这里给出了另外一种遍历list 和dfs 的方法，传入的是部分 list，上面那道题传入了完整的list。 123456789101112131415161718192021222324252627class Solution(object): # @param &#123;integer[]&#125; candidates # @param &#123;integer&#125; target # @return &#123;integer[][]&#125; def combinationSum2(self, candidates, target): candidates.sort() # 排序不影响 时间复杂度的，因为时间复杂度大于排序的时间复杂度 #res=set() res =[] self.findcombination(candidates,target,[],res) #return [list(i) for i in res] return res def findcombination(self,candidates,target,ls,res): if target==0 and ls not in res: # 对于 set() 中使用 add() ，list 中使用 append() #res.add(tuple(ls)) res.append(ls) return # 下面这个判断用和不用 都是相同的效果(时间和空间复杂度上) if target&lt;0: return # not use: c72 ms,11.7M for i in range(len(candidates)): if target&lt;candidates[i]: return self.findcombination(candidates[i+1:],target-candidates[i],ls+[candidates[i]],res) Permutations Given a collection of distinct integers, return all possible permutations. Input: [1,2,3]Output:[ [1,2,3], [1,3,2], [2,1,3], [2,3,1], [3,1,2], [3,2,1]] Tips : extend 是因为list of list ，而不是单独的list，这样能保证最后的结果还是 list of list 123456789101112131415161718192021class Solution(object): def permute(self, nums): return self.doPermute(nums) def doPermute(self, num_list): if len(num_list) ==1: return [num_list] res_list =[] for i in range(len(num_list)): num_list[0], num_list[i] =num_list[i], num_list[0] sub_list =self.doPermute(num_list[1:]) list_head =[num_list[0]] #new_list =list_head+ sub_list new_list = [list_head + list1 for list1 in sub_list] # 可以理解这个是 sub_list 是有一系列的解， 然后再每个解上都加上一个头元素 res_list.extend(new_list) # extend，The list.extend method extends a list by appending elements from an iterable # append 是当做一个整体进行操作 return res_list Permutations II Given a collection of numbers that might contain duplicates, return all possible unique permutations. Tips： 这个 duplicates 是通过 sort 函数，然后在选择 某个index 时候，进行判断一下是否和第一个重合，这样的方式去handle。 12345678910111213141516171819202122232425262728class Solution(object): def doPermuteUnique(self, nums): if len(nums) ==1: return [nums] res_list =[] for i in range(len(nums)): if i&gt;0 and nums[0] ==nums[i]: continue nums[0], nums[i] =nums[i], nums[0] sub_list =self.doPermuteUnique(nums[1:]) list_head =[nums[0]] new_list =[list_head +list1 for list1 in sub_list] res_list.extend(new_list) return res_list def permuteUnique(self, nums): """ :type nums: List[int] :rtype: List[List[int]] """ nums.sort() return self.doPermuteUnique(nums) Climbing Stairs You are climbing a stair case. It takes n steps to reach to the top.Each time you can either climb 1 or 2 steps. In how many distinct ways can you climb to the top?Note: Given n will be a positive integer. Tips： 数学题，斐波那契数列。 解法一： 12345678910111213141516171819202122class Solution(object): # 可以换成数学模型，发现就是 斐波那契数列 # 不使用数字，使用三个变量也是可以的额 def climbStairs(self, n): """ :type n: int :rtype: int """ if n ==1: return 1 elif n ==2: return 2 arr = [0] *(n+1) arr[1] =1 arr[2] =2 for i in range(3, n+1): arr[i] =arr[i-1] +arr[i-2] return arr[n] 解法二123456789101112def climbStairs(self, n): if n ==1: return 1 elif n ==2: return 2 a, b =1,2 c =0 for i in range(3, n+1): c = a+b a, b =b,c return c Combinations Given two integers n and k, return all possible combinations of k numbers out of 1 … n. Tips： 这个是处理的list 和 dfs()的问题，然后使用的传入 index和完整的 list 来控制进度。 12345678910111213141516171819202122class Solution(object): """ 好好理解递归这种逐渐加深的层次 """ def combine(self, n, k): res =[] self.dfs(list(range(1, n+1)), k, 0, [], res) return res def dfs(self, nums, k, index, path, res): # backtracking #if k &lt;0: #return # 这种 return 和result 结合使用的操作是经常常见的 if k ==0: res.append(path) return # 这个index 是很重要的， 在这个index 的基础上选择的 for i in range(index, len(nums)): self.dfs(nums, k-1, i +1, path+ [nums[i]], res) Subsets Given a set of distinct integers, nums, return all possible subsets (the power set). Tips: 使用的是第二种方式，传入部分list，从而由大问题转移成小问题。 12345678910111213141516class Solution(object): # 这种是最简单的深度优先的搜索了， def subsets(self, nums): res =[] self.dfs(nums, [], res) return res def dfs(self, nums, path, res): # 一般来说这个是有跳出条件，回溯的，但是这种情况是没有的，只有最后一个 # [[],[1],[1,2],[1,2,3],[1,3],[2],[2,3],[3]]， 当输出 [1, 2,3] 的时候，return，但是这个return 到了 [1, 3] 这个层次 res.append(path) for i in range(len(nums)): self.dfs(nums[i+1:], path+[nums[i]], res) Subsets II Given a collection of integers that might contain duplicates, nums, return all possible subsets (the power set). Tips： 这个含有duplicates，使用功能sort 然后在 for 循环 的时候进行判断一下。 123456789101112131415161718192021222324class Solution(object): # 递归 def subsetsWithDup(self, nums): """ :type nums: List[int] :rtype: List[List[int]] """ res =[] nums.sort() self.dfs(nums, 0, [], res) return res def dfs(self, nums, index, path, res): if path not in res: res.append(path) #res.append(path) for i in range(index, len(nums)): if i &gt; index and nums[i] ==nums[i-1]: continue self.dfs(nums, i+1, path+[nums[i]], res) # 下面的代码是错误的 memory 但是不知道为什么 Decode Ways A message containing letters from A-Z is being encoded to numbers using the following mapping: &apos;A&apos; -&gt; 1 &apos;B&apos; -&gt; 2 ... &apos;Z&apos; -&gt; 26 Given a non-empty string containing only digits, determine the total number of ways to decode it. Tips： 多少种解码方式。本质是裴波拉契数列, 感觉自己并没有get 到这个本质上是 该数列 1234567891011121314151617181920212223242526class Solution(object): """ DP[i] = DP[i-1] + DP[i-2] \ \___________(if str[i-2] exists and 10&lt;= int(str[i-1] + str[i]))&lt;=26 ) \___________(If str[i-1] exists and str[i] != '0' ) """ def numDecodings(self, s): """ :type s: str :rtype: int """ if not s: return 0 if s =='10': return 1 dp =[0] *(len(s) +1) dp[0] =1 for i in range(1, len(s)+1): if s[i-1] !='0': dp[i] +=dp[i-1] if i &gt;1 and '10' &lt;=s[i-2:i] &lt;='26': dp[i] += dp[i-2] return dp[-1] Binary Tree Inorder Traversal Given a binary tree, return the inorder traversal of its nodes’ values. Tips： 递归。 1234567891011121314151617# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution(object): # inorder 中序遍历, recursive 递归， iterative 迭代 # 这个是递归的 recursively def helper(self, root, res): if root: self.helper(root.left, res) res.append(root.val) self.helper(root.right, res) Tips： 递归的容易写，循环的也好会，使用的stack 先保存左子树，然后不断的node 其右子树。 12345678910111213141516def inorderTraversal(self, root): # 如果使用 迭代，那么就是 stack结构了 res, stack =[], [] while True: while root: stack.append(root) root =root.left if not stack: return res node =stack.pop() res.append(node.val) root =node.right Validate Binary Search Tree Given a binary tree, determine if it is a valid binary search tree (BST).Assume a BST is defined as follows: The left subtree of a node contains only nodes with keys less than the node’s key. The right subtree of a node contains only nodes with keys greater than the node’s key. Both the left and right subtrees must also be binary search trees. Tips： 二叉搜索树的特点，中序遍历，先得到遍历结果，然后判断是否是不减的（只是需要O(N)）. 1234567891011121314151617181920212223242526272829303132# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution(object): def isValidBST(self, root): """ :type root: TreeNode :rtype: bool """ output =[] self.inOrder(root, output) for i in range(1, len(output)): if output[i-1] &gt;= output[i]: return False return True def inOrder(self, root, output): if not root: return self.inOrder(root.left, output) output.append(root.val) self.inOrder(root.right, output) Same Tree Given two binary trees, write a function to check if they are the same or not.Two binary trees are considered the same if they are structurally identical and the nodes have the same value. Tips: 对应的值相同，对应的结构相同。 1234567891011121314151617181920212223242526# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution(object): # 根据上一一个题目的要求，这个是也是可以先进行遍历，然后再比较最后的遍历结果吗 def isSameTree(self, p, q): """ :type p: TreeNode :type q: TreeNode :rtype: bool """ if not p and not q: return True elif not p or not q: return False if p.val ==q.val: return self.isSameTree(p.left, q.left) and self.isSameTree(p.right, q.right) else: return False Symmetric Tree Given a binary tree, check whether it is a mirror of itself (ie, symmetric around its center). Tip： 对称和 same 是在于比较的方式是不一样的。 1234567891011121314151617181920212223242526272829303132# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution(object): # 该题目和 isSameTree 是有点相似的，只是修改部分代码就可以 def isSymmetric(self, root): """ :type root: TreeNode :rtype: bool """ # 这个如果是 [] 或者 None， 是返回true， 因为输入的形式是 list ，所以判断条件是 if root ==[], 这样形式 if not root: return True return self.dfs(root.left, root.right) def dfs(self, p, q): if not p and not q: return True elif not p or not q: return False if p.val == q.val: return self.dfs(p.left, q.right) and self.dfs(p.right, q.left) else: return False Binary Tree Zigzag Level Order Traversal Given a binary tree, return the zigzag level order traversal of its nodes’ values. (ie, from left to right, then right to left for the next level and alternate between). For example:Given binary tree [3,9,20,null,null,15,7], 3 / \ 9 20 / \ 15 7结果是这样的：[ [3], [20,9], [15,7]] Tips： 一行是从左往右，一行是从右往左。层序遍历的变体。从左向右使用 append() ，然后从右向左使用 insert()，这个是没有问题的。 12345678910111213141516171819202122232425262728# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution(object): # 层次遍历 + 奇偶性来决定是否 reverse def zigzagLevelOrder(self, root): """ :type root: TreeNode :rtype: List[List[int]] """ res = [] self.dfs(root, 0, res) return res def dfs(self, root, level, res): if root: if len(res) &lt; level + 1: res.append([]) if level % 2 == 0: res[level].append(root.val) else: res[level].insert(0, root.val) self.dfs(root.left, level+1, res) self.dfs(root.right, level+1, res) Maximum Depth of Binary Tree Given a binary tree, find its maximum depth.The maximum depth is the number of nodes along the longest path from the root node down to the farthest leaf node.Note: A leaf is a node with no children. Tips: 左右子树的max+1，这个是树的深度。 12345678910111213141516171819# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution(object): # 递归， 最简单的方式 def maxDepth(self, root): """ :type root: TreeNode :rtype: int """ if not root: return 0 # 这个是最简单的代码了 return 1 +max(self.maxDepth(root.left), self.maxDepth(root.right)) Binary Tree Level Order Traversal II Given a binary tree, return the bottom-up level order traversal of its nodes’ values. (ie, from left to right, level by level from leaf to root). Tips: 层序遍历，但是 res 需要存储成list of list，这样最后进行reverse，能够表示出 层数的信息。 1234567891011121314151617181920212223242526272829# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution(object): # 需要有一个 level的index， 然后翻转就行了 def levelOrderBottom(self, root): """ :type root: TreeNode :rtype: List[List[int]] """ res =[] self.dfs(root, 0, res) #res 这个是整体的导致，一层 element的倒置，不涉及 element内部的倒置 return res[::-1] def dfs(self, root, level, res): if root: if len(res) &lt; level+1: res.append([]) # 这个是append 一个空的 [] 这种结构，然后下面使用该 list；否则的话 直接进行append res[level].append(root.val) # 这个很重要哦 self.dfs(root.left, level+1, res) self.dfs(root.right, level +1, res) Convert Sorted Array to Binary Search Tree Given an array where elements are sorted in ascending order, convert it to a height balanced BST.For this problem, a height-balanced binary tree is defined as a binary tree in which the depth of the two subtrees of every node never differ by more than 1. Tips： 不减的array 就是 binary search tree 中的中序遍历的结果。递归思想，先要找到 root，然后划分左右子树。递归进行。 1234567891011121314151617181920212223242526# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution(object): # balance tree 这种是递归进行定义的，左右子树相差最多为1 # 主要是不太清楚 如何保证这种 balanced tree def sortedArrayToBST(self, nums): """ :type nums: List[int] :rtype: TreeNode """ if not nums: return None mid =len(nums)//2 root =TreeNode(nums[mid]) root.left =self.sortedArrayToBST(nums[:mid]) root.right =self.sortedArrayToBST(nums[mid+1:]) return root Balanced Binary Tree Given a binary tree, determine if it is height-balanced.For this problem, a height-balanced binary tree is defined as:a binary tree in which the depth of the two subtrees of every node never differ by more than 1. Tip：左右子树的差值不能大于1 为balanced，这个盘别题目，比较容易，重点是 getHeight() 的实现、根节点是 balanced，左右子树也是balanced 1234567891011121314151617181920212223242526# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution(object): # 使用了之前的 求解 树的height 的东西，然后使用定义进行做题 def isBalanced(self, root): """ :type root: TreeNode :rtype: bool """ if not root: return True return abs(self.getHeight(root.left) -self.getHeight(root.right))&lt;2 and self.isBalanced(root.left) and self.isBalanced(root.right) def getHeight(self, root): if not root: return 0 return 1 +max(self.getHeight(root.left), self.getHeight(root.right)) Minimum Depth of Binary Tree Given a binary tree, find its minimum depth.The minimum depth is the number of nodes along the shortest path from the root node down to the nearest leaf node.Note: A leaf is a node with no children. Tip：求 height的变形，如果是叶子节点，那么返回 1+max(left, right) 否则的话，返回左右子树中较小的高度。如果是求高度，那么就不管了什么情况下都是返回 max()+1 1234567891011121314151617181920212223# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution(object): # 求解根节点的左右子树的最小高度 # 如果左右子树都有，那么就是调用该函数， 如果有一个又，那么直接求解高度就行了 def minDepth(self, root): """ :type root: TreeNode :rtype: int """ if not root: return 0 if not root.left or not root.right: # 这个是求解高度的 return 1 +max(self.minDepth(root.left), self.minDepth(root.right)) else: return min(self.minDepth(root.left), self.minDepth(root.right))+1 Path Sum Given a binary tree and a sum, determine if the tree has a root-to-leaf path such that adding up all the values along the path equals the given sum.Note: A leaf is a node with no children. Tips: 有条件的dfs(), 有条件的进行树的路径，树在加深的同时，target 数字也是不断的减少，最后如果相等，那么就是一个合适的解。 12345678910111213141516171819202122232425262728293031323334353637383940# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution(object): # 树的路径 # 总结一下 any all 这种到自己的博客 """ any: Returns true if any of the items is True. It returns False if empty or all are false. Any can be thought of as a sequence of OR operations on the provided iterables. all: Returns true if all of the items are True (or if the iterable is empty). All can be thought of as a sequence of AND operations on the provided iterables. It also short circuit the execution i.e. stop the execution as soon as the result is known. """ def hasPathSum(self, root, sum): """ :type root: TreeNode :type sum: int :rtype: bool """ res =[] self.dfs(root, sum, res) return any(res) def dfs(self, root, target, res): if not root: return False # 对于叶子结点的定义 if not root.left and not root.right: if root.val == target: res.append(True) if root.left: self.dfs(root.left, target-root.val, res) if root.right: self.dfs(root.right, target-root.val, res) Path Sum II Given a binary tree and a sum, find all root-to-leaf paths where each path’s sum equals the given sum.Note: A leaf is a node with no children. Tips： 这个和上面的区别在于，一个是 true or false，一个find all paths，所以需要有一个变量去存储正确的路径。 12345678910111213141516171819202122232425262728293031# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution(object): # 上一道题目是 return true or false，这个是找到所有的路径 def pathSum(self, root, sum): """ :type root: TreeNode :type sum: int :rtype: List[List[int]] """ res =[] self.dfs(root, sum, [], res) return res def dfs(self, root, target, path, res): if not root: return [] if not root.left and not root.right: if root.val == target: res.append(path+[root.val]) if root.left: #这种条件是可以减少迭代的次数 self.dfs(root.left, target-root.val, path+[root.val], res) if root.right: self.dfs(root.right, target-root.val, path+[root.val], res) Flatten Binary Tree to Linked List Given a binary tree, flatten it to a linked list in-place. Tips： 比较有意思，将 tree 的左右子树 flatten 成 linked list的左右结点。其中的 self.pre 就类似一种全局变量，将整个遍历， 1234567891011121314151617181920212223242526272829# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution(object): # 这种 flatten 就是 拉平（先序遍历）， 然后转成linkedlist # 并且这种操作是要求 in-place的 def __init__(self): self.pre =TreeNode('dummy') def flatten(self, root): """ :type root: TreeNode :rtype: None Do not return anything, modify root in-place instead. """ if not root: return tmp =root.right # 这个保存下来，是为了下面的flatten 使用 self.pre.right =root self.pre.left =None self.pre =root self.flatten(root.left) self.flatten(tmp) Populating Next Right Pointers in Each Node You are given a perfect binary tree where all leaves are on the same level, and every parent has two children. The binary tree has the following definition: Tip，属于树的结构的优化，多了一个next 指针指向的是同层的右节点。这个树的操作一般是 in-place，所以在某个递归过程中 return 是不必return value，本生就是在修改。 123456789101112131415161718192021222324252627282930"""# Definition for a Node.class Node(object): def __init__(self, val, left, right, next): self.val = val self.left = left self.right = right self.next = next"""class Solution(object): # perfect binary tree, # 题目的要求， populate each next pointer to its next right node def helper(self, left, right): if not left or not right: return left.next = right # 三种关系，先后顺序是没有关系的 self.helper(left.left, left.right) self.helper(left.right, right.left) self.helper(right.left, right.right) def connect(self, root): if not root: return self.helper(root.left, root.right) return root Populating Next Right Pointers in Each Node II Given a binary tree struct Node { int val; Node left; Node right; Node *next; }Populate each next pointer to point to its next right node. If there is no next right node, the next pointer should be set to NULL. Tips：注意从图片上观察这一题和上一题的区别，这图中表民一个子树的左子树是可以指向另一个子树的右子树，说明这个明显类似层次遍历，而不像上一题那样。 123456789101112131415161718192021222324252627282930313233"""# Definition for a Node.class Node(object): def __init__(self, val, left, right, next): self.val = val self.left = left self.right = right self.next = next"""class Solution(object): # 这个是不太明白的 # 这个相对于上一道题目，只是少了 perfect binary tree, 翻译成中文，满二叉树（完美二叉树），包括最后一层都是满的 def connect(self, root): if root is None: return None queue = [root] while queue: prev,curr = None,None size = len(queue) # 有点类似层次遍历的意思 for i in range(size): curr = queue.pop(0) # 这个 if 只有在for 之内才是有效的，第一次是无效的 if prev : prev.next = curr if curr.left: queue.append(curr.left) if curr.right: queue.append(curr.right) prev = curr curr.next = None return root Binary Tree Maximum Path Sum Given a non-empty binary tree, find the maximum path sum.For this problem, a path is defined as any sequence of nodes from some starting node to any node in the tree along the parent-child connections. The path must contain at least one node and does not need to go through the root. Input: [-10,9,20,null,null,15,7] -10 / \ 9 20 / \ 15 7Output: 42 Tips： 这个的难点在于，可以从任意点开始，然后再任意点结束，并且过不过根节点都是可以的。 分制到底部，在返回的时候传入左右任意一遍最大值加上目前root.val:cur = max(left, right) + root.val 这种情况处理了从Root到左右任意一边的最大值，也就是 root.val + left 和 root.val + right； 还有一种情况就是当最大值 = root.val + left + right， 我们在放入global变量的时候何其比较。 对于最底部叶子节点传上来的值，我们将其设置成0: return cur if cur &gt; 0 else 0 12345678910111213141516171819202122232425262728# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution(object): # 根据以往的经验，树的递归解法一般都是递归到叶节点，然后开始边处理边回溯到根节点。 # 但是这个题目不是， 这个是可以任意 start， 任意 end，然后不一定要经过根节点 def maxPathSum(self, root): """ :type root: TreeNode :rtype: int """ # 使用 self 标志 就意味这个是一种全局的变量， 类似在 init 中进行初始化的 self.res = - float('inf') self.dfs(root) return self.res def dfs(self, root): if not root: return 0 left = self.dfs(root.left) right = self.dfs(root.right) self.res = max(self.res, left + right + root.val) cur = max(left, right) + root.val return cur if cur &gt; 0 else 0 Sum Root to Leaf Numbers Given a binary tree containing digits from 0-9 only, each root-to-leaf path could represent a number.An example is the root-to-leaf path 1-&gt;2-&gt;3 which represents the number 123.Find the total sum of all root-to-leaf numbers.Note: A leaf is a node with no children. Input: [1,2,3] 1 / \ 2 3Output: 25Explanation:The root-to-leaf path 1-&gt;2 represents the number 12.The root-to-leaf path 1-&gt;3 represents the number 13.Therefore, sum = 12 + 13 = 25. Tips: 路径组成的数字代表一个数字，然后所有的路径和相加起来。关键代码只要 cur =pre*10 + root.val， 还是树的路径的遍历吧。 123456789101112131415161718192021222324252627282930313233343536# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution(object): # 对于树的 类型，大概就是这样了， 递归，找出递归的跳出的条件，然后处理保存结果 def sumNumbers(self, root): """ :type root: TreeNode :rtype: int """ self.result =0 self.sumNum(root, 0) return self.result def sumNum(self, root, pre): if not root: return cur = pre *10 +root.val if not root.left and not root.right: self.result += cur return if root.left: self.sumNum(root.left, cur) if root.right: self.sumNum(root.right, cur) Binary Tree Preorder Traversal Given a binary tree, return the preorder traversal of its nodes’ values. Tips：非递归版本（迭代），使用栈（递归的思想就是栈的思想）。 1234567891011121314151617181920212223242526272829# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution(object): # Recursive solution is trivial, could you do it iteratively? def preorderTraversal(self, root): """ :type root: TreeNode :rtype: List[int] """ if not root: return [] res, queue =[], [root] while queue: cur =queue.pop() if cur: res.append(cur.val) queue.append(cur.right) queue.append(cur.left) #queue.append(cur.right) return res LRU Cache Design and implement a data structure for Least Recently Used (LRU) cache. It should support the following operations: get and put.get(key) - Get the value (will always be positive) of the key if the key exists in the cache, otherwise return -1.put(key, value) - Set or insert the value if the key is not already present. When the cache reached its capacity, it should invalidate the least recently used item before inserting a new item.The cache is initialized with a positive capacity. Tips：这个题目纯粹手解，太麻烦了，在python3 中有 collections.OrderedDict() 的实现，这是作弊的写法。特点在于dict +队列（不完全是队列，因为访问之后还会放到队列的最后，而不是弹出）。因为一般的dict 存储的时候是无序（不是按照放入的先后书序），ordereddict 是按照放入的先后顺序进行存储的。题目本身就是先进先出的队列，只不过存储的是 (key, value) 这样的键值对。使用get 的时候，get到一个不能删除，应该放到最后；put的时候在 OrderedDict.popitem()有一个可选参数last（默认为True），当last为True时它从OrderedDict中删除最后一个键值对并返回该键值对，当last为False时它从 OrderedDict中删除第一个键值对并返回该键值对。 123456789101112131415161718192021222324252627282930class LRUCache(object): # python3 environment def __init__(self, capacity): self.size =capacity self.cache = collections.OrderedDict() def get(self, key): if key not in self.cache: return -1 val =self.cache[key] self.cache.move_to_end(key) # Python &gt;= 3.2 return val def put(self, key, val): if key in self.cache: del self.cache[key] self.cache[key] =val if len(self.cache) &gt; self.size: self.cache.popitem(last= False) # Your LRUCache object will be instantiated and called as such:# obj = LRUCache(capacity)# param_1 = obj.get(key)# obj.put(key,value) Insertion Sort List Sort a linked list using insertion sort. Tips： linkedlist 擅长于修改元素（直接修改指向），其中的 if while 是经常搭配使用，发现.. 然后就处理… 1234567891011121314151617181920212223242526272829303132# Definition for singly-linked list.# class ListNode(object):# def __init__(self, x):# self.val = x# self.next = Noneclass Solution(object): # 前插 def insertionSortList(self, head): """ :type head: ListNode :rtype: ListNode """ p =dummy =ListNode(0) cur =dummy.next =head while cur and cur.next: next_val =cur.next.val if cur.val &lt;= next_val: cur =cur.next continue # the sequence is not sorted inorder # find the proper situation # 从头开始找 if p.next.val &gt; next_val: p =dummy while p.next.val &lt;= next_val: p =p.next p.next, cur.next.next, cur.next =cur.next, p.next, cur.next.next return dummy.next Sort List Sort a linked list in O(n log n) time using constant space complexity. Input: 4-&gt;2-&gt;1-&gt;3 Output: 1-&gt;2-&gt;3-&gt;4 Tips: mergesort 的思想，显示把list 分成left and right（分），然后最后merge 算法 1234567891011121314151617181920212223242526272829303132333435# Definition for singly-linked list.# class ListNode(object):# def __init__(self, x):# self.val = x# self.next = Noneclass Solution(object): def merge(self, h1,h2): dummy =tail =ListNode(-1) while h1 and h2: if h1.val &lt; h2.val: tail.next, h1 =h1, h1.next else: tail.next, h2 =h2, h2.next tail =tail.next tail.next =h1 or h2 return dummy.next def sortList(self, head): if not head or not head.next: return head pre, slow, fast =None, head, head # slow fast 直接是两种快慢的不影响的index 遍历方式，slow 是下一个链表的结点 while fast and fast.next: pre, slow, fast =slow, slow.next, fast.next.next pre.next =None # 下面的两种写法是等价的 return self.merge(self.sortList(head), self.sortList(slow)) # return self.merge(*map(self.sortList, (head, slow))) Number of Islands Given a 2d grid map of ‘1’s (land) and ‘0’s (water), count the number of islands. An island is surrounded by water and is formed by connecting adjacent lands horizontally or vertically. You may assume all four edges of the grid are all surrounded by water. Tips: 这个dfs 跟之前的不一样之处在于，需要对于每个点进行 dfs() ，其他的还好。提供了两种解法，第一种比较代码比较少。比较喜欢第一种代码的风格，这样两个函数看起来比较均衡。 https://leetcode.com/problems/number-of-islands/ 解法一：1234567891011121314151617181920212223242526272829class Solution(object): def numIslands(self, grid): """ :type grid: List[List[str]] :rtype: int """ if not grid: return 0 count =0 for i in range(len(grid)): for j in range(len(grid[0])): if grid[i][j] =='1': self.dfs(grid, i,j) # 写法比较巧妙 count +=1 return count def dfs(self, grid, i,j): if i &lt;0 or j&lt;0 or i&gt;=len(grid) or j&gt;= len(grid[0]) or grid[i][j] !='1': return grid[i][j] ='0' self.dfs(grid, i+1, j) self.dfs(grid, i-1, j) self.dfs(grid, i, j+1) self.dfs(grid, i, j-1) 解法二：两种思想一样，写法不一样。 12345678910111213141516171819202122232425262728293031323334353637383940class Solution(object): def numIslands(self, grid): """ :type grid: List[List[str]] :rtype: int """ if not grid: return 0 used =[ [False]* len(grid[0]) for _ in range(len(grid))] count =0 for i in range(len(grid)): for j in range(len(grid[0])): num =self.dfs(grid, used, len(grid)-1, len(grid[0])-1, i, j) if num&gt;0: count +=1 return count def dfs(self, grid, used, row, col, x, y): if grid[x][y] =='0' or used[x][y]: return 0 used[x][y] =True num =1 if x!=0: num += self.dfs(grid, used, row, col, x -1, y) if x !=row: num += self.dfs(grid, used, row, col, x +1,y) if y!=0: num += self.dfs(grid, used, row, col, x, y-1) if y !=col: num += self.dfs(grid, used, row, col, x, y+1) return num]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode-List]]></title>
    <url>%2F2019%2F06%2F16%2Fleetcode-list%2F</url>
    <content type="text"><![CDATA[LeetCode 刷题总结（二）， 使用Python 实现。该篇题目类型主要是： list, linkedList 还有简单的 tree。 Add Two Numbers You are given two non-empty linked lists representing two non-negative integers. The digits are stored in reverse order and each of their nodes contain a single digit. Add the two numbers and return it as a linked list. Input: (2 -&gt; 4 -&gt; 3) + (5 -&gt; 6 -&gt; 4)Output: 7 -&gt; 0 -&gt; 8Explanation: 342 + 465 = 807. Tips: 加法的过程，使用 \% 和 整除进行求解，使用linkedList 进行存储。https://leetcode.com/problems/add-two-numbers/ 123456789101112131415161718192021222324252627# Definition for singly-linked list.# class ListNode(object):# def __init__(self, x):# self.val = x# self.next = Noneclass Solution(object): def addTwoNumbers(self, l1, l2): """ :type l1: ListNode :type l2: ListNode :rtype: ListNode """ dummy = cur = ListNode(0) carry = 0 while l1 or l2 or carry: if l1: carry += l1.val l1 = l1.next if l2: carry += l2.val l2 = l2.next cur.next = ListNode(carry%10) cur = cur.next carry //= 10 return dummy.next Remove Nth Node From End of List Given a linked list, remove the n-th node from the end of list and return its head. Tips: 可以使用”两指针“ 方法进行求解，前后两指针相差 N步数。 12345678910111213141516171819202122232425262728293031323334353637# Definition for singly-linked list.# class ListNode(object):# def __init__(self, x):# self.val = x# self.next = Noneclass Solution(object): def removeNthFromEnd(self, head, n): """ :type head: ListNode :type n: int :rtype: ListNode """ if not head or not head.next: return None new_head =ListNode(-1) new_head.next =head fast =new_head # 先走n 步 for i in range(n): # 这里很好的处理了 n &gt; 链表长度 情况 if fast.next: fast =fast.next else: return head slow =new_head # 一块走 # 因为是 fast 先走到none，所以这个判断条件 while fast.next: fast =fast.next slow =slow.next slow.next =slow.next.next # 这个指向是经验性，还是超级nice的 return new_head.next Merge Two Sorted Lists Merge two sorted linked lists and return it as a new list. The new list should be made by splicing together the nodes of the first two lists. Tips: 归并操作中的”并“ 操作。 https://leetcode.com/problems/merge-two-sorted-lists/ 123456789101112131415161718192021222324252627282930313233343536373839404142# Definition for singly-linked list.# class ListNode(object):# def __init__(self, x):# self.val = x# self.next = Noneclass Solution(object): def mergeTwoLists(self, l1, l2): """ :type l1: ListNode :type l2: ListNode :rtype: ListNode """ if not l1 and not l2: return None elif not l1 or not l2: return l1 or l2 # 这里新建了 node，作为最后返回list 中的head，因为使用哪个子list 都是不确定的 new_node =ListNode(-1) cur =new_node head1 =l1 head2 =l2 while head1 and head2: if head1.val &lt; head2.val: cur.next =head1 head1 =head1.next else: cur.next =head2 head2 =head2.next cur =cur.next # 对于这种 if else 需要是相当的清楚 if head1: cur.next =head1 elif head2: cur.next =head2 return new_node.next Swap Nodes in Pairs Given a linked list, swap every two adjacent nodes and return its head.You may not modify the values in the list’s nodes, only nodes itself may be changed. Given 1-&gt;2-&gt;3-&gt;4, you should return the list as 2-&gt;1-&gt;4-&gt;3. Tips: 常见的类型，使用三个指针修改指向。经常创建 dummy指针，如果head 可能被改变的话。 1234567891011121314151617181920212223242526272829303132# Definition for singly-linked list.# class ListNode(object):# def __init__(self, x):# self.val = x# self.next = Noneclass Solution(object): """ 三个指针的修改，应该是没有问题的 """ def swapPairs(self, head): """ :type head: ListNode :rtype: ListNode """ if not head: return head dummy=pre =ListNode(-1) dummy.next =head while True: cur =pre.next if not cur: break nex =cur.next if not nex: break pre.next, cur.next, nex.next, pre =nex, nex.next, cur, cur return dummy.next Count and Say The count-and-say sequence is the sequence of integers with the first five terms as following: 1. 1 2. 11 3. 21 4. 1211 5. 111221 1 is read off as &quot;one 1&quot; or 11. 11 is read off as &quot;two 1s&quot; or 21. 21 is read off as &quot;one 2, then one 1&quot; or 1211. Tips: 这个是属于循环，字符串处理。 12345678910111213141516171819202122232425262728class Solution(object): def doCountAndSay(self, string): char =string[0] num =0 result ="" for c in string: if char ==c: num +=1 else: result += (str(num)+ char) char =c num =1 result += (str(num) +char) return result def countAndSay(self, n): if 0 ==n: return "" elif 1== n: return "1" result ='1' for i in range(1, n): result =self.doCountAndSay(result) return result Trapping Rain Water Given n non-negative integers representing an elevation map where the width of each bar is 1, compute how much water it is able to trap after raining. Tips: 算法比较巧妙，左右两边进行遍历找出”累计“最高点，在O(N) 时间内完成。能装的水，取决于左右两边( neighbor) 的小值- height[i]。 https://leetcode.com/problems/trapping-rain-water/ 1234567891011121314151617181920212223242526class Solution(object): def trap(self, height): if not height: return 0 len_h =len(height) leftmax=[0]* len_h max_h=0 for i in range(len_h): if height[i] &gt;max_h: max_h =height[i] leftmax[i] =max_h rightmax =[0] *len_h max_h =0 for i in range(len_h-1, -1, -1): if height[i]&gt; max_h: max_h =height[i] rightmax[i] =max_h result =0 for i in range(len_h): result += (min(leftmax[i], rightmax[i])- height[i]) return result Maximum Subarray Given an integer array nums, find the contiguous subarray (containing at least one number) which has the largest sum and return its sum. Input: [-2,1,-3,4,-1,2,1,-5,4], Output: 6 Explanation: [4,-1,2,1] has the largest sum = 6. Tips: 一维数组返回最大的子数组，字符串处理。https://leetcode.com/problems/maximum-subarray/ 1234567891011121314151617181920212223class Solution(object): def maxSubArray(self, nums): """ :type nums: List[int] :rtype: int """ len_n =len(nums) if len_n ==1: return nums[0] max_n =nums[0] # 如果提前初始化，那么之后在更新max_n 时候就不用进行 None 的判断了 sum_n =0 for num in nums: sum_n += num if sum_n&gt; max_n: max_n =sum_n if sum_n &lt;0: sum_n =0 # continue return max_n Insert Interval Given a set of non-overlapping intervals, insert a new interval into the intervals (merge if necessary).You may assume that the intervals were initially sorted according to their start times. Input: intervals = [[1,3],[6,9]], newInterval = [2,5] Output: [[1,5],[6,9]] Tips: 使用( 0, 1) 去区分 start, end 这种点，类似一种数据结构的样子, 排序之后结果的start index 和end index 分别出现在首段和尾段，使用栈进行存储 start index 就行了。https://leetcode.com/problems/insert-interval/ 123456789101112131415161718192021222324252627282930313233343536class Solution(object): def insert(self, intervals, newinterval): """ 这个每个都是有 数据类型的，这个就是 LeetCode 的优点 :type intervals: List[List[int]] :type newInterval: List[int] :rtype: List[List[int]] """ if not intervals: return [newinterval] datas =[] if intervals: datas.append((newinterval[0], 0)) datas.append((newinterval[1], 1)) for interval in intervals: datas.append((interval[0], 0)) datas.append((interval[1], 1)) datas.sort() # sort() 是一个骚操作, 默认的排序显示根据 tuple[0] 进行排序，如果相同，那么根据 tuple[1] 进行排序 # 所以排在前面的一定是 start index。 merged =[] stack =[datas[0]] for i in range(1, len(datas)): data =datas[i] if data[1] ==0: stack.append(data) elif data[1] ==1: if stack: start =stack.pop() if len(stack) ==0: # 这个时候 data 是 end point merged.append((start[0], data[0])) return merged Rotate List Given a linked list, rotate the list to the right by k places, where k is non-negative. Tips： 细节在于k 可能大于 list 的长度。 123456789101112131415161718192021222324252627282930313233343536# Definition for singly-linked list.# class ListNode(object):# def __init__(self, x):# self.val = x# self.next = Noneclass Solution(object): def rotateRight(self, head, k): if k ==0: return head if not head: return head # 头指针 dummy =ListNode(-1) dummy.next =head p =dummy count =0 while p.next: p =p.next count +=1 # 指向了头指针，连成了一个环，下一步开始找头指针 p.next =dummy.next step =count -(k% count) for i in range(step): p =p.next # 找到了头指针，那么下一个就是尾指针 head =p.next p.next =None return head Unique Paths A robot is located at the top-left corner of a m x n grid (marked ‘Start’ in the diagram below).The robot can only move either down or right at any point in time. The robot is trying to reach the bottom-right corner of the grid (marked ‘Finish’ in the diagram below).How many possible unique paths are there? Tips： 最后求解的是unique paths 的数量，而不是具体的路径，所以可以不使用 dp，成了一道模拟排列组合的数学题。 12345678910111213141516171819202122class Solution(object): # 这个总的步数是一定的 m+n -1 ，然后下行和右行也是一定的， # 所以这个是模拟的“排列组合” 的思想 def uniquePaths(self, m, n): """ :type m: int :type n: int :rtype: int """ if m ==0 or n ==0: return 1 up =1 # 这个是分子 for i in range(m+n-2, n -1, -1): up *=i down =1 # 这个是分母 for j in range(1, m): down *= j return up/down Sort Colors Given an array with n objects colored red, white or blue, sort them in-place so that objects of the same color are adjacent, with the colors in the order red, white and blue.Here, we will use the integers 0, 1, and 2 to represent the color red, white, and blue respectively. Tips: 快排思想， 中间数字1 当做key index，左右两边分别是left，right index。 1234567891011121314151617class Solution(object): """ 0, 1, 2 (red, white, blue) """ def sortColors(self, nums): # zero and r record the position of "0" and "2" respectively index, two, zero = 0, len(nums) - 1, 0 while index &lt;= two: if nums[index] == 0: nums[index], nums[zero] = nums[zero], nums[index] index += 1; zero += 1 elif nums[index] == 2: nums[index], nums[two] = nums[two], nums[index] two -= 1 else: index += 1 Remove Duplicates from Sorted List II Given a sorted linked list, delete all nodes that have duplicate numbers, leaving only distinct numbers from the original list. Tips: 常规题，经常出现这样的逻辑， if… while ，如果发现有重复的，那么一直就找到不重复为止。 12345678910111213141516171819202122232425262728# Definition for singly-linked list.# class ListNode(object):# def __init__(self, x):# self.val = x# self.next = Noneclass Solution(object): """ 就是在删除节点的时候，如果head 节点也得删除，这个时候 常常创建一个 dummy 结点 求解的是distinct 的list """ def deleteDuplicates(self, head): dummy =pre =ListNode(0) dummy.next =head while head and head.next: if head.val ==head.next.val: while head and head.next and head.val ==head.next.val: head =head.next head =head.next pre.next =head else: # 这个更新很有意思， head =head.next, pre.next =head pre =pre.next head =head.next return dummy.next Partition List Given a linked list and a value x, partition it such that all nodes less than x come before nodes greater than or equal to x.You should preserve the original relative order of the nodes in each of the two partitions. Tips: 新建了两个结点，分别连接小于 x 和不小于 x 的结点，最后两个结点相连。 list 是直接进行交换位置，但是linkedList 不是这样的。 123456789101112131415161718192021222324252627282930# Definition for singly-linked list.# class ListNode(object):# def __init__(self, x):# self.val = x# self.next = Noneclass Solution(object): # 局部排序，不是全局排序 def partition(self, head, x): """ :type head: ListNode :type x: int :rtype: ListNode """ small =l1 =ListNode(0) great =l2 =ListNode(0) while head: if head.val &lt;x : l1.next =head l1 =l1.next else: l2.next =head l2 =l2.next head =head.next # 这个是一个细节， 最后l2 是需要一个none 进行结束标记 l2.next =None l1.next =great.next return small.next Reverse Linked List II Reverse a linked list from position m to n. Do it in one-pass.Note: 1 ≤ m ≤ n ≤ length of list. Tips: 局部进行reverse，找到该节点，然后迭代进行就可以了。 1234567891011121314151617181920212223242526# Definition for singly-linked list.# class ListNode(object):# def __init__(self, x):# self.val = x# self.next = Noneclass Solution(object): # The idea is simple and intuitive: find linkedlist [m, n], reverse it, then connect m with n+1, connect n with m-1 def reverseBetween(self, head, m, n): pre =dummy =ListNode(0) dummy.next =head #cur, pre =head, dummy for _ in range(m-1): #cur =cur.next pre =pre.next cur =pre.next for _ in range(n-m): tmp =cur.next cur.next =tmp.next tmp.next =pre.next pre.next =tmp return dummy.next Unique Binary Search Trees Given n, how many structurally unique BST’s (binary search trees) that store values 1 … n? Tips: 从处理子问题的角度来看，选取一个结点为根，就把结点切成左右子树，以这个结点为根的可行二叉树数量就是左右子树可行二叉树数量的乘积，所以总的数量是将以所有结点为根的可行结果累加起来。 123456789101112131415161718class Solution(object): # 二叉搜索树，当且仅当中序遍历的时候是单调非减的时候。 # 数学问题 def numTrees(self, n): """ :type n: int :rtype: int """ arr =[0]*(n+1) arr[0] =1 for i in range(1, n+1): for j in range(1, i+1): # 处理的是左右子树的乘积 arr[i] += arr[j-1] *arr[i-j] return arr[-1] Best Time to Buy and Sell Stock Say you have an array for which the ith element is the price of a given stock on day i.If you were only permitted to complete at most one transaction (i.e., buy one and sell one share of the stock), design an algorithm to find the maximum profit.Note that you cannot sell a stock before you buy one. Tips: 虽然这个说是最多一次买入卖出，但是这个价格变化是”连续“的，所以只要是下一个大于上一个就是可以 += profit 中的。 12345678910111213141516class Solution(object): # 从代码上来看，毫无算法可言 def maxProfit(self, prices): """ :type prices: List[int] :rtype: int """ if not prices or len(prices) ==1: return 0 profit =0 for i in range(1, len(prices)): if prices[i] &gt; prices[i-1]: profit += prices[i]-prices[i-1] return profit Best Time to Buy and Sell Stock II Say you have an array for which the ith element is the price of a given stock on day i.Design an algorithm to find the maximum profit. You may complete as many transactions as you like (i.e., buy one and sell one share of the stock multiple times).Note: You may not engage in multiple transactions at the same time (i.e., you must sell the stock before you buy again). Tips: 可以进行多次买卖。和上面的区别在于 下一个只要不小于上一个就是可以累加的。 123456789101112131415class Solution(object): # 可以多次买卖， 买一次然后卖一次。不能多次买入 # 这种就如同寻找的是 增序列。 def maxProfit(self, prices): """ :type prices: List[int] :rtype: int """ total =0 for i in range(1, len(prices)): if prices[i]&gt;= prices[i-1]: total += prices[i]-prices[i-1] return total Longest Consecutive Sequence Given an unsorted array of integers, find the length of the longest consecutive elements sequence.Your algorithm should run in O(n) complexity. Tips： 这种方法很巧妙，如果x-1 not in，那么去 try x+1，然后计数。 123456789101112131415161718class Solution(object): # 第一印象是 先进行排序， 然后选择的过程， 但是限制条件是 O(n) 的复杂度 def longestConsecutive(self, nums): """ :type nums: List[int] :rtype: int """ nums =set(nums) best =0 for x in nums: if x-1 not in nums: y =x+1 while y in nums: y +=1 best =max(best, y-x) return best Candy There are N children standing in a line. Each child is assigned a rating value.You are giving candies to these children subjected to the following requirements: Each child must have at least one candy. Children with a higher rating get more candies than their neighbors.What is the minimum candies you must give? Tips: 因为涉及到 neighbors，所以左右两边进行遍历，因为如果ratings 大的话，那么结果一定得大，所以返回的是较大者。 1234567891011121314151617181920class Solution(object): def candy(self, ratings): """ :type ratings: List[int] :rtype: int """ # 满足第一个条件，at least one candy res =len(ratings) *[1] # left to right, higher then more candies for i in range(1, len(ratings)): if ratings[i] &gt; ratings[i-1]: # 这个是严格的 &gt; res[i] = res[i-1] +1 # right to left, higher then more candy, neighbors for i in range(len(ratings)-1, 0, -1): if ratings[i-1] &gt; ratings[i]: res[i-1] =max(res[i-1], res[i] +1) return sum(res) Single Number Given a non-empty array of integers, every element appears twice except for one. Find that single one. Tips: 异或的性质 1234567891011121314class Solution(object): # 分分钟 异或就出来了 # integers, -2 -1 0 1 2 这样的数字 def singleNumber(self, nums): """ :type nums: List[int] :rtype: int """ standard =0 # 如果正好是其中的 0 出现一次，也是没有关系的， 因为初始化的 0 和 list 中的单数 0 正好匹配，异或操作之后相同为 0 # 结果上是没有什么问题的 for num in nums: standard = num^ standard return standard Single Number II Given a non-empty array of integers, every element appears three times except for one, which appears exactly once. Find that single one. Tips：这个 three times不能使用 异或，从二进制的角度进行考虑，以二进制的形式，将数字存储起来，如果是出现了 3次，那么 %3 结果就是0，最后只是剩下了 那个出现一次的数字 1234567891011121314151617181920212223242526class Solution(object): # 只是出现的一个的single one， 其他的出现三次 # var |= value is short for var = var | value def singleNumber(self, nums): """ :type nums: List[int] :rtype: int """ bit = [0] * 32 for num in nums: for i in range(32): bit[i] += num &gt;&gt; i &amp; 1 # 这个就是从左往右的顺序，先是进行 &gt;&gt; 运算，然后是 &amp; 运算 # 可以想象这个重复计算比较多，因为每次都需要 num &gt;&gt; i 进行位运算 res = 0 for i, val in enumerate(bit): # if the single numble is negative, # this case should be considered separately , 补码 和原码的转换关系 if i == 31 and val % 3: res = -((1 &lt;&lt; 31) - res) else: res |= (val % 3) * (1 &lt;&lt; i) # | 这个是位操作，更加类似不断的取 1 的过程， 然后和该位置的权重相乘 return res Copy List with Random Pointer A linked list is given such that each node contains an additional random pointer which could point to any node in the list or null.Return a deep copy of the list. Tips：在剑指offer 上是通过 指针操作进行做题，但是使用 defaultdict 基本上就不用出，使用dict 来处理这种关系，最后返回的是根节点、 123456789101112131415161718192021222324252627282930"""# Definition for a Node.class Node(object): def __init__(self, val, next, random): self.val = val self.next = next self.random = random"""# 使用dict 有没有感觉在作弊class Solution(object): # 做过这个 def copyRandomList(self, head): """ :type head: Node :rtype: Node """ import ipdb dic = collections.defaultdict(lambda: Node(0, None, None)) # 这个就是给定了一个默认的值, 直接初始化dict 中的value 为这个node # dict 的本身就是存储一种node 的关系，所以dict[n].val , next, random 可以这样进行操作 dic[None] = None n = head while n: dic[n].val = n.val dic[n].next = dic[n.next] dic[n].random = dic[n.random] n = n.next #ipdb.set_trace() return dic[head] Linked List Cycle Given a linked list, determine if it has a cycle in it.To represent a cycle in the given linked list, we use an integer pos which represents the position (0-indexed) in the linked list where tail connects to. If pos is -1, then there is no cycle in the linked list. Tips：快慢两个指针的问题，给了两种方法来实现。 1234567891011121314151617181920212223242526272829303132333435# Definition for singly-linked list.# class ListNode(object):# def __init__(self, x):# self.val = x# self.next = Noneclass Solution(object): # O(1) == constant memory, def hasCycle(self, head): """ :type head: ListNode :rtype: bool """ ''' # 这种不用进行判断 fast 是否可以访问的原因在于 try except 的使用 try: slow = head fast = head.next while slow is not fast: slow = slow.next fast = fast.next.next return True except: return False ''' # 这个是比较中规中矩的写法 slow = fast = head # 注意使用的是 fast进行判断，因为这个走的快 while fast and fast.next: fast = fast.next.next slow = slow.next if slow == fast: return True return False Reorder List Given a singly linked list L: L0→L1→…→Ln-1→Ln,reorder it to: L0→Ln→L1→Ln-1→L2→Ln-2→…You may not modify the values in the list’s nodes, only nodes itself may be changed. Tips： 从中间断开，后半部分翻转，然后和前半部分轮流连接 12345678910111213141516171819202122232425262728293031323334353637383940414243# Definition for singly-linked list.# class ListNode(object):# def __init__(self, x):# self.val = x# self.next = Noneclass Solution(object): def reorderList(self, head): """ :type head: ListNode :rtype: None Do not return anything, modify head in-place instead. """ if not head: return fast, slow = head.next, head # first part has the same or one more node while fast and fast.next: fast = fast.next.next slow = slow.next # reverse the send half p = slow.next slow.next = None node = None # 类似上一个结点， p 是cur 的结点 while p: nex = p.next p.next = node node = p p = nex # combine head part and node part p = head while node: tmp = node.next node.next = p.next # 两个 next 指向操作, 需要next 两次 p.next = node p =p.next.next node = tmp]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode- Array]]></title>
    <url>%2F2019%2F06%2F16%2Fleetcode-array%2F</url>
    <content type="text"><![CDATA[LeetCode 刷题总结（一）， 使用Python 实现。该篇题目类型主要是： array 和string 的相关处理。 Two Sum Given an array of integers, find two numbers such that they add up to a specific target number. Tips： 返回的是 index，所以 dict 中存储的 (num, index) 这样的组合, 是两个不相同的数字的index https://leetcode.com/problems/two-sum/ 12345678910111213class Solution(object): # 不能重复使用一个，return index def twoSum(self, nums, target): """ :type nums: List[int] :type target: int :rtype: List[int] """ cache = &#123;&#125; for i in range(len(nums)): if target - nums[i] in cache and cache[target - nums[i]] != i: # if else 用的是比较简洁的 return [cache[target - nums[i]], i] cache[nums[i]] = i Median of Two Sorted Arrays There are two sorted arrays nums1 and nums2 of size m and n respectively. Find the median of the two sorted arrays. The overall run time complexity should be O(log (m+n)). Tips: 先是 merge，然后选择 median，常规做法，时间0(mn)，不是最优的，还可以达到O(min(m, n) ) 这样的时间复杂度 123456789101112131415161718192021222324252627282930313233343536373839class Solution(object): # 使用 merge() 操作，然后根据，然后取得中位数 def median(self, nums): len_n =len(nums) if len_n &amp;1 ==1: return nums[len_n//2] else: return float(nums[len_n//2]+nums[len_n//2-1])/2 def findMedianSortedArrays(self, nums1, nums2): """ :type nums1: List[int] :type nums2: List[int] :rtype: float """ res =[] if not nums1 or not nums2: res =nums1 or nums2 return self.median(res) else: left, right =0, 0 len_1, len_2 =len(nums1)-1, len(nums2)-1 while left&lt;= len_1 and right &lt;=len_2: if nums1[left] &lt;nums2[right]: res.append(nums1[left]) left +=1 else: res.append(nums2[right]) right +=1 if left &lt;= len_1: res.extend(nums1[left:]) if right &lt;=len_2: res.extend(nums2[right:]) return self.median(res) ZigZag Conversion The string “PAYPALISHIRING” is written in a zigzag pattern on a given number of rows like this: (you may want to display this pattern in a fixed font for better legibility) P A H NA P L S I I GY I R And then read line by line: “PAHNAPLSIIGYIR” Tips: 字符串处理 12345678910111213141516171819202122class Solution(object): def convert(self, s, numRows): """ :type s: str :type numRows: int :rtype: str """ if numRows == 1 or numRows &gt;= len(s): # string of list return s L = [''] * numRows # string of list row, step = 0, 1 for x in s: L[row] += x if row == 0: step = 1 elif row == numRows -1: step = -1 row += step return ''.join(L) # array (list) 转成string 常用的方法 Container With Most Water Given n non-negative integers a1, a2, …, an , where each represents a point at coordinate (i, ai). n vertical lines are drawn such that the two endpoints of line i is at (i, ai) and (i, 0). Find two lines, which together with x-axis forms a container, such that the container contains the most water. The above vertical lines are represented by array [1,8,6,2,5,4,8,3,7]. In this case, the max area of water (blue section) the container can contain is 49. Tips: 左右双指针问题。首先移动高度较小的点，因为两者的距离是肯定变小，如果移动高度大的，那么最后的面积肯定变小；但是如果 移动高度较小的点，那么最后的面积是有可能变大的。所以这个是一个可能性的东西。 1234567891011121314151617181920212223class Solution(object): def maxArea(self, height): len_h = len(height) if len_h == 1: return 0 max_area = 0 left = 0 right = len_h - 1 # left, right =0, len_h -1 while left &lt; right: if height[left] &lt; height[right]: area = (right - left) * height[left] left += 1 else: area = (right - left) * height[right] right -= 1 if area &gt; max_area: max_area = area return max_area Longest Common Prefix Write a function to find the longest common prefix string amongst an array of strings.If there is no common prefix, return an empty string “”. Input: [&quot;flower&quot;,&quot;flow&quot;,&quot;flight&quot;] Output: &quot;fl&quot; Tips: 一个个寻找交集，最朴素的想法、 123456789101112131415161718192021222324252627282930313233class Solution(object): # 时间复杂度是 O(N^2), N=len(strs), 笼统的说 def longestCommonPrefix(self, strs): """ :type strs: List[str] :rtype: str """ if not strs: return "" n =len(strs) if n ==0: return "" elif n ==1: return strs[0] predix =strs[0] for s in strs[1:]: # 这种结构见过了，就是不断迭代，不断地的去寻找 ”交集“ predix =self.findPrefix(predix, s) if "" ==predix: break return predix def findPrefix(self, s1, s2): min_len =min(len(s1), len(s2)) # 这个if 和 return 写的都是很巧妙的 for i in range(0, min_len): if s1[i] != s2[i]: return s1[:i] return s1[:min_len] Remove Duplicates from Sorted Array Given a sorted array nums, remove the duplicates in-place such that each element appear only once and return the new length.Do not allocate extra space for another array, you must do this by modifying the input array in-place with O(1) extra memory. Tips: 前后两个指针覆盖的思想，最后返回的是index，如果发现了后者覆盖前者 12345678910111213141516171819202122class Solution(object): def removeDuplicates(self, nums): """ :type nums: List[int] :rtype: int """ if None == nums: return 0 len_n = len(nums) if len_n &lt;= 1: return len_n m = 0 n = 1 while n &lt; len_n: if nums[m] != nums[n]: m += 1 if m != n: nums[m] = nums[n] n += 1 return m + 1 Remove Element Given an array nums and a value val, remove all instances of that value in-place and return the new length.Do not allocate extra space for another array, you must do this by modifying the input array in-place with O(1) extra memory. Tips: in-place 表示不能创建数组，可以使用（临时）变量。通过双指针进行处理，想想为什么可以使用这么简洁的代码进行处理呢。m n 分别从左到右、从右到左进行遍历，将和 val 相同的元素都放到右边，不相同的放到左边 1234567891011121314151617181920212223242526class Solution(object): def removeElement(self, nums, val): """ :type nums: List[int] :type val: int :rtype: int """ if not nums: return 0 len_n =len(nums) m,n =0, len_n-1 # 注意跳出条件，遍历的方向和跳出条件是相关的, 这个 等号取于不取 一是比较难把握，可以具体带个值 while m &lt;=n: if val ==nums[m]: if val !=nums[n]: # 这个是不能使用 while 找，因为有比较多的case 需要考虑，所以使用 if 进行单步操作 nums[m], nums[n] =nums[n], nums[m] m +=1 n -=1 else: n -=1 else: m +=1 return m # 因为 m 是从0开始的 Search in Rotated Sorted Array Suppose an array sorted in ascending order is rotated at some pivot unknown to you beforehand.(i.e., [0,1,2,4,5,6,7] might become [4,5,6,7,0,1,2]).You are given a target value to search. If found in the array return its index, otherwise return -1. Tips：一定是 二分思想，关键是判断 增序列 和 target 的关系，所以有两层 if 判断条件，一层是增序列， 一层是 target 是否在增序列下面这个观点是要有的： 整个数组由两个有序的子序列构成，且左子序列中的每个元素都&gt;,右子序列中的每个元素。 123456789101112131415161718192021222324class Solution(object): # @param nums, a list of integers # @param target, an integer to be searched # @return an integer def search(self, nums, target): left = 0; right = len(nums) - 1 # 这个 == 是不容易进行取舍的， while left &lt;= right: mid = (left + right) / 2 if target == nums[mid]: return mid # 我当时面试的时候就是这种思路，一定要有条理就是了 # 左边是增序列 if nums[mid] &gt;= nums[left]: if target &lt; nums[mid] and target &gt;= nums[left]: right = mid - 1 else: left = mid + 1 elif nums[mid] &lt; nums[right]: if target &gt; nums[mid] and target &lt;= nums[right]: left = mid + 1 else: right = mid - 1 return -1 Search Insert Position Given a sorted array and a target value, return the index if the target is found. If not, return the index where it would be if it were inserted in order.You may assume no duplicates in the array. Tips： 二分查找，之前是found or not found，现在如果没有找见返回的是 index，没有什么本质区别。 1234567891011121314151617181920212223242526272829class Solution(object): def searchInsert(self, nums, target): """ :type nums: List[int] :type target: int :rtype: int """ if not nums: return None len_n =len(nums) if nums[0] &gt; target: return 0 if nums[-1] &lt; target: return len_n left, right =0, len_n-1 while left&lt;=right: mid =(left +right)/2 if nums[mid] ==target: return mid elif nums[mid] &lt;target: left =mid +1 else: right =mid -1 return left Rotate Image You are given an n x n 2D matrix representing an image.Rotate the image by 90 degrees (clockwise). Given input matrix =[ [1,2,3], [4,5,6], [7,8,9]],rotate the input matrix in-place such that it becomes:[ [7,4,1], [8,5,2], [9,6,3]] Tips: 好好观察四个等式，就是行变列，然后其他的一个坐标是对称的，这个就是旋转 90度；然后有五个变量（有重复的） 1234567891011 class Solution(object): def rotate(self, matrix): n = len(matrix) if 1 == n: return round = int(n / 2) for x in range(0, round): for y in range(x, n - x - 1): matrix[n - y - 1][x], matrix[n - x - 1][n - y - 1], matrix[y][n - x - 1], matrix[x][y] = matrix[n - x - 1][n - y - 1], matrix[y][n - x - 1], matrix[x][y], matrix[n - y - 1][x] Spiral Matrix Given a matrix of m x n elements (m rows, n columns), return all elements of the matrix in spiral order.螺旋形 Input:[ [ 1, 2, 3 ], [ 4, 5, 6 ], [ 7, 8, 9 ]]Output: [1,2,3,6,9,8,7,4,5] Tips: 在处理”行“ 信息的时候，是可以数组切割的。在处理列信息的时候，需要一个个append() 123456789101112131415161718192021222324252627282930313233343536class Solution(object): # 这个不是跟那个 剑指offer 中的顺时针打印输出一样吗， # 这个版本是比较容易理解，所以选择这个版本 def spiralOrder(self, matrix): if matrix ==[]: return [] top, bottom =0, len(matrix)-1 left, right =0, len(matrix[0])-1 res =[] # 当有一个等于的时候就应该跳出来了 while left &lt;right and top&lt; bottom: # 对于行 处理python 是有比较简单的方式的 res += matrix[top][left: right+1] for x in range(top+1, bottom): res.append(matrix[x][right]) res += matrix[bottom][left:right+1][::-1] # 倒叙 for x in range(bottom-1, top, -1): res.append(matrix[x][left]) top, bottom, left, right =top+1, bottom-1, left+1, right-1 if top ==bottom: res += matrix[top][left:right] elif left ==right: for x in range(top, bottom+1): res.append(matrix[x][right]) return res Spiral Matrix II Given a positive integer n, generate a square matrix filled with elements from 1 to $n^2$ in spiral order. Tips : 注意边角的细节。初始化赋值的应该是常见的操作，这里的cur 是比较核心的东西。 123456789101112131415161718192021222324252627282930313233343536class Solution(object): # version 1 是遍历获取， version 2 是填充。这个真是有趣的东西 # 还是设置上下左右四个坐标进行遍历的处理 def generateMatrix(self, n): ans =[ [0] *n for _ in range(n)] top, bottom, left, right =0, n-1, 0, n-1 cur =1 while left &lt;= right and top &lt;= bottom: for i in range(left, right+1): ans[top][i] =cur cur +=1 top +=1 # 根据问题需求，是可以在题目中 设置这种break，不需要等到 while 的判断 if top &gt; bottom: break for i in range(top, bottom+1): ans[i][right] =cur cur +=1 right -=1 if left &gt; right: break # 好好体会这个连接点的处理，左边是能够访问到的，右边为了能够访问到 # 进行了 -1 的操作 for i in range(right, left-1, -1): ans[bottom][i] =cur cur +=1 bottom -=1 if bottom &lt;top: break for i in range(bottom, top-1, -1): ans[i][left] =cur cur +=1 left +=1 return ans Merge Intervals Given a collection of intervals, merge all overlapping intervals. Tips: 需要区分区间的start 和end 点，分别使用 (0 1) 进行区分，然后 sort() ，那么那么start 就出现了最前面，end 就出现了最后面。默认的sort 是先按照 第一个元素排序，然后按照第二个元素排序，所以标识 (0, 1) 这个是没有收到影响的。 12345678910111213141516171819202122232425class Solution(object): def merge(self, intervals): if not intervals: return [] data = [] for interval in intervals: data.append((interval[0], 0)) data.append((interval[1], 1)) data.sort() merged = [] stack = [data[0]] for i in range(1, len(data)): d = data[i] if d[1] == 0: # this is a lower bound, push this onto the stack stack.append(d) elif d[1] == 1: if stack: start = stack.pop() if len(stack) == 0: # we have found our merged interval merged.append( (start[0], d[0])) return merged Length of Last Word Given a string s consists of upper/lower-case alphabets and empty space characters ‘ ‘, return the length of last word in the string.If the last word does not exist, return 0. Tips: 不能使用 split() 因为太多的case需要单独的处理，所以应该使用字母为基本，一个个处理。等不等于 ‘ ‘进行的切分。 1234567891011121314151617class Solution(object): # 这个是需要从 字母角度考虑，而不是从单词角度考虑 def lengthOfLastWord(self, s): len_s =len(s) if 0==len_s: return 0 index =len_s -1 # 找到第一个不是 ' '的字母 while index&gt;=0 and ' ' ==s[index]: index -=1 len_last_word =0 while index &gt;=0 and ' ' != s[index]: index -=1 len_last_word +=1 return len_last_word Valid Number Validate if a given string can be interpreted as a decimal number. Tips :对于小数(decimal ) 各种 case 的熟知程度 123456789101112131415161718192021222324252627class Solution(object): def isNumber(self, s): """ :type s: str :rtype: bool """ s = s.strip() digits = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'] met_dot = met_e = met_digit = False for i, char in enumerate(s): if char in ['+', '-']: if i &gt; 0 and s[i-1] != 'e': return False elif char == '.': if met_dot or met_e: return False met_dot = True elif char == 'e': if met_e or not met_digit: return False met_e, met_digit = True, False #elif char.isdigit(): elif char in digits: met_digit = True else: return False return met_digit Plus One Given a non-empty array of digits representing a non-negative integer, plus one to the integer.The digits are stored such that the most significant digit is at the head of the list, and each element in the array contain a single digit.You may assume the integer does not contain any leading zero, except the number 0 itself. Input: [1,2,3] Output: [1,2,4] Explanation: The array represents the integer 123. Tips: 重点在于加法的处理，一般使用 求余得到digit，然后使用carry 得到进位。 1234567891011121314151617181920class Solution(object): def plusOne(self, digits): """ :type digits: List[int] :rtype: List[int] """ len_s =len(digits) carry =1 for i in range(len_s-1, -1, -1): total =digits[i] +carry digit =int(total %10) carry =int(total //10) digits[i] =digit # 这个是最后一个进位 if carry ==1: digits.insert(0, 1) return digits Simplify Path Given an absolute path for a file (Unix-style), simplify it. Or in other words, convert it to the canonical path. Tips： 12345678910111213141516171819class Solution(object): # 这个从考点上是 stack，但是使用python字符串处理更好 # 按照 '/' 进行split() def simplifyPath(self, path): """ :type path: str :rtype: str """ stack = [] for token in path.split('/'): if token in ('', '.'): pass # continue 这两个是一样的效果， pass 就类似一种占位符，在测试的时候常见 elif token == '..': if stack: stack.pop() else: stack.append(token) return '/' + '/'.join(stack) Edit Distance Given two words word1 and word2, find the minimum number of operations required to convert word1 to word2. Tips: 动态规划就通过存储子问题结果来加快运算，但一个好的动态规划算法会尽量减少空间复杂度。 然后是可以继续优化的，使用 O(n) 的空间的复杂度. 真正的写出来之后，发现代码是比想法更加简单的。 提供了两种解法，第一种比较常规 dp，比较容易理解。123456789101112131415161718192021222324class Solution(object): def minDistance(self, word1, word2): """ :type word1: str :type word2: str :rtype: int """ m = len(word1) n = len(word2) table = [[0] * (n + 1) for _ in range(m + 1)] for i in range(m + 1): table[i][0] = i for j in range(n + 1): table[0][j] = j for i in range(1, m + 1): for j in range(1, n + 1): if word1[i - 1] == word2[j - 1]: table[i][j] = table[i - 1][j - 1] else: table[i][j] = 1 + min(table[i - 1][j], table[i][j - 1], table[i - 1][j - 1]) return table[-1][-1] 第二种就是参考一下吧。 1234567891011121314151617class Solution(object): # 从实现的角度讲，这个是需要把握住有一个 word的index 是不变的 def minDistance(self, word1, word2): l1, l2 = len(word1)+1, len(word2)+1 pre = [0 for _ in range(l2)] for j in range(l2): pre[j] = j for i in range(1, l1): cur = [i]*l2 for j in range(1, l2): cur[j] = min(cur[j-1]+1, pre[j]+1, pre[j-1]+(word1[i-1]!=word2[j-1])) #pre = cur[:] pre =cur return pre[-1] Set Matrix Zeroes Given a m x n matrix, if an element is 0, set its entire row and column to 0. Do it in-place. Tips: 使用第一行和第一列作为标记，使用 0作为标记。 123456789101112131415161718192021class Solution(object): def setZeroes(self, matrix): """ :type matrix: List[List[int]] :rtype: None Do not return anything, modify matrix in-place instead. """ firstRowHasZero = not all(matrix[0]) # all() 只有所有的不为0 返回的才不为0，否则返回0 for i in range(1,len(matrix)): for j in range(len(matrix[0])): if matrix[i][j] == 0: # 这种遍历并标记的方法还是比较优秀的 matrix[0][j] = 0 matrix[i][0] = 0 for i in range(1,len(matrix)): for j in range(len(matrix[0])-1,-1,-1): # 注意是从后往前标记的 if matrix[0][j] == 0 or matrix[i][0] == 0: matrix[i][j] = 0 if firstRowHasZero: matrix[0] = [0]*len(matrix[0]) #最后处理第一行 Remove Duplicates from Sorted List Given a sorted linked list, delete all duplicates such that each element appear only once. Tips: 注意这道题和上面有道题是有差别的，这个是 delete all duplicates 1234567891011121314151617181920212223# Definition for singly-linked list.# class ListNode(object):# def __init__(self, x):# self.val = x# self.next = Noneclass Solution(object): """ 使用两个 while 是因为，逻辑上简单 """ def deleteDuplicates(self, head): """ :type head: ListNode :rtype: ListNode """ cur =head while cur: while cur.next and cur.val == cur.next.val: cur.next =cur.next.next # skip duplicates cur =cur.next return head Merge Sorted Array Given two sorted integer arrays nums1 and nums2, merge nums2 into nums1 as one sorted array. The number of elements initialized in nums1 and nums2 are m and n respectively. You may assume that nums1 has enough space (size that is greater or equal to m + n) to hold additional elements from nums2. Tips: 题目中说了 nums1 是不会出现 index 访问报错的。从后往前遍历，因为这个是要求 merge 2 into 1的。 1234567891011121314151617181920212223242526class Solution(object): def merge(self, nums1, m, nums2, n): """ :type nums1: List[int] :type m: int :type nums2: List[int] :type n: int :rtype: None Do not return anything, modify nums1 in-place instead. """ i, j, k =m-1, n-1, m+n-1 while i &gt;=0 and j&gt;=0: if nums1[i] &gt; nums2[j]: nums1[k] =nums1[i] i -=1 else: nums1[k] =nums2[j] j -=1 k -=1 #import ipdb #ipdb.set_trace() if j&gt;=0: nums1[:k+1] =nums2[:j+1] Restore IP Addresses Given a string containing only digits, restore it by returning all possible valid IP address combinations. Tips： 细节比较多，在进行 dfs 的时候 123456789101112131415161718192021222324252627282930class Solution(object): def restoreIpAddresses(self, s): """ :type s: str :rtype: List[str] """ ans = [] self.helper(ans, s, 4, []) # ans 中的item 之间使用 . 进行隔开，这种技术，是非常常见的 # 这个是 list of list， 然后转换成了 list of string return ['.'.join(x) for x in ans] def helper(self, ans, s, k, temp): if len(s) &gt; k * 3: return if k == 0: #ans.append(temp[:]) ans.append(temp) else: for i in range(min(3, len(s) - k + 1)): # s 是一个字符串，当只有一位时，0可以成某一段，如果有两位或三位时，像 00， 01， 001， 011， 000等都是不合法的， # 只能是 0.1.0.0 而不能是00.1.0.0 ，这个是ip语法 # 这个是有连个并列的判断条件 if i == 2 and int(s[:3]) &gt; 255 or i &gt;0 and s[0] =='0': continue self.helper(ans, s[i + 1:], k - 1, temp + [s[:i + 1]]) Interleaving String Given s1, s2, s3, find whether s3 is formed by the interleaving of s1 and s2. Tips： interleaving 插入；s3 是否能够用s1 和s2 组成, len(s1) +len(s2) == len(s3) 这个样子的。行列分别表示s1 和s2 中的字母，然后 (x, y) 值表示当前的能够”走“ 通的路径。 12345678910111213141516171819202122232425262728293031class Solution(object): """ interleaving, 判断s3是否由s1和s2交叉构成， """ def isInterleave(self, s1, s2, s3): """ :type s1: str :type s2: str :type s3: str :rtype: bool """ r, c, l =len(s1), len(s2), len(s3) if r+c !=l: return False # 0 行和 0列的初始化，使用 true or false 来进行表示结果 dp =[ [True] * (c+1) for _ in range(r+1)] for i in range(1, r+1): dp[i][0] =dp[i-1][0] and s1[i-1] == s3[i-1] for j in range(1, c+1): dp[0][j] =dp[0][j-1] and s2[j-1] ==s3[j-1] # 看到代码之后觉得很简单， for i in range(1, r+1): for j in range(1, c+1): dp[i][j] = dp[i-1][j] and s1[i-1] ==s3[i+j-1] or dp[i][j-1] and s2[j-1] ==s3[i+j-1] return dp[-1][-1] 方法二：12345678910111213141516# 从运行的结果来说，内存下降了0.1M， 但是这个时间却商城了def isInterleave(self, s1, s2, s3): r, c, l= len(s1), len(s2), len(s3) if r+c != l: return False dp = [True for _ in range(c+1)] for j in range(1, c+1): dp[j] = dp[j-1] and s2[j-1] == s3[j-1] for i in range(1, r+1): dp[0] = (dp[0] and s1[i-1] == s3[i-1]) for j in range(1, c+1): dp[j] = (dp[j] and s1[i-1] == s3[i-1+j]) or (dp[j-1] and s2[j-1] == s3[i-1+j]) return dp[-1] Pascal’s Triangle Given a non-negative integer numRows, generate the first numRows of Pascal’s triangle. Tips: 这个是小学数学题，变成了编程题、对应好index 进行了。最后 res 可能不是正三角形（直角三角形）但一定是可以这样做的。 123456789101112131415class Solution(object): def generate(self, numRows): """ :type numRows: int :rtype: List[List[int]] """ res = [[1 for _ in range(i+1)] for i in range(numRows)] for i in range(2, numRows): for j in range(1, i): # 这个就是一个数学问题 # 就是上一行(i-1) 的 j-1 和j 元素的相加 res[i][j] =res[i-1][j-1] + res[i-1][j] return res Given a non-negative index k where k ≤ 33, return the kth index row of the Pascal’s triangle.Note that the row index starts from 0. Tips： 相比于上一个，这个只是返回了最后一行。 123456789101112131415class Solution(object): # 相对比上一个，只是输出最后一行的信息， rowIndex def getRow(self, rowIndex): """ :type rowIndex: int :rtype: List[int] """ res =[ [1 for _ in range(i+1) ] for i in range(rowIndex+1)] for i in range(2, rowIndex+1): for j in range(1, i): res[i][j ] = res[i-1][j] + res[i-1][j-1] return res[-1] Valid Palindrome Given a string, determine if it is a palindrome, considering only alphanumeric characters and ignoring cases.Note: For the purpose of this problem, we define empty string as valid palindrome. Input: &quot;A man, a plan, a canal: Panama&quot; Output: true Tips： 建议使用 is.alnum() 这个python 中自带的函数，因为这种判断还是挺常见的。回文数。先是预处理，然后才是 lower() 判断。 1234567891011121314151617181920212223242526class Solution(object): # palindrome 回文数， alphanumeric ， 字母与数字并用的; # 预处理之后，然后比较前后两个字符的异同 # Python isalnum() 方法检测字符串是否由字母和数字组成，这种函数只有在 歪果仁的代码中常见 # s[i] &gt;= 'a' and s[i] &lt;= 'z' or s[i] &gt;= '0' and s[i] &lt;= '9' or s[i] &gt;= 'A' and s[i] &lt;= 'Z':, 这个是国人的写法 # a=''.join([x for x in s if x.isalpha() or x.isdigit()]).lower() 或者这样 # 喜欢写源码 def isPalindrome(self, s): """ :type s: str :rtype: bool """ left, right =0, len(s)-1 while left &lt; right: while left &lt; right and not s[left].isalnum(): left +=1 while left &lt; right and not s[right].isalnum(): right -=1 if s[left].lower() != s[right].lower(): return False left +=1 right -=1 return True Gas Station There are N gas stations along a circular route, where the amount of gas at station i is gas[i].You have a car with an unlimited gas tank and it costs cost[i] of gas to travel from station i to its next station (i+1). You begin the journey with an empty tank at one of the gas stations.Return the starting gas station’s index if you can travel around the circuit once in the clockwise direction, otherwise return -1. Input:gas = [1,2,3,4,5]cost = [3,4,5,1,2]Output: 3Explanation:Start at station 3 (index 3) and fill up with 4 unit of gas. Your tank = 0 + 4 = 4Travel to station 4. Your tank = 4 - 1 + 5 = 8Travel to station 0. Your tank = 8 - 2 + 1 = 7Travel to station 1. Your tank = 7 - 3 + 2 = 6Travel to station 2. Your tank = 6 - 4 + 3 = 5Travel to station 3. The cost is 5. Your gas is just enough to travel back to station 3.Therefore, return 3 as the starting index. Tips: 看着挺吓人的，但是落实到代码上，就是一个循环，当不能出发时， r (rest) 是为0，然后寻求下一个可以出发的点。 12345678910111213141516171819class Solution(object): # leetcode 是有 1000 多到题目，如果是刷题那么肯定是刷不完的，我估计就是刷 150道左右的样子 # https://leetcode.com/problems/gas-station/discuss/274646/Python-One-Pass-Greedy # 这个只是要求是 return the start index oof gas station index, if exists, 那么这个不就是超级简单了了吗 def canCompleteCircuit(self, gas, cost): """ :type gas: List[int] :type cost: List[int] :rtype: int """ if sum(gas) &lt; sum(cost): return -1 n, s, r = len(gas), 0, 0 for i in range(n): if gas[i] + r &lt; cost[i]: s, r = i+1, 0 else: r += gas[i]-cost[i] return s Evaluate Reverse Polish Notation Evaluate the value of an arithmetic expression in Reverse Polish Notation.Valid operators are +, -, *, /. Each operand may be an integer or another expression. Division between two integers should truncate toward zero. The given RPN expression is always valid. That means the expression would always evaluate to a result and there won&apos;t be any divide by zero operation. Tips： 术语，逆波兰表达式（操作数在前，操作符在后）的一种形式。栈是存储操作数 和运算结果的。对于负数不能整除的，向着 0 靠拢 1234567891011121314151617181920212223242526272829class Solution(object): # 计算逆波兰表达式：把操作数放前面，把操作符后置的一种写法 # 这个明显就是 栈的使用呀，两个栈， def evalRPN(self, tokens): """ :type tokens: List[str] :rtype: int """ stack =[] for t in tokens: if t not in "+-*/": stack.append(int(t)) else: right, left =stack.pop(), stack.pop() if t =="+": stack.append(left +right) elif t =="-": stack.append(left-right) elif t =="*": stack.append(left*right) else: # case like 1/(-2) 负数且不能整除 if left*right &lt;0 and left % right!=0: stack.append(left/right +1) else: stack.append(left/right) return stack.pop() Reverse Words in a String Given an input string, reverse the string word by word. Tips： 正常的思路是第一次全翻转，第二次按照 word 进行翻转。但是python 十分擅长 字符串的处理。 123456789class Solution(object): # 两次翻转。第一次是全翻转，然后第二次是word 翻转 # 字符串类型的算法题目，使用python 是无法get 到算法层面的 hahaha def reverseWords(self, s): """ :type s: str :rtype: str """ return " ".join(s.split()[::-1]) Search a 2D Matrix Write an efficient algorithm that searches for a value in an m x n matrix. This matrix has the following properties: Integers in each row are sorted from left to right. The first integer of each row is greater than the last integer of the previous row. Tips: 注意第二个条件，下一行的开头是大于上一行的末尾，所以如果 dense 一下，是可以看成大的有序，所以思路就是二叉排序。 https://leetcode.com/problems/search-a-2d-matrix/ Input:matrix = [ [1, 3, 5, 7], [10, 11, 16, 20], [23, 30, 34, 50]]target = 3Output: true 123456789101112131415161718192021222324252627class Solution(object): def searchMatrix(self, matrix, target): """ :type matrix: List[List[int]] :type target: int :rtype: bool """ # 如果写成 not target 是有问题，当 target ==0 的时候，这个是不成立的，所以需要看一下数据的范围 # 注意区分 if not matrix or target ==None: return False rows, cols=len(matrix), len(matrix[0]) low, high =0, rows*cols-1 # 总的二叉 while low &lt;=high: mid =(low +high) /2 num =matrix[mid/cols][mid%cols] if num ==target: return True elif num&lt; target: low =mid +1 else: high =mid -1 return False Search a 2D Matrix II Write an efficient algorithm that searches for a value in an m x n matrix. This matrix has the following properties: Integers in each row are sorted in ascending from left to right. Integers in each column are sorted in ascending from top to bottom. [ [1, 4, 7, 11, 15], [2, 5, 8, 12, 19], [3, 6, 9, 16, 22], [10, 13, 14, 17, 24], [18, 21, 23, 26, 30]] Tips: 注意区分和上一道题目的第二点的区别。这个只能是一步步走，下面的程序是从右上方开始走，如果 target 大则向下直走，否则左走。 https://leetcode.com/problems/search-a-2d-matrix-ii/ 123456789101112131415161718192021class Solution(object): def searchMatrix(self, matrix, target): """ :type matrix: List[List[int]] :type target: int :rtype: bool """ if not matrix: return False m, n = len(matrix), len(matrix[0]) row, col = 0, n - 1 while row &lt; m and col &gt;= 0: if matrix[row][col] == target: return True elif matrix[row][col] &lt; target: row += 1 else: col -= 1 return False]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[逻辑回归概念]]></title>
    <url>%2F2019%2F06%2F09%2Flr%2F</url>
    <content type="text"><![CDATA[本文主要介绍逻辑回归（logistics regression）和决策树（Decision Tree）。逻辑回归从线性回归出发到逻辑回归，然后手推公式和相关的一些特点；介绍一下决策树的特点。 逻辑回归逻辑回归是线性模型，虽然叫做”回归“，究其原因 逻辑回归从线性回归引申而来，对回归的结果进行 logistic 函数运算，将范围限制在[0,1]区间，并更改损失函数为二值交叉熵损失，使其可用于2分类问题(通过得到的概率值与阈值比较进行分类)。 公式推导从线性回归问题到逻辑回归过程的推导。 线性二分模型： $$f ( x ) = \theta ^ { T } x$$ 逻辑回归决策函数是将此线性二分类嵌套一个sigmoid函数： $$ f ( x ) = \frac { 1 } { 1 + e ^ { - \theta ^ { T } x } }$$ 损失函数：如果用平方误差（MSE）作为逻辑回归的损失函数,那么函数曲线将是跳跃式的,非凸的(non-convex),原因是logistic函数将数据范围限制在[0,1]区间,而真实标签值非0即1.最小化 MSE 损失容易陷入局部极小点.逻辑回归损失是如下的分情况的凸函数(单个x与y的损失)。 $$P ( y = 1 | x ; \theta ) = h _ { \theta } ( x )$$ $$P ( y = 0 | x ; \theta ) = 1 - h _ { \theta } ( x )$$最初是上述的分段函数，合并成下面的函数，方便计算。$$p ( y | x ; \theta ) = \left( h _ { \theta } ( x ) \right) ^ { y } \left( 1 - h _ { \theta } ( x ) \right) ^ { 1 - y }$$使用最大似然的思想求解。假设我们有n个独立的训练样本{(x1, y1) ,(x2, y2),…, (xn, yn)}，y={0, 1}。那每一个观察到的样本(xi, yi)出现的概率是： 上述似然函数乘法太难算了，然后使用log 将其改为加法，变成了对数似然函数。$$J( \theta ) = \log ( L ( \theta ) ) = \sum _ { i = 1 } ^ { m } y ^ { ( i ) } \log \left( h \left( x ^ { ( i ) } \right) \right) + \left( 1 - y ^ { ( i ) } \right) \log \left( 1 - h \left( x ^ { ( i ) } \right) \right)$$ 求导优化问题sigmoid 函数的特殊性质：$$\sigma ^ { \prime } ( x ) = \sigma ( x ) ( 1 - \sigma ( x ) )$$ 分成三部分求导： 用L(θ)对θ求导，得到：$$\begin{split}\frac { d } { d \theta _ { i } } \operatorname { loss } ( \theta ) &amp;= \left( y \frac { 1 } { \sigma \left( \theta ^ { T } x \right) } - ( 1 - y ) \frac { 1 } { 1 - \sigma \left( \theta ^ { T } x \right) } \right) \frac { d } { d \theta _ { i } } \sigma \left( \theta ^ { T } x \right) \\&amp;= \left( y \frac { 1 } { \sigma \left( \theta ^ { T } x \right) } - ( 1 - y ) \frac { 1 } { 1 - \sigma \left( \theta ^ { T } x \right) } \right) \sigma \left( \theta ^ { T } x \right) \left( 1 - \sigma \left( \theta ^ { T } x \right) \right) \frac { d } { d \theta _ { i } } \theta ^ { T } x \\&amp;= \left( y \left( 1 - \sigma \left( \theta ^ { T } x \right) \right) - ( 1 - y ) \sigma \left( \theta ^ { T } x \right) \right) x _ { i } \\&amp;= \left( y - h _ { \theta } ( x ) \right) x _ { i }\end{split}$$ 注意一会儿有 $\sum$ 一会儿没有的，其实我们更倾向于不用，采用矩阵相乘的方式更加简洁。只是在表达似然函数，使用$ \sum$更加直观$$\theta _ { i } : = \theta _ { j } + \alpha \left( y ^ { ( i ) } - h _ { \theta } \left( x ^ { ( i ) } \right) \right) x _ { j } ^ { ( i ) }$$ 逻辑回归的特点 优点： LR 能以概率的形式输出结果,而非只是 0,1 判定， 可以做 ranking model； LR 的可解释性强,可控度高； 训练快 缺点： 容易欠拟合，一般准确度不太高 只能处理两分类问题. (可以应用多个逻辑回归实现多分类,类似SVM的方式; 另外对于父子类别同时分类的情况,使用逻辑回归要比Softmax等方式效果好) “海量离散特征+简单模型” 同“少量连续特征+复杂模型”的权衡。既可以离散化用线性模型，也可以用连续特征加深度学习 为什么对特征进行离散化 特征从连续变量状态到离散化的初衷在于我们认为不同的区间对于最后的结果的重要性是不同的。同样在工业界，很少直接将连续值作为逻辑回归模型的特征输入，而是将连续特征离散化为一系列0、1特征(one-hot编码)交给逻辑回归模型，这样做的优势有以下几点 离散特征的增加和减少都很容易，易于模型的快速迭代； 稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展； 离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄&gt;30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰； 单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合 离散化后可以进行特征交叉 究其原因，使用 LR+离散模型在于可控可解释。而GBDT 直接使用连续的变量，一方面的原因在于如果特征过多，那么GBDT 是跑不动的。 为什么LR模型的损失函数是交叉熵,而线性回归模型的损失函数却是最小二乘呢？能否随意确定一个损失函数作为目标呢？ 模型的损失函数由各自的响应变量y的概率分布决定，对于线性回归模型，其输出是连续值，所以我们对于该问题假设y服从正态分布；相对的，LR模型一般用来解决二分类问题，所以其输出是0/1，故而我们假设其输出服从伯努利分布；而进一步地，两者的损失函数都是通过极大似然估计推导的来的，所以模型的损失函数并非随意确定。分类模型与回归模型之间有种种联系,比如 SVM 模型可以看作逻辑回归加L2正则项, 并使用了不同的损失函数. 为什么不使用回归模型来做分类?这是一种不好的做法, 因为阈值不好确定, 随着数据集的变动, 阈值也需要有较大变化. 正则项 L2 解决过拟合 L1 解决数据稀疏性 L1和L2正则先验分别服从什么分布从信息论的角度看，向系统加入了正确先验这个信息，肯定会提高系统的性能。两者的差别感性的理解？L1是拉普拉斯分布，L2是高斯分布。拉普拉斯分布：$$f ( x | \mu , b ) = \frac { 1 } { 2 b } e ^ { - \frac { | x - \mu | } { b } }$$高斯分布：$$f \left( x | \mu , \sigma ^ { 2 } \right) = \frac { 1 } { \sqrt { 2 \pi \sigma ^ { 2 } } } e ^ { - \frac { ( x - \mu ) ^ { 2 } } { 2 \sigma ^ { 2 } } }$$ Decision tree主要介绍一下 决策树的特点。 从这次学习中明显的感受到这个 decision tree 是非常容易过拟合的。 We can make our tree more complex by increasing its size , which will result in more and more partitions trying to emulate the circular boundary. 优点在于：可以handle 非线性的变化。decision tree 给人的感觉就是线性或者离散的(category)的都是可以使用，因为decision tree 得到就是一个离散的结果，最大的缺点就是容易过拟合。 This brings us to the biggest problem associated with Decision Trees, that is, they are highly biased class of models. You can make a decision tree model on your training set which might outperform all other algorithms but it’ll prove to be a poor predictor on your test set. You’ll have to rely heavily on pruning and cross validation to get a non-over-fitting model with Decision Trees. 过拟合是可以通过剪枝或者 cross validation 进行缓解 overfit的效果的或者使用 random forest随机性进行”中和“。 This problem of over-fitting is overcome to large extent by using Random Forests, which are nothing but a very clever extension of decision trees. But random forest take away easy to explain business rules because now you have thousands of such trees and their majority votes to make things complex. Also by decision trees have forced interactions between variables , which makes them rather inefficient if most of your variables have no or very weak interactions.]]></content>
      <tags>
        <tag>LR</tag>
        <tag>Decision tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SVM All You Need to Know]]></title>
    <url>%2F2019%2F06%2F08%2Fsvm-all-you-need%2F</url>
    <content type="text"><![CDATA[本文主要介绍SVM 相关内容，包括理论原理、在线性可分条件下的公式推导和 SVM的应用特点。最后综合 LR 和 Decision Tree的这篇博客，给出了一些小的建议。 SVM 理论支持向量机分为三个部分，线性可分支持向量机、线性支持向量机、非线性支持向量机。 SVM 原理SVM 是一种二类分类模型。它的基本模型是在特征空间中寻找间隔最大化的分离超平面的线性分类器。 当训练样本线性可分时，通过硬间隔最大化，学习一个线性分类器，即线性可分支持向量机；当训练数据近似线性可分时，引入松弛变量，通过软间隔最大化，学习一个线性分类器，即线性支持向量机；当训练数据线性不可分时，通过使用核技巧及软间隔最大化，学习非线性支持向量机。以上各种情况下的数学推到应当掌握，硬间隔最大化（几何间隔）、学习的对偶问题、软间隔最大化（引入松弛变量）、非线性支持向量机（核技巧）。 SVM 为什么采用间隔最大化当训练数据线性可分时，存在无穷个分离超平面可以将两类数据正确分开。感知机利用误分类最小策略，求得分离超平面，不过此时的解有无穷多个。线性可分支持向量机利用间隔最大化求得最优分离超平面，这时，解是唯一的。另一方面，此时的分隔超平面所产生的分类结果是最鲁棒的，对未知实例的泛化能力最强。可以借此机会阐述一下几何间隔以及函数间隔的关系。 为什么要将求解 SVM 的原始问题转换为其对偶问题一是对偶问题往往更易求解，当我们寻找约束存在时的最优点的时候，约束的存在虽然减小了需要搜寻的范围，但是却使问题变得更加复杂。为了使问题变得易于处理，我们的方法是把目标函数和约束全部融入一个新的函数，即拉格朗日函数，再通过这个函数来寻找最优点。二是可以自然引入核函数，进而推广到非线性分类问题。 为什么 SVM 要引入核函数当样本在原始空间线性不可分时，可将样本从原始空间映射到一个更高维的特征空间，使得样本在这个特征空间内线性可分。而引入这样的映射后，所要求解的对偶问题的求解中，无需求解真正的映射函数，而只需要知道其核函数。核函数的定义：K(x,y)=&lt;ϕ(x),ϕ(y)&gt;，即在特征空间的内积等于它们在原始样本空间中通过核函数 K 计算的结果。一方面数据变成了高维空间中线性可分的数据，另一方面不需要求解具体的映射函数，只需要给定具体的核函数即可，这样使得求解的难度大大降低。 为什么SVM对缺失数据敏感这里说的缺失数据是指缺失某些特征数据，向量数据不完整。SVM 没有处理缺失值的策略。而 SVM 希望样本在特征空间中线性可分，所以特征空间的好坏对SVM的性能很重要。缺失特征数据将影响训练结果的好坏。 SVM 核函数之间的区别一般选择线性核和高斯核，也就是线性核与 RBF 核。 线性核：主要用于线性可分的情形，参数少，速度快，对于一般数据，分类效果已经很理想了。 RBF 核：主要用于线性不可分的情形，参数多，分类结果非常依赖于参数。有很多人是通过训练数据的交叉验证来寻找合适的参数，不过这个过程比较耗时。 如果 Feature 的数量很大，跟样本数量差不多，这时候选用线性核的 SVM。 如果 Feature 的数量比较小，样本数量一般，不算大也不算小，选用高斯核的 SVM。 以上是几个问题在面试中遇到 SVM 算法时，几乎是必问的问题，另外，大家一定要做到自己可以推导集合间隔、函数间隔以及对偶函数，并且理解对偶函数的引入对计算带来的优势。 支持向量 如图所示，上面只有三个点与求解的优化问题有关，它们就叫做支持向量。 SVM公式推导(线性可分条件下)假定样本空间如下$${ ( x _ { 1 } , y _ { 1 } ) , ( x _ { 2 } , y _ { 2 } ) , \ldots , ( x _ { N } , y _ { N } ) }$$ 共有N个向量，其中$x_k$是一个特征向量而不是一个单一数值。 这是一个二分类问题，所以$y=+1 $或者$ y=−1$。那么我们就可以得到 那么，我们可以得到$$y _ { i } \cdot ( w ^ { T } x _ { i } + b ) \geq 1 , i = 1,2 , \ldots , N$$ 因为我们现在只讨论线性可分情况下的支持向量机，那么在这个样本空间中一定存在一个超平面可以将样本集按照y的值分割城两个部分，这个超平面可以表示为$$w ^ { T } x + b = 0$$ 根据这个超平面的表达式以及第一步推到中我们得到的结果，可以得到这个样本集中任意一个样本点距离超平面的距离：$$\gamma = \frac { | w ^ { T } x + b | } { | w | } \geq \frac { 1 } { | w | }$$由此，我们还可以进一步得到整个margin的宽度：$$\gamma = \frac { 2 } { | w | }$$ 由此，根据第一步和第三步的结果，我们可以得到最基本的目标函数： $$\arg \max _ { w , b } \frac { 2 } { | w | } , \text {s.t. } y _ { i } ( w ^ { T } x _ { i } + b ) \geq 1 , i = 1,2 , \ldots , N$$ 我们还可以对这个目标函数进一步做变化： $$\arg \min _ { w , b } \frac { 1 } { 2 } | w | ^ { 2 }, \text {s.t. } y _ { i } ( w ^ { T } x _ { i } + b ) \geq 1 , i = 1,2 , \ldots , N $$ 我们无法继续直接进行计算了，因此引入拉格朗日乘子$$L ( w , b , \alpha ) = \frac { 1 } { 2 } | w | ^ { 2 } + \sum _ { i } \alpha _ { i } [ 1 - y _ { i } ( w ^ { T } x _ { i } + b ) ]$$ 对w和b分别求L的偏导，并令其偏导数等于0：$$\frac { \partial L } { \partial w } = w - \sum _ { i } \alpha _ { i } y _ { i } x _ { i } = 0 \Rightarrow w = \sum _ { i } \alpha _ { i } y _ { i } x _ { i }$$$$\frac { \partial L } { \partial b } = \sum _ { i } \alpha _ { i } y _ { i } = 0$$ 将第七步得到的w和b代入L函数 至此，我们的目标函数已经变成了$$\arg \max _ { \alpha } ( \sum _ { i } \alpha _ { i } - \frac { 1 } { 2 } \sum _ { i } \sum _ { j } \alpha _ { i } \alpha _ { j } y _ { i } y _ { j } x _ { i } ^ { T } x _ { j } )$$$$\text { s.t. } \sum _ { i } \alpha _ { i } y _ { i } = 0$$$$\alpha _ { i } \geq 0 , i = 1,2 , \ldots , N$$ 用数值方法解出α以后，我们带入式子 7就可以得到 $$w^ { * } = \sum _ { i } \alpha _ { i } ^ { * } y _ { i } x _ { i }$$ SVM的特点SVM的优点：就是当大量的特征出现的时候，使用SVM handle large feature spaces; 然而此时 LR 不是一个很好的选择。 SVM can handle large feature spaces which makes them one of the favorite algorithms in text analysis which almost always results in huge number of features where logistic regression is not a very good choice. SVM Pros: Can handle large feature space Can handle non-linear feature interactions Do not rely on entire data SVM Cons: Not very efficient with large number of observations It can be tricky to find appropriate kernel sometimes TakeOff首先使用 LR 进行尝试，不妨试一下 DT，然后 如果特征比较多，但是数据量不是很多，这个时候使用SVM。Always start with logistic regression, if nothing then to use the performance as baselineSee if decision trees (Random Forests) provide significant improvement. Even if you do not end up using the resultant model, you can use random forest results to remove noisy variablesGo for SVM if you have large number of features and number of observations are not a limitation for available resources and time附上链接：https://www.edvancer.in/logistic-regression-vs-decision-trees-vs-svm-part2/]]></content>
      <tags>
        <tag>SVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NLP Papers Reading-Sentence Embedding]]></title>
    <url>%2F2019%2F06%2F01%2Fnlp-papers-reading-sentence-embedding%2F</url>
    <content type="text"><![CDATA[Why Consider Sentence Embedding?One simple way you could do this is by generating a word embedding for each word in a sentence, adding up all the embeddings and divide by the number of words in the sentence to get an “average” embedding for the sentence. Alternatively, you could use a more advanced method which attempts to add a weighting function to word embeddings which down-weights common words. This latter approach is known as Smooth Inverse Frequency (SIF). These methods can be used as a successful proxy for sentence embeddings. However, this “success” depends on the dataset being used and the task you want to execute. So for some tasks these methods could be good enough However, there are a number of issues with any of these types of approaches: They ignore word ordering. So your product is easy to use, I do not need any help is identical to I do need help, your product is not easy to use. This is obviously problematic. It’s difficult to capture the semantic meaning of a sentence. The word crash can be used in multiple contexts, e.g. I crashed a party, the stock market crashed, or I crashed my car. It’s difficult to capture this change of context in a word embedding. Sentence length becomes problematic. With sentences we can chain them together to create a long sentence without saying very much, The Philadelphia Eagles won the Super Bowl, The Washington Post reported that the Philadelphia Eagles won the Super Bowl, The politician claimed it was fake news when the Washington Post reported that the Philadelphia Eagles won the Super Bowl, and so on. All these sentences are essentially saying the same thing but if we just use word embeddings, it can be difficult to discover if they are similar. They introduce extra complexity. When using word embeddings as a proxy for sentence embeddings we often need to take extra steps in conjunction with the base model. For example, we need to remove stop words, get averages, measure sentence length and so on. sentence embedding的引用场景：Similar approaches can be used to go beyond representations and semantic search, to document classification and understanding and eventually document summarizing or generation. Words Embed平均词向量与TFIDF加权平均词向量SIF加权平均词向量来自论文 A simple but tough-to-beat baseline for sentence embeddings，更多信息可以参考这里。 利用n-grams embeddingfasttext 介绍。 DAN（Deep Unordered Composition Rivals Syntactic Methods for Text Classification）其实DAN(Deep Averaging Networks)应该属于Bag of Words类的算法。因为比较特殊，单独列出来。 它是在对所有词语取平均后，在上面加上几层神经网络。特殊的地方在于它在sentiment analysis中表现也不错，这在BOW类方法中比较罕见。 文中提出了DAN(Deep average network)，说白了就是对于一个句子或者一个段落，把每个单词的embedding进行平均求和，得到第一层固定维度的向量，然后在套几层全连接神经网络。本质来讲，这个模型没有考虑单词之间的顺序，not在第一个位置和在最后一个位置对于DAN来讲输入都是一样的，所以自然conver不住这种情况。这是模型本身的问题，没有办法改进，除非换模型，比如textcnn就能很好的解决这种情况对于否定词敏感，比如but,not等，常常判断为negative。训练速度快，且结果较好，和Syntactic Composition性能差不多，但是消耗的计算资源少作为有监督学习任务来讲，可以试一试。但是由于全连接层，无法进行无监督学习。相反，NBOW可以无监督学习，比如文本相似度计算等。当然。对于DAN而言，可以通过迁移学习，预训练好全连接参数，实现无监督学习 总结一下：对简单的任务来说，用简单的网络结构进行处理基本就够了，但是对比较复杂的任务，还是依然需要更复杂的网络结构来学习sentence representation的。 Unsupervised Sentence Embed基于Encoder-decoder的Skip-Thought VectorsContinuing the tour of older papers that started with our ResNet blog post, we now take on Skip-Thought Vectors by Kiros et al. Their goal was to come up with a useful embedding for sentences that was not tuned for a single task and did not require labeled data to train. They took inspiration from Word2Vec skip-gram (you can find my explanation of that algorithm here) and attempt to extend it to sentences. Changing a single word has had almost no effect on the meaning of that sentence. To account for these word level changes, the skip-thought model needs to be able to handle a large variety of words, some of which were not present in the training sentences. The authors solve this by using a pre-trained continuous bag-of-words (CBOW) Word2Vec model and learning a translation from the Word2Vec vectors to the word vectors in their sentences. Below are shown the nearest neighbor words after the vocabulary expansion using query words that do not appear in the training vocabulary: 论文描述了一种通用、分布式句子编码器的无监督学习方法。使用从书籍中提取的连续文本，训练了一个编码器-解码器模型，借鉴了word2vec中skip-gram模型，通过一句话来预测这句话的上一句和下一句。语义和语法属性一致的句子被映射到相似的向量表示。接着引入一个简单的词汇扩展方法来编码不再训练集内的单词，令词汇量扩展到一百万词。本文的模型被称为skip-thoughts，生成的向量称为skip-thought vector。模型采用了当下流行的端到端框架，通过搜集了大量的小说作为训练数据集，将得到的模型中encoder部分作为feature extractor，可以给任意句子生成vector。 skip-thought模型结构借助了skip-gram的思想。在skip-gram中，是以中心词来预测上下文的词；在skip-thought同样是利用中心句子来预测上下文的句子。 论文采用了GRU-RNN作为encoder和decoder，encoder部分的最后一个词的hiddenstate作为decoder的输入来生成词。这里用的是最简单的网络结构，并没有考虑复杂的多层网络、双向网络等提升效果。decoder部分也只是一个考虑了encoder last hidden state的语言模型，并无其他特殊之处，只是有两个decoder，是一个one maps two的情况，但计算方法一样。模型中的目标函数也是两个部分，一个来自于预测下一句，一个来自于预测上一句。我们将构造一个类似于自编码器的序列到序列结构，但是它与自编码器有两个主要的区别。第一，我们有两个 LSTM 输出层：一个用于之前的句子，一个用于下一个句子；第二，我们会在输出 LSTM 中使用教师强迫（teacher forcing）。这意味着我们不仅仅给输出 LSTM 提供了之前的隐藏状态，还提供了实际的前一个单词（可在上图和输出最后一行中查看输入）。 看上去，Skip-thought和Skip-gram挺象。唯一的遗憾是Skip-thought的decoder那部分，它是作为language modeling来处理的. 从这里的讲解知道这个是不存在 ”正负“样本的， 这个的损失函数是 正确的上下句和生成的上下句之间的reconstruction error。 The end product of Skip-Thoughts is the Encoder. The Decoders are thrown away after training. The trained encoder can then be used to generate fixed length representations of sentences which can be used for several downstream tasks such as sentiment classification, semantic similarity, etc. The encoder utilises a word embedding layer that serves as a look up table. This converts each word in the input sentence to its corresponding word embedding, effectively converting the input sentence into a sequence of word embeddings. This embedding layer is also shared with both of the decoders. The model is then trained to minimise the reconstruction error of the previous and next sentences using the resulting embedding h(i) generated from sentence s(i) after it is passed through the encoder. Back propagating the reconstruction error from the decoder allows the encoder to learn the best representation of the input sentence while capturing the relation between itself and the surrounding sentences.Skip-Thoughts is designed to be a sentence encoder and the result is that the decoders are actually discarded after the training process. The encoder along with the word embedding layer is used as a feature extractor able to encode new sentences that are fed through it. Using cosine similarity on the resulting encoded sentence embeddings, provides a powerful semantic similarity mechanism, where you can measure how closely two sentences relate in terms of meaning as well as syntax. Encoder Network: The encoder is typically a GRU-RNN which generates a fixed length vector representation h(i) for each sentence S(i) in the input. The encoded representation h(i) is obtained by passing final hidden state of the GRU cell (i.e. after it has seen the entire sentence) to multiple dense layers. Decoder Network: The decoder network takes this vector representation h(i) as input and tries to generate two sentences — S(i-1) and S(i+1), which could occur before and after the input sentence respectively. Separate decoders are implemented for generation of previous and next sentences, both being GRU-RNNs. The vector representation h(i) acts as the initial hidden state for the GRUs of the decoder networks. 词汇扩展 作者在训练完过后用在Google News dataset上预训练的模型对Vocabulary进行了词汇扩展主要是为了弥补我们的 Decoder 模型中词汇不足的问题。具体的做法就是：(from https://www.cnblogs.com/jiangxinyang/p/9638991.html) 该思路借鉴于Tomas Mikolov的一篇文章Exploiting Similarities among Languages for Machine Translation中解决机器翻译missing words问题的思路，对训练集产生的词汇表V(RNN)进行了扩展，具体的思路可参考Mikolov的文章，达到的效果是建立了大数据集下V(Word2Vec)和本文V(RNN)之间的映射，V(Word2Vec)的规模远远大于V(RNN)，论文中V(RNN)包括了20000个词，V(Word2Vec)包括了930000多个词，成功地解决了这一问题，使得本文提出的无监督模型有大的应用价值。 评价观点这个方法只是适用于长文本，要求是至少有两个衔接的句子，思想和skip-gram 比较相近。 源码google 的实现作者的实现论文 Quick-Thought vectors2018年发表的论文An efficient framework for learning sentence representations提出了一种简单且有效的框架用于学习句子表示。和常规的编码解码类模型（如skip-thoughts和SDAE）不同的是，本文采用一种分类器的方式学习句子表示。具体地，模型的输入为一个句子$s$以及一个候选句子集合$S_{cand}$，其中$S_{cand}$包含一个句子$s_{ctxt}$是$s$的上下文句子（也就是$s $)的前一个句子或后一个句子）以及其他不是$s$上下文的句子。模型通过对$s$以及$S_{cand}$中的每个句子进行编码，然后输入到一个分类器中，让分类器选出$S_{cand}$中的哪个句子是$s_{ctxt}$。实验设置候选句子集合大小为3，即$S_{cand}​$包含1个上下文句子和两个无关句子。模型结构如下： 模型有如下两个细节需要注意：模型使用的分类器（得分函数）$c$非常简单，是两个向量内积，即$c(u, v)=u^Tv$，计算$s$的embedding与所有$S_{cand}$中的句子向量内积得分后，输入到softmax层进行分类。使用简单分类器是为了引导模型着重训练句子编码器，因为我们的目的是为了得到好的句子向量表示而不是好的分类器。虽然某些监督任务模型如文本蕴含模型是参数共享的，$s$的编码器参数和候选句子编码器参数是不同的（不共享），因为句子表示学习往往是在大规模语料上进行训练，不必担心参数学习不充分的问题。测试时，给定待编码句子$s$，通过该模型得到的句子表示是两种编码器的连结 $[ f ( s ) ;g ( s ) ]$。 看上去，Skip-thought和Skip-gram挺象。唯一的遗憾是Skip-thought的decoder那部分，它是作为language modeling来处理的。而Skip-gram则是利用一个classifier预测周围的词(通过hierarchical softmax 或者negative sampling）。QT针对这个问题，对decoder部分做了大的调整，它直接把decoder拿掉，取而代之的是一个classifier。这个classifier负责预测哪些句子才是context sentences。 QT的classifier取代了Skip-thought的Decoder。这样做的好处是运行的速度大大提升了，用判别问题取代了生成式问题。有趣的是，虽然QT出现的比Skip-thought更晚，但是方法更简单，也更加接近Word2Vec算法。QT是一种新的state-of-art的算法。它不光效果好，而且训练时间要远小于其他算法。在算法方法上和效果上，都可称为是句子表征界的Word2Vec一般的存在。和前面几篇介绍的不同算法放在一起比较，同样都是为了找到好的句子表征，它们采取了不同的路径：InferSent在寻找NLP领域的ImageNet, 它的成功更像是在寻找数据集和任务上的成功，当然它成功的找到了SNLI; Concatenated p-means在寻找NLP领域的convolutional filter, 即怎样才能更好的提炼出句子级别的特征，它找到了p-means操作，以及利用了不同的embeddings; QT则是直接在算法层面上，寻找句子级别的Word2Vec, 算法上的改进让它受益。我们看到不同的方法在不同的方向上都作出了努力和取得了成效，很难讲哪种努力会更有效或者更有潜力。唯一唯一可以肯定的是，从应用层面上来讲，合适的才是最好的。 Supervised Sentence EmbedInferSent来自论文Supervised Learning of Universal Sentence Representations from Natural Language Inference Data，更多信息参考 这里 Multi-task learning Sentence EmbedUniversal Sentence Encoder来自论文 Universal Sentence Encoder，更多信息参考 Universal Sentence Encoder]]></content>
      <categories>
        <category>NLP</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[NLP Papers Reading- BERT]]></title>
    <url>%2F2019%2F05%2F27%2Fpaper-reading-nlp%2F</url>
    <content type="text"><![CDATA[nlp 论文阅读笔记, 随时 update… attention is all you needSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The ouput is computed as a weighted sum of the values, where the weight assigned to each value is computed cy a compatibility function of the query with the corresponding key. 中文的理解：深度学习里的Attentionmodel其实模拟的是人脑的注意力模型，举个例子来说，当我们观赏一幅画时，虽然我们可以看到整幅画的全貌，但是在我们深入仔细地观察时，其实眼睛聚焦的就只有很小的一块，这个时候人的大脑主要关注在这一小块图案上，也就是说这个时候人脑对整幅图的关注并不是均衡的，是有一定的权重区分的。这就是深度学习里的AttentionModel的核心思想。所谓注意力机制，就是说在生成每个词的时候，对不同的输入词给予不同的关注权重。通过注意力机制，我们将输入句子编码为一个向量序列，并自适应地选择这些向量的一个子集，同时对译文进行译码，例如where are you——&gt;你在哪？现在我们在翻译“你”的时候给”you”更多的权重，那么就可以有效的解决对齐问题。 Background: 主要是面临的三个问题。 Model: Encoder: encoder由6个相同的层堆叠而成，每个层有两个子层。第一个子层是多头自我注意力机制(multi-head self-attention mechanism)，第二层是简单的位置的全连接前馈网络(position-wise fully connected feed-forward network)。在两个子层中会使用一个残差连接，接着进行层标准化(layer normalization)。也就是说每一个子层的输出都是LayerNorm(x + sublayer(x))。网络输入是三个相同的向量q, k和v，是word embedding和position embedding相加得到的结果。为了方便进行残差连接，我们需要子层的输出和输入都是相同的维度。 Decoder: decoder也是由N（N=6）个完全相同的Layer组成，decoder中的Layer由encoder的Layer中插入一个Multi-Head Attention + Add&amp;Norm组成。输出的embedding与输出的position embedding求和做为decoder的输入，经过一个Multi-HeadAttention + Add&amp;Norm（（MA-1）层，MA-1层的输出做为下一Multi-Head Attention + Add&amp;Norm（MA-2）的query（Q）输入，MA-2层的Key和Value输入（从图中看，应该是encoder中第i（i = 1,2,3,4,5,6）层的输出对于decoder中第i（i = 1,2,3,4，5,6）层的输入）。MA-2层的输出输入到一个前馈层（FF），经过AN操作后，经过一个线性+softmax变换得到最后目标输出的概率。 对于decoder中的第一个多头注意力子层，需要添加masking，确保预测位置i的时候仅仅依赖于位置小于i的输出。 层与层之间使用的Position-wise feed forward network。 从整体上来看，Transformer依旧是一个“Sequence to Sequence”框架，拥有Encoder和Decoder两部分： Transformer的Encoder其实有6层，Decoder也有6层，从Encoder的角度，低层的Encoder是表层的词法信息，逐步向上进行抽象之后，在上层将表示抽象语义信息。Encoder部分还在最上层连了几条线到每个Decoder的部分，这是为了在Decoder中进行Attention操作，Decoder的网络中和Encoder也有信息传递和交互的。最后一个特点是Decoder和Encoder画的大小是一样的，因为它们层的维度大小是一样的。 也就是说encoder的输出，会和每一层的decoder进行结合。Encoder和Decoder的内部结构： 模型的特点：Positional embedding；（位置嵌入向量——其实类似word2vec，处理的语序的信息）。multi-head attention; (多头注意力机制——点乘注意力的升级版本， 这个就类似ensemble的思想，不同的子空间的attention 进行融合）Position-wise Feed-Forward Networks（位置全链接前馈网络——MLP变形） 有两种常用的注意力函数，一种是加法注意力(additive attention)，另外一种是点乘注意力(dot-productattention)，论文所采用的就是点乘注意力，这种注意力机制对于加法注意力而言，更快，同时更节省空间。 加法注意力还是以传统的RNN的seq2seq问题为例子，加性注意力是最经典的注意力机制，它使用了有一个隐藏层的前馈网络（全连接）来计算注意力分配： 公式:$$\alpha _ { i j } = \frac { \exp \left( e _ { i j } \right) } { \sum _ { k = 1 } ^ { L } e _ { i k } }$$ Scaled Dot-Product这篇论文计算query和key相似度使用了dot-product attention，即query和key进行点乘（内积）来计算相似度。 Multi-Head Attention: 在实际中为了并行计算，可以在一组queries上计算注意力函数，将多个query堆叠成Q，同理keys和values也被堆叠成K和V，通过下面的公式来计算矩阵输出:self-attention 模型就是自己对自己求attention，即𝑄=𝐾=𝑉$$\text { Attention } ( Q , K , V ) = \operatorname { softmax } \left( \frac { Q K ^ { T } } { \sqrt { d _ { k } } } \right) V$$之所以用内积除以维度的开方，论文给出的解释是：假设Q和K都是独立的随机变量，满足均值为0，方差为1，则点乘后结果均值为0，方差为dk。也即方差会随维度dk的增大而增大，而大的方差导致极小的梯度(我认为大方差导致有的输出单元a（a是softmax的一个输出）很小，softmax反向传播梯度就很小（梯度和a有关））。为了避免这种大方差带来的训练问题，论文中用内积除以维度的开方，使之变为均值为0，方差为1。 除了计算一个单独的注意力函数，论文提出对queries，keys和values做h次不同的投影, 然后都经过Scaled Dot-Product Attention，将结果拼接在一起，最后通过一个线性映射输出，通过多头注意力，模型能够获得不同子空间下的位置信息。如下图所示，公式如下:$$\text {MultiHead} ( Q , K , V ) =Concat(head_1, head_2, …, head_h) W ^ { o }$$ Self-Attention那么首先要明白什么是Attention。从语言学的角度，它是表示词与词之间的关联关系，像下图所示，这是一个Self-Attention的示意，它这个it会和其他位置的词发生关系，颜色越深的是说关系越紧密，从中图中看到它很正确的关联到了animal它实际指代的一个词。从机器学习的角度，这个Attention是神经网络隐层之间一个相似度的表示，什么是Self-Attention？就是表示句子内部词与词之间的关联关系，就像这里的it到animal，可以用于指代消解等问题。 Positional EncodingPosition Embedding，也就是“位置向量”，将每个位置编号，然后每个编号对应一个向量，通过结合位置向量和词向量，就给每个词都引入了一定的位置信息，这样Attention就可以分辨出不同位置的词了。 Residual connection和layer-normalization（这两个操作主要是应对 深度网络而提出的）对于学习CV的人估计对这个结构一点也不陌生，Residual connection是对于较为深层的神经网络有比较好的作用，比如网络层很深时，数值的传播随着weight不断的减弱，Residual connection是从输入的部分，就是图中虚线的部分，实际连到它输出层的部分，把输入的信息原封不动copy到输出的部分，减少信息的损失。 layer-normalization这种归一化层是为了防止在某些层中由于某些位置过大或者过小导致数值过大或过小，对神经网络梯度回传时有训练的问题，保证训练的稳定性，这是神经网络设计比较常用的case。 结论：self-attention层的好处是能够一步到位捕捉到全局的联系，解决了长距离依赖，因为它直接把序列两两比较（代价是计算量变为 O(n2)，当然由于是纯矩阵运算，这个计算量相当也不是很严重），而且最重要的是可以进行并行计算。相比之下，RNN 需要一步步递推才能捕捉到，并且对于长距离依赖很难捕捉。而 CNN 则需要通过层叠来扩大感受野，这是 Attention 层的明显优势。 A simple but tough-to-beat baseline for sentence embeddings这种motivation 还是很值得好好看的，验证论文的好坏是可以通过看最后的效果/ 结果是否按照 motivation 那样的。 “relative weights” 是针对 word2vec 中的效果改进的，从motivation 的角度没有考虑到 melo or BERT 中的 context . 当然论文的创新点在于 无监督学习方法，当别人都在转向有监督和多任务的时候，在缩短运行时间的同时，最后的效果和神经网络旗鼓相当。 Taking the average of the word embeddings in a sentence tends to give too much weight to words that are quite irrelevant, semantically speaking. Smooth Inverse Frequency tries to solve this problem in two ways: Weighting: like our tf-idf baseline above, SIF takes the weighted average of the word embeddings in the sentence. Every word embedding is weighted by a/(a + p(w)), where a is a parameter that is typically set to 0.001 and p(w) is the estimated frequency of the word in a reference corpus. Common component removal: next, SIF computes the principal component of the resulting embeddings for a set of sentences. It then subtracts from these sentence embeddings their projections on their first principal component. This should remove variation related to frequency and syntax that is less relevant semantically.As a result, SIF downgrades unimportant words such as but, just, etc., and keeps the information that contributes most to the semantics of the sentence. 本文是用无监督方法做句子级别的 embedding，用的是一个十分简单但却又很有效的传统方法，这在神经网络泛滥的年代算是一股清流了。这张图上的信息还是很多的，所以好好归纳整理一下。尽管长期以来句子的无监督表示学习是主流，最近几个月（2017年末/2018年初），我们看到了许多非常有趣的工作，显示了向监督学习和多任务学习转向的趋势。 强力/迅速的基线：FastText、词袋（Bag-of-Words） 当前最先进模型：ELMo、Skip-Thoughts、Quick-Thoughts、 InferSent、MILA/MSR的General Purpose Sentence Representations、Google的Universal Sentence Encoder 关于nlp 中的word embedding 是可以有 phrases, sentences, and paragraphs 三个不同类别的 embedding，所以还是挺好的。 近五年来提出了大量词嵌入方法。其中最常用的模型是word2vec和GloVe，这两个模型都是基于分布假说（distributional hypothesis）的无监督方法。（根据分布假说，出现在相同上下文中的单词倾向于具有相似的含义）。尽管有一些工作通过并入语义或语法知识等增强这些无监督方法，纯无监督方法在2017-2018年期间取得了有趣的进展，其中最重大的是FastText（word2vec的扩展）和ELMo（当前最先进的上下文词向量）。 在ELMo中，嵌入基于一个双层的双向语言模型（biLM）的内部状态计算，ELMo也是因此得名的：Embeddings from Language Models（来自语言模型的嵌入）。ELMo的特性：ELMo的输入是字符而不是单词。这使得它可以利用子字（sub-word）单元为词汇表以外的单词计算有意义的表示（和FastText类似）。ELMo是biLM的多层激活的连接（concatenation）。语言模型的不同层编码了单词的不同信息。连接所有层使得ELMo可以组合多种词表示，以提升下游任务的表现。 普适句嵌入词袋方法这一领域的一般共识是，直接平均一个句子的词向量这一简单方法（所谓词袋方法），为许多下游任务提供了强力的基线。Arora等去年在ICLR发表的论文A Simple but Tough-to-Beat Baseline for Sentence Embeddings提供了一个很好的算法：选择一种流行的词嵌入，编码句子为词向量的线性加权组合，然后进行相同成分移除（根据首要主成分移除向量投影）。这一通用方法具有深刻而强大的理论动机，基于在语篇向量上随机行走以生成文本的生成式模型。 （有人实践）这个方法在短文本上效果更好，在语料不足的时候效果不能保证。这种模型没有考虑词顺序（也可以说只能理解词意思，但是不能理解语义），而深度网络模型是可以考虑语义的。可能再相似度问题上可以取得比较好的效果，但是在文本分类，情感分类上效果一般。 思考：从直觉上理解, 短文本上的 word2vec、SIF 这种没有 handle 语序的模型得到的效果就已经足够的好，对于中长文本（句子、段落等）elmo 和BERT 这种模型的效果是更加的。 程序的运行只需要十几分钟，与神经网络的效果旗鼓相当。 Supervised Learning of Universal Sentence Representations from Natural Language Inference Data文章成功的找到了NLP领域的ImageNet — SNLI (Stanford Natural Language Inference dataset), 并且试验了不同的深度学习模型，最终确定bi-LSTM max pooled 为最佳模型。 域 数据 任务 模型(编码器) CV ImageNet image classification Le-Net, VGG-Net, Google-Net, ResNet, DenseNet NLP SNLI NLI ? 基于监督学习方法学习sentence embeddings可以归纳为两个步骤：第一步选择监督训练数据，设计相应的包含句子编码器Encoder的模型框架；第二步选择（设计）具体的句子编码器，包括DAN、基于LSTM、基于CNN和Transformer等。 数据集： 本文采用的是Stanford Natural Language Inference Datasets，简称SNLI （NLP领域的ImageNet ）。SNLI包含570K个人类产生的句子对，每个句子对都已经做好了标签，标签总共分为三类：蕴含、矛盾和中立（Entailment、contradiction and neutral）。下面是这些数据集的一个例子： 从上图可以看出，每个句子对为（text, hypothesis）,中间的judgments为它们的标签。可以看到标签是综合了5个专家的意见，根据少数服从多数的原则得到的。 7种不同的architectures： standard recurrent encoders with LSTM ，取最后一个隐状态 standard recurrent encoders with GRU ，取最后一个隐状态上述两种是基础的recurrent encoder，在句子建模中通常将网络中的最后一个隐藏状态作为sentence representation； conncatenation of last hidden states of forward and backward GRU这种方法是将单向的网络变成了双向的网络，然后用将前向和后向的最后一个状态进行连接，得到句子向量； Bi-directional LSTMs (BiLSTM) with mean pooling Bi-directional LSTMs (BiLSTM) with max pooling这两种方法使用了双向LSTM结合一个pooling层的方法来获取句子表示，具体公式如下： self-attentive network这个网络在双向LSTM的基础上加入了attention机制，具体网络结构如下： hierarchical convolutional networks Now that we have discussed the various sentence encoding architectures used in the paper, let’s go through the part of the network which takes these sentence embeddings and predicts the output label. After the sentence vectors are fed as input to this model, 3 matching methods are applied to extract relations between the text, u and hypothesis, v – concatenation of the two representations (u, v) element-wise product u * v and, absolute element-wise difference |u – v | The resulting vector captures information from both the text, u and the hypothesis, v, and is fed into a 3-class classifier consisting of multiple fully connected layers followed by a softmax layer. Universal Sentence Encoder这篇文章基于InferSent， 也是想找到一个universal encoder。不同之处在于文章把InferSent的bi-lstm换成了DAN（或者Transformer)，而使用DAN这样“简单”的encoder的效果竟然相当好（尤其是时间和内存消耗和其他算法比小很多。） The Google Sentence Encoder is Google’s answer to Facebook’s InferSent. It comes in two forms: an advanced model that takes the element-wise sum of the context-aware word representations produced by the encoding subgraph of a Transformer model. a simpler Deep Averaging Network (DAN) where input embeddings for words and bigrams are averaged together and passed through a feed-forward deep neural network.The Transformer-based model tends to give better results, but at the time of writing, only the DAN-based encoder was available. In contrast to InferSent, the Google Sentence Encoder was trained on a combination of unsupervised data (in a skip-thought-like task) and supervised data (the SNLI corpus). DAN其实DAN(Deep Averaging Networks)应该属于Bag of Words类的算法。因为比较特殊，单独列出来。 它是在对所有词语取平均后，在上面加上几层神经网络。特殊的地方在于它在sentiment analysis中表现也不错，这在BOW类方法中比较罕见。 新方法 类型 基于的旧算法 贡献 SIF 无监督 BOW 一个简单而有效的baseline算法 InferSent 监督 NA 找到了NLP领域的ImageNet – SNLI， 并给出了一个state-of-art 算法 P-mean 无监督 BOW 比SIF更简单且有效的一个算法且适用于cross-lingual Universal-sentence-encoder 监督 InferSent 更加简单的encoder 文章共提出两种基于不同网络架构的Universal Sentence Encoder：Transformer and Deep Averaging Network (DAN).Our two encoders have different design goals. One based on the transformer architecture targets high accuracy at the cost of greater model complexity and resource consumption. The other targets efficient inference with slightly reduced accuracy. deep contextualized word representationsintroduction: 这种embedding -context 必要性的介绍，感觉是有更好，没有也是能够理解的。Why do we need contextualized representations?As an illustrative example, take the following two sentences: “The bank on the other end of the street was robbed”“We had a picnic on the bank of the river” Both sentences use the word “bank”, but the meaning of the word differs completely between them. This phenomenon where two identical words change meaning depending on the context is known as “polysemy“, and has been an issue in the NLP deep learning community ever since word embeddings really took off. Most current neural networks are bad at handling polysemy because they use a single vector to represent the meaning of the word “bank”, regardless of the context. In reality, the vector representing any word should change depending on the words around it. 什么是一个好的词向量：ELMo能够学习到词汇用法的复杂性，比如语法、语义。ELMo能够学习不同上下文情况下的词汇多义性。 之前的做法的缺点是对于每一个单词都有唯一的一个embedding表示, 而对于多义词显然这种做法不符合直觉, 而单词的意思又和上下文相关, ELMo的做法是我们只预训练language model, 而word embedding是通过输入的句子实时输出的, 这样单词的意思就是上下文相关的了, 这样就很大程度上缓解了歧义的发生. 这种算法的特点是：每一个word representation都是整个输入语句的函数。具体做法就是先在大语料上以language model为目标训练出bidirectional LSTM模型，然后利用LSTM产生词语的表征。ELMo故而得名(Embeddings from Language Models)。为了应用在下游的NLP任务中，一般先利用下游任务的语料库(注意这里忽略掉label)进行language model的微调,这种微调相当于一种domain transfer; 然后才利用label的信息进行supervised learning。 （这个描述跟 cv 是惊人的相似）ELMo表征是“深”的，就是说它们是biLM的所有层的内部表征的函数。这样做的好处是能够产生丰富的词语表征。高层的LSTM的状态可以捕捉词语意义中和语境相关的那方面的特征(比如可以用来做语义的消歧)，而低层的LSTM可以找到语法方面的特征(比如可以做词性标注)。如果把它们结合在一起，在下游的NLP任务中会体现优势。 Salient featuresELMo representations are: Contextual: The representation for each word depends on the entire context in which it is used. Deep: The word representations combine all layers of a deep pre-trained neural network. Character based: ELMo representations are purely character based, allowing the network to use morphological clues to form robust representations for out-of-vocabulary tokens unseen in training. related work: 针对传统词向量是固定的，与上下文语境无关的缺点，先前的工作多通过两种方式来解决： (1) 通过引入字符级(subword)信息丰富词向量表达； (2) 学习每个单词不同含义的独立向量； ELMo也利用了字符卷积（Character-Convolutions）引入字符级信息，并同时结合了深度双向语言模型的各层隐状态来丰富词向量表达。 P.s.：基于字符的模型不仅能够通过引入字符级信息丰富词向量表达，也能够在很大程度上解决NLP领域的OOV（Out-Of-Vocabulary）问题。 ELMo用到上文提到的双向的language model, 给定N个tokens (t1, t2,…,tN), language model通过给定前面的k-1个位置的token序列计算第k个token的出现的概率:$$p \left( t _ { 1 } , t _ { 2 } , \ldots , t _ { N } \right) = \prod _ { k = 1 } ^ { N } p \left( t _ { k } | t _ { 1 } , t _ { 2 } , \ldots , t _ { k - 1 } \right)$$后向的计算方法与前向相似: $$p \left( t _ { 1 } , t _ { 2 } , \ldots , t _ { N } \right) = \prod _ { k = 1 } ^ { N } p \left( t _ { k } | t _ { k + 1 } , t _ { k + 2 } , \ldots , t _ { N } \right)$$biLM训练过程中的目标就是最大化:$$\sum _ { k = 1 } ^ { N } \left( \log p \left( t _ { k } | t _ { 1 } , \ldots , t _ { k - 1 } ; \Theta _ { x } , \vec { \Theta } _ { L S T M } , \Theta _ { s } \right) + \log p \left( t _ { k } | t _ { k + 1 } , \ldots , t _ { N } ; \Theta _ { x } , \overline { \Theta } _ { L S T M } , \Theta _ { s } \right) \right)$$ ELMo对于每个token $t_k$, 通过一个L层的biLM计算出2L+1个表示:$$R_{ k } = \left{ x _ { k } ^ { L M } , \vec { h } _ { k , j } ^ { L M } , h _ { k , j } ^ { L M } | j = 1 , \ldots , L \right} = \left{ h _ { k , j } ^ { L M } | j = 0 , \ldots , L \right}$$其中$h _ { k , 0 } ^ { L M }$是对token进行直接编码的结果(这里是字符通过CNN编码), $h _ { k , j } ^ { L M } = \left[ \vec { h } _ { k , j } ^ { L M } ; \overline { h } _ { k , j } \right]$ 是每个biLSTM层输出的结果. 在实验中还发现不同层的biLM的输出的token表示对于不同的任务效果不同. 应用中将ELMo中所有层的输出R压缩为单个向量, ELMok=E(Rk;Θϵ), 最简单的压缩方法是取最上层的结果做为token的表示:$E \left( R _ { k } \right) = h _ { k , L } ^ { L M }$ 更通用的做法是通过一些参数来联合所有层的信息:$$E L M o _ { k } ^ { t a s k } = E \left( R _ { k } ; \Theta ^ { t a s k } \right) = \gamma ^ { t a s k } \sum _ { j = 0 } ^ { L } s _ { j } ^ { t a s k } h _ { k , j } ^ { L M }$$ 其中$s_j$是一个softmax出来的结果, $γ$是一个任务相关的scale参数, 我试了平均每个层的信息和学出来$s_j$发现学习出来的效果会好很多. 文中提到$γ$在不同任务中取不同的值效果会有较大的差异, 需要注意, 在SQuAD中设置为0.01取得的效果要好于设置为1时. ELMo: Context Matters Instead of using a fixed embedding for each word, ELMo looks at the entire sentence before assigning each word in it an embedding. It uses a bi-directional LSTM trained on a specific task to be able to create those embeddings. ELMo provided a significant step towards pre-training in the context of NLP. The ELMo LSTM would be trained on a massive dataset in the language of our dataset, and then we can use it as a component in other models that need to handle language. What’s ELMo’s secret? ELMo gained its language understanding from being trained to predict the next word in a sequence of words - a task called Language Modeling. This is convenient because we have vast amounts of text data that such a model can learn from without needing labels. We can see the hidden state of each unrolled-LSTM step peaking out from behind ELMo’s head. Those come in handy in the embedding proecss after this pre-training is done. ELMo actually goes a step further and trains a bi-directional LSTM – so that its language model doesn’t only have a sense of the next word, but also the previous word.ELMo comes up with the contextualized embedding through grouping together the hidden states (and initial embedding) in a certain way (concatenation followed by weighted summation). lstm-based language modelIn case you are unfamiliar with language models, a language model is simply a model that can predict how “likely” a certain sequence of words is to be a real piece of text. This is generally done by training a model to take a part of sentence (say, the first n words) and predict the next word – or more precisely, output the probability of each word in the vocabulary being the next word (In this blog post, we’ll focus on LSTM-based language models which are the focus of this paper). One trick that this paper uses is to train a language model with reversed sentences that the authors call the “backward” language model.这种模型：上一个模型的输出到下一个模型输入Furthermore, instead of using a single-layer LSTM, this paper uses a stacked, multi-layer LSTM. Whereas a single-layer LSTM would take the sequence of words as input, a multi-layer LSTM trains multiple LSTMs to take the output sequence of the LSTM in the previous layer as input (of course, the first layer takes the sequence of words as input). This is best illustrated in the following illustration: 最后的embedding 是是将不同的层 combination起来，这个系数是通过学习出来的。In ELMo, the part that is task specific is the combination of the task-agnostic representations. The weight is learned for each task and normalized using the softmax function. The parameter $\gamma$ is a task-dependent value that allows for scaling the entire vector, which is important during optimization.]]></content>
      <categories>
        <category>NLP</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Dynamic Programming Examples]]></title>
    <url>%2F2019%2F05%2F27%2FDynamic-Programming-Examples%2F</url>
    <content type="text"><![CDATA[Dynamic Programming is a method for solving a complex problem by breaking it down into a collection of simpler subproblems, solving each of those subproblems just once, and storing their solutions using a memory-based data structure (array, map, etc.). So the next time the same sub-problem occurs, instead of recomputing its solution, one simply looks up the previously computed solution, thereby saving computation time. knapsack problem: https://leetcode.com/problems/partition-equal-subset-sum/ Given a non-empty array containing only positive integers, find if the array can be partitioned into two subsets such that the sum of elements in both subsets is equal. Actually, this is a 0/1 knapsack problem, for each number, we can pick it or not. Let us assume dp[i][j] means whether the specific sum j can be gotten from the first i numbers. If we can pick such a series of numbers from 0-i whose sum is j, dp[i][j] is true, otherwise it is false.Base case: dp[0][0] is true; (zero number consists of sum 0 is true)Transition function: For each number, if we don’t pick it, dp[i][j] = dp[i-1][j], which means if the first i-1 elements has made it to j, dp[i][j] would also make it to j (we can just ignore nums[i]). If we pick nums[i]. dp[i][j] = dp[i-1][j-nums[i]], which represents that j is composed of the current value nums[i] and the remaining composed of other previous numbers. Thus, the transition function is dp[i][j] = dp[i-1][j] || dp[i-1][j-nums[i]] 12345678910111213141516171819202122class Solution(object): def canPartition(self, nums): """ :type nums: List[int] :rtype: bool """ n =len(nums) s = sum(nums) if s &amp;1 ==1: return False dp =[0 for _ in range(s+1)] dp[0] =1 #import ipdb for num in nums: for i in range(s, -1, -1): # ipdb.set_trace() if dp[i]: dp[i+num] =1 if dp[s//2]: return True return False longest commone substring Given two strings ‘X’ and ‘Y’, find the length of the longest common substring. Input : X = “GeeksforGeeks”, y = “GeeksQuiz”Output : 5 1234567891011121314151617181920212223242526272829"""Given two strings ‘X’ and ‘Y’, find the length of the longest common substring."""class Solution(object): def minDistance(self, word1, word2): m =len(word1) n =len(word2) #dp=[ [None] for _ in range(n+1) for _ in range(m+1)] dp = [[None] *(n +1) for _ in range(m+1) ] for i in range(m+1): for j in range(n+1): if i ==0 or j ==0: dp[i][j] =0 # 这个是python 中语法决定的 word1[len(word1)] 是访问不到的，这个访问是从0开始的，所以只能是这样的 elif word1[i-1] == word2[j-1]: dp[i][j] =dp[i-1][j-1] +1 else: dp[i][j] =max(dp[i-1][j], dp[i][j-1]) return dp[m][n]solution =Solution()word1 ='abcdaf'word2 ='acbcf'result =solution.minDistance(word1, word2)print(result) matrix chain multiplication Given a sequence of matrices, find the most efficient way to multiply these matrices together. The problem is not actually to perform the multiplications, but merely to decide in which order to perform the multiplications. dp 的思想就是借助之前的 subquestion 的结果，然后计算更大的question，这个的核心在于减少了重复子单元的计算。 1234567891011121314151617181920212223242526272829303132333435import sys# 就是这个 l 和1 是很不容易分清楚的，所以这个是慎重使用的 def MatrixChainOrder(list1, len1): """ :param list1: list of matrix style :param l: len1gth of list :return: """ dp = [[0 for _ in range(len1)] for _ in range(len1)] for i in range(1, len1): dp[i][i] = 0 # l 个matrix 连成的意思 for ll in range(2, len1): for i in range(1, len1 - ll + 1): j = i + ll - 1 dp[i][j] = sys.maxint for k in range(i, j): tmp = dp[i][k] + dp[k + 1][j] + list1[i - 1] * list1[k] * list1[j] if tmp &lt; dp[i][j]: dp[i][j] = tmp return dp[1][len1 - 1]# Driver program to test above functionarr = [1, 2, 3, 4]arr1 = [2, 3, 6, 4, 5]size = len(arr)size1 = len(arr1)print("Minimum number of multiplications is " + str(MatrixChainOrder(arr, size)))print("Minimum number of multiplications is " + str(MatrixChainOrder(arr1, size1)))]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Beyond Word Embedding]]></title>
    <url>%2F2019%2F05%2F22%2Fbeyond-word-embedding%2F</url>
    <content type="text"><![CDATA[从one-hot 到 word2vec， 到elmo，简单介绍一下 NLP中词向量的过程。 进入word2vec 之前有必要先说一下 one-hot 编码。word embedding 属于稠密向量，然后one-hot属于稀疏向量； 前者想要达到的目的是在某个映射空间中 相近的词语能够在该空间中距离比较近。这是在克服one-hot的缺点：向量不能表示相似度。并且这种word embedding的size 是不随着字典的增大而线性增大的，这在存储和计算上都有很大的优势。 Traditional Word VectorsBefore diving directly into Word2Vec it’s worth while to do a brief overview of some of the traditional methods that pre-date neural embeddings. 这个是用来描述文章的，有一个大的dict，然后一片文章是如何进行表示、Bag of Words or BoW vector representations are the most common used traditional vector representation. Each word or n-gram is linked to a vector index and marked as 0 or 1 depending on whether it occurs in a given document. An example of a one hot bag of words representation for documents with one word. 局限性: 一方面只是一种counter，没有考虑语义信息；另一方面有些 words 是明显的 relevant than others.BoW representations are often used in methods of document classification where the frequency of each word, bi-word or tri-word is a useful feature for training classifiers. One challenge with bag of word representations is that they don’t encode any information with regards to the meaning of a given word.In BoW word occurrences are evenly weighted independently of how frequently or what context they occur. However in most NLP tasks some words are more relevant than others. 这个是可以认识是对于 bag of words “relevant” 上的改进：使得 选择的words 更加的 “representative” 文章的调性。TF-IDF, short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word or n-gram is to a document in a collection or corpus. They provide some weighting to a given word based on the context it occurs.The tf–idf value increases proportionally to the number of times a word appears in a document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently than others. 但是对于 bag of words 中“没有体现语义” 的缺陷还是没有 deal with。However even though tf-idf BoW representations provide weights to different words they are unable to capture the word meaning. 这个名字只是因为有定义而存在的名字（网络模型 or 深度网络的出现就是为了 handle 语义信息）Distributional Embeddings enable word vectors to encapsulate contextual context. Each embedding vector is represented based on the mutual information it has with other words in a given corpus.重点就是这种方式是要 predict a target word from context words，一定是要能够体现语境的。Predictive models learn their vectors in order to improve their predictive ability of a loss such as the loss of predicting the vector for a target word from the vectors of the surrounding context words. word2vec 两种类型 word2vec 是一种思想，有两种CBOW 和skip-gram 两种实现。Word2Vec is a predictive embedding model. There are two main Word2Vec architectures that are used to produce a distributed representation of words: CBOW中的 context words 没有体现 words order，但是训练速度快。Continuous bag-of-words (CBOW) — The order of context words does not influence prediction (bag-of-words assumption). skip-gram weights nearby context words heavily 效果相对于 cbow 更加， 但训练速度相对比较慢。Continuous skip-gram weighs nearby context words more heavily than more distant context words. While order still is not captured each of the context vectors are weighed and compared independently vs CBOW which weighs against the average context. CBOW is faster while skip-gram is slower but does a better job for infrequent words. glove （g lou v）这个不属于 word2vec，其只是考虑了 local context，没有考虑 global contextBoth CBOW and Skip-Grams are “predictive” models, in that they only take local contexts into account. word2vec does not take advantage of global context.(细节 能看懂就看) GloVe embeddings by contrast leverage the same intuition behind the co-occurrence matrix (共生矩阵) used distributional embeddings, but uses neural methods to decompose the co-occurrence matrix into more expressive and dense word vectors.效果：和word2vec 相比没有 definitively better resultsWhile GloVe vectors are faster to train, neither GloVe or Word2Vec has been shown to provide definitively better results rather they should both be evaluated for a given dataset. 说一下word2vec 和glove 的区别：Predictive的模型，如Word2vec，根据context预测中间的词汇，要么根据中间的词汇预测context，分别对应了word2vec的两种训练方式cbow和skip-gram。Count-based模型，如GloVe，本质上是对共现矩阵进行降维。首先，构建一个词汇的共现矩阵，每一行是一个word，每一列是context。共现矩阵就是计算每个word在每个context出现的频率。由于context是多种词汇的组合，其维度非常大，我们希望像network embedding一样，在context的维度上降维，学习word的低维表示。该向量表示属于 sparse vectors sparse vectors 词-文档矩阵(Term-document matrix) 和 词共现矩阵(Term-term matrix)。Term-document matrix表示每个单词在文档中出现的次数(词频)，每一行是一个 term，每一列是一个 document两篇文档的向量相似 =&gt; 两篇文档相似，如上图 doc3 和 doc4，我们就认为它们是相似的。两个单词的向量相似 =&gt; 两个单词相似，如上图的 fool 和 clown，就是相似的。 Term-term matrix然后我们可以考虑更小的粒度，更小的上下文，也就是不用整篇文档，而是用段落(paragraph)，或者小的窗口(window of ±4 words)，所以这个时候，向量就是对上下文单词的计数，大小不再是文档长度 |D|，而是窗口长度 |V| 了，所以现在 word-word matrix 是 |V|*|V| 而 word2vec 得到的向量 dense vectors。不使用 negative sampling 的Wordvec 非常快，但准确率不高（57.4\%）,毕竟模型没有告诉什么是无关的word，模型很难对无关词汇进行惩戒，提高准确率。对于 synonym 问题，word2vec 是好于 glove，但从最终的效果上看，两者是不分彼此的。glove 使用了整体的信息，word2vec 只是使用了局部信息（local context）。 fasttext这个主要是 each word + n-gram within each word， 最后的效果是好于 word2vec 的。FastText, builds on Word2Vec by learning vector representations for each word and the n-grams found within each word. The values of the representations are then averaged into one vector at each training step. While this adds a lot of additional computation to training it enables word embeddings to encode sub-word information. FastText vectors have been shown to be more accurate than Word2Vec vectors by a number of different measures. 简单说 fasttext 和word2vec 模型上的不同有两点： 模型的输出层：word2vec的输出层，对应的是每一个term，计算某term的概率最大；而fasttext的输出层对应的是 分类的label。不过不管输出层对应的是什么内容，起对应的vector都不会被保留和使用； fasttext则充分利用了h-softmax的分类功能，遍历分类树的所有叶节点，找到概率最大的label（一个或者N个） 模型的输入层：word2vec的输出层，是 context window 内的term；而fasttext对应的整个sentence的内容，包括term，也包括 n-gram的内容 更多信息可以参看博客。 overview of Neural NLP ArchitecturesDeep Feed Forward Networks 1D CNNs RNNs (LSTM/GRU) encoder- decoder 结构 attention and copy mechanisms这个是 attention 机制提出的背景：解决 句子中的长依赖；contextual impact (specific words may carry more importance at different steps)While in theory they can capture long term dependencies they tend to struggle modeling longer sequences, this is still an open problem. One cause for sub-optimal performance standard RNN encoder-decoder models for sequence to sequence tasks such as NER or translation is that they weight the impact each input vector evenly on each output vector when in reality specific words in the input sequence may carry more importance at different time steps.Attention mechanisms provide a means of weighting the contextual impact of each input vector on each output prediction of the RNN. These mechanisms are responsible for much of the current or near current state of the art in Natural language processing. attention In sum, algorithms can allocate attention, and they can learn how to do so, by adjusting the weights they assign to various inputs. Imagine a heat map over a photo. The heat is attention. 这个是要引出来 context word embeddings.One of the limits of traditional word vectors is that they presume that a word’s meaning is relatively stable across sentences.并不是物理上的二维关系能够表示词语之间的 relationship，有时候是需要高纬空间进行表示的。In fact, the strongest relationships binding a given word to the rest of the sentence may be with words quite distant from it.从 credit assignment的角度阐述了 neural networks 就是 allocating importance to input features。The fundamental task of all neural networks is credit assignment. Credit assignment is allocating importance to input features through the weights of the neural network’s model. Learning is the process by which neural networks figure out which input features correlate highly with the outcomes the net tries to predict, and their learnings are embodied in the adjusted quantities of the weights that result in accurate decisions about the data they’re exposed to.这个是传统的 LSTM （encoder -decoder） 模型，问题在于当句子过长（比如说大于20 words）之后，encoder 是无法 memory 之前的所有 words，所以效果就会变得差一些。 但是 attention 就是模仿了人翻译过程，一段作为一个单位，然后进行翻译。这样就可以持续保证较高中确率的输出。In neural networks, attention primarily serves as a memory-access mechanism. 每次的输出都是关注不同的地方，但是至于哪里更加重要，这个交给了 feedback mechanism 反向传播。下面的图片十分清晰的展示了 在翻译的过程中 “focus” 是不断地变化的。Above, a model highlights which pixels it is focusing on as it predicts the underlined word in the respective captions. Below, a language model highlights the words from one language, French, that were relevant as it produced the English words in the translation. As you can see, attention provides us with a route to interpretability. We can render attention as a heat map over input data such as words and pixels, and thus communicate to human operators how a neural network made a decision. (This could be the basis of a feedback mechanism whereby those humans tell the network to pay attention to certain features and not others.) (This could be the basis of a feedback mechanism whereby those humans tell the network to pay attention to certain features and not others.) Additionally in Machine Reading Comprehension and Summarization systems RNNs often tend to generate results, that while on first glance look structurally correct are in reality hallucinated or incorrect. One mechanism that helps mitigate some of these issues is the Copy Mechanism.copy mechanism 简单说来就是 word embedding or raw text.The copy mechanism is an additional layer applied during decoding that decides whether it is better to generate the next word from the source sentence or from the general embedding vocabulary. reading comprehension and summary 上面是说的在 machine translation，下面说的是 阅读理解 和 summary领域。Additionally in Machine Reading Comprehension and Summarization systems RNNs often tend to generate results, that while on first glance look structurally correct are in reality hallucinated or incorrect. One mechanism that helps mitigate some of these issues is the Copy Mechanism.copy mechanism 简单说来就是 decide word embedding from model or raw text.The copy mechanism is an additional layer applied during decoding that decides whether it is better to generate the next word from the source sentence or from the general embedding vocabulary. Taming Recurrent Neural Networks for Better Summarization有两种不同的 summarization:Two types of summarization：Extractive （You might think of these approaches as like a highlighter.） Abstractive（By the same analogy, these approaches are like a pen.）The great majority of existing approaches to automatic summarization are extractive – mostly because it is much easier to select text than it is to generate text from scratch.但是一个问题在于，只是使用 extrative way 可能得到相同的words，Problem 1: The summaries sometimes reproduce factual details inaccurately (e.g. Germany beat Argentina 3-2). This is especially common for rare or out-of-vocabulary words such as 2-0.Problem 2: The summaries sometimes repeat themselves (e.g. Germany beat Germany beat Germany beat…)Easier Copying with Pointer-Generator Networks。这个跟 attention 不是很相关，简单说就是In this way, the pointer-generator network is a best of both worlds, combining both extraction (pointing) and abstraction (generating). To tackle Problem 2 (repetitive summaries), we use a technique called coverage. The idea is that we use the attention distribution to keep track of what’s been covered so far, and penalize the network for attending to same parts again. elmo (e l mo)elmo 产生一个 embedding 是根据 context 产生的。ELMo is a model generates embeddings for a word based on the context it appears thus generating slightly different embeddings for each of its occurrence.（感觉理解一个概念都是 根据其 for example 进行理解的）For example, the word “play” in the sentence above using standard word embeddings encodes multiple meanings such as the verb to play or in the case of the sentence a theatre production. In standard word embeddings such as Glove, Fast Text or Word2Vec each instance of the word play would have the same representation. 参考blog:https://towardsdatascience.com/beyond-word-embeddings-part-2-word-vectors-nlp-modeling-from-bow-to-bert-4ebd4711d0ec http://www.abigailsee.com/2017/04/16/taming-rnns-for-better-summarization.html https://skymind.ai/wiki/word2vec]]></content>
      <categories>
        <category>NLP</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Hyper-parameter Optimization for Machine Learning]]></title>
    <url>%2F2019%2F05%2F22%2FHyperparameter-optimization-for-machine-learning%2F</url>
    <content type="text"><![CDATA[Following are four common methods of hyperparameter optimization for machine learning in order of increasing efficiency: Manual Grid search Random search Bayesian model-based optimization Random SearchFirst we will implement a common technique for hyperparameter optimization: random search. Each iteration, we choose a random set of model hyperparameters from a search space. Empirically, random search is very effective, returning nearly as good results as grid search with a significant reduction in time spent searching. However, it is still an uninformed method in the sense that it does not use past evaluations of the objective function to inform the choices it makes for the next evaluation. Random search uses the following four parts, which also are used in Bayesian hyperparameter optimization: Domain: values over which to search Optimization algorithm: pick the next values at random! (yes this qualifies as an algorithm) Objective function to minimize: in this case our metric is cross validation ROC AUC Results history that tracks the hyperparameters tried and the cross validation metric Random search can be implemented in the Scikit-Learn library using RandomizedSearchCV, however, because we are using Early Stopping (to determine the optimal number of estimators), we will have to implement the method ourselves (more practice!). This is pretty straightforward, and many of the ideas in random search will transfer over to Bayesian hyperparameter optimization. 123456789101112131415161718192021222324252627282930313233343536373839# Load librariesfrom scipy.stats import uniformfrom sklearn import linear_model, datasetsfrom sklearn.model_selection import RandomizedSearchCV# data and model# Load datairis = datasets.load_iris()X = iris.datay = iris.target# Create logistic regressionlogistic = linear_model.LogisticRegression()# Create Hyperparameter Search Space# Create regularization penalty space# 如果比较少，那么久枚举出来penalty = ['l1', 'l2']# 如果是有规律的连续的，就使用这种方式列举出来# Create regularization hyperparameter distribution using uniform distributionC = uniform(loc=0, scale=4)# Create hyperparameter optionshyperparameters = dict(C=C, penalty=penalty)# cv: cross validation, This approach involves randomly dividing the set of observations into k groups, or folds, of approximately equal size. The first fold is treated as a validation set, and the method is fit on the remaining k − 1 folds.# Create randomized search 5-fold cross validation and 100 iterationsclf = RandomizedSearchCV(logistic, hyperparameters, random_state=1, n_iter=100, cv=5, verbose=0, n_jobs=-1)# Fit randomized searchbest_model = clf.fit(X, y)# View best hyperparameters# 注意这种获取best params 的方式print('Best Penalty:', best_model.best_estimator_.get_params()['penalty'])print('Best C:', best_model.best_estimator_.get_params()['C'])# Predict target vectorbest_model.predict(X) grid search and random search 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364import numpy as npfrom time import timefrom scipy.stats import randint as sp_randintfrom sklearn.model_selection import GridSearchCVfrom sklearn.model_selection import RandomizedSearchCVfrom sklearn.datasets import load_digitsfrom sklearn.ensemble import RandomForestClassifier# get some datadigits = load_digits()X, y = digits.data, digits.target# build a classifierclf = RandomForestClassifier(n_estimators=20)# Utility function to report best scoresdef report(results, n_top=3): for i in range(1, n_top + 1): candidates = np.flatnonzero(results['rank_test_score'] == i) for candidate in candidates: print("Model with rank: &#123;0&#125;".format(i)) print("Mean validation score: &#123;0:.3f&#125; (std: &#123;1:.3f&#125;)".format( results['mean_test_score'][candidate], results['std_test_score'][candidate])) print("Parameters: &#123;0&#125;".format(results['params'][candidate])) print("")# specify parameters and distributions to sample fromparam_dist = &#123;"max_depth": [3, None], "max_features": sp_randint(1, 11), "min_samples_split": sp_randint(2, 11), "bootstrap": [True, False], "criterion": ["gini", "entropy"]&#125;# run randomized searchn_iter_search = 20random_search = RandomizedSearchCV(clf, param_distributions=param_dist, n_iter=n_iter_search, cv=5, iid=False)start = time()random_search.fit(X, y)print("RandomizedSearchCV took %.2f seconds for %d candidates" " parameter settings." % ((time() - start), n_iter_search))report(random_search.cv_results_)# use a full grid over all parametersparam_grid = &#123;"max_depth": [3, None], "max_features": [1, 3, 10], "min_samples_split": [2, 3, 10], "bootstrap": [True, False], "criterion": ["gini", "entropy"]&#125;# run grid searchgrid_search = GridSearchCV(clf, param_grid=param_grid, cv=5, iid=False)start = time()grid_search.fit(X, y)print("GridSearchCV took %.2f seconds for %d candidate parameter settings." % (time() - start, len(grid_search.cv_results_['params'])))report(grid_search.cv_results_) Random search without in-built function:write it yourself, more information.https://github.com/WillKoehrsen/hyperparameter-optimization/blob/master/Bayesian%20Hyperparameter%20Optimization%20of%20Gradient%20Boosting%20Machine.ipynb Bayesian Hyperparameter OptimizationThe one-sentence summary of Bayesian hyperparameter optimization is: build a probability model of the objective function and use it to select the most promising hyperparameters to evaluate in the true objective function. The basic idea is: spend a little more time selecting the next hyperparameters in order to make fewer calls to the objective function. In the case of hyperparameter optimization, the objective function is the validation error of a machine learning model using a set of hyperparameters. The aim is to find the hyperparameters that yield the lowest error on the validation set in the hope that these results generalize to the testing set. Evaluating the objective function is expensive because it requires training the machine learning model with a specific set of hyperparameters. Ideally, we want a method that can explore the search space while also limiting evaluations of poor hyperparameter choices. Bayesian hyperparameter tuning uses a continually updated probability model to “concentrate” on promising hyperparameters by reasoning from past results. 有很多基于这种思想的实现，hyperopt 只是其中一种There are several Bayesian optimization libraries in Python which differ in the algorithm for the surrogate of the objective function. In this article, we will work with Hyperopt, which uses the Tree Parzen Estimator (TPE) Other Python libraries include Spearmint (Gaussian Process surrogate) and SMAC (Random Forest Regression). There are four parts to a Bayesian Optimization problem: Objective Function: what we want to minimize, in this case the validation error of a machine learning model with respect to the hyperparameters （原来model 中的 objective function） Domain Space: hyperparameter values to search over （调参空间） Optimization algorithm: method for constructing the surrogate model and choosing the next hyperparameter values to evaluate （loss 和调参空间的 新的关系） Result history: stored outcomes from evaluations of the objective function consisting of the hyperparameters and validation loss （result 没有什么好说的） 其中的 Bayesian Hyperparameter Optimization using Hyperopt是可以好好学习的。 data scientists 这种东西更加贴近于 data scientist 真的。参考一：https://github.com/WillKoehrsen/hyperparameter-optimization/blob/master/Bayesian%20Hyperparameter%20Optimization%20of%20Gradient%20Boosting%20Machine.ipynb 参考二：https://github.com/WillKoehrsen/hyperparameter-optimization/blob/master/Introduction%20to%20Bayesian%20Optimization%20with%20Hyperopt.ipynb]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Introduction to Natural Language Processing for Text]]></title>
    <url>%2F2019%2F05%2F21%2FIntroduction-to-Natural-Language-Processing-for-Text%2F</url>
    <content type="text"><![CDATA[Natural Language Processing is used to apply machine learning algorithms to text and speech. For example, we can use it to create systems like speech recognition, document summarization, machine translation, spam detection, named entity recognition, question answering, autocomplete, predictive typing and so on. NLTK (Natural Language Toolkit) is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to many corpora and lexical resources. Also, it contains a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning. Best of all, NLTK is a free, open source, community-driven project. In this article, we’ll cover the following topics.这些功能都是可以使用nltk 进行实现的。text Lemmatization 比如，单词“cars”词形还原后的单词为“car”，单词“ate”词形还原后的单词为“eat”。 Sentence Tokenization段落成句。Sentence tokenization (also called sentence segmentation) is the problem of dividing a string of written language into its component sentences. The idea here looks very simple. In English and some other languages, we can split apart the sentences whenever we see a punctuation mark.（标点符号） Word Tokenization句子成词，颗粒度变得更小。Word tokenization (also called word segmentation) is the problem of dividing a string of written language into its component words. In English and many other languages using some form of Latin alphabet, space is a good approximation of a word divider. Text Lemmatization and Stemming这种操作如果被认为是一种 normalization，那么一个优点就是加快了运行的速度。从不同的形式到统一的形式，这可以认为减少了变量。感觉这个更加涉及语法，语法树之类的东西。For grammatical reasons, documents can contain different forms of a word such as drive, drives, driving. Also, sometimes we have related words with a similar meaning, such as nation, national, nationality. Stemming and lemmatization are special cases of normalization. However, they are different from each other. Stemming usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes.Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma. Stop Words因为 stop words往往是带了 noise rather than useful information，所以这个是要去掉的。Stop words are words which are filtered out before or after processing of text. When applying machine learning to text, these words can add a lot of noise. That’s why we want to remove these irrelevant words. stop words dictionary 可以理解成一种过滤词表，是可以根据应用的不同，然后 change的。Stop words usually refer to the most common words such as “and”, “the”, “a” in a language, but there is no single universal list of stopwords. The list of the stop words can change depending on your application. 在存储 stopword 的时候使用 set rather than list 主要原因是 much faster than search operations in a set.You might wonder why we convert our list into a set. Set is an abstract data type that can store unique values, without any particular order. The search operation in a set is much faster than the search operation in a list. For a small number of words, there is no big difference, but if you have a large number of words it’s highly recommended to use the set type. RegexA kind of search pattern. A regular expression, regex, or regexp is a sequence of characters that define a search pattern. Let’s see some basics. 12345678910. - match any character except newline\w - match word\d - match digit\s - match whitespace\W - match not word\D - match not digit\S - match not whitespace[abc] - match any of a, b, or c[^abc] - not match a, b, or c[a-g] - match a character between a &amp; g 这个解释说明了为什么在正则表达式 中使用 r”” 作为一种前缀。因为正则表达是中 ”\“ 的使用和 python 中的”\” 使用有冲突。简而言之，如果加上了 r”” 那么这个就是一种完全的 正则表达式的语法了。 Regular expressions use the backslash character (‘\’) to indicate special forms or to allow special characters to be used without invoking their special meaning. This collides with Python’s usage of the same character for the same purpose in string literals; for example, to match a literal backslash, one might have to write ‘\\‘ as the pattern string, because the regular expression must be \, and each backslash must be expressed as \ inside a regular Python string literal.The solution is to use Python’s raw string notation for regular expression patterns; backslashes are not handled in any special way in a string literal prefixed with ‘r’. So r”\n” is a two-character string containing ‘\’ and ‘n’, while “\n” is a one-character string containing a newline. Usually, patterns will be expressed in Python code using this raw string notation. An example, 1234import resentence = "The development of snowboarding was inspired by skateboarding, sledding, surfing and skiing."pattern = r"[^\w]"print(re.sub(pattern, " ", sentence)) Bag of words Machine learning algorithms cannot work with raw text directly, we need to convert the text into vectors of numbers. This is called feature extraction.The bag-of-words model is a popular and simple feature extraction technique used when we work with text. It describes the occurrence of each word within a document. 这个是 bag of words的”特点“： order or structure of words 没有体现出来。Any information about the order or structure of words is discarded. That’s why it’s called a bag of words. This model is trying to understand whether a known word occurs in a document, but don’t know where is that word in the document. The intuition is that similar documents have similar contents. Also, from a content, we can learn something about the meaning of the document. To use this model, we need to: Design a vocabulary of known words (also called tokens) Choose a measure of the presence of known words 1) 最简单的方式是 “occurrence” ，如果出现了 标为1 否则标为0；这种是最为简单的 bag of words 最的方式，这四个是一一对应的。注意体会。 The complexity of the bag-of-words model comes in deciding how to design the vocabulary of known words (tokens) and how to score the presence of known words. bag of words 中使用 “occurrence” 的方式的缺点：稀疏矩阵（当dict 很大的时候，文章的 representation中有相当成分的0）。 In some cases, we can have a huge amount of data and in this cases, the length of the vector that represents a document might be thousands or millions of elements. Furthermore, each document may contain only a few of the known words in the vocabulary.Therefore the vector representations will have a lot of zeros. These vectors which have a lot of zeros are called sparse vectors. They require more memory and computational resources.We can decrease the number of the known words when using a bag-of-words model to decrease the required memory and computational resources. We can use the text cleaning techniques we’ve already seen in this article before we create our bag-of-words model: 减少 dictionary size 的方式。 Ignoring punctuationRemoving the stop words from our documentsReducing the words to their base form (Text Lemmatization and Stemming)Fixing misspelled words n-gram 的思想是很广泛：通过 sequence of words，这个是可以增加文本的表达力的。An n-gram is a sequence of a number of items (words, letter, numbers, digits, etc.). In the context of text corpora, n-grams typically refer to a sequence of words. A unigram is one word, a bigram is a sequence of two words, a trigram is a sequence of three words etc. 关于如何去 score the presence of word： 这里是有三种方式的。We saw one very simple approach - the binary approach (1 for presence, 0 for absence).Some additional scoring methods are:2) Counts. Count the number of times each word appears in a document.3) Frequencies. Calculate the frequency that each word appears in document out of all the words in the document. TF-IDF 这个语境 是相对于 frequency 而言的，关键词是不一定有 频率所决定，而一些 rarer or domain-specific words 可能是更加常见的。One problem with scoring word frequency is that the most frequent words in the document start to have the highest scores. These frequent words may not contain as much “informational gain” to the model compared with some rarer and domain-specific words. One approach to fix that problem is to penalize words that are frequent across all the documents. This approach is called TF-IDF. TF-IDF 的关键在于体现了“语料库”。TF-IDF, short for term frequency-inverse document frequency is a statistical measure used to evaluate the importance of a word to a document in a collection or corpus. 参考资料https://towardsdatascience.com/introduction-to-natural-language-processing-for-text-df845750fb63]]></content>
      <categories>
        <category>NLP</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[剑指offer-其他]]></title>
    <url>%2F2019%2F05%2F17%2F%E5%89%91%E6%8C%87offer-%E5%85%B6%E4%BB%96%2F</url>
    <content type="text"><![CDATA[这是剑指offer 系列四部曲中的最后一部，因为有些算法题目类别数量太少就汇总到了”其他“, 比如位运算、正则匹配等。第一部关于字符串和数组，第二部是栈、队列、链表和树， 第三部递归、回溯和动态规划。 二进制中1的个数 输入一个整数，输出该数二进制表示中1的个数。其中负数用补码表示。 Tips：首先处理正数, bitwise and operation， 很简单。对于负数，需要转换成 正数然后进行处理，math。 123456789101112131415161718192021class Solution: def NumberOf1(self, n): # write code here if n == 0: return 0 if n &gt; 0: counts = self.number_of_positive(n) else: n = abs(n) - 1 counts = 32 - self.number_of_positive(n) return counts def number_of_positive(self, n): if n == 0: return 0 counts = 0 while n: counts += (n &amp; 1) n = n &gt;&gt; 1 return counts 数值的整数次方 给定一个double类型的浮点数base和int类型的整数exponent。求base的exponent次方。 Tips: 次方使用乘法来进行累乘 1234567891011121314151617181920212223242526272829303132333435class Solution: """ 这个就是边界条件比较多而已，需要分别判断 base 和 exponent 的正负 """ def Power(self, base, exponent): # write code here if base == 0 and exponent != 0: return 0 if base != 0 and exponent == 0: return 1 flag = 1 if base &lt;= 0 and (exponent % 2 == 1): flag = -1 base = abs(base) result = 1 if exponent &gt; 0: reverse = 0 else: reverse = 1 exponent = abs(exponent) if exponent % 2 == 0: result = base * base for i in range(exponent // 2 - 1): result = result * result else: result = base * base for i in range(exponent // 2 - 1): result = result * result result = result * base if reverse: result = 1.0 / result return result * flag 最小的K个数 输入n个整数，找出其中最小的K个数。例如输入4,5,1,6,2,7,3,8这8个数字，则最小的4个数字是1,2,3,4,。 Tips： 这个有点投机取巧了，使用了 “heapq” 的库函数。这个题目跟 第 K个 smallest 是有差别的，快排中的 partition 是找到了 一个数字在最后排序结果中的位置。对于有”累加“前 K个数字还是要使用常规的排序。比较好的就是堆排序。 123456789class Solution: # 想说的是既然是使用这种开源的库函数 那么就记住这种函数名字 def GetLeastNumbers_Solution(self, tinput, k): # write code here if len(tinput) &lt; k: return [] import heapq res = heapq.nsmallest(k, tinput) return res The function partition puts the numbers smaller than nums[left] to its left and then returns the new index of nums[left]. The returned index is actually telling us how small nums[left] ranks in nums. 123456789101112131415161718192021222324252627282930313233343536class Solution(object): def findKthLargest(self, nums, k): """ :type nums: List[int] :type k: int :rtype: int """ left, right = 0, len(nums) - 1 while True: pos = self.partition(nums, left, right) # 这个在排序的时候，是把大的数字放到前面，而前面是pos 是从0 开始的， # 所以这里是 k-1 if pos == k - 1: return nums[pos] # 左边的并不足以构成k 个， 那么在右边 elif pos &lt; k - 1: left = pos + 1 else: right = pos - 1 def partition(self, nums, left, right): # choose nums[left] as pivot pivot = nums[left] # p1, p2就类似 working 中的left right p1, p2 = left + 1, right while p1 &lt;= p2: if nums[p1] &lt; pivot and nums[p2] &gt; pivot: nums[p1], nums[p2] = nums[p2], nums[p1] p1, p2 = p1 + 1, p2 - 1 elif nums[p1] &gt;= pivot: p1 += 1 else: #nums[p2] &lt;= pivot: p2 -=1 nums[left], nums[p2] = nums[p2], nums[left] return p2 整数中1出现的次数（从1到n整数中1出现的次数） 求出1~13的整数中1出现的次数,并算出100~1300的整数中1出现的次数？为此他特别数了一下1~13中包含1的数字有1、10、11、12、13因此共出现6次,但是对于后面问题他就没辙了。ACMer希望你们帮帮他,并把问题更加普遍化,可以很快的求出任意非负整数区间中1出现的次数（从1 到 n 中1出现的次数）。 Tips：math, 计数原理，按位统计该位为1时可能包含的数字总数.由低位向高位依次遍历数字n的每一位curn。记当前位数为c，curn左边（高位）的数字片段为highn，cur右边（低位）的数字片段为lown，lowc = 10 ^ c 若curn = 0，则高位范围为0 ~ highn - 1，低位0 ~ lowc - 1 若curn = 1，则高位范围为0 ~ highn - 1，低位0 ~ lowc - 1；或者 高位为highn， 低位0 ~ lown 若curn ＞ 1，则高位范围为0 ~ highn， 低位为0 ~ lowc - 1 123456789101112131415161718192021222324252627282930313233343536class Solution: # 数字的基本结构分成 weights +working_num + n%base 这三个部分 # 然后一个while 循环是处理一个数字 def NumberOf1Between1AndN_Solution(self, n): # write code here if n &lt; 1: return 0 num = n counts = 0 base = 1 while num: weights = num % 10 num = num // 10 counts += base * num if weights == 1: counts += (n % base) + 1 elif weights &gt; 1: counts += base base *= 10 return counts ``` - 把数组排成最小的数&gt; 输入一个正整数数组，把数组里所有数字拼接起来排成一个数，打印能拼接出的所有数字中最小的一个。例如输入数组&#123;3，32，321&#125;，则打印出这三个数字能排成的最小数字为321323。Tips：使用sorted() 函数， string 类型的排序 和 int 类型的排序是一样的，在python 里面来说。```pythonclass Solution: def PrintMinNumber(self, numbers): # write code here sorted_list = sorted(numbers, cmp=lambda a, b: cmp(str(a) + str(b), str(b) + str(a))) # 这个时候已经排好序，然后只要一个个连接起来就行了 return ''.join(map(str, sorted_list)) 丑数 把只包含质因子2、3和5的数称作丑数（Ugly Number）。例如6、8都是丑数，但14不是，因为它包含质因子7。 习惯上我们把1当做是第一个丑数。求按从小到大的顺序的第N个丑数。 Tips：之后的丑数肯定是2，3或5 的倍数，分别单独计数，然后选择最小的。 123456789101112131415161718192021class Solution: # 在进行 append 操作的时候去重， def GetUglyNumber_Solution(self, index): # write code here if index &lt;1: return 0 list1 = [1] # 意味着只能是 append() 操作了 i, j, k = 0, 0, 0 while len(list1) &lt; index: num = min(list1[i] * 2, list1[j] * 3, list1[k] * 5) if num &gt; list1[-1]: list1.append(num) if num == list1[i] * 2: i += 1 elif num == list1[j] * 3: j += 1 else: k += 1 return list1[-1] 正则表达式匹配 请实现一个函数用来匹配包括’.’和’‘的正则表达式。模式中的字符’.’表示任意一个字符，而’‘表示它前面的字符可以出现任意次（包含0次）。 在本题中，匹配是指字符串的所有字符匹配整个模式。例如，字符串”aaa”与模式”a.a”和”abaca”匹配，但是与”aa.a”和”ab*a”均不匹配 Tips: dp 问题。转换方程 dp[i][j] i 表示 string 的index j表示 pattern 的index， dp[i][j] ==dp[i-1][j-1] or dp[i][j] =dp[i][j-2] or dp[i][j-1] 。 123456789101112131415161718192021222324class Solution: # s, pattern都是字符串 # https://www.youtube.com/watch?v=l3hda49XcDE 心中一定要有这个表格, a[i][j] 这个更像是一种指针 def match(self, s, pattern): if len(s) == 0 and len(pattern) == 0: return True dp = [[False for _ in range(len(pattern) + 1)] for _ in range(len(s) + 1)] dp[0][0] = True for j in range(1, len(pattern) + 1): if pattern[j - 1] == "*": dp[0][j] = dp[0][j - 2] for i in range(1, len(s) + 1): for j in range(1, len(pattern) + 1): if pattern[j - 1] == s[i - 1] or pattern[j - 1] == ".": dp[i][j] = dp[i - 1][j - 1] elif pattern[j - 1] == "*": dp[i][j] = dp[i][j - 2] if s[i - 1] == pattern[j - 2] or pattern[j - 2] == ".": dp[i][j] = dp[i][j] or dp[i - 1][j] else: dp[i][j] = False return dp[len(s)][len(pattern)] 数据流中的中位数 如何得到一个数据流中的中位数？如果从数据流中读出奇数个数值，那么中位数就是所有数值排序之后位于中间的数值。如果从数据流中读出偶数个数值，那么中位数就是所有数值排序之后中间两个数的平均值。我们使用Insert()方法读取数据流，使用GetMedian()方法获取当前读取数据的中位数。 Tips：主要是体现了数据流，要求能够 insert 元素，然后基于当前的状态去 getmedian() ，是动态的，而不是静态的。 1234567891011121314151617181920class Solution: """ 对于数据流 这个应该是第二次接触了，需要使用一个全局变量 """ # 虽然知道这个使用 堆的思想是更优的，搜索时间可以O（1）， 堆的调整是 O(log n) # 但是没有什么很好的教程，所以我也没有学会啊 def __init__(self): self.list1 = [] def Insert(self, num): self.list1.append(num) def GetMedian(self, ch): length = len(self.list1) # 我记得有一个更加快一些 self.list1 = sorted(self.list1) if length % 2 == 0: return (self.list1[length // 2] + self.list1[length // 2 - 1]) / 2.0 else: return self.list1[length // 2] 滑动窗口的最大值 给定一个数组和滑动窗口的大小，找出所有滑动窗口里数值的最大值。例如，如果输入数组{2,3,4,2,6,2,5,1}及滑动窗口的大小3，那么一共存在6个滑动窗口，他们的最大值分别为{4,4,6,6,6,5}； 针对数组{2,3,4,2,6,2,5,1}的滑动窗口有以下6个： {[2,3,4],2,6,2,5,1}， {2,[3,4,2],6,2,5,1}， {2,3,[4,2,6],2,5,1}， {2,3,4,[2,6,2],5,1}， {2,3,4,2,[6,2,5],1}， {2,3,4,2,6,[2,5,1]}。 Tips：使用max(list1) 这样的操作是可行的。 123456789class Solution: # 最简单的模拟滑动窗口 的过程 def maxInWindows(self, num, size): slip = [] if not num or len(num) &lt; size or size == 0: return [] for i in range(len(num) - size + 1): slip.append(max(num[i:i + size])) return slip]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>剑指offer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[剑指offer-递归、回溯和动态规划]]></title>
    <url>%2F2019%2F05%2F17%2F%E5%89%91%E6%8C%87offer-%E9%80%92%E5%BD%92-%E5%9B%9E%E6%BA%AF%E5%92%8C%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%2F</url>
    <content type="text"><![CDATA[这是剑指offer 系列四部曲中的第三部：递归、回溯和动态规划。第一部关于字符串和数组，第二部是栈、队列、链表和树， 最后一部分在这里。 斐波那契数列 大家都知道斐波那契数列，现在要求输入一个整数n，请你输出斐波那契数列的第n项（从0开始，第0项为0）。n&lt;=39 Tips: 简单的递归，可以转换成循环。 1234567891011121314class Solution: # python 中list 的初始化，最开始的是从0 开始，所以是需要多进行一个初始化的 def Fibonacci(self, n): # write code here if n == 0: return 0 if n == 1: return 1 arr = [0] * (n + 1) arr[0] = 0 arr[1] = 1 for i in range(2, n + 1): arr[i] = arr[i - 1] + arr[i - 2] return arr[n] 跳台阶 一只青蛙一次可以跳上1级台阶，也可以跳上2级。求该青蛙跳上一个n级的台阶总共有多少种跳法（先后次序不同算不同的结果）。 Tips： 同上。 123456789101112131415class Solution: def jumpFloor(self, number): # write code here if number == 1: return 1 if number == 2: return 2 arr = [0] * (number + 1) arr[1] = 1 arr[2] = 2 for i in range(3, number + 1): arr[i] = arr[i - 1] + arr[i - 2] return arr[number] 跳台阶2 一只青蛙一次可以跳上1级台阶，也可以跳上2级……它也可以跳上n级。求该青蛙跳上一个n级的台阶总共有多少种跳法。 Tips：同上。 12345678910111213class Solution: """ 在使用 for循环的时候，注意 range() 这种取值，究竟是使用 range() 作为次数的计量； 还是要使用range 中的index 。两者是不相同的操作，尤其是对于前后的取值。 """ def jumpFloorII(self, number): # write code here if number == 1: return 1 nums = 1 for i in range(number - 1): nums = nums * 2 return nums 矩形覆盖 我们可以用2*1的小矩形横着或者竖着去覆盖更大的矩形。请问用n个2*1的小矩形无重叠地覆盖一个2*n的大矩形，总共有多少种方法？ Tips: math, 找出递归方程。 1234567891011121314151617"""既然结果只是最后一个解，那么就没有必要保存中间变量，所以只是，所以空间复杂度从O（n） -&gt; O(1) ，这个是超级nice的"""class Solution: def rectCover(self, number): # write code here if number &lt;= 0: return 0 if number &lt;= 2: return number # 只是两个变量罢了 a, b = 1, 2 while number &gt; 2: a, b = b, a + b number -= 1 return b 机器人的运动范围 地上有一个m行和n列的方格。一个机器人从坐标0,0的格子开始移动，每一次只能向左，右，上，下四个方向移动一格，但是不能进入行坐标和列坐标的数位之和大于k的格子。 例如，当k为18时，机器人能够进入方格（35,37），因为3+5+3+7 = 18。但是，它不能进入方格（35,38），因为3+5+3+8 = 19。请问该机器人能够达到多少个格子？ Tips：递归，转移方程不难，在上下左右四个方向进行尝试，需要判断的条件比较多，比如是否访问过，数位之和等一些条件。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465class Solution: def movingCount(self, threshold, rows, cols): # visited 不一定是二维的，只要是能够"自圆其说" 就行。 visited = [False] * (rows * cols) count = self.movingCountCore(threshold, rows, cols, 0, 0, visited) return count def movingCountCore(self, threshold, rows, cols, row, col, visited): count = 0 # 就是这个访问记录是需要进行变化的， 如果是false ，然后访问之后 是需要设置为 true的 if self.check(threshold, rows, cols, row, col, visited): visited[row * cols + col] = True count = 1 + self.movingCountCore(threshold, rows, cols, row, col - 1, visited) + \ self.movingCountCore(threshold, rows, cols, row, col + 1, visited) + \ self.movingCountCore(threshold, rows, cols, row + 1, col, visited) + \ self.movingCountCore(threshold, rows, cols, row - 1, col, visited) return count def check(self, threshold, rows, cols, row, col, visited): if row &gt;= 0 and row &lt; rows and col &gt;= 0 and col &lt; cols and self.judge(threshold, row, col) and not visited[row * cols + col]: return True else: return False def judge(self, threshold, i, j): if sum(map(int, str(i) + str(j))) &lt;= threshold: return True else: return False``` - 矩阵中的路径&gt; 请设计一个函数，用来判断在一个矩阵中是否存在一条包含某字符串所有字符的路径。路径可以从矩阵中的任意一个格子开始，每一步可以在矩阵中向左，向右，向上，向下移动一个格子。如果一条路径经过了矩阵中的某一个格子，则之后不能再次进入这个格子。 例如 a b c e s f c s a d e e 这样的3 X 4 矩阵中包含一条字符串"bcced"的路径，但是矩阵中不包含"abcb"路径，因为字符串的第一个字符b占据了矩阵中的第一行第二个格子之后，路径不能再次进入该格子。Tips： 在二维数组中每个点上都进行尝试，每个点上同样是上下左右进行尝试，返回符合条件的。```pythonclass Solution: # 递归 这个是 true or false 判断类型的。 # 思路：先是 rows* cols 这样的全部遍历 def hasPath(self, matrix, rows, cols, path): # 如果使用 [ for _in range(rows) ] for _ in range(cols) ， 这个是有结构的 rows* cols assist = [True] * rows * cols for i in range(rows): for j in range(cols): if self.rightPath(matrix, rows, cols, i, j, path, assist): return True return False def rightPath(self, matrix, rows, cols, i, j, path, assist): if not path: return True index = i * cols + j if i &lt; 0 or i &gt;= rows or j &lt; 0 or j &gt;= cols or matrix[index] != path[0] or assist[index] == False: return False assist[index] = False if (self.rightPath(matrix, rows, cols, i + 1, j, path[1:], assist) or self.rightPath(matrix, rows, cols, i - 1, j, path[1:], assist) or self.rightPath(matrix, rows, cols, i, j - 1, path[1:], assist) or self.rightPath(matrix, rows, cols, i, j + 1, path[1:], assist)): return True assist[index] = True return False]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>剑指offer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[剑指Offer-栈、队列、链表和树]]></title>
    <url>%2F2019%2F05%2F17%2F%E5%89%91%E6%8C%87Offer-%E6%A0%88-%E9%98%9F%E5%88%97-%E9%93%BE%E8%A1%A8%E5%92%8C%E6%A0%91%2F</url>
    <content type="text"><![CDATA[这是剑指offer 系列四部曲中的第二部：栈、队列、链表和树。第一部关于字符串和数组，第三部是递归、回溯和动态规划， 最后一部分在这里。 从尾到头打印链表 输入一个链表，按链表值从尾到头的顺序返回一个ArrayList。 Tips: 从尾到头，考察是栈的数据结构，在python 中使用list 来实现栈。 123456789101112131415161718# -*- coding:utf-8 -*-# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: # 返回从尾部到头部的列表值序列，例如[1,2,3] # += , -= 这个都是同一种类型的 def printListFromTailToHead(self, listNode): # write code here arraylist =[] head = listNode while head != None: arraylist += [head.val] # 这个在这里等效于 arraylist.append(head.val) head = head.next return arraylist[::-1] % 重建二叉树 输入某二叉树的前序遍历和中序遍历的结果，请重建出该二叉树。假设输入的前序遍历和中序遍历的结果中都不含重复的数字。例如输入前序遍历序列{1,2,4,7,3,5,6,8}和中序遍历序列{4,7,2,1,5,3,8,6}，则重建二叉树并返回。 Tips: 递归，二叉树的题目大多数都是可以使用递归的思想进行解决，因为二叉树本身结构就是递归定义的。递归优点在于代码量比较少。从先序遍历中找出根节点，从中序遍历中找出左右子树。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152# -*- coding:utf-8 -*-# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: # 返回构造的TreeNode根节点 # 需要理解在前序遍历中是先遍历左子树的，并且中序和前序中左子树的个数是不会变的 def reConstructBinaryTree(self, pre, tin): # write code here if len(pre) == 0: return None root = TreeNode(pre[0]) # 这个index 函数是需要记住的 index = tin.index(pre[0]) # 这里也是需要修改的 # pre 和 tin都是需要空出一个 root.value 的位置，只不过选择空的位置是不一样的 root.left = self.reConstructBinaryTree(pre[1:index + 1], tin[:index]) root.right = self.reConstructBinaryTree(pre[index + 1:], tin[index + 1:]) return root``` %- 用两个栈实现队列&gt; 用两个栈来实现一个队列，完成队列的Push和Pop操作。 队列中的元素为int类型。Tips： 在python 中栈等同于使用list 实现。使用两个栈，意味着一个是push_stack 一个是pop_stack，使用两个栈的“后进先出”表示队列的先进先出（push and pop）从语法上讲 ，if list1 ==[], 那么 list1 ==None, 这两个条件是可以交换判断的。（在list 中）```pythonclass Solution: def __init__(self): self.list1 =[] self.list2 =[] def push(self, node): # write code here self.list1.append(node) def pop(self): # return if not self.list1 and not self.list2 : return None if self.list2 : return self.list2.pop() else: while self.list1: self.list2.append(self.list1.pop()) return self.list2.pop() 链表中倒数第k个结点 输入一个链表，输出该链表中倒数第k个结点。 Tips： 两种解法。一种是遍历存储到list 中，空间复杂度是O(N), 另外一种是两个指针p1，p2，距离相差k，当p2 到达链表尾部，p1 就在导数第k 个位置。 1234567891011121314151617181920212223242526272829303132# -*- coding:utf-8 -*-# class ListNode:# def __init__(self, x):# self.val = x# self.next = None"""尝试使用两个指针版本p1 p2 并且这种 length 在命名上是需要规范的, 并且这种指针操作，最好是拷贝出来进行操作不管怎么说，还是应该求解出来 length of listNode，这种才是正途可以使用两个指针，"""class Solution: def FindKthToTail(self, head, k): # write code here if head == None or k &lt;= 0: return None p1 = head p2 = head len1 = 0 while p1: len1 += 1 p1 = p1.next if k &gt; len1: return None p1 = head while k: p1 = p1.next k -= 1 while p1: p1 = p1.next p2 = p2.next return p2 反转链表 输入一个链表，反转链表后，输出新链表的表头。 Tips： 需要三个指针，cur，next_node, pre。 1234567891011121314151617181920212223242526# -*- coding:utf-8 -*-# class ListNode:# def __init__(self, x):# self.val = x# self.next = None"""修改链表是需要三个指针的 pre, cur, next_node 如果对三个指针名进行命名好了，那么这个就是成功的一般了， 这个不容易想到的是设置pre =None ，这个是一个细节经验性的问题"""class Solution: # 返回ListNode def ReverseList(self, pHead): # write code here if pHead ==None: return None pre =None cur =pHead while cur: next_node =cur.next cur.next =pre pre, cur =cur, next_node return pre 合并两个排序的链表 Tips： 归并排序中的“并” 操作，只不过由原来的list 操作到现在的 linkedlist 操作。 输入两个单调递增的链表，输出两个链表合成后的链表，当然我们需要合成后的链表满足单调不减规则。 1234567891011121314151617181920212223242526272829303132# def __init__(self, x):# self.val = x# self.next = None"""就是在使用两个或者多个 index (p1 or p2) 遍历的时候，一个常见的错误就是忘记了不断更新index"""class Solution: # 返回合并后列表 def Merge(self, pHead1, pHead2): # write code here if pHead1 == None: return pHead2 if pHead2 == None: return pHead1 head = ListNode(-1) head1 = head p1 = pHead1 p2 = pHead2 while p1 and p2: if p1.val &lt; p2.val: head.next = p1 p1 = p1.next else: head.next = p2 p2 = p2.next head = head.next if p1 == None: head.next = p2 if p2 == None: head.next = p1 return head1.next 树的子结构 输入两棵二叉树A，B，判断B是不是A的子结构。（ps：我们约定空树不是任意一个树的子结构） Tips：根节点相同，左右子树相同。（数值和结构）。实现的使用有两个递归程序，意味着有两个跳出的条件。一个是从A树中找结点和B 树的根节点，一个是根节点相同之后，判断左右子树是否相同。 12345678910111213141516171819202122232425262728293031323334# -*- coding:utf-8 -*-# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = None"""分成两部：首先寻找两个根节点的值是否相同；然后判断子树是否完全相同subTree 这个函数就是判断子树是否完全相同的，所以函数的功能一定要搞好"""class Solution: def HasSubtree(self, pRoot1, pRoot2): if not pRoot1: return False if not pRoot2: return False result =False if pRoot1.val ==pRoot2.val: result =self.subTree(pRoot1, pRoot2) if result ==False: result = self.HasSubtree(pRoot1.left, pRoot2) or self.HasSubtree(pRoot1.right, pRoot2) return result def subTree(self, root1, root2): if not root2: return True if not root1: return False if root1.val ==root2.val: return self.subTree(root1.left, root2.left) and self.subTree(root1.right, root2.right) return False 二叉树的镜像 操作给定的二叉树，将其变换为源二叉树的镜像。 Tips：求解二叉树镜像，A 的左右子树对应着B 的右左子树。 123456789101112131415161718192021# -*- coding:utf-8 -*-# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = None"""就是在某个左（右）子树是None 的情况下，这个也是可以进行交换的，结束的标志应该是根节点是否为空"""class Solution: # 返回镜像树的根节点 def Mirror(self, root): # write code here if not root: return None root.left , root.right =root.right, root.left if root.left: self.Mirror(root.left) if root.right: self.Mirror(root.right) return root 包含min函数的栈 定义栈的数据结构，请在该类型中实现一个能够得到栈中所含最小元素的min函数（时间复杂度应为O（1））。 Tips: 这个跟“使用两个栈表示队列” 是差不多的，就是单独使用一个list 存储min 函数调用的一个列表，这样的话能达到时间复杂度是 O(1). 1234567891011121314151617181920212223242526272829303132# -*- coding:utf-8 -*-"""这个栈中最小的元素是变化的，好好理解一下，如果弹出了一个比较大的元素，那么栈中最小的元素是不变的所含元素的最小元素top() and min() 操作是不需要删除元素的， pop 是删除了元素"""class Solution: def __init__(self): self.all_list = [] self.min_list = [] def push(self, node): # write code here if not self.min_list: self.min_list.append(node) else: self.min_list.append(min(node, self.min())) self.all_list.append(node) def pop(self): self.all_list.pop() self.min_list.pop() # write code here def top(self): return self.all_list[-1] # write code here def min(self): return self.min_list[-1] 栈的压入、弹出序列 输入两个整数序列，第一个序列表示栈的压入顺序，请判断第二个序列是否可能为该栈的弹出顺序。假设压入栈的所有数字均不相等。例如序列1,2,3,4,5是某栈的压入顺序，序列4,5,3,2,1是该压栈序列对应的一个弹出序列，但4,3,5,1,2就不可能是该压栈序列的弹出序列。（注意：这两个序列的长度是相等的） Tips: 使用一个list 来模拟压入和弹出过程，遍历弹出序列popV，如果结束，那么return True。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849class Solution:def IsPopOrder(self, pushV, popV): if not pushV: return False tmp =[] while popV: if tmp and popV[0] == tmp[-1]: popV.pop(0) tmp.pop() elif pushV: tmp.append(pushV.pop(0)) else: return False return True ``` - 从上往下打印二叉树&gt; 从上往下打印出二叉树的每个节点，同层节点从左至右打印。Tips： 层次遍历，遍历根节点之后加入左右结点。```python# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: # 返回从上到下每个节点值列表，例：[1,2,3] # 层序遍历二叉树， 这个跟数据结构 队列有类似的 # nodes 装上结点，然后vlaues 装上数值 def PrintFromTopToBottom(self, root): # write code here if not root: return [] nodes =[] values = [] nodes.append(root) while nodes: node = nodes.pop(0) values.append(node.val) if node.left: nodes.append(node.left) if node.right: nodes.append(node.right) return values 二叉搜索树的后序遍历序列 输入一个整数数组，判断该数组是不是某二叉搜索树的后序遍历的结果。如果是则输出Yes,否则输出No。假设输入的数组的任意两个数字都互不相同。 Tips：二叉搜索树，按照中序遍历的话，就是一个排序的二叉树，根节点大于左子树，右子树大于根节点。后序遍历序列中最后一个是根节点，小于根节点是左子树，大于根节点的是右子树，这样进行判断。 123456789101112131415class Solution: # 后序遍历结果， 最后一个是根节点，这个是递归的思想 # 二叉搜索树， 左子树小于根节点，右子树大于根节点 def VerifySquenceOfBST(self, sequence): # write code here if not sequence: return False root = sequence[-1] for i in range(len(sequence)): if sequence[i] &gt; root: break for j in range(i, len(sequence)): if sequence[j] &lt; root: return False return True 二叉树中和为某一值的路径 输入一颗二叉树的跟节点和一个整数，打印出二叉树中结点值的和为输入整数的所有路径。路径定义为从树的根结点开始往下一直到叶结点所经过的结点形成一条路径。(注意: 在返回值的list中，数组长度大的数组靠前) Tips： 树的遍历，深度优先算法（dfs） 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: # 返回二维列表，内部每个列表表示找到的路径 # 深度优先 dfs() 这样的一个算法 def FindPath(self, root, expectNumber): # write code here if not root: return [] self.target = expectNumber paths = [] self.dfs(root, [root.val], paths) return pathsdef dfs(self, root, path, paths): if not root.left and not root.right and sum(path) == self.target: paths.append(path) if root.left: self.dfs(root.left, path + [root.left.val], paths) if root.right: self.dfs(root.right, path + [root.right.val], paths) ``` - 复杂链表的复制&gt; 输入一个复杂链表（每个节点中有节点值，以及两个指针，一个指向下一个节点，另一个特殊指针指向任意一个节点），返回结果为复制后复杂链表的head。（注意，输出结果中请不要返回参数中的节点引用，否则判题程序会直接返回空）Tips: 先是在原来的链表上进行了相同结点的copy和next 指针的指向，然后是random 指针的指向，最后是将原始链表和copy 的链表进行分离。```python # -*- coding:utf-8 -*- # class RandomListNode: # def __init__(self, x): # self.label = x # self.next = None # self.random = None class Solution: # 返回 RandomListNode # 首先是结点的复制和 next 指针的连接， 然后是random 指针的连接，最后是选择出复制的结点 def Clone(self, pHead): # write code here if not pHead: return None self.clone_nodes(pHead) self.connect_nodes(pHead) return self.select_nodes(pHead) def clone_nodes(self, head): if not head: return None while head: cloned = RandomListNode(head.label) cloned.next = head.next head.next = cloned head = cloned.next def connect_nodes(self, head): if not head: return None while head: cloned = head.next if head.random: cloned.random = head.random.next head = cloned.next def select_nodes(self, head): if not head: return None cloned =cloned_head =None # 这个if 的作用是为了保存一个 cloned_head的结点， # 一定要从这个功能出发 if head: cloned =cloned_head =head.next head.next =cloned.next head =head.next while head: cloned.next =head.next cloned =cloned.next head.next =cloned.next head =head.next return cloned_head 二叉搜索树与双向链表 输入一棵二叉搜索树，将该二叉搜索树转换成一个排序的双向链表。要求不能创建任何新的结点，只能调整树中结点指针的指向。 Tips：中序遍历二叉搜索树就是一种排序的书的结点，然后树的左右指针可以作为链表中的指向使用。 12345678910111213141516171819202122232425262728293031323334# -*- coding:utf-8 -*-# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: # 使用的树的结构 表示一种双向链表 # 二叉搜索树 ，左子树小于根节点，右子树大于根节点 # 中序遍历得到就是一种排好序的结构 # 只能调整树中结点指针的指向 def Convert(self, pRootOfTree): # write code here if not pRootOfTree: return None tree = pRootOfTree res = [] self.helper(tree, res) for i in range(len(res) - 1): res[i].right = res[i + 1] res[i + 1].left = res[i] # 这个返回值也是比较鬼畜呀， 就是需要这样返回 return res[0]def helper(self, root, res): if not root: return None if root.left: self.helper(root.left, res) res.append(root) if root.right: self.helper(root.right, res) 两个链表的第一个公共结点 输入两个链表，找出它们的第一个公共结点。 Tips：就是一个 m*n 的问题（m，n 分别代表两个链表的长度） 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647# -*- coding:utf-8 -*-# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: # 两个指针指向的是 一个结点，一个内存的两个指向 # 将可能不同长度的两个链表转换成相同长度的两个链表的比较，使用 def FindFirstCommonNode(self, pHead1, pHead2): # write code here if not pHead1 or not pHead2: return None p1 = pHead1 p2 = pHead2 while p1 != p2: # 这个p1 只能指向了最后一个结点，但最后一个节点不一定相同 p1 = pHead2 if not p1 else p1.next p2 = pHead1 if not p2 else p2.next return p1 ``` - 二叉树的深度&gt; 输入一棵二叉树，求该树的深度。从根结点到叶结点依次经过的结点（含根、叶结点）形成树的一条路径，最长路径的长度为树的深度。Tips：递归，相比于二叉树的路径，这个只是返回一个数值就行。```python# -*- coding:utf-8 -*-# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: """ 分别求解 左右子树的深度，然后max(left, right) 这样的操作 """ def TreeDepth(self, pRoot): if not pRoot: return 0 left = self.TreeDepth(pRoot.left) + 1 right = self.TreeDepth(pRoot.right) + 1 # 这个return 是最后执行一次的，然后上面那个都是不断的在进行递归加深 # 这个 left right 已经完成了，最后的效果只是 返回 max(left, right) 这样子 return max(left, right) 平衡二叉树 输入一棵二叉树，判断该二叉树是否是平衡二叉树。 Tips： 左右子树的深度差最大不超过1。两个递归，一个是计算树的深度的递归，一个是判断左右子树是否是平衡二叉树的递归。 123456789101112131415161718192021222324252627# -*- coding:utf-8 -*-# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: # 递归常见的都会有两个return 跳出条件，一个是异常的条件，一个是正确的返回 def get_depth(self, root): if not root: return 0 left =self.get_depth(root.left) right =self.get_depth(root.right) return max(left, right) +1 def IsBalanced_Solution(self, pRoot): if not pRoot: return True left =self.get_depth(pRoot.left) right =self.get_depth(pRoot.right) if abs(left-right) &gt;1: return False return self.IsBalanced_Solution(pRoot.left) and self.IsBalanced_Solution(pRoot.right) 链表中环的入口结点 给一个链表，若其中包含环，请找出该链表的环的入口结点，否则，输出null。 Tips： 两个快慢指针，开指针在环内相遇慢指针。（两个指针一个需要再环外，一个在环内，然后同样的速度走，最后才能相遇）重置快指针到头结点，两个指针相同速度，当再次相遇时候，那就是入口结点。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556 # -*- coding:utf-8 -*- # class ListNode: # def __init__(self, x): # self.val = x # self.next = None class Solution: # 现在长个记性吧，在使用next 这样的时候 要先判断这个是不是存在的 def EntryNodeOfLoop(self, pHead): # write code here if not pHead or not pHead.next or not pHead.next.next: return None twoTimes =pHead.next.next oneTime =pHead.next while twoTimes != oneTime: twoTimes =twoTimes.next.next oneTime =oneTime.next twoTimes =pHead while twoTimes != oneTime: twoTimes =twoTimes.next oneTime =oneTime.next return twoTimes ``` - 删除链表中重复的结点&gt; 在一个排序的链表中，存在重复的结点，请删除该链表中重复的结点，重复的结点不保留，返回链表头指针。 例如，链表1-&gt;2-&gt;3-&gt;3-&gt;4-&gt;4-&gt;5 处理后为 1-&gt;2-&gt;5```python# -*- coding:utf-8 -*-# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: def deleteDuplication(self, pHead): # write code here head = ListNode(-1) head.next = pHead curr = pHead last = head while curr and curr.next: # val =curr.val # 这个条件比较简单，所以可以放到前面 if curr.val != curr.next.val: curr = curr.next last = last.next else: # 这个条件 curr 还是需要注意一下的 val = curr.val # python 中 condition1 and condition2 这种是有先后顺序的 # 可能是存在短路现象的， 如果 curr 不成立，那么后面的是不会执行的 # 草拟 while curr and val == curr.val: curr = curr.next last.next = curr return head.next 二叉树的下一个结点 给定一个二叉树和其中的一个结点，请找出中序遍历顺序的下一个结点并且返回。注意，树中的结点不仅包含左右子结点，同时包含指向父结点的指针。 Tips：中序遍历的下一个结点，如果存在右节点，那么下一个结点是右节点最左边的一个点；如果该结点是其父节点的左结点，那么下一节点是其父节点，否则一直回溯。 123456789101112131415161718192021222324252627282930# -*- coding:utf-8 -*-# class TreeLinkNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = None# self.next = Noneclass Solution: # https://blog.csdn.net/fuxuemingzhu/article/details/79723819 # 这个是求解中序遍历中某个结点的下一个结点 # 这pNode 就是一个普通的结点 def GetNext(self, pNode): # write code here if not pNode: return None # 如果存在右结点 if pNode.right: pNode = pNode.right while pNode.left: pNode = pNode.left return pNode # 如果是父节点的左子树 else: # 这里使用 pNode.next 表示父节点 while pNode.next: if pNode == pNode.next.left: return pNode.next # 这个是右结点 pNode = pNode.next return None 对称的二叉树 请实现一个函数，用来判断一颗二叉树是不是对称的。注意，如果一个二叉树同此二叉树的镜像是同样的，定义其为对称的。 Tips: 判断镜像和递归生成进行还是不太一样的哈。递归判断，根节点相同，然后左右子树是否是对称。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677# -*- coding:utf-8 -*-# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: # 镜像的概念 和递归 # isSame() 这个就是判断两个子树是否镜像的操作 def isSame(self, p, q): if not p and not q: return True # 好好思考 下面这两个跳出条件为什么是不合适的 if p and q and p.val == q.val: return self.isSame(p.left, q.right) and self.isSame(p.right, q.left) def isSymmetrical(self, pRoot): # write code here # 最开始的条件 如果都是 none 那么这个是对称的 if not pRoot: return True if pRoot.left and not pRoot.right: return False if not pRoot.left and pRoot.right: return False return self.isSame(pRoot.left, pRoot.right) ``` - 按之字形顺序打印二叉树&gt; 请实现一个函数按照之字形打印二叉树，即第一行按照从左到右的顺序打印，第二层按照从右至左的顺序打印，第三行按照从左到右的顺序打印，其他行以此类推。Tips：层序遍历的升级版，有两种思路，一种是使用单独 stack (list) 的思想存储偶数层数，一种是先按照原先层序遍历的思想，最后对于偶数的结果进行“翻转” 处理。选择后者，因为代码上比较简单。```python# -*- coding:utf-8 -*-# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: # 层序遍历 + 偶数翻转 # https://blog.csdn.net/fuxuemingzhu/article/details/79724959 def level(self, root, level, res): """ root: the root of tree level: res: result """ if not root: return if len(res) == level: res.append([]) res[level].append(root.val) if root.left: self.level(root.left, level + 1, res) if root.right: self.level(root.right, level + 1, res) def Print(self, pRoot): # write code here if not pRoot: return [] res = [] self.level(pRoot, 0, res) for level in range(1, len(res), 2): res[level] = res[level][::-1] return res 把二叉树打印成多行 从上到下按层打印二叉树，同一层结点从左至右输出。每一层输出一行。 Tips: 和上一个题目类似，在遍历二叉树的时候，关键是加入了 [level] 层数这种信息。 123456789101112131415161718192021222324252627282930# -*- coding:utf-8 -*-# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: # 返回二维列表[[1,2],[4,5]] def level(self, root, level, res): # 你这里也没有说要返回值的意思呀，这个直接是 return if not root: return if level == len(res): res.append([]) res[level].append(root.val) if root.left: self.level(root.left, level + 1, res) if root.right: # res[level] =self.level(root.right, level+1, res) # 因为这个是 传的值，所以不需要使用返回值的 self.level(root.right, level + 1, res) def Print(self, pRoot): if not pRoot: return [] res = [] self.level(pRoot, 0, res) return res 序列化二叉树 请实现两个函数，分别用来序列化和反序列化二叉树 Tips：序列号和反序列化只是一种约定的存储的形式。 # -*- coding:utf-8 -*- # class TreeNode: # def __init__(self, x): # self.val = x # self.left = None # self.right = None class Solution: """ 序列化就是从树结构 转成字符串的结构；反之，也是成立的。 使用先序遍历的方法。 https://suixinblog.cn/2019/03/target-offer-serialize-binary-tree.html#%E4%BB%A3%E7%A0%81 """ def __init__(self): self.flag = -1 def Serialize(self, root): # write code here if not root: return "#" return str(root.val) + "," + self.Serialize(root.left) + "," + self.Serialize(root.right) def Deserialize(self, s): # write code here self.flag += 1 string = s.split(',') if self.flag &gt; len(string): return None root = None if string[self.flag] != '#': root = TreeNode(int(string[self.flag])) root.left = self.Deserialize(s) root.right = self.Deserialize(s) return root 二叉搜索树的第k个结点 给定一棵二叉搜索树，请找出其中的第k小的结点。例如， （5，3，7，2，4，6，8） 中，按结点数值大小顺序第三小结点的值为4。 Tips: 二叉搜索树，中序遍历之后有序，然后取第 k 个结点。 # class TreeNode: # def __init__(self, x): # self.val = x # self.left = None # self.right = None class Solution: def middle(self, root, result): if not root: return if root.left: self.middle(root.left, result) result.append(root) if root.right: self.middle(root.right, result) def KthNode(self, pRoot, k): # write code here if not pRoot: return result = [] self.middle(pRoot, result) if len(result) &lt; k or k &lt; 1: return return result[k - 1]]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>剑指offer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[剑指Offer-字符串和数组]]></title>
    <url>%2F2019%2F05%2F17%2F%E5%89%91%E6%8C%87Offer-%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%92%8C%E6%95%B0%E7%BB%84%2F</url>
    <content type="text"><![CDATA[题目都是来自牛客网在线刷题中的剑指offer。最近找实习工作，作为刷题记录，顺便从考察知识点的角度分类整理。主要分成以下四大类： 字符串、数组 链表、树 递归、回溯、动态规划 其他, 比如位运算、正则匹配等 不同类别以一章介绍，之后可能会随时update。这是剑指offer 系列四部曲中的第一部。第一部关于字符串和数组，第二部是栈、队列、链表和树，第三部递归、回溯和动态规划， 最后一部分在这里。 二维数组中的查找 在一个二维数组中（每个一维数组的长度相同），每一行都按照从左到右递增的顺序排序，每一列都按照从上到下递增的顺序排序。请完成一个函数，输入这样的一个二维数组和一个整数，判断数组中是否含有该整数。 Tips: 数组是从左上方到右下方这样的递增，指针（两个）有两个运动方向，一个是向左一个是向下。 12345678910111213141516171819class Solution:# array 二维列表sdef Find(self, target, array): # write code here # 可以尝试一下 坐标移动的思想, 所以这个就是一种坐标移动的思想 # 就是一个条件有了之后 那么接下来的else 也可以顺着就写上来的 rows =len(array) -1 cols =len(array[0]) -1 row =0 col =cols while row &lt;= rows and col &gt;=0: if array[row][col] == target: return True elif array[row][col] &gt; target: col -=1 else: row +=1 return False 替换空格 请实现一个函数，将一个字符串中的每个空格替换成“\%20”。例如，当字符串为We Are Happy.则经过替换之后的字符串为We\%20Are\%20Happy。 Tips： 字符串的遍历对于 python 而言是比较简单的。 123456789101112class Solution:# s 源字符串def replaceSpace(self, s): # write code here # python 中的 str 就是 array of char converted ="" for ch in s: if ch ==" ": converted += "%20" else: converted += ch return converted 旋转数组的最小数字 把一个数组最开始的若干个元素搬到数组的末尾，我们称之为数组的旋转。 输入一个非减排序的数组的一个旋转，输出旋转数组的最小元素。 例如数组{3,4,5,1,2}为{1,2,3,4,5}的一个旋转，该数组的最小值为1。 NOTE：给出的所有元素都大于0，若数组大小为0，请返回0。 123456789101112131415161718192021222324class Solution:'''题目是不减，所以下面判断的时候也是不减。解题的关键点： 设置第一个元素为假定的min_value,然后注意这个是非减的数组（注意处理等号的情况）'''def minNumberInRotateArray(self, rotateArray): # write code here arr =rotateArray if len(arr) ==0: return 0 min_value =arr[0] left, right =0, len(arr)-1 while right-left &gt;1: mid =(left+right) //2 if arr[left] &lt;=arr[mid]: left =mid elif arr[mid] &lt;= arr[right]: right =mid min_value =arr[right] return min_value 调整数组顺序使奇数位于偶数前面 输入一个整数数组，实现一个函数来调整该数组中数字的顺序，使得所有的奇数位于数组的前半部分，所有的偶数位于数组的后半部分，并保证奇数和奇数，偶数和偶数之间的相对位置不变。 Tips： 这个跟快速排序是有点像的，从右左各找到一个不符合条件，然后交换位置。快排中这个过程是线性的，非并行。下面的实现从算法角度并不是nice的，使用了python 中的list，最好的应该是 inplace 的那种，立地交换。 1234567891011121314151617"""list 和list 之间的连接，使用 list1+ list2 就是可以的"""class Solution: def reOrderArray(self, array): # write code here if array ==[]: return [] odd_list =[] even_list =[] for item in array: if item% 2 ==1: odd_list.append(item) else: even_list.append(item) return odd_list+even_list 上面版本保留了原始数字相对的顺序，下面这个没有保留相对的顺序。前者的空间复杂度是O(N), 后者的空间复杂度是O(1). 123456789101112131415161718class Solution: def reOrderArray(self, array): if len(array) ==0: return [] left =0 right =len(array)-1 while left &lt; right: # 如果是奇数 key =array[left] while left &lt; right and array[right] &amp; 1 == 0: right -= 1 array[left] = array[right] # 如果是偶数 while left &lt; right and array[left] &amp; 1 == 1: left += 1 array[right] = key return array 顺时针打印矩阵 输入一个矩阵，按照从外向里以顺时针的顺序依次打印出每一个数字，例如，如果输入如下4 X 4矩阵： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 则依次打印出数字1,2,3,4,8,12,16,15,14,13,9,5,6,7,11,10. Tips: 给定一个起始点，然后按照顺时针旋转去遍历。最后处理单行或者单列的情况。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768class Solution: """ 最后的 必须是三种特殊情况，如果修改成两种，那么”只剩下一个“ 这种特殊情况就被计算了两次 """# matrix类型为二维列表，需要返回列表def printMatrix(self, matrix): rows = len(matrix) cols = len(matrix[0]) top = 0 left = 0 down = rows - 1 right = cols - 1 result = [] while top &lt; down and left &lt; right: for j in range(left, right + 1): result.append(matrix[top][j]) top += 1 for i in range(top, down + 1): result.append(matrix[i][right]) right -= 1 for j in range(right, left - 1, -1): result.append(matrix[down][j]) down -= 1 for i in range(down, top - 1, -1): result.append(matrix[i][left]) left += 1 if top == down and left &lt; right: for j in range(left, right + 1): result.append(matrix[top][j]) if top &lt; down and left == right: for i in range(top, down + 1): result.append(matrix[i][left]) if top == down and left == right: result.append(matrix[top][left]) return result ``` - 字符串的排列&gt; 输入一个字符串,按字典序打印出该字符串中字符的所有排列。例如输入字符串abc,则打印出由字符a,b,c所能排列出来的所有字符串abc,acb,bac,bca,cab和cba。Tips: 递归```pythonclass Solution:# 递归： 变换方程： 第一字母和 剩下的所有的字母def Permutation(self, ss): # write code here if not ss: return [] res = [] self.helper(ss, '', res) return sorted(list(set(res)))def helper(self, ss, path, res): if not ss: res.append(path) for i in range(len(ss)): self.helper(ss[:i] + ss[i + 1:], path + ss[i], res) 数组中出现次数超过一半的数字 数组中有一个数字出现的次数超过数组长度的一半，请找出这个数字。例如输入一个长度为9的数组{1,2,3,2,2,2,5,4,2}。由于数字2在数组中出现了5次，超过数组长度的一半，因此输出2。如果不存在则输出0。 Tips： 如果某个数字出现的次数多于一半，那么其他所有非该数字的出现的频数是小于该数字的，所以形成一个二分类。 1234567891011121314151617181920class Solution:# 如果存在这样的数字，那么这个数字的频数一定是大于其他所有的频数# 所以可以统计一下这个def MoreThanHalfNum_Solution(self, numbers): # write code here if not numbers: return 0 target = numbers[0] nums = 0 # 统计出现次数最多的数字 for i in numbers: if target == i: nums += 1 elif nums == 0: target = i nums = 1 else: nums -= 1 res = target if numbers.count(target) &gt; len(numbers) // 2 else 0 return res 连续子数组的最大和 HZ偶尔会拿些专业问题来忽悠那些非计算机专业的同学。今天测试组开完会后,他又发话了:在古老的一维模式识别中,常常需要计算连续子向量的最大和,当向量全为正数的时候,问题很好解决。但是,如果向量中包含负数,是否应该包含某个负数,并期望旁边的正数会弥补它呢？例如:{6,-3,-2,7,-15,1,2,2},连续子向量的最大和为8(从第0个开始,到第3个为止)。给一个数组，返回它的最大连续子序列的和，你会不会被他忽悠住？(子向量的长度至少是1) Tips: 一维数组，遍历一遍，然后最大子数组和的过程。 12345678910111213141516class Solution:# largest , sum 这是两个不同的状态# 注意初始化def FindGreatestSumOfSubArray(self, array): # write code here if not array: return [] largest =array[0] sum_of_array =0 for i in array: sum_of_array += i if sum_of_array &gt; largest: largest =sum_of_array elif sum_of_array &lt;0: sum_of_array =0 return largest 第一个只出现一次的字符 在一个字符串(0&lt;=字符串长度&lt;=10000，全部由字母组成)中找到第一个只出现一次的字符,并返回它的位置, 如果没有则返回 -1（需要区分大小写）. 1234567891011121314151617181920212223242526272829303132class Solution: """ Three ways to get dictionary of string s """ def FirstNotRepeatingChar(self, s): if not s: return -1 # get dictionary from collections import defaultdict dict1 =defaultdict(int) for string in s: dict1[string] += 1 # or this way # from collections import Counter # dict1 =Counter(s) # or do it yourself #dict1 =self.Counter_self(s) for index, val in enumerate(s): if dict1[val] == 1: return -1 def Counter_self(self, s): dict1 = &#123;&#125; for val in s: if val not in dict1: dict1[val] = 1 else: dict1[val] = dict1[val] + 1 return dict1 数组中的逆序对 在数组中的两个数字，如果前面一个数字大于后面的数字，则这两个数字组成一个逆序对。输入一个数组,求出这个数组中的逆序对的总数P。并将P对1000000007取模的结果输出。 即输出P%1000000007 123456789101112131415161718192021222324252627282930313233343536373839404142# -*- coding:utf-8 -*-# 之前在牛客网上是可以通过的，现在因为时间复杂度又没办法通过了class Solution: # 这个一斤难出天机了 先不看了 # 这个到后来就已经背下来了 def InversePairs(self, data): # write code here if not data: return 0 temp = [i for i in data] return self.mergeSort(temp, data, 0, len(data ) -1) % 1000000007 def mergeSort(self, temp, data, low, high): if low &gt;= high: temp[low] = data[low] return 0 mid = (low + high) / 2 # 不懂 data 和 temp 为什么是颠倒顺序 left = self.mergeSort(data, temp, low, mid) right = self.mergeSort(data, temp, mid +1, high) count = 0 i = low j = mid +1 index = low while i &lt;= mid and j &lt;= high: if data[i] &lt;= data[j]: temp[index] = data[i] i += 1 else: temp[index] = data[j] count += mid - i +1 j += 1 index += 1 while i &lt;= mid: temp[index] = data[i] i += 1 index += 1 while j &lt;= high: temp[index] = data[j] j += 1 index += 1 return count + left + right 数字在排序数组中出现的次数 统计一个数字在排序数组中出现的次数。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859class Solution:# 二分查找，当 data[mid] ==key 的时候是顺序查找，是递归跳出的条件def GetNumberOfK(self, data, k): # write code here # 这个是有两个跳出条件的，一个是正确的跳出，一个是不正确的跳出 if not data: return 0 mid =len(data) // 2 if data[mid] == k: left = right = mid for i in range(mid - 1, -1, -1): if data[i] == k: left -= 1 for i in range(mid + 1, len(data)): if data[i] == k: right += 1 return right - left + 1 # 一半一半的舍去数据 elif data[mid] &lt; k: return self.GetNumberOfK(data[mid + 1:], k) else: return self.GetNumberOfK(data[:mid - 1], k) ``` - 数组中只出现一次的数字&gt; 一个整型数组里除了两个数字之外，其他的数字都出现了两次。请写程序找出这两个只出现一次的数字。Tips: 异或操作，两个十进制数字经过异或（二级制计算过程），最后的结果是10进制的形式。如果两个相同的数字异或，那么最后的结果是0，如果是不同的数字，最后的结果是非0.比如&gt; 2^4 # 4&gt; 3^4 # 7&gt; 40^42 # 2结果的二进制形式一定至少有一个 "1". 使用index 得到两个不同的数字二进制形式下的位置，然后从该位置将原来的数组分成两类，那么每类中只含有一个出现一次的数字，接着使用异或操作。```python class Solution: # 返回[a,b] 其中ab是出现一次的两个数字 # 使用异或的性质，如果只有一个不同，其他的偶次出现，那么全部异或的结果 # 就是那个单一的数字 def FindNumsAppearOnce(self, array): # write code here remain, index =0, 1 for num in array: remain = remain ^ num # 找出第一个是1 的位置 # index 都是 while (remain &amp; index) ==0: index = index &lt;&lt;1 res1, res2 =0,0 for num in array: # 这个条件必须是0, 表示两个在这个位数是相同的， if num &amp; index ==0: res1 =res1 ^ num else: res2 =res2 ^ num return [res1, res2] 和为S的连续正数序列 小明很喜欢数学,有一天他在做数学作业时,要求计算出9~16的和,他马上就写出了正确答案是100。但是他并不满足于此,他在想究竟有多少种连续的正数序列的和为100(至少包括两个数)。没多久,他就得到另一组连续正数和为100的序列:18,19,20,21,22。现在把问题交给你,你能不能也很快的找出所有和为S的连续正数序列? Tips: 滑动窗口（两个指针） 12345678910111213141516def FindContinuousSequence(self, tsum): # write code here if tsum &lt; 2: return [] left = 1 right = left + 1 res = [] while left &lt; tsum // 2 + 1: if sum(range(left, right)) == tsum: res.append(range(left, right)) left += 1 elif sum(range(left, right)) &lt; tsum: right += 1 else: left += 1 return res 和为S的两个数字 输入一个递增排序的数组和一个数字S，在数组中查找两个数，使得他们的和正好是S，如果有多对数字的和等于S，输出两个数的乘积最小的。 和上一个题目的不同点在于，该题目是给定了某个递增的数组。上一个题目默认的是 (0, tsum//2+1) 这样的序列。 12345678910111213141516class Solution: def FindNumbersWithSum(self, array, tsum): # write code here if len(array) &lt; 2: return [] left = 0 right = len(array) - 1 while left &lt; right: if array[left] + array[right] == tsum: return [array[left], array[right]] elif array[left] + array[right] &lt; tsum: left += 1 else: right -= 1 return [] 左旋转字符串 汇编语言中有一种移位指令叫做循环左移（ROL），现在有个简单的任务，就是用字符串模拟这个指令的运算结果。对于一个给定的字符序列S，请你把其循环左移K位后的序列输出。例如，字符序列S=”abcXYZdef”,要求输出循环左移3位后的结果，即“XYZdefabc”。是不是很简单？OK，搞定它！ Tips: python中字符串的处理是没有压力的 1234567891011121314151617181920212223242526272829303132333435363738394041424344class Solution: def LeftRotateString(self, s, n): # write code here if len(s) &lt; n: return '' return s[n:] + s[:n]``` - 翻转单词顺序列&gt; 牛客最近来了一个新员工Fish，每天早晨总是会拿着一本英文杂志，写些句子在本子上。同事Cat对Fish写的内容颇感兴趣，有一天他向Fish借来翻看，但却读不懂它的意思。例如，“student. a am I”。后来才意识到，这家伙原来把句子单词的顺序翻转了，正确的句子应该是“I am a student.”。Cat对一一的翻转这些单词顺序可不在行，你能帮助他么？Tips：首先定义一个helper() 功能是翻转字符串，然后两次翻转。第一次是全部翻转，第二次是单词翻转。```pythonclass Solution: def Reverse(self, s, left, right): while left &lt;right: s[left], s[right] = s[right], s[left] left +=1 right -=1 def ReverseSentence(self, s): # write code here if not s: return s # from immutable string to mutable list s =list(s) self.Reverse(s, 0, len(s) - 1) start, end = 0, 0 # 这个小于号 是python 中特有的坑，真正能够访问的区间是 [0, len(s)-1] 这样的区间 while start &lt; len(s): if s[start] == " ": start += 1 end += 1 elif end == len(s) or s[end] == " ": self.Reverse(s, start, end - 1) # update 操作 end += 1 start = end else: end += 1 return "".join(s) 扑克牌顺子 LL今天心情特别好,因为他去买了一副扑克牌,发现里面居然有2个大王,2个小王(一副牌原本是54张^_^)…他随机从中抽出了5张牌,想测测自己的手气,看看能不能抽到顺子,如果抽到的话,他决定去买体育彩票,嘿嘿！！“红心A,黑桃3,小王,大王,方片5”,“Oh My God!”不是顺子…..LL不高兴了,他想了想,决定大/小 王可以看成任何数字,并且A看作1,J为11,Q为12,K为13。上面的5张牌就可以变成“1,2,3,4,5”(大小王分别看作2和4),“So Lucky!”。LL决定去买体育彩票啦。 现在,要求你使用这幅牌模拟上面的过程,然后告诉我们LL的运气如何， 如果牌能组成顺子就输出true，否则就输出false。为了方便起见,你可以认为大小王是0。 Tips：list 中的空缺数量 需要不大于 大小王总数，这样才能构成顺子。 12345678910111213141516171819202122232425class Solution: """ 空缺是1 意味着这两个数字是连续的 比如说 1 2， 这个big -small ==1, 所以这个空缺是0，不用进行填充。 """ def IsContinuous(self, numbers): # write code here if not numbers: return False numbers.sort() # sort() sorted() 这种怎么使用，返回值是什么，这些基本的东西 zeros =numbers.count(0) gaps = 0 left = zeros # 因为这个是排序之后的结果，所以可以这样进行操作 right = left + 1 # 实际上还是两个指针， 所以可以使用两个指针进行操作 # 本质上是两个 相邻指针在进行移动，因为是排序之后，所以没有问题 while right &lt; len(numbers): if numbers[left] == numbers[right]: return False gaps += numbers[right] - numbers[left] - 1 left = right right += 1 # 这种是真的 很简洁， gaps &lt;= zeros 少去了很多if else的判断 return gaps &lt;= zeros 孩子们的游戏(圆圈中最后剩下的数) 每年六一儿童节,牛客都会准备一些小礼物去看望孤儿院的小朋友,今年亦是如此。HF作为牛客的资深元老,自然也准备了一些小游戏。其中,有个游戏是这样的:首先,让小朋友们围成一个大圈。然后,他随机指定一个数m,让编号为0的小朋友开始报数。每次喊到m-1的那个小朋友要出列唱首歌,然后可以在礼品箱中任意的挑选礼物,并且不再回到圈中,从他的下一个小朋友开始,继续0…m-1报数….这样下去….直到剩下最后一个小朋友,可以不用表演,并且拿到牛客名贵的“名侦探柯南”典藏版(名额有限哦!!^_^)。请你试着想下,哪个小朋友会得到这份礼品呢？(注：小朋友的编号是从0到n-1) Tips: 约瑟夫环的问题， 要求求解的是最后胜利者的编号，所以应用数学技巧就可以了。 $ f(x) = (f( x-1) + m ) % (x) $ , 共有 m 个编号，n 个人 12345678910111213class Solution:# mod 求余 的操作， a mod b ==c ,说明 a除以b 之后余数是c# https://blog.csdn.net/gatieme/article/details/51435055， 从做题思路上讲解的比较好# n 个小朋友，然后是m 个编号def LastRemaining_Solution(self, n, m): # write code here if n&lt; 1 or m&lt;1: return -1 last =0 for i in range(2, n+1): # 这个相当于 是一个 “挑选人” 的逆过程， 因为使用的 mod 操作就是取余的操作 last =(last +m) %i return last 求1+2+3+…+n 求1+2+3+…+n，要求不能使用乘除法、for、while、if、else、switch、case等关键字及条件判断语句（A?B:C）。 Tips： 不使用条件判断，来控制跳出；这里使用的是 “短路条件” 来 控制 递归的跳出。 12345678910111213141516class Solution: # 如果你想使用全局变量，那么放在 __init__ 中就是一个很好的方式 def __init__(self): self.ans =0 def Sum_Solution(self, n): # write code here self.recur(n) return self.ans # n&gt;0 就是一个短路条件，这个直接决定了后面递归会不会继续执行下去，也就是跳出的条件 # 至于会不会回到原来最初的状态，这个是不重要的，最后的结果是 self.ans ，false之后直接使用这个就行了 def recur(self, n): self.ans += n n -= 1 return n &gt; 0 and self.Sum_Solution(n) 不用加减乘除做加法 写一个函数，求两个整数之和，要求在函数体内不得使用+、-、*、/四则运算符号。 Tips: 使用 异或和与 来进行 “加”、“减”的操作。加法是分成当前位相加 和进位两个部分的。ps，python 中 函数名是分大小写的。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253class Solution: """ 不能使用四则运算符，我们就可以使用位运算符。对这两个数在更底层的角度上进行运算。也就是从 01 这种子串的角度进行考虑 https://blog.csdn.net/derrantcm/article/details/46798763 这个博客对于数的运算过程和 位运算是如何一一对应的。分为不进位相加和进位相加。 """ # 这个只能是正整数的操作，真是热力狗 # 这个弄出来是真心不容易哈 def add(self, num1, num2): while num2 != 0: carry = num1 &amp; num2 num1 = num1 ^ num2 # 这个应该理解为到高位 而不是*2 这样的操作 num2 = carry &lt;&lt; 1 return num1 def sub(self, num1, num2): while num2 != 0: carry = (~num1) &amp; num2 num1 = num1 ^ num2 num2 = carry &lt;&lt; 1 return num1 def Add(self, num1, num2): if num1 &gt;= 0 and num2 &gt;= 0: result = self.add(num1, num2) elif num1 &gt; 0 and num2 &lt; 0: flag = 1 if num1 &gt; abs(num2) else -1 # num2 =abs(num2) # keep num1 bigger than num2 if num1 &lt; abs(num2): num1, num2 = abs(num2), num1 result = self.sub(num1, abs(num2)) result = result * flag elif num1 &lt; 0 and num2 &gt; 0: flag = 1 if abs(num1) &lt; num2 else -1 if abs(num1) &lt; num2: num1, num2 = num2, abs(num1) result = self.sub(abs(num1), num2) result = result * flag else: flag = -1 num1 = abs(num1) num2 = abs(num2) result = self.add(num1, num2) result = result * flag return result 把字符串转换成整数 将一个字符串转换成一个整数(实现Integer.valueOf(string)的功能，但是string不符合数字要求时返回0)，要求不能使用字符串转换整数的库函数。 数值为0或者字符串不是一个合法的数值则返回0。 Tips：还是python 中处理 string， 使用 dictionary 处理字符串和 数字的匹配。 12345678910111213141516171819202122class Solution:# 有很多不合法的输入，比如空字符串，正负号，非数字字符 数据溢出，所以从反面考虑更加简单一些# 合法的输入只有数字和符号位 + 和-def StrToInt(self, s): # write code here int_list=['0', '1', '2', '3', '4', '5', '6','7', '8', '9', '+', '-'] if s ==" ": return 0 sum1 =0 flag =1 # 正负号 for string in s: if string not in int_list: return 0 if string =="+": flag =1 continue elif string =="-": flag = -1 continue else: sum1 =sum1 *10 +int_list.index(string) return sum1*flag 数组中重复的数字 在一个长度为n的数组里的所有数字都在0到n-1的范围内。 数组中某些数字是重复的，但不知道有几个数字是重复的。也不知道每个数字重复几次。请找出数组中任意一个重复的数字。 例如，如果输入长度为7的数组{2,3,1,0,2,5,3}，那么对应的输出是第一个重复的数字2。 Tips： 当然这个是可以使用dictionary，需要O(N) 这样的空间。还有一种思路，这个所有的数字都是 (0, n-1) 这样的区间，所以是可以和index 进行联系一下的。这个 “交换” 是会进行排序的（index 对应着 number），但排序不是最终的目的，最终目的是得到首个重复的数字。 12345678910111213141516171819202122232425262728293031323334353637383940414243class Solution: # 这里要特别注意~找到任意重复的一个值并赋值到duplication[0] # 函数返回True/False # 第二种方式，如果这个是有序的 那么 numbers[i] ==i 这个是成立的 def duplicate(self, numbers, duplication): # write code here length =len(numbers) for i in range(length): while i != numbers[i]: if numbers[numbers[i]] == numbers[i]: duplication[0] = numbers[i] return True else: numbers[numbers[i]], numbers[i] = numbers[i], numbers[numbers[i]] return False``` - 构建乘积数组&gt; 给定一个数组A[0,1,...,n-1],请构建一个数组B[0,1,...,n-1],其中B中的元素B[i]=A[0]*A[1]*...*A[i-1]*A[i+1]*...*A[n-1]。不能使用除法。Tips: 分成上下三角形进行计算，不能每个B[i] 都进行单独重复计算。下三角形是从上往下遍历，上三角形是从下往上遍历。ans 存储各个不同的结果。```python class Solution: # 思路： 转换成图形的就容易想一些， https://blog.csdn.net/u010005281/article/details/80200398 # 代码：https://blog.csdn.net/fuxuemingzhu/article/details/79718543 # A 是一个list ，只是自己构建的是一个矩阵 def multiply(self, A): # write code here ans =[] tmp =1 length =len(A) # 值得是 rows # 首先是下三角形 各个部分的数值的相乘， 从上往下遍历 for i in range(length): ans.append(tmp) tmp *= A[i] tmp =1 # 上三角形 从下往上进行遍历 for i in range(length-1, -1, -1): ans[i] *= tmp tmp *= A[i] return ans 表示数值的字符串 请实现一个函数用来判断字符串是否表示数值（包括整数和小数）。例如，字符串”+100”,”5e2”,”-123”,”3.1416”和”-1E-16”都表示数值。 但是”12e”,”1a3.14”,”1.2.3”,”+-5”和”12e+4.3”都不是。 123456789101112131415161718192021222324252627282930313233343536373839class Solution: # s字符串 # 第一种方法是 float()强转，一种是 re 正则表达式匹配 最后一种逻辑判断之类的 # 以 e 为分割符，分成front and behind 两部分，behind 长度不能为0 或者出现 . # digit的判断，+- 只能出现在首位， . 只能出现一次 """ https://github.com/leeguandong/Interview-code-practice-python/blob/master/%E5%89%91%E6%8C%87offer/%E8%A1%A8%E7%A4%BA%E6%95%B0%E5%80%BC%E7%9A%84%E5%AD%97%E7%AC%A6%E4%B8%B2.py """ def isNumeric(self, s): if not s or len(s) == 0: return s s = [i.lower() for i in s] if 'e' in s: index = s.index('e') front = s[:index] behind = s[index + 1:] if len(behind) == 0 or '.' in behind: return False f = self.Digit(front) b = self.Digit(behind) return f and b else: isNum = self.Digit(s) return isNum def Digit(self, s): dotNum = 0 allowNum = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '.', '+', '-'] for i in range(len(s)): if s[i] not in allowNum: return False if s[i] == '.': dotNum += 1 if s[i] in '+-' and i != 0: return False if dotNum &gt; 1: return False return True 字符流中第一个不重复的字符 请实现一个函数用来找出字符流中第一个只出现一次的字符。例如，当从字符流中只读出前两个字符”go”时，第一个只出现一次的字符是”g”。当从该字符流中读出前六个字符“google”时，第一个只出现一次的字符是”l”。 12345678910111213141516171819class Solution: # 返回对应char # 这个没有了dict 那么依赖于 count 函数 # 主要差别在于有了一个 字符流，是动态的，所以需要有一个大的存储的list def __init__(self): self.list1 =[] def FirstAppearingOnce(self): # write code here for string in self.list1: if self.list1.count(string) == 1: return string return "#" def Insert(self, char): # write code here self.list1.append(char)]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>剑指offer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pygen]]></title>
    <url>%2F2019%2F04%2F24%2Fpygen%2F</url>
    <content type="text"><![CDATA[pygen功能：有关联的随机生成人名，邮箱，ID Card (ssn)，电话，地址等信息，并且可以选择保存为 pandas dataframe格式, 数据库”.db” 文件, Excel 文件和csv 文件格式，用于机器学习训练。项目地址github。 随机生成虚假个人信息具有很大的应用空间。首先，虚假的生成数据可以用于机器学习模型的“准备数据”，当真实的数据比较少或者难以获得的时候，可以使用生成数据进行训练模型，待模型调通之后，然后使用真实的模型。并且，当真实的数据集中缺少某些特征时候，可以使用这种方法进行特征的填充。比如大的数据集中缺少现居城市地址的时候，可以调用该库中的 “city_real” 进行填充。 当前使用最为广泛的是 Faker 开源库用于数据的生成。虽然该库支持中文，但是对于中文的支持力度有限，所以有时候并不能满足我的需求，比如说生成的身份证 (ssn) 和姓名所能体现的性别是不匹配(了解更多可以参考这里)、生成的姓名中缺少复姓和电话邮箱等信息不符合我们的使用习惯等等。所以我将从以下几点改进： 增强数据之间相关性 生成名字的多样性 符合国人使用习惯的邮箱电话 提供保存多种保存文件格式，更加适合机器学习的训练 中文名字有很强的性别属性。例如名字中带有“杰”“志”“宏”等字的一般为男性，带有“琬”“佩”“梅”等字的一般为女性。当然也有一些比较中性的字，例如“文”“安”“清”等，比较难猜测性别，关于这点会在另一个博客中展开，请期待。 faker 对中文的支持有限，比如下面这种情况。 1234from faker import Fakerfake = Faker('zh_CN')for _ in range(10): print(fake.name(),fake.ssn(),fake.phone_number()) 从图中可以明显的看出 “王玉梅”和 “李桂花”都是两个女性，但是这种身份证信息（ssn）都没有体现这点。关于身份证的科普信息可以从这里获得。简单来说倒数第二位表示性别信息，如果是男性就是奇数如果是女性就是偶数。faker 生成的数据是不具有数据之间的相关性的。 基于此，我们进行了改进。首先是姓名的生成，然后是性别的判断，最后再生成相应性别的身份证号码。 123from pygen import pygendb =pygen()db.gen_dataframe(fields =['name', 'ssn', 'phone', 'email']) 效果如下： 红色线条表示姓名和性别对应一致，蓝色线条表示结果不确定（“镜阳炎” 像是一个中性的名字），绿色表示生成了含有复姓的名字，增强了数据的多样性。 从上图的 “mail” 一列可以看出邮箱前缀的命名基本上是中文名字中“姓” 和“民”的拼音组合，加强了数据之间的相关性和真实度。 另外，电话号码按照运营商分为三类：0 表示移动，1表示联通，2表示电信。 print(&apos;移动字段:&apos;) for _ in range(5): print(db.simple_ph_num(types =0)) print(&apos;联通字段:&apos;) for _ in range(5): print(db.simple_ph_num(types =1)) print(&apos;电信字段：&apos;) for _ in range(5): print(db.simple_ph_num(types =2)) 输出： 移动字段: 15023689929 16771753917 16790223946 15950129353 15271129554联通字段: 13869739303 13786227031 13950354445 15137578545 15240836142电信字段： 17172983067 15658567011 18562313243 17073127396 15543448286 最后提供了多种文件保存格式，包括”.csv”, “.db” 和”.xlsx”等格式。可以使用如下： from pygen import pygen db =pygen() db.gen_table(filename =filename, fields =[&apos;name&apos;, &apos;ssn&apos;, &apos;phone&apos;, &apos;email&apos;]) db.gen_excel(filename =filename, fields =[&apos;name&apos;, &apos;ssn&apos;, &apos;phone&apos;, &apos;email&apos;]) db.gen_csv(filename =filename, fields =[&apos;name&apos;, &apos;ssn&apos;, &apos;phone&apos;, &apos;email&apos;])]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Data Enhancement</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mode Collapse in GANs]]></title>
    <url>%2F2019%2F04%2F18%2Fmode-collapse-in-gan%2F</url>
    <content type="text"><![CDATA[Mode collapse, a failure case for GANs where the generator generate a limited diversity of samples, regardless of the input. But what causes the mode collapse? There are four reasons for that. The objective of GANs The generator, generates new data, while the discriminator evaluates it for authenticity but not for the diversity of generated instances. the generator can win by producing a polynomial number of training examples. And a low capacity discriminator cannot detect this process, thus, it cannot guide the generator to approximate the target distribution. Even if a high discriminator identifies and assigns the collapse part a low probability, then the generator will simply move from its collapsed output to focus on another fixed output point. Generator No matter the objective function is, if it only considers individual samples (without looking forward or backward) then the generator is not directly incentivised to produce diverse examples. From [1], standard GAN training corresponds exactly to updating the generator parameters using only the first term in this gradient because of a fixed discriminator during GAN training. Therefore, in standard GAN training, each generator update step is a partial collapse towards a delta function. $$\frac { \mathrm { d } f _ { K } \left( \theta _ { G } , \theta _ { D } \right) } { \mathrm { d } \theta _ { G } } = \frac { \partial f \left( \theta _ { G } , \theta _ { D } ^ { K } \left( \theta _ { G } , \theta _ { D } \right) \right) } { \partial \theta _ { G } } + \frac { \partial f \left( \theta _ { G } , \theta _ { D } ^ { K } \left( \theta _ { G } , \theta _ { D } \right) \right) } { \partial \theta _ { D } ^ { K } \left( \theta _ { G } , \theta _ { D } \right) } \frac { \mathrm { d } \theta _ { D } ^ { K } \left( \theta _ { G } , \theta _ { D } \right) } { \mathrm { d } \theta _ { G } }$$ Some methods have been proposed. Multiple generators and weight-sharing generators are developed to capture more modes of the distribution. Discriminator The mode collapse is often explained as gradient exploding of discriminator, which comes from the imbalance between the discriminator and the generator. For example, the technique of TTUR could help discriminator to keep its optimality. But some researchers believe that this is a desirable goal since a good discriminator can give good feedback and ignore the fact. In addition, the discriminator process each example independently, the generator depends on discriminator, thus no mechanism to tell the outputs of the generator to become more similar to each other. The idea from [2], that we could use mini-batch discrimination to help generator give better feedback A straightforward approach to handle multimodality is to take random noise vectors along with the conditional contexts as inputs, where the contexts determine the main content and noise vectors are responsible for variations.The noise vectors are ignored or of minor impacts, since cGANs pay more attention to learn from the high-dimensional and structured conditional contexts. Another question Mode collapse may happen only partially?since training is stochastic progress, the input of generator network will vary and the sample drawn from the real distribution will also vary But sometimes mode collapse is not all bad news. In style transfer using GAN, we are happy to convert one image to just a good one, rather than finding all variants. Indeed, the specialization in the partial mode collapse sometimes creates higher quality images. referrences:[1]. Section 2.4 of Unrolled Generative Adversarial Networks[2]. Section 3.2 of Improved Techniques for Training GANs[3]. Mode Seeking Generative Adversarial Networks for Diverse Image Synthesis[4]. Improving Generalization and Stability of Generative Adversarial Networks]]></content>
      <categories>
        <category>Deep learning</category>
      </categories>
      <tags>
        <tag>GANs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[A Not-So-Gentle Introduction to Hyper-parameters Tuning]]></title>
    <url>%2F2019%2F04%2F17%2Fa-not-so-gentle-introduction-to-hyperparameters-tuning%2F</url>
    <content type="text"><![CDATA[Setting the hyper-parameters seems like a black art that requires years of experience to acquire. Currently, there are no simple and easy ways to set hyper-parameters, especifically, batch size, learning rate, momentum, and weight decay. A grid search or random search maybe sounds like a good idea. In this blog, I’d like to share you my idea from reading papers and my projects. Hyper-parametersBatch SizeLearning rate is maybe the most important hyper-parameters, but we choose batch size firstly because large batch size needs a large learning rate in most circumstances. A general principle is: use as a large batch size as possible to fit your CPU memory or/both GPU memory. There are several reasons: larger batch sizes permit the use of larger learning rates A constant number of iterations favors larger batch sizes However, small batch sizes add regularization while large batch sizes add less. So utilize it while balancing the proper amount of regularization. Learning RateWe will introduce the idea from [Cyclical Learning Rates for Training Neural Networks][1]: Cyclical Learning Rates. Instead of monotonically decreasing the learning rate, this method lets the learning rate cyclically vary between reasonable boundary values. The essence of this learning rate policy comes from the observation that increasing the learning rate might have a short term negative effect and yet achieve a longer term beneficial effect. This observation leads to the idea of letting the learning rate vary within a range of values rather than adopting a stepwise fixed or exponentially decreasing value. That is, one sets minimum and maximum boundaries and the learning rate cyclically varies between these bounds.%From Cyclical Learning Rates for Training Neural Networks An intuitive understanding of why CLR methods work comes from considering the loss function topology. Dauphin et al. argue that the difficulty in minimizing the loss arises from saddle points rather than poor local minima. Saddle points have small gradients that slow the learning process. However, increasing the learning rate allows for more rapid traversal of saddle point plateaus. But the question is that how can we find the Minimum bound and Maximum bound. There is a simple way to estimate the reasonable minimum and maximum boundary values with one training run of the network for a few epochs. It is a “LR range test”; run your model for several epochs while letting the learning rate increase linearly between low and high LR values. For example, set both the step size and maxiter to the same number of iterations. In this case, the learning rate will increase linearly from the minimum value to the maximum value during this short run. Next, plot the accuracy versus learning rate. Note the learning rate value when the accuracy starts to increase and when the accuracy slows, becomes ragged, or starts to fall. These two learning rates are good choices for bounds; that is, set $base-{lr}$ to the first value and set $max-{lr}$ to the latter value. MomentumSince learning rate is regarded as the most important hyper-parameter to tune then momentum is also important. Like learning rates, it is valuable to set momentum as large as possible without causing instabilities during training. The large learning rate can deal with local minimum but works fail when it comes to saddle point where momentum comes to rescue. The local minimum is like the following picture.In mathematics, a saddle point or minimax point is a point on the surface of the graph of a function where the slopes (derivatives) in orthogonal directions are all zero (a critical point), but which is not a local extremum of the function. Your first step from the very top would likely take you down, but then you’d be on a flat rice terrace. The gradient would be zero, and you’d have nowhere to go. To remedy this, we employ momentum - the algorithm remembers its last step and adds some psroportion of it to the current step. This way, even if the algorithm is stuck in a flat region, or a small local minimum, it can get out and continue towards the true minimum. In summary: when performing gradient descent, learning rate measures how much the current situation affects the next step, while momentum measures how much past steps affect the next step. Weights DecayWhen training neural networks, it is common to use “weight decay,” where after each update, the weights are multiplied by a factor slightly less than 1. This prevents the weights from growing too large and can be seen as gradient descent on a quadratic regularization term. But why? Large weights might correlate with certain patterns in the input data (x), this means that the model almost hard codes certain values. This then makes our training data fit well but our test data fit less well. The idea of weight decay is simple: to prevent overfitting, every time we update a weight $w$ with the gradient $∇J$ in respect to $w$, we also subtract from it $λ∙w$. This gives the weights a tendency to decay towards zero, hence the name. L2 is a type of weights decay.$$J ( W ; X , y ) + \frac { 1 } { 2 } \lambda \cdot | W | ^ { 2 }$$ But weights decay is not necessarily true for all gradient-base algorithms and was recently shown to not be the case for adaptive gradient algorithms, such as Adam. In addition, weight decay is not the only regularization technique. In the past few years, some other approaches have been introduced such as Dropout, Bagging, Early Stop, and Parameter Sharing which work very well in NNs. Takeoff Batch Size Use as a large batch size as possible to fit your memory Learning Rate Perform a learning rate range test to identify a “large” learning rate. Momentum Test with short runs of momentum values 0.99, 0.97, 0.95, and 0.9 to get the best value for momentum. If using the 1-cycle learning rate schedule, it is better to use a cyclical momentum (CM) that starts at this maximum momentum value and decreases with increasing learning rate to a value of 0.8 or 0.85. Weights Decay A grid search to determine the proper magnitude but usually does not require more than one significant figure accuracy.A more complex dataset requires less regularization so test smaller weight decay values, such as $10^{−4} $, $10^{−5} $, $10^{−6} $, 0.A shallow architecture requires more regularization so test larger weight decay values, such as $10^{−2} $, $10^{−3} $, $10^{−4} $. References[1]. Cyclical Learning Rates for Training Neural Networks[2]. A disciplined approach to neural network hyper-parameters]]></content>
      <categories>
        <category>Deep learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Weights Initialization]]></title>
    <url>%2F2019%2F04%2F17%2Fweights-initialization%2F</url>
    <content type="text"><![CDATA[weights initialization 知识点 和分享两个图像领域的架构（resnet 和 inception v3）。 Training a neural network consists of four steps: initialize weights and biases, forward propagation, compute the loss function and backward propagation. This blog mainly focuses on the first part: weights initialization. After completing this tutorial, you will know: Four main types of weights initialization How to choose between Xavier /zivier/ initialization and He initialization Types of Weights Initialization Initializing weights with zero When you set all weights in a neural network to zero, the derivative with respect to loss function is the same for every $ w$ in the same layer, thus all the weights have the same values in the subsequent iteration, which makes your model equivalent to a linear model. Initializing weights randomly You can get weights like this (Python): w =np.random.randn(layer_size[l],layer_size[l-1]) The weighs follows standard normal distribution while it can potentially lead to two issues: vanishing gradients and exploding gradients.下面的情况是很容易发生，因为网络中特征足够多（网络结构足够宽），所以 random 得到数值有足够的 coverage，所以就会出现 weights too small or too large 这种情况。 If the weights start too small, then the signal shrinks as it passes through each layer until it’s too small to be useful. If the weights start too large, then the signal grows as it passes through each layer until it’s too massive to be useful (big value in sigmoid function). Thus there are two necessary conditions to consider: The values of each activation layer won’t be zero The values of each activation layer won’t go into the area of saturation Xavier/Glorot Initialization For deep networks, we can use a heuristic to initialize the weights depending on the non-linear activation function. This applies to Xavier and He initialization. Xavier/Glorot Initialization initializes the weights in your network by drawing them from a distribution with zero mean and a specific variance.$$ { var } ( w _ { i } ) = \frac { 1 } { layer_{l-1}}$$ w=np.random.randn(layer_size[l],layer_size[l-1])*np.sqrt(1/layer_size[l-1]) In practice, it works better for layers with sigmoid or tanh function. He Initialization Using RELU or Leaky RELU is relatively robust to the vanishing/ exploding gradient issues compared with sigmoid function especially for networks that are not too deep. And it the case of Leaky RELU, it never has zero gradients. For RELU, we multiply the randomly generated values of $w$ by: $$\sqrt { \frac { 2 } { layer _ { [ l - 1 ] } } }$$ w=np.random.randn(layer_size[l],layer_size[l-1])*np.sqrt(2/layer_size[l-1]) Sometimes, we combine the idea of Xavier initialization and He initializaiton so the variance becomes the following: $$\sqrt { \frac { 2 } { layer _ { [ l - 1 ] } + \operatorname { layer } _ { [ l ] } } }$$ w=np.random.randn(layer_size[l],layer_size[l-1])*np.sqrt(2/(layer_size[l-1]+layer_size[l])) The idea behind this is that we set the weights neither too much bigger than 1 nor too much less than 1 so the gradients do not vanish or explode too quickly. TakeoffIn summary, the main difference in machine learning is the following: He initialization works better for layers with ReLu(s) activation. Xavier initialization works better for layers with sigmoid activation. Referrence:He initialization Xavier initialization ResNetResNet 诞生于一个美丽而简单的观察：为什么非常深度的网络在增加更多层时会表现得更差？ResNet 的作者将这些问题归结成了一个单一的假设：直接映射是难以学习的。而且他们提出了一种修正方法：不再学习从 x 到 H(x) 的基本映射关系，而是学习这两者之间的差异，也就是「残差（residual）」。然后，为了计算 H(x)，我们只需要将这个残差加到输入上即可。假设残差为$F(x)=H(x)-x $，那么现在我们的网络不会直接学习$ H(x)$ 了，而是学习 $F(x)+x$。 ResNet 的每一个「模块（block）」都由一系列层和一个「捷径（shortcut）」连接组成，这个「捷径」将该模块的输入和输出连接到了一起。然后在元素层面上执行「加法（add）」运算，如果输入和输出的大小不同，那就可以使用零填充或投射（通过 1×1 卷积）来得到匹配的大小。 Inception V3如果 ResNet 是为了更深，那么 Inception 家族就是为了更宽。第一个见解与对层的操作有关。在传统的卷积网络中，每一层都会从之前的层提取信息，以便将输入数据转换成更有用的表征。 见解 1：为什么不让模型选择？这种模型架构的信息密度更大了，这就带来了一个突出的问题：计算成本大大增加。不仅大型（比如 5×5）卷积过滤器的固有计算成本高，并排堆叠多个不同的过滤器更会极大增加每一层的特征映射的数量。而这种计算成本增长就成为了我们模型的致命瓶颈。 这就涉及到了见解 2：使用 1×1 卷积来执行降维。为了解决上述计算瓶颈，Inception 的作者使用了 1×1 卷积来「过滤」输出的深度。一个 1×1 卷积一次仅查看一个值，但在多个通道上，它可以提取空间信息并将其压缩到更低的维度。比如，使用 20 个 1×1 过滤器，一个大小为 64×64×100（具有 100 个特征映射）的输入可以被压缩到 64×64×20。通过减少输入映射的数量，Inception 可以将不同的层变换并行地堆叠到一起，从而得到既深又宽（很多并行操作）的网络。 Inception Net v3 incorporated all of the above upgrades stated for Inception v2, and in addition used the following: RMSProp Optimizer. Factorized 7x7 convolutions. BatchNorm in the Auxillary Classifiers. Label Smoothing (A type of regularizing component added to the loss formula that prevents the network from becoming too confident about a class. Prevents over fitting). GANs 网络结构]]></content>
      <categories>
        <category>Deep learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[CycleGAN & StyleGAN]]></title>
    <url>%2F2019%2F04%2F09%2Fcyclegan-stylegan%2F</url>
    <content type="text"><![CDATA[In the past few yeas, GANs have been used in lots of different applications such as generating synthetic data, style transfer, super-resolution and text2image generation. But we donn’t aim to give a overview of what GANs are made for. There are many great and detailed blogs for your understanding. What this post is about Main ideas of CycleGAN Keypoints in StyleGAN A Gentle Introduction of GANsWe assume the reader has some prior experience with neural networks. In addition, if you are familiar with GANs you can skip this section. The famous minimax objective function can be formulated as following:$$\min _ { \theta _ { g } } \max _ { \theta _ { d } } \left[ \mathbb { E } _ { x \sim p _ { d a t a } } \log D _ { \theta _ { d } } ( x ) + \mathbb { E } _ { z \sim p ( z ) } \log \left( 1 - D _ { \theta _ { d } } \left( G _ { \theta _ { g } } ( z ) \right) \right) \right)$$But in practical, the loss function cannot work very well. So we have alternative objective function: Gradient ascent on discriminator $$\max _ { \theta _ { d } } \left[ \mathbb { E } _ { x \sim p _ { d a t a } } \log D _ { \theta _ { d } } ( x ) + \mathbb { E } _ { z \sim p ( z ) } \log \left( 1 - D _ { \theta _ { d } } \left( G _ { \theta _ { g } } ( z ) \right) \right) \right]$$ Gradient ascent on generator$$\max _ { \theta _ { g } } \mathbb { E } _ { z \sim p ( z ) } \log \left( D _ { \theta _ { d } } \left( G _ { \theta _ { g } } ( z ) \right) \right)$$ The reasoning behind this can be found in original paper. Simplily speaking, we can get higher gradient signal for bad samples, which works much better in practice.bad case 的时候，使用原来的min-max function会使得学习率不够，使用 gradient ascent 就会好一些。 From Stanford CS231 Lecture 13 — Generative Models Main ideas of CycleGANCycleGAN was introduced in 2017 out of Berkeley, Unpaired Image-to-Image Translation Using Cycle-Coonsistent Adversarial Networks. This task is performed on unpaired data. Recent methods such as Pix2Pix depend on the availability of training examples where the samee data is availabel in both domains. However, CycleGAN is able to learning such pair information without one-to-one mapping between training data in source and target domains. Network ArchitectureWe build three networks. A generator $F$ to convert image $y$ to image $ \hat{x}$ A generator $G$ to convert image $\hat{x}$ to image $ \hat{y}$ A discriminator $D$ to identify real image or generated picture Simplified version of CycleGAN architecture can be showed in the following.The function $F$ and $G$ are generator network, which consists of encoder, transformer and decoder. Encoder is extracting the features from an image which is done by convolution networks. Each convolution layer leads to extraction of progressively higher level features. We would like to transform the feature emebdding of an image from domain $X$ to that of domain $Y$. So for this, authors have used 6 layers of ResNet blocks. ResNet block is a neural network layer which consists of two convolutiona layers when a residue of input ia added to the output. This is done to ensure properties of input of previous layers are available for later layers as well. ResNet block can be summarized in following imageThe decoder transfer embedding from $y$ back to original embedding $x$. Loss function（对于loss 实际上只有两个，只不过在第一个loss 中 x 和y 分别使用了两次，所以变成了两个公式。）There are two types of losses in CycleGAN. Besides adversarial loss, we have another loss named reconstruction cost.Adversarial loss is similaity to original GAN.$$\operatorname { Loss } _ { a d v } \left( F , D _ { x } , Y \right) = \frac { 1 } { m } \sum _ { i = 1 } ^ { m } \left( 1 - D _ { x } \left( F \left( y _ { i } \right) \right) \right) ^ { 2 }$$$$\operatorname { Loss } _ { a d v } \left( G , D _ { y } , X \right) = \frac { 1 } { m } \sum _ { i = 1 } ^ { m } \left( 1 - D _ { y } \left( G \left( x _ { i } \right) \right) \right) ^ { 2 }$$However, the adversarial loss alone is not sufficient to produce good looking images, which can not enfore that the input and output are recognizably the same. The cycle consistency loss addresses this issue. It relies on the expectation that if you convert an image to the other domain and back again, and then you should get back something similar to what you put in. It enforces that $F ( G ( x ) ) \approx x$ and $G ( F ( y ) ) \approx y$.$$\operatorname { Loss } _ { c y c } ( G , F , X , Y ) = \frac { 1 } { m } \sum _ { i = 1 } ^ { m } \left[ F \left( G \left( x _ { i } \right) \right) - x _ { i } \right] + \left[ G \left( F \left( y _ { i } \right) \right) - y _ { i } \right]$$We can get the full objective function by putting these two together.$$\mathcal { L } \left( G , F , D _ { x} , D _ { y } \right) = \mathcal { L } _ { \text { GAN } } \left( G , D _ { y } , X , Y \right) + \mathcal { L } _ { \text { GAN } } \left( F , D _ { x } , Y , X \right) + \lambda \mathcal { L } _ { \text { cyc } } ( G , F )$$ Keypoints of StyleGAN(想要解决的问题，ProGAN 是训练过程中不是一个 fine-tune 的过程，而是从一种状态到另一种状态的过程，所以styleGAN 想要 control specific features during training)The StyleGAN offers an upgrade version of ProGAN’s image generator, with a focus on the generator. ProGAN generates high-quality images but, in most models, its ability to control specific features of the generated image is very limited. In other word, the features are entangled and therefore attempting to tweak the input, even a bit, usually affects multiple features at the same time. A good illustrations would be following pictures.Compared with first version (ProGAN), the new generator includes several additions to ProGAN’s generators. Mapping NetworkThe mapping network’s goal in to encode the input vector into an intermediate vector whose different elements control different visual features, which consists of 8 fully connected layers and its output $w$ is of the same size as the input. Style Modules (AdaIN)The AdaIn (Adaptive Instance Normalization) module transfers the encoded information $w$, created by the mapping network, into the generated image. Removing traditional inputSince the encoded information $w$ from mapping network was used into generator image, the traditional random input can be omitted and replaced by constant values.]]></content>
      <categories>
        <category>Deep learning</category>
      </categories>
      <tags>
        <tag>GANs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python from Beginner to Master]]></title>
    <url>%2F2019%2F04%2F09%2Fpython-for-beginners%2F</url>
    <content type="text"><![CDATA[Basic Skillsmodulepython 文件可以当做主文件进行运行或者当做函数的集合进行调用。如果是前者一般是需要包含”__name__ ==”__main__”。对于后者就是在其他的python文件中进行调用。12import my_module # python文件from my_module import my_object packagesfrom packageroot.packagefolder.mod import my_object Note: Ensure each directory within your package import contains a file __init__.py python-pathpython2 和python3 使用不同的解释器，导致在一些函数命名和计算上有一些差别，最好在文件的开头标明使用的解释器。 while or forwhile : provide a condition and run the loop until the condition is not met. for: loop for a number of specific times; loop over items or characters of a string. examples:1234[Variable] AggregateFunction([Value] for [item] in [collection])x =[1, 2,3, 4, 5]y =[ 2*a for a in x if a%2 ==0]y &gt;&gt; [4, 8] 或者可以使用这样更加简洁的语句：12345678910111213 lambda arguments : expression fun1 = lambda a,b,c : a+b+c print(fun1(5,6,2))``` 来个比较复杂的例子:```python nums =[1,2,3,4,5] letters =['a', 'b', 'c','d','e'] # 两个for 循环也是要熟练 nums_letters =[[n, l] for n in nums for l in letters ] nums_letters break, continue, or passThe break, continue, and pass statements in Python will allow you to use for loops and while loops more effectively in your code.12345678910number = 0for number in range(10): number = number + 1 if number == 5: pass # pass here print('Number is ' + str(number))print('Out of loop') The pass statement occurring after the if conditional statement is telling the program to continue to run the loop and ignore the fact that the variable number evaluates as equivalent to 5 during one of its iterations. yield 可以用用作新的 if的测试, return results without termination The pass statement can create minimal classes, or act as a placeholder when working on new code and thinking on an algorithmic level before hammering out details.pass 的存在就是占坑，否则这个地方就是报错（IndentationError）。用于想要扩展的地方，但是现在还没有扩展。比如在某个method 下面或者某个 if 条件下。 yield or return经常被用来作为生成器。 when you call a normal function with a return statement the function is terminated whenever it encounters a return statement. In a function with a yield statement the state of the function is ‘saved’ from the last call and can be picked up the next time you call a generator function. for examples12345678910111213141516171819gen_exp =(x **2 for x in range(10) if x %2 ==0)for x in gen_exp: print(x)def my_gen(): for x in range(5): yield xgen1 =my_gen()next(gen1)def my_generator1(): yield 1 yield 2 yield 3 my_gen =my_generator1()# 使用 next() 进行调用下一个next(my_gen) recursionA function calling itself is known as recursion. list, tuples, or dictionary在python 中是使用频繁的data structure，这个是属于 collection 类别，里面放的是element. list: to add/update/ delete an item of a collection 123456my_list.append('C') #adds at the endmy_list[1] = 'D' #updatemy_list.pop(1) # removesmylist.pop() # 默认就是类似 栈的结构，就是pop 出来最后一个mylist.pop(0) # 当然也可以根据index 指定特定的 pop(delete) 的element or 12del mylist[1:2] # 通过指定 index range 然后进行delmylist.sort() # 支持 sorting 然后是从小到大, 这个sort是一种操作，inplace 的操作 tuples: tuples store a sequence of objects, the object can be of any type. Tuples are faster than lists. dictionary: It stores key/value pair objects. 12345678910111213141516171819202122232425262728293031 my_dict =dict() my_dict['key'] ='value' or my_dict =&#123;'key': 'value', ...&#125; for key in my_dict: # do something if 'some key' in my_dict: # do something``` ### Iterators###```pythonclass yrange: def __init__(self, n): self.i = 0 self.n = n # 这个表明是一个 iterator，make an object iterable def __iter__(self): return self # 这个next 函数就被当做是 class的属性，可以被外部调用的， def next(self): if self.i &lt; self.n: i = self.i self.i += 1 return i else: raise StopIteration() shallow vs deep copypython3 中：对于简单的数据类型，像int ，string，这种 copy() 和copy.deepcopy() 这两者都是相同的，copy 都是一种映射，都是相当于”值“ 上的引用；12345aa =2bb =aaprint(id(aa), id(bb)) # 相同bb =3print(id(aa), id(bb)) # 不同，因为把3 这个值重新复制给了变量bb 对于复杂的数据类型，使用deepcopy() 的时候，本来就是会重新拷贝一份到内存中。在python3 中copy() 和deepcopy() 这个是没有什么区别的。12345list1 =['a', 'b']list2 =list1 # 这个是引用，所以和list1 是相同的list3 =copy.copy(list1) # 这个id 和list1 不同list4 =copy.deepcopy(list1)# 这个id 和list1 不同 print(id(list1), id(list2), id(list3), id(list4)) object oriented design1234567891011121314151617181920212223242526272829303132333435363738394041424344454647 class ParentClass: def my_function(self): print 'I am here' class SubClass1(ParentClass): class SubClass2(ParentClass): ``` 对于多继承的支持 （接口）```python class A(B,C): #A implments B and C``` 如果想要call parent class function then you can dp:```python super(A, self).funcion_name()``` ### garbage collection###all the objects in python are stored in a heap space. Python has an in-built garbage collection mechanism. Memory management in Python involves a private heap containing all Python objects and data structures. The management of this private heap is ensured internally by the Python memory manager.In computer science, a heap is a specialized tree-based data structure which is essentially an almost complete[1] tree that satisfies the heap property: if P is a parent node of C, then the key(the value) of P is either greater than or equal to (in a max heap) or less than or equal to (in a min heap) the key of C.### try...catch###```python # raise exceptions try: raise TyeError except: print('exception') # catching exceptions try: do_something() except: print('exception') # try/ catch /finally try: do_something() except TypeError: print('exception') finally: close_connections() Advanced FeaturesLet’s move on to advanced features. Lambda functionsA Lambda Function is a small, anonymous function — anonymous in the sense that it doesn’t actually have a name. A lambda function can take any number of arguments, but must always have only one expression: 1234x = lambda a, b : a * b print(x(5, 6)) # prints '30' # 匿名函数也是函数，调用的时候使用这样的方式 x = lambda a : a*3 + 3 print(x(3)) # prints '12' MapsMap() is a built-in Python function used to apply a function to a sequence of elements like a list or dictionary. It’s a very clean and most importantly readable way to perform such an operation.相对于 lambda, map 使用的频率更少了。 最后返回的是一个list。 1234567891011def square_it_func(a): return a * ax = map(square_it_func, [1, 4, 7])print(x) # prints '[1, 16, 49]'def multiplier_func(a, b): return a * bx = map(multiplier_func, [1, 4, 7], [2, 5, 8])print(x) # prints '[2, 20, 56]' FilteringThe Filter built-in function is quite similar to the Map function in that it applies a function to a sequence (list, tuple, dictionary). The key difference is that filter() will only return the elements which the applied function returned as True. 123456789101112 # Our numbersnumbers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]# Function that filters out all numbers which are odddef filter_odd_numbers(num): if num % 2 == 0: return True else: return Falsefiltered_numbers = filter(filter_odd_numbers, numbers)print(filtered_numbers)# filtered_numbers = [2, 4, 6, 8, 10, 12, 14] 123456from itertools import *def check_for_drop(x): print ('Checking: ', x) return (x &gt; 5)for i in dropwhile(check_for_drop, [2, 4, 6, 8, 10, 12]): print ('Result: ', i) Itertools123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263from itertools import *# zip 就是一块访问的那种形式，返回的是一个tuple 数据类型# zip ,joing two lists into a list of tuples# Easy joining of two lists into a list of tuplesfor i in zip([1, 2, 3], ['a', 'b', 'c']): print (i)# ('a', 1)# ('b', 2)# ('c', 3)# 就是一个count() 计数功能# The count() function returns an interator that # produces consecutive integers, forever. This # one is great for adding indices next to your list # elements for readability and convenience# in python3, no need to import izip, use zip directly# 这个 count() 只有在这里才有意义，如果只是单独调用，没有感觉有多大的意义for i in zip(count(1), ['Bob', 'Emily', 'Joe']): print (i)# (1, 'Bob')# (2, 'Emily')# (3, 'Joe') # check ， becomes false for the first time 这个条件很关键, 可以理解成只是找到第一个false 的条件，然后就不再执行该函数# The dropwhile() function returns an iterator that returns # all the elements of the input which come after a certain # condition becomes false for the first time. def check_for_drop(x): print 'Checking: ', x return (x &gt; 5)for i in dropwhile(check_for_drop, [2, 4, 6, 8, 10, 12]): print 'Result: ', i# 注意理解这个输出# Checking: 2# Result: 2# Result: 4# Result: 6# Result: 8# Result: 10# Result: 12# 我的理解这个 groupby 就和数据库中的groupby 是相同的效果# The groupby() function is great for retrieving bunches# of iterator elements which are the same or have similar # propertiesfrom itertools import groupbythings = [("animal", "bear"), ("animal", "duck"), ("plant", "cactus"), ("vehicle", "speed boat"), ("vehicle", "school bus")]for key, group in groupby(things, lambda x: x[0]): for thing in group: print ("A %s is a %s." % (thing[1], key))#A bear is a animal.#A duck is a animal.#A cactus is a plant.#A speed boat is a vehicle.#A school bus is a vehicle. GeneratorGenerator functions allow you to declare a function that behaves like an iterator, i.e. it can be used in a for loop. This greatly simplifies your code and is much more memory efficient than a simple for loop. 12345678910111213141516numbers = list()# range()for i in range(1000): numbers.append(i + 1)total = sum(numbers)# (2) Using a generatordef generate_numbers(n): num = 0 while num &lt; n: yield num # 这个yield 之后，函数并没有结束，不像 return 那种函数 num += 1total = sum(generate_numbers(1000))print(total)total = sum(range(1000 + 1))print(total) List Comprehension常见的几种形式：(An iterable is something you can loop over) list comprehensions vs loops: list comprehensions are more efficient both computationally and coding space Every list comprehension can be rewritten as a for loop, but not every for loop can be rewritten as a list comprehension. 从优化的角度 list comprehensions是优于 for loop 中的if else 操作的。因为前者是 predicatable pattern 是可以预测的。However, keep in mind that list comprehensions are faster because they are optimized for the Python interpreter to spot a predictable pattern during looping. a small code demo:在于使用功能 timeit libary 进行函数的计时比较。12345678910111213import timeitdef squares(size): result = [] for number in range(size): result.append(number * number) return resultdef squares_comprehension(size): return [number * number for number in range(size)] print(timeit.timeit("squares(50)", "from __main__ import squares", number=1_000_000))print(timeit.timeit("squares_comprehension(50)", "from __main__ import squares_comprehension", number=1_000_000)) more complex list comprehensions: 这种if 的写法 是两个进行并列的。其实可以写成 123456numbers = [1, 2, 3, 4, 5, 6, 18, 20]squares = [number for number in numbers if number % 2 == 0 if number % 3 == 0]# orsquares = [number for number in numbers if number % 2 == 0 and number % 3 == 0]print(squares)# output: [6, 18] 在 output expression 中，也是可以使用 if else 进行进一步输出筛选。12345numbers = [1, 2, 3, 4, 5, 6, 18, 20] squares = ["small" if number &lt; 10 else "big" for number in numbers if number % 2 == 0 if number % 3 == 0] print(squares)ouput: ['small', 'big'] converting nested loops into list comprehension代码功能： 都是把二维的 matrix 转成了一个 list （flattened）123456matrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]flattened = []for row in matrix: for item in row: flattened.append(item)print(flattened) 注意这个顺序，先是row in matrix 然后是 item in row.123 matrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]flattened = [item for row in matrix for item in row] print(flattened) ouput matric from nested list comprehensions:12matrix = [[item for item in range(5)] for row in range(3)]print(matrix) 对于 dictionary 的支持： 主要是 dict1.items() 和 key, value 的使用123prices = &#123;"beer": 2, "fish": 5, "apple": 1&#125;float_prices = &#123;key:float(value) for key, value in prices.items()&#125;print(float_prices) 从代码的角度，可以看出，操作和最后的返回的形式是没有很大的关系，上面是 [], 这个是 {}, 分别对应的是 list 和 set 两种不同的格式。123numbers = [10, 10, 20, 30, 12, -20, 0, 1]unique_squares = &#123;number**2 for number in numbers&#125;print(unique_squares) python operatorsPython Arithmetic Operator Addition(+) Subtraction(-) Multiplication(*) Division(/) Exponentiation(**) Floor Division(//) 向下取整 Modulus(%) Python Relational Operator Less than(&lt;) Greater than(&gt;) Less than or equal to(&lt;=) Greater than or equal to(&gt;=) Equal to(= =) Not equal to(!=) Python Assignment Operator Assign(=) Add and Assign(+=) Subtract and Assign(-=) Divide and Assign(/=) Divide and Assign(/=) Modulus and Assign(%=) Exponent and Assign(**=) Floor-Divide and Assign(//=) Python Logical Operator(会有某种机制简化运算，比如 condition1 or condition2 ，如果condition1 是正确的，那么最后的结果就是正确的。)用于逻辑判断 and or not Python Membership Operator in ,not in Python Identity Operator is, is not , Python Bitwise Operator (这其中的 | 表现的是一种二级制’ 和’的 关系，如果在二进制下，0 | 1 那么就是1 ) 属于集合操作。 Binary AND(&amp;) Binary OR(|) Binary XOR(^) Binary XOR(^) Binary Left-Shift(&lt;&lt;) Binary Right-Shift(&gt;&gt;) Working with files Working with CSV, Json and XML Over the years, the list of possible formats that you can store your data in has grown significantly. But, there are 3 that dominate in their everyday usage: CSV, JSON, and XML. In this article, I’m going to share with you the easiest ways to work with these 3 popular data formats in Python! 有两种方式去读写 csv file：一种是 pd.read_csv() ，一种是built-in 的library 中的库函数之前一直使用的pd.read_csv(), 现在才发现python 有built-in 的library。We can do both read and write of a CSV using the built-in Python csv library. Usually, we’ll read the data into a list of lists. python in-built function.12345678910111213141516171819import csv filename = "my_data.csv"fields = [] rows = [] with open(filename, 'r') as csvfile: csvreader = csv.reader(csvfile) # 如果单单是这个for，那么内存是消耗比较大的 # fields = csvreader.next() for row in csvreader: rows.append(row)for row in rows[:5]: print(row)# Writing to csv file with open(filename, 'w+') as csvfile: csvwriter = csv.writer(csvfile) csvwriter.writerow(fields) csvwriter.writerows(rows) 12345678910111213141516171819202122import pandas as pdfrom dicttoxml import dicttoxmlimport json# Building our dataframedata = &#123;'Name': ['Emily', 'Katie', 'John', 'Mike'], 'Goals': [12, 8, 16, 3], 'Assists': [18, 24, 9, 14], 'Shots': [112, 96, 101, 82] &#125;df = pd.DataFrame(data, columns=data.keys())# Converting the dataframe to a dictionary# Then save it to filedata_dict = df.to_dict(orient="records")with open('output.json', "w+") as f: json.dump(data_dict, f, indent=4)# Converting the dataframe to XML# Then save it to filexml_data = dicttoxml(data_dict).decode()with open("output.xml", "w+") as f: f.write(xml_data) 12345678910111213141516171819import jsonimport pandas as pd# Read the data from file# We now have a Python dictionarywith open('data.json') as f: data_listofdict = json.load(f) # We can do the same thing with pandasdata_df = pd.read_json('data.json', orient='records')# We can write a dictionary to JSON like so# Use 'indent' and 'sort_keys' to make the JSON# file look nicewith open('new_data.json', 'w+') as json_file: json.dump(data_listofdict, json_file, indent=4, sort_keys=True)# And again the same thing with pandasexport = data_df.to_json('new_data.json', orient='records') text filepython 中 open( mode =’rt’) 的选项：w,r,wt,rt都是python里面文件操作的模式。w是写模式，r是读模式。t是windows平台特有的所谓text mode(文本模式）,区别在于会自动识别windows平台的换行符。类Unix平台的换行符是\n，而windows平台用的是\r\n两个ASCII字符来表示换行，python内部采用的是\n来表示换行符。rt模式下，python在读取文本时会自动把\r\n转换成\n.wt模式下，Python写文件时会用\r\n来表示换行。 参考资料：https://towardsdatascience.com/the-easy-way-to-work-with-csv-json-and-xml-in-python-5056f9325ca9]]></content>
      <categories>
        <category>Python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[The Evaluation of Sentence Similarity]]></title>
    <url>%2F2019%2F04%2F06%2FThe-evaluation-of-sentence-similarity%2F</url>
    <content type="text"><![CDATA[I am trying to write my first english blog based on two reasons: First, the data set used in this blog is english; Second, I’d like to expand my reach and attract more audiences, although I should admit that nobody cares. DataInitially I want to use chinese corpus, but I cannot find a proper one. The data should sound like this one: word1 word2 similarity score阿拉伯人 阿拉伯 7.2畜产 农业 5.6垂涎 崇敬 3.4次序 秩序 4.7定心丸 药品 4.3房租 价格 5.2翡翠 宝石 6.7高科技 技术 7.5购入 购买 8.5观音 菩萨 8.2归并 合并 7.7 not like this: 为何我无法申请开通花呗信用卡收款 支付宝开通信用卡花呗收款不符合条件怎么回事 1花呗分期付款会影响使用吗 花呗分期有什么影响吗 0为什么我花呗没有临时额度 花呗没有临时额度怎么可以负 0能不能开花呗老兄 花呗逾期了还能开通 0我的怎么开通花呗收钱 这个花呗是个什么啥？我没开通 我怎么有账单 0蚂蚁借呗可以停掉么 蚂蚁借呗为什么给我关掉了 0我想把花呗功能关了 我去饭店吃饭，能用花呗支付吗 0为什么我借呗开通了又关闭了 为什么借呗存在风险 0支付宝被冻了花呗要怎么还 支付功能冻结了，花呗还不了怎么办 1 If you can find the dataset where ‘similarity score’ is double, please donot hesitate to email me. So, the choice has to be enlgish corpus. The dataset used in this experiment are STSbenchmark and SICK data. The SICK data contains 10,000 sentence paris labeled with semantic relatedness and entailment relation. Similarity MethodsBaselineAs the baseline, we just take the embedding of the words in sentence, and compute the average, weighted by frequency of each word. 1234567891011121314151617181920212223242526272829303132def run_avg_benchmark(sentences1, sentences2, model=None, use_stoplist=False, doc_freqs=None): if doc_freqs is not None: N = doc_freqs["NUM_DOCS"] sims = [] for (sent1, sent2) in zip(sentences1, sentences2): tokens1 = sent1.tokens_without_stop if use_stoplist else sent1.tokens tokens2 = sent2.tokens_without_stop if use_stoplist else sent2.tokens tokens1 = [token for token in tokens1 if token in model] tokens2 = [token for token in tokens2 if token in model] if len(tokens1) == 0 or len(tokens2) == 0: sims.append(0) continue tokfreqs1 = Counter(tokens1) tokfreqs2 = Counter(tokens2) weights1 = [tokfreqs1[token] * math.log(N / (doc_freqs.get(token, 0) + 1)) for token in tokfreqs1] if doc_freqs else None weights2 = [tokfreqs2[token] * math.log(N / (doc_freqs.get(token, 0) + 1)) for token in tokfreqs2] if doc_freqs else None embedding1 = np.average([model[token] for token in tokfreqs1], axis=0, weights=weights1).reshape(1, -1) embedding2 = np.average([model[token] for token in tokfreqs2], axis=0, weights=weights2).reshape(1, -1) sim = cosine_similarity(embedding1, embedding2)[0][0] sims.append(sim) return sims Smooth Inverse FrequencyThe baseline, like we did before, is very simple and crude of computing sentence embedding. Word frequency cannot reliably reflect its importance to sentence, semantically speaking. Smooth Inverse Frequency (SIF) tries to solve this problem. SIF is very similar to the weighted average we used before, with the difference that it’s weighted by this formular. $$\operatorname { SIF } ( w ) = \frac { a } { ( a + p ( w ) )}$$where $a$ is a hyper-parameter (set to 0.001 by default) and $ p(w)$ is the estimated word frequency in the corpus. we need to perform common component removal: subtract from the sentence embedding obtained above the first principal component of the matrix. This corrects for the influence of high-frequency words that have syntactic or dicourse function, such as ‘but’, ‘and’, etc. You can find more information from this paper. 12345678910111213141516171819202122232425262728293031323334353637def remove_first_principal_component(X): svd = TruncatedSVD(n_components=1, n_iter=7, random_state=0) svd.fit(X) pc = svd.components_ XX = X - X.dot(pc.transpose()) * pc return XXdef run_sif_benchmark(sentences1, sentences2, model, freqs=&#123;&#125;, use_stoplist=False, a=0.001): total_freq = sum(freqs.values()) embeddings = [] # SIF requires us to first collect all sentence embeddings and then perform # common component analysis. for (sent1, sent2) in zip(sentences1, sentences2): tokens1 = sent1.tokens_without_stop if use_stoplist else sent1.tokens tokens2 = sent2.tokens_without_stop if use_stoplist else sent2.tokens tokens1 = [token for token in tokens1 if token in model] tokens2 = [token for token in tokens2 if token in model] weights1 = [a / (a + freqs.get(token, 0) / total_freq) for token in tokens1] weights2 = [a / (a + freqs.get(token, 0) / total_freq) for token in tokens2] embedding1 = np.average([model[token] for token in tokens1], axis=0, weights=weights1) embedding2 = np.average([model[token] for token in tokens2], axis=0, weights=weights2) embeddings.append(embedding1) embeddings.append(embedding2) embeddings = remove_first_principal_component(np.array(embeddings)) sims = [cosine_similarity(embeddings[idx * 2].reshape(1, -1), embeddings[idx * 2 + 1].reshape(1, -1))[0][0] for idx in range(int(len(embeddings) / 2))] return sims Google Sentence EncoderInferSent is a pre-trained encoder that produces sentence embedding, which opensourced by Facebook. The Google Sentence Encoder is Google’s answer to Facebook’s InferSent. In contrast to InferSent, the Google Sentence Encoder was trained on a combination of unsupervised data and supervised data (SNLI corpus), which tends to give better results. The codes can be used in Google Jupyter Notebook 12345678910111213141516171819202122232425import tensorflow_hub as hubtf.logging.set_verbosity(tf.logging.ERROR)embed = hub.Module("https://tfhub.dev/google/universal-sentence-encoder/1")def run_gse_benchmark(sentences1, sentences2): sts_input1 = tf.placeholder(tf.string, shape=(None)) sts_input2 = tf.placeholder(tf.string, shape=(None)) sts_encode1 = tf.nn.l2_normalize(embed(sts_input1)) sts_encode2 = tf.nn.l2_normalize(embed(sts_input2)) sim_scores = tf.reduce_sum(tf.multiply(sts_encode1, sts_encode2), axis=1) with tf.Session() as session: session.run(tf.global_variables_initializer()) session.run(tf.tables_initializer()) [gse_sims] = session.run( [sim_scores], feed_dict=&#123; sts_input1: [sent1.raw for sent1 in sentences1], sts_input2: [sent2.raw for sent2 in sentences2] &#125;) return gse_sims Experiments1234567891011121314def run_experiment(df, benchmarks): sentences1 = [Sentence(s) for s in df['sent_1']] sentences2 = [Sentence(s) for s in df['sent_2']] pearson_cors, spearman_cors = [], [] for label, method in benchmarks: sims = method(sentences1, sentences2) pearson_correlation = scipy.stats.pearsonr(sims, df['sim'])[0] print(label, pearson_correlation) pearson_cors.append(pearson_correlation) spearman_correlation = scipy.stats.spearmanr(sims, df['sim'])[0] spearman_cors.append(spearman_correlation) return pearson_cors, spearman_cors Helper function: 1234567891011import functools as ftbenchmarks = [ ("AVG-GLOVE", ft.partial(run_avg_benchmark, model=glove, use_stoplist=False)), ("AVG-GLOVE-STOP", ft.partial(run_avg_benchmark, model=glove, use_stoplist=True)), ("AVG-GLOVE-TFIDF", ft.partial(run_avg_benchmark, model=glove, use_stoplist=False, doc_freqs=doc_frequencies)), ("AVG-GLOVE-TFIDF-STOP", ft.partial(run_avg_benchmark, model=glove, use_stoplist=True, doc_freqs=doc_frequencies)), ("SIF-W2V", ft.partial(run_sif_benchmark, freqs=frequencies, model=word2vec, use_stoplist=False)), ("SIF-GLOVE", ft.partial(run_sif_benchmark, freqs=frequencies, model=glove, use_stoplist=False)),] Results123import matplotlib.pyplot as pltplt.rcParams['figure.figsize'] = (20,13)spearman[['AVG-GLOVE', 'AVG-GLOVE-STOP','AVG-GLOVE-TFIDF', 'AVG-GLOVE-TFIDF-STOP','GSE']].plot(kind="bar").legend(loc="lower left") Take Off Smooth Inverse Frequency methods are better than baseline, no matter with word2vec or Glove embeddings. Google Sentence Encoder has the similar performance as Smooth Inverse Frequency. Using tf-idf weights does not help and using a stoplist looks like a reasonable choice. Pearson CorrelationSpearman Correlation Full codes can be found in here.]]></content>
      <categories>
        <category>NLP</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[深度网络中的小概念]]></title>
    <url>%2F2019%2F03%2F26%2F%E6%B7%B1%E5%BA%A6%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E7%A2%8E%E7%A2%8E%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[介绍深度网络中小的基本概念，比如权重初始化、激活函数和优化器。 Weights Initializationweights 的初始化在网络的训练起到重要的作用，初始化的好坏能够直接影响到网络是否可以正常收敛。这里的初始化都是指的是weights初始化。bias 表示偏差，噪声，作用在于企图去描述真实的分布（高斯分布），通过引入随机性来表示这个是具有推广性的。主要介绍常见的三种初始化方法和选择方法。 Here’s another trick — before squishing our scalar value (called an activation) into the sigmoid function, we can add a little number called the bias, b. There will be two biases, one for each user and one for each movie. Random Initialization总结来说就是容易出现梯度消失和梯度爆炸，尤其是在layer_size（特征数量） 比较大的时候。从均值方差的角度进行分析。 a) If weights are initialized with very high values the term np.dot(W,X)+becomes significantly higher and if an activation function like sigmoid() is applied, the function maps its value near to 1 where slope of gradient changes slowly and learning takes a lot of time.b) If weights are initialized with low values it gets mapped to 0, where the case is same as above. 1w =np.random.randn(layer_size[l],layer_size[l-1]) 另外的表述方式： If the weights start too small, then the signal shrinks as it passes through each layer until it’s too small to be useful.If the weights start too large, then the signal grows as it passes through each layer until it’s too massive to be useful (big value in sigmoid function). Xavier initialization /ˈzeɪvjər/sFor deep networks, we can use a heuristic to initialize the weights depending on the non-linear activation function. This applies to Xavier and He initialization. Xavier/Glorot Initialization initializes the weights in your network by drawing them from a distribution with zero mean and a specific variance.$$ { var } ( w _ { i } ) = \frac { 1 } { layer_{l-1}}$$ 1w=np.random.randn(layer_size[l],layer_size[l-1])*np.sqrt(1/layer_size[l-1]) In practice, it works better for layers with sigmoid or tanh function. 总的思想原则：They set the weights neither too much bigger that 1, nor too much less than 1.就是本来就是在 (0,1) 标准正太分布出来，然后进行了进一步的约束条件。思想当特征数量越大的时候，weights 的波动情况是成反比的，最后的weights 数值越接近于均值附近。 He InitializationUsing RELU or Leaky RELU is relatively robust to the vanishing/ exploding gradient issues compared with sigmoid function especially for networks that are not too deep. And it the case of Leaky RELU, it never has zero gradients. For RELU, we multiply the randomly generated values of $w$ by: $$\sqrt { \frac { 2 } { layer _ { [ l - 1 ] } } }$$1w=np.random.randn(layer_size[l],layer_size[l-1])*np.sqrt(2/layer_size[l-1]) Sometimes, we combine the idea of Xavier initialization and He initializaiton so the variance becomes the following: $$\sqrt { \frac { 2 } { layer _ { [ l - 1 ] } + \operatorname { layer } _ { [ l ] } } }$$ 12w=np.random.randn(layer_size[l],layer_size[l-1])*np.sqrt(2/(layer_size[l-1]+layer_size[l]))# 代码中 random() 中的两个参数是 shape，最后的np.sqrt 标准差 The idea behind this is that we set the weights neither too much bigger than 1 nor too much less than 1 so the gradients do not vanish or explode too quickly. 所以上述的初始化从数学的角度去理解： 从(0,1) 标准正太分布 转换成了 (0, np.sqrt(2/ size_l -1)) 这样的分布，就是你网络结构是越宽，那么这个方差就是越小的，最后的结果是越集中的，就越集中的 均值 u 左右。从图像的角度看，方差越大，图像越矮胖；方差越小，图像越瘦高。 Takeoff In summary, the main difference in machine learning is the following: He initialization works better for layers with ReLu(s) activation. Xavier initialization works better for layers with sigmoid activation. Activation function总的说可以分为线性和非线性的激活函数， activation function 的作用 就是对于网络的输出 说yes or no. It maps the resulting values in between 0 to 1 or -1 to 1 etc. Sigmoid function (Logistic Activation)the only reason why we use sigmoid is because it exists between 0 to 1. 这个非常有利于 predict probability. 因为自然映射到 0 是不存在 然后1 是存在。而当多分类的时候，使用softmax。 Tanh function The advantage is that the negative inputs will be mapped strongly negative and the zero inputs will be mapped near zero in the tanh graph. Relu (Rectified Linear Unit) Activation本质上是分段函数。 range: [0, infinity]. The function and its derivative both are monotonic. Leaky Relu每当一个伟大的东西产出，总会伴随着比较明显的错误，然后紧跟着就是有一个 rectified(improved) 这种版本。这个相比之前就是修正了 当输入是负数的时候，怎么办的问题。 Softmax这个需要从数学的角度去理解，数学公式就是下面。有一个真实的例子： 如果一张”4” 的图片，输出一个网络，最后是softmax 激活函数，那么最后得到的 “4” 的概率是最大的，但是也是有其他的可能性存在的，这个就是softmax 的效果。 最主要的功能是 convert number into probabilities. 这种效果，不像sigmoid 那样有很明确的数学公式。 $$\sigma ( z ) _ { j } = \frac { e ^ { z _ { j } } } { \sum _ { k = 1 } ^ { K } e ^ { z _ { k } } }$$ Softmax function, a wonderfulactivation function that turns numbers aka logits into probabilities that sum to one.Remember the takeaway is: the essential goal of softmax is to turn numbers into probabilities. OptimizerGradient Descent最基本的应该是 gradient descent，这个算是开山鼻祖。给出了每次梯度变化的方向 Momentum个人更喜欢将其翻译成动量，这个gradient 考虑之前的gradient 的方向和大小，有点像动量的的含义。不仅考虑了current step, 而且accumulates gradient of the past steps. A very popular technique that is used along with SGD is called Momentum. Instead of using only the gradient of the current step to guide the search, momentum also accumulates the gradient of the past steps to determine the direction to go. 好处在于: most recent is weighted than the less recent onesthe weightage of the most recent previous gradients is more than the less recent ones.for example: RMSpropRMSprop, or Root Mean Square Propogation has an interesting history. RMSProp also tries to dampen the oscillations, but in a different way than momentum. Adam这种套路总是一样的，总是有一个中和者，关键是如何整合到一起。 Adam or Adaptive Moment Optimization algorithms combines the heuristics of both Momentum and RMSProp. The hyperparameter beta1 is generally kept around 0.9 while beta_2 is kept at 0.99. Epsilon is chosen to be 1e-10 generally. 对于公式的解释，Eq 1 and Eq 2是come from RMSprop, Eq 3 comes from Adam。最后一步是 update 操作，条理清楚，没有问题。(公式中的v 表示导数，g 表示导数e 不太清楚)Here, we compute the exponential average of the gradient as well as the squares of the gradient for each parameters (Eq 1, and Eq 2). To decide our learning step, we multiply our learning rate by average of the gradient (as was the case with momentum) and divide it by the root mean square of the exponential average of square of gradients (as was the case with momentum) in equation 3. Then, we add the update.) 卷积网络一个卷积神经网络由若干卷积层、Pooling层、全连接层组成。 卷积层$Input \rightarrow [Conv] \times N \rightarrow [Pool] \times M \rightarrow [FK] \times K$ 下面的动画显示了包含两个filter的卷积层的计算。我们可以看到 $7 \times 7 \times 3$ 输入，经过两个$3 \times 3 \times 3 $filter的卷积(步幅为2)，得到了$ 3 \times 3 \times 2 $的输出。另外我们也会看到下图的Zero padding是1，也就是在输入元素的周围补了一圈0。Zero padding对于图像边缘部分的特征提取是很有帮助的。 对于包含两个333的fitler的卷积层来说，其参数数量仅有 $(3 \times 3 \times3+1) \times 2 =56 $个，且参数数量与上一层神经元个数无关。与全连接神经网络相比，其参数数量大大减少了。 卷积层输出大小计算：输入大小： $W 1 \times H 1 \times D 1$超参数（filter信息 +是否填充）： filter 个数( K), filter 大小( F), 步长 (S )，边界填充( P)输出：$$\begin{split}W _ { 2 } &amp; = \left( W _ { 1 } - F + 2 P \right) / S + 1 \\H _ { 2 } &amp;= \left( H _ { 1 } - F + 2 P \right) / S + 1 \\D _ { 2 } &amp;= K\end{split}$$ 卷积层参数量计算： 这里有一个重要的概念叫做权值共享，给定一张图片，用filter 去扫描这图片，filter 里面的数叫权重，这张图是被同样的filter 扫描的，权值是一样的，所以叫做权值共享。这个概念是和全连接层中的权值进行比较的，简单来说就是降低了权重的使用。权重共享即 filter 值的共享。对于 三维图片来说，每个filter需要FFD1个权重值，总共K个filter，需要FFD1*K权重值。和一维一样，整个滑动过程中filter W0和W1值保持不变，可称作权值共享。而且，补充一句，对于三维的input，权值只是在input的每个depth slice上共享的。对于一层的 filter 只是有一个bias。 for example:Filter个数：32原始图像shape：$224 \times 224 \times 3$卷积核大小为：$2 \times 2$一个卷积核的参数：$ 2 \times 2 \times 3=12 $16个卷积核的参数总额：$ 16 \times 12 + 16 =192 + 16 = 208 $$ weight \times x + bias $根据这个公式，即可算的最终的参数总额为：208 Pooling 层Pooling层主要的作用是下采样，主要有两点作用，一个是提取重要特征，一个是简化网络的计算。Pooling的方法很多，最常用的是Max Pooling。Max Pooling实际上就是在n*n的样本中取最大值，作为采样后的样本值。下图是 max pooling： 除了Max Pooing之外，常用的还有Mean Pooling——取各样本的平均值。 池化层往往在卷积层后面，通过池化来降低卷积层输出的特征向量，同时改善结果（不易出现过拟合）。 池化层参数个数计算： 这个很明显是没有参数的。 全连接层连接所有的特征，将输出值送给分类器（如softmax分类器） 比如说上一层（池化层）的输出为：$(111 \times 111 \times 16) $，从第一层到第二层，只是图片大小发生了变化，深度没有发生变化，而Dense对应的神经元个数为133个，那么还是根据公式：$weight \times x + bias$，计算得：$133 \times16+133=2261$]]></content>
      <categories>
        <category>Deep learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[siamese network]]></title>
    <url>%2F2019%2F03%2F26%2Fsiamese-network%2F</url>
    <content type="text"><![CDATA[主要是介绍自己论文中的网络结构：siamese network。 但凡优化，无非两条路：在好的基础上更快，在快的基础上效果更好。 Siamese network训练速度快，所以只是需要其在训练效果上更好。 先来分析一下为什么训练速度快。那么不得不分析该网络结构。整个网络的输入是 (img1, img2, y) 这样的三元组，img 表示图片，y表示label。图片可以是同一类别的图片，也可以是不同类别的，y表示两张图片之间的相似程度，y的取值 (0,1)，0表示相似（同一类别），1 表示不相似（不同类别）。并且y 是double 类型，属于0-1 区间任意的数字。网路结构最后的输出是 0-1 区间的任意数字。通常是以0.5 作为分界线，如果小于0.5 那么认为两种图片是属于同一类别，或者说更相似；反之也成立。重要的一点是中间的weights 实现了权值共享，这样可以加快网络的训练速度。 loss function这个是属于经典的 contrastive loss function。当y 接近于0的时候，右半部分消失，这个是表示两张图片很是相似，然后就不断使得 欧氏距离减少；当y 接近于1的时候，左半部分消失，这个时候两张图片很不相似，然后右边就是 hinge loss。参数m 作为一种margin 是是可以调节，我的实验中 m 取1.总的思想：就是使得相近的图像距离相近，不想近的图像距离变远。 $L ( W , ( Y , X _ { 1 } , X _ { 2 } ) ) = ( 1 - Y ) \frac { 1 } { 2 } ( D _ { W } ) ^ { 2 } + ( Y ) \frac { 1 } { 2 } { \max ( 0 , m - D _ { W } ) } ^ { 2 }$ Spectral Normalization图像输入到网络之前使用正则化，然后输入到激活函数之前也是使用正则化，所以这种效果也是扩展到 weights，直接对 weights 进行正则化使其符合 Lipschitz 约束，避免使用大的gradients。在GAN 网络中的 discriminator 或者 generator 都发现了其可以稳定训练的过程。在实验中，我们扩大了了这种使用范围，把其应用到所有的网络的layer上。 self-attention mechanismAttention 机制自从 “Attention Is All You Need” 开始火爆，并且实验的效果也是很好的，然后在图像领域也开始尝试使用 attention 机制来解决长依赖的问题。应用到图像领域主要是 explore spatial locality information, 说白了就是细节的信息。 If we look at the DCGAN model, we see that regular GANs are heavily based on convolution operations, which use a local receptive field (convolutional kernel) to learn representations. Simple features like edges and corners are learned in the first few layers. Also, ConvNets are able to use these simple representations to learn more complex ones. However, long-range dependency might be hard to learn. Long-range dependency (long-term dependency) is from RNN, which we can say anything larger than trigram as a long term dependency. Thus, most of the image content does not exhibit elaborated shape such as sky or the ocean looks fine. The task of creating geometrically complex forms, such as four-legged animals, is far more challenging. This is where attention comes into play. 而 self-attention 中QKV 三个部分是相同的，对于这种处理方法和Res_block 还是有点相似的。 结果训练数据集使用是 Cifar-10，记录了训练过程中 acc 和loss 的变化情况。除了训练的效果比较好外，训练速度也是非常快的，可以清楚的看到model acc 在接近25 epoches的时候就开始收敛。]]></content>
      <categories>
        <category>Deep learning</category>
      </categories>
      <tags>
        <tag>siamese network</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[fastText & faiss]]></title>
    <url>%2F2019%2F03%2F25%2FfastText-faiss%2F</url>
    <content type="text"><![CDATA[fastTextfastText结合了自然语言处理和机器学习中最成功的理念。这些包括了使用词袋以及n-gram袋表征语句，还有使用子字(subword)信息，并通过隐藏表征在类别间共享信息。另外采用了一个softmax层级(利用了类别不均衡分布的优势)来加速运算过程。 fastText 主要是用来解决 word representations和 sentence classification. 有趣的是前者是无监督的学习方式，后者是有监督的学习方式。分别主要来自 ”Enriching Word Vectors with Subword Information“ 和 “Bag of Tricks for Efficient Text Classification” 两篇论文。并且使用的是 shallow neural network 而不是深度网络。 FastText is a library created by the Facebook Research Team for efficient learning of word representations and sentence classification. Take off:fastText 方法包含三部分：模型架构、层次 Softmax 和 N-gram 特征。fasttext 有两个用处： text classification 和 word embedding 。使用场景：大型数据，高效计算 下面进行细说： 模型架构这个是总的框架图。 抱歉哈 这个引用找不见了，如果有侵权，please email me..分为两个部分介绍这个网络结构：从input -&gt; hidden:输入层输入的是一个已经分词后短文本。短文本中每个词的词向量是由该短文本的one-hot矩阵乘以一个初始化的矩阵w得到的。（原理图：下图是fasttext 运行的时候，这个分词是再处理成单词和n-gram 组成的特征，这个是不需要我们进行显性的操作的）从 hidden -&gt; output：插播一句，我们经常使用的预训练模型中的weights 是从input-&gt; hidden。 Hierarchical Softmax从名字上就知道这个是基于softmax的改进版本，主要是运算上的改进。具体说来：利用了类别（class）不均衡这个事实（一些类别出现次数比其他的更多），通过使用 Huffman 算法建立用于表征类别的树形结构。fastText 模型使用了层次 Softmax 技巧。层次 Softmax 技巧建立在Huffman的基础上，对标签进行编码，能够极大地缩小模型预测目标的数量。 这个是softmax 的原始的计算公式：采用二叉树的结构之后，时间上优化不少。$O ( N) \rightarrow O \left( \log _ { 2 } N \right)$。见下图。 抱歉哈 这个引用找不见了，如果有侵权，please email me..和之前的神经网络模型相比，这里的huffmax树的所有内部节点就类似之前神经网络隐藏层的神经元。其中，根节点的词向量对应我们的投影后的词向量，而所有的叶子节点就类似于之前神经网络softmax输出层的神经元。叶子节点的个数就是词汇表的大小. 和之前的相比，从隐藏层到输出层的softmax映射不是一下就完成的，而是沿着 huffman树一步步完成的，因此这种 softmax取名为”Hierarchical softmax”. N-gram 特征N-gram是基于这样的思想：某个词的出现依赖于其他若干个词；我们获得的信息越多，预测越准确。我想说，我们每个人的大脑中都有一个N-gram模型，而且是在不断完善和训练的。我们的见识与经历，都在丰富着我们的阅历，增强着我们的联想能力。N-gram 是一种思想，可以有两种level 的实现，一种是基于 character-level，一种是基于 word level，前者是扩充了对于”不常见“单词，后者是考虑了部分的词的顺序，都是考虑了”周边“ 信息。所以比较难界定 fasttext 训练出来的是不是有比较强的词序。 N-gram模型是一种语言模型（Language Model，LM），语言模型是一个基于概率的判别模型，它的输入是一句话（单词的顺序序列），输出是这句话的概率，即这些单词的联合概率（joint probability）。 这样的作用，使用N-gram来给文本添加额外的特征获得关于局部词顺序的部分信息。举个栗子：对于句子：“我 喜欢 喝 咖啡”, 如果不考虑顺序，那么就是每个词，“我”，“喜欢”，“喝”，“咖啡”这五个单词的word embedding求平均。如果考虑2-gram, 那么除了以上五个词，还有“我喜欢”，“喜欢喝”，“喝咖啡”等词。“我喜欢”，“喜欢喝”，“喝咖啡”这三个词就作为这个句子的文本特征。我们经常见到的场景：输入法的预选词汇。就是可以通过这种方式实现的。 当然使用了更多的特征意味着造成了效率下降，于是该作者提出了两种解决方法：过滤掉低词频；使用词粒度代替字粒度。比如说海慧寺使用上面那个句子”我喜欢喝咖啡“，如果使用子粒度的2-gram，那么产生的特征是“我喜”，“喜欢”，“欢喝”，“喝咖”，“咖啡”。如果使用词粒度为2-gram，那么产生的特征是“我喜欢”，“喜欢喝”，“喝咖啡”。 补充一句，subwords就是一个词的character-level的n-gram。比如单词”hello”，长度至少为3的char-level的ngram有”hel”,”ell”,”llo”,”hell”,”ello”以及本身”hello”。n-gram 是一种思想，可以是针对words 之间的，也是可以针对一个word 内部的。前者就是候选词，哪些词语容易组合出现，后者主要是对于单词本身的伸展，可以把没有在dict 中的单词，使用字词（subword） 进行表示，扩充了模型的表达能力。 Negative Sampling但凡效果优化提升，要么是基于计算上的，要么是基于最后效果上的。 主要是减轻计算量的角度考虑的，每次让一个训练样本仅仅更新一部分的权重参数，这个技术不是 fastText 首创的，但是本着总结知识点的，也就放在这里了。CBOW / Skip-gram模型 （这个论文中）提出了两种方法，一种是Hierarchical Softmax，另一种是Negative Sampling。论文中提出的两种方法都是用来提高计算效率的，下面说一下负采样。在训练神经网络时，每当接受一个训练样本，然后调整所有神经单元权重参数，来使神经网络预测更加准确。换句话说，每个训练样本都将会调整所有神经网络中的参数。而 Negative Sampling 每次让一个训练样本仅仅更新一小部分的权重参数，从而降低梯度下降过程中的计算量。如果 vocabulary 大小为1万时， 当输入样本 ( “fox”, “quick”) 到神经网络时， “ fox” 经过 one-hot 编码，在输出层我们期望对应 “quick” 单词的那个神经元结点输出 1，其余 9999 个都应该输出0。在这里，这9999个我们期望输出为0的神经元结点所对应的单词我们称为 negative word，随机选择一小部分的 negative words，比如选 10个 negative words 来更新对应的权重参数。解决的问题，在最后一层 softmax 的计算量太大，相当于每一次word 都是需要整个dict 量的级别的更新。然后选择 k 个negative words，只是计算这些softmax 的值。 Training a neural network means taking a training example and adjusting all of the neuron weights slightly so that it predicts that training sample more accurately. In other words, each training sample will tweak all of the weights in the neural network.As we discussed above, the size of our word vocabulary means that our skip-gram neural network has a tremendous number of weights, all of which would be updated slightly by every one of our billions of training samples!Negative sampling addresses this by having each training sample only modify a small percentage of the weights, rather than all of them. Here’s how it works.When training the network on the word pair (“fox”, “quick”), recall that the “label” or “correct output” of the network is a one-hot vector. That is, for the output neuron corresponding to “quick” to output a 1, and for all of the other thousands of output neurons to output a 0.With negative sampling, we are instead going to randomly select just a small number of “negative” words (let’s say 5) to update the weights for. (In this context, a “negative” word is one for which we want the network to output a 0 for). We will also still update the weights for our “positive” word (which is the word “quick” in our current example).The paper says that selecting 5-20 words works well for smaller datasets, and you can get away with only 2-5 words for large datasets.Recall that the output layer of our model has a weight matrix that’s 300 x 10,000. So we will just be updating the weights for our positive word (“quick”), plus the weights for 5 other words that we want to output 0. That’s a total of 6 output neurons, and 1,800 weight values total. That’s only 0.06% of the 3M weights in the output layer!In the hidden layer, only the weights for the input word are updated (this is true whether you’re using Negative Sampling or not). Positive samples and Negative samplesOne little detail that’s missing from the description above is how do we select the negative samples.（下面说的是如何进行选择negative sample的问题：基本思路是根据出现频率进行选择）The negative samples are chosen using the unigram distribution. Essentially, the probability of selecting a word as a negative sample is related to its frequency, with more frequent words being more likely to be selected as negative samples. Instead of using the raw frequency in the original word2vec paper, each word is given a weight that’s equal to it’s frequency (word count) raised to the 3/4 power. The probability for selecting a word is just it’s weight divided by the sum of weights for all words.$$P \left( w _ { i } \right) = \frac { f \left( w _ { i } \right) ^ { 3 / 4 } } { \sum _ { i = 0 } ^ { n } \left( f \left( w _ { j } \right) ^ { 3 / 4 } \right) }$$This decision to raise the frequency to the 3/4 power appears to be empirical; as the author claims it outperformed other functions (e.g. just using unigram distribution).Side note: The way this selection is implemented in the original word2vec C code is interesting. They have a large array with 100M elements (which they refer to as the unigram table). They fill this table with the index of each word in the vocabulary multiple times, and the number of times a word’s index appears in the table is given by Then, to actually select a negative sample, we just generate a random integer between 0 and 100M, and use the word at that index in the table. Since the higher probability words occur more times in the table, we’re more likely to pick those. 这个也是有讲 任何进行negative sample的选择http://jalammar.github.io/illustrated-word2vec/一般来说在 word2vec 中context 是会选择到 5，然后这个 positive / negative sample 会是(1/6), 然后 nagative sample 是随机在 dictionary里面选的（所以有可能选到 positive sample）， 这个是这个dictionary 是根据频率，出现次数越多的，被选中的可能性也越大。The number of negative samples is another factor of the training process. The original paper prescribes 5-20 as being a good number of negative samples. It also states that 2-5 seems to be enough when you have a large enough dataset. The Gensim default is 5 negative samples. To address this, we need to introduce negative samples to our dataset – samples of words that are not neighbors. Our model needs to return 0 for those samples. Now that’s a challenge that the model has to work hard to solve – but still at blazing fast speed.This idea is inspired by Noise-contrastive estimation. We are contrasting the actual signal (positive examples of neighboring words) with noise (randomly selected words that are not neighbors). This leads to a great tradeoff of computational and statistical efficiency. Essentially, the probability for selecting a word as a negative sample is related to its frequency, with more frequent words being more likely to be selected as negative samples. They fill this table with the index of each word in the vocabulary multiple times, and the number of times a word’s index appears in the table is given by $P(wi)*P(wi)$ table_size. Then, to actually select a negative sample, you just generate a random integer between 0 and 100M, and use the word at that index in the table. Since the higher probability words occur more times in the table, you’re more likely to pick those. (ps 这种数量比不是 1：1，常常是 positive ： negative =1：5， 这个是经验值，在传统机器学习中可能认为是 data unbalanced)It’s now time to build out our skip-gram generator which will give us pair of words and their relevance (word, word in the same window), with label 1 (positive samples). (word, random word from the vocabulary), with label 0 (negative samples). 使用第一个应用场景：词向量。fastText作为训练词向量认为可以有两种模式，一种是根据周围词语预测中心词汇的CBOW （continuous bag-of-words）模型，另一种是根据中心词汇预测上下文的 skip-gram 模型。 ./fasttext – It is used to invoke the FastText library. skipgram/cbow – It is where you specify whether skipgram or cbow is to be used to create the word representations. -input – This is the name of the parameter which specifies the following word to be used as the name of the file used for training. This argument should be used as is. data.txt – a sample text file over which we wish to train the skipgram or cbow model. Change this name to the name of the text file you have. -output – This is the name of the parameter which specifies the following word to be used as the name of the model being created. This argument is to be used as is. model – This is the name of the model created.Running the above command will create two files named model.bin and model.vec. model.bin contains the model parameters, dictionary and the hyperparameters and can be used to compute word vectors. model.vec is a text file that contains the word vectors for one word per line. 最后生成有两个文件，一个 xxx.bin 文件，一个是 xxx.vec 文件，前者是预训练模型，后者是词向量。这两个可能是最重要的格式了。 The most important parameters of the model are its dimension and the range of size for the subwords. 常见的代码格式： ./fasttext skipgram -input data/fil9 -output result/fil9 -minn 2 -maxn 5 -dim 300 跑偏一下说一下shell的小技巧。使用echo 或者 &lt; 这样进行单个词或者多个词的词向量的查询。 ./fasttext print-word-vectors model.bin &lt; queries.txtecho “word” | ./fasttext print-word-vectors model.bin Finding simialr words: ./fasttext nn model.bin 最重要的几个参数： The most important parameters of the model are its dimension and the range of size for the subwords. The dimension (dim) controls the size of the vectors, the larger they are the more information they can capture but requires more data to be learned. But, if they are too large, they are harder and slower to train. By default, we use 100 dimensions, but any value in the 100-300 range is as popular. The subwords are all the substrings contained in a word between the minimum size (minn) and the maximal size (maxn). By default, we take all the subword between 3 and 6 characters, but other range could be more appropriate to different languages: 1$ ./fasttext skipgram -input data/fil9 -output result/fil9 -minn 2 -maxn 5 -dim 300 The following arguments for the dictionary are optional: -minCount minimal number of word occurrences [5] -minn min length of char ngram [3] -maxn max length of char ngram [6] The following arguments for training are optional -dim size of word vectors [100] -ws size of the context window [5] 第二个应用场景：文本分类。 Sentiment analysis and email classification are classic examples of text classification （BERT 也是采用的这种label 的格式）在训练数据集中label 默认是使用 “__label__” 进行表示的，当然也是可以进行自定义的。 ./fasttext supervised -input train.ft.txt -output model_kaggle -label __label__ -lr 0.5 就是进行predict的时候，有时候并不是很能想起来只是predict top 3 这样的东西。 # Predicting on the test dataset ./fasttext predict model_kaggle.bin test.ft.txt # Predicting the top 3 labels ./fasttext predict model_kaggle.bin test.ft.txt 3 faiss用途：相似度检测和稠密向量的聚类。 Faiss is a library for efficient similarity search and clustering of dense vectors. 之前的实习经历主要是用faiss 处理文本的representation，但是这个是有偏差的，凡是能够打成词向量，都是可以使用faiss 进行计算的，当然这词向量需要满足：相近内容在相近的空间。 Once the vectors are extracted by learning machinery (from images, videos, text documents, and elsewhere), they’re ready to feed into the similarity search library. faiss的实现过程首先使用 index对于向量进行预处理，然后选择不同的模式。 牺牲了一些精确性来使得运行速度更快。 Similarity search can be made orders of magnitude faster if we’re willing to trade some accuracy; that is, deviate a bit from the reference result. For example, it may not matter much if the first and second results of an image similarity search are swapped, since they’re probably both correct results for a given query. Accelerating the search involves some pre-processing of the data set, an operation that we call indexing. 向量的比较有两种metric：一种是L2 一种是基于consine (点乘)进行检索。前者是求解最小的值，后者是通过inner——product 求解maximum. 并且是支持GPU的，在原来CPU上建立的index，然后很好的迁移到 GPU上。 在涉及index使用考虑速度，acc和内存大小三个不同的维度。然后不同的index 是有不同的侧重的。]]></content>
      <categories>
        <category>NLP</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[NLP中的碎碎念]]></title>
    <url>%2F2019%2F03%2F25%2FNLP%E4%B8%AD%E7%9A%84%E7%A2%8E%E7%A2%8E%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[顾名思义，主要整理一下自己在文本处理中遇到的小的知识点，比如关键词提取技术，分词软件包的原理。 关键词提取TF-IDF这个是可以参看之前自己写的一个博客 卡方分布卡方检验是以χ2分布为基础的一种常用假设检验方法，它的无效假设H0是：观察频数与期望频数没有差别。该检验的基本思想是：首先假设H0成立，基于此前提计算出χ2值，它表示观察值与理论值之间的偏离程度。而实际应用到特征选择中的时候，我们不需要知道自由度，不要知道卡方分布，我们只需要根据算出来的χ2 进行排序就好了，越大我们就越喜欢！挑选最大的一堆，于是就完成了利用卡方检验来进行特征提取。卡方分布的缺点：没有考虑词频 Textrank有一个与之很像的概念 pageRanking，最开始是用来计算网页的重要性。Textrank 主要用来提取文章的关键词，然后比较适合长文本。 CBOW和skip-gram举一个简单的小例子说明 CBOW和skip-gram的区别：skip-gram 是根据中心词汇然后预测上下文词汇，这个不是一下子输入上下文词汇的，而是一个过程，中心词汇和上下文词汇1 ，中心词汇和上下文词汇2，这样的进行输入。 cbow 和其的区别，在于简单相加了上下文词汇作为一个整体，然后和中心词汇进行输入，所以最后是这样的结果。 使用skip gram训练的时间更长，但是对于出现频率不高的词汇，效果比较好。但CBOW的训练速度是相对来说比较快一些。 分词软件的原理下面是常见的分词方式，但是并没有说明哪个软件是什么分词方法。 基于规则的分词方法这种方法又叫作机械分词方法、基于字典的分词方法，它是按照一定的策略将待分析的汉字串与一个“充分大的”机器词典中的词条进行匹配 基于统计的分词方法该方法的主要思想：词是稳定的组合，因此在上下文中，相邻的字同时出现的次数越多，就越有可能构成一个词。因此字与字相邻出现的概率或频率能较好地反映成词的可信度。可以对训练文本中相邻出现的各个字的组合的频度进行统计，计算它们之间的互现信息。互现信息体现了汉字之间结合关系的紧密程度。当紧密程 度高于某一个阈值时，便可以认为此字组可能构成了一个词。该方法又称为无字典分词。 基于语义的分词方法语义分词法引入了语义分析，对自然语言自身的语言信息进行更多的处理，如扩充转移网络法、知识分词语义分析法、邻接约束法、综合匹配法、后缀分词法、特征词库法、矩阵约束法、语法分析法等。]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>keywords</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python中的多线程和多进程]]></title>
    <url>%2F2019%2F03%2F25%2FPython%E4%B8%AD%E7%9A%84%E5%A4%9A%E7%BA%BF%E7%A8%8B%E5%92%8C%E5%A4%9A%E8%BF%9B%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[多线程和多进程问题是可以对应到 并发 （cncurrency）和并行(parallelism)上的。 并发，就是一个单核cpu同时开始了多个任务，但是这个任务并不是同时独立进行的，而是通过cpu的不断切换，保存现场，然后重启这样的快速的切换，给用户的感觉是并发，但是实际上是cpu的计算能力受到了限制，用户体验比较好一些。如果在多核cpu （比如我的mac 是一cpu 6核）这样的话完全是可以达到并行的，这个是真正的独立操作(parallelism)，对应的是多进程的。对应python 中的实现多线程是使用threading，处理的是io 响应；多进程是Concurrency，使用multiprocessing包，处理的是多核cpu的操作。 Take off: 如果处理io 响应，那么使用多线程；如果是计算，那么使用多进程。 So, before we go deeper into the multiprocessing module, it’s worthwhile ensuring you know the advantages of using multiprocessing over multithreading. The general rule of thumb is that, if you are trying to improve the performance of CPU-bound tasks, multiprocessing is what you want to use. However, if your particular task is Input/Output bound, then you’ll generally want to use multithreading to improve the performance of your applications. 下面是多线程的demo。 import multiprocessing as mp def my_func(x): print(mp.current_process()) return x ** x def main(): pool = mp.Pool(mp.cpu_count()) # 这个还是很好的 pool 这个的个数和你的cpu count 是保持一致的 result = pool.map(my_func, [4, 2, 3, 5, 3, 2, 1, 2]) result_set_2 = pool.map(my_func, [4, 6, 5, 4, 6, 3, 23, 4, 6]) print(result) print(result_set_2) if __name__ == &quot;__main__&quot;: main() 这个是多进程的demo。 import threading class Worker(threading.Thread): # Our workers constructor, note the super() method which is vital if we want this # to function properly def __init__(self): super(Worker, self).__init__() def run(self): for i in range(10): print(i) def main(): thread1 = Worker() thread1.start() thread2 = Worker() thread2.start() thread3 = Worker() thread3.start() thread4 = Worker() thread4.start() if __name__ == &quot;__main__&quot;: main()]]></content>
      <categories>
        <category>Python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Data Pre-processing 学习笔记]]></title>
    <url>%2F2019%2F03%2F25%2FData-Pre-processing%2F</url>
    <content type="text"><![CDATA[主要介绍机器学习中的数据预处理，包括 data cleaning、data integration、 data transformation、data reduction、data imbalanced 和一些概念。 Data Cleaning:这个步骤主要处理 missing values 和 noisy data (outlier).对于missing values ，可以分成两个问题，要不要处理 和如何处理，具体说来有一下处理手段： ignore the tuple; fill in the missing value manually use a global constant to fill in the missing value use the attribute mean to fill in the missing value (均值) use the most probable value to fill in the missing value (mode 众数) 有时候就是根据某几个特征然后弄一个简单的回归模型，根据模型进行predict 关于这几种方法如何去选择，我如果说 “it depends”，那么其他人不认为这是一个具有说服力的答案，他们更像知道 it depends what, and when and why to use specific method? 我认为应该是根据缺省值程度和重要性进行经验性的选择，这也去就是 empirical study吧。 接着是 noisy data (outlier)，我的观点是首先得认识到这个是错误的数据，不是真实的数据来源，可能是来自人为的笔误 或者仪器记录的问题，这个是需要修改的。可以使用聚类 (clustering) 进行noisy data 的检测，找到之后这个就类似 missing value了，可以采取以上的手段进行操作，应该注意到的这个 noisy data 所占比例不会很高，否则就成了主要的数据分布了。 Data Integration:处理数据库数据，经常是需要处理子表信息的，那么必然存在着主表，而子表系信息往往是主表信息的某一方面的细化。所以有必要将两者连接起来。 Data Transformation:In data transformation, the data are transformed or consolidated into forms appropriate for mining.这里想要澄清的是很多相同的内容都可以用不同的方式表达，并且可以放在数据处理的不同阶段，并且这种工作不是一次性完成的，而是迭代的 until you run out your patience and time.首先我接触的最常见的就是 discrete variables -&gt; continuous variables. 当然对于 discrete variables，基于树结构的机器学习模型是可以处理的，这里想说的是有这种方式。这种 transformation 常见的处理方式: one-hot 或者 label encoding. 如果按照 data transformation的预设，那么 normalization 就也属于该模块的内容。 不论是在 machine learning 还是在 图像处理的时候，对于原始的数据经常采取 normalization. 一方面这个是可以预防梯度消失 或者 gradient exploding, 如果你采用了 Sigmoid的激活函数的话。另一方面我认为更加重要的原因是将 不同的数据放在了同一个尺度下，如果你采取了 normalization之后。 Data Reduction:一般来说很少提及到到 data reduction的必要性，如果非要给出原因，那么可以从时间和空间的角度进行考虑。更加需要关注的是如何做的问题。 我的理解reduction 可以从两个维度进行考虑，假设一个 matrics A 是 m*n，这个是一个二维的矩阵，那么可以从 行列两方面入手。映射到机器学习中一般这样描述 从dimension 和 data两个角度去描述，分别称之为 dimension reduction 和 data compression. 前者指的是特征的选取，后者是数据size的减少。dimension reduction: where irrelevant, weakly relevant, or redundant attributes or dimensions may be detected and removed.data compression: PCA 线性降维 to reduce the data set size. 这个是针对某一个特征展开的。 why normalization？映射到 N(0,1) 的这种行为，叫做归一化。Feature scaling is the method to limit the range of variables so that they can be compared on common grounds. 有三个主要的原因。 Because most of the Machine Learning models are based on Euclidean Distance. Age- 40 and 27Salary- 72000 and 48000 这两个特征，这两个距离相差很大；但是这个并不是我们想要的，我们想要的是相对值，而不是绝对值。 即使最后的loss function不是 euclidean distance，比如说decision tree，实践证明经过正则化的之后的数据的训练速度是快于 没有经过正则化的数据的。 经过归一化之后，数据是不容易出现梯度消失或者梯度爆炸的。 很多模型的基本假设 就是 N(0,1) 高斯分布。 实现的三种手段： rescaling (min-max normalization) $$x ^ { \prime } = \frac { x - \min ( x ) } { \max ( x ) - \min ( x ) }$$ mean normalization $$x ^ { \prime } = \frac { x - \operatorname { average } ( x ) } { \max ( x ) - \min ( x ) }$$ standardization $$x ^ { \prime } = \frac { x - \overline { x } } { \sigma }$$ data imbalanced这个举例子应该很好举，使用 auc 的理由也应该是会的。 机器学习中的特征工程是有一定技巧可言，其中我觉得最为有趣的是: generation or you can call it abstraction. 对于特征的泛的提取才是对于问题本身或者特征的理解，这不仅需要积累，更需要对于该问题领域的专业知识， that’s all.举个栗子，在 “Home Credit Default Risk” (kaggle 竞赛)中，原始的训练数据有信贷金额和客户的年收入，这个时候 “credit_income_percent” 就是类似这种性质的提取特征。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Dimension Reduction]]></title>
    <url>%2F2019%2F03%2F25%2FDimension-Reduction%2F</url>
    <content type="text"><![CDATA[对于dimension reduction最近有了新的理解，介绍一下降维的概念和相关的分类，最后说一下无监督中的knn 和k-means及相关的差别。 降维概念广义上将降维就是使用更少的数据 (bits) 却保存了尽可能多的信息。You can use this concept to reduce the number of features in your dataset without having to lose much information and keep (or improve) the model’s performance. 不必纠结于采用降维的必要性，直接进入 techniques of dimension reduction. low variances这个是针对一个特征内部的，如果一个特征的数据本身没有什么变化，那么这个类似就是一种“死”数据。 high correlation filter用来判别特征 x 和最后的 target之间的相关性 常见的降维手段 principal component analysis (PCA)our old good friend. 如果你提降维，但是你不知道PCA，那么就说不过去。该方法的基本思路：一个基（向量空间）的变换，使得变换后的数据有着最大的方差。It works on a condition that while the data in a higher dimensional space is mapped to data in a lower dimension space, the variance of the data in the lower dimensional space should be maximum. 下面是PCA的一些特点： A principal component is a linear combination of the original variablesPrincipal components are extracted in such a way that the first principal component explains maximum variance in the datasetSecond principal component tries to explain the remaining variance in the dataset and is uncorrelated to the first principal componentThird principal component tries to explain the variance which is not explained by the first two principal components and so on 主成分是不断生成的，在前者基础之上生成的。 The first component is the most important one, followed by the second, then the third, and so on. Singular Value Decomposition (SVD) 翻译成中文感觉还是挺别扭的，奇异值分解。关于奇异值，特征值这些数学概念打算另外写一个主题，wait a moment. 简单理解PCA 是针对方阵 (mm), SVD是针对矩阵(m n)，所以后者是具有更大的适用范围。 Independent Component Analysis (ICA) 这个是在面试的时候被问道的一种降维方法。抓住独立向量应该就没有问题。 Independent Component Analysis (ICA) is based on information-theory and is also one of the most widely used dimensionality reduction techniques. The major difference between PCA and ICA is that PCA looks for uncorrelated factors while ICA looks for independent factors. pca 和 ica 的差别在于，相关性和独立性的差别。 基本假设： This algorithm assumes that the given variables are linear mixtures of some unknown latent variables. It also assumes that these latent variables are mutually independent, i.e., they are not dependent on other variables and hence they are called the independent components of the observed data. ICA 和PCA的异同：从线性代数的角度去理解，PCA和ICA都是要找到一组基，这组基张成一个特征空间，数据的处理就都需要映射到新空间中去。ICA相比与PCA更能刻画变量的随机统计特性，且能抑制高斯噪声。 T-SNE 就是指出 t-SNE 这个可以非线性降维。有local approaches 和 global approaches 两种方式，我大概的理解就是：类之间还能尽可能的远离，类内保持差异性。这个使用场景是在可视化中，经常会看见将数据或者 So far we have learned that PCA is a good choice for dimensionality reduction and visualization for datasets with a large number of variables. But what if we could use something more advanced? What if we can easily search for patterns in a non-linear way? t-SNE is one such technique. There are mainly two types of approaches we can use to map the data points:Local approaches : They maps nearby points on the manifold to nearby points in the low dimensional representation.Global approaches : They attempt to preserve geometry at all scales, i.e. mapping nearby points on manifold to nearby points in low dimensional representation as well as far away points to far away points. 下面介绍两种不是那么“常规”，但是也符合”dimension reduction” 定义的方式。 projection By projecting one vector onto the other, dimensionality can be reduced. autoencoder 网络结构通常有 encoder和decoder两部分组成，那么encoder 就作为 information abstraction,而 decoder作为一种重新映射。从这个角度NLP中的词向量也是可以是一种降维手段。 kmeans &amp; knn简而言之，knn 是有监督分类学习；kmeans 是无监督聚类。 In short, the algorithms are trying to accomplish different goals. K-nearest neighbor is a subset of supervised learning classification (or regression) algorithms (it takes a bunch of labeled points and uses them to learn how to label other points). It is supervised because you are trying to classify a point based on the known classification of other points. In contrast, K-means is a subset of unsupervised learning clustering algorithms (it takes a bunch of unlabeled points and tries to group them into clusters). It is unsupervised because the points have no external classification. The $ k $ in each case mean different things. In K-NN, the $ k $ represents the number of neighbors who have a vote in determining a new player’s position. The $ k $ in K-means, determine the number of clusters we want to end up. In a K-NN algorithm, a test sample is given as the class of majority of its nearest neighbours. For example, if we have three classes and the goal is to find a class label for the unknown example $ x_j $ then, by using the Euclidean distance and a value of $ k=5 $ neighbors, the unknown sample is classified to the category of the most voted neighbors. How it works?Step 1: Determine the value for KStep 2: Calculate the distances between the new input (test data) and all the training data. The most commonly used metrics for calculating distance are Euclidean, Manhattan and MinkowskiStep 3: Sort the distance and determine k nearest neighbors based on minimum distance valuesStep 4: Analyze the category of those neighbors and assign the category for the test data based on majority voteStep 5: Return the predicted class The situation with K-means is that, given some data you will cluster them in k-groups or clusters. The initial step of the algorithm is to randomly spawn $ k $ centroids (centers). At every iteration the center of each cluster is moved slightly to minimize the objective function. The algorithm will terminate if the iterations are maximized or if the centroids stop to move. The objective function of K-means is $ J = \sum_{j=1}^{k}\sum_{i=1}^{n}\left |x_i^{j}-c_j \right |^{2} $ How it works?Step 1: Determine K value by Elbow method and specify the number of clusters KStep 2: Randomly assign each data point to a clusterStep 3: Determine the cluster centroid coordinatesStep 4: Determine the distances of each data point to the centroids and re-assign each point to the closest cluster centroid based upon minimum distanceStep 5: Calculate cluster centroids againStep 6: Repeat steps 4 and 5 until we reach global optima where no improvements are possible and no switching of data points from one cluster to other.]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>knn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linear Algebra in ML]]></title>
    <url>%2F2019%2F03%2F25%2FLinear-Algebra-in-ML%2F</url>
    <content type="text"><![CDATA[我觉得到 ML 中的一个难点：就是由原来简单的 linear equations 直接过渡到了 matrics and vectors。这个过程是没有人跟你说的。网络结构可以当作是一个complicated 并且是无法表示的函数，于是很多使用者把它当作黑匣子，关心于输入和输出，中间过程 don’t care. 变量（特征个数）和解的关系多变量和最后target的关系是可以使用 matrices 进行表示的，这就是一种数学公式化。 Broadly speaking, in linear algebra data is represented in the form of linear equations. These linear equations are in turn represented in the form of matrices and vectors. 先直观的感受一下变量和图形（可视化）的关系。两个变量组成的equations 是两条线的相交情况。而三个变量在空间中有三种情况： 相交，平行，不在一个平面上。三个变量组成的equations 是三个面的相交情况。有四种情况 (try hard to figure it out)：No intersection at all.Planes intersect in a line.They can intersect in a plane.All the three planes intersect at a point. 当到达4 dims 的时候，it’s impossible to visulize it. terms in related to matrix这些词汇 (terms) 经常在文献中出现，需要对于其含义有个比较好的认识。Order of matrix – If a matrix has 3 rows and 4 columns, order of the matrix is 34 i.e. rowcolumn. (翻译成 矩阵的阶)Square matrix – The matrix in which the number of rows is equal to the number of columns.Diagonal matrix – A matrix with all the non-diagonal elements equal to 0 is called a diagonal matrix.Upper triangular matrix – Square matrix with all the elements below diagonal equal to 0.Lower triangular matrix – Square matrix with all the elements above the diagonal equal to 0.Scalar matrix – Square matrix with all the diagonal elements equal to some constant k.Identity matrix – Square matrix with all the diagonal elements equal to 1 and all the non-diagonal elements equal to 0.Column matrix – The matrix which consists of only 1 column. Sometimes, it is used to represent a vector.Row matrix – A matrix consisting only of row.Trace – It is the sum of all the diagonal elements of a square matrix.Rank of a matrix – Rank of a matrix is equal to the maximum number of linearly independent row vectors in a matrix.Determinant of a matrix - 矩阵的行列式转置 -在图形 matrix中还是很常见的。$$\mathrm { A } _ { \mathrm { ij } } ^ { \mathrm{T}} = \mathrm { A } _ { \mathrm { ji } }$$ 这个矩阵乘法和元素相称的区别，后者是element-wise 进行的。可以从另外一个角度去列及矩阵相称： This operation on a vector is called linear transformation. 就是后面的vector 映射到了前面的矩阵空间。 特征值和奇异值着两个是分别对应着PCA 和SVD。Eigenvalues and Eigenvectors如公式所示，特征值和特征向量的乘积就是方阵和特征向量的乘积，原先的方阵是可以降维表示成特征向量和特征值的。$ A x = \lambda x $ 对于奇异值分解，最常见的就是这种表达：$A = U \Sigma V ^ { T }$特征值分解和奇异值分解都是给一个矩阵(线性变换)找一组特殊的基，特征值分解找到了特征向量这组基，在这组基下该线性变换只有缩放效果。而奇异值分解则是找到另一组基，这组基下线性变换的旋转、缩放、投影三种功能独立地展示出来了。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Back to my blog]]></title>
    <url>%2F2019%2F03%2F07%2FBack-to-my-blog%2F</url>
    <content type="text"><![CDATA[直到某一天发现个人网站中的图片都显示不出来了，查了一下才发现之前的图床不能用了（点名批评七牛），果断弃之，换了个大厂子产品。证明一下图片是能出来的。ps：之前的图片有时间再整理到新的平台上。]]></content>
      <categories>
        <category>人间不值得</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[基于simhash的大文本相似度比较]]></title>
    <url>%2F2018%2F08%2F23%2F%E5%9F%BA%E4%BA%8Esimhash%E7%9A%84%E6%96%87%E6%9C%AC%E7%9B%B8%E4%BC%BC%E5%BA%A6%E6%AF%94%E8%BE%83%2F</url>
    <content type="text"><![CDATA[本文主要记录使用simhash比较中文大文本的相似度问题。先说一下文本特征，数据属于中文文本，每篇文章的字数大于500,小于2000,基本上属于大文本。步骤如下： 基于tf-idf提取文本的关键词。如果这些关键词在之后的比较中是相同的，那么认为对应的文章也是相同。简而言之，这些提取的关键词可以看做原文章的”代表”. 根据关键字计算simhash编码，然后使用hamming distance进行比较两者的不同。如果对于上述概念比较模糊，建议首先阅读该篇博客。 实战顺滑过渡到代码实现：123456789101112131415# 常规导包import sys,codecsimport pandas as pdimport numpy as npimport jieba.possegimport jieba.analysefrom sklearn import feature_extractionfrom sklearn.feature_extraction.text import TfidfTransformerfrom sklearn.feature_extraction.text import CountVectorizer# 数据集的路径path =&quot;../tianmao2.csv&quot;names =[&apos;where&apos;, &apos;time&apos;, &apos;title&apos;, &apos;url&apos;, &apos;contents&apos;]data =pd.read_csv(path, delimiter=&apos;\t&apos;, names= names, nrows=200)data[&apos;id&apos;] =data.index+1data.head() 我们使用title和contents 组合作为原始处理的数据，我们认为该数据能够就是文章的内容。1stopkey = [w.strip() for w in codecs.open(&apos;../keyword_extraction/data/stopWord.txt&apos;, &apos;r&apos;).readlines()] 该stop words是中文停用词，就是常见的”的 了”。常见的有百度停用词表、哈尔滨工业大学停用词表以及中科院的停用词表。这里使用的是中科院的停用词。对于停用词的存储，可以使用set ，因为set 要比 list的检索要快。12345678def dataPrepos(text, stopkey): l = [] pos = [&apos;n&apos;, &apos;nz&apos;, &apos;v&apos;, &apos;vd&apos;, &apos;vn&apos;, &apos;l&apos;, &apos;a&apos;, &apos;d&apos;] # 定义选取的词性 seg = jieba.posseg.cut(text) # 分词 for i in seg: if i.word not in stopkey and i.flag in pos: # 去停用词 + 词性筛选 l.append(i.word) return l 我们选择名词作为主要的分析对象。12345678idList, titleList, abstractList = data[&apos;id&apos;], data[&apos;title&apos;], data[&apos;contents&apos;]corpus = [] # 将所有文档输出到一个list中，一行就是一个文档# 这个 虽然使用 &quot; &quot; 进行分割 但是实际上还是一个打的listfor index in range(len(idList)): text = &apos;%s。%s&apos; % (titleList[index], abstractList[index]) # 拼接标题和摘要 text = dataPrepos(text, stopkey) # 文本预处理 text = &quot; &quot;.join(text) # 连接成字符串，空格分隔 corpus.append(text) 这里的corus 是将所有的经过预处理文档作为当前计算 idf 的语料库。123456789vectorizer = CountVectorizer()X = vectorizer.fit_transform(corpus) # 词频矩阵,a[i][j]:表示j词在第i个文本中的词频# 2、统计每个词的tf-idf权值transformer = TfidfTransformer()tfidf = transformer.fit_transform(X)# 3、获取词袋模型中的关键词word = vectorizer.get_feature_names()# 4、获取tf-idf矩阵，a[i][j]表示j词在i篇文本中的tf-idf权重weight = tfidf.toarray() 使用sklearn 内置的函数计算tf-idf。1234567891011121314151617181920212223242526272829topK = 10ids, titles, keys, weights = [], [], [], []for i in range(len(weight)): print(&quot;-------这里输出第&quot;, i + 1, &quot;篇文本的词语tf-idf------&quot;) ids.append(idList[i]) titles.append(titleList[i]) df_word, df_weight = [], [] # 当前文章的所有词汇列表、词汇对应权重列表 for j in range(len(word)): # print(word[j],weight[i][j]) df_word.append(word[j]) df_weight.append(weight[i][j]) df_word = pd.DataFrame(df_word, columns=[&apos;word&apos;]) df_weight = pd.DataFrame(df_weight, columns=[&apos;weight&apos;]) word_weight = pd.concat([df_word, df_weight], axis=1) # 拼接词汇列表和权重列表 word_weight = word_weight.sort_values(by=&quot;weight&quot;, ascending=False) # 按照权重值降序排列 # 在这里可以查看 k的选取的数值应该是多大， # from ipdb import set_trace # set_trace() keyword = np.array(word_weight[&apos;word&apos;]) # 选择词汇列并转成数组格式 word_split = [keyword[x] for x in range(0, topK)] # 抽取前topK个词汇作为关键词 word_split = &quot; &quot;.join(word_split) keys.append(word_split) wei = np.array(word_weight[&apos;weight&apos;]) wei_split = [str(wei[x]) for x in range(0, topK)] wei_split = &quot; &quot;.join(wei_split) weights.append(wei_split) # 这里的命名 容易混淆result = pd.DataFrame(&#123;&quot;id&quot;: ids, &quot;title&quot;: titles, &quot;key&quot;: keys, &apos;weight&apos;: weights&#125;, columns=[&apos;id&apos;, &apos;title&apos;, &apos;key&apos;, &apos;weight&apos;]) 选择前10个频率最高的词语作为该篇文章的代表，当然这个参数是可以调整，需要根据具体的问题和结果进行调整。1result.head() 最后的效果如上。至此我们第一步的提取文章的关键词就已经做完。下面进行相似度的比较。 1234import jiebaimport jieba.analyseimport pandas as pd#日常导包 数据和上述的一样，所以就不截图了。123datasets =pd.read_csv(&quot;../tianmao2-tf-idf.csv&quot;)tokens =datasets[&apos;key&apos;]weights =datasets[&apos;weight&apos;] 提取关键词和对应的权重。123456789101112print(tokens[0], len(tokens[0]))print(weights[0], len(weights[0]))tokens0 =tokens[0].split()weights0 =weights[0].split()len(tokens0)len(weights0)tokens1 =tokens[1].split()weights1 =weights[1].split()import astweights0 =[ ast.literal_eval(i) for i in weights0]weights1 =[ ast.literal_eval(i) for i in weights1] 构造测试用例。因为权重是字符串，所以简单处理转成整数。 12dict0 =dict(zip(tokens0, weights0))dict1 =dict(zip(tokens1, weights1)) 定义一个Simhash，提供对文档的数值映射和文档间相似度计算的功能.123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081class Simhash(object): # 初始化函数 def __init__(self, weights_dict, tokens=&apos;&apos;, hashbits=64): self.hashbits = hashbits self.hash = self.simhash_function(tokens, weights_dict) # toString函数 # 不懂这个 self.hash ，凡是带有self 的函数都是可以类变量，所以这个就是返回的 self.hash这个变量 #凡是使用__str__ 这种类型的函数 都是重写 原来的函数 def __str__(self): return str(self.hash) &quot;&quot;&quot; ord() 函数是 chr() 函数（对于8位的ASCII字符串）或 unichr() 函数（对于Unicode对象）的配对函数，它以一个字符（长度为1的字符串）作为参数，返回对应的 ASCII 数值，或者 Unicode 数值 &quot;&quot;&quot; # 给每一个单词生成对应的hash值 # 这个操作搞懂之后一定很简洁， 但是现在很难理解，因为不是字符串，而是位操作 def _string_hash(self, source): if source == &apos;&apos;: return 0 else: x = ord(source[0]) &lt;&lt; 7 # &lt;&lt; 表示 乘以2^7 ; &gt;&gt; 表示除以 ; ** 表示次方的意思 # ^ : 按位异或 (二进制进行异或)； &amp; 按位进行与 操作 # 左移位操作也是可以理解为 2^x 的操作，因为存储是二进制，这样左移一位 表示×2 一次 m = 1000003 mask = 2 ** self.hashbits - 1 for c in source: x = ((x * m) ^ ord(c)) &amp; mask x ^= len(source) if x == -1: x = -2 return x # 生成simhash值 def simhash_function(self, tokens, weights_dict): v = [0] * self.hashbits # 这种使用 &#123;&#125; dictionary 然后强行得到item 再进行遍历也是牛逼 for key, t in &#123;x: self._string_hash(x) for x in tokens&#125;.items(): for i in range(self.hashbits): bitmask = 1 &lt;&lt; i if t &amp; bitmask: v[i] += weights_dict[key] else: v[i] -= weights_dict[key] fingerprint = 0 for i in range(self.hashbits): if v[i] &gt;= 0: fingerprint += 1 &lt;&lt; i return fingerprint # 求文档间的海明距离 def hamming_distance(self, other): x = (self.hash ^ other.hash) &amp; ((1 &lt;&lt; self.hashbits) - 1 ) tot = 0 while x : tot += 1 x &amp;= x - 1 return tot #求相似度 # 这个相似度的计算，十分简单，如果两个数接近，那么就是认为相似。越是接近1 越是相似， # 不是原先那种以某一个参数整数 如3 为距离的相似度 def similarity(self, other): a = float(self.hash) b = float(other.hash) if a &gt; b: return b / a else: return a / b if __name__ == &apos;__main__&apos;: hash0 = Simhash(weights_dict=dict0, tokens=tokens0) print(hash0) hash1 = Simhash(weights_dict=dict1, tokens=tokens1) print(hash1) print(hash0.hamming_distance(hash1)) print(hash0.similarity(hash1)) 结果如上。可以看出该例子中使用的两两比较的方式，对于大数据来说，一般可能会用到倒排索引和cpu并行技术。 原理：步骤Simhash 分为5个步骤：分词、hash（md5 要求均匀映射到某空间就行，不要求反应原始样本的相关性）、加权、合并（列项相加）、降维（正数为1 负数为0），得到每篇文章的simhash 之后，计算两个文章的海明距离（两个字符串对应位置的不同字符的个数）。对于64 位的simhash 值，在3以内就可以认为是比较相似的。两张比较能说明问题的图片。 第一步：文本预处理得到分词（去重，去除的了stop words）,然后weight权重可以使用分词的frequency 或者tfidf 进行得到第二步：进行hash 映射（可以使用md5这种传统的映射方式，基本的要求就是均匀映射到一个空间，这种映射并不能反映原始样本的相关性）第三步：hash 映射值和weight 进行相乘，如果原来是1 则乘以1，如果是0 则乘以-1，第四步： 列向相加，得到summing weights,进行降维如果是正数那么为1，如果是负数那么为-1然后这个simhash就出来了之后进行的文本比较常常使用hamming distance，操作就是64位两两比较如果如果不同counter就+1。最后的就得到了两两文本的distance ，然后一般使用3，作为阈值，如果大于3，那么就认为两者是不相同的。 评价：simhash算法有缺点是：只考虑到文章存在哪些词，没有考虑到词的顺序。不过相应的优点是，可以实现海量文章相似度计算。文章相似度计算忽略词的顺序之后效果更好。当文本内容较长时，使用SimHash准确率很高，SimHash处理短文本内容准确率往往不能得到保证。文本内容中每个term对应的权重如何确定要根据实际的项目需求，一般是可以使用IDF权重来进行计算。 SimHash与传统hash函数的区别:传统的Hash算法只负责将原始内容尽量均匀随机地映射为一个签名值，原理上仅相当于伪随机数产生算法。如果原始内容在不相同，使用传统hash 得到的signature除了是不同，不能提供其他的信息；而simhash 能够在一定程度上反映这种不相同的程度，表征原内容的相似度。一个字节的差异和五个字节的差异得到的相似度是不一样的。普通hash是random 的完全没有这种信息。因为我们使用的simhash是局部敏感哈希，这个算法的特点是只要相似的字符串只有个别的位数是有差别变化。 补充材料：基于不同表示带来的不同的相似度比较的指标： jaccard 还是余弦距离，海明距离Jaccard系数：简单说就是交集/ 并集两个集合A和B交集元素的个数在A、B并集中所占的比例，称为这两个集合的杰卡德系数，用符号 J(A,B) 表示。杰卡德相似系数是衡量两个集合相似度的一种指标（余弦距离也可以用来衡量两个集合的相似度）。]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>文本相似度</tag>
        <tag>Simhash</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[文本相似度比较基本知识]]></title>
    <url>%2F2018%2F08%2F23%2F%E6%96%87%E6%9C%AC%E7%9B%B8%E4%BC%BC%E5%BA%A6%E6%AF%94%E8%BE%83%E5%9F%BA%E6%9C%AC%E7%9F%A5%E8%AF%86%2F</url>
    <content type="text"><![CDATA[本文介绍的SimHash是一种局部敏感hash，它也是Google公司进行海量网页去重使用的主要算法。 simhash对于文本去重而言，目前有很多NLP相关的算法可以在很高精度上来解决，但是我们现在处理的是大数据维度上的文本去重，这就对算法的效率有着很高的要求。但是在小的样本上这个是不一定有保证有效的。SimHash算法是Google公司进行海量网页去重的高效算法，它通过将原始的文本映射为64位的二进制数字串，然后通过比较二进制数字串的差异进而来表示原始文本内容的差异。本文服务于该篇博客,主要进行名词解释。 基本概念simhash 也是一种hash，一般的hash 函数映射规则只需要满足以下两个条件： 对很多不同的特征来说，它们对所对应的向量是均匀随机分布的 相同的特征来说对应的向量是唯一简单来说普通的hash映射需要满足随机分布和唯一性两个条件。simhash想要实现的是，如果原来的文本的特征是相似，那么映射之后的编码也是相似。这里使用 hamming distance 进行比较simhash映射之后的距离。根据经验值，对64位的 SimHash值，海明距离在3以内的可认为相似度比较高。编码之后的表示在英文中是 fingerprint(指纹)。simhash最初被google 用于网页去重，当时使用的fingerprint 是64,所以这里沿用了这个传统。64位的签名可以表示多达264个象限，因此只保存所在象限的信息也足够表征一个文档了。更进一步，表示的文档的数字最多是多少？这个应该可以准确计算特征的个数应为如果用三位(01) 表示，那么有8种，那么2^64 这么多种特征，所以16*10^18 这么多。算法步骤第一步：文本预处理得到分词（去重，去除的了stop words）,然后weight权重可以使用分词的frequency 或者tfidf 进行得到第二步：进行hash 映射（可以使用md5这种传统的映射方式，基本的要求就是均匀映射到一个空间，这种映射并不能反映原始样本的相关性）第三步：hash 映射值和weight 进行相乘，如果原来是1 则乘以1，如果是0 则乘以-1，第四步： 列向相加，得到summing weights,进行降维如果是正数那么为1，如果是负数那么为-1然后这个simhash就出来了.有图有真相 simhash的局限性：只考虑到文章存在哪些词，没有考虑到词的顺序。不过相应的优点是，可以实现海量文章相似度计算。文章相似度计算忽略词的顺序之后效果更好。所以在处理大文本时候，simhash是有效的，但是在处理小文本，这种效果往往不能被保证。直观上理解，在一片段文章或者段落中，词语出现的顺序还是比较重要的。 minhash可以参考该视频和这篇文章。 Locality Sensitive HashingLocality Sensitive Hashing(局部敏感哈希)作用就是从海量的数据中挖掘出相似的数据，可以具体应用到文本相似度检测、网页搜索等领域。上面的simhah和minhash 就是该思想的实现。 距离函数这里的距离函数都是用来文本相似度。 Jaccard相似度简单来说交集除以并集。这个集合中存放的是文章或者段落的关键词。1234567891011def JaccardSim(str_a, str_b): &apos;&apos;&apos; Jaccard相似性系数 计算sa和sb的相似度 len（sa &amp; sb）/ len（sa | sb） &apos;&apos;&apos; seta = splitWords(str_a)[1] setb = splitWords(str_b)[1] sa_sb = 1.0 * len(seta &amp; setb) / len(seta | setb) return sa_sb 可以看到核心代码很简单，经过分词之后，就是seta 和setb 进行的操作。 cosine12345def cos_sim(a, b): a = np.array(a) b = np.array(b) # return &#123;&quot;文本的余弦相似度:&quot;:np.sum(a*b) / (np.sqrt(np.sum(a ** 2)) * np.sqrt(np.sum(b ** 2)))&#125; return np.sum(a * b) / (np.sqrt(np.sum(a ** 2)) * np.sqrt(np.sum(b ** 2))) 将文本的关键词映射成某种高维函数，然后在高维空间中计算两者的相似度。 tf-idf在simhash 中使用 tf-idf作为我们的比较函数。TF-IDF的主要思想就是：如果某个词在一篇文档中出现的频率高，也即TF高；并且在语料库中其他文档中很少出现，即DF的低，也即IDF高，则认为这个词具有很好的类别区分能力。词频(term frequency)有两种计算方式,后者考虑了相对的情况。计算idf(inverse document frequency):TF-IDF 优点是简单快速，比较符合实际。缺点，无法体现词的位置信息，所有的位置都是被认为重要性相同，但是开头结尾，段落的开头和段落的结尾，therefore，so，but这些词语都是没有体现的。 Hamming distancehamming distance就是比较01串的不同，按照位进行比较。算法：异或时，只有在两个比较的位不同时其结果是1 ，否则结果为0，两个二进制“异或”后得到1的个数即为海明距离的大小。123456789101112131415161718hashbits =64 # 使用64位进行编码def simhash_function(tokens, weights_dict): v = [0] * hashbits # 这种 &#123;key: value&#125;.item() 的操作也是没有了谁了 for key, t in &#123;x: _string_hash(x) for x in tokens&#125;.items(): for i in range(hashbits): bitmask = 1 &lt;&lt; i if t &amp; bitmask: v[i] += weights_dict[key] else: v[i] -= weights_dict[key] fingerprint = 0 for i in range(hashbits): if v[i] &gt;= 0: fingerprint += 1 &lt;&lt; i return fingerprintfingerprint = simhash_function(tokens, weights) min edit distance123456789101112131415161718192021222324# 最小编辑距离def min_edit_distance(str1, str2): rows =len(str2) +1 cols =len(str1) +1 arr =[[0 for _ in range(cols)] for _ in range(rows)] # 这种简洁的代码也是牛逼 for j in range(cols): arr[0][j] =j for i in range(rows): arr[i][0] =i for i in range(1, rows): for j in range(1, cols): # 因为string 是从0 ，len(str) -1的 if str2[i-1] ==str1[j-1]: arr[i][j] =arr[i-1][j-1] else: # 以后见到这样的式子，就要想到这个二维的数组，因为这个是可以帮助记忆的 arr[i][j] =1 +min(arr[i-1][j-1], arr[i-1][j], arr[i][j-1]) # 右下角就是距离 return arr[rows-1][cols-1]str_a =&quot;abcdef&quot;str_b =&quot;azced&quot;result =min_edit_distance(str_a, str_b)result 具体可以参看该视频讲解。(ps. 如果刷leetcode,也可以参看该视频) 分词在英文中存在天然的空格可以进行分词操作，但是中文的分词就比较复杂了。常用的中文分词开源工具有 jieba和HanLP前者简单易行，容易上手；后者在自然语言处理作为汉语言处理包，可以用于词性标注，命名实体识别等一系列功能。常用的英文分词 corenlp 倒排索引倒排索引使用python在实现上就是一个dictionary 嵌套一个 set(). 一般的索引都是数字或者英文字母映射内容，具体在放到simhash的情景下就是使用文章的序列号对应提取出来的关键词。但是倒排索引就是关键词对应文章的序列号，类似与原来的”值”对应这”键”，所以称之为倒排索引。一般使用在召回的场景下，使用关键词然后出现了该关键词下的index 的集合。可以参考这篇文章。]]></content>
      <tags>
        <tag>Simhash</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Differences Between l1 and l2 as Loss Function and Regularization]]></title>
    <url>%2F2018%2F07%2F21%2Fdifferences-between-l1-and-l2-as-loss-function-and-regularization%2F</url>
    <content type="text"><![CDATA[本文主要介绍 L1 和L2 分别作为 loss function 和 regularization，最后谈一下 overfitting的事情。 L1 和L2 作为Loss function和 regularization，个人感觉是一个经常容易混淆的概念。但是如果读者觉得很清楚，那么就可以跳过了。本文大量借鉴于该博客，原文是英文，如果读者英文能够handle，建议读原文。 As loss functionloss function or error function 是用来衡量真实y 和生成的f(x) 之间差距的函数。在模型训练中我们一般情况下不断训练模型使得loss function不断下降（如果task要求loss function是增大，这时候一般加上符号或者转换成 1- loss fucntion，最后实现的还是loss function下降）。好的回到L1 loss function和L2 loss function. L1-norm loss function is also known as least absolute deviations(LAD), least absolute errors. It is basically minimizing the sum of the absolute differences between the target value Y and the estimated values (f(x)). $$S = \sum _ { i = 1 } ^ { n } \left| y _ { i } - f \left( x _ { i } \right) \right|$$ L2-norm loss function is also known as least squares error(LSE). It is basically minimizing the sum of the square of the differences between the target value Y and the estimated values (f(x)). $$S = \sum _ { i = 1 } ^ { n } \left( y _ { i } - f \left( x _ { i } \right) \right) ^ { 2 }$$ 主要你从一下三个指标去衡量两者的不同： robustness，stability和是否具有唯一解。wiki 中关于robust 中是这样定义： A learning algorithm that can reduce the chance of fitting noise is called robust。具有更好的泛化性能，不去过度拟合noise. 关于stability) wiki 是这样定义：Stability, also known as algorithmic stability, is a notion in computational learning theory of how a machine learning algorithm is perturbed by small changes to its inputs. 我对于前两个指标的理解：robustness 是对于原始的train 样本中离群点的态度，如果某个模型是robustness的，那么对于该数据集中的离群点是能够抗干扰的。反之则是不具有robustness的。stability是对于原始train 数据的轻微的平移的反应，如果对于某个原始数据的轻微平移，最后的结果没有产生很大的波动，那么该模型就是具有stability, 反之，则不具有stability. 比较L1-norm 和L2-norm在前两个评价指标中的表现： 对于第三点，我想在下面进行介绍。因为这点和后面和下面的solution uniqueness是相同的。 As regularization从XGBoost调参指南中我们知道objective function = loss funcion + regularization. 而我们大多数情况下提及的都是loss function,常常忽略了regularization 的作用。 The regularization term controls the complexity of the model, which helps us to avoid overfitting.对于模型训练，一开始的想法是尽量的overfitting, 因为就现在不成熟的经验而言，对于overfitting这个问题有很多处理方法，比如卷积深度神经网络中的dropout, LightGBM中的early stop 和随机采样的思想。 这些方法都是可以缓解overfitting，所以可以出现overfitting。相反，如果你的模型是underfitting，那么你就微显尴尬了。好，收回到L1 and L2。 先上公式L1 regularization on least squares:$$\mathbf { w } ^ { * } = \underset { \mathbf { w } } { \arg \min } \sum _ { j } \left( t \left( \mathbf { x } _ { j } \right) - \sum _ { i } w _ { i } h _ { i } \left( \mathbf { x } _ { j } \right) \right) ^ { 2 }$$ L2 regularization on least squares: The difference between their properties can be promptly summarized as follows: 对于第一点computational efficient的理解：平方比绝对值更容易计算，平方可以求导直接求最值，但是绝对值就无法求导。并且L1 regularization在 non-sparse cases中是 computational inefficient，但是在 sparse(0比较多) cases中是有相应的稀疏算法来进行优化的，所以是computational efficient.对于第二点是否具有sparse solution可以从几何意义的角度解读：The green line(L2-norm) is the unique shortest path, while the red, blue yellow(L1-norm) are all same length for the same route. Built-in feature selection is frequently mentioned as a useful property of the L1-norm, which the L2-norm does not. This is actually a result of the L1-norm, which tends to produces sparse coefficients (explained below). Suppose the model have 100 coefficients but only 10 of them have non-zero coefficients, this is effectively saying that “the other 90 predictors are useless in predicting the target values”. L2-norm produces non-sparse coefficients, so does not have this property. 所以表格中第三点也是顺理成章的了。至此，我们区分了L1-norm vs L2-norm loss function 和L1-regularization vs L2-regularization，下面说一下 overfitting的东西。 overfitting现在，我们的训练优化算法是一个由两项内容组成的函数：一个是损失项，用于衡量模型与数据的拟合度，另一个是正则化项，用于衡量模型复杂度。 对于过拟合有两种解读方式，一种是模型是复杂，然后是去拟合了所有的数据，没有了泛化性能； 一种是从数据角度，模型去拟合了noise ，这些random的数据。然后从loss function的角度去优化的话，就是降低模型的复杂度，正则项就是降低模型复杂度的一种手段。所以从这个角度，L2 是降低模型复杂度的一种手段，也是一种减轻overfit的手段，这两者是相辅相成的。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>l1</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LightGBM和XGBoost及其调参]]></title>
    <url>%2F2018%2F07%2F21%2FLightGBM%E5%92%8CXGBoost%E5%8F%8A%E5%85%B6%E8%B0%83%E5%8F%82%2F</url>
    <content type="text"><![CDATA[先主要介绍树的基本知识，然后介绍LightGBM和XGBoost及其调参. 进入正文之前简单的说一下决策树。一棵树很容易过拟合或者欠拟合（根据树的深度），然后需要使用多棵树进行组合预测，而GBDT是实现这个手段的方式之一。sklearn中也是实现了 GBDT 这种思想，但是比较难用，训练速度跟不上。但是 lightGBM 和XGBoost 实现的效果就比较好。 lightGBM调参(常用参数)Since lightGBM is based on decision tree algorithms, it splits the tree with the best fit whereas boosting algorithms split the tree depth wise or level wise rather than leaf-wise. So when growing on the same leaf in Light GBM, the leaf-wise algorithm can reduce more loss than the level-wise algorithm and hence results in much better accuracy which can rarely be achieved by any of the existing boosting algorithms. Also, it is surprisingly very fast, hence the word ‘Light’. Leaf wise splits lead to increase in complexity and may lead to overfitting and it can be overcome by specifying another parameter max_depth parameter. Advantages of LightGBM faster training speed and higher efficiencyLight GBM use histogram based algorithm i.e it buckets continuous feature values into discrete bins which fasten the training procedure. lower memory usageReplaces continuous values to discrete bins which result in lower memory usage. better accuracy than any other boosting algorithmIt produces much more complex trees by following leaf wise split approach rather than a level-wise approach which is the main factor in achieving higher accuracy. compatibility with large datasetsIt is capable of performing equally good with large datasets with a significant reduction in training time as compared to XGBOOST. parallel learning supported lightGBM调参(常用参数) taskdefault= train, option: train, prediction applicationdefault= regression, option: regression, binary, multiclass, lambdarank(lambdarank application) datatraining data, 这个比较诡异，你需要创建一个lightGBM类型的data num_iterationsdefault =100, 可以设置为的大一些，然后使用early_stopping进行调节。 early_stopping_rounddefault =0, will stop training if one metric of one validation data doesn’t improve in last early_stopping_round rounds. num_leavesdefault =31, number of leaves in a tree devicedefault =cpu, options: gpu, cpu, choose gpu for faster training. max_depthspecify the max depth to which tree will grow, which is very important. feature_fractiondefault =1, specifies the fraction of features to be taken for each iteration. bagging_fractiondefault =1, spefifies the fraction of data to be used for each iteration and it generally used to speed up the training and avoid overfitting. max_binmax number of bins to bucket the feature values.因为模型是基于bin训练的，如果bin 数量越多，得到better accuracy,同时更加容易 overfitting. num_threads labelspecify the label columns. categorical_featurespecify the categorical features num_classdefault =1, used only for multi-class classification referrencewhich-algorithm-takes-the-crown-light-gbm-vs-xgboostLightGBM 如何调参官方文档param_tuning官方文档parameter XGBoost调参Advantage of XGBoost regularizationstandard GBM implementation has no regularization, in fact, XGBoost is also known as ‘regularized boosting’ technique. parallel processingwe know that boosting is sequential process so how can it be parallelized? this link to explore further. high flexibilityXGBoost allow users to define custom optimization objectives and evaluation criteria handling missing valuesvery useful property. XGBoost has an in-built routine to handle missing values. User is required to supply a different value than other observations and pass that as a parameter. XGBoost tries different things as it encounter a missing value on each node and learns which path to take for missing values in future. Tree pruningA GBM would stop splitting a node when it encounters a negative loss in the split. Thus it is more of a greedy algorithm. XGBoost on the other hand make splits upon the max_depth specified and then start pruning the tree backwards and remove splits beyond which there is no positive gain. built-in cross-validationThis is unlike GBM where we have to run a grid-search and only a limited values can be tested. continue on existing model XGBoost Parametersgeneral parametersGeneral Parameters: Guide the overall functioning booster:default =gbtree, can be gbtree, gblinear or dart. 一般使用gbtree. silent:default =0, silent mode is activated if set to 1(no running messages will be printed) nthread:default to maximum of threads. booster parametersBooster Parameters: Guide the individual booster (tree/regression) at each step eta(learning rate):default=0.3, typical final values to be used: 0.01-0.2, using CV to tune min_child_weight:minimum sum of instance weight (hessian) needed in a child. If the tree partition step results in a leaf node with the sum of instance weight less than min_child_weight, then the building process will give up further partitioning. In linear regression mode, this simply corresponds to minimum number of instances needed to be in each node. The larger, the more conservative the algorithm will be.default =1,too high values can lead to under-fitting, it should be tuned using CV. 数值越小越容易过拟合，越大越容易 under-fitting. max_depth:default =6, typical values: 3-10, should be tuned using CV. gamma:default =0, Gamma specifies the minimum loss reduction required to make a split.如果在分裂过程中小于该值，那么就不会继续分裂。 subsample:default =1, typical values: 0.5-1. Denotes the fraction of observations to be randomly samples for each tree. colsample_bytree:default =1, typical values: 0.5-1. colsample_bytree和subsample不同点：colsample_by是特征的随机fraction, subsample是rows的随机fraction。 lambda:default =1, L2 regularization term on weights(analogous to Ridge regression). Though many data scientists don’t use it often, it should be explored to reduce overfitting. alpha:default =0, L1 regularization term on weight (analogous to Lasso regression) scale_pos_weight:default =1, a value greater than 0 should be used in case of high class imbalance as it helps in faster convergence. learning task parametersLearning Task Parameters: Guide the optimization performed objectivebinary: logistic- returns predicated probability(not class)multi: softmax- returns predicated class(not probabilities)multi: softprob- returns predicated probability of each data point belonging to each class. eval_metircdefault according to objective(rmse for regression and error for classification), used for validation data.typical values: rmse(root mean square error), mse(mean absolute error), logloss(negative log-likelihood), error(binary classification error rate, 0.5 threshold), auc(area under the curve) seeddefault =0, used for reproducible results and also for parameter tuning. Control OverfittingThere are in general two ways that you can control overfitting in xgboost. The first way is to directly control model complexity. The second way is to add regularization parameters Referrencecomplete guide parameter tuning xgboost with codes python官方文档 param_tuning官方文档 parameter 补充一些理论知识 LightGBM 和 XGBoost 的一些区别 树的增长方式 这个原先是lightGBM所特有的，然后xgboost 在最新的版本上也实现该中方式，这估计就是开源，并不是一成不变的。两者都是基于叶子进行增长的，但是增长的方式是不同的。一种是 level-wise 一种是 leaf-wiseboth xgboost and lightGBM use the leaf-wise growth strategy when growing the decision tree.there are two strategies that can be employed: level-wise and leaf-wise. level-wise maintains a balanced tree（平衡树，左右子树的高度差不超过1）;但是 leaf-wise 这个就比较随意了。这个是这两者的区别：当叶子总数相同的时候，leaf-wise 这种生长方式得到的树的深度是大于 level-wise 的深度的。Compared to the case of level-wise growth, a tree grown with leaf-wise growth will be deeper when the number of leaves is the same. find the best split The key challenge in training a GBDT is the process of finding the best split for each leaf. The computational complexity is thus $O \left( n _ { \text {data} } n _ { \text {features} } \right)$.这两者采用的方式都是：现阶段的中的的数据 数据量和特征量都是很大的，所以这种方式是不可取的。然后这两种方法都是采用了 Histogram-based methods，这样最后的时间复杂度降低到：reducing the computational complexity to $O \left( n _ { d a t a } n _ { b i n s } \right)$. 这个复杂度是取决于number of bins。这个是引入了一个超参数，number of bins ，trade off between precision and time, 当更多的 bins 的时候，这个precision 会提高，但是 time 也会增大。 Ignoring sparse inputs 这个是处理缺省值（或者 0）的手段：两者在split 分裂点的时候，都是先不处理数值 0；然后找到分裂点之后，把0 放到哪边造成的loss 下降的比较大，然后就放到哪边。 Subsampling the data: Gradient-based One-Side Sampling (lightGBM)biased sampling(抽样的基本原则是随机性，但是在抽样过程中由于一系列因素造成偏差抽样，造成样本是不符合真实样本的分布)这个就是缓解，也不是为了彻底让其均衡，lightgbm increases the weigths of the samples.This means that it is more efficient to concentrate on data points with larger gradients.In order to mitigate this problem, lightGBM also randomly samples from data with small gradients.lightGBM increases the weight of the samples with small gradients when computing their contribution to the change in loss (this is a form of importance sampling, a technique for efficient sampling from an arbitrary distribution).]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>XGBoost</tag>
        <tag>LightGBM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[那些年的算法题目（二）]]></title>
    <url>%2F2018%2F07%2F21%2F%E9%82%A3%E4%BA%9B%E5%B9%B4%E7%9A%84%E7%AE%97%E6%B3%95%E9%A2%98%E7%9B%AE%EF%BC%88%E4%BA%8C%EF%BC%89%2F</url>
    <content type="text"><![CDATA[最长01相同子串已知一个长度为N的字符串，只由0和1组成， 求一个最长的子串，要求该子串出现0和1的次数相等。思路：最简单的方式是先生成字串，然后判断每个字串是否满足0的个数和1的个数相同。这种暴力求解时间复杂度O(n^3),明显是不合理的。下面说一下简单的做法：定义一个数组B[N]，B[i]表示从A[0…i]中 num_of_0 - num_of_1，0的个数与1的个数的差 。那么如果A[i] ~ A[j]是符合条件的子串，一定有 B[i] == B[j]，因为中间的部分0、1个数相等，相减等于0。 时间复杂度：O(n)，空间复杂度：O(n) 算法的思路是一样的。 Following is a solution that uses O(n) extra space and solves the problem in O(n) time complexity.Let input array be arr[] of size n and maxsize be the size of output subarray.1) Consider all 0 values as -1. The problem now reduces to find out the maximum length subarray with sum = 0.2) Create a temporary array sumleft[] of size n. Store the sum of all elements from arr[0] to arr[i] in sumleft[i]. This can be done in O(n) time.3) There are two cases, the output subarray may start from 0th index or may start from some other index. We will return the max of the values obtained by two cases.4) To find the maximum length subarray starting from 0th index, scan the sumleft[] and find the maximum i where sumleft[i] = 0.5) Now, we need to find the subarray where subarray sum is 0 and start index is not 0. This problem is equivalent to finding two indexes i &amp; j in sumleft[] such that sumleft[i] = sumleft[j] and j-i is maximum. To solve this, we can create a hash table with size = max-min+1 where min is the minimum value in the sumleft[] and max is the maximum value in the sumleft[]. The idea is to hash the leftmost occurrences of all different values in sumleft[]. The size of hash is chosen as max-min+1 because there can be these many different possible values in sumleft[]. Initialize all values in hash as -16) To fill and use hash[], traverse sumleft[] from 0 to n-1. If a value is not present in hash[], then store its index in hash. If the value is present, then calculate the difference of current index of sumleft[] and previously stored value in hash[]. If this difference is more than maxsize, then update the maxsize.7) To handle corner cases (all 1s and all 0s), we initialize maxsize as -1. If the maxsize remains -1, then print there is no such subarray. 代码实现12345678910111213141516171819202122232425262728def lengest01SubStr(s): &apos;&apos;&apos; 最长0,1 相等的子串长度 &apos;&apos;&apos; count =[0, 0] B =[0]*len(s) dic =&#123;&#125; # 保存 0 1 的差值 lengest =0 for i in range(len(s)): count[int(s[i])] +=1 B[i] =count[0] - count[1] # start from 0th index if B[i] ==0: lengest +=1 continue if B[i] in dic: # i -dic[B[i]] , not from 0th index lengest =max(lengest, i- dic[B[i]]) else: dic[B[i]] =i return lengesta =&apos;1011010&apos;b =&apos;10110100&apos;print(lengest01SubStr(a)) # 6 # &apos;011010&apos;print(lengest01SubStr(b)) # 8 # &apos;10110100&apos; 顺时针打印矩阵输入一个矩阵，按照从外向里以顺时针的顺序依次扫印出每一个数字。思路：发现网上有很多使用递归的，但是使用四个循环就可以解决这个问题。找到每次开始的起点，然后按照最上面一行，最右面一列，最小面一行和最左面一行这样的顺序进行打印即可。 思路：找到左上角的，一个start_point， 然后根据这个点进行上下左右的循环。 123456789101112131415161718192021222324252627282930313233343536373839404142def printMatrix(matrix): if matrix ==[[]]: return # 第一次见这样判断空的matrix row =len(matrix) column =len(matrix[0]) # 这里的left, right, up, down 都是真实能够access到数据的 left =0 right =column -1 up =0 down =row -1 res =[] while right &gt;left and up &lt;down: # from left to right for i in range(left, right+1): res.append(matrix[up][i]) # from up to down for i in range(up+1, down+1): res.append(matrix[i][right]) # from right to left for i in range(right-1, left-1, -1): res.append(matrix[down][i]) for i in range(down-1, up, -1): res.append(matrix[i][left]) left +=1 right -=1 up +=1 down -=1 # 最后对于这种特殊情况的处理是容易忘记的 # left one row 这种情况很特殊，只是从左往右遍历 if up ==down and left &lt;right: for i in range(left, right+1): res.append(matrix[up][i]) # left one column 只有可能是从上往下遍历 if left ==right and up &lt;down: for i in range(up, down+1): res.append(matrix[i][left]) if up ==down and left ==right: res.append(matrix[left][up]) return resprint(printMatrix(matrix)) 下面这个版本并没有运行成功，但是中间有个语法点是可以学习的。12345678910111213141516171819202122232425262728293031323334353637def printMatrix(matrix): res =[] # 第一个坐标表示行数，第二个坐标表示列数 # m 表示行数，n 表示列数 m =len(matrix) n = len(matrix[0]) if m ==1 and n ==1: res =[matrix[0][0]] return res else: for o in range(int((min(m,n)+1)/2)): # 不加这个[] 会有语法错误 [res.append(matrix[o][i]) for i in range(o, n-o)] [res.append(matrix[j][n-o-1]) for j in range(o, m-o) if matrix[j][n-o-1] not in res ] # 感觉这个最后的not in 是不合理的，万一这两个数字就是相同呢，试一下 [res.append(matrix[m-o-1][k]) for k in range(n-1, o-1, -1) if matrix[m-o-1][k] not in res] [res.append(matrix[l][o]) for l in range(m-1-o, o-1,-1) if matrix[l][o] not in res] # 这种写法还是少用，可以使用append,就少用这种，可读性不强，容易错 return resdef printMatrix(matrix): res =[] # 第一个坐标表示行数，第二个坐标表示列数 # m 表示行数，n 表示列数 m =len(matrix) n = len(matrix[0]) if m ==1 and n ==1: res =[matrix[0][0]] return res else: for o in range(int((min(m,n)+1)/2)): # 不加这个[] 会有语法错误 [res.append(matrix[o][i]) for i in range(o, n-o)] [res.append(matrix[j][n-o-1]) for j in range(o, m-o) if matrix[j][n-o-1] not in res ] # 感觉这个最后的not in 是不合理的，万一这两个数字就是相同呢，试一下 [res.append(matrix[m-o-1][k]) for k in range(n-1, o-1, -1) if matrix[m-o-1][k] not in res] [res.append(matrix[l][o]) for l in range(m-1-o, o-1,-1) if matrix[l][o] not in res] # 这种写法还是少用，可以使用append,就少用这种，可读性不强，容易错 return res]]></content>
      <categories>
        <category>算法</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Loss Activation and Optimisation Function]]></title>
    <url>%2F2018%2F07%2F07%2FLoss-Activation-and-Optimisation-Function%2F</url>
    <content type="text"><![CDATA[主要介绍 activation function， loss function 等等概念和分类。 Activation FunctionWhat? It’s just a thing (node) that you add to the output end of any neural network. It is also known as Transfer Function. It can also be attached in between two Neural Networks. $$Output = activation function \left( x _ { 1 } w _ { 1 } + x _ { 2 } w _ { 2 } + \cdots + x _ { n } w _ { n } + b i a s \right)$$ A weighted sum is computed as:$$x _ { 1 } w _ { 1 } + x _ { 2 } w _ { 2 } + \cdots + x _ { n } w _ { n }$$Then, the computed value is fed into the activation function, which then prepares an output.$$activation function \left( x _ { 1 } w _ { 1 } + x _ { 2 } w _ { 2 } + \cdots + x _ { n } w _ { n } + b i a s \right)$$ Think of the activation function as a mathematical operation that normalises the input and produces an output. The output is then passed forward onto the neurons on the subsequent layer. 作用：增加非线性 The thresholds are pre-defined numerical values in the function. This very nature of the activation functions can add non-linearity to the output. Activation Function Types Linear Activation Function: $$ output = k * x$$where $k$ is a scalar value, as an instance 2, and $x$ is the input. Sigmoid or Logistic Activation Function The sigmoid activation function is “S” shaped. It can add non-linearity to the output and returns a binary value of 0 or 1. $$Output = \frac { 1 } { 1 + e ^ { - x } }$$ 这个函数有一个很好的导数形式，在反向传播的时候，效果比较明显。 Tanh Activation Function Tanh is an extension of the sigmoid activation function. Hence Tanh can be used to add non-linearity to the output. The output is within the range of -1 to 1. Tanh function shifts the result of the sigmoid activation function: $$\text { Output } = \frac { 2 } { 1 + e ^ { - 2 x } } - 1$$ Rectified Linear Unit Activation Function (RELU) RELU is one of the most used activation functions. It is preferred to use RELU in the hidden layer. The concept is very straight forward. It also adds non-linearity to the output. However the result can range from 0 to infinity. $$ Output = \max ( 0 , x )$$这个是很高的评价了。If you are unsure of which activation function you want to use then use RELU. Softmax Activation Function Softmax is an extension of the Sigmoid activation function. Softmax function adds non-linearity to the output, however it is mainly used for classification examples where multiple classes of results can be computed. $$Output = \frac { e ^ { x } } { \operatorname { sum } \left( e ^ { x } \right) }$$ 这个一般使用在最后，作为多分类的结束。 Loss Function(Error Function)what? 衡量模型好坏的 function，如果模型表现好，那么loss 应该是小；如果模型表现不好，那么loss 应该是大的。 At its core, a loss function is incredibly simple: it’s a method of evaluating how well your algorithm models your dataset. If your predictions are totally off, your loss function will output a higher number. If they’re pretty good, it’ll output a lower number. As you change pieces of your algorithm to try and improve your model, your loss function will tell you if you’re getting anywhere. Log Loss (Cross Entropy Loss) Cross-entropy loss, or log loss, measures the performance of a classification model whose output is a probability value between 0 and 1. Cross-entropy loss increases as the predicted probability diverges from the actual label. So predicting a probability of .012 when the actual observation label is 1 would be bad and result in a high loss value. A perfect model would have a log loss of 0. The graph above shows the range of possible loss values given a true observation (isDog = 1). As the predicted probability approaches 1, log loss slowly decreases. As the predicted probability decreases, however, the log loss increases rapidly. Log loss penalizes both types of errors, but especially those predictions that are confident and wrong! Cross-entropy and log loss are slightly different depending on context, but in machine learning when calculating error rates between 0 and 1 they resolve to the same thing. In binary classification, where the number of classes M equals 2, cross-entropy can be calculated as: $$- ( y \log ( y ) + ( 1 - y ) \log ( 1 - y ) )$$ If $ M&gt;$2 (i.e. multiclass classification), we calculate a separate loss for each class label per observation and sum the result. $$- \sum _ { c = 1 } ^ { M } y _ { o , c } \log \left( y _ { o , c } \right)$$ M - number of classes (dog, cat, fish)log - the natural logy - binary indicator (0 or 1) if class label c is the correct classification for observation o 想要表达的是 log loss 是从 Likelihood Loss，改进过来的，有没有发现最大似然的痕迹。log loss 表达式如下：$$\begin{split}P(Y | X) &amp;= P(X_1 | Y) \times P(X_2 | Y) \times \dots \times P(X_n | Y) \times P(Y) = P(Y) \prod_{i}^{n} P(X_i | Y) \\&amp;\Rightarrow log(P(Y | X)) = log(\prod_{i}^{n} P(X_i | Y) \Rightarrow \sum_{i}^{n} log(P(X_i | Y))\end{split}$$交叉熵表达式：$$CE(\hat{y}, y) = - \sum_{i=1}^{n} y_i log(\hat{y}) + (1 - y_i) log(1 - \hat{y})$$ Mean Squared Error 在回归中使用 Mean Squared Error (MSE), or quadratic, loss function is widely used in linear regression as the performance measure, and the method of minimizing MSE is called Ordinary Least Squares (OSL)。 To calculate MSE, you take the difference between your predictions and the ground truth, square it, and average it out across the whole dataset. $$Loss = \frac { 1 } { n } \sum _ { i = 1 } ^ { n } \left( y ^ { ( i ) } - \hat { y } ^ { ( i ) } \right) ^ { 2 }$$ Mean Absolute Error Mean Absolute Error (MAE) is a quantity used to measure how close forecasts or predictions are to the eventual outcomes, which is computed by $$Loss = \frac { 1 } { n } \sum _ { i = 1 } ^ { n } \left| y ^ { ( i ) } - \hat { y } ^ { ( i ) } \right|$$ where $| .|$ denotes the absolute value. Albeit, both MSE and MAE are used in predictive modeling, there are several differences between them. MSE has nice mathematical properties which makes it easier to compute the gradient. However, MAE requires more complicated tools such as linear programming to compute the gradient. Because of the square, large errors have relatively greater influence on MSE than do the smaller error. Therefore, MAE is more robust to outliers since it does not make use of square. On the other hand, MSE is more useful if concerning about large errors whose consequences are much bigger than equivalent smaller ones. MSE also corresponds to maximizing the likelihood of Gaussian random variables. L2 这两个loss function 在这里介绍过，所以本博客中简单说一下。 L2 loss function is the square of the L2 norm of the difference between actual value and predicted value. It is mathematically similar to MSE, only do not have division byn, it is computed by $$ Loss = \sum _ { i = 1 } ^ { n } \left( y ^ { ( i ) } - \hat { y } ^ { ( i ) } \right) ^ { 2 }$$ Kullback Leibler (KL) Divergence（计算的是两个分布的问题）KL Divergence, also known as relative entropy, information divergence/gain, is a measure of how one probability distribution diverges from a second expected probability distribution. KL divergence loss function is computed by$$D _ { K L } ( p | q ) = \sum _ { x } p ( x ) \log \frac { p ( x ) } { q ( x ) }$$ 交叉熵的定义：$$H ( p , q ) = - \sum _ { x } p ( x ) \log q ( x )$$两者的关系推导，$$\begin{split}D _ { K L } ( p | q ) &amp;= \sum _ { x } p ( x ) \log \frac { p ( x ) } { q ( x ) } \\&amp;= \sum _ { x } ( p ( x ) \log p ( x ) - p ( x ) \log q ( x ) ) \\&amp;= - H ( p ) - \sum _ { x } p ( x ) \log q ( x ) \\&amp;= - H ( p ) + H ( p , q )\end{split}$$所以说， cross entropy 也是可以写成这样：$$H ( p , q ) = D _ { K L } ( p | q ) + H ( p )$$ logistic loss 和 cross entropy的关系 当 $ p \in { y , 1 - y }$, $q \in { \hat { y } , 1 - \hat { y } }$ ，cross entropy 可以写成 logistic loss: $$H ( p , q ) = - \sum _ { x } p ( x ) \log q ( x ) = - y \log \hat { y } - ( 1 - y ) \log ( 1 - \hat { y } )$$]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>优化</tag>
        <tag>Loss Function</tag>
        <tag>Activation Function</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[特征工程]]></title>
    <url>%2F2018%2F06%2F29%2F%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[关于特征工程的概念，特征离散化、特征生成、组合特征、特征选取的方法。 特征生成机器学习的一大任务在于手工 create 特征。特征主要来源有两个，一个统计学知识，一个该场景下的特征。前者属于常规操作，后者需要有对业务领域比较熟悉的了解。后者举个例子，比如在衡量个人信用的时候， days_employed / days_birth 就是一个属于金融（保险）领域的较为熟悉，才能理解的一个特征, 这种百分比，占比的思想还是非常常见的。前者同样举个栗子，对于子表（传统机器学习还是有很多来自数据库的信息的）常用的操作是 aggregation 操作（min max sum variance mean）等聚合操作，然后和主表进行连接。一般来说子表是可以广泛的使用 aggregation 操作，但是对于主表来说，这个就取决于信息是什么，特征是什么内容，聚合函数的本质在于”总结“，就是你操作的变量是否有必要这样做，看一下数据是不是流水账。 特征离散化？连续化特征就是一些不可枚举的有理数。那么什么是离散化特征呢？ 离散化特征就是可枚举的特征。离散化的作用是把数据变成可计算状态。而特征工程就是从原始字段中根据业务提取出对模型有效的特征出来。 在线性模型下(w.x)，w已经确定的情况下，x的某个特征的值是20，或者30，w.x的值相差是很大的，但是往往20岁的人跟30岁的人对同一个广告的兴趣差距不会那么大。连续特征离散化的基本假设，是默认连续特征不同区间的取值对结果的贡献是不一样的。离散化和连续化最大的区别是，对一个字段做连续化后的结果就还只是一个特征，而离散化后的这一列有多少个key(字段可能的值)就会抽取出多少个特征。当经过离散化之后，特征各有各的权重，彼此之间就没有关系了。 模型是使用离散特征还是连续特征, 其实是一个“海量离散特征+简单模型” 同 “少量连续特征+复杂模型”的权衡。既可以离散化用线性模型，也可以用连续特征加深度学习。 常用的选取离散点的方法：等距离离散，等样本离散、画图观察趋势和决策树模型(天生就可以对连续特征分段)。 在工业界，很少直接将连续值作为特征喂给逻辑回归模型，而是将连续特征离散化为一系列0、1特征交给逻辑回归模型，这样做的优势有以下几点： 单变量离散化为N个后，每个变量有单独的权重，在激活函数的作用下相当于为模型增加了非线性，能够提升模型表达能力，加大拟合。 离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄&gt;30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰，因为特征值的异常会导致权重也就是w的值也会异常。 一定有同学担心特征过多会导致运算缓慢，但是LR是线性模型，我们在内部计算的时候是向量化计算，而不是循环迭代。稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展。 所以不用担心像GBDT算法那样，特征多了就跑不动了(我们都说GBDT不能用离散特征不是因为它处理不了离散特征，而是因为离散化特征后会产生特别多的特征，决策树的叶子节点过多，遍历的时候太慢了)。 所以海量离散特征＋LR是业内常见的一个做法。而少量连续特征+复杂模型是另外一种做法，例如GBDT。 当然也可以通过深度学习来进行特征选择：目前这种手段正在随着深度学习的流行而成为一种手段，尤其是在计算机视觉领域，原因是深度学习具有自动学习特征的能力，这也是深度学习又叫unsupervised feature learning的原因。从深度学习模型中选择某一神经层的特征后就可以用来进行最终目标模型的训练了。 参考文献:https://blog.csdn.net/lujiandong1/article/details/52412123 组合特征先是离散化，然后是特征组合。交叉从理论上而言是为了引入特征之间的交互，也即为了引入非线性。LR(逻辑回归）分类算法:因为线性函数的表达能力有限，所以我们引入激活函数就是给LR增加非线性关系。能让一条直线变成曲线。这样可以拟合出更好的效果。（也由此才后了后来说的过拟合问题而引入了正则化超参数） LR模型之所以很受欢迎，主要是因为LR模型本质是对数线性模型，实现简单，易于并行，大规模扩展方便，迭代速度快，同时使用的特征比较好解释，预测输出在0与1之间契合概率模型。（模型的可解释性举例，比如A-B的权重比较大，A代表用户，B代表物品，那么可以认为A是对B比较感兴趣的）但是，线性模型对于非线性关系缺乏准确刻画，特征组合正好可以加入非线性表达，增强模型的表达能力。另外，广告LR中，基本特征可以认为是用于全局建模，组合特征更加精细，是个性化建模，因为在这种大规模离散LR中，单对全局建模会对部分用户有偏，对每一用户建模又数据不足易过拟合同时带来模型数量爆炸，所以基本特征+组合特征兼顾了全局和个性化。 从统计的角度解释，基本特征仅仅是真实特征分布在低维的映射，不足以描述真实的分布，加入特征在高维空间拟合真实分布，使得预测更加准确。 正则化真正测试一个模型的不是简单与否，更重要在于它在预测新的情况时表现如何。小权重意味着网络的行为不会因为我们随意更改了一些输入而改变太多。这是我们加了正则化之后的成本函数，可以看我们后面加入了正则化 λ 的表达式来完善成本函数。为什么加入λ能够减轻过拟合呢？直观一点的解释是设置的λ值越大，那么参数w的值就会被压缩的越小(在梯度下降中, 每次迭代的步长，也就是这个公式w=w - 学习率*成本函数对w的导数， 现在由于成本函数增加了正则项，使得J和w变得数值相关了)。 假设λ设置的足够大，那么w会无限的趋近于0. 把多隐藏层的单元的权重设置为0以后，那么基本上就是消除掉了这些单元的作用，而使得网络模型得到简化，就像下面的图一样。由于正则化的设置，消除了一些隐藏单元的作用。而使得整个模型越来越接近于线性化，也就是从下图中的过拟合往欠拟合偏转。当然我们有一个适合的λ的值，能让我们的拟合状态达到最佳。所以我们在训练模型的时候，往往都会有一个Ｌ２正则项的超参数需要我们设置。 这是我们的tanh激活函数， 可以看到当z的值越大时，整个函数的非线性就越大，而z的值越小(图中红色加粗部分),函数就越是呈现出线性分布。 所以当我们增加λ的值， w得值就越小，相应的z的值也就越小。因为z = wx + b。 而我们第一次说激活函数的时候就说过神经网络中基本上是不使用线性函数作为激活函数的，因为不论有多少层，多少个单元，线性激活函数会使得所有单元所计算的都呈现线性状态。 feature selectionfeature selection 的过程就是dimension reduction的过程。就是说由较多的数据集 映射到 较少的数据集，这种方式就叫做降维。 Using dimensionality reduction techniques, of course. You can use this concept to reduce the number of features in your dataset without having to lose much information and keep (or improve) the model’s performance. 必要性分析：时间角度，空间（内存）角度。减少冗余信息，就减少了模型去拟合”噪声“数据的可能性。 常见的有以下几种方式。 Remove collinear features具体说来 显示 corr,然后上三角形，然后根据阈值筛选相关性比较强的特征，最后drop。 PCA之类的，解决的是feature 个数的问题。 Remove features with greater than a threshold percentage of missing values根据isnull() 计算百分比，然后排序比较， 解决的是feature 个数的问题。 Keep only the most relevant features using feature importances from a model根据feature importance 进行feature selection， 解决的feature 个数的问题。 根据数值型数据本身变化，如果variance 很小，则舍去。因为网络本身就是学习的该特征的波动是如何影响最后的结果，所以如果数据本身没有波动性，那么是没有什么价值的。解决的是feature 个数的问题。 插一句，如何 handle 单个特征本身表示长度的问题，思考一下 word2vec 是怎样进行操作的。 Principal Component Analysis (PCA)这个属于一句话的的思想：PCA的问题其实是一个基的变换，使得变换后的数据有着最大的方差。 pca 的假设：这个说得很清楚，higher dimensional space is mapped to lower dimension, 然后这种在lower dimensional 中数据的variance应该是保持max的。It works on a condition that while the data in a higher dimensional space is mapped to data in a lower dimension space, the variance of the data in the lower dimensional space should be maximum. 就是需要理解这个主成分是不断的生成的，在前者的基础之上生成的。below are some of the key points you should know about PCA before proceeding further: A principal component is a linear combination of the original variables Principal components are extracted in such a way that the first principal component explains maximum variance in the dataset Second principal component tries to explain the remaining variance in the dataset and is uncorrelated to the first principal component Third principal component tries to explain the variance which is not explained by the first two principal components and so on The first component is the most important one, followed by the second, then the third, and so on. SVDSVD decomposes the original variables into three constituent matrices. It is essentially used to remove redundant features from the dataset. It uses the concept of Eigenvalues and Eigenvectors to determine those three matrices. We will not go into the mathematics of it due to the scope of this article, but let’s stick to our plan, i.e. reducing the dimensions in our dataset. $$\operatorname { Data } _ { m \times n } = U _ { m \times m } \Sigma _ { m \times n } V _ { n \times n } ^ { T }$$SVD将原始的数据集矩阵Data分解成三个矩阵：U、Sigma、VT，如果原始矩阵是m行n列，那么U、Sigma和VT分别就是m行m列、m行n列、n行n列。比较值得一提的是矩阵Sigma，该矩阵只有对角元素，其他元素均为0，有一个惯例是：Sigma的对角元素是从大到小排列的。这些对角元素就称为奇异值. 投影也是一种降维手段这种思想真的是服气，虽然我也不是很懂，但是思想是很好的By projecting one vector onto the other, dimensionality can be reduced.By projecting one vector onto the other, dimensionality can be reduced.讲解了什么是 manifold 这个概念，还是很好的These small portions where the Earth looks flat are manifolds, T-sne就是指出 t-sne 这个可以非线性降维。有local approaches 和 global approaches 两种方式，我大概的理解就是：类之间还能尽可能的远离，类内保持差异性。（不知道对不对）So far we have learned that PCA is a good choice for dimensionality reduction and visualization for datasets with a large number of variables. But what if we could use something more advanced? What if we can easily search for patterns in a non-linear way? t-SNE is one such technique. There are mainly two types of approaches we can use to map the data points:Local approaches : They maps nearby points on the manifold to nearby points in the low dimensional representation.Global approaches : They attempt to preserve geometry at all scales, i.e. mapping nearby points on manifold to nearby points in low dimensional representation as well as far away points to far away points. 杂货铺 对于encodding 的理解，从字符转成数字，并且尽可能的保留原来的信息。接触的有三种，一种 label encoding，适合类别信息只有两类。如果大于两类那么就使用one-hot。第三种就是万物可以embedding，使用神经网络的思想。 特征工程可以分为特征处理、Feature Selection（特征选择）、Feature Extraction（特征提取）和Feature construction（特征构造）等阶段。归一化（去中心，方差归一）是属于特征(预)处理:把特征值压缩成0~1的区间。 One-hot（也叫One-of-k）的方法把每个无序特征转化为一个数值向量。比如一个无序特征color有三种取值：red，green，blue。那么可以用一个长度为3的向量来表示它，向量中的各个值分别对应于red，green，blue。如：这种方法在NLP里用的很多，就是所谓的词向量模型。变换后的向量长度对于词典长度，每个词对应于向量中的一个元素。 对于欠拟合: 增加神经网络复杂度，出现欠拟合的原因之一是由于函数的非线性不足，所以用更复杂的网络模型进行训练来加深拟合。对于过拟合：增加数据规模， 出现过拟合的原因之一是数据规模不足而造成的数据分布不均，扩展数据规模能比较好的解决这个问题。当然另一个做法是正则化，我们采取使用正则化来解决过拟合问题，常用的是L2正则，其他的还有L1和 Dropout正则。 很多人会想着既然线性分类器搞不定，那就直接找个非线性的好了，比如高斯核的SVM。我们确实可以通过这种简单换算法的方式解决这个简单的问题。但对于很多实际问题（如广告点击率预测），往往特征非常多，这时候时间约束通常不允许我们使用很复杂的非线性分类器。这也是为什么算法发展这么多年，广告点击率预测最常用的方法还是Logistic Regression (LogisticReg)。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>PCA</tag>
        <tag>SVD</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常见的排序算法总结]]></title>
    <url>%2F2018%2F06%2F29%2F%E5%B8%B8%E8%A7%81%E7%9A%84%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[分类和总结 根据待排序的数据大小不同，使得排序过程中所涉及的存储器不同，可分为内部排序和外部排序。 排序关键字可能出现重复，根据重复关键字的排序情况可分为稳定排序和不稳定排序。 对于内部排序，依据不同的排序原则，可分为插入排序、交换(快速)排序、选择排序、归并排序和计数排序。 针对内部排序所需的工作量划分，可分为:简单排序 O(n^2)、先进排序 O(nlogn)和基数排序 O(d*n)。常见算法的性质总结： 排序算法实现默认都是升序… 插入排序(Insert Sort)思想： 有序数组+insert one every one time 插入排序的工作方式非常像人们排序一手扑克牌一样。开始时，我们的左手为空并且桌子上的牌面朝下。然后，我们每次从桌子上拿走一张牌并将它插入左手中正确的位置。为了找到一张牌的正确位置，我们从右到左将它与已在手中的每张牌进行比较，如下图所示：步骤： 从第一个元素开始，该元素可以认为已经被排序 取出下一个元素，在已经排序的元素序列中从后向前扫描 如果该元素（已排序）大于新元素，将该元素移到下一位置 重复步骤3，直到找到已排序的元素小于或者等于新元素的位置 将新元素插入到该位置后 重复步骤2~5 123456789101112131415def insert_sort(lists): count = len(lists) for i in range(1, count): key =lists[i] j =i-1 while j &gt;= 0: if lists[j] &gt;key: lists[j+1] =lists[j] lists[j] =key j -= 1 return lists# testlists =[1,2,-3, 90,34]print(insert_sort(lists)) 选择排序(Select Sort)思想和步骤： select one every time 简单选择排序是最简单直观的一种算法，基本思想为每一趟从待排序的数据元素中选择最小（或最大）的一个元素作为首元素，直到所有元素排完为止，简单选择排序是不稳定排序。 分析： 无论什么数据进去都是 O(n²) 的时间复杂度。所以用到它的时候，数据规模越小越好。唯一的好处可能就是不占用额外的内存空间了吧。 代码实现: 12345678910111213def select_sort(lists): count =len(lists) for i in range(0, count): min =i for j in range(i+1, count): if lists[min] &gt;lists[j]: min =j lists[min], lists[i] =lists[i], lists[min] return lists# testlists =[1, 23,45, 0,-1]print(select_sort(lists)) 冒泡排序(Bubble Sort)思想： move smallest one time (假设不减) 有两种说法 从右往左： 最小值被移到了最左边。 【冒泡法】的本意 从左往右： 最大值被移到了最右边。 此时其实应该叫 【沉石法】 从左向右 沉石法 图解： 分析：平均时间复杂度：O(n^2)最坏空间复杂度： 总共 O(n)，需要辅助空间 O(1) 1234567891011def bubble_sort(lists): count =len(lists) for i in range(0, count): for j in range(i+1, count): if lists[i]&gt; lists[j]: lists[i], lists[j] =lists[j], lists[i] return lists# testlists =[1,34,45,0,89]print(bubble_sort(lists)) 归并排序(Merge Sort)思想： divide-and-conquer +递归 归并排序（MERGE-SORT）是利用归并的思想实现的排序方法，该算法采用经典的分治（divide-and-conquer）策略（分治法将问题分(divide)成一些小的问题然后递归求解，而治(conquer)的阶段则将分的阶段得到的各答案”修补”在一起，即分而治之)。 分析：时间复杂度：O(nlogn)空间复杂度：O(N)，归并排序需要一个与原数组相同长度的数组做辅助来排序 1234567891011121314151617181920212223242526def merge(left, right): i, j =0,0 result =[] while i&lt;len(left) and j &lt;len(right): if left[i] &lt;= right[j]: result.append(left[i]) i +=1 else: result.append(right[j]) j +=1 result += left[i:] result += right[j:] return resultdef merge_sort(lists): if len(lists) &lt;=1: return lists num =int(len(lists)/2) left =merge_sort(lists[:num]) right = merge_sort(lists[num:]) return merge(left, right)# merge_sort 是先切分，然后再整合，quick sort 是两个指针# testlists =[1, 34, 23,45,0,9]print(merge_sort(lists)) 快速排序(Quick Sort)思想：任意选择一个key(通常选择a[0])，将比他小的数据放在它的前面，比他大的数字放在它的后面。递归进行。 步骤： 从数列中挑出一个基准值。 将所有比基准值小的摆放在基准前面，所有比基准值大的摆在基准的后面(相同的数可以到任一边)；在这个分区退出之后，该基准就处于数列的中间位置。 递归地把”基准值前面的子数列”和”基准值后面的子数列”进行排序。 分析：快速排序的时间复杂度在最坏情况下是O($N^2$)，平均的时间复杂度是O(N*logN)，采用的是分治的思想，二叉树的结构。 下面以数列 a={30,40,60,10,20,50} 为例，演示它的快速排序过程(如下图)。 这个是经过一个迭代的结果，每一个迭代，都排好了基准数字的位置。按照同样的方法，对子数列进行递归遍历。最后得到有序数组！ 12345678910111213141516171819202122232425def quick_sort(lists, left, right): if left &gt;= right: return lists key =lists[left] low =left high =right while left &lt; right: # 因为你最初key 取得是left，然后从右边找到一个比key小的，然后替换left 的位置 while left &lt;right and lists[right]&gt;=key: right -= 1 lists[left] =lists[right] while left &lt; right and lists[left] &lt;= key: left +=1 lists[right] =lists[left] lists[left] =key # 这里的left 和right 都是可以的，因为从while 中出来之后两者是相同的 # 这个步伐是1,所以只能是一个个变化 quick_sort(lists, low, left-1) quick_sort(lists, left+1, high) # 因为left的位置已经被占了，所以只是划分左边一块，右边一块就是可以的 return lists# testlists =[3,2,45, 100,1,56,56]#lists =[1,2,3,2,2,2,5,4,2]print(quick_sort(lists, 0, len(lists)-1)) Kth Largest Element in an Array Find the kth largest element in an unsorted array. Note that it is the kth largest element in the sorted order, not the kth distinct element. Tips: 使用的是 快排的思想，最后的平均时间复杂度是O(N) ，所以是一个不错的算法。 kth 和 ”快排“ 实现的时候稍微有一些区别，前者只有发现 一组数据（一个比pivot 大，一个比 pivot 小）才进行交换，后者是是 两个while， 只要发现一个就进行交换。这个是细微的差别。 123456789101112131415161718192021222324252627282930313233343536class Solution(object): def findKthLargest(self, nums, k): """ :type nums: List[int] :type k: int :rtype: int """ left, right = 0, len(nums) - 1 while True: pos = self.partition(nums, left, right) # 这个在排序的时候，是把大的数字放到前面，而前面是pos 是从0 开始的， # 所以这里是 k-1 if pos == k - 1: return nums[pos] # 左边的并不足以构成k 个， 那么在右边 elif pos &lt; k - 1: left = pos + 1 else: right = pos - 1 def partition(self, nums, left, right): # choose nums[left] as pivot pivot = nums[left] # p1, p2就类似 working 中的left right p1, p2 = left + 1, right while p1 &lt;= p2: if nums[p1] &lt; pivot and nums[p2] &gt; pivot: nums[p1], nums[p2] = nums[p2], nums[p1] p1, p2 = p1 + 1, p2 - 1 elif nums[p1] &gt;= pivot: p1 += 1 else: #nums[p2] &lt;= pivot: p2 -=1 nums[left], nums[p2] = nums[p2], nums[left] return p2 堆排序参看另外一篇博客 参考文献算法动图效果排序算法分类]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>排序算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[那些年的算法题目（一）]]></title>
    <url>%2F2018%2F06%2F22%2F%E9%82%A3%E4%BA%9B%E5%B9%B4%E7%9A%84%E7%AE%97%E6%B3%95%E9%A2%98%E7%9B%AE%EF%BC%88%E4%B8%80%EF%BC%89%2F</url>
    <content type="text"><![CDATA[完全二叉树插入问题描述已知一个完全二叉树的结构，现在需要将一个节点插入到这颗完全二叉树的最后，使得它还是一个完全二叉树。第一种解法：如果该树为满二叉树或者左子树不为满二叉树，那么就进入左子树，否则进入右子树，递归进行。 二叉树(Binary Tree)强行补充一下关于二叉树概念的知识。完全二叉树(Complete Binary Tree):若设二叉树的深度为h，除第h层外，其它各层(1～h-1)的结点数都达到最大个数，第h层所有的结点都连续集中在最左边，这就是完全二叉树。满二叉树:树中除了叶子节点，每个节点都有两个子节点。满二叉树是一种特殊的完全二叉树。二叉搜索树(binary search tree):所有非叶子结点至多拥有两个儿子（Left和Right）；所有结点存储一个关键字,非叶子结点的左指针指向小于其关键字的子树，右指针指向大于其关键字的子树。平衡二叉树(AVL树)：它是一颗空树或它的左右两个子树的高度差的绝对值不超过1。哈夫曼树：带权路径长度达到最小的二叉树，也叫做最优二叉树。树的深度和高度：深度是从上往下数；高度是从下往上数 代码实现平滑过渡到本问题的代码实现。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152#include&lt;iostream&gt;using namespace std;typedef struct Node&#123; int value; struct Node *lchild, *rchild;&#125;Tree;int GetLeftDepth(Tree* root)&#123; Tree* pNode =root-&gt;lchild ; int depth =0; while(pNode != NULL) &#123; depth ++; pNode =pNode-&gt;lchild; &#125; return depth;&#125;int GetRightDepth(Tree* root)&#123; Tree* pNode =root-&gt;rchild; int depth =0 ; while(pNode != NULL) &#123; depth ++ ; pNode =pNode-&gt;rchild ; &#125; return depth;&#125;bool IsFullBinaryTree(Tree* root)&#123; return GetLeftDepth(root) == GetRightDepth(root) ;&#125;void insert(Tree* root, Tree * node)&#123; if (IsFullBinaryTree(root) || !IsFullBinaryTree(root-&gt;lchild))&#123; insert(root-&gt;lchild, node); return ; &#125; if (root-&gt;rchild ==NULL)&#123; root-&gt;rchild =node ; return ; &#125; insert(root-&gt;rchild, node) ;&#125;int main()&#123; Node* a = new Node(); a-&gt;value =1;&#125; 第二种思路，如果已知之前树的个数，那么可以使用前序遍历的方式，得到将要插入的的节点的位置，然后插入。123456789101112int insert(Tree *t, int n, struct Node *node);# n表示原来二叉树节点的个数# 前序遍历 void printTree(Tree* root)&#123; if(root ==NULL) &#123; return ; &#125; print(root-&gt;value) printTree(root-&gt;lchild) printTree(root-&gt;rchild)&#125; 第三种思路：因为是完全二叉树，那么插入的点只能在最下一层。于是我们可以去找最下一层的中间点，找到根的左子树的最右下的节点，如果这个点存在，那么说明，最下一层的左边已经填满，递归右子树，否则递归左子树。 参考文献http://www.voidcn.com/article/p-kbxsvnyq-yq.htmlhttps://www.toutiao.com/i6192546626911126017/https://blog.csdn.net/psc0606/article/details/48742239 inplace 去除连续的 0给定一个一维整数数组，不使用额外的空间，本地去掉数组中连续的0。123456789101112131415161718192021222324252627282930#include&lt;iostream&gt;using namespace std;int RemoveDuplicates(int* sortBuffer,int length)&#123; if(sortBuffer == NULL || length == 0) &#123; return false; &#125; int count = 0; for(int i = 1; i &lt; length; i++) &#123; if(sortBuffer[i] ==0 &amp;&amp; 0 == sortBuffer[i-1]) &#123; continue; &#125; else &#123; sortBuffer[count]=sortBuffer[i]; count++; &#125; &#125; return count; &#125;int main()&#123; int length =sizeof(array)/sizeof(int); &#125; 最大连续子数组和已知一个整数二维数组，求最大的子数组和(子数组的定义从左上角(x0,y0) 到右下角(x1,y1)的数组)先考虑一维整数数组的情况。最大连续子序列的DP动态转移方程为 1234567891011121314151617181920212223242526#include&lt;iostream&gt;using namespace std;int Max(int a, int b)&#123; return a&gt;b ?a:b;&#125;int FindGreatestSubarray(int *arr, int n)&#123; int sum =arr[0]; int max =arr[0]; for(int i =1; i&lt;n; i++)&#123; sum =Max(sum+arr[i], arr[i]); max =Max(sum, max) /** if(sum &gt;=max)&#123; max =sum; &#125; */ &#125; return max;&#125;int main()&#123; return 0;&#125;讲解链接：http://kubicode.me/2015/06/23/Algorithm/Max-Sum-in-SubMatrix/ 本题目的要求是从二位的数组中求解最大的子矩阵。我们可以将其转化成一维数组的问题。如果是二维数组可以压缩为一维数组（我当时也是不懂这里）。如果最大子矩阵和原矩阵等高，就可以这样压缩。12345678910111213141516171819202122232425262728293031323334353637383940414243#include&lt;stdio.h&gt;#include&lt;iostream&gt;#include&lt;vector&gt;using namespace std;#define inf 0x3f3f3f3fint Max(int a, int b)&#123; return a&gt;b? a:b;&#125;// 求解一维数组的最大连续子数列int FindGreatestSubarray(int *arr, int n)&#123; int sum =arr[0]; int max =arr[0]; for(int i =1;i&lt;n;i++)&#123; sum =Max(sum+arr[i], arr[i]) if(sum &gt;=max)&#123; max =sum; &#125; &#125; return max;&#125;int GreatestMatrix(int[][] arr, int rows, int cols)&#123; int maxVal =- inf for(int i =0 ; i &lt;rows; i++)&#123; vector&lt;int&gt; temp(arr[i]); maxVal =Max(maxVal, FindGreatestSubarray(temp)); // 得到第一行的最大和 // 将行的n个元素加到上一行，然后计算最大和 for(int j =i+1; j&lt;rows; j++)&#123; for(int k =0;k&lt;cols ;k++)&#123; temp[k] =arr[j][k]; &#125; // 依次0~k行的最大和 maxVal =Max(maxVal, FindGreatestSubarray(temp)) &#125; &#125;&#125;int main()&#123;&#125; 聚类(clustering )聚类是一种无监督学习(Unsupervised Learning)，该算法基于数据内部的特征寻找样本的自然族群(集群)。通常使用数据可视化来评价结果。应用场景：新闻聚类，文章推荐，细分客户。 最常见的聚类算法就是K均值(K-Means)：以空间中k个点为中心进行聚类，对最靠近他们的对象归类，通过迭代的方法，逐次更新各聚类中心的值，直到得到最好的聚类结果。最后希望达到的目的：聚类中的对象相似度较高，聚类之间的相似度比较低。 相似度计算：不同的算法需要的”相似度”是不一样的，对于空间中的点，我们一般选取欧式距离来进行衡量，认为距离越近，数据之间越相似。 堆排序堆是一棵顺序存储的完全二叉树，堆排序是一种树形选择排序，其时间复杂度为O(nlogn)，空间复杂度:对于记录较少的文件不推荐使用，对于较大的文件还是有效的.堆分为大根堆和小根堆。大根堆的要求是每个节点的值都不大于其父节点的值，即A[PARENT[i]] &gt;= A[i]。小根堆的要求是每个节点的值都不小于其父节点的值，即A[PARENT[i]] &lt;= A[i]。 堆的每次调整交换堆顶和最后一个元素，然后只是调整堆顶和堆顶的左右孩子树的关系。 这个是讲解视频 1234567891011121314151617181920212223242526272829303132333435# heap modifydef MAX_Heapify(heap, HeapSize, root): left =2* root+1 right = left +1 larger =root if left &lt;HeapSize and heap[larger] &lt;heap[left]: larger =left if right &lt; HeapSize and heap[larger] &lt;heap[right]: larger =right # if modify the larger then exchange it if larger != root: heap[larger], heap[root] =heap[root], heap[larger] MAX_Heapify(heap, HeapSize, larger)# Build the heapdef Build_MAX_Heap(heap): HeapSize =len(heap) # from the end to the begin for i in range((HeapSize -2)//2, -1,-1): MAX_Heapify(heap, HeapSize, i)# sort after building the heapdef HeapSort(heap): Build_MAX_Heap(heap) for i in range(len(heap)-1, -1, -1): heap[0], heap[i] =heap[i], heap[0] MAX_Heapify(heap, i, 0) return heapif __name__ =="__main__": a =[30, 50, 57, 77, 62, 78, 94, 80, 84] print(a) print("without sort but with build heap") Build_MAX_Heap(a) print(a) 补充：stack vs heap vs queue: 中文翻译的时候有偏差，最好使用因为来进行理解。stack 就是常见的栈， we say Last in first Out (LIFO) or First in Last out (FILO); 于此相对应的是 queue, the first person in line is the first person to get out of line. This is FIFO. 这个和数据结构中的 栈和队列是一一对应的。然后说一下 heap，是算法中的一种特殊的树形结构。Heap is a tree with some special property. That special property of the heap is, the value of a node must be &gt;= or &lt;= to its children. And one most important property of heap is all leaves must be at level h or at h-1. (where h is the height of the tree). This also called heap must be a complete binary tree. 参考文献https://blog.csdn.net/minxihou/article/details/51850001https://blog.csdn.net/chibangyuxun/article/details/53018294 KMP(字符串高效查找)假设两个字符串的长度分别是m，n(m&gt;n)，在长度为m中的字符串查找长度为n的字符串，最常见的方式是暴力求解，但是这个常规解法的时间复杂度是O(nm)。KMP通过一个O(n)的预处理，可以使得时间复杂度降为O(n+m).代码实现(视频讲解(科学上网))123456789101112131415161718192021222324def kmp_match(s, p): m, n =len(s) ,len(p) cur =0 table = partial_table(p) while cur &lt;= m-n: for i in range(n): if s[i+cur] != p[i]: cur += max(i -table[i-1], 1) break else: return True return Falsedef partial_table(p): prefix =set() postfix =set() ret =[0] for i in range(1, len(p)): prefix.add(p[:i]) postfix =&#123; p[j:i+1] for j in range(1, i+1)&#125; ret.append(len((prefix &amp; postfix or &#123;&apos;&apos;&#125;).pop())) # &amp;两个set求交集 return retprint(partial_table(&apos;ABCDABD&apos;))print(kmp_match(&quot;BBC ABCDAB ABCDABCDABDE&quot;, &quot;ABCDABD&quot;)) 参考文献https://www.cnblogs.com/fanguangdexiaoyuer/p/8270332.html 二叉树的遍历在python中二叉树的结构:12345class BinNode(): def __init__(self, val): self.value =val self.lchild =None self.rchild =None 先序遍历(preOrder)第一种思路是递归实现，第二种思路借助栈的结构来实现。栈的大小空间为O(h)，h为二叉树高度；时间复杂度为O(n)，n是树的节点的个数。123456789101112131415161718192021# 递归def preOrder(self, root): if root == None: return print(root.val) self.preOrder(root.lchild) self.preOrder(root.rchild)# 借助栈结构def preOrder(self, root): if root == None: return myStack =[] node =root while node or myStack: while node: print(node.val) myStack.append(node) node =node.lchild node =myStack.pop() node =node.rchild 中序遍历(inOrder)递归和非递归两种实现思路。入栈的顺序是一样的，只是改变的遍历(print())的顺序.123456789101112131415161718192021# 递归def inOrder(self, root): if root ==None: return self.inOrder(root.lchild) print(root.val) self.inOrder(root.rchild)# 借助栈结构def inOrder(self, root): if root ==None: return myStack =[] node =root while node or myStack: while node: myStack.append(node) node =node.lchild node = myStack.pop() print(node.val) node =node.rchild 后序遍历(post order)仍然是递归和非递归版本，非递归中使用两个stack,两个stack的后进先出等于一个先进先出。12345678910111213141516171819202122232425# 递归def postOrder(self, root): if root == None: return self.postOrder(root.lchild) self.postOrder(root.rchild) print(root.val)# 借助栈结构def postOrder(self, root): if root ==None: return myStack1 =[] myStack2 =[] node =root myStack1.append(node) while myStack1: node =myStack1.pop() if node.lchild: myStack1.append(node.lchild) if node.rchild: myStack1.append(node.rchild) myStack2.append(node) while myStack2: print(myStack2.pop().val) 层序遍历使用到了队列的思想，先进先出。实际上，用的是Python中list.pop(0).注意默认是list.pop(-1),也就是默认弹出的是最后一个元素。1234567891011121314def levelOrder(self, root): if root ==None: return myQueue =[] node =root myQueue.append(node) while myQueue: # remove and return item at index (default last) node =myQueue.pop(0) print(node.val) if node.lchild != None: myQueue.append(node.lchild) if node.rchild != None: myQueue.append(node.rchild) 参考文献https://blog.yangx.site/2016/07/22/Python-binary-tree-traverse/ 旋转数组找最小值把一个数组最开始的若干个元素搬到数组的末尾，我们称之为数组的旋转。 输入一个非递减排序的数组的一个旋转，输出旋转数组的最小元素。 例如数组{3,4,5,1,2}为{1,2,3,4,5}的一个旋转，该数组的最小值为1。 NOTE：给出的所有元素都大于0，若数组大小为0，请返回0。 暴力求解大多数的问题都是可以暴力求解的–鲁迅。(In China 凡是不知道是谁说，都可以说是鲁迅说的; In US，凡是不知道谁说的，as said by Albert Einstein)因为原来的数组假设是增序，所以如果出现了的某一个元素比上一个元素小，该元素就是这个序列中的最小值。(这个情况具有唯一性吧).时间复杂度O(N)123456789101112def minNumberInRotateArray(rotateArray): arr =rotateArray # just because of laziness if not arr: return 0 if len(arr) ==2: return arr[1] num =arr[0] for i in range(1, len(arr)): if arr[i] &gt;= num: num =arr[i] else: return arr[i] 递归版本旋转数组也是一种有序数组，时间复杂度O(N)…，面试官说改进吧…使用二分法降到O(logN)。 In big-O() notation, constant factors are removed. Converting from one logarithm base to another involves multiplying by a constant factor. 所以这关键是后面的变量而不是以2 或者e 为底。这两种写法都是成立的。 So O(log N) is equivalent to O(log2 N) due to a constant factor.12345678910111213def minNumberInRotateArray(rotateArray): arr = rotateArray if not arr: return 0 if len(arr) ==2: return arr[1] mid =int(len(arr) /2) if arr[mid] &gt; arr[0]: return minNumberInRotateArray(arr[mid:]) elif arr[mid] &lt;arr[0]: return minNumberInRotateArray(arr[:mid+1]) else: return minNumberInRotateArray(arr[1:]) 非递归版本递归版本占的内存比较多，改进吧..于是非递归的版本就出来了。需要注意的是该版本的判断比较条件(其中一点是和 arr[right]进行比较)一定要小心，都是小坑…这种解法关键是需要找到非减序列（和原序列相同的形式），然后就变得可预测，可以排除这部分其他的数字。 下面这个确实是是正确的代码，注意体会细节。非递减的array，然后是从后往前比较的。if lese 中的条件是可以调换的，虽然这话听起来像是废话。这说明两个条件的判断的顺序不应该产生不同的结果。1234567891011121314def minNumberInRotateArray(rotateArray): arr =rotateArray left =0 right =len(arr) -1 while left &lt; right: mid = int((left+ right)/2) if arr[mid] &gt;arr[right]: left =mid+1 # 不包含mid 因为mid 绝不可能是 最小值 elif arr[mid] &lt;arr[right]: right =mid # 包含mid 因为mid 可能是最小值 else: right -=1 # 这个也是可以换成 left +=1 ，只要是能够渐进的 return arr[left] 文章的小标题是求解最小(大)值，上述讲述的都是最小值。如果求解最大值，稍微修改一下特殊情况的判断条件，将返回的index-1 即可。因为最小值的位置是”某一个元素比上一个元素小”，那么 index-1 之后这个元素就是该数组序列中最大的。 参考文献https://blog.csdn.net/u010005281/article/details/79823154 单链表反转单链表的反转有循环迭代和递归两种方法。单链表节点123456class Node(object): def __init__(self): self.value =None self.next =None def __str__(self): return str(self.value) 循环迭代循环迭代需要维持三个变量：pre, head, next。pre是head的pre，next是head的next.(废话)1234567891011def reverse_Linkedlist(head): if not head or not head.next : #空指针或者只有一个结点 return head pre =None # 需要创建一个None 作为最后的指向 while head: next = head.next head.next =pre pre = head head =next # 最后一次循环迭代 Head==None，而pre指向了头结点 return pre 递归一开始正常情况下不会执行if判断，利用递归走到链表的末端，new_head的值没有发生改变，为链表的最后一个节点，反转之后就成为了新链表的head。12345678def reverse_Linkedlist(head): if not head or not head.next: return head new_head = reverse_Linkedlist(head.next) # 将当前节点设置为后面节点的后续节点 head.next.next =head head.next =None return new_head 测试12345678910111213141516171819202122232425if __name__ == &quot;__main__&quot;: three = Node() three.value =3 two =Node() two.value =2 two.next =three one =Node() one.value =1 one.next =two head =Node() head.value =0 head.next =one &quot;&quot;&quot; while head: print(head.value, ) head =head.next print(&quot;******&quot;) &quot;&quot;&quot; newhead = reverse_Linkedlist(head) while newhead: print(newhead.value) newhead =newhead.next 参考文献https://foofish.net/linklist-reverse.html]]></content>
      <categories>
        <category>算法</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Introduction to Ensemble]]></title>
    <url>%2F2018%2F06%2F13%2FIntroduction-to-Ensemble%2F</url>
    <content type="text"><![CDATA[虽然在Titanic Challenge博客中介绍了模型融合(Ensemble)，但是其是以特征提取为主旋律，所以这篇博客将结合实战从最基本的Ensemble到比较复杂的Ensemble,最后安利一个用于模型融合的package. 在文中可能出现一些概念，如果不太熟悉，可以先去文末找找看，如果我没有补充，那么请自行Google吧。为了不影响阅读的流畅性，我可能在其中不会涉及很多概念解释，比如说ROC曲线和AUC值。好，我们进入正文。 Dataset本文书写过程中沿用参考博客(Introduction to Python Ensembles)的数据集。可以去这里下载，当然推荐使用原作者处理之后的数据集，you can find here。 简单介绍一下这个数据集：Federal Election Commission这个组织收集了2007到2016年的donations记录，最后得出 When Scientists Donate To Politicians, It’s Usually To Democrats这样的结论。 好了，我不想在数据集这里花太多时间，即使你不太明白数据集的具体含义，完全不影响下文的阅读，因为你很快就会发现下文并没有进行很多和原数据集相关的内容，更多的是模型融合。当然你如果能够看懂，可以感受一下上述结论的有趣之处。 Give me codes:1234567891011121314151617181920import numpy as npimport pandas as pdimport matplotlib.pyplot as plt%matplotlib inline# import data# Always good to set a seed for reproducibilitySEED = 222np.random.seed(SEED)df = pd.read_csv(&apos;input.csv&apos;)from sklearn.model_selection import train_test_splitfrom sklearn.metrics import roc_auc_scoredef get_train_test(test_size= 0.95): y =1*(df.cand_pty_affiliation ==&apos;REP&apos;) X = df.drop([&quot;cand_pty_affiliation&quot;], axis=1) X = pd.get_dummies(X, sparse=True) X.drop(X.columns[X.std() == 0], axis=1, inplace=True) return train_test_split(X, y, test_size=test_size, random_state=SEED)xtrain, xtest, ytrain, ytest = get_train_test()df.head() 简单看一下数据长什么样子，虽然有人可能不太懂。 Begin with ensemble之前的博客主要从ensemble分类的角度阐述，现在从概念的角度阐述。Ensemble: combining predictions from several models averages out idiosyncratic(怪异的) errors and yield better overall predictions.(有时候我觉得英文说得很清楚，所以就不翻译成中文了，求不被打。) 简单的说，就是可以防止过拟合。当过拟合时，边界曲线就回去过分考虑某一个或某一些少数的点，这时候 ensemble通过某种combine机制，然后得到一个泛化性能比较好的边界曲线，也就是比较好的模型。 Decision Tree首先我们从 decision tree开始。A decision tree, which is a tree of if-then rules. The deeper the tree, the more complex the patterns it can capture, but the more prone to overfitting it will be. Because of this, we will need an alternative way of building complex models of decision trees, and an ensemble of different decision trees is one such way.我们先使用 depth =112345678from IPython.display import Imagefrom sklearn.metrics import roc_auc_scorefrom sklearn.tree import DecisionTreeClassifiert1 = DecisionTreeClassifier(max_depth=1, random_state=SEED)t1.fit(xtrain, ytrain)p = t1.predict_proba(xtest)[:, 1]print(&quot;Decision tree ROC-AUC score: %.3f&quot; % roc_auc_score(ytest, p)) Decision tree ROC-AUC score: 0.672 发现结果不太理想，加深depth.1234t2 =DecisionTreeClassifier(max_depth=3, random_state=SEED)t2.fit(xtrain, ytrain)p =t2.predict_proba(xtest)[:, 1]print(&quot;Decision tree ROC-AUC score: %.3f&quot; % roc_auc_score(ytest, p)) Decision tree ROC-AUC score: 0.751 由于我们最后要ensemble，而这要求我们要构造有差异但每个不是那么差的模型。首先我们考虑到使用不同的数据集。1234567xtrain_slim =xtrain.drop([&apos;transaction_amt&apos;], axis=1)xtest_slim =xtest.drop([&apos;transaction_amt&apos;], axis=1)t3=DecisionTreeClassifier(max_depth=3, random_state=SEED)t3.fit(xtrain_slim, ytrain)p =t3.predict_proba(xtest_slim)[:,1]print(&apos;Decision tree ROC-AUC score:&#123;&#125;&apos;.format(roc_auc_score(ytest, p))) Decision tree ROC-AUC score:0.7403182587884118通过corr()来检验两者的相关性(差异性)123p1 =t2.predict_proba(xtest)[:,1]p2 =t3.predict_proba(xtest_slim)[:,1]pd.DataFrame(&#123;&apos;full_data&apos;:p1, &apos;red_data&apos;:p2&#125;).corr() 发现有一定的相关性，但是还是可以容忍的。于是开始融合。1234p1 = t2.predict_proba(xtest)[:, 1]p2 = t3.predict_proba(xtest_slim)[:, 1]p = np.mean([p1, p2], axis=0)print(&quot;Average of decision tree ROC-AUC score: %.3f&quot; % roc_auc_score(ytest, p)) Average of decision tree ROC-AUC score: 0.783我们发现：两个旗鼓相当（0.74 0.73）的可以得到一个更好的结果，可以减少决策失误 的平均是0.78。 需要注意的是我们在构造第二个模型时候通过drop()丢掉一个feature，反而得到一个更好的模型。于是乎，我们想通过使用不同的子集（不同的特征）构造不同的模型，是不是能得到更好的模型？ Random Forest(Bagging)A fast approach that works well in practice is to randomly select a subset of features, fit one decision tree on each draw and average their predictions.This process is known as bootstrapped averaging (often abbreviated bagging), and when applied to decision trees, the resultant model is a Random Forest. (我觉得原作者比我说的清楚，借用了)我的理解，在上面小节中我们使用的是Decision Tree,在实际应用中发现有差异的多个子树的效果要更好一些。而实现这个途径快速的方法就是 Random Forest。Random 在这里表示任意几个子树(特征)，然后这些Tree组成了Forest。123456789from sklearn.ensemble import RandomForestClassifierrf =RandomForestClassifier( n_estimators=10, max_features= 3, random_state=SEED)rf.fit(xtrain, ytrain)p =rf.predict_proba(xtest)[:, 1]print(&apos;Average of decision tree ROC-AUC score:&#123;&#125;&apos;.format(roc_auc_score(ytest, p))) Average of decision tree ROC-AUC score:0.844018408542404这就是叫做”质的飞跃”从那个0.783-&gt; 0.844(将近6个百分点，好吧，有点神经质了…)From nobody to somebody, we are on something.. Ensemble of various models可以看出上述模型中，最后的模型（Random Forest）的子模型(Decision Tree)。但是子模型并不是局限树这一种结构，我们更多的选择：linear models, kernel-based models, non-parametric models, neural networks or even other ensembles! 为了避免代码的冗余构造了以下的helper function.123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657# A host of Scikit-learn modelsfrom sklearn.svm import SVC, LinearSVCfrom sklearn.naive_bayes import GaussianNBfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifierfrom sklearn.linear_model import LogisticRegressionfrom sklearn.neighbors import KNeighborsClassifierfrom sklearn.neural_network import MLPClassifierfrom sklearn.kernel_approximation import Nystroemfrom sklearn.kernel_approximation import RBFSamplerfrom sklearn.pipeline import make_pipelinedef get_models(): &quot;&quot;&quot;Generate a library of base learners.&quot;&quot;&quot; nb = GaussianNB() svc = SVC(C=100, probability=True) # C越大边界越复杂，会导致过拟合 knn = KNeighborsClassifier(n_neighbors=3) # KNN算法寻找训练数据中的K个最近的数据，它使用指向最多的那个类别来作为预测的输出。 lr = LogisticRegression(C=100, random_state=SEED) # 对于这个 c 能知道的就是正则化系数 smaller values specify stronger regularization. nn = MLPClassifier((80, 10), early_stopping=False, random_state=SEED) gb = GradientBoostingClassifier(n_estimators=100, random_state=SEED) # 子模型的数量，默认是100, gbc通常 robust to over-fitting, so a large number results in better performance rf = RandomForestClassifier(n_estimators=10, max_features=3, random_state=SEED) models = &#123;&apos;svm&apos;: svc, &apos;knn&apos;: knn, &apos;naive bayes&apos;: nb, &apos;mlp-nn&apos;: nn, &apos;random forest&apos;: rf, &apos;gbm&apos;: gb, &apos;logistic&apos;: lr, &#125; return models def train_predict(model_list): P =np.zeros((ytest.shape[0], len(model_list))) P =pd.DataFrame(P) print(&apos;Fitting models&apos;) cols =list() for i, (name, m) in enumerate(models.items()): print(&quot;%s...&quot; % name, end=&quot; &quot;, flush=False) m.fit(xtrain, ytrain) P.iloc[:, i] =m.predict_proba(xtest)[:, 1] cols.append(name) print(&apos;Done&apos;) P.columns =cols print(&apos;Done.\n&apos;) return P def score_models(P, y): &quot;&quot;&quot;Score model in predication DF&quot;&quot;&quot; print(&apos;Scoring models&apos;) for m in P.columns: score =roc_auc_score(y, P.loc[:, m]) print(&quot;%-26s: %.3f&quot; % (m, score)) print(&apos;Done, \n&apos;) let’s go…123models =get_models()P =train_predict(models)score_models(P, ytest) This is our base line.Gradient Boosting Machine(GBM) 果然名不虚传, does best我们来分析一下模型之间的相关性，原作者使用的mlens package(You can install it with: pip install mlens)，我这里用的是seaborn(install it with: pip install seaborn).在检查相关性(Pearson相关性:衡量两个数据集合的线性相关性)时候，我们使用的是经过处理的相关性。具体说来可以称之为 error correlation,详细见代码。 12345678# look at error correlations is more promising, errors are significantly correlated import seaborn as snsplt.subplots(figsize=(10,8)) corrmat = P.apply(lambda pred: 1*(pred &gt;= 0.5) - ytest.values).corr()sns.set(font_scale=1.5) hm = sns.heatmap(corrmat, cbar=True, annot=True, square=True, fmt=&apos;.2f&apos;, annot_kws=&#123;&apos;size&apos;: 10&#125;) plt.title(&apos;Pearson Correlation of Features&apos;, y=1.05, size=15) plt.show() 预测值和真实值之间的差异称之为error，查看error的 Pearson correlation，效果更加明显。1print(&quot;Ensemble ROC-AUC score: %.3f&quot; % roc_auc_score(ytest, P.mean(axis=1))) Ensemble ROC-AUC score: 0.884结果高于每一个单独的模型，但是不是那么明显。 Visualize Curve ROC(helper function)我们注意到之前使用的所有的结果的评价标准都是 roc_auc_score，但是并没有提及这是什么。当然在正文中也不打算解释，如果不是很清楚，可以查看本文最后补充概念.简单来说AUC可以用来衡量”二分问题”的泛化能力，是一种评价指标。我们这里想说的是 visualize Curve ROC,可视化。12345678910111213141516171819202122# a helper function for roc_curvefrom sklearn.metrics import roc_curvedef plot_roc_curve(ytest, P_base_learners, P_ensemble, labels, ens_label): &quot;&quot;&quot;Plot the roc curve for base learners and ensemble.&quot;&quot;&quot; plt.figure(figsize=(10, 8)) plt.plot([0, 1], [0, 1], &apos;k--&apos;) cm = [plt.cm.rainbow(i) for i in np.linspace(0, 1.0, P_base_learners.shape[1] + 1)] for i in range(P_base_learners.shape[1]): p = P_base_learners[:, i] fpr, tpr, _ = roc_curve(ytest, p) plt.plot(fpr, tpr, label=labels[i], c=cm[i + 1]) fpr, tpr, _ = roc_curve(ytest, P_ensemble) plt.plot(fpr, tpr, label=ens_label, c=cm[0]) plt.xlabel(&apos;False positive rate&apos;) plt.ylabel(&apos;True positive rate&apos;) plt.title(&apos;ROC curve&apos;) plt.legend(frameon=False) plt.show()plot_roc_curve(ytest, P.values, P.mean(axis=1), list(P.columns), &quot;ensemble&quot;) 我们顺便把刚才的模型可视化，发现ensemble的AUC是最大的，意味这泛化性能是最好，这也是符合我们的认知的 。 Beyond ensembles as a simple average我们回到主线上，在上一个模型中我们提及最后的 ensemble的结果是好于每个单独的模型，但是没有那么突出。根据ROC曲线我们也可以看出，有的模型(KNN)在这里表现的没有那么好，我们在想是不是由于因为这个而拉底了最后的结果，当然这只是猜测，于是我们找到了 try的方向。可能第一直觉去掉这个模型再进行融合，在这个实验表明该策略最后的结果0.883，相比与0.884，你懂得，并没有变好。我们还有一种策略:learn a sensible set of weights to use when averaging predictions.让模型自己去学习如何调整各个模型之间的比例。 Learning to combine predications为了让模型自学习各个之间的预测比例，我们引入了 meta learner(meta是元，理解为最基础的) to learn how to best combine these predictions. 除此之外，我们将使用不同的数据集，像Random Forest使用不同的数据子集(不同的特征组成的数据集)。于是我们需要a method for splitting the training data between the base learners and the meta learner.12345678910base_learners =get_models()meta_learner = GradientBoostingClassifier( n_estimators=1000, loss=&quot;exponential&quot;, max_features=4, max_depth=3, subsample=0.5, learning_rate=0.005, random_state=SEED) 使用最强模型GBM作为 meta learner并定义好 base_learners.To keep things simple, we split the full training set into a training and prediction set of the base learners. This method is sometimes referred to as Blending. Unfortunately, the terminology differs between communities, so it’s not always easy to know what type of cross-validation the ensemble is using.12345678910# sefine a procedure for generating train and test setsxtrain_base , xpred_base, ytrain_base, ypred_base =train_test_split(xtrain, ytrain, test_size=0.5, random_state=SEED)def train_base_learners(base_learners, inp, out, verbose =True): if verbose: print(&apos;Fitting models&apos;) for i, (name, m) in enumerate(base_learners.items()): if verbose: print(&quot;%s...&quot; % name, end=&quot; &quot;, flush=False) m.fit(inp, out) if verbose: print(&apos;Done.&apos;)train_base_learners(base_learners, xtrain_base, ytrain_base) (注意我们只是使用了50%的data去train, test_size =0.5)1234567891011121314def predict_base_learners(pred_base_learners, inp, verbose=True): &quot;&quot;&quot;Generate a prediction matrix.&quot;&quot;&quot; P = np.zeros((inp.shape[0], len(pred_base_learners))) if verbose: print(&quot;Generating base learner predictions.&quot;) for i, (name, m) in enumerate(pred_base_learners.items()): if verbose: print(&quot;%s...&quot; % name, end=&quot; &quot;, flush=False) p = m.predict_proba(inp) # With two classes, need only predictions for one class P[:, i] = p[:, 1] if verbose: print(&quot;done&quot;) return PP_base = predict_base_learners(base_learners, xpred_base) 现在我们得到了base_learners的predications，接下来我们应该使用的是这个流程，在base learners的基础上（类似两层结构了 meta learner 学习的如何搭配这些base learner使得最后的结果 predications最大）进行训练，而不是训练原来的数据集。 1meta_learner.fit(P_base, ypred_base) 1234567#meta_learner.fit(P_base, ypred_base)def ensemble_predict(base_learners, meta_learner, inp, verbose=True): &quot;&quot;&quot;Generate predictions from the ensemble.&quot;&quot;&quot; P_pred = predict_base_learners(base_learners, inp, verbose=verbose) return P_pred, meta_learner.predict_proba(P_pred)[:, 1]P_pred, p = ensemble_predict(base_learners, meta_learner, xtest)print(&quot;\nEnsemble ROC-AUC score: %.3f&quot; % roc_auc_score(ytest, p)) 最后的结果是0.881 相比与之前最好的0.884（使用相同的数据集，没有进行 meta_learner的操作）这是因为我们在划分数据集的使用只是使用了0.5的数据集，而前者的模型使用了全部的train sets。有人不免疑问：为什么不使用全部的data？我的理解是划分xtrain_base , xpred_base, ytrain_base, ypred_base使用的是 train_test_split()，总是需要设定一个数值的，即使train_size =0.01，也是没有用到全部的datas. Training with cross-validation我们使用cross-validation 来缓解上面那个问题。During cross-validated training of the base learners, a copy of each base learner is fitted on K−1 folds, and predict the left-out fold. This process is iterated until every fold has been predicted. The more folds we specify, the less data is being left out in each training pass. This makes cross-validated predictions less noisy and a better reflection of performance during test time. The cost is significantly increased training time. Fitting an ensemble with cross-validation is often referred to as stacking, while the ensemble itself is known as the Super Learner. To understand how cross-validation works, we can think of it as an outer loop over our previous ensemble. The outer loop iterates over K distinct test folds, with the remaining data used for training. The inner loop trains the base learners and generate predictions for the held-out data. Here’s a simple stacking implementation:12345678910111213141516171819202122232425262728293031323334353637383940414243from sklearn.base import clonedef stacking(base_learners, meta_learner, X, y, generator): &quot;&quot;&quot;Simple training routine for stacking.&quot;&quot;&quot; # Train final base learners for test time print(&quot;Fitting final base learners...&quot;, end=&quot;&quot;) train_base_learners(base_learners, X, y, verbose=False) print(&quot;done&quot;) # Generate predictions for training meta learners # Outer loop: print(&quot;Generating cross-validated predictions...&quot;) cv_preds, cv_y = [], [] for i, (train_idx, test_idx) in enumerate(generator.split(X)): fold_xtrain, fold_ytrain = X[train_idx, :], y[train_idx] fold_xtest, fold_ytest = X[test_idx, :], y[test_idx] # Inner loop: step 4 and 5 fold_base_learners = &#123;name: clone(model) for name, model in base_learners.items()&#125; train_base_learners( fold_base_learners, fold_xtrain, fold_ytrain, verbose=False) fold_P_base = predict_base_learners( fold_base_learners, fold_xtest, verbose=False) cv_preds.append(fold_P_base) cv_y.append(fold_ytest) print(&quot;Fold %i done&quot; % (i + 1)) print(&quot;CV-predictions done&quot;) # Be careful to get rows in the right order cv_preds = np.vstack(cv_preds) cv_y = np.hstack(cv_y) # Train meta learner print(&quot;Fitting meta learner...&quot;, end=&quot;&quot;) meta_learner.fit(cv_preds, cv_y) print(&quot;done&quot;) return base_learners, meta_learner 尤其在cv_preds和cv_y的维度问题上，注意小心。The basic difference between blending and stacking is therefore that stacking allows both base learners and the meta learner to train on the full data set. Using 2-fold cross-validation, we can measure the difference this makes in our case:1234567from sklearn.model_selection import KFold# Train with stackingcv_base_learners, cv_meta_learner = stacking( get_models(), clone(meta_learner), xtrain.values, ytrain.values, KFold(2))P_pred, p = ensemble_predict(cv_base_learners, cv_meta_learner, xtest, verbose=False)print(&quot;\nEnsemble ROC-AUC score: %.3f&quot; % roc_auc_score(ytest, p)) Ensemble ROC-AUC score: 0.889这是目前为止最好的结果了。Stacking yields a sizeable increase in performance: in fact, it gives us our best score so far. This outcome is typical for small and medium-sized data sets, where the effect of blending can be severe. As the data set size increases, blending and stacking performs similarly. Use packages快要接近尾声了，在文章的开始，我们提及要安利一个resemble好用的package. So, it’s now.Here, we use Ensemble and build our previous generalized ensemble, but now using 10-fold cross-validation:123456789101112131415161718192021from mlens.ensemble import SuperLearner# Instantiate the ensemble with 10 foldssl = SuperLearner( folds=10, random_state=SEED, verbose=2, backend=&quot;multiprocessing&quot;)# Add the base learners and the meta learnersl.add(list(base_learners.values()), proba=True) sl.add_meta(meta_learner, proba=True)# Train the ensemblesl.fit(xtrain, ytrain)# Predict the test setp_sl = sl.predict_proba(xtest)print(&quot;\nSuper Learner ROC-AUC score: %.3f&quot; % roc_auc_score(ytest, p_sl[:, 1])) So simple!1plot_roc_curve(ytest, p.reshape(-1, 1), P.mean(axis=1), [&quot;Simple average&quot;], &quot;Super Learner&quot;) 发现super learner(meta learner)和 basic learner的mean的结果已经不相上下了。super learner得到了很好的训练。 ROC曲线和AUC值ROC全称是“受试者工作特征”（Receiver Operating Characteristic）。ROC曲线的面积就是AUC（Area Under the Curve）。AUC用于衡量“二分类问题”机器学习算法性能（泛化能力）。说到这里不得不提及就是经常使用的符号，TP(True Positive), FP(False, Positive), TN(True Negative),FN(False Negative)。他们是根据原来真实数据和预测类别进行的排列组合（当然这是针对二分问题）。 ROC 曲线: ROC 曲线（接收者操作特征曲线）是一种显示分类模型在所有分类阈值下的效果的图表。该曲线绘制了以下两个参数：真正例率 和 假正例率。 真正例率 (TPR) 是召回率的同义词，因此定义如下：$$T P R = \frac { T P } { T P + F N }$$假正例率 (FPR) 的定义如下：$$F P R = \frac { F P } { F P + T N }$$ ROC 中 TPR =(True positive / ( True positive +False negative)), 那个false negative 也是真实的类别，只不过是错误的当做了 negative（false negative） 2针对一个二分类问题，将实例分成正类(postive)或者负类(negative)。但是实际中分类时，会出现四种情况. 若一个实例是正类并且被预测为正类，即为真正类(True Postive TP) 若一个实例是正类，但是被预测成为负类，即为假负类(False Negative FN) 若一个实例是负类，但是被预测成为正类，即为假正类(False Postive FP) 若一个实例是负类，但是被预测成为负类，即为真负类(True Negative TN) ROC 曲线是如何绘制的: 采用不同分类阈值时的 TPR 与 FPR。降低分类阈值会导致将更多样本归为正类别，从而增加假正例（FP）和真正例（TP）的个数，可以理解为降低了被认为正确的标准，数量自然就增多了。下图显示了一个典型的 ROC 曲线。注意观察图中TPR 和 FPR是呈正相关的，验证了上述的结论。为了计算 ROC 曲线上的点，我们可以使用不同的分类阈值多次评估逻辑回归模型，但这样做效率非常低。幸运的是，有一种基于排序的高效算法可以为我们提供此类信息，这种算法称为曲线下面积。 ROC曲线，一般适用于你的分类器输出一个“概率值”，即这个样本属于某个类的概率是多少。 如此的话，你就需要设定一个阈值， 大于这个阈值属于正类，小于这个阈值属于负类。 从而，对于这个阈值P0， 就会得到对应的TPR, FPR, 也就是ROC曲线上的一个点，你设置不同的阈值，就会得到不同的TPR, FPR， 从而构成ROC曲线。 通常来说 阈值降低，即进入正类的门槛变低， TPR会变大，但是FPR也会变大， 看他们谁变的快。 1234567891011121314gbc = GradientBoostingClassifier()gbc.fit(x_train, y_train)resu = gbc.predict(x_test) #进行预测y_pred_gbc = gbc.predict_proba(x_test)[:,1] ###这玩意就是预测概率的fpr, tpr, threshold = roc_curve(y_test, y_pred_gbc) ###画图的时候要用预测的概率，而不是你的预测的值plt.plot(fpr, tpr, 'b', label='AUC = %0.2f' % rocauc)#生成ROC曲线plt.legend(loc='lower right')plt.plot([0, 1], [0, 1], 'r--')plt.xlim([0, 1])plt.ylim([0, 1])plt.ylabel('真正率')plt.xlabel('假正率')plt.show() 接着我们计算TRP(True Positive Radio)，FRP(False Positive Ratio)用于描述ROC曲线，分别表示该曲线的Y轴，X轴。TPR=TP/(TP+FN)FPR=FP/(FP+TN)最后就形成了类似这样的图像(来源于上述的训练模型) 我们希望的结果是TRU越大（接近1），FRU越小（接近0）。AUC的值是ROC所覆盖的面积，当AUC越大时候，分类器的效果越好。从图中可以看出模型(ensemble)的面积是最大的，分类效果也是最好的。关于该图像还有一点，如果你的曲线拟合对角线（图中虚线），那么相当于随机猜测。 TakeOff If you have an imbalanced dataset accuracy can give you false assumptions regarding the classifier’s performance, it’s better to rely on precision and recall, in the same way a Precision-Recall curve is better to calibrate the probability threshold in an imbalanced class scenario as a ROC curve. ROC Curves: summarise the trade-off between the true positive rate and false positive rate for a predictive model using different probability thresholds. Precision-Recall curves: summarise the trade-off between the true positive rate and the positive predictive value for a predictive model using different probability thresholds. ROC curves are appropriate when the observations are balanced between each class, whereas precision-recall curves are appropriate for imbalanced datasets. In both cases the area under the curve (AUC) can be used as a summary of the model performance. Why a ROC curve cannot measure well? The Receiver Operating Characteristic (ROC) curves plot FPR vs. TPR as shown below. Because TPR only depends on positives, ROC curves do not measure the effects of negatives. The area under the ROC curve (AUC) assesses overall classification performance . AUC does not place more emphasis on one class over the other, so it does not reflect the minority class well. Davis and Goadrich in this paper propose that Precision-Recall (PR) curves will be more informative than ROC when dealing with highly skewed datasets. The PR curves plot precision vs. recall (FPR). Because Precision is directly influenced by class imbalance so the Precision-recall curves are better to highlight differences between models for highly imbalanced data sets. When you compare different models with imbalanced settings, the area under the Precision-Recall curve will be more sensitive than the area under the ROC curve. 最强讲解 how to handle unbalanced dataImbalanced data typically refers to a problem with classification problems where the classes are not represented equally. The accuracy paradox is the name for the exact situation in the introduction to this post. Data approachOversample minority class and Undersample majority class Over-sampling increases the number of minority class members in the training set. The advantage of over-sampling is that no information from the original training set is lost, as all observations from the minority and majority classes are kept. On the other hand, it is prone to overfitting. (You can add copies of instances from the under-represented class called over-sampling or more formally sampling with replacement) Under-sampling, on contrary to over-sampling, aims to reduce the number of majority samples to balance the class distribution. Since it is removing observations from the original data set, it might discard useful information. (You can delete instances from the over-represented class, called under-sampling.) Some Rules of Thumb Consider testing under-sampling when you have an a lot data (tens- or hundreds of thousands of instances or more) Consider testing over-sampling when you don’t have a lot of data (tens of thousands of records or less) Consider testing random and non-random (e.g. stratified) sampling schemes. Consider testing different resampled ratios (e.g. you don’t have to target a 1:1 ratio in a binary classification problem, try other ratios) Try Different Algorithms基于树的这种结构的模型还是表现比较给力的。That being said, decision trees often perform well on imbalanced datasets. The splitting rules that look at the class variable used in the creation of the trees, can force both classes to be addressed. Try Penalized ModelsPenalized classification imposes an additional cost on the model for making classification mistakes on the minority class during training. These penalties can bias the model to pay more attention to the minority class. Try Changing Your Performance Metric 使用 precision and recall curves or F1 去评价你的网络效果 Precision: A measure of a classifiers exactness. Recall: A measure of a classifiers completeness F1 Score (or F-score): A weighted average of precision and recall. 而 ROC curves 通常不是一个最好的选择。 https://medium.com/anomaly-detection-with-python-and-r/sampling-techniques-for-extremely-imbalanced-data-281cc01da0a8 GBC参数这些参数中，类似于Adaboost，我们把重要参数分为两类，第一类是Boosting框架的重要参数，第二类是弱学习器即CART回归树的重要参数。n_estimators: 也就是弱学习器的最大迭代次数，或者说最大的弱学习器的个数。learning_rate: 即每个弱学习器的权重缩减系数ν，也称作步长对于分类模型，有对数似然损失函数”deviance”和指数损失函数”exponential”两者输入选择。默认是对数似然损失函数”deviance”。一般来说，推荐使用默认的”deviance”。它对二元分离和多元分类各自都有比较好的优化。而指数损失函数等于把我们带到了Adaboost算法。对于回归模型，有均方差”ls”, 绝对损失”lad”, Huber损失”huber”和分位数损失“quantile”。默认是均方差”ls”。一般来说，如果数据的噪音点不多，用默认的均方差”ls”比较好。如果是噪音点较多，则推荐用抗噪音的损失函数”huber”。而如果我们需要对训练集进行分段预测的时候，则采用“quantile”。max_features:可以使用很多种类型的值，默认是”None”,意味着划分时考虑所有的特征数.subsample: 选择小于1的比例可以减少方差，即防止过拟合，但是会增加样本拟合的偏差，因此取值不能太低。推荐在[0.5, 0.8]之间，默认是1.0，即不使用子采样。 参考文献GBC参数设置ROC曲线和AUC值Introduction to Python Ensembles]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>ROC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[对抗生成网络实验对比]]></title>
    <url>%2F2018%2F06%2F05%2F%E5%AF%B9%E6%8A%97%E6%80%A7%E7%94%9F%E6%88%90%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[GAN模型是典型的隐式无监督生成模型，建模过程中没有利用到数据的语义标签。但在实际应用中生成模型的可控性至关重要，根据标签生成对可控有样本更具实际应用价值。图像生成模型被广泛应用于数据增强、风格转换和数据补全领域，需要可控且语义完备的生成模型。条件GAN模型在GAN模型建模思路的基础上，将语义标签加入了建模过程，将无监督生成模型转变为有监督的条件生成模型。条件GAN模型具体包括Conditional GAN模型、Semi-GAN模型和AC-GAN模型。 实验模型介绍（1）Conditional GAN模型GAN模型中判别器D对输入的数据样本的来源进行判别，是典型的判别模型流程。如果在GAN框架中加入有监督信息来辅助训练，如图像的类别信息来辅助判别器D进行判别，则会帮助生成更加真实的样本。其中最初的尝试方式是Conditional GAN模型，结构如图所示。Conditional GAN模型在生成器G和判别器D输入中都加入了标签信息，试图让生成器G学习到从数据标签y到样本x的映射，让判别器D学习对样本x和标签y的组合进行判别。与GAN模型相比，Conditional GAN模型增加了标签信息的输入将模型转变为条件生成模型，在一定程度上提高了模型的稳定性。Conditional GAN模型训练过程中，判别器D对样本x和标签y的类别组合进行训练，并没有输入样本x和标签y的类别错误组合进行训练，因此模型并没有学习样本x和标签y的联合分布。（2）Semi-GAN模型Conditional GAN模型利用了标签信息进行建模，但没有对标签语义的信息进行表征，导致模型能够学到的信息有限。在生成模型过程中，如果判别器D能够明确指出生成样本的类别错误，则可为生成器G提供更加精确的梯度信息，最终能生成更加真实的样本。Semi-GAN基于此思路进行改进，具体结构如图9所示。Semi-GAN模型在Conditional GAN模型的基础上，对判别器D的分类输出进行细化加入了半监督学习过程。（3）AC-GAN模型与Conditional GAN模型相比，Semi-GAN模型中判别器D能够判别真实样本的来源，增强了判别器D的判别能力。但研究表明过强的判别信息会影响生产样本的质量，具体原因为Semi-GAN模型的建模过程为半监督分类过程，目标优化函数为无监督分类和有监督分类目标函数之和。若判别器D的监督分类信息过强，则会削弱判别器D对样本来源的判别能力。Conditional GAN模型能够生成指定类别的样本，Semi-GAN模型能够判别样本的类别信息。AC-GAN模型将以上两个模型思路进行整合，得到够进行条件生成的生成器G，和能够判别样本类别和来源信息的判别器D。AC-GAN模型的结构如图10所示。AC-GAN模型在Conditional GAN的基础上，让判别器D在判别样本来源的同时，让样本进行分类。此时的判别器D的输出分为样本来源信息LS和样本分类LC信息。 不同模型比较下面用表格的方式对比在实验中使用的模型的目标函数(ps,图画比较丑，之后再修改) Name Paper Link Value Function GAN Arxiv DCGAN Arxiv Semi-GAN Arxiv 和GAN 模型相同 CGAN Arxiv ACGAN Arxiv our model Arxiv 数据集介绍在常用于图像生成的图像数据集中，大部分数据的标签类型为离散类型。其中MNIST和Fashion-MNIST为常用的灰度图像数据，每类样本分布较为独立，常用于进行图像分类和样本生成的实验；SVHN和CIFAR10为彩色数据集图像像素分布较为复杂，其中CIFAR10常用来检验分类网络性能的评价数据集；CelebA为大规模的人脸识别和属性分类数据集，每幅人脸图像包括40个属性标签；ImageNet为图像分类和识别数据集，数据集类别分布比较复杂具体包括自然图像和人为图像。UnityEyes为人眼视觉合成数据集，数据集标签包括瞳孔标签和视觉方向标签，其中视觉方向标签为连续的语言标签。常见的离散标签图像数据集的样例: 我们的模型在原始GAN模型中，目标函数定义为生成器G和判别器D的博弈过程，定义V(G;D)为模型的目标函数，由生成器G和判别器D组成。语义匹配目标函数FMloss。基于语义匹配的条件生成网络模型的生成器G和判别器D的目标函数分别为：如上式，LS为样本来源，LC为分类结果。判别器D目标为最大化LC+ LS + FMloss，其试图对输入样本进行分类，并通过来源和语义匹配区分生成样本和原始样本。生成器G的目的是最大化LC − LS −FMloss，其试图通过样本分类结果、样本来源和语义匹配结果来欺骗判别器D。LS来源损失与原始GAN模型相同，LC为语义标签分类损失，在类别分类中使用交叉信息熵，在数值回归中则使用均方差回归。 生成结果对比MNIST数据集生成结果Fashion-MNIST数据集生成结果SVHN数据集生成结果CIFAR10数据集生成结果CelebA数据集生成结果UnityEyes数据集生成结果]]></content>
      <categories>
        <category>Deep learning</category>
      </categories>
      <tags>
        <tag>GANs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Titanic Challenge]]></title>
    <url>%2F2018%2F06%2F05%2FTitanic-Challenge%2F</url>
    <content type="text"><![CDATA[用此博客记录自己解决 Kaggle Titanic challenge过程中的个人总结。 问题描述说道Titanic(泰坦尼克号)，最熟悉莫过于Titanic(1997 film),这部由著名导演詹姆斯·卡梅隆执导，莱昂纳多·迪卡普里奥、凯特·温斯莱特领衔主演的电影，经久不衰…但是我们今天的画风不是这样的… 我们今天解决的问题是以该事件问背景，但是没有那么浪漫。关于这个案例的介绍网上有很多内容，为了避免累赘，在这里就不进行详述。总的要求：预测在这个事件中乘客是否死亡。附上对于train sets和 test sets中数据的介绍。更多详细的内容参看：https://www.kaggle.com/c/titanic 数据分析顺滑过渡到第二阶段，数据分析，对于竞赛而言，我感觉对于数据的认识的重要性完全不亚于模型的重要性。之后我们将再次提到这句话。我将结合代码进行数据分析。 pandas 原生数据分析函数1234import pandas as pdtrain =pd.read_csv(&apos;data/train.csv&apos;)test =pd.read_csv(&apos;data/test.csv&apos;)train.describe(include=&apos;all&apos;) 除了上面 train.describe()，下面这两个也是比较常用的12train.head()train.columns 因为总体的数据量比较少，所以我们选择把train set 和test set连接起来进行数据分析和处理。1combined2 = pd.concat([train_data, test_data], axis=0) 数据质量分析 缺省值对于缺省值，常用的手段就是填充，但是针对不同的数据有不同的填充手段，有的是均值填充，有的是默认值填充还有的是根据现有数据训练一个 regression进行拟合(这种情况出现在缺省的数据比较重要，对于结果的预测有比较强的相关性的时候)。1combined2.Embarked.fillna(&apos;S&apos;, inplace=True) Embardked(上船港口)不是那么能表现出和结果(survival)相关的变量，我么可以直接采用某个默认值进行填充。1combined2.Fare.fillna(np.median(combined2.Fare[combined2.Fare.notnull()]), inplace=True) Fare(船票)我们选择使用均值填充 12345678classers = [&apos;Fare&apos;,&apos;Parch&apos;,&apos;Pclass&apos;,&apos;SibSp&apos;,&apos;TitleCat&apos;,&apos;CabinCat&apos;,&apos;Sex_female&apos;,&apos;Sex_male&apos;, &apos;EmbarkedCat&apos;, &apos;FamilySize&apos;, &apos;NameLength&apos;, &apos;FamilyId&apos;]age_et = ExtraTreesRegressor(n_estimators=200)X_train = full_data.loc[full_data.Age.notnull(),classers]Y_train = full_data.loc[full_data.Age.notnull(),[&apos;Age&apos;]]X_test = full_data.loc[full_data.Age.isnull(),classers]age_et.fit(X_train,np.ravel(Y_train))age_preds = age_et.predict(X_test)full_data.loc[full_data.Age.isnull(),[&apos;Age&apos;]] = age_preds 因为在特征提取看来 age 是一个比较重要的属性（下文中使用age来进一步计算性别特征，而性别特征对于survival 是重要的因素），所以需要通过 fit来进行填充 null 值。 异常值异常值的检测12combined2.boxplot()plt.ylim(0, 1000) 异常值处理大多数情况下我们都采取忽视，但是有时候异常值中却跟结果有比较强的相关性，比如说该题目分数在0.9的一位大神在博客中使用的特征包含名字长度。这个在我一开始的特征提取中确实没有太在意名字长度也可以当作一种和结果(servival or dead)相关的特征。 重复值重复值的检测1train[train.duplicated()==True] 如果运行结果为空，那么就是没有重复值,如果有重复值，一般使用下面类似的代码都是可以去除掉的。1df.drop_duplicates() 分布特征分析1234567#分布分析fig ,ax =plt.subplots(2,2, figsize=(8,6))sns.countplot(&apos;Embarked&apos;, data =train, ax =ax[0,0])sns.countplot(&apos;Pclass&apos;, data =train, ax =ax[0,1])sns.violinplot(&apos;Survived&apos;, &apos;Age&apos;, data= train, ax =ax[1,0]).set(ylim =(-10, 80))sns.countplot(x =&apos;Survived&apos;, data= train, ax =ax[1,1])plt.tight_layout() 运行需要导入 seaborn1import seaborn as sns 这里安利一个数据可视化工具-seaborn。回正题 counplot()可以直观的看出单个数据的特征，但是我们更加关心的是数据和数据之间的关系，更准确的是数据和预测数据(survival )之间的关系。所以,我们进行相关性分析。123456plt.subplots(figsize=(10,8)) corrmat = train[train.columns[1:]].corr()sns.set(font_scale=1.5) hm = sns.heatmap(corrmat, cbar=True, annot=True, square=True, fmt=&apos;.2f&apos;, annot_kws=&#123;&apos;size&apos;: 10&#125;) plt.title(&apos;Pearson Correlation of Features&apos;, y=1.05, size=15) plt.show() 从图中可以看出，在原始数据集特征中(为什么这么说，嗯，这意味着我们下文还要进行 generate new features),Fare特征是和 survival最相关的。这从数据角度这你船票价钱越高，你生存的几率就越大(三观尽毁)。嗯，这是符合经济社会的运行规律的。我们在分析 Pearson Correlation的时候，关注的是数值的绝对值，如果是正值，表示正相关；如果是负值，表示负相关。 如果细心的小伙伴发现，这个并没有把所有的变量的相关性表示出来，是的，下文我将给出一个加强版的。 特征工程当我们对数据有了一个初步的认识，这时候就可以进行特征工程了。网上流传很广的一句话”数据特征决定了机器学习的上限，而算法优化只是尽可能逼近这个上限”，我深有体会。因为之前进行特征提取，然后在kaggle的submission score是0.73205,经过模型融合然后达到了0.78947，提高了5个百分点。当自己在思考数据特征重新进行特征提取的时候，最后的score 是0.82296.这都是以10个百分点的提高啊。所以这句话很有道理，我试图找到这句话的出处，以表示我对于版权的尊重，但是科学上网能力有限，没有找见，也许这句话来自群众的智慧吧。图：我在kaggle 的submission 和相应的score 但是我想强调的是特征提取是个很难有模板化的东西，这得看个人对于这个问题的理解和对于数据的理解，对于数据异常值的处理。并且还想说的是这个一个迭代的过程，不是一步到位的。当初步构造好自己特征之后可以使用图形化工具进行简单的分析一下。(下图是我第一次构造的特征工程的图形化)12345678910111213141516171819def correlation_heatmap(df): _ , ax = plt.subplots(figsize =(14, 12)) colormap = sns.diverging_palette(220, 10, as_cmap = True) _ = sns.heatmap( df.corr(), cmap = colormap, square=True, cbar_kws=&#123;&apos;shrink&apos;:.9 &#125;, ax=ax, annot=True, linewidths=0.1,vmax=1.0, linecolor=&apos;white&apos;, annot_kws=&#123;&apos;fontsize&apos;:12 &#125; ) plt.title(&apos;Pearson Correlation of Features&apos;, y=1.05, size=15) train2 =train1.drop([&apos;NullCabin&apos;], axis =1)correlation_heatmap(train2) 上图是第一次特征分析的结果(kaggle score: 0.73205),survival和Pclass和Fare是有较强的正相关性。 该图是第二次特征分析分析结果（kaggle score:0.82296,survival与male_adult(-0.56)、sex_male(-0.54)负相关,和female_adult(0.54)、sex_female(0.52)正相关。你的survival的概率和你的性别和年龄有关，如果你是成年女子，那么你很大的概率不会死亡(像Rose那样)；如果你是成年男子，那么你有很大概率体现英伦的绅士风度，主动(Jack那样)或者被选择死亡。瞬间想起了Titanic电影中Jack和Rose 的场景，好感人啊!!! 模型训练发现写了这么久，还没有开始训练模型。加快脚步…下面的内容以第一次训练模型为例。在建立基本模型之前我们需要先引入评价函数，以评价不同模型性能的好坏。 通过均值和方差来评价模型性能的优劣1234from sklearn import cross_validation def rmsl(clf): s = cross_validation.cross_val_score(clf, X_train, y_train, cv=5) return (s.mean(),s.std()) 建立基本模型1234567891011121314151617181920212223242526272829303132333435363738394041424344from sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis,gaussian_processNLA =[ #ensemble methods ensemble.AdaBoostClassifier(), ensemble.BaggingRegressor(), ensemble.GradientBoostingClassifier(), ensemble.RandomForestClassifier(n_estimators=60), # Gaussian process gaussian_process.GaussianProcessClassifier(), # LM linear_model.LogisticRegression(C=1.0, penalty=&apos;l1&apos;), #Navies Bayes naive_bayes.GaussianNB(), # Nearest Neighbor neighbors.KNeighborsClassifier(n_neighbors=3), # Svm svm.SVC(probability=True), svm.LinearSVC(), #Tree tree.DecisionTreeClassifier(), tree.ExtraTreeClassifier() ]#create table to compare MLAMLA_columns = [&apos;MLA Name&apos;, &apos;MLA Parameters&apos;,&apos;MLA Train Accuracy Mean&apos;, &apos;MLA Test Accuracy Mean&apos;, &apos;MLA Test Accuracy Min&apos; ,&apos;MLA Time&apos;] MLA_compare = pd.DataFrame(columns = MLA_columns) row_index = 0 for alg in MLA: #set name and parameters MLA_compare.loc[row_index, &apos;MLA Name&apos;] = alg.__class__.__name__ MLA_compare.loc[row_index, &apos;MLA Parameters&apos;] = str(alg.get_params()) #score model with cross validation: cv_results = model_selection.cross_validate(alg, X_train, y_train, cv =5,return_train_score=True) MLA_compare.loc[row_index, &apos;MLA Time&apos;] = cv_results[&apos;fit_time&apos;].mean() MLA_compare.loc[row_index, &apos;MLA Train Accuracy Mean&apos;] = cv_results[&apos;train_score&apos;].mean() MLA_compare.loc[row_index, &apos;MLA Test Accuracy Mean&apos;] = cv_results[&apos;test_score&apos;].mean() MLA_compare.loc[row_index, &apos;MLA Test Accuracy Min&apos;] = cv_results[&apos;test_score&apos;].min() #let&apos;s know the worst that can happen! row_index+=1MLA_compare.sort_values(by = [&apos;MLA Test Accuracy Mean&apos;], ascending = False, inplace = True) 当我们发现某个模型效果比较好的时候，我们仍然可以进一步调参。但是这种调参并不是每次会得到better result,有时候只是一个decent result。调参是个技术活。以上图中的 DecisionTreeClassifier为例进行调参。12345678param_grid = &#123;&apos;criterion&apos;: [&apos;gini&apos;, &apos;entropy&apos;], &apos;splitter&apos;: [&apos;best&apos;, &apos;random&apos;], &apos;max_depth&apos;: [None, 2,4,6,8,10], &apos;min_samples_split&apos;: [5,10,15,20,25], &apos;max_features&apos;: [None, &apos;auto&apos;, &apos;sqrt&apos;, &apos;log2&apos;] &#125;tune_model = model_selection.GridSearchCV(tree.DecisionTreeClassifier(), param_grid=param_grid, scoring = &apos;accuracy&apos;, cv = 5) cv_results = model_selection.cross_validate(tune_model, X_train, y_train, cv = 5) 使用的是sklearn 中的model_selection 模块，进行GridSearch，其实只是把调参过程自动化程序化。得到的结果是1230.863068608315 #train mean0.79803322024 # test mean0.77094972067 #test min 我们通过比对发现这个结果和上图的结果是稍微变差的。可视化显示各个算法的效率：1234sns.barplot(x=&apos;MLA Test Accuracy Mean&apos;, y = &apos;MLA Name&apos;, data = MLA_compare, color = &apos;m&apos;) plt.title(&apos;Machine Learning Algorithm Accuracy Score \n&apos;) plt.xlabel(&apos;Accuracy Score (%)&apos;) plt.ylabel(&apos;Algorithm&apos;) 对比之后我们选取几个效果比较“好”的模型，然后进行下一步的模型融合。12345678910111213141516171819MLA_best = [ #Ensemble Methods ensemble.AdaBoostClassifier(), # 0.76076 ensemble.BaggingClassifier(), # 0.72248 ensemble.GradientBoostingClassifier(), # 0.73684 ensemble.RandomForestClassifier(n_estimators = 60), # 0.72727 #GLM linear_model.LogisticRegression(C=1.0, penalty=&apos;l1&apos;, tol=1e-6), # 0.77990 linear_model.RidgeClassifierCV(), # 0.77033 linear_model.LogisticRegressionCV() #0.77033 ] row_index = 0 for alg in MLA_best: algname = alg.__class__.__name__ alg.fit(X_train, y_train) predictions = alg.predict(X_test) result = pd.DataFrame(&#123;&apos;PassengerId&apos;:test[&apos;PassengerId&apos;].as_matrix(), &apos;Survived&apos;:predictions.astype(np.int32)&#125;) result.to_csv(algname+&quot;.csv&quot;, index=False) # save the results row_index+=1 模型融合简单的说模型融合就是通过多个decent模型的结果通过某种方式的结合，产生了比原来单个模型better的结果。关于模型融合的详细内容，请移步另一篇文章模型融合(Ensemble learning)我们这里以stacking(二层)为例说明模型融合。1234567891011121314151617181920212223ntrain = train.shape[0] #891 ntest = test.shape[0] #418 SEED = 0 # for reproducibility NFOLDS = 5 # set folds for out-of-fold prediction kf =model_selection.KFold(n_splits=NFOLDS, random_state=SEED)# 封装算法基本操作 class SklearnHelper(object): def __init__(self, clf, seed=0, params=None): params[&apos;random_state&apos;] = seed self.clf = clf(**params) def train(self, x_train, y_train): self.clf.fit(x_train, y_train) def predict(self, x): return self.clf.predict(x) def fit(self,x,y): return self.clf.fit(x,y) def feature_importances(self,x,y): print(self.clf.fit(x,y).feature_importances_) return self.clf.fit(x,y).feature_importances_ 下面是定义五折交叉验证的方法，默认是三折。123456789101112131415161718def get_oof(clf, x_train, y_train, x_test): oof_train = np.zeros((ntrain,)) oof_test = np.zeros((ntest,)) oof_test_skf = np.empty((NFOLDS, ntest)) for i, (train_index, test_index) in enumerate(kf.split(x_train)): x_tr = x_train[train_index] y_tr = y_train[train_index] x_te = x_train[test_index] clf.train(x_tr, y_tr) oof_train[test_index] = clf.predict(x_te) oof_test_skf[i, :] = clf.predict(x_test) oof_test[:] = oof_test_skf.mean(axis=0) return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1) # 想让z变成只有一列，行数不知道多少 1234567891011121314151617181920212223242526272829303132from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier,AdaBoostClassifier, GradientBoostingClassifier# 定义四个不同的弱分类器的参数值 # Random Forest parameters rf_params = &#123; &apos;n_jobs&apos;: -1,&apos;n_estimators&apos;: 500,&apos;warm_start&apos;: True, &apos;max_depth&apos;: 6,&apos;min_samples_leaf&apos;: 2, &apos;max_features&apos; : &apos;sqrt&apos;,&apos;verbose&apos;: 0#&apos;max_features&apos;: 0.2, &#125; # Extra Trees Parameters et_params = &#123; &apos;n_jobs&apos;: -1,&apos;n_estimators&apos;:500,&apos;max_depth&apos;: 8,&apos;min_samples_leaf&apos;: 2,&apos;verbose&apos;: 0 #&apos;max_features&apos;: 0.5, &#125; # AdaBoost parameters ada_params = &#123; &apos;n_estimators&apos;: 500,&apos;learning_rate&apos; : 0.75 &#125; # Gradient Boosting parameters gb_params = &#123; &apos;n_estimators&apos;: 500,&apos;max_depth&apos;: 5,&apos;min_samples_leaf&apos;: 2, &apos;verbose&apos;: 0 #&apos;max_features&apos;: 0.2, &#125; # Support Vector Classifier parameters # svc_params = &#123; # &apos;kernel&apos; : &apos;linear&apos;,&apos;C&apos; : 0.025 # &#125; # 创建四个若分类器模型 rf = SklearnHelper(clf=RandomForestClassifier, seed=SEED, params=rf_params) et = SklearnHelper(clf=ExtraTreesClassifier, seed=SEED, params=et_params) ada = SklearnHelper(clf=AdaBoostClassifier, seed=SEED, params=ada_params) gb = SklearnHelper(clf=GradientBoostingClassifier, seed=SEED, params=gb_params) 1234567891011121314151617181920#X_train =X_train.values#X_test =X_test.values# 使用五折交叉方法分别计算出使用不同算法的预测结果，这些结果将用于Stacking的第二层预测 et_oof_train, et_oof_test = get_oof(et, X_train, y_train, X_test) # Extra Trees rf_oof_train, rf_oof_test = get_oof(rf,X_train, y_train, X_test) # Random Forest ada_oof_train, ada_oof_test = get_oof(ada, X_train, y_train, X_test) # AdaBoost gb_oof_train, gb_oof_test = get_oof(gb,X_train, y_train, X_test) # Gradient Boost rf_feature = rf.feature_importances(X_train,y_train) et_feature = et.feature_importances(X_train, y_train) ada_feature = ada.feature_importances(X_train, y_train) gb_feature = gb.feature_importances(X_train,y_train) feature_dataframe = pd.DataFrame( &#123;&apos;features&apos;: cols, &apos;Random Forest feature importances&apos;: rf_feature, &apos;Extra Trees feature importances&apos;: et_feature, &apos;AdaBoost feature importances&apos;: ada_feature, &apos;Gradient Boost feature importances&apos;: gb_feature &#125;) 接下来以第一层为为基础训练第二层12345678base_predictions_train = pd.DataFrame( &#123; &apos;RandomForest&apos;: rf_oof_train.ravel(),# # ravel函数在降维时默认是行序优先 &apos;ExtraTrees&apos;: et_oof_train.ravel(), &apos;AdaBoost&apos;: ada_oof_train.ravel(), &apos;GradientBoost&apos;: gb_oof_train.ravel() &#125;) X_train2 = np.concatenate(( et_oof_train, rf_oof_train, ada_oof_train, gb_oof_train), axis=1) X_test2 = np.concatenate(( et_oof_test, rf_oof_test, ada_oof_test, gb_oof_test), axis=1) 使用XGBoost训练第二层的数据。关于XGBoost为什么是有效的和相关的概念，请移步XGBoost123456789101112131415# XGboost import xgboost as xgbgbm = xgb.XGBClassifier( #learning_rate = 0.02, n_estimators= 2000, max_depth= 4, min_child_weight= 2, #gamma=1, gamma=0.9, subsample=0.8, colsample_bytree=0.8, objective= &apos;binary:logistic&apos;, nthread= -1, scale_pos_weight=1).fit(X_train2, y_train) predictions = gbm.predict(X_test2) 最后产生结果文件 StackingSubmission.csv12StackingSubmission = pd.DataFrame(&#123;&apos;PassengerId&apos;:test.PassengerId, &apos;Survived&apos;: predictions &#125;) StackingSubmission.to_csv(&quot;StackingSubmission.csv&quot;, index=False) # 0.78947 参考文献本文在效果可视化中借鉴该博客特征提取参看kaggle多位大神，在这里就谢过…]]></content>
      <categories>
        <category>竞赛</category>
      </categories>
      <tags>
        <tag>Kaggle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[模型融合]]></title>
    <url>%2F2018%2F06%2F05%2F%E6%A8%A1%E5%9E%8B%E8%9E%8D%E5%90%88%2F</url>
    <content type="text"><![CDATA[多个有差异性的模型融合可以提高整体的性能。它能同时降低最终模型的bias 和variance，从而在提高竞赛分数的同时降低overfitting 的风险。 从结果文件中融合这种做法不需要重新训练模型，融合竞赛提交的结果文件就可以，简单便捷。 Voting投票制：少数服从多数。如一个分类问题，多个模型的投票（当然可以设置权重，若没有就是平均投票），最终投票数最多的类就是被预测的类。对于加权表决融合，性能表现较差的模型（权值比较低）只能通过和其他模型保持一致增强自己的说服力。对于结果取平均融合，在不同的评估准则上也能获得不错的效果在于：取均值常常能减少过拟合的现象。如图所示：如果单个模型过拟合产生了绿色的边缘，这时候去平均这种策略使得决策边界变成黑色的边缘，这样的效果更好。机器学习的目的并不是让模型记住训练数据，而是具有更好的泛化性。 RankingRank的思想其实和Averaging一致，但Rank是把排名做平均，对于AUC指标比较有效。 训练模型融合 Bagging:使用训练数据的不同随机子集来训练每个 Base Model，最后进行每个 Base Model 权重相同的 Vote。也即 Random Forest 的原理。 Boosting:Boosting是一种迭代的方法，每一次训练会更关心上一次被分错的样本，比如改变被错分的样本的权重的Adaboost方法。也即 Gradient Boosting，Adaboost 的原理。比 Bagging 效果好，但更容易 Overfit。 Blending用不相交的数据训练不同的 Base Model，将它们的输出取（加权）平均。实现简单，但对训练数据利用少了。 Stackinig(以二层为例)在网上为数不多的关于stacking的内容中，相信你已经看过这张图片：PS:这不是原图，是在原图的基础上经过修改（把最上面标题的model 1，2,3,4,5 修改为model 1,1,1,1,1。因为这个一个model 的不同阶段，不是多个模型） 想比较而言我更加喜欢下面这张图片，因为它把stacking 的不同阶段表达的更加清楚，尤其是经过model 1之后，model 2是在model 1的基础上进行训练的。 对于第二阶段使用的样本集合，上图使用的是第一阶段的结果数据集，当然还有一种方式，如下图所示。 在该图中上一阶段的结果(prob 1-N)列和原始数据集组成新的特征向量，训练第二阶段模型。 名词解释 cross validation交叉验证当评估不同的参数设置，对算法表现的影响时，仍然存在则过拟合的风险。因为在调整参数，优化测试集的算法表现时，测试集的信息已经泄漏进模型中了。为了解决这个问题，需要一部分数据作为验证集(validation set)。 这样，用训练集(Train set)的数据训练模型；用验证集对模型参数调剂，如上述程序中的C值；最后，算法的评价在测试集(Test set)上完成。但是当数据有原来的两份化成三份之后，降低了寻数据量；另外算法的表现依赖于三个数据集的划分。解决上述两个问题的常见方法: cross validation. 测试集仍然单独划分出来，但是 validation set不用单独划分。将训练集划分为k个小的数据集，称之为k-fold CV。对每个fold进行下列过程： 用其他k-1 folds 作为training sets，训练模型 模型的结果用剩下的一个 fold进行评价模型的性能用上述循环中的 k-fold 交叉验证集的平均值表现，这样的做法增加了计算量，但是提高数据的利用效率。 参考文献https://blog.csdn.net/u013395516/article/details/79745063https://blog.csdn.net/u012969412/article/details/76636336https://blog.csdn.net/u012604810/article/details/77579782]]></content>
      <categories>
        <category>竞赛</category>
      </categories>
      <tags>
        <tag>Kaggle</tag>
        <tag>优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[XGBoost]]></title>
    <url>%2F2018%2F06%2F05%2FXGBoost%2F</url>
    <content type="text"><![CDATA[Introduction to XGBoostXGBoost is short for “Extreme Gradient Boosting”, where the term “Gradient Boosting” is proposed in the paper Greedy Function Approximation: A Gradient Boosting Machine, by Friedman. XGBoost is based on this original model. And most of the content is based on the websit(http://xgboost.readthedocs.io/en/latest/model.html) element of Supervised learningXGBoost is used for supervised learning problems, where we use the training data (with multiple features) xi to predict a target variable yi. Objective Function: Training Loss + RegularizationA very important fact about objective functions is they must always contain two parts: training loss and regularization. For example, a commomly used training loss is mean squared error. Another commonly used loss function is logistic loss for logistic regression The regularization term is what people usually forget to add. The regularization term controls the complexity of the model, which helps us to avoid overfitting.The general principle is we want both a simple and predictive model. The tradeoff between the two is also referred as bias-variance tradeoff in machine learning. Decision Tree and random forest Decision Trees: It is one of my favorite techniques. It can be used as a ultimate solution to tackle multiple challenges like missing values, outliers and identifying significant variables. It worked well in our Data Hackathon also. 现在理解可能是基于”增益“ 之类的东西，关于 RF 最后一条。 Random Forest: Similar to decision tree is Random Forest. I would also recommend using the in-built feature importance provided by random forests to select a smaller subset of input features. Just be careful that random forests have a tendency to bias towards variables that have more no. of distinct values i.e. favor numeric variables over binary/categorical values. Tree EnsembleThe tree ensemble model is a set of classification and regression trees(CART). Here is a simple example of a CART that classifies whether someone will like computer games. A CART is a bit different from decision trees, where the leaf only contains decision values. In CART, a real score is associated with each of the leaves, which gives us richer interpretations that go beyond classification. This also makes the unified optimization step easier, as we will see in a later part of this tutorial. Usually, a single tree is not strong enough to be used in practice. What is actually used is the so-called tree ensemble model, which sums the prediction of multiple trees together. Here is an example of a tree ensemble of two trees. The prediction scores of each individual tree are summed up to get the final score. If you look at the example, an important fact is that the two trees try to complement each other. Now here comes the question, what is the model for random forests? It is exactly tree ensembles! So random forests and boosted trees are not different in terms of model, the difference is how we train them. This means if you write a predictive service of tree ensembles, you only need to write one of them and they should directly work for both random forests and boosted trees. One example of why elements of supervised learning rock. Tree BoostingAfter introducing the model, let us begin with the real training part. How should we learn the trees? The answer is, as is always for all supervised learning models: define an objective function, and optimize it! Assume we have the following objective function (remember it always needs to contain training loss and regularization) Additive TrainingFirst thing we want to ask is what are the parameters of trees? You can find that what we need to learn are those functions fi, with each containing the structure of the tree and the leaf scores. This is much harder than traditional optimization problem where you can take the gradient and go. It is not easy to train all the trees at once. Instead, we use an additive strategy: fix what we have learned, and add one new tree at a time. It remains to ask, which tree do we want at each step? A natural thing is to add the one that optimizes our objective. Training modelThe XGBoost model for classification is called XGBClassifier(regression is called XGBRegressor). We can create and and fit it to our training dataset. Models are fit using the scikit-learn API and the model.fit() function.]]></content>
      <categories>
        <category>竞赛</category>
      </categories>
      <tags>
        <tag>XGBoost</tag>
        <tag>Kaggle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[等概率生成器]]></title>
    <url>%2F2018%2F05%2F28%2F%E7%AD%89%E6%A6%82%E7%8E%87%E7%94%9F%E6%88%90%E5%99%A8%2F</url>
    <content type="text"><![CDATA[总体来说是可以有两种思路：一种是二级制的理解，一种是生成更大的数字的集合，然后求余+剪枝（重投）。 给定一个可以等概率生成1-3的rand3()函数生成器，求解可以随机等概率生成1-7的rand7()函数生成器。 解题思路 使用rand3()函数生成1-9的数字，然后丢弃8,9两种可能。 实现代码 12345678910class Solution: def random7(self): x =8 while x&gt;7: x =self.random3() + (self.random3()-1)*3 return x def random3(self): return if __name__ =="__main__": print(Solution().random7()) Given a random number generator rand5() generate gen7() 12345def rand7a(): rand7 =22 while rand7&gt;=21: rand7 =rand5()+ rand5()*5 return rand7 %7 1st approach:rand2() in binary is 000 or 001 with 50-50 probability.rand2()2 is 000 or 010 with 50-50 probability.rand2()4 is 000 or 100 with 50-50 probability.So the sum is binary xxx where each x has a 50-50 probability of 0 or 1; so each 000 to 111 has a probablilty of 1/8. 1234567def rand7b(): rand7 =7 while rand7 ==7: rand7 =rand2() +rand2()*2+ rand2()*4 return rand7 2nd approach: going through each of the 8 possibilities of the 3 rand2()s:0+0+0 or 1+0+0 or 0+2+0 or 1+2+0 or 0+0+4 or 1+0+4 or 0+2+4 or 1+2+4all of these with equal probablilty 1/8. 从大数字的 random 到小数字的random，需要做的是等概率的生成，所以对于不符合要求的是 就是 重新rand() 12345int rand2() &#123; int x = rand5(); if x == 4 return rand2(); // restart else return x % 2;&#125; Given rand2(), you should get rand5() 这个是可以从二进制的角度进行解析的。 12345int rand7() &#123; int x = rand2() * 4 + rand2() * 2 + rand2(); if (x == 7) return rand7(); // restart else return x;&#125; 其实不管 rand7() 是不是产生了7，即使产生了7 那么也是可以通过 “if condition” 进行提前进行处理的。 这种算法都不是唯一的。]]></content>
      <categories>
        <category>算法</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[机器学习概念]]></title>
    <url>%2F2018%2F05%2F28%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[解释一些机器学习中小的基本的概念。机器学习的优化方法有很多，这里主要解释的是 Gradient Descent 和 Newton’s Method. 优化方法 梯度下降（gradient descent） 梯度下降法是最早最简单，也是最为常用的最优化方法。梯度下降法实现简单，当目标函数是凸函数时，梯度下降法的解是全局解。接下来衍生的的子类:批量梯度下降法（Batch Gradient Descent，BGD）随机梯度下降（Stochastic Gradient Descent，SGD） 牛顿法和拟牛顿法（Newton’s method &amp; Quasi-Newton Methods） 牛顿法最大的特点就在于它的收敛速度很快。个人感觉牛顿法只是在在每次迭代的时候进行了两次运算，和梯度下降在总的运算次数上并没有很大的差别，为什么会收敛速度更快呢？ 名词解释 凸优化:对凸优化的问题我们在基础数学上面已经有了很多解决方法，例如可以将凸优化问题Lagerange做对偶化，然后用Newton、梯度下降算法求解。凸集合: 凸函数：Jacobian矩阵和Hessian矩阵：在向量分析中, 雅可比矩阵是一阶偏导数以一定方式排列成的矩阵, 其行列式称为雅可比行列式。Hessian矩阵： bias and variance偏差和方差又与「欠拟合」及「过拟合」紧紧联系在一起。由于随机误差是不可消除的，所以此篇我们讨论在偏差和方差之间的权衡（Bias-Variance Tradeoff）。 当模型处于欠拟合状态时，训练集和验证集上的误差都很高； 当模型处于过拟合状态时，训练集上的误差低，而验证集上的误差会非常高。 这两个概念和模型的复杂度又有着很深的联系：]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>优化</tag>
        <tag>Gradient Descent</tag>
        <tag>Newton&#39;s Method</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[求解平方根]]></title>
    <url>%2F2018%2F05%2F28%2F%E6%B1%82%E8%A7%A3%E5%B9%B3%E6%96%B9%E6%A0%B9%2F</url>
    <content type="text"><![CDATA[求解平方根的算法主要有两种：二分法(binary search) 和牛顿迭代法(Newton’s Method) Just give me codes…1234567891011121314151617181920212223242526272829from math import sqrt# you needn't import sqrt, I do that just for comparing the results of different methodsclass Solution: # in-built function def my_sqrt(self, n): return sqrt(n) def sqrt_binary(self, n): low, high =0,n mid =int((low+high)/2) while abs(mid*mid- n)&gt; 1e-9: if mid*mid&gt;n: high =mid else: low =mid mid =(low+high)/2 return mid # Newton method: math def newton_method(self, n): k =1 while abs(k*k-n) &gt;1e-9: k =(k+n/k)/2 return k if __name__ =="__main__": print(Solution().my_sqrt(2)) print(Solution().sqrt_binary(2)) print(Solution().newton_method(2)) 运行结果1231.4142135623730951 #built-in function1.4142135623842478 # binary search1.4142135623746899 # Newton method 从结果中看，Newton method比 binary sqrt更加接近系统自带的sqrt function. 并且从数学上可以证明 Newton method 比 binary sqrt需要更少的迭代次数。 附录：牛顿迭代法是求方程根的重要方法之一，其最大优点是在方程f(x) = 0的单根附近具有平方收敛，而且该法还可以用来求方程的重根,复根。牛顿迭代法结论其实就是取泰勒级数前两项等于0求得的,泰勒公式表示为更简练的写法为：通过 Newton methond逼近方程的解的过程可以表示为下图： referrences:https://blog.csdn.net/ycf74514/article/details/48996383]]></content>
      <categories>
        <category>算法</category>
      </categories>
  </entry>
</search>
