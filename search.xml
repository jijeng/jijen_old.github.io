<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[图相关算法]]></title>
    <url>%2F2019%2F12%2F17%2Fgraph_algorithms%2F</url>
    <content type="text"><![CDATA[之前在介绍宽度优先遍历、深度优先遍历、拓扑排序、最小生成树、并查集的时候零零散散介绍过图，这篇以图为中心，介绍相关的算法。 定义和表示（1）定义 An undirected graph is connected if every pair of vertices is connected by a path. A forest is an acyclic graph, and a tree is a connected acyclic graph. A graph that has weights associated with each edge is called a weighted graph.从图的角度解读树。 统一符号，$V$（vertex）表示顶点，$E$（edge）表示边（权重）。 （2）邻接表表示 The adjacency list representation of a graph G = (V, E) consists of an array $Adj_{[1..|V |]} $ of lists. Each list $Adj_{[v]} $is a list of all vertices adjacent to v. （3）邻接矩阵表示 Adjacency matrices have a value $a_{i,j} = 1 $ if nodes i and j share an edge; 0 otherwise. In case of a weighted graph, $a_{i,j} = w_{i,j} $, the weight of the edge.通过0和1 标识是否存在边；是对称矩阵，可以只存储上三角形和下三角形。 （4）分类 对于图来说，根据有无权重可以分为两类；根据是否有方向可以分为两类；根据是否有环，可以分成两类；根据点和边的个数相对大小，可以分为稠密图和稀疏图；并且这种是可以线性组合的，比如说有向无权重，有向有权重。 图的遍历算法Graph Search Algorithms 宽度优先算法 Breadth-first search (BFS) is an algorithm for traversing or searching tree or graph data structures. It starts at the tree root (or some arbitrary node of a graph, sometimes referred to as a search key), and explores all of the neighbor nodes at the present depth prior to moving on to the nodes at the next depth level.宽度优先（Breadth-First Search）是图或者树的一种遍历算法。宽度优先算法从树或者图的某一点出发，首先遍历当前节点周围邻居节点，然后进入下一层的遍历。这种遍历的思想和队列非常相像，所以在BFS 中经常出现队列的数据结构。 深度优先算法 Depth-first search (DFS) is an algorithm for traversing or searching tree or graph data structures. The algorithm starts at the root node (selecting some arbitrary node as the root node in the case of a graph) and explores as far as possible along each branch before backtracking.深度优先算法（Depth-First Search）选择一个根节点（或者随意一个起点）然后沿着一条路径尽可能走下去，当走不通的时候就回溯。这种思想和栈非常类似。 最小生成树 A spanning tree of an undirected graph G is a subgraph of G that is a tree containing all the vertices of G.生成树的定义：包含所有节点且包含部分边的图。 A minimum spanning tree (MST) for a weighted undirected graph is a spanning tree with minimum weight. 最小生成树的定义：在有权重的图中，图权重为所有边权重之后，那么最小生成树就是所以生成树中权重和最小大的。 Prim Algorithm In computer science, Prim’s (also known as Jarník’s) algorithm is a greedy algorithm that finds a minimum spanning tree for a weighted undirected graph. This means it finds a subset of the edges that forms a tree that includes every vertex, where the total weight of all the edges in the tree is minimized. The algorithm operates by building this tree one vertex at a time, from an arbitrary starting vertex, at each step adding the cheapest possible connection from the tree to another vertex.基于贪心的策略。从任意一点开始，每次迭代选择的是点（最小的边是附带产物），适合在稠密图中使用。时间复杂度$O(V^2 +E)$，其中$V$表示点的个数， $E$表示边的个数。 A demo for Prim’s algorithm based on Euclidean distance. 图中所示，给定一个起点（任意一个起点），然后选择距离该点距离最近（这里是欧式距离）点，加入到现有的点的集合中，以此类推，直到有$E-1$条边或者连接了所有的点。 Prim算法求最小生成树 时间复杂度是 $O(V^3)$， 其中 $V$表示点的个数。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849#include&lt;iostream&gt;#include&lt;cstring&gt;#include&lt;algorithm&gt;using namespace std;const int N =510, INF =0x3f3f3f3f;int n, m;int g[N][N];int dist[N];bool st[N];int prim()&#123; memset(dist, 0x3f, sizeof dist); int res =0; for(int i =0; i&lt; n; i++) &#123; int t =-1; // 选择权重最小的边 for (int j =1; j&lt;=n; j++) &#123; if(!st[j] &amp;&amp; (t ==-1 || dist[t] &gt; dist[j])) t =j; &#125; if(i &amp;&amp; dist[t] ==INF) return INF; if(i) res += dist[t]; for(int j =1; j&lt;=n; j++) dist[j] =min(dist[j], g[t][j]); st[t] =true; &#125; return res;&#125;int main()&#123; cin &gt;&gt; n&gt;&gt;m; memset(g, 0x3f, sizeof g); while(m --) &#123; int a, b,c; cin &gt;&gt;a&gt;&gt;b&gt;&gt;c; g[a][b] =g[b][a] =min(g[a][b], c); &#125; int t =prim(); if(t ==INF) cout &lt;&lt; "impossible" &lt;&lt; endl; else cout &lt;&lt; t &lt;&lt; endl; return 0;&#125; Kruskal Algorithm基于贪心的策略。从权重最小的边开始，每次迭代选择的是边（点是附带的产物），适合在稀疏图中使用。时间复杂度$ElogE$，其中 $E$表示边的个数， $V$表示点的个数（虽然没有用到）。算法步骤（A demo for Kruskal algorithm based on Euclidean distance.）： 图中所示，每次选择是距离最短的边（顺便把点连接起来），在这过程中需要判断是否形成了环（如果是回溯），直到连接了所有的点或者是$V-1$条边。 Kruskal算法求最小生成树 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455#include&lt;iostream&gt;#include&lt;algorithm&gt;using namespace std;const int N =1e5+11, M =2e5+11, INF =0x3f3f3f3f;int n, m;int p[N];struct Edge&#123; int a, b, w;&#125;edges[M];int find(int x)&#123; // 这个是递归的定义，只需要一次if 就行，不要使用 while if(p[x] != x) p[x] =find(p[x ]); return p[x];&#125;bool cmp(Edge a, Edge b)&#123; return a.w&lt; b.w;&#125;int kruskal()&#123; sort(edges, edges +m, cmp); for(int i =1; i&lt;=n; i++) p[i] =i; int res =0, cnt =0; for(int i =0; i&lt;m ;i++) &#123; int a =edges[i].a, b =edges[i].b, w =edges[i].w; a =find(a), b =find(b); if(a !=b) &#123; p[a]=b; res +=w; cnt ++; &#125; &#125; if (cnt !=n -1) return INF; return res;&#125;int main()&#123; cin &gt;&gt; n&gt;&gt;m; for(int i =0; i&lt;m; i++) &#123; int a, b, w; cin &gt;&gt;a&gt;&gt;b&gt;&gt;w; edges[i] =&#123;a, b, w&#125;; &#125; int t =kruskal(); if(t ==INF) puts("impossible"); else cout&lt;&lt;t&lt;&lt;endl;&#125; 最短路径最短路径的常用算法有迪杰克斯拉算法（Dijkstra Algorithm），贝尔曼福特算法（Bellman-Ford Algorithm）和弗洛伊德算法（Floyd-Warshall Algorithm）。相同点：三个算法的核心思想：边$(u, v)$是从结点$u$到结点$v$， 如果 $dist(v) &gt; dist(u) + w(u, v)$，那么 dist(v) 就可以被更新。不同点：其中Floyd 算法是多源最短路径，即求解任意点到任意点的最短路径，而Dijkstra 算法和Bellman-Ford Algorithm 是求解单源最短路径，即单个点到任一点的最短路径。其中Dijkstra算法要求权值全部为正，其他的两种可以处理负权边，但是不能出现负环（所谓的负环，就是权值总和为负的环）。 Dijkstra’s Algorithm单源最短路径算法（Single-Source Shortest Paths）的特点： Dijkstra 是起点到终点（其他所有点）的最短路径（单源） 要求权值非负 堆优化之后时间复杂度 $O(Elog V)$， $E$表示边数，$V$ 表示点数；普通的算法时间是 $O(V^2 + E)$ 如果想要得到所有点的最短路径，那么需要在 $V$个点上执行相同的操作，总的时间复杂度是 $O(V^3)$ 算法步骤示意图： （1） Network Delay Time 时间复杂度是$O(E +VlogV)$, 空间复杂度是$O(V+E)$ 12345678910111213141516171819202122import heapqclass Solution: def networkDelayTime(self, times: List[List[int]], N: int, K: int) -&gt; int: graph=collections.defaultdict(dict) for source, dest, cost in times: graph[source][dest] =cost distances =&#123;i : float('inf') for i in range(1, N+1)&#125; distances[K] =0 min_heap=[(0, K)] visited =set() while min_heap: dis, vec = heapq.heappop(min_heap) if vec in visited: continue visited.add(vec) for neighbor in graph[vec]: if neighbor in visited: continue new_dis =dis + graph[vec][neighbor] if new_dis &lt; distances[neighbor]: distances[neighbor] =new_dis heapq.heappush(min_heap, (new_dis, neighbor)) if len(visited) != len(distances): return -1 return max(distances.values()) （2）Cheapest Flights Within K Stops 多关键字的最短路径问题，时间复杂度是$O(E + Vlog KE)$，其中 $E$表示边的个数， $V$表示点的个数。 12345678910111213141516class Solution: def findCheapestPrice(self, n: int, flights: List[List[int]], src: int, dst: int, K: int) -&gt; int: graph =collections.defaultdict(list) for beg, end, w in flights: graph[beg].append([w, end]) min_heap =[[0, src, 0]] # 如果是小根堆，那么使用python实现，大根堆使用c++ 实现 while min_heap: w, beg , stops =heapq.heappop(min_heap) if beg == dst: return w if stops &gt; K: continue for nex_w, nex_dst in graph[beg]: heapq.heappush(min_heap, [w + nex_w, nex_dst, stops +1]) return -1 Bellmann-Ford Algorithm The basic idea of SPFA is the same as Bellman–Ford algorithm in that each vertex is used as a candidate to relax its adjacent vertices. The improvement over the latter is that instead of trying all vertices blindly, SPFA maintains a queue of candidate vertices and adds a vertex to the queue only if that vertex is relaxed. This process repeats until no more vertex can be relaxed.Bellmann-Ford Algorithm (SPFA，Shortest Path Faster Algorithm)用于处理有环且含有负权重的加权有向图。基本的原理是对图进行 V-1次松弛操作，得到所有可能的最短路径。 Bellmann-Ford Algorithm的特点： 用于求含有负权边的最小生成树或者判断负权环 队列优化之后，平均时间 $O(E)$，最坏的情况下是 $O(EV)$，其中 $E$表示边数， $V$表示点数 A demo of SPFA based on Euclidean distance. Red lines are the shortest path covering (so far observed). Blue lines indicate where relaxing happens, i.e., connecting $v$ with a node $u \in Q$, which gives a shorter path from the source to $v$. （1） Network Delay Time 1234567891011121314class Solution: def networkDelayTime(self, times: List[List[int]], N: int, K: int) -&gt; int: arr =[float('inf')] *N arr[K-1] =0 update =True while update: update =False for e in times: x, y , w =e[0] -1, e[1] -1, e[2] if arr[x] &lt; float('inf') and arr[y] &gt; arr[x] +w: arr[y] =arr[x] +w update =True return max(arr) if max(arr) &lt; float('inf') else -1 Floyd 算法 In computer science, the Floyd–Warshall algorithm (also known as Floyd’s algorithm, the Roy–Warshall algorithm, the Roy–Floyd algorithm, or the WFI algorithm) is an algorithm for finding shortest paths in a weighted graph with positive or negative edge weights (but with no negative cycles). The Floyd–Warshall algorithm is an example of dynamic programming A single execution of the algorithm will find the lengths (summed weights) of shortest paths between all pairs of vertices. Although it does not return details of the paths themselves, it is possible to reconstruct the paths with simple modifications to the algorithm. Floyd 算法是任意两点之间的最短路径（多源） 权值可以为负数，不能有负数的环 时间复杂是$O(V^3)$， $V$表示点的个数 （1） Network Delay Time 时间复杂度是 $O(V^3)$， 空间是 $O(V^2)$。Floyd相对于 Dijkstra 算法是比较简单，但是时间复杂度是比较高。 12345678910111213class Solution: def networkDelayTime(self, times: List[List[int]], N: int, K: int) -&gt; int: dist = [[float('inf') for _ in range(N)] for _ in range(N) ] for u, v, w in times: dist[u-1][v-1] =w for i in range(N): dist[i][i] =0 for k in range(N): for i in range(N): for j in range(N): dist[i][j] =min(dist[i][j], dist[i][k] + dist[k][j]) return max(dist[K-1]) if max(dist[K-1]) &lt; float('inf') else -1 拓扑排序见Breadth First Search 中有向图小结。 连通性拓扑排序判断是否有环（无环图的所有点都是可以进行拓扑排序） 并查集Redundant Connection 时间复杂度是$O(n)$， 讲解。 1234567891011121314151617181920212223242526272829303132333435363738394041class Solution &#123;public: vector&lt;int&gt; p; void init(int n) &#123; for(int i =0; i&lt; n; i++) p.push_back(i); &#125; int find(int x) &#123; if(x != p[x]) p[x] =find(p[x]); return p[x]; &#125; bool is_union(int x, int y) &#123; int t1 =find(x); int t2 =find(y); if( t1 ==t2) return true; else &#123; p[t1] =p[t2]; return false; &#125; &#125; vector&lt;int&gt; findRedundantConnection(vector&lt;vector&lt;int&gt;&gt;&amp; edges) &#123; int n =edges.size() ; init(n+1); vector&lt;int&gt; res(2, 0); for(int i =0; i&lt; n; i++) &#123; int x =edges[i][0]; int y =edges[i][1]; if(is_union(x, y)) &#123; res[0] =x; res[1] =y; &#125; &#125; return res; &#125;&#125;; 并查集讲解 参考文献 WikipediaLeetcode]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>graph_algorithms</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[可迭代对象 vs. 迭代器 vs. 生成器]]></title>
    <url>%2F2019%2F12%2F01%2Fiterable-python%2F</url>
    <content type="text"><![CDATA[在python 中经常遇到以下的概念：容器（container）、可迭代（iterable）、迭代器（iterator）和生成器（generator）。上述概念经常混淆，整理一下，总结为图： 容器（container） 容器（container）从字面的意思上很好理解，包含一定数量元素的物体。比如说 list, set, tuple, dictioinary , string等。容器的特点在于数量有限，大多数是可迭代，布隆过滤器（Bloom filter）一般是不可迭代的，因为太大了。 可迭代（iterable） 容器大多数是可迭代，但是可迭代的不一定是容器，不一定是某个数据结构，比如按行读取文件，打开一个sockets等操作。容器数据量一般是有限的，但是sockets操作可能是无限的数据量。所以任何物体都有可能满足可迭代性质。 在python 语言，可迭代类需要实现___iter__() 和 __next__() 两个方法。下面的例子使用 Fibonacci数建立一个迭代器。 12345678910111213class fib: def __init__(self): self.prev =0 self.cur =1 def __iter__(self): return self def __next__(self): val =self.cur self.cur += self.prev self.prev =val return valf =fib()list(islice(f, 0, 10 )) #[1, 1, 2, 3, 5, 8, 13, 21, 34, 55] islice 是itertools中切片选择的函数，其中，iterable 是可迭代对象，start 是开始索引，stop 是结束索引，step 是步长，start 和 step 可选。1islice(iterable, [start,] stop [, step]) 迭代器（iterators） 任何具有__next__() 方法的物体都是迭代器，使用 next()方法或者 ‘for’循环可以获取下一个元素。python中的模块 itertools 可以用于构建高效的迭代器。迭代的特点是惰性求值（lazy evaluation），即只要当迭代到该值的时候，才会被计算，非常适合于大文件或者无限集合的遍历，不用一次性存储在内存中。该模块中的迭代器类型分为以下三种： 无限迭代器 有限迭代器 组合迭代器 (1) 无限迭代器 count(firstval=0, step=1)创建一个从 firstval (默认值为 0) 开始，以 step (默认值为 1) 为步长的的无限整数迭代器 123from itertools import countcounter =count(start =13)next(counter) cycle(iterable)对 iterable 中的元素反复执行循环，返回迭代器 123456from itertools import cyclecolors =cycle(['red', 'white', 'blue'])next(colors) # rednext(colors)next(colors)next(colors) # red repeat(object [,times]反复生成 object，如果给定 times，则重复次数为 times，否则为无限 (2) 有限迭代器 chain() dropwhile() groupby() chain的使用123from itertools import chainfor item in chain([1, 2, 3], [&apos;a&apos;, &apos;b&apos;, &apos;c&apos;]): print item dropwhile的使用1dropwhile(predicate, iterable) 其中，predicate 是函数，iterable 是可迭代对象。对于 iterable 中的元素，如果 predicate(item) 为 true，则丢弃该元素，否则返回该项及所有后续项。（这种逻辑还是比较有意思的，只是进行一次的筛选）123from itertools import dropwhilelist(dropwhile(lambda x: x &lt; 5, [1, 3, 6, 2, 1])) #[6, 2, 1]list(dropwhile(lambda x: x &gt; 3, [2, 1, 6, 5, 4])) #[2, 1, 6, 5, 4] (3) 组合生成器itertools 模块还提供了多个组合生成器函数，用于求序列的排列、组合等： product permutations combinations combinations_with_replacement product用于求多个可迭代对象的笛卡尔积，和嵌套的for 循环是等价的123from itertools import productfor item in product(&apos;ABCD&apos;, &apos;xy&apos;): print item permutations用于生成一个排列1permutations(iterable[, r]) 其中，r 指定生成排列的元素的长度，如果不指定，则默认为可迭代对象的元素长度。12from itertools import permutationspermutations('ABC', 2) combinations用于求序列的组合，它的使用形式如下12from itertools import combinationslist(combinations('ABC', 2)) 迭代器函数的返回值是迭代器，而不是list。 并且上述容器可以转换成迭代器。1234x =[1, 2, 3]z =iter(x)next(z)next(z) 生成器（generator） 生成器必然是一种迭代器。写法更加简洁，不用实现__iter__() 和__next__() 方法，需要关键字 yield就ok。还是使用 Fibonacci数作为例子。1234567def fib(): pre, cur =0,1 while True: yield cur pre, cur =cur, pre+ curf =fib()list(islice(f, 0, 10)) 有木有代码量骤减的感觉。 在python 中有两种类型的生成器：生成器函数和生成器表达。包含关键字yield 就是生成器函数（如上例子），生成器表达更加简洁，相当于list comprehension。比如1234567891011numbers =[1, 2, 3, 4, 5]# list comprehension[x*x for x in numbers]# set comprehension&#123;x*x for x in numbers&#125;# or dict comprehension&#123;x: x*x for x in numbers&#125;# not a tuple comprehension, 这个是一个 生成器表达lazy= (x*x for x in numbers) # a generatornext(lazy)lsit(lazy) 在处理训练数据集的时候，从12345def something(): res =[] for ... in ..: res.append(x) return res 转换成123def iter_something(): for ... in ...: yield x # yield 并没有结束，等待下一个 next() 12345678910111213141516numbers = list()# range()for i in range(1000): numbers.append(i + 1)total = sum(numbers)# (2) Using a generatordef generate_numbers(n): num = 0 while num &lt; n: yield num # 这个yield 之后，函数并没有结束，不像 return 那种函数 num += 1total = sum(generate_numbers(1000))print(total)total = sum(range(1000 + 1))print(total) 123456789101112131415161718192021222324252627282930313233# generator 的第一种实现 ()g =(x*x for x in range(10))print(next(g))print(next(g))# 实现一个 image loader# generator的第二种实现 yield 关键字实现，def read_data(file1): f =open(file1) x =[] y =[] while True: line =f.readline() if not line: break line =line.split() x =line[:-1] y =line[-1] y =map(float, y) data.append(line) return datadef train_data(file1, batch_size): train_x, train_y =read_data(file1) num_batch =len(datas) //batch_size for i in range(num_batch): x = train_x[batch_size*i: batch_size*(i+1)] y =train_y[batch_size*i: batch_size*(i+1)] yield np.array(x), np.array(y)next(train_data(file1, batch_size)) 参考文献 Iterables vs. Iterators vs. GeneratorsBloom filter - Wikipedia]]></content>
      <tags>
        <tag>iterable</tag>
        <tag>generator_python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[贪心算法]]></title>
    <url>%2F2019%2F11%2F29%2Fgreedy-algorithm%2F</url>
    <content type="text"><![CDATA[贪心算法，分为定义，使用条件，不足之处和常见例题（代码）等部分。 概念（1）定义 A greedy algorithm is any algorithm that follows the problem-solving heuristic of making the locally optimal choice at each stage with the intent of finding a global optimum.贪心算法根据当前一步的局部最优解来求解最后的全局最优解。所谓启发式（heuristic）是在可接受的代价（计算时间和空间）条件下，给出一个可行解，可行解和最优解偏离程度无法保证，计算时间无法保证。 （2）使用条件1). 贪心选择性质（Greedy choice property） A global (overall) optimal solution can be reached by choosing the optimal choice at each step.(hard to prove its correctness!). If you make a choice that seems the best at the moment and solve the remaining sub-problems later, you still reach an optimal solution. You will never have to reconsider your earlier choices.在每一步都是计算最优解，不需要考虑之前的决策，最优解可以局部最优解获得。（很难去证明） 2). 最优子结构（optimal substructure） A problem has an optimal substructure if an optimal solution to the entire problem contains the optimal solutions to the sub-problems.全局的最优解包含子问题的最优解。比如说在起点$P_s$到终点$P_e$的全局最优路径包含中间某点 $P_m$到终点 $P_e$的最优路径。 （3）贪心算法和动态规划算法的异同 1). 都满足最优子结构的性质2). 全局最优解可以通过局部最优解得到 是和动态规划的主要区别。动态规划是自底向上，而贪心算法通常是自顶向下求解。 （4）优缺点优点：一个算法问题使用暴力解可能需要指数级别的时间；使用动态规划思想消除重复的子问题，可以达到多项式级别的复杂度；如果满足贪心选择性质，可以进一步降低到线性级别的复杂度。所以如果能使用贪心，那么时间复杂度是比较小的。缺点： Sometimes greedy algorithms fail to find the globally optimal solution because they do not consider all the data. The choice made by a greedy algorithm may depend on choices it has made so far, but it is not aware of future choices it could make. 例题（代码）（1）Fractional Knapsack problem（分数，小数背包问题） 问题描述：给定n种物品和一个背包。物品i的重量是Wi，其价值为Vi，背包的容量为C 应如何选择装入背包的物品 使得装入背包中物品的总价值最大? 这里，在选择物品i装入背包时，可以选择物品i的一部分，而不一定要全部装入背包，不能重复装载。（物品可以任意切分） （如果问题不可分，那么应该参考动态规划中的01 背包问题） （2）区间调度问题（Interval Scheduling） 1). Non-overlapping Intervals 找到不会重叠的区间，那么剩下的就是需要去除的区间。下图更加形象说明这一过程 12345678910111213141516171819202122class Solution &#123;public: static bool cmp(vector&lt;int&gt; &amp;x, vector&lt;int&gt; &amp;y) &#123; return x[1] &lt; y[1]; &#125; int eraseOverlapIntervals(vector&lt;vector&lt;int&gt;&gt;&amp; intervals) &#123; if(intervals.empty()) return 0; sort(intervals.begin(), intervals.end(), cmp); int ans =1; int right =intervals[0][1]; for(int i =1; i&lt; intervals.size(); i++) &#123; if(right &lt;= intervals[i][0]) &#123; ans +=1; right =intervals[i][1]; &#125; &#125; return intervals.size() -ans; &#125;&#125;; c++语法123对于双元素（pair or 结构体），默认是先按照第一个元素增序排列，然后按照第二个元素增序排列；如果是自定义排序方法，那么就是按照第二个元素排序，第一个元素是无序的。static 静态修饰 cmp 函数 python代码实现。 123456789101112class Solution: def eraseOverlapIntervals(self, intervals: List[List[int]]) -&gt; int: if not intervals: return 0 # 贪心算法 intervals.sort(key =lambda x: x[1]); counts =1 right =intervals[0][1] for i in range( len(intervals)): if(right &lt;= intervals[i][0]): counts +=1 right =intervals[i][1] return len(intervals) -counts 2). 活动安排的题目 c++ 实现。贪心算法贪心算法+ 大根堆，思路和区间问题差不多按照结束时间排序，统计一个总的时长，如果总时长没有超过 结束时间，那么就可以加到里面；否则弹出耗时最长的那个c++ 中默认是大根堆, python 中默认是小根堆；因为c++ 是比较大的，所以大根堆 12345678910111213141516171819202122232425262728293031class Solution &#123;public: static bool cmp(vector&lt;int&gt; &amp;a, vector&lt;int&gt; &amp;b) &#123; return a[1]&lt; b[1]; &#125; int scheduleCourse(vector&lt;vector&lt;int&gt;&gt;&amp; courses) &#123; priority_queue&lt;int&gt; max_heap; sort(courses.begin() , courses.end(), cmp); int t2n = 0; // time to noew for(int i =0; i&lt; courses.size(); i++) &#123; if(t2n +courses[i][0] &lt;= courses[i][1]) &#123; max_heap.push(courses[i][0]); t2n += courses[i][0]; &#125; else if(!max_heap.empty()) &#123; if(max_heap.top() &gt; courses[i][0]) &#123; t2n = t2n -max_heap.top() +courses[i][0]; max_heap.pop(); max_heap.push(courses[i][0]); &#125; &#125; &#125; return max_heap.size(); &#125;&#125;; python中使用 heapq.heappush() 和heapq.heappop() 两个方法构建小根堆的push 和pop 操作，需要另外申请一个list。 12345678910111213class Solution: def scheduleCourse(self, courses: List[List[int]]) -&gt; int: # 使用大根堆保存时长，按照结束时间进行排序（时间安排类问题常见的做法） courses.sort(key =lambda x: x[1]) # python 中使用 heapq.heappush() heapq.heappop() 来构建大根堆和小根堆 t2n =0 min_heap =[] for t, d in courses: heapq.heappush(min_heap, -t) t2n += t if t2n &gt;d: t2n += heapq.heappop(min_heap) return len(min_heap) （2）旅行商问题（ Travelling Salesman ） The travelling salesman problem (also called the travelling salesperson problem[1] or TSP) asks the following question: “Given a list of cities and the distances between each pair of cities, what is the shortest possible route that visits each city and returns to the origin city?” It is an NP-hard problem in combinatorial optimization, important in operations research and theoretical computer science. 给定若干个城市和每个城市之间的距离，寻找一条最短可能的路径使得遍历每个城市，并且回到原先出发的城市。这是一个NP 问题，意味着无法在多项式的时间内解决，随着城市个数的增加，排雷组合的方式增长（阶乘，非指数）。目前其中一种解决方案是基于启发式的方式，贪心。 （3）best time to buy and sell stock 1). best-time-to-buy-and-sell-stock 贪心，基于当前计算profit 和买入点；然后遍历全部。最多进行一次交易（买入一次，卖出一次） 1234567891011121314class Solution &#123;public: int maxProfit(vector&lt;int&gt;&amp; prices) &#123; if(prices.empty()) return prices; // 贪心，选择当前最优的；遍历一遍更新 int profit =0, buy =prices[0]; for(int i =1; i&lt; prices.size(); i++) &#123; profit =max(profit, prices[i]- buy); buy =min(buy, prices[i]); &#125; return profit; &#125;&#125;; pyhton版本 12345678910111213class Solution(object): def maxProfit(self, prices): """ :type prices: List[int] :rtype: int """ if not prices: return 0 buy =prices[0] profit =0 for i in range(1, len(prices)): profit =max(profit, prices[i] -buy) buy =min(buy, prices[i]) return profit 2). Best Time to Buy and Sell Stock II 第二版本是能够买卖无限次。还是使用贪心的思想。只有是当前价格大于前一天的价格，那么就买入。 c++ 版本 12345678910class Solution &#123;public: int maxProfit(vector&lt;int&gt;&amp; prices) &#123; if (prices.empty()) return 0; int profit =0; for(int i =1; i&lt; prices.size() ; i++) profit += prices[i] &gt;prices[i-1] ? prices[i] -prices[i-1] :0; return profit; &#125;&#125;; python 版本1234567891011class Solution(object): def maxProfit(self, prices): """ :type prices: List[int] :rtype: int """ if not prices: return 0 profit =0; for i in range(1, len(prices)): profit += prices[i] -prices[i-1] if prices[i] &gt; prices[i-1] else 0 return profit 还可以使用动态规划的思想。 状态表示 $f(i)$ 表示第$i$ 天，不持有股票的最大收益， $g(i)$ 表示第$i$ 天，持有股票的最大收益 转移方程， $f(i)$第 $i$天不持有股票，有两种可能性：前一天不持有股票或者前一天持有并且当天卖出，两者取最大值；$g(i)$第$i$天持有股票，有两种可能性：前一天持有股票或者前一天不持有股票当天买入，然后两者取最大值。数学表达为$$\begin{split}f(i) &amp;= max(f(i-1), g(i-1) + prices[i]) \\g(i) &amp;= max(g(i-1), f(i-1) -prices[i])\end{split}\tag{1.3}$$ 初始化, $f(0) =0$, $g(0)=-\infty $ 时间复杂度 $O(n)$，空间复杂度度可以优化为 $O(1)$。 3). Best Time to Buy and Sell Stock III 两次遍历，第一次从左到右，找出一次购买的最大值；第二次从右到左，找出第二次购买的最大值。$ f[i] $表示前i 的最大利润，$ f[i] =max(f[i-1], prices[i] -minv[i-1])$。 123456789101112131415161718192021222324252627282930class Solution &#123;public: int maxProfit(vector&lt;int&gt;&amp; prices) &#123; int n =prices.size(); if(!n) return 0; vector&lt;int&gt; f(n, 0); int minv =INT_MAX; for(int i =0; i&lt;n; i++ ) &#123; if(i) f[i] =f[i-1]; if(prices[i] &gt;minv ) &#123; f[i] =max(f[i], prices[i] -minv); &#125; minv =min(minv, prices[i]); &#125; int maxv =INT_MIN; int res =f[n-1]; for(int i =n-1; i&gt;0; i--) &#123; if(prices[i] &lt; maxv) &#123; res =max(res, maxv-prices[i]+ f[i-1]); &#125; maxv =max(maxv, prices[i]); &#125; return res; &#125;&#125;; python实现。 1234567891011121314151617181920class Solution: def maxProfit(self, prices: List[int]) -&gt; int: # 两次遍历 ， f[i] 表示前 i天的最大的收益； 计算方法就是股票- 最小值 if not prices: return 0 n =len(prices) f =[0] *n minv = prices[0] for i in range(n): if(i): f[i] =f[i-1] if(prices[i] &gt; minv) : f[i] =max(f[i], prices[i] -minv) minv =min(minv, prices[i]) # 第二次 res = f[n-1] maxv =prices[n-1] for i in range(n-1 ,0, -1 ): if(prices[i] &lt; maxv): res =max(res, maxv- prices[i] + f[i-1]) maxv =max(maxv, prices[i]) return res 4). Best Time to Buy and Sell Stock IV 动态规划的思想 定义dp状态dp[i][j] 表示第j 天，第i笔交易产生的最大的收益 转移方程 dp[i][j] =max(dp[i][j-1], tmp +prices[j] ) 处理卖出的情况 tmp 表示已经获得的最大的利润, 处理的是买入的情况 如果买卖的次数$k \ge \frac{n}{2}$，那么就如同第二个题目。 12345678910111213141516171819class Solution(object): def maxProfit(self, k, prices): # tmp=max(tmp, dp[i-1][j-1]-prices[j] ) # 买入 # 初始化 dp[i][j] =&#123;0&#125;, tmp = -prices[i] if k &gt;= len(prices )&gt;&gt; 1: return self.maxProfit2(prices) dp =[ [0] *(len(prices)) for _ in range(k+1)] for i in range(1, k+1): tmp =-prices[0] for j in range(1, len(prices)): dp[i][j] =max(dp[i][j-1], tmp + prices[j]) tmp =max(tmp, dp[i-1][j -1] -prices[j]) return dp[k][-1] def maxProfit2(self, prices): res =0 for i in range(1, len(prices)): if(prices[i] &gt; prices[i-1]): res += prices[i] -prices[i-1] return res （4）钱币找零 （这两道题目应该归结到动态规划中） 题目大意如下，对于人民币的面值有1元 5元 10元 20元 50元 100元，下面要求设计一个程序，输入找零的钱，输出找钱方案中最少张数的方案，比如123元，最少是1张100的，1张20的，3张1元的，一共5张！ 1). Coin Change 完全背包问题，$f[j]$表示价值为$j$ 的所需的最少的数量，有如下的转移方程：$f[j] =min(f[j], f[j -coins[i]]+1)$。dp 关键是填表格，找到转移方程。 c++ 实现 1234567891011121314151617class Solution &#123;public: int coinChange(vector&lt;int&gt;&amp; coins, int amount) &#123; int INF=10000000; vector&lt;int&gt; f(amount+1, INF); f[0] =0; for(int i =0; i&lt; coins.size() ; i++) &#123; for(int j =coins[i]; j&lt;= amount; j++) &#123; f[j] =min(f[j], f[j -coins[i]]+1); &#125; &#125; if(f[amount] ==INF) f[amount] =-1; return f[amount]; &#125;&#125;; python 实现 12345678910111213141516class Solution(object): def coinChange(self, coins, amount): """ :type coins: List[int] :type amount: int :rtype: int """ # 使用dp的思想去做，关键是在于转移方程，填写二维或者三维表格的问题 # python中float('inf')表示无穷大 f=[float('inf')]*(amount +1) f[0] =0 for coin in coins: for i in range(coin, amount+1): f[i] =min(f[i], f[i -coin] +1) if(f[amount] ==float('inf')): f[amount] =-1 return f[amount] float(&#39;inf&#39;)表示浮点数的最大值， float(&#39;-inf&#39;)表示浮点数的最小值。sys.maxsize 表示int 的最大值， -sys.maxsize-1 表示int 的最小值。 2). Coin Change 2 上一道题目求解的是能否问题，这一道题求解的是组合，共有多少种凑齐的方式。上面求解的是数量最少的一种方案。这里求解的是总共的方案。 C++ 实现 123456789101112131415class Solution &#123;public: int change(int amount, vector&lt;int&gt;&amp; coins) &#123; vector&lt;int&gt; f(amount+1, 0); f[0] =1; for(int i =0; i&lt; coins.size(); i++) &#123; for(int j =coins[i] ; j&lt;=amount ; j++) &#123; f[j] += f[j-coins[i]]; &#125; &#125; return f[amount]; &#125;&#125;; python实现 12345678910111213class Solution(object): def change(self, amount, coins): """ :type amount: int :type coins: List[int] :rtype: int """ f =[0] *(amount +1) f[0] =1 for coin in coins: for i in range(coin, amount+1): f[i] = f[i] + f[i-coin] return f[amount] （5） Largest Number 设有N个正整数，现在需要你设计一个程序，使他们连接在一起成为最大的数字，例3个整数 12,456,342 很明显是45634212为最大，4个整数 342，45,7,98显然为98745342最大程序要求：输入整数N 接下来一行输入N个数字，最后一行输出最大的那个数字！ 12345678910111213141516171819class Solution &#123;public: // 使用nlogn的算法，先进行排序 static bool cmp(int a, int b) &#123; string sa =to_string(a), sb =to_string(b); return sa+sb &gt;sb +sa; // 这个保证是降序 &#125; string largestNumber(vector&lt;int&gt;&amp; nums) &#123; sort(nums.begin(), nums.end(), cmp); string res =""; for(auto num : nums) res += to_string(num); for(int i =0; i&lt; res.length(); i++) if(i ==res.length() -1 || res[i] !='0') return res.substr(i, res.length() -i); // 第二个参数是长度 return res; &#125;&#125;; c++语法，使用它 to_string() 和 substr(start, length) 两个很好用的字符串函数。 python 实现123456789class Solution(object): def largestNumber(self, nums): """ :type nums: List[int] :rtype: str """ nums =map(str, nums) nums.sort(cmp =lambda a, b : cmp(a+b, b+a), reverse= True) return ''.join(nums).lstrip('0') or '0' sort() 内置的cmp自定义函数是在python2 中使用的，在python3 中 from functools import cmp_to_key 进行自定义比较函数。 python中默认是升序 reverse =False, 但是这里需要 reverse =True 转换成降序cmp(x, y), return 1 if x &gt;y; return 0, if x ==y ; return -1 , if x&lt; y （6）最小生成树 1). Prim’s algorithm Prim’s algorithm is a greedy algorithm that finds a minimum spanning tree for a weighted undirected graph. This means it finds a subset of the edges that forms a tree that includes every vertex, where the total weight of all the edges in the tree is minimized. The algorithm operates by building this tree one vertex at a time, from an arbitrary starting vertex, at each step adding the cheapest possible connection from the tree to another vertex. prim 是一种贪心算法，无向有权图中寻找最短生成树的算法。这意味着选定了图中边的子集，这些边集包含了所有的顶点，并且边集的权重是最小。算法步骤如下 将边集合按照权重升序排列 选择最小权重边对应的点，判断是否构成了环，如果是，那么忽略该边否则加入到边的集合中 不断重复第二步，知道有 $V-1$条边 该网站提供了很好的动图显示上述过程。 适用于稠密图，时间复杂度 $O(n^2)$核心思想：每次挑一条与当前集合相连的最短边。 1234567891011121314151617181920212223242526272829int n; // n表示点数int g[N][N]; // 邻接矩阵，存储所有边int dist[N]; // 存储其他点到当前最小生成树的距离bool st[N]; // 存储每个点是否已经在生成树中// 如果图不连通，则返回INF(值是0x3f3f3f3f), 否则返回最小生成树的树边权重之和int prim()&#123; memset(dist, 0x3f, sizeof dist); int res = 0; for (int i = 0; i &lt; n; i ++ ) &#123; int t = -1; for (int j = 1; j &lt;= n; j ++ ) if (!st[j] &amp;&amp; (t == -1 || dist[t] &gt; dist[j])) t = j; if (i &amp;&amp; dist[t] == INF) return INF; if (i) res += dist[t]; st[t] = true; for (int j = 1; j &lt;= n; j ++ ) dist[j] = min(dist[j], g[t][j]); &#125; return res;&#125; 2). Kruskal’s Minimal Spanning Tree Algorithm Kruskal’s algorithm is a minimum-spanning-tree algorithm which finds an edge of the least possible weight that connects any two trees in the forest. It is a greedy algorithm in graph theory as it finds a minimum spanning tree for a connected weighted graph adding increasing cost arcs at each step. This means it finds a subset of the edges that forms a tree that includes every vertex, where the total weight of all the edges in the tree is minimized. If the graph is not connected, then it finds a minimum spanning forest (a minimum spanning tree for each connected component). Kruskal算法也是一种贪心算法。在无向有权连通图中，每次寻找权重最小的边，然后添加到集合中。 Prim’s algorithm 和Kruskal 的区别： 当边的权重都不相同时候，两个算法得到相同的结果 当边的权重存在相同，两个算法并不总是产生相同的结果 Kruskal 算法适合稀疏图，即满足$E =O(V)$或者边更少； 边权已经排好序或者能够在线性时间内排序完成 Prim 算法适合稠密图，即满足$E =O(V^2)$左右。 在算法步骤中的不同： Prim 在每一步骤中生成树总是相连的，Kruskal每一步中在通常情况下子树之间没有相连 Prim 从任意一个顶点开始，然后加上和其相连最短边权的点，Kruskal每次加上最小权重的边到现有的树（森林）中 Prim 算法在稠密图中更快，Kruskal在系数图中更快 适用于稀疏图，时间复杂度 $O(mlogm)$核心思想：从小到大挑不多余的边 123456789101112131415161718192021222324252627282930313233343536373839404142int n, m; // n是点数，m是边数int p[N]; // 并查集的父节点数组struct Edge // 存储边&#123; int a, b, w; bool operator&lt; (const Edge &amp;W)const &#123; return w &lt; W.w; &#125;&#125;edges[M];int find(int x) // 并查集核心操作&#123; if (p[x] != x) p[x] = find(p[x]); return p[x];&#125;int kruskal()&#123; sort(edges, edges + m); for (int i = 1; i &lt;= n; i ++ ) p[i] = i; // 初始化并查集 int res = 0, cnt = 0; for (int i = 0; i &lt; m; i ++ ) &#123; int a = edges[i].a, b = edges[i].b, w = edges[i].w; a = find(a), b = find(b); if (a != b) // 如果两个连通块不连通，则将这两个连通块合并 &#123; p[a] = b; res += w; cnt ++ ; &#125; &#125; if (cnt &lt; n - 1) return INF; return res;&#125; （7） 最短路径 1). Dijkstra’s algorithm for shortest paths from a single source 单源最短路径问题 Dijkstra’s algorithm (or Dijkstra’s Shortest Path First algorithm, SPF algorithm) is an algorithm for finding the shortest paths between nodes in a graph, which may represent, for example, road networks.Dijkstra’s迪杰斯特拉算法是计算两点之间的最短路径。（单源最短路径） For a given source node in the graph, the algorithm finds the shortest path between that node and every other. It can also be used for finding the shortest paths from a single node to a single destination node by stopping the algorithm once the shortest path to the destination node has been determined.该算法是单源路径算法，给定源点，到达目的节点或者其余个各点的最短路径。 The Dijkstra algorithm uses labels that are positive integers or real numbers, which are totally ordered.该算法要求结点的数值是非负。 时间复杂度 $O(mlogn)$, $n$表示点数，$m$表示边数 123456789101112131415161718192021222324252627282930313233343536373839typedef pair&lt;int, int&gt; PII;int n; // 点的数量int h[N], w[N], e[N], ne[N], idx; // 邻接表存储所有边int dist[N]; // 存储所有点到1号点的距离bool st[N]; // 存储每个点的最短距离是否已确定// 求1号点到n号点的最短距离，如果不存在，则返回-1int dijkstra()&#123; memset(dist, 0x3f, sizeof dist); dist[1] = 0; priority_queue&lt;PII, vector&lt;PII&gt;, greater&lt;PII&gt;&gt; heap; heap.push(&#123;0, 1&#125;); // first存储距离，second存储节点编号 while (heap.size()) &#123; auto t = heap.top(); heap.pop(); int ver = t.second, distance = t.first; if (st[ver]) continue; st[ver] = true; for (int i = h[ver]; i != -1; i = ne[i]) &#123; int j = e[i]; if (dist[j] &gt; distance + w[i]) &#123; dist[j] = distance + w[i]; heap.push(&#123;dist[j], j&#125;); &#125; &#125; &#125; if (dist[n] == 0x3f3f3f3f) return -1; return dist[n];&#125; 1). Cheapest Flights Within K Stops 堆优化的Dijkstra求最短路，时间复杂度是$O(mlog(nK))$，其中$m$表示边的个数，$n$表示点的个数， $K$表示中转站的个数。这是一个双关键字最短路劲问题，$dis[i][j]$ 表示第$i$个点到达第$j$个点的最短的距离。如果使用普通队列，需要的时间复杂度是$O(mlogm)$。空间复杂度 邻接表需要$O(m)$空间，优先队列需要$O(m)$空间。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152struct Node &#123; int no, stops; int distance; Node(int no_, int stops_, int distance_): no(no_), stops(stops_), distance(distance_) &#123;&#125; bool operator &gt; (const Node &amp;other) const &#123; return distance &gt; other.distance; &#125;&#125;;class Solution &#123;public: int findCheapestPrice(int n, vector&lt;vector&lt;int&gt;&gt;&amp; flights, int src, int dst, int K) &#123; const int INF = 1000000000; vector&lt;vector&lt;pair&lt;int, int&gt;&gt;&gt; graph(n); for (const auto &amp;v: flights) graph[v[0]].emplace_back(v[1], v[2]); K++; vector&lt;vector&lt;int&gt;&gt; dis(n, vector&lt;int&gt;(K + 1, INF)); vector&lt;vector&lt;bool&gt;&gt; vis(n, vector&lt;bool&gt;(K + 1, false)); priority_queue&lt;Node, vector&lt;Node&gt;, greater&lt;Node&gt;&gt; q; q.push(Node(src, 0, 0)); dis[src][0] = 0; while (!q.empty()) &#123; Node u = q.top(); q.pop(); if (vis[u.no][u.stops]) continue; vis[u.no][u.stops] = true; for (const auto &amp;v: graph[u.no]) if (u.stops + 1 &lt;= K &amp;&amp; dis[v.first][u.stops + 1] &gt; dis[u.no][u.stops] + v.second) &#123; dis[v.first][u.stops + 1] = dis[u.no][u.stops] + v.second; q.push(Node(v.first, u.stops + 1, dis[v.first][u.stops + 1])); &#125; &#125; int ans = INF; for (int i = 1; i &lt;= K; i++) ans = min(ans, dis[dst][i]); if (ans == INF) ans = -1; return ans; &#125;&#125;; python实现 1234567891011121314151617181920212223242526272829303132class Solution(object): def findCheapestPrice(self, n, flights, src, dst, K): """ :type n: int :type flights: List[List[int]] :type src: int :type dst: int :type K: int :rtype: int """ graph = &#123;&#125; for flight in flights: if flight[0] in graph: graph[flight[0]][flight[1]] = flight[2] else: graph[flight[0]] = &#123;flight[1]:flight[2]&#125; rec = &#123;&#125; heap = [(0, -1, src)] heapq.heapify(heap) while heap: cost, stops, city = heapq.heappop(heap) if city == dst: return cost if stops == K or rec.get((city, stops), float('inf')) &lt; cost: continue if city in graph: for nei, price in graph[city].items(): summ = cost + price if rec.get((nei, stops+1), float('inf')) &gt; summ: rec[(nei, stops+1)] = summ heapq.heappush(heap, (summ, stops+1, nei)) return -1 2). Reachable Nodes In Subdivided Graph 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263struct Node &#123; int x, d; Node(int x_, int d_): x(x_), d(d_)&#123;&#125; bool operator &lt; (const Node &amp;v) const &#123; return d &gt; v.d; &#125;&#125;;class Solution &#123;public: void dijkstra(const vector&lt;vector&lt;pair&lt;int, int&gt;&gt;&gt;&amp; e, int N, vector&lt;int&gt;&amp; dis) &#123; priority_queue&lt;Node&gt; heap; dis[0] = 0; heap.push(Node(0, 0)); while (!heap.empty()) &#123; Node sta(heap.top()); heap.pop(); if (sta.d &gt; dis[sta.x]) continue; for (auto edge : e[sta.x]) &#123; if (dis[edge.first] &gt; dis[sta.x] + edge.second) &#123; dis[edge.first] = dis[sta.x] + edge.second; heap.push(Node(edge.first, dis[edge.first])); &#125; &#125; &#125; &#125; int reachableNodes(vector&lt;vector&lt;int&gt;&gt;&amp; edges, int M, int N) &#123; int m = edges.size(); const int inf = 200000000; vector&lt;vector&lt;pair&lt;int, int&gt;&gt;&gt; e(m); for (auto edge : edges) &#123; e[edge[0]].emplace_back(make_pair(edge[1], edge[2] + 1)); e[edge[1]].emplace_back(make_pair(edge[0], edge[2] + 1)); &#125; vector&lt;int&gt; dis(N, inf); dijkstra(e, N, dis); int ans = 0; for (int i = 0; i &lt; N; i++) if (dis[i] &lt;= M) ans++; for (auto edge : edges) &#123; int x = edge[0], y = edge[1]; if (dis[x] &lt;= M &amp;&amp; dis[y] &lt;= M) ans += min(edge[2], 2 * M - dis[x] - dis[y]); else if (dis[x] &lt;= M &amp;&amp; dis[y] &gt; M) ans += min(edge[2], M - dis[x]); else if (dis[y] &lt;= M &amp;&amp; dis[x] &gt; M) ans += min(edge[2], M - dis[y]); &#125; return ans; &#125;&#125;; 2). Floyd算法 In computer science, the Floyd–Warshall algorithm (also known as Floyd’s algorithm, the Roy–Warshall algorithm, the Roy–Floyd algorithm, or the WFI algorithm) is an algorithm for finding shortest paths in a weighted graph with positive or negative edge weights (but with no negative cycles).Floyd 算法是任意两点之间的最短路径（多源）；权重可以是负数，但是不能有负数的环。 时间复杂度是$O(V^3)$，其中$V$表示点的个数。Floyd是个动态规划， 1234567891011121314初始化： for (int i = 1; i &lt;= n; i ++ ) for (int j = 1; j &lt;= n; j ++ ) if (i == j) d[i][j] = 0; else d[i][j] = INF;// 算法结束后，d[a][b]表示a到b的最短距离void floyd()&#123; for (int k = 1; k &lt;= n; k ++ ) for (int i = 1; i &lt;= n; i ++ ) for (int j = 1; j &lt;= n; j ++ ) d[i][j] = min(d[i][j], d[i][k] + d[k][j]);&#125; 参考文献 wiki Greedy_algorithm leetcode 题目]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>greedy algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Breadth First Search]]></title>
    <url>%2F2019%2F11%2F24%2Fbfs%2F</url>
    <content type="text"><![CDATA[宽度优先遍历（Breadth First Search，BFS）概念和实践介绍。概念部分包括定义，使用的场景和时空复杂度；实践就是代码部分，包括树的深度优先遍历、无向图的深度优先遍历（迪杰斯特拉算法，Dijkstra’s algorithm）和有向图的深度优先遍历（拓扑排序）。 理论部分(1). BFS 的定义 Breadth-first search (BFS) is an algorithm for traversing or searching tree or graph data structures. It starts at the tree root (or some arbitrary node of a graph, sometimes referred to as a search key), and explores all of the neighbor nodes at the present depth prior to moving on to the nodes at the next depth level.宽度优先是图或者树的一种遍历算法。宽度优先算法从树或者图的某一点出发，首先遍历当前节点周围邻居节点，然后进入下一层的遍历。 (2). 和DFS的对比 DFS使用栈的数据结构，适合写成递归的形式；BFS 使用队列的数据结构，适合写成迭代的形式; DFS 适合寻找离源点较远的解，BFS适合寻找离源点近的解； DFS是按照某个条件针对一个点一直深入，如果不满足条件之后才回溯；BFS是优先考虑周围的点，分层扩展。 BFS相比于DFS，其中一个特性就是BFS可以求出“最小值”，比如最短距离，最小步数等等。 例题树的层序遍历(1). Binary Tree Level Order Traversal 二叉树的层序遍历。 c++ 实现，关键是用一个空指针来表示一行的结束。 12345678910111213141516171819202122232425262728293031323334353637/** * Definition for a binary tree node. * struct TreeNode &#123; * int val; * TreeNode *left; * TreeNode *right; * TreeNode(int x) : val(x), left(NULL), right(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: vector&lt;vector&lt;int&gt;&gt; levelOrder(TreeNode* root) &#123; vector&lt;vector&lt;int&gt;&gt; res; vector&lt;int&gt; tmp; queue&lt;TreeNode *&gt; q; //没有 unorderd queue,队列是不需要排序的 q.push(root); q.push(nullptr); // 通过空指针进行判断一行是否结束 while(q.front()) &#123; auto t =q.front(); q.pop(); vector&lt;int&gt; level; // 这个是处理的一层的结点 while(t) &#123; level.push_back(t-&gt;val); // 这个是处理的下一层的结点 if(t-&gt;left) q.push(t-&gt;left); if(t-&gt;right) q.push(t-&gt;right); t =q.front(), q.pop(); &#125; q.push(nullptr); res.push_back(level); &#125; return res; &#125;&#125;; python实现，使用 from collections import deque 实现。 访问的话使用deque.popleft() 得到是队列的首元素，deque.append() 进行添加（和list是一样的操作的） 123456789101112131415161718192021222324# Definition for a binary tree node.# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Nonefrom collections import dequeclass Solution: # python 中queue的概念使用 deque（双向队列）实现 def levelOrder(self, root: TreeNode) -&gt; List[List[int]]: if not root: return [] queue, res =deque([root]), [] while queue: level, size =[], len(queue) # 因为python 中没有 front() 访问的属性，所以使用 size() 来代替这种属性 for i in range(size): node =queue.popleft() # python deque 中popleft() 是访问得到队列的首元素 level.append(node.val) if node.left: queue.append(node.left) if node.right: queue.append(node.right) res.append(level) return res (2). Binary Tree Zigzag Level Order Traversal 这道题是上一道题目的延伸，需要多设置一个参数 zigzag 表明这个是不是要reverse 123456789101112131415161718192021222324252627282930313233343536373839/** * Definition for a binary tree node. * struct TreeNode &#123; * int val; * TreeNode *left; * TreeNode *right; * TreeNode(int x) : val(x), left(NULL), right(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: vector&lt;vector&lt;int&gt;&gt; zigzagLevelOrder(TreeNode* root) &#123; //# 在层序遍历上进行扩展， zigzag traversal ， 需要处理奇偶问题 queue&lt;TreeNode *&gt; q; vector&lt;vector&lt;int&gt;&gt; res; if (!root) return res; bool zigzag =true; q.push(root); q.push(nullptr); while(q.front()) &#123; auto t =q.front(); q.pop(); vector&lt;int&gt; level; while(t) &#123; level.push_back(t-&gt;val); if(t-&gt;left) q.push(t-&gt;left); if(t-&gt;right) q.push(t-&gt;right); t =q.front(), q.pop(); &#125; q.push(nullptr); if(!zigzag) reverse(level.begin(), level.end()); zigzag =!zigzag; res.push_back(level); &#125; return res; &#125;&#125;; (3). N-ary Tree Level Order Traversal python 实现，使用 deque 队列数据结构；对于多叉树，只有 children属性 12345678910111213141516class Solution: # n-ary 和二叉树是一样的解法，层次遍历的结果 def levelOrder(self, root: 'Node') -&gt; List[List[int]]: from collections import deque if not root: return [] q =deque([root]) res =[] while q: level =[] for i in range(len(q)): #使用确切的遍历的次数，不用使用 空指针进行判断了 cur =q.popleft() for c in cur.children: # 是维护queue的代码 q.append(c) level.append(cur.val)# 维护res 的结果, 对于多叉树，只是需要判断 child 属性即可 res.append(level) return res c++ 实现，通过 size 来遍历当下的层，而不是通过 nullptr 指针来进行判断。 12345678910111213141516171819202122232425262728293031323334353637383940414243/*// Definition for a Node.class Node &#123;public: int val; vector&lt;Node*&gt; children; Node() &#123;&#125; Node(int _val) &#123; val = _val; &#125; Node(int _val, vector&lt;Node*&gt; _children) &#123; val = _val; children = _children; &#125;&#125;;*/class Solution &#123;public: vector&lt;vector&lt;int&gt;&gt; levelOrder(Node* root) &#123; vector&lt;vector&lt;int&gt;&gt; res; if(!root) return res; res=vector&lt;vector&lt;int&gt;&gt; res; queue&lt;Node*&gt; q; q.push(root); while(q.size()) &#123; vector&lt;int&gt; level =[]; int n =q.size(); while (n--) &#123; Node * t =q.front(); q.pop(); for(int i =0; i&lt;t-&gt;children.size(); i++ ) &#123; q.push(t-&gt;children[i]); &#125; level.push_back(t-&gt;val); &#125; res.push_back(level); &#125; return res; &#125;&#125;; 无向图BFS的队列可以分段来看，层层扩展：第一次先把所有距离是0的点都加进队列，第二步把所有距离是1的点加入队列，依次类推，所以可以保证求出的距离就是最小值。 (1). 01 Matrix 寻找矩阵中距离数字0最近的距离。思路：如果矩阵位置本身是0，那么距离就是0，如果是1，那么以0为当前点进行扩展。 C++ 实现123456789101112131415161718192021222324252627282930313233class Solution &#123;public: // bfs 是比较擅长解决最小值 ，最短距离这样的问题 // 使用队列存储，求解到0 的最短距离 vector&lt;vector&lt;int&gt;&gt; updateMatrix(vector&lt;vector&lt;int&gt;&gt;&amp; matrix) &#123; int dx[4] =&#123;-1, 0, 1, 0&#125;, dy[4] =&#123;0, 1, 0 , -1&#125;; int n =matrix.size(), m =matrix[0].size(); vector&lt;vector&lt;int&gt;&gt; res(n, vector&lt;int&gt;(m , -1)); queue&lt;pair&lt;int, int&gt;&gt; q; for(int i =0; i&lt;n; i++) for(int j =0; j&lt;m ;j++) if(matrix[i][j] ==0) &#123; res[i][j] =0; q.push(make_pair(i,j)); &#125; while(q.size()) &#123; pair&lt;int, int&gt; t =q.front(); q.pop(); for(int i =0; i&lt; 4; i++) &#123; int a =t.first+dx[i]; int b =t.second +dy[i]; if(a &lt;0 || a&gt;= n|| b&lt;0 || b&gt;=m || res[a][b]!=-1 ) continue; res[a][b] =res[t.first][t.second] +1; // 因为是bfs ，所以自然是一层层的遍历，所以每一层都是会加一 q.push(make_pair(a, b)); // 作为扩展层 &#125; &#125; return res; &#125;&#125;; 对于周围”点“的扩展，数组（图）是按照”上右下左“的规则进行扩展；树是按照”根节点到叶结点“的顺序进行扩展。两者没有什么本质的差别。扩展的同时顺便解决了问题，所以这类问题本质上是遍历。 PS: c++的 stl 中只有unordered_map, 和 unordered_set 两种表示无序的数据结构，和有序的map 和set想对应。类似 queue和 stack 这类想想都是无序的。 有向图(1). Course Schedule 考察的是拓扑排序，中间有用到 bfs 的思想。 拓扑排序虽然说是排序，但是更像是一种图的遍历。 拓扑排序通常用来处理”排序“具有依赖关系问题。 排序方法关键需要维护一个入度为0的点集合，最后的时间复杂度是 $O(n +m) $，其中$n$ 和$m$ 分别表示点数和边数。 图的存储图有邻接表和邻接矩阵两种存储方式。对于稀疏图和稠密图的定义没有明显的划分边界。稀疏图（sparse graph）的边数 $E$和点数$V$保持一致；稠密图（dense graph）一般接近于最多的边数。有向图中最多有 $E(E-1) $边，无向图中最多有$\frac{E(E-1)}{2}$条边。所以，如果是稠密图，使用邻接矩阵存储；如果是稀疏图，使用邻接表存储。 该问题中的图是稀疏图，使用邻接表进行存储。 拓扑排序是可以用来解决类似选课问题等有先后依赖关系的问题。该问题中需要维护一个邻接表和一个入度数组和一个队列。 12345678910111213141516171819202122232425262728293031class Solution &#123;public: bool canFinish(int num, vector&lt;vector&lt;int&gt;&gt;&amp; pre) &#123; vector&lt;vector&lt;int&gt;&gt; adj(num); vector&lt;int&gt; in_degree(num, 0); for(int i =0; i&lt; pre.size() ; i++) &#123; in_degree[pre[i][0]] ++; adj[pre[i][1]].push_back(pre[i][0]); &#125; for(auto u: in_degree) cout &lt;&lt; u&lt;&lt;" "; queue&lt;int&gt; q; int couts =0; for(int i =0; i&lt; num; i++) &#123; if(in_degree[i] ==0) q.push(i); &#125; while(!q.empty()) &#123; int t =q.front(); q.pop(); couts +=1; for(int i =0; i&lt; adj[t].size(); i++) &#123; in_degree[adj[t][i]] --; if(in_degree[adj[t][i]] ==0) q.push(adj[t][i]); &#125; &#125; return couts ==num; &#125;&#125;; python 实现 123456789101112131415161718192021class Solution: def canFinish(self, n: int, pre: List[List[int]]) -&gt; bool: #adj =collections.defaultdict(set) #indegrees =collections.defaultdict(set) # 手动进行初始化，因为下文是根据 set() 判断的 indegrees =&#123;i : set() for i in range(n)&#125; adj =&#123;i : set() for i in range(n)&#125; for i,j in pre: adj[j].add(i) indegrees[i].add(j) q =collections.deque([i for i in indegrees if not indegrees[i]]) counts =0 while q: t =q.popleft() counts +=1 for i in adj[t]: indegrees[i].remove(t) if not indegrees[i]: q.append(i) return n ==counts (2) Course Schedule II 相对于上一个题目 Course Schedule，本题要求记录课程安排记录。所以还是 拓扑排序+ BFS的思想，只不过是把拓扑排序的结果存储起来了。 123456789101112131415161718192021222324252627282930313233class Solution &#123;public: vector&lt;int&gt; findOrder(int n, vector&lt;vector&lt;int&gt;&gt;&amp; pre) &#123; // 类似图的结果，首先使用邻接矩阵存储 adj，然后保存一个入度为0的输出，使用queue 表示拓扑排序的过程 vector&lt;vector&lt;int&gt;&gt; adj(n); vector&lt;int&gt; in_degrees(n, 0); queue&lt;int&gt; q; for(int i =0; i&lt;pre.size(); i++) &#123; adj[pre[i][1]].emplace_back(pre[i][0]); in_degrees[pre[i][0]] ++; &#125; // 遍历入度为0 的点，放到 queue 中 for(int i =0; i&lt;n; i++) if(in_degrees[i] ==0) q.push(i); int counts =0; vector&lt;int&gt; res; while(q.size()) &#123; auto t =q.front(); q.pop(); counts +=1; res.emplace_back(t); for(int i =0; i&lt; adj[t].size(); i++) &#123; in_degrees[adj[t][i]] -=1; if(in_degrees[adj[t][i]] ==0) q.push(adj[t][i]); &#125; &#125; if(counts == n) return res; else return vector&lt;int&gt;&#123;&#125;; &#125;&#125;; python解法 12345678910111213141516171819class Solution: # 需要使用到的数据结构如队列，dictionary 存储adj 邻接表和 in_degree 记录信息 def findOrder(self, n: int, pre: List[List[int]]) -&gt; List[int]: adj =collections.defaultdict(set) in_degrees =&#123;i : set() for i in range(n)&#125; for i, j in pre: adj[j].add(i) in_degrees[i].add(j) count, res =0, [] q =collections.deque([i for i in in_degrees if not in_degrees[i]]) # 处理的是入度为0 的情况 while q: node =q.popleft() res.append(node) count +=1 for i in adj[node]: in_degrees[i].remove(node) if not in_degrees[i]: q.append(i) return res if count ==n else [] (3). Number of Islands 同上面的题目类似，但是这里适合使用dfs 进行求解。 12345678910111213141516171819202122232425262728class Solution &#123;public: int dx[4] =&#123;-1, 0, 1, 0&#125;, dy[4] =&#123;0, 1, 0, -1&#125;; int n, m; int numIslands(vector&lt;vector&lt;char&gt;&gt;&amp; grid) &#123; if(grid.empty()) return 0; n =grid.size(), m =grid[0].size(); int nums =0; for(int i =0; i&lt; n; i++) for(int j =0; j&lt; m ; j++) if(grid[i][j] =='1') &#123; nums ++; dfs(grid, i, j); // 使用dfs 进行标记的作用 &#125; return nums; &#125; void dfs(vector&lt;vector&lt;char&gt;&gt; &amp; grid, int x, int y) &#123; grid[x][y] ='0'; for(int i =0; i&lt; 4; i++) &#123; int a =x +dx[i], b =y +dy[i]; if(a &gt;= 0 &amp;&amp; a&lt;n &amp;&amp; b&gt;=0 &amp;&amp; b&lt;m &amp;&amp; grid[a][b] =='1') dfs(grid, a, b); &#125; &#125;&#125;; python 版本， 注意如果是在一个类中，那么只能是在 __init__ 全局变量的申明。如果写到函数外面，是不认的。如果真个函数是 __main__的形式，那么在函数外面声明的变量是全局变量。 12345678910111213141516171819class Solution: def __init__(self): self.dx =[-1,0, 1, 0] self.dy =[0, 1, 0, -1] def numIslands(self, grid: List[List[str]]) -&gt; int: nums =0 for i in range(len(grid)): for j in range(len(grid[0])): if(grid[i][j] =='1'): nums +=1 self.dfs(grid, i, j) return nums def dfs(self, grid, x, y): grid[x][y] ='0' for i in range(4): a, b =x +self.dx[i], y +self.dy[i] if a &gt;=0 and a&lt; len(grid) and b &gt;= 0 and b&lt; len(grid[0]) and grid[a][b] =='1': self.dfs(grid, a, b) (4). Dijkstra’s 算法 定义 Dijkstra’s algorithm (or Dijkstra’s Shortest Path First algorithm, SPF algorithm) is an algorithm for finding the shortest paths between nodes in a graph, which may represent, for example, road networks.迪杰斯特拉算法是在图中寻找节点之间的最短路径的算法。 使用场景 For a given source node in the graph, the algorithm finds the shortest path between that node and every other. It can also be used for finding the shortest paths from a single node to a single destination node by stopping the algorithm once the shortest path to the destination node has been determined. For example, if the nodes of the graph represent cities and edge path costs represent driving distances between pairs of cities connected by a direct road (for simplicity, ignore red lights, stop signs, toll roads and other obstructions), Dijkstra’s algorithm can be used to find the shortest route between one city and all other cities.迪杰斯特拉算法可以寻找一点到其他所有点的最短路径。如果想要点到点的最短路径，那么找到目标点的时候，就可以停止。比如说：城市之间的最短路径问题；OSPF中使用该算法计算路由之间的最短的距离。 使用的条件 The Dijkstra algorithm uses labels that are positive integers or real numbers, which are totally ordered.一般迪杰斯特拉算法要求边的权重是非负数。 例题 给定一个n个点m条边的有向图，图中可能存在重边和自环，所有边权均为正值。请你求出1号点到n号点的最短距离，如果无法从1号点走到n号点，则输出-1。 1). Dijkstra求最短路 I 数据范围：$$\begin{split}1 \leq &amp; n \leq 500 \\1 \leq &amp; m \leq 10^5\end{split}$$其中 $n$ 表示点的个数， $m$ 表示边的个数。则表明该图为稠密图，使用邻接矩阵存储。朴素迪杰斯特拉算法时间复杂度是$O(n^2)$。寻找路径最短的点是$O(n^2)$，加入集合是$O(n)$， 更新距离是$O(m)$ ，所以总的时间复杂度是$O(n^2)$ 输入格式：第一行包含整数n和m。接下来m行每行包含三个整数x，y，z，表示点x和点y之间存在一条有向边，边长为z。 输出格式输出一个整数，表示1号点到n号点的最短距离。如果路径不存在，则输出-1。 输入判断时候的优化。 123int a, b, c;scanf("%d%d%d", &amp;a, &amp;b, &amp;c);g[a][b] =min(g[a][b], c); //从优化的角度，如果有重边，那么保留最短的一条。 2). Dijkstra求最短路 II 数据范围：$$\begin{split}1 \leq n, m \leq 10^5\end{split}$$其中 $n$ 表示点的个数， $m$ 表示边的个数。则表明该图是稀疏图，所以使用邻接表进行存储。对朴素迪杰斯特拉算法中寻找距离最短的点小根堆优化，从原来的 $O(n^2)$ 优化成$O(mlogn)$。其中堆取最小值是 $O(1)$，调整堆是$O(logn)$, 最多有 $m$条边，所以堆优化之后的时间复杂度是 $mlogn$。 c++ 代码实现 c++ 中默认是大根堆，可以在构造方法中修改成小根堆。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061#include&lt;iostream&gt;#include&lt;vector&gt;#include&lt;queue&gt;#include&lt;cstring&gt;#include &lt;climits&gt;using namespace std;const int N =1e5+11;int m,n;struct Node&#123; int vec; int w;&#125;;vector&lt;Node&gt; g[N];bool vis[N];int dis[N];typedef pair&lt;int, int&gt; P;// dijkstra 算法需要维护点的集合，寻找当前点到其余点的最短的路径，然后把其加入到该集合中void dijkstra()&#123; memset(vis, 0, sizeof vis); for(int i =1; i&lt;=m; i++) dis[i] =INT_MAX; dis[1] =0; // 当前的结点从1 开始计数 priority_queue&lt;P, vector&lt;P&gt;, greater&lt;P&gt;&gt; min_heap; // c++ 中默认是大根堆，本题需要小根堆 min_heap.push(&#123;0, 1&#125;); //&#123;distance, vec&#125; while(min_heap.size()) &#123; auto t =min_heap.top(); min_heap.pop(); int vec =t.second; int distance =t.first; if(vis[vec] ==1) continue; vis[vec] =1; for(int i =0; i&lt; g[vec].size(); i++) &#123; int next_node =g[vec][i].vec; if(distance +g[vec][i].w &lt; dis[next_node]) &#123; dis[next_node] =distance +g[vec][i].w; min_heap.push(&#123;dis[next_node], next_node&#125;); &#125; &#125; &#125;&#125;int main()&#123; cin &gt;&gt; m &gt;&gt;n; // m 个点，n 个边 Node node; int a, b, c; for(int i =0; i&lt;n; i++) &#123; cin &gt;&gt; a&gt;&gt;b&gt;&gt;c; node.vec =b; node.w =c; g[a].emplace_back(node); &#125; dijkstra(); if(dis[m] ==INT_MAX) cout &lt;&lt; -1&lt;&lt; endl; else cout &lt;&lt; dis[m] &lt;&lt; endl; return 0;&#125; 空间上：能使用 emplace_back() 就不要使用 push_back()，后者在往 vector 中放元素的过程中，涉及到拷贝操作，是对于内存的浪费。同时拷贝操作的同时，浪费了时间。 python 代码实现 123456789101112131415161718192021222324import heapqclass Solution: # 使用小根堆优化，当寻找点的时候，小根堆log(V) 的时间，总共是 O(ElogV) 时间 # 如果没有优化，那么时间复杂度是 O(V^2) def networkDelayTime(self, times: List[List[int]], N: int, K: int) -&gt; int: graph = collections.defaultdict(dict) # 默认初始化的 dic of ，可以直接 += 操作 for source, des, cost in times: graph[source][des] =cost distances =&#123;i : float('inf') for i in range(1, N+1)&#125; # 使用字典存储距离 distances[K] =0 # 从该点出发 min_heap =[(0, K)] visited =set() # python 中set 本来就是无序的，和c++ 中的set 和unordered_set 是不一样的, 这个是内置的函数，不需要import while min_heap: dis, vec = heapq.heappop(min_heap) # python 中默认是小根堆， 需要import heapq if vec in visited: continue visited.add(vec) for neighbor in graph[vec]: if neighbor in visited: continue new_dis = dis+ graph[vec][neighbor] if new_dis &lt; distances[neighbor]: distances[neighbor] =new_dis heapq.heappush(min_heap, (new_dis, neighbor)) if len(visited) != len(distances) : return -1 return max(distances.values()) 时间复杂度分析 如果是树模型，那么时间复杂度是 $O(T)$, T 表示树结点的个数 图模型：使用堆优化之后是 $O(Elog V)$，其中 $E$表示边的个数， $V$ 表示点的个数 数组类型： 时间复杂度是 $O(mn)$，其中 $m$, $n$ 表示数组的长度和宽度 参考资料 Dijkstra’s algorithm Dijkstra求最短路 leetcode题目]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>bfs</tag>
        <tag>dijkstra</tag>
        <tag>topo_sort</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Decision Tree]]></title>
    <url>%2F2019%2F11%2F23%2Fdecision-tree%2F</url>
    <content type="text"><![CDATA[介绍决策树的定义、分类，重点介绍决策树中特征选择的三种算法和决策树的优劣。 Decision Trees (DTs) are a non-parametric supervised learning method used for classification and regression. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features.决策树是有监督无参数的机器学习方法，可以用于分类和回归问题的求解。 从数学上讲，决策树是一个分段函数。在机器学习中，是一种基本的分类和回归方法。决策树的学习通常包括三个步骤： 特征选择、决策树的生成和决策树的修建。主要有三种算法： ID3、C4.5生成算法和CART 算法。 A learned binary tree is actually a partitioning of the input space. You can think of each input variable as a dimension on a p-dimensional space. The decision tree split this up into rectangles (when p=2 input variables) or some kind of hyper-rectangles with more inputs. 当决策树训练完之后，就相当于把 $p$ 维空间分成了若干个子空间（数量是和叶子节点数量相同），然后predict 的过程就是把某个样本放到某个空间的过程。 决策树分类分类树： 穷举每一个特征属性的每个阈值，然后按照 feature &lt; 阈值 和 feature &gt; 阈值选择熵和阈值。根据上述标准得到新的节点，用同样的方式继续分支直到将所有数据点都分入性别唯一的叶子节点，或达到了预设的终止条件，若最终叶子节点中的性别不唯一，那么以多数人的性别作为该叶子节点的性别。 回归树： 在每个节点（不一定是叶子节点）都会得到一个预测值，以年龄为例，该预测值是这个节点所有人年龄的平均值。分支穷举每一个feature 的每个阈值找到最好的分割点，但是这个标准不是最大熵，而是最小化均方差。最终的结束条件是达到预设的终止条件或者每个叶子节点上人的年龄都一样（几乎不可能），如果不统一，则以该节点上所有人的平均年龄作为该叶子节点的预测年龄。 特征选择算法决策树的建立包含特征选择、决策树构建和剪枝三个过程。特征选择有很多算法，本文中介绍最常见的三种算法。 Iterative Dichotimiser 3 (ID3) C4.5 CART - independently invented around the same time as C4.5, also still very popular ID3 算法（1）算法流程 计算数据集 $D$ 的熵 $H(D)$ $$IG(S, a) = H(S) – H(S | a)$$其中 $IG(S, a) $ 是数据集 $S$ 对于变量 $a$ 的信息增益， $H(S)$ 表示数据集整个熵， $H(S| a) $表示条件熵。 （2）特点 该算法是基于贪心实现，每次选取特征都是当前的最优解，并一定是全局的最优解。 每次迭代时候，依据特征属性划分，然后计算信息增益。如果特征的取值较多（类别多），那么最后的信息增益大，容易被当做分裂点。但是基于取值多少进行划分的特征选择并不能保证分类效果好。当前特征使用过之后，接下来的树的分裂不能再次使用该特征。 不能处理缺省值。 C4.5 算法基于 ID3进行改进，ID3 一般会优先选择属性值比较多的特征（信息增益反应的是给定一个条件之后不确定性减少的程度，必然分得越细的数据集确定性越高，也就是条件熵越小，信息增益越大）。为了避免这个问题， C4.5 算法使用信息比率作为特征选择的方式。除此之外，C4.5 还弥补了ID3 中不能处理特征连续值的问题。 首先还是计算信息增益$$IG(D, A) = H(D) – H(D | A)$$然后计算信息增益率$$GainRation(D, A) = \frac{IG(D, A)}{ IV(A)}$$其中 $IV(A)$ 表示如下：$$IV (A) = - \sum_{v=1}^{V} \frac{ |D^v|}{ |D|} \log _2 \frac{ |D^v|}{ |D|}$$其中 $A $被称为分支标准的固有值，作为分支标准的属性可取值越多，那么 $IV$ 值越大。信息增益率对于特征取值比较少的属性有偏向，所以在 C4.5 决策树中先从候选划分属性中找出信息增益高于均值的，然后在从其中选择信息增益率最高的。 （1）处理连续值特征 C4.5 算法中处理连续值是先将连续属性离散化，然后再处理。对于 $N$个样本，有 $N-1$中离散化方法，这种计算量是非常大的。通过以下的方式可以减少计算量：可以先按照连续属性排序，只有在分类发生改变的地方才需要将其切开。 （2）C4.5 的提升点 Handling both continuous and discrete attributes - In order to handle continuous attributes, C4.5 creates a threshold and then splits the list into those whose attribute value is above the threshold and those that are less than or equal to it. Handling training data with missing attribute values - C4.5 allows attribute values to be marked as ? for missing. Missing attribute values are simply not used in gain and entropy calculations. Handling attributes with differing costs. Pruning trees after creation - C4.5 goes back through the tree once it’s been created and attempts to remove branches that do not help by replacing them with leaf nodes.相对于ID3的提升点： 能够处理连续变量和离散变量 能够处理缺省值（处理方式比较粗暴，直接在熵的计算过程中忽略） 树建立之后剪枝，如果分支没有对最后的结果提升，那么就剪枝 CART 算法 CART algorithm can be used for building both Classification and Regression Decision Trees. The impurity (or purity) measure used in building decision tree in CART is Gini Index. The decision tree built by CART algorithm is always a binary decision tree (each node will have only two child nodes).既可以使用在分类场景也可以使用在回归场景，基于二叉树构建。对于分类问题使用 Gini 系数作为损失函数；对于回归问题使用均方误差作为损失函数。 （1）定义 数据集合 𝐷 的纯度可用基尼指数来度量：$$Gini ( D ) = \sum _ { k = 1 } ^ { | \mathcal { Y } | } \sum _ { k ^ { \prime } \neq k } p _ { k } p _ { k ^ { \prime } } = 1 - \sum _ { k = 1 } ^ { \mathcal { V } } p _ { k } ^ { 2 }$$直观来看，$Gini(𝐷) $ 反映了从数据集 𝐷 中随机抽取两个样本，其类别标记不一致的概率。因此，$Gini(𝐷) $ 越小，则数据集 𝐷 的纯度越高。 对特定属性 𝑎 的基尼指数定义如下：$$Gini _{index}( D , a ) = \sum _ { v = 1 } ^ { V } \frac { | D _ { v } | } { | D | } \operatorname { Gini } ( D _ { v } )$$我们在候选属性集合 𝐴 中，选择那个使得划分后基尼指数最小的属性作为最优划分属性，即： $$a _ { * } = \arg \min _ { a \in A } Gini _{index } ( D , a )$$ 采用基尼指数作为划分属性的判据的决策树是一种 CART 决策树。 （2）和C4.5 的比较 Gini Index, unlike information gain, isn’t computationally intensive as it doesn’t involve the logarithm function used to calculate entropy in information gain, which is why Gini Index is preferred over Information gain.Gini index 的计算量相对于信息增益是要小，因为后者有log 运算。 关于例子介绍更加详细的内容，参看Decision Tree Introduction with example 剪枝剪枝分为预剪枝和后剪枝。 The most common stopping procedure is to use a minimum count on the number of training instances assigned to each leaf node. If the count is less than some minimum then the split is not accepted and the node is taken as a final leaf node.在决策树构造的过程中，许多分支反应的是训练数据集中的异常，预剪枝通过减去这样的分支，处理过分适应数据问题。常见的预剪枝策略如下： 限制树的高度 叶子节点中的最少样本数，如果叶子节点中包含适当大的样本数，那么可以防止过拟合 The fastest and simplest pruning method is to work through each leaf node in the tree and evaluate the effect of removing it using a hold-out test set. Leaf nodes are removed only if it results in a drop in the overall cost function on the entire test set. You stop removing nodes when no further improvements can be made.后剪枝，当训练完成之后，自底向上对叶子节点进行检查，如果去掉不影响在 测试集上的loss，那么就去掉。方法也有很多，比如 代价复杂剪枝 最小误差剪枝 优劣优点： 决策树建立的决策规则很容易被理解，可以比较容易的转换成规则 应用范围广，可以用于回归和分类问题 不需要进行数据的预处理；可以处理数值类型和连续类型的特征；适用于较大的数据集，因为树的规模和数据的规模是独立的 为什么不需要进行预处理？ 数值缩放不影响分裂点位置，对树模型的结构不造成影响。 按照特征值进行排序的，排序的顺序不变，那么所属的分支以及分裂点就不会有不同。 树模型是不能进行梯度下降的，因为构建树模型（回归树）寻找最优点时是通过寻找最优分裂点完成的，因此树模型是阶跃的，阶跃点是不可导的，并且求导没意义，也就不需要归一化。 缺点： 很容易在训练过程中形成复杂的树结构，造成过拟合（overfitting）。通过剪枝是可以缓解过拟合的副作用 处理缺省值比较困难（只是把缺省值ignore）]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>decision_tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Optimizer]]></title>
    <url>%2F2019%2F11%2F17%2Foptimizer%2F</url>
    <content type="text"><![CDATA[深度学习中优化器 （optimizer）讲解。根据学习率的情况分成三个阶段，基础版本Gradient Descent，然后是人工设置学习率阶段和自适应学习率阶段。最后对应pytorch，说明常用的几种学习率优化策略。 vanilla(1 ) Gradient Descent 针对整个数据集 $$\theta = \theta - \eta \cdot \nabla _ { \theta } J ( \theta )$$特点： 使用整个数据集计算梯度，计算起来非常慢 能够找到全局最优点 (2 ) Stochastic Gradient Descent (SGD) SGD 又走入了另外一个极端，SGD 拿到一个数据之后，马上计算梯度，然后对参数进行更新. $$\theta = \theta - \eta \cdot J \left( \theta ; x ^ { ( i ) } , y ^ { ( i ) } \right)$$ 特点： 无法使用矩阵加速运算 收敛速度快 (3) Mini-Batch Gradient Descent （MBGD） Mini-batch 的方法是在上述两个方法中取了个折衷，每次从全部的熟练数据中取一个 mini-batch 的数据计算。 $$\theta = \theta - \eta \cdot J \left( \theta ; x ^ { ( i : i + n ) } , y ^ { ( i : i + n ) } \right)$$ batch size 的选择 n： 一般取值在 50～256 目前，mini-batch 的方法是深度学习中主流方法，各种深度学习工具默认也是这种方法。也可以把上述两种方法看成是 mini-batch 的特例，Batch 的方法，就是 mini-batch size 是整个数据集，SGD 方法就是 min-batch=1 的情况. 目前遇到的问题 learning rate 如何进行自动调整 如何跳出马鞍点 在pytorch中的实现： 1class torch.optim.SGD(params, lr=&lt;required parameter&gt;, momentum=0, dampening=0, weight_decay=0, nesterov=False) 功能： pytorch中实现的是带有动量优化的SGD，其中常见的参数： params (iterable) 待优化的参数或者定义了参数组的dict lr (float) 学习率 momentum (float 可选) -动量因子，默认是0 weight_decay (float, 可选) -权重衰减（L2 惩罚），默认是0 注意：pytorch中的使用SGD 和其他的框架不同，pytorch 中是这样的： $$\begin{split}v &amp;= \rho * v+g \\ p &amp;= p- lr * v \\ &amp; = p- lr * \rho * v- lr * g\end{split}$$而其他的框架是 $$\begin{split}v &amp;=\rho * v+ lr * g \\ p &amp;=p- v=p- \rho * v- lr * g\end{split}$$ 其中 $\rho$ 是动量， $v$ 是速度， $g$ 是梯度， pytorch中将 $ \rho * v $这一项也是乘以了一个学习率。 人工设置学习率 Momentum (heavy-ball method) $$x_{t+1}=x_{t}-\alpha \nabla f\left(x_{t}\right)+\mu\left(x_{t}-x_{t-1}\right), \quad \mu \in[0,1], \alpha&gt;0$$ 其中 $\mu$ 是动量因子，取值 0.9 左右。 优点： 可以加速 SGD， 并且抑制震荡 缺点： Lessard et al. 发现对于简单的凸函数 $f(x)$ ，momentum 不能收敛，课件里给出了一个bad case Nesterov Accelerated Gradient 对于上面 momentum出现的bad case， Nesterov 来背锅。 $$x_{t+1}=x_{t}+\mu\left(x_{t}-x_{t-1}\right)-\gamma \nabla f\left(x_{t}+\mu\left(x_{t}-x_{t-1}\right)\right)$$其中学习率$\gamma$, momentum系数$\mu$ 和参数的初始化$x_0$（公式中没有显示出来） 两者的区别，一切尽在图中。（相同的颜色表示相同含义） Nesterov Accelerated Gradient的关键在于计算梯度之前加上了之前积累的梯度。Notice how the gradient step with Polyak’s momentum is always perpendicular to the level set. taken from here 两种的相同点：都是基于momentum的 optim，都是针对凸优化函数；不同点：momentum 对于简单强凸优化函数，比如导数是二次方的函数，可能出现bad case；相对于而言，Nesterov Accelerated Gradient在凸优化函数中适用性更广。 自适应学习率方法 Adagrad （Adaptive gradient algorithm） intuition：自动调节学习率。 $$\begin{split}n_{t} &amp;=n_{t-1}+g_{t}^{2} \\\Delta \theta_{t} &amp;=-\frac{\eta}{\sqrt{n_{t}+\epsilon}} * g_{t}\end{split}$$ 其中 $n_t$表示从1到$t$ 形成的一个递推项，作为一个约束项， $\epsilon$ 用来保证分母非0，一般取 $1e-8$.实验表明，如果没有进行平方根操作，那么效果很差。 特点： 不用人工去tune 学习率，但是仍然需要在开始时候，给定一个学习率（依赖全局初始化学习率） 每次增加的嗾使一个正数（平方和），所以训练的中后期，学习率有可能变得非常小（缺点） 实际的使用效果：这个学习率的变化会受到梯度的大小和迭代次数的影响。梯度越大，学习率越小；梯度越小，学习率越大。对于稀疏数据集，效果很好。比如说在RNN 中训练词向量的过程，经常使用到。 Adadelta intuition： 解决使用Adagrad训练中后期，梯度可能为0 的问题 不依赖于初始化的learning rate 首先处理第一个问题：梯度为0. 不使用之前全部的梯度平方和，使用部分。这个时候一种是暴力的滑动窗口的相反，但是作者使用另一种方法： decaying。$$E[g ^2] _t=\gamma E[ g ^2] _{t-1}+(1-\gamma) g _t ^2$$其中 $\gamma$ 是类似momentum中的超参数，大概在 0.9左右。 第二个问题是初始化学习率问题。定义了另一个decay 系数，只不过这个是应用在参数 $\theta$ 上的，而不是上面的 $g_t$上。 $$E[\Delta \theta ^2] _t=\gamma E[\Delta \theta ^2 ] _{t-1}+(1-\gamma) \Delta \theta _t ^2$$ 然后是常用的平方根操作：$$R M S[\Delta \theta] _t=\sqrt{E [\Delta \theta ^2 ] _t+\epsilon}$$ 所以将上述两个方面的更新整合起来：$$\begin{split}\Delta \theta _t &amp;=-\frac{R M S[\Delta \theta] _{t-1}}{R M S[g] _{t}} g _t \\\theta _{t+1} &amp;=\theta _{t}+\Delta \theta _t\end{split}$$ 特点：不用设置全局的学习率，学习率是随着迭代次数变化的 RMSprop RMSprop是 Geoff Hinton 在其课程中讲到的一种方法。intuition 也是为了解决 Adagrad 中的出现的梯度为0. 实际上 RMSprop 就是 Adadelta 中的第一个方面的公式。 $$\begin{split}E[g ^2] _t &amp;=\gamma E[ g ^2] _{t-1}+(1-\gamma) g _t ^2 \\\theta _{t+1} &amp;= \theta _t-\frac{\eta}{\sqrt{E\left[g^{2}\right] _{t}+\epsilon}} g _{t}\end{split}$$其中 Hinton推荐超参数 $\gamma =0.9$， $\epsilon =0.001$。（ps RMSprop 和 Adadelta解决的方案叫做： exponentially decaying average of past squared gradients） 特点：解决了 Adagrad 中出现的梯度为0的情况 1class torch.optim.RMSprop(params, lr=0.01, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False) 参数： params (iterable) 待优化参数的iterable 或者是低你够了参数组的dict lr (float, 可选) 学习率 (默认是 1e-2) momentum （float, 可选）动量因子（默认是 0） alpha (float, 可选) 平滑因子（默认是 0.99） eps (float，可选) 分母中的数值，可以增加数值的稳定性（默认是 1e-8） weight_decay (float, 可选) 权重衰减(L2 惩罚) （默认是 0） Adam：Adaptive Moment Estimation intuition：如果是momentum机制类似一个 ball running down a slope, 那么 Adam behaves a heavy ball with friction。 这种性质是有利于处理非凸优化问题，可能中间出现了一些平滑的局部最优解。如同Adadelta 和 RMSprop，使用了梯度的二阶导数，如同momentum，使用了梯度的一阶导数，把这两者信息结合起来，就是momentum。 In addition to storing an exponentially decaying average of past squared gradients $v_t$, like Adadelta and RMSprop, Adam also keeps an exponentially decaying average of past gradients $m_t$, similar to momentum. $$\begin{split}m _{t} &amp;=\beta _{1} m _{t-1}+(1-\beta _{1}) g _{t} \\v _{t} &amp;=\beta _{2} v _{t-1}+(1-\beta _{2}) g _{t} ^{2}\end{split}$$ 其中 $m_t$和 $v_t$ 分别是对梯度的一阶矩估计 （first moment ，the mean） 和二阶矩估计（the second moment ，the uncentered variance），这个是 Adam名字的由来。实验中发现上述公式在初始化阶段，$m_t$ 和$v_t$都是趋向于0，尤其是当 $\beta_1$ 和$\beta_2$ 解决1的时候。 于是进行了 bias-corrected$$\begin{split}\hat{m} _{t} &amp;=\frac{m _{t}}{1-\beta _{1} ^{t}} \\\hat{v} _{t} &amp;=\frac{v _{t}}{1-\beta _{2} ^{t}}\end{split}$$最后进行了梯度的更新：$$\theta _{t+1}=\theta _{t}-\frac{\eta}{\sqrt{\hat{v} _{t}}+\epsilon} \hat{m} _{t}$$作者推荐的超参数 $\beta_1 =0.9$， $\beta_2 =0.999$, $\epsilon =10 ^{-8}$ 特点：从实践的角度，Adam是优于上述几种的。所以就不用选择了。 1class torch.optim.Adam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False) 参数 params (iterable) 待优化参数的iterable 或者是定义了参数组的dict lr (float, 可选) 学习率（默认是 13-3） betas （Tuple [float, float], 可选） 用于计算梯度以及梯度平方的运行平均值的系数 eps (float, 可选) 增加数值稳定性而加到分母中的项 weight_decay （float， 可选） 权重衰减（L2 惩罚） 默认是 0 图例如果空间中存在鞍点： 如果空间中存在若干和局部最优点： 如果… pytorch中的 optim 包（1）pytorch 中optim 优化器 参数组（param groups） optimizer 是通过 param_group 来管理参数组， param_group 中保存了参数组和相应的学习率，动量等。可以针对不同的参数设置不同的学习率。 1234567891011# 有两个`param_group`即,len(optim.param_groups)==2optim.SGD([ &#123;'params': model.base.parameters()&#125;, &#123;'params': model.classifier.parameters(), 'lr': 1e-3&#125; ], lr=1e-2, momentum=0.9)#一个参数组optim.SGD(model.parameters(), lr=1e-2, momentum=.9)# 获得学习率print('learning rate: &#123;&#125;'.format(optimizer.param_groups[0]['lr']))print('weight decay: &#123;&#125;'.format(optimizer.param_groups[0]['weight_decay'])) （2） pytorch中学习率调整策略 PyTorch提供的学习率调整策略分为三大类，分别是 有序调整：等间隔调整(Step)，按需调整学习率(MultiStep)，指数衰减调整(Exponential)和 余弦退火CosineAnnealing。 自适应调整：自适应调整学习率 ReduceLROnPlateau。 自定义调整：自定义调整学习率 LambdaLR。 有序调整 1). 等间隔调整学习率 StepLR1torch.optim.lr_scheduler.StepLR(optimizer, step_size, gamma=0.1, last_epoch=-1) 参数 step_size(int) - 学习率下降间隔数，若为 30，则会在 30、 60、 90…个 step 时，将学习率调整为 lr*gamma。 gamma(float)- 学习率调整倍数，默认为 0.1 倍，即下降 10 倍。 last_epoch(int)- 上一个 epoch 数，这个变量用来指示学习率是否需要调整。当last_epoch 符合设定的间隔时，就会对学习率进行调整。当为-1 时，学习率设置为初始值。 2) 指数衰减调整学习率1torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma, last_epoch=-1) 参数 gamma- 学习率调整倍数的底，指数为 epoch，即 gamma**epoch ast_epoch(int)- 上一个 epoch 数，这个变量用来指示学习率是否需要调整。当last_epoch 符合设定的间隔时，就会对学习率进行调整。当为-1 时，学习率设置为初始值 自适应调整 1). 按照指标调整学习率（ ReduceLROnPlateau） 当某指标不再变化（下降或升高），调整学习率，这是非常实用的学习率调整策略。 例如，当验证集的 loss 不再下降时，进行学习率调整；或者监测验证集的 accuracy，当accuracy 不再上升时，则调整学习率。 1torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=&apos;min&apos;, factor=0.1, patience=10, verbose=False, threshold=0.0001, threshold_mode=&apos;rel&apos;, cooldown=0, min_lr=0, eps=1e-08) mode(str)- 模式选择，有 min 和 max 两种模式， min 表示当指标不再降低(如监测loss)， max 表示当指标不再升高(如监测 accuracy)。 factor(float)- 学习率调整倍数(等同于其它方法的 gamma)，即学习率更新为 lr = lr * factor patience(int)- 忍受该指标多少个 step 不变化，当忍无可忍时，调整学习率。 threshold_mode(str)- 选择判断指标是否达最优的模式，有两种模式， rel 和 abs。当 threshold_mode == rel，并且 mode == max 时， dynamic_threshold = best ( 1 +threshold )；当 threshold_mode == rel，并且 mode == min 时， dynamic_threshold = best ( 1 -threshold )；当 threshold_mode == abs，并且 mode== max 时， dynamic_threshold = best + threshold ；当 threshold_mode == rel，并且 mode == max 时， dynamic_threshold = best - threshold； eps(float)- 学习率衰减的最小值，当学习率变化小于 eps 时，则不调整学习率。 自定义调整 自定义调整学习率 LambdaLR1torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda, last_epoch=-1) lr_lambda(function or list)- 一个计算学习率调整倍数的函数，输入通常为 step，当有多个参数组时，设为 list. last_epoch (int) – 上一个 epoch 数，这个变量用来指示学习率是否需要调整。当last_epoch 符合设定的间隔时，就会对学习率进行调整。当为-1 时，学习率设置为初始值。 参考文献 深度学习最全优化方法总结比较（SGD，Adagrad，Adadelta，Adam，Adamax，Nadam）An overview of gradient descent optimization algorithms]]></content>
      <categories>
        <category>Deep learning</category>
      </categories>
      <tags>
        <tag>optimizer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Entropy 相关的概念]]></title>
    <url>%2F2019%2F11%2F17%2Fentropy%2F</url>
    <content type="text"><![CDATA[介绍熵相关的概念：包括信息熵、相对熵、交叉熵、KL 散度和互信息。并且基于python 实现上述的算法。 （1）香侬信息量 计算机中，信息量用一个信息所需要的编码长度表示。信息量的长度和出现的概率呈负相关。一个词出现的频率越高，那么编码方式越短，同时表示的代价越高（使用该长度的01串表示之后，其他的信息不能使用该前缀的01串表示）。一个词出现的频率越高，信息量越大。 \begin{equation}I=\log _{2}\left(\frac{1}{p(x)}\right)=-\log _{2}(p(x))\end{equation} （2）信息熵（Entropy） In information theory, we like to describe the “surprise” of an event. Low probability events are more surprising therefore have a larger amount of information. Whereas probability distributions where the events are equally likely are more surprising and have larger entropy.在信息论中，使用“surprise”来描述事件。低概率事件具有更高的信息，反之亦然。从而可以推出： 均匀分布具有更高的信息熵 正太分布具有较小的信息熵 对于离散变量 \begin{equation} H(X)=-\sum_{x \in X} p(x) \log p(x)\end{equation} 对于连续变量 \begin{equation}H(X)=-\int_{X} p(x) \log p(x) d x\end{equation} 如果未加特殊说明，本文中一般认为 $p(x)$ 认为是真实分布， $q(x)$是用来生成分布，$q(x)$用来近似表示$p(x)$ （3）交叉熵（Cross-Entropy） Cross-entropy is related to divergence measures that quantifies how much one distribution differs from another.交叉熵是衡量分布差异的指标。可以看出是用一个猜测的分布的编码方式$q(x)$去编码真实的分布 $p(x)$, 计算平均的编码长度（信息量） \begin{equation}H(p, q)=\sum_{X} p(x) \log _{2}\left(\frac{1}{q(x)}\right)\end{equation} 其中 $p(x)$是真实的分布，$q(x)$ 是猜测的分布。 1). 交叉熵作为损失函数 在机器学习中，交叉熵是非常常见的一种损失函数。交叉熵可以用于学习“真实分布”和“预测分布”之间的不同，当交叉熵最低的时候，模型很好得学习到了训练数据集分布，，并不能保证很好的扩展性。训练分布并不是真实分布，所以在模型中需要加上一个正则项，提高模型的泛化性能。 二分类交叉熵损失函数 \begin{equation}L=-[y \cdot \log (p)+(1-y) \cdot \log (1-p)]\end{equation} 其中 $y$表示真实样本 label（1为正类，0为负类）；$p$ 为预测为正的概率。 多分类交叉熵损失函数 \begin{equation}L=-\sum_{c=1}^{M} y_{c} \log \left(p_{c}\right)\end{equation} 其中 $M$表示总的类别数量， $y_c$ 表示指示变量（1或者0， 1 表示预测类别和真实类别相同，否则为0），$p_c$表示预测类比属于 $c$的概率。 2). 交叉熵损失函数相对于平方损失函数的优点 交叉熵搭配 LR激活函数，得到的导数形式为：$$ \frac{\partial}{\partial \theta_{j}} \ell(\theta)= \left(y-h_{\theta}(x)\right) x_{j}$$ 其中$y$表示真实的标签， $h_{\theta}(x)$ 表示模型预测的结果。导数的形式可以说是相当的简单，所以在反向传播的时候计算量很小。上述公式推导过程可以看这里。而平方损失函数的导数形式为：$$\frac{\partial}{\partial \theta_{j}} \ell(\theta) = \left(y-h_{\theta}(x)\right) h^{\prime}_{\theta}(x_j)$$ 可以发现该导数形式需要求一阶导数。所以，交叉熵损失函数相对于平方损失函数是具有计算简单的优点。 3). 代码实现部分 交叉熵定义计算方法 12345678910111213141516# example of calculating cross entropyfrom math import log2# calculate cross entropydef cross_entropy(p, q): return -sum([p[i]*log2(q[i]) for i in range(len(p))])# define datap = [0.10, 0.40, 0.50]q = [0.80, 0.15, 0.05]# calculate cross entropy H(P, Q)ce_pq = cross_entropy(p, q)print('H(P, Q): %.3f bits' % ce_pq)# calculate cross entropy H(Q, P)ce_qp = cross_entropy(q, p)print('H(Q, P): %.3f bits' % ce_qp) 根据KL 散度计算交叉熵（具体推导关系在下面） 1234567891011121314151617181920212223242526# example of calculating cross entropy with kl divergencefrom math import log2# calculate the kl divergence KL(P || Q)def kl_divergence(p, q): return sum(p[i] * log2(p[i]/q[i]) for i in range(len(p)))# calculate entropy H(P)def entropy(p): return -sum([p[i] * log2(p[i]) for i in range(len(p))])# calculate cross entropy H(P, Q)def cross_entropy(p, q): return entropy(p) + kl_divergence(p, q)# define datap = [0.10, 0.40, 0.50]q = [0.80, 0.15, 0.05]# calculate H(P)en_p = entropy(p)print('H(P): %.3f bits' % en_p)# calculate kl divergence KL(P || Q)kl_pq = kl_divergence(p, q)print('KL(P || Q): %.3f bits' % kl_pq)# calculate cross entropy H(P, Q)ce_pq = cross_entropy(p, q)print('H(P, Q): %.3f bits' % ce_pq) 交叉熵作为损失函数计算。（注意如果是二分类需要构造两个分布（0， 1）） 12345678910111213141516171819202122232425# calculate cross entropy for classification problemfrom math import logfrom numpy import mean# calculate cross entropydef cross_entropy(p, q): return -sum([p[i]*log(q[i]) for i in range(len(p))])# define classification datap = [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]q = [0.8, 0.9, 0.9, 0.6, 0.8, 0.1, 0.4, 0.2, 0.1, 0.3]# calculate cross entropy for each exampleresults = list()for i in range(len(p)): # create the distribution for each event &#123;0, 1&#125; expected = [1.0 - p[i], p[i]] predicted = [1.0 - q[i], q[i]] # calculate cross entropy for the two events ce = cross_entropy(expected, predicted) print('&gt;[y=%.1f, yhat=%.1f] ce: %.3f nats' % (p[i], q[i], ce)) results.append(ce)# calculate the average cross entropymean_ce = mean(results)print('Average Cross Entropy: %.3f nats' % mean_ce) 4). 交叉熵和log loss 的关系 Log Loss 是LR 模型中的损失函数。在称述的过程中 交叉熵，log loss和 negative log-likehood 经常交替使用。其实还是有一点区别的。 log loss 是LR 模型基于最大似然估计推导出来的，LR 模型是基于伯努利分布，具体的推导过程可以参考这里。 We can see that the negative log-likelihood is the same calculation as is used for the cross-entropy for Bernoulli probability distribution functions (two events or classes). In fact, the negative log-likelihood for Multinoulli distributions (multi-class classification) also matches the calculation for cross-entropy.当交叉熵基于伯努利分布计算的时候，得到的结果和log loss 是一样的。但是两者不是一个东西， （4）KL 散度（相对熵，Relative Entropy） The Kullback-Leibler Divergence score, or KL divergence score, quantifies how much one probability distribution differs from another probability distribution.KL散度是一种分布距离函数（Statistical Distance），但是更加准确的理解是衡量分布相似度的计算函数，而不是距离计算函数，因为其并不满足严格意义上关于距离的定义。 离散变量表示为： \begin{equation}D_{\mathrm{KL}}(p | q)=-\int_{X} p(x) \log \frac{q(x)}{p(x)} d x\end{equation} 连续变量表示为： \begin{equation}D_{\mathrm{KL}}(p | q)=-\sum_{x \in X} p(x) \log \frac{q(x)}{p(x)}\end{equation} 1). KL 散度的特点 当 $p =q$ 的时候， KL 的值为0 不具有对称，即 $D_{\mathrm{KL}}(p | q) \ne D_{\mathrm{KL}}(q | p)$ 2). 交叉熵和 KL 散度的关系：交叉熵等于熵加上KL 散度之和。推导也很简单 \begin{equation}\begin{aligned} H(p, q) &amp;=-\sum_{x} p(x) \log q(x) \\ &amp;=-\sum_{x} p(x) \log p(x)-\sum_{x} p(x) \log \frac{q(x)}{p(x)} \\ &amp;=H(p)+K L(p | q) \end{aligned}\end{equation} 上述推导以于离散变量为例， 连续变量的推导类似，只是把求和符号$\sum$换成积分符号$\int$。 3). 上代码 1234567891011121314151617181920212223242526272829303132333435# define distributionsevents = ['red', 'green', 'blue']p = [0.10, 0.40, 0.50]q = [0.80, 0.15, 0.05]# plot of distributionsfrom matplotlib import pyplot# define distributionsevents = ['red', 'green', 'blue']p = [0.10, 0.40, 0.50]q = [0.80, 0.15, 0.05]print('P=%.3f Q=%.3f' % (sum(p), sum(q)))# plot first distributionpyplot.subplot(2,1,1)pyplot.bar(events, p)# plot second distributionpyplot.subplot(2,1,2)pyplot.bar(events, q)# show the plotpyplot.show()from math import log2# calculate the kl divergencedef kl_divergence(p, q): return sum(p[i] * log2(p[i]/q[i]) for i in range(len(p))) # calculate (P || Q)kl_pq = kl_divergence(p, q)print('KL(P || Q): %.3f bits' % kl_pq)# calculate (Q || P)kl_qp = kl_divergence(q, p)print('KL(Q || P): %.3f bits' % kl_qp)#KL(P || Q): 1.927 bits #KL(Q || P): 2.022 bits （5）JS 散度 It is more useful as a measure as it provides a smoothed and normalized version of KL divergence, with scores between 0 (identical) and 1 (maximally different), when using the base-2 logarithm.JS 也是一种分布距离函数（Statistical Distance）。JS 散度的存在主要是弥补KL 散度的非对称性。 It uses the KL divergence to calculate a normalized score that is symmetrical. This means that the divergence of P from Q is the same as Q from P, or stated formally: $$JS(P || Q) == JS(Q || P) $$The JS divergence can be calculated as follows:$$JS(P || Q) = \frac{1}{2} \cdot KL(P || M) + \frac{1}{2} \cdot KL(Q || M)$$Where M is calculated as:$$M = \frac{1}{2} \cdot (P + Q)$$ 数据和上面的一样，有了KL 散度的计算， JS也是比较简单的。 1234567891011121314# calculate the js divergencedef js_divergence(p, q): m = 0.5 * (p + q) return 0.5 * kl_divergence(p, m) + 0.5 * kl_divergence(q, m)# calculate JS(P || Q)js_pq = js_divergence(p, q)print('JS(P || Q) divergence: %.3f bits' % js_pq)print('JS(P || Q) distance: %.3f' % sqrt(js_pq))# calculate JS(Q || P)js_qp = js_divergence(q, p)print('JS(Q || P) divergence: %.3f bits' % js_qp)print('JS(Q || P) distance: %.3f' % sqrt(js_qp)) 同样sklearn 中有相应的实现123456789101112# calculate the jensen-shannon distance metricfrom scipy.spatial.distance import jensenshannonfrom numpy import asarray# define distributionsp = asarray([0.10, 0.40, 0.50])q = asarray([0.80, 0.15, 0.05])# calculate JS(P || Q)js_pq = jensenshannon(p, q, base=2)print('JS(P || Q) Distance: %.3f' % js_pq)# calculate JS(Q || P)js_qp = jensenshannon(q, p, base=2)print('JS(Q || P) Distance: %.3f' % js_qp) （6）联合熵，条件熵和互信息（信息增益） 根据熵的定义，拓展到两个变量，就得到了联合熵。\begin{equation}H(X, Y)=-\sum_{x \in X} \sum_{y \in Y} p(x, y) \log p(x, y)\end{equation} 其中 $X$, $Y$ 是两个离散变量，他们的联合分布是 $p(x, y)$。 条件熵定义为 \begin{equation}\begin{aligned}H(Y | X) &amp;=-\sum_{x \in X} p(x) H(y | X =x)\\&amp;=- \sum_{x \in X} \sum{y \in Y} p(x, y) \log p(y|x)\\&amp;= E [- \log (y|x)] \end{aligned}\end{equation} The entropy is a sum of conditional entropies熵和条件熵的关系： \begin{equation}H\left(X_{1}, X_{2}, X_{3}\right)=H\left(X_{1}\right)+H\left(X_{2} | X_{1}\right)+H\left(X_{3} | X_{2}, X_{1}\right)\end{equation} 如果说联合熵是并集，那么互信息是交集。\begin{equation}\begin{aligned} I(X ; Y) &amp;=H(X)-H(X | Y)=H(Y)-H(Y | X) \\ &amp;=H(X)+H(Y)-H(X, Y) \\ &amp;=H(X, Y)-H(X | Y)-H(Y | X) \end{aligned}\end{equation}互信息(Mutual Information) ：一个随机变量由于已知另一个随机变量而减少的不确定性。从上面公式也是可以知道，当增加变量之后，互信息只能是越变越小。 互信息还可以使用 KL 散度进行定义： \begin{equation}\begin{aligned}I(X ; Y) &amp;= D_{KL} (p(X, Y) || p(X) p(Y)) \\&amp;= - \sum_{x \in X} \sum_{y \in Y} p(x, y) \log \frac{p(x, y)}{ p(x) p(y)}\end{aligned}\end{equation} 所以当 $X$ 和 $Y$ 独立的时候，即 $p(x, y) =p(x)p(y)$, 即$I(X ;Y) =0$。（认为规定 $0 \log 0 =0$） 不加证明，互信息是具有对称性和非负性。 上述熵的关系可以使用一张图表示。 （7）决策树ID3中的信息增益（互信息） Mutual information calculates the statistical dependence between two variables and is the name given to information gain when applied to variable selection.互信息计算的是两个变量统计上的依赖关系。 It is commonly used in the construction of decision trees from a training dataset, by evaluating the information gain for each variable, and selecting the variable that maximizes the information gain, which in turn minimizes the entropy and best splits the dataset into groups for effective classification.信息增益是使得熵不断减少，最后的信息越来越确定。 Information Gain, or IG for short, measures the reduction in entropy or surprise by splitting a dataset according to a given value of a random variable. $$IG(S, a) = H(S) – H(S | a)$$其中 $IG(S, a) $ 是数据集 $S$ 对于变量 $a$ 的信息增益， $H(S)$ 表示数据集整个熵， $H(S| a) $表示条件熵。 上代码： 问题是二分类，其中 $label =1$ 的样本有7个，$label =0$ 的样本有13个；其中有一个特征，该特征只有两个值 $s_1, s_2$，定义符号 $s_{i,j}$ 其中 $i$表示特征值， $j $表示label 值。所以 $s_{1, 0} =7$, $s_{1, 1} =1$, $s_{2, 0} =6$, $s_{2, 1} =6$。然后计算熵 和信息增益。 123456789101112131415161718192021222324252627282930from math import log2 # calculate the entropy for the split in the datasetdef entropy(class0, class1): return -(class0 * log2(class0) + class1 * log2(class1)) # split of the main datasetclass0 = 13 / 20class1 = 7 / 20# calculate entropy before the changes_entropy = entropy(class0, class1)print('Dataset Entropy: %.3f bits' % s_entropy) # split 1 (split via value1)s1_class0 = 7 / 8s1_class1 = 1 / 8# calculate the entropy of the first groups1_entropy = entropy(s1_class0, s1_class1)print('Group1 Entropy: %.3f bits' % s1_entropy) # split 2 (split via value2)s2_class0 = 6 / 12s2_class1 = 6 / 12# calculate the entropy of the second groups2_entropy = entropy(s2_class0, s2_class1)print('Group2 Entropy: %.3f bits' % s2_entropy) # calculate the information gaingain = s_entropy - (8/20 * s1_entropy + 12/20 * s2_entropy)print('Information Gain: %.3f bits' % gain) 使用sklearn 计算 12from sklearn.tree import DecisionTreeClassifiermodel =sklearn.tree.DecisionTreeClassifier(criterion ='entropy') 决策树 相关的还需要总结。包括 有收藏的连接 和本地的文件。 （8）信息增益率 信息增益率是决策树 C4.5 引入的划分特征准则，（9）基尼系数]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>kl 散度</tag>
        <tag>交叉熵</tag>
        <tag>cross entropy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python learning 学习笔记3]]></title>
    <url>%2F2019%2F11%2F04%2Fpython-learning3%2F</url>
    <content type="text"><![CDATA[python 学习笔记，之前有两篇笔记，因为太多了，所以新开一篇。 python 中常见的判别函数 isinstance() 判断一个item 是否是list or tuple12345678def myextend(alist): blist = [] for item in alist: if isinstance(item, (list, tuple)): blist.extend(myextend(item)) else: blist.append(item) return blist 基本类型对于 classinfo 可以是1int，float，bool，complex，str(字符串)，list，dict(字典)，set，tuple isinstance() 与 type() 区别： type() 不会认为子类是一种父类类型，不考虑继承关系。 isinstance() 会认为子类是一种父类类型，考虑继承关系。如果要判断两个类型是否相同推荐使用 isinstance()。12345678910class A: pass class B(A): pass isinstance(A(), A) # returns Truetype(A()) == A # returns Trueisinstance(B(), A) # returns Truetype(B()) == A # returns False 针对字符串的判别函数 1234567s.isalnum() 所有字符都是数字或者字母s.isalpha() 所有字符都是字母s.isdigit() 所有字符都是数字s.islower() 所有字符都是小写s.isupper() 所有字符都是大写s.istitle() 所有单词都是首字母大写，像标题s.isspace() 所有字符都是空白字符、\t、\n、\r 对于数字的判别 1234567891011121314isdigit()True: Unicode数字，byte数字（单字节），全角数字（双字节），罗马数字False: 汉字数字Error: 无isdecimal()True: Unicode数字，，全角数字（双字节）False: 罗马数字，汉字数字Error: byte数字（单字节）isnumeric()True: Unicode数字，全角数字（双字节），罗马数字，汉字数字False: 无Error: byte数字（单字节） 具体实例可以看这里 字符串的拼接 在python 开发中，很多时候需要使用到字符串拼接，python 字符串拼接有很多，这里就总结一下： 123格式化类：%、format()、template拼接类：+、()、join()插值类：f-string 使用+ 拼接 使用 % 拼接 使用 join() 方法拼接 使用 format() 方法拼接 使用 f-string 拼接 使用+ 拼接 这个适合数据量比价小的时候，当总长度小于 20 的时候，可以使用 使用 % 拼接 借鉴了C语言中 printf 函数的功能，如果你有C语言基础，看下文档就知道了。这种方式用符号“%”连接一个字符串和一组变量，字符串中的特殊标记会被自动用右边变量组中的变量替换：1print '%s %s'%('Python', 'Tab') 使用 join() 方法拼接如果你想要合并的字符串是在一个序列或者 iterable 中，那么最快的方式就是使用 join() 方法。比如：1234567&gt;&gt;&gt; parts = ['Is', 'Chicago', 'Not', 'Chicago?']&gt;&gt;&gt; ' '.join(parts)'Is Chicago Not Chicago?'&gt;&gt;&gt; ','.join(parts)'Is,Chicago,Not,Chicago?'&gt;&gt;&gt; ''.join(parts)'IsChicagoNotChicago?' 使用 format() 方法拼接 12str = 'There are &#123;&#125;, &#123;&#125;, &#123;&#125; on the table'str.format(fruit1,fruit2,fruit3) 还可以指定参数对应位置：12str = 'There are &#123;2&#125;, &#123;1&#125;, &#123;0&#125; on the table'str.format(fruit1,fruit2,fruit3) #fruit1出现在0的位置 最重要的需要引起注意的是，当我们使用加号(+)操作符去连接大量的字符串的时候是非常低效率的， 因为加号连接会引起内存复制以及垃圾回收操作。 特别的，你永远都不应像下面这样写字符串连接代码： 123s = ''for p in parts: s += p 同样还得注意不必要的字符串连接操作。有时候程序员在没有必要做连接操作的时候仍然多此一举。比如在打印的时候：123print(a + ':' + b + ':' + c) # Uglyprint(':'.join([a, b, c])) # Still uglyprint(a, b, c, sep=':') # Better 当混合使用I/O操作和字符串连接操作的时候，有时候需要仔细研究你的程序。 比如，考虑下面的两端代码片段：123456# Version 1 (string concatenation)f.write(chunk1 + chunk2)# Version 2 (separate I/O operations)f.write(chunk1)f.write(chunk2) 如果两个字符串很小，那么第一个版本性能会更好些，因为I/O系统调用天生就慢。 另外一方面，如果两个字符串很大，那么第二个版本可能会更加高效， 因为它避免了创建一个很大的临时结果并且要复制大量的内存块数据。 还是那句话，有时候是需要根据你的应用程序特点来决定应该使用哪种方案。 这种方式也1print('python', 'tab' ,'i', sep =',') 使用 f-string 拼接 从Python3.6版本引入。这种方式在可读性上秒杀format()方式，处理长字符串的拼接时，速度与join()方法相当。12345name = 'world'myname = 'python_cat'words = f'Hello &#123;name&#125;. My name is &#123;myname&#125;.'print(words)&gt;&gt;&gt; Hello world. My name is python_cat. 总结： 当要处理字符串列表等序列结构时，采用join()方式；拼接长度不超过20时，选用+号操作符方式；长度超过20的情况，高版本选用f-string，低版本时看情况使用format()或join()方式。 python中strip()，lstrip()，rstrip()函数 想要去掉什么字符，那么就传入什么字符，默认是空格。如果没有输入，那么就是删除首尾空格的意思： strip() 删除前导和后缀字符 但是当有了参数（传入的是一个字符数组）那么就会删除所有字符数组中的所有数组。比如说： 12345678910theString = 'saaaay yes no yaaaass'print theString.strip('say') print theString.strip('say ') #say后面有空格 print theString.lstrip('say') print theString.rstrip('say') # 输出结果是yes no es no yes no yaaaass saaaay yes no Python 中的 is 和 == is is the identity comparison. #比较引用是否相同 == is the equality comparison. #比较内容是否相同 1The operator a is b returns True if a and b are bound to the same object, otherwise False. When you create two empty lists you get two different objects, so is returns False (and therefore is notreturns True) magic method 定义：函数声明时候的函数名是以双下划线开始和结尾的，比如说__init__这样的函数声明，被认为是 magic函数。常见的魔法函数有以下几种： （1）创建 __new__函数 创建类并且返回类的实例。 （2）初始化__init__函数 初始化该实例 （3）__str__函数 （4）__len__函数 运算符相关的魔法方法很多，比如说（5）__cmp__()函数 （6）__eq__()函数 （7）__add__(self, other)函数 python中魔术方法（magic method）是一种重载（overloading）函数。 （6）python交换值为何不使用中间变量 在C++和C 中交换两个变量需要使用到临时变量tmp，但是在python 中是不必的。交换两个变量一般使用如下的语句： 1x, y =y, x 一般来说python语句是从左到右解析的，但是在赋值操作的时候，因为右值具有更高的计算优先级，所以是从右到左解析的。执行顺序如下： 计算等号右边 y, x （可能是比较复杂的表达式），然后创建元组，分别存储 y, x的值 计算左边的标识符，元组中的element 分别赋值（如果等号左右两边标识符数量不等的时候会出现 ValueError， 当左边只有一个变量时候，右边计算值就pack成一个元组传递给了左边的变量） python2 和python3 的差异使用了from __future__ import xxx可以在python2，python3环境下运行同一份代码而不出错 1234from __future__ import absolute_importfrom __future__ import divisionfrom __future__ import print_functionfrom __future__ import unicode_literals 以 为例子 123456#coding:utf-8from datetime import datetimenow = datetime.now()print now.strftime('%m月%d日 %H:%M')# 03月12日 21:53 123456from __future__ import unicode_literalsfrom datetime import datetimenow = datetime.now()print now.strftime('%m月%d日 %H:%M')# 报错 UnicodeEncodeError: 'ascii' codec can't encode character u'\u6708' in position 2: ordinal not in range(128) 原因： 因为datetime.strftime()只接受str类型的字符串，不接受unicode类型的字符串。解决方案：推荐做法： 传入 str类型的参数123456#coding:utf-8from __future__ import unicode_literalsfrom datetime import datetimenow = datetime.now()print now.strftime('%m月%d日 %H:%M'.encode('utf-8')) # 指明str类型字符串 不推荐做法： 设置运行的时候的编码为 utf-812345678910#coding:utf-8from __future__ import unicode_literalsimport sysfrom datetime import datetimereload(sys)sys.setdefaultencoding('utf-8')now = datetime.now()print now.strftime('%m月%d日 %H:%M') python中*的使用python 中 *有乘法和赋值 ([1] *5) 的用法，除此之外还有解压（unpack）的功能。而zip() 是压缩的功能，如下代码所示：12345stuff = ['apple','banana','peach']money = [10, 5, 7]pair = list(zip(stuff,money))# pair = [('apple',10),('banana',5),('peach',7)] 同样的，当我们想要还原成原来的两个list， stuff 和money 时候，我们需要用到 * 符号。1stuff, money =zip(*pair) 当然，并不是说 *只能使用在 list 中，凡是可迭代对象都是可以使用 *符号。1234567def do_something(x,y,z): print('x is ', x) print('y is ', y) print('z is ', z)list1 = ['a','b','c']do_something(*list1) 可见， *操作符自动将list1 中的三个元素赋给了三个形参，所以是一种非常方便的用法。 ** 的用法 对于字典（dict） 来说，也是可以使用*运算符，但是一个 * 得到的是key 值，如果想要得到 value 值，那么是需要使用 **运算符。12dict2 = &#123;'z':1, 'x':2, 'y':3&#125;do_something(**dict2) 所以上面那种传参的方式就是在程序中经常见到的。 *args 和 **kwargs 是我们经常见到的两个参数。用法很简单，当不确定有什么待传入的参数时候，就使用*args 将形参传入到 args 元组里面。同理 **kwargs效果则是将输入的未定义的形参及其对应的值存在kwargs字典。 123456def intro(**data): print("\nData type of argument:",type(data)) for key, value in data.items(): print("&#123;&#125; is &#123;&#125;".format(key,value))intro(Firstname="Sita", Lastname="Sharma", Age=22, Phone=1234567890) 也可以进行字典的合并 **。 123date_info = &#123;'year': "2020", 'month': "01", 'day': "01"&#125; track_info = &#123;'artist': "Beethoven", 'title': 'Symphony No 5'&#125; all_info = &#123;**date_info, **track_info&#125; 随机数 We do not need true randomness in machine learning. Instead we can use pseudorandomness. Pseudorandomness is a sample of numbers that look close to random, but were generated using a deterministic process.很难得到到真实的随机数，然而是可以生成伪随机数。 python 中的random() 库函数是基于马特赛特旋转演算法 （Mersenne Twister）算法生成的。 The pseudorandom number generator is a mathematical function that generates a sequence of nearly random numbers. It takes a parameter to start off the sequence, called the seed. The function is deterministic, meaning given the same seed, it will produce the same sequence of numbers every time. The choice of seed does not matter.给定特定的种子（seed），可以产生 deterministic 的结果，所以可以重复多次，所以可以复现相同的实验结果。 Random with Python（1） Random Floating Point Values 1234567from random import seedfrom random import randomseed(1)for _ in range(10): # generate random numbers between 0-1 val =random() print(val) 产生的是[0, 1]区间的浮点数，可以 rescale to 任意的范围。 1scaled_value =min +(val *(max -min)) （2） Random Integer Values 使用 randint() 得到整数 1234567frmo random import seedfrom random import randintseed(1)for _ in range(10): val =randint(0, 10) print(val) （3）Random Gaussian Values 使用的是 gauss 分布。12345678from random import seedfrom random import gaussseed(1)for _ in range(10): val =gauss(0, 1) print(val) （4） Random Choosing From a List 使用到了 choice() 函数，choice 每次只是选择了其中的一个。 1234567891011from random import seedfrom random import choiceseed(1)sequence =[i for i in range(20)]print(sequence)for _ in range(5): selection =choice(sequence) print(selection) （5）Random Subsample From a List 123456789from random import seedfrom random import sampleseed(1)sequence =[i for i in range(20)]print(sequence)# default without replacement，不放回抽样subset =sample(sequence, 5) python 中的 random.sample() 是没有放回抽样的。如果想要实现，一种方式是基于 randint() 改造，一种是使用numpy 中的函数：numpy.random.choice(replace =True)。但是python自带的包中可以设置 weight权重，也就是有权重的sample。 （5） Randomly Shuffle a List shuffle 操作在python 中是 in-place() 的 12345678from random import seedfrom random import shuffleseed(1)sequence =[i for i in range(10)]print(sequence)shuffle(sequence)print(sequence) Random Numbers with Numpy The NumPy pseudorandom number generator is different from the Python standard library pseudorandom number generator. Importantly, seeding the Python pseudorandom number generator does not impact the NumPy pseudorandom number generator. It must be seeded and used separately.Numpy 中有自己实现的Random 机制，该机制和Numpy 中的机制不一样，当然Numpy 中也有马特赛特旋转演算法 （Mersenne Twister）算法。重要的是，两者是需要单独设置的。 （1） Seed The Random Number Generator 1234567# seed the pseudorandom number generatorfrom numpy.random import seedfrom numpy.random import rand# seed random number generatorseed(1)# generate some random numbersprint(rand(3)) （2）Array of Random Floating Point Values 12345from numpy.random import seedfrom numpy.random import randseed(1)vals =rand(10)print(vals) （3）Array of Random Integer Values 1234567from numpy.random import seedfrom numpy.random import randintseed(1)# start, end, numbersvalues =randint(0, 10, 20)print(values) （4）Array of Random Gaussian Values 12345from numpy.randon import seedfrom numpy.random import randnseed(1)values =randn(10)print(values) （5）Shuffle Numpy Array这个也是 in-place 的操作12345678from numpy.random import seedfrom numpy.random import shuffleseed(1)sequence =[i for i in range(20)]print(sequence)shuffle(sequence)print(sequence) 总结：随机数的生成需要设置 seed，如果生成number，那么使用 python standard library； 如果是生成array, 那么使用 NumPy library。 参考文献 How to Generate Random Numbers in Python]]></content>
      <categories>
        <category>CS基础</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[基于隐马尔科夫的命名实体识别]]></title>
    <url>%2F2019%2F09%2F06%2Fhmm%2F</url>
    <content type="text"><![CDATA[介绍自然语言处理中的命名实体识别（Named Entity Recognition，NER ）问题，目前主流的方式是通过一个语言模型（比如LSTM，BERT）+CRT（条件随机场）实现。要想要理解CRF模型，可以先从隐马尔科夫模型（Hidden Markov Model）学习，所以本文主要主要介绍基于HMM的序列标注问题。 命名实体识别 Named-entity recognition (NER) (also known as entity identification, entity chunking and entity extraction) is a subtask of information extraction that seeks to locate and classify named entity mentions in unstructured text into pre-defined categories such as the person names, organizations, locations, medical codes, time expressions, quantities, monetary values, percentages, etc. 给定一个句子，然后识别出句子中有意义的实体，比如说人名，机构名，地名，医药编码，时间表达，数量词，金钱表达和百分比。 上述问题可以看成一个文本多分类的问题，按照道理讲使用一个语言模型应该是可以解决的，但是为什么还要加上概率图模型？ “自贸区”对应的标注是: 自(B-LOC)贸(I-LOC)区(I-LOC), 这三个字都对应一个”地名”的标签, 但是第一个字属于实体开头的字, 所以使用”B”开头的标签, 后面两个字的标签都是”I”开头.当单独使用语言模型时候可能出现上述的问题。所以如果加上概率图模型，比如条件随机场，就可以约束模型的输出，防止出现不合规的标注输出。 HMM（1）定义 马尔科夫模型（markov model） In probability theory, a Markov model is a stochastic model used to model randomly changing systems. It is assumed that future states depend only on the current state, not on the events that occurred before it (that is, it assumes the Markov property). Generally, this assumption enables reasoning and computation with the model that would otherwise be intractable.在概率中，马尔科夫假设未来的状态仅仅由当前的状态决定，和其他的任何状态都无关。这种假设极大的简化了运算。 隐马尔科夫模型 A hidden Markov model is a Markov chain for which the state is only partially observable. In other words, observations are related to the state of the system, but they are typically insufficient to precisely determine the state. Several well-known algorithms for hidden Markov models exist. For example, given a sequence of observations, the Viterbi algorithm will compute the most-likely corresponding sequence of states, the forward algorithm will compute the probability of the sequence of observations, and the Baum–Welch algorithm will estimate the starting probabilities, the transition function, and the observation function of a hidden Markov model. 隐马尔科夫模型描述的是由不可观测的隐状态序列（实体标注）生成可观测状态（可读文本）的过程。使用前向传递算法计算观测序列的概率，使用viterbi 算法得到最优的标注徐磊，使用Baum-Welch 算法估计初始的状态，转移概率矩阵和发射矩阵。 （2）两大基本假设 1) 第$t$个隐状态(实体标签)只跟前一时刻的$t-1$隐状态(实体标签)有关, 与除此之外的其他隐状态(如$t-2,\ t+3$)无关.例如上图中: 蓝色的部分指的是$i_t$只与$i_{t-1}$有关, 而与蓝色区域之外的所有内容都无关, 而$P(i_{t}|i_{t-1})$指的是隐状态$i$从$t-1$时刻转向$t$时刻的概率, 具体转换方式下面会细讲.2) 观测独立的假设, HMM模型中是由隐状态序列(实体标记)生成可观测状态(可读文本)的过程, 观测独立假设是指在任意时刻观测$o_t$只依赖于当前时刻的隐状态$i_t$, 与其他时刻的隐状态无关. 例如上图中: 粉红色的部分指的是$i_{t+1}$只与$o_{t+1}$有关, 跟粉红色区域之外的所有内容都无关. （3）三大参数 1) HMM的转移概率矩阵（transition probabilities） 转移矩阵表示为$A$矩阵, 则$A_{ij}$表示矩阵中第i行第j列:$$A_{ij}=P(i_{t+1}= q_j | i_{t} = q_i) \quad q_i \in Q_{hidden}$$上式表示在$t$ 时刻的实体标签是 $q_i$， 在 $t+1$时刻的实体标签转换成了 $q_j$，概率表示为上式。 2) HMM的发射概率矩阵（emission probabilities） 发射矩阵表示为$B$矩阵, 则$B_{jk}$表示矩阵中第$j$行第$k$列:$$B_{jk}=P(o_{t}= v_k | i_{t} = q_j) \quad q_i \in Q_{hidden} \quad v_k \in V_{obs.}={v_0, v_1, … , v_{M-1} }$$上式表示指的是在$t$时刻由实体标签(隐状态)$q_j$生成汉字(观测结果)$v_k$的概率. 3) HMM的初始隐状态概率（initial probabilities） 通常用$\pi$来表示（不是圆周率）:$$\pi=P(i_1=q_i) \quad q_i \in Q_{hidden} = { q_0, q_1, … , q_{N-1}}$$上式指的是语言序列中第一个字$o_1$的实体标记是$q_i$的概率, 也就是初始隐状态概率. Viterbi算法（1）定义 The Viterbi algorithm is a dynamic programming algorithm for finding the most likely sequence of hidden states—called the Viterbi path—that results in a sequence of observed events, especially in the context of Markov information sources and hidden Markov models (HMM). 维特比算法（viterbi algorithm）使用的是动态规划的思想解决HMM 和CRF 中的预测问题。即在HMM模型中寻找最有可能的隐状态的路径。使用一句话概括为：在每一时刻，计算当前时刻是由上一个时刻中的每种隐状态导致的最大概率，并且记录这个最大概率是从前一时刻哪个隐状态转移过来的，最后回溯最大概率，即为路径。 （2）数学表达 1) $A_{i, j}^{t-1, t}$, 是转移概率矩阵$A$中的第$i$行第$j$列(下标), 指的是在$t-1$时刻实体标签为$q_i$, 而在$t$时刻实体标签转换到$q_j$的概率.2) $B_{jk}$是发射矩阵的第j行第k列, 指的是在第$t$时刻, 由隐状态$q _j$生成观测$v _k$的概率.3) 有了上面两点, 则$\hat{q} _j = A _{ij}B _{jk}$表示在$t$时刻的隐状态为$q _j$的概率估计. （3）计算过程 假设我们通过HMM建模并学习, 得到了模型所估计的参数$(A, B, \pi)$, 注意下面的$A, B$矩阵按行求和为$1$; 需要申请$T1, T2$两个表格：如果$T1, T2$表格是$i*j$的矩阵, 则矩阵中第$j$列指的是第$j$时刻, 第$i$行指的是第$i$种隐状态, $T1[i, \ j]$指的是在第$j$时刻, 落到隐状态$i$的最大可能的概率是多少(不要着急, 到了下一个时刻就会明白最大是什么意思), 而$T2[i, \ j]$记录的是这个最大可能的概率是从第$j-1$时刻(上一时刻)的哪一种隐状态$i$转移过来的, 也就是说我们记录的是最大可能的概率的转移路径.我们现在将第一时刻的计算结果填入$T1, T2$表格, 注意在第$0$时刻的隐状态是由初始隐状态概率矩阵提供的, 而不是从上一时刻的隐状态转移过来的, 所以我们直接在$T2$表格上记为$NAN(not \ a \ number)$。然后计算表格上每个位置的最大概率。 （4）时间复杂度 假设有$N$种隐状态，在每个时刻之间，一共可能有 $N^2$种方式，假设有$T$时刻，那么viterbi 算法的时间复杂度$O(TN^2)$ 分析代码（1）数据处理部分 12self.tag_size = len(self.tag2idx) # 对于tag标签是len 取得长度self.vocab_size = max([v for _, v in self.char2idx.items()]) + 1 # 对于vocab 是需要求解index的最大值，有可能vocab中空着一些，以备后用 123self.pi = np.log(self.pi) # 在计算矩阵相乘的时候，log 运算转换成了矩阵加法，防止下溢出self.transition = np.log(self.transition)self.emission = np.log(self.emission) 12T1_table[i, :] = np.max(curr_score, axis=0) # 使用viterbi 算法时候，t1 table存储概率最大值，t2 table 存储最大值所属于上一个隐状态的序号T2_table[i, :] = np.argmax(curr_score, axis=0) 1eval() # 执行一个字符串表达式，然后返回运行的结果；一般来说可能不安全 参考文献1.Hidden Markov model2.NER_hidden_markov_model]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>hmm</tag>
        <tag>ner</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[容器（Docker）的原理和常见的命令]]></title>
    <url>%2F2019%2F09%2F04%2Fdocker%2F</url>
    <content type="text"><![CDATA[介绍容器（Docker）的原理和常见的命令。（ongoing） Docker 是对于 Linux 容器的一种封装，提供了见到那的容器使用接口。Linux 容器是一种虚拟化技术，在正常的进程外面加上了一个保护层，实现了该进程与底层系统的隔离。Docker 是使用Go语言实现的。 （1）虚拟机（virtual machine）和Docker 的联系和区别 虚拟机是在一个系统中运行了另一个系统。比如在mac 电脑中装上了windows（stupid）；在windows 中装上了Ubuntu。 相同点：软件开发中环境可能存在两种问题：”程序能在我的机器上跑“ （It worked on my machine）；程序中新老版本不兼容，如python2 和python3 中一些库函数。虚拟机和docker 都是为了解决软件开发中的环境配置问题。 不同点：虚拟机是操作系统级别。虚拟机本身需要占用几百兆运行，并且启动慢。Docker 是对于进程的封装，不是模拟完整的操作系统。所以启动快，占用资源少。 联系：Docker 相比于虚拟机有了更大程度的共享，更小级别的隔离。虚拟机虚拟出的是一套硬件，而Docker 虚拟出的是一个操作系统，不同容器之间是可以共享一个操作系统的。 （2）Docker的核心概念 1). 镜像（Image）： 是一个只读的模板，类似安装操作系统的iso 文件，通过镜像可以用来完成各种应用的部署2). 容器（Container）：镜像和容器的唯一区别是容器的最上一层是可读可写的。所以容器 =镜像+ 可读层。类似一个操作系统，可以create， start， run，stop，kill，pause和rm操作。3). 仓库：存放镜像的一个场合，仓库分为公开仓库和私有仓库 如果说Docker 是面向对象的，那么容器与镜像的关系类似于面向对象编程中的对象与类。 Docker 的缺点：对于开发者是非常友好的，但是对于运维人员，想要在已经封装好的容器上做一些调整，是很难的。]]></content>
      <categories>
        <category>CS基础</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dynamic Programming]]></title>
    <url>%2F2019%2F08%2F29%2Fdynamic-programming%2F</url>
    <content type="text"><![CDATA[先介绍了动态规划的概念，使用的必要条件，和其他类似算法的区别和联系。然后是实战，coding部分。 概念 Dynamic programming (DP) is as hard as it is counterintuitive. dp 不太好想出来，所以是 counterintuitive，但如果想出来，那么程序时间效率上会很高。 定义： Dynamic programming amounts to breaking down an optimization problem into simpler sub-problems, and storing the solution to each sub-problem so that each sub-problem is only solved once. 使用dp 的两个必要条件： has an optimal substructure. has overlapping subproblems. 最优子结构最优子结构，如果简单理解，那么可以等同于递归。如果一个问题能够使用递归解决，那么就具有最优子结构。 Optimal substructure simply means that you can find the optimal solution to a problem by considering the optimal solution to its subproblems. 对于一个子问题的最优解，那么也是原来问题的最优解。 举例说明一个不是最优子结构的问题. according to wikipedida: “Using online flight search, we will frequently find that the cheapest flight from airport A to airport B involves a single connection through airport C, but the cheapest flight from airport A to airport C involves a connection through some other airport D.” 重复的子问题 正因为有了 overlapping subproblems，那么才是需要 memory，才是需要cache。使用 Fibonacci problem 举例说明： 如上图所示，计算 fib(4) 和计算 fib(3) 都是需要计算fib(1) 和fib(2)。 那么这个时候就是有重复的子问题。 时间复杂度分析 In programming, Dynamic Programming is a powerful technique that allows one to solve different types of problems in time $O(n^2)$ or $O(n^3)$ for which a naive approach would take exponential time. 使用 $O(n^2) $或者 $O(n^3)$的时间复杂度从而避免指数级运算。所以有下面的额表示方式： One can think of dynamic programming as a table-filling algorithm: you know the calculations you have to do, so you pick the best order to do them in and ignore the ones you don’t have to fill in.Tabulation is an approach where you solve a dynamic programming problem by first filling up a table, and then compute the solution to the original problem based on the results in this table. 表格实现了memory机制，优化了时间效率。memory 就等同于 cache，这个基本假设是有多次相同的问题需要被解决，如果没有重复的问题，那么也是没有必要使用memory 进行时间上的优化的。 两种不同的思考方式： Top-Down : Start solving the given problem by breaking it down. If you see that the problem has been solved already, then just return the saved answer. If it has not been solved, solve it and save the answer. This is usually easy to think of and very intuitive. This is referred to as Memoization. Bottom-Up : Analyze the problem and see the order in which the sub-problems are solved and start solving from the trivial subproblem, up towards the given problem. In this process, it is guaranteed that the subproblems are solved before solving the problem. This is referred to as Dynamic Programming. 第一种类似带有memory机制的递归。memory机制就是一种典型的空间换取时间的方式。 dp 和其他算法的不同 Divide and conquer is slightly a different technique. In that, we divide the problem in to non-overlapping subproblems and solve them independently, like in merge sort and quick sort. Greedy Algorithms which make a decision once and for all every time they need to make a choice, in such a way that it leads to a near-optimal solution. Dynamic programming is basically, recursion plus using memory. The intuition behind dynamic programming is that we trade space for time. 大多数dp 问题都可以归结为这两类： Optimization problems. Combinatorial problems.The optimization problems expect you to select a feasible solution, so that the value of the required function is minimized or maximized. Combinatorial problems expect you to figure out the number of ways to do something, or the probability of some event happening. 最优解得到是最大值或者最小值；组合问题得到是解的个数。 实例解题步骤 Define subproblems Write down the recurrence that relates subproblems Recognize and solve the base cases 对问题的定义（一般是一维、二维表格） 找到当前问题和子问题的转换关系 初始化 1-dimensional DP Example问题一： Problem: given n, find the number of different ways to write n as the sum of 1, 3, 4Example: for n = 5, the answer is 6 12345675= 1+1+1+1+1= 1+1+3 = 1+3+1= 3+1+1= 1+4= 4+1 解题步骤 对问题的定义 Let $D_n$ be the number of ways to write the sum of 1, 3, 4 转换关系 $D_n = D_{n−1} + D_{n−3} + D_{n−4}$ 初始化 $D_0 =D_1 =D_2 =1$, and $D_3 =2$ c++ 代码实现 版本一：就是一种dfs 的思路1234567int dfs(int n)&#123; // 这个说是dp 但是更像是dfs，只不过 if (n ==0 || n ==1 ||n ==2) return 1; if(n ==3) return 2; return dfs(n-1) +dfs(n-3)+dfs(n-4);&#125; 空间复杂度是 $O(n)$，时间是 $O(n)$， 空间换取时间。12345678910111213unordered_map&lt;int, int&gt; cache;int dfs(int n)&#123; // 这个说是dp 但是更像是dfs，只不过 if (n ==0 || n ==1 ||n ==2) return 1; if(n ==3) return 2; if (cache.count(n)) return cache[n]; else &#123; cache[n] =dfs(n-1) +dfs(n-3)+dfs(n-4); return cache[n]; &#125;&#125; python 实现，时间复杂度 $O(n)$ 空间复杂度 $O(n)$ 12345678910cache =&#123;&#125;def dp(n): if n==0 or n ==1 or n==2: return 1 if n ==3: return 2 if n in cache: return cache[n] else: cache[n] =dp(n-1) +dp(n-3) +dp(n -4) return cache[n]print(dp(5)) 问题二： Tri Tiling （这里是需要补充的） Given n, find the number of ways to fill a 3×n board with dominoes c++ 代码，关键是动态转移方程。$$\begin{split}A_n &amp;= A_{n-2} +B_{n-1} + C_{n-1} \\B_n &amp;= A_{n-1} + B_{n-2}\end{split}$$ 讲解 12345678910111213141516171819int fun(int n , vector&lt;int&gt; &amp; a, vector&lt;int&gt;&amp; b)&#123; a[0] =1, a[1] =0; b[0] =1, b[1] =1; for(int i =2; i&lt;=n; i++) &#123; a[i] =a[i-2] +2*b[i-1]; b[i] =a[i-1] +b[i-2]; &#125; return a[n];&#125;int main()&#123; int n; cin &gt;&gt;n; vector&lt;int&gt; a1(n+1), b1(n+1); cout &lt;&lt; fun(n, a1, b1)&lt;&lt; endl; return 0;&#125; 还有一个比较简单的题目， Given n, find the number of ways to fill a 2*n board with dominoes 动态转移方程（这个相对上一个考虑的情况少一些）$$\begin{split}A_n &amp;=n if n ==1 || n ==2 \\A_n &amp; = A_{n-1} +A_{n -2} else\\\end{split}$$ 123456789101112131415int fun(int n , vector&lt;int&gt; &amp; a)&#123; a[1] =1, a[2] =2; for(int i =3 ; i&lt;=n; i++) a[i] =a[i-1] + a[i-2]; return a[n];&#125;int main()&#123; int n; cin &gt;&gt;n; vector&lt;int&gt; a(n+1); cout &lt;&lt; fun(n, a)&lt;&lt; endl; return 0;&#125; 2-dimensional DP Example问题一：最长子序列 Problem: given two strings x and y, find the longest common subsequence (LCS) and print its length 解题步骤： 对问题的定义$D_{ij}$ 表示 $x_{1, …, i}$ 和$y_{1, …, j}$ 的LCS 的长度 转换关系（回文串就是前后缀问题） $D_{ij} =D_{i-1, j-1} +1$ if $x_i =y_j$， 因为both of them contribute to the LCS $D_{ij} =max{D_{i-1, j}, D_{i, j-1}} $ 否则的话 初始化(就是解决最小的问题，就是当 $i-1 ==0$ 的时候， 或者bad case 是如何进行处理)$D_{i0} =D_{0j} =0$ python代码实现 12 c++ 代码实现1234567891011121314151617181920212223242526#include&lt;iostream&gt;using namespace std;const int MAXV =10;int n,m;int lcs(char * str1, char * str2)&#123; vector&lt;vector&lt;int&gt;&gt; dp(n+1, vector&lt;int&gt;(m+1, 0)); for(int i =1; i&lt;= n; i++) for(int j =1; j&lt;=m; j++) &#123; if(str1[i] ==str2[j]) dp[i][j] =dp[i-1][j-1] +1; else dp[i][j] =max(dp[i][j-1], dp[i-1][j]); &#125; return dp[n][m];&#125;int main()&#123; char s1[MAXV], s2[MAXV]; cin &gt;&gt;n&gt;&gt;m; cin &gt;&gt;s1+1&gt;&gt; s2+1; cout &lt;&lt; lcs(s1, s2)&lt;&lt; endl; return 0;&#125; python实现。在申请空间的时候，多申请一圈的空间。并且在判断的时候，使用 str1[i-1] ==str2[j-1] 来处理越界的问题。 123456789101112131415# 边界条件是根据实际意义出发str1 ="abcd"str2 ="bad"def lcs(str1, str2): n, m =len(str1), len(str2) dp =[ [0]*(m+1) for _ in range(n+1)] for i in range(1, n+1): for j in range(1, m +1): if(str1[i-1] ==str2[j-1]): dp[i][j] =dp[i-1][j-1] +1 else: dp[i][j] =max(dp[i-1][j], dp[i][j-1]) return dp[n][m]print(lcs(str1, str2)) How can we now find the sequence? To find the sequence, we just walk backwards through matrix starting the lower-right corner. If either the cell directly above or directly to the right contains a value equal to the value in the current cell, then move to that cell (if both to, then chose either one). If both such cells have values strictly less than the value in the current cell, then move diagonally up-left (this corresponts to applying Case 2), and output the associated character. This will output the characters in the LCS in reverse order. For instance, running on the matrix above, this outputs DABA. 上面的是从底到上思路（迭代） 然后看第二种思路（从上到下），那么就递归+ memory 12345678LCS(S,n,T,m)&#123;if (n==0 || m==0) return 0;if (arr[n][m] != unknown) return arr[n][m]; // &lt;- added this line (*) if (S[n] == T[m]) result = 1 + LCS(S,n-1,T,m-1);else result = max( LCS(S,n-1,T,m), LCS(S,n,T,m-1) );arr[n][m] = result;return result;&#125; 扩展题目 More about LCS: Discussion and Extensions. An equivalent problem to LCS is the “mini- mum edit distance” problem, where the legal operations are insert and delete. 转移方程$$ dp[i][j] = \begin{cases}1 +min{dp[i][j-1], dp[i-1][j], dp[i-1][j-1]} +1&amp; x_i \neq x_j \\dp[i-1][j-1]&amp; x_i = x_j\end{cases}$$ c++ 实现。1234567891011121314151617int minDistance(string word1, string word2) &#123; int n =word1.size(), m =word2.size(); vector&lt;vector&lt;int&gt;&gt; dp(n+1, vector&lt;int&gt;(m+1, 0)); // 初始化 for(int i =0; i&lt;=m ; i++) dp[i][0] =i; for(int i =0; i&lt;=n ; i++) dp[0][i] =i; for(int i =1; i&lt;=n ; i++) for(int j =1; j&lt;=m; j++ ) &#123; if(str1[i-1] ==str2[j-1]) dp[i][j] =dp[i-1][j-1] ; else dp[i][j] =min(dp[i-1][j-1], min(dp[i][j-1], dp[i-1][j])) +1; &#125; return dp[n][m];&#125; python 实现 12345678910111213141516def edit_distance(str1, str2): n, m =len(str1), len(str2) dp =[ [0] *(m+1) for _ in range(n+1)] # 初始化 for i in range(n+1): dp[i][0] =i for i in range(m +1): dp[0][i] =i for i in range(1, n+1): for j in range(1, m+1): if str1[i-1] ==str2[j-1]: dp[i][j] =dp[i-1][j-1] else: dp[i][j] =min(dp[i-1][j-1], dp[i-1][j], dp[i][j-1]) +1 return dp[n][m]str1 ="horse"str2 ="ros"print(edit_distance(str1, str2)) Interval DP (间隔或区间dp) Problem: given a string $x = x_{1…n} $, find the minimum number of characters that need to be inserted to make it a palindrome 解题思路 问题的定义Let $D_{ij}$ be the minimum number of characters that need to be inserted to make $x_{i…j}$ into a palindrome 转换关系$$ dp[i][j] = \begin{cases}1 + min(dp[i+1][j], dp[i][j-1]) &amp; x_i \neq x_j \\dp[i+1][j-1]&amp; x_i \eq x_j\end{cases}$$ 初始化 Find and solve the base cases: $D_{ii} = D_i, i−1 = 0 $ for all $ i$The entries of D must be filled in increasing order of $j − i$ An alternate solution Reverse $x $to get $x^R$ The answer is $n−L $, where $ L $is the length of the LCS of $ x$ and $x^R$ 填充表格的过程是按照主对角线进行填充的，最后填充的点是 dp[0][n-1]。第一层循环是一个gap 的循环，是按照主对角线进行遍历的。\\ 这样遍历，非常nice 的代码。 代码实现部分（按照道理是只需要初始化主对角线就行，但是下面的代码把所有的值初始化为0） c++ 代码实现部分 1234567891011121314151617181920212223#include&lt;iostream&gt;#include&lt;string&gt;#include&lt;vector&gt;using namespace std;int main()&#123; int n ; string str1; cin &gt;&gt;n; cin &gt;&gt; str1; vector&lt;vector&lt;int&gt;&gt; dp(n, vector&lt;int&gt;(n, 0)); for(int gap =1; gap&lt;n; gap++) // 这个是一种全新的遍历方式 for(int i =0, j =gap +i; j&lt;n; i++, j++) &#123; if(str1[i] ==str1[j]) dp[i][j]= dp[i+1][j-1]; else dp[i][j] =min(dp[i+1][j], dp[i][j-1] ) +1; &#125; cout &lt;&lt; dp[0][n-1]&lt;&lt; endl; return 0;&#125; python 实现 1234567891011121314def dp(str1): n =len(str1) # 初始化 dp =[[0]*n for _ in range(n)] for gap in range(1, n): i =0 j =i +gap while j&lt;n: dp[i][j] = dp[i+1][j-1] if str1[i] ==str1[j] else min(dp[i+1][j], dp[i][j-1]) +1 i += 1 j += 1 return dp[0][n-1]str1 ="abcd"print(dp(str1)) The Knapsack Problem 背包问题也是可以纳入到 dp系列中In the knapsack problem we are given a set of n items, where each item i is specified by a size si and a value vi. We are also given a size bound S (the size of our knapsack). The goal is to find the subset of items of maximum total value such that sum of their sizes is at most S (they all fit into the knapsack). matrix product parenthesization problem.参考文献standford Dynamic Programmingdp-cmu 课件Demystifying Dynamic ProgrammingThe Ultimate Guide to Dynamic ProgrammingDynamic programming [step-by-step example]Introduction to Dynamic Programming 1Tutorial For Dynamic Programming 之前的动态规划的求解需要考虑三个问题： 状态怎么表示 怎么计算每个状态 怎么初始化 Unique Paths II 原题连接 对于动态规划，需要考虑三个问题： 状态怎么表示： f[i][j] 表示是(i, j) 这点的方案数 怎么计算下一步： f[i][j] 只有两个来源，一种是从上面走下来，一种是从左边走下来 怎么初始化： dp很重要的一点： 使用到的状态之前都已经计算过。针对这道题目，求解方案数，有可能最后的结果超过了int 的范围，所以使用long long应该是比较好的。 12345678910111213141516171819202122232425262728293031323334class Solution &#123;public: // f[i][j] 表示(i, j) 这个点路径的个数 // 转移 f[i][j] =f[i-1][j] or f[i][j-1] 这样的方式 // 初始化, 0 if nums[i][j] ==1 , else 1 int uniquePathsWithObstacles(vector&lt;vector&lt;int&gt;&gt;&amp; nums) &#123; if(!nums.size() || !nums[0].size()) return 0; int n =nums.size(), m =nums[0].size(); if (nums[0][0]) return 0; vector&lt;vector&lt;long long&gt;&gt; f(n, vector&lt;long long&gt;(m)); // 默认初始为0， 因为这个是在 public 中 f[0][0] =1;// 这个是从左上角开始的, 因为上文已经判断case，所以可以直接进行初始化 for(int i =0; i&lt; n; i++) &#123; for(int j =0; j&lt;m; j++) &#123; if(i || j) f[i][j] =0;// 只有在 非 if(!nums[i][j]) &#123; if(i) f[i][j] += f[i-1][j] ; if(j) f[i][j] += f[i][j-1]; &#125; &#125; &#125; return f[n-1][m-1]; &#125;&#125;; triangle LeetCode 版本 12345678910111213141516171819202122class Solution &#123;public: // 实现的时候，如何进行遍历是，我的弱点。 // 这种三角形在(i, j) 上是有特点的，因为类似一种直角三角形 int minimumTotal(vector&lt;vector&lt;int&gt;&gt;&amp; triangle) &#123; int n =triangle.size(); // 滚动数组 vector&lt;int&gt; f(triangle[n-1].begin(), triangle[n-1].end()), g(n);// f()是拷贝， g是初始化 // cpp 语法，这里是没有 triangle[-1] 表示的，不是python 中的 for(int i =n-2; i&gt;=0; i--) &#123; for(int j =0; j&lt;=i; j++) // 直角三角形的特点 &#123; g[j] =min(f[j], f[j+1]) +triangle[i][j]; &#125; f =g; &#125; return f[0]; &#125;&#125;; 单机版 1234567891011121314151617181920212223242526272829#include&lt;iostream&gt;#include&lt;vector&gt;using namespace std;int main()&#123; int n; cin &gt;&gt;n; vector&lt;vector&lt;int&gt;&gt; arr(n, vector&lt;int&gt;(n)); for(int i =0;i&lt;n; i++) for(int j =0; j&lt;=i; j++) cin&gt;&gt; arr[i][j]; vector&lt;int&gt; f(arr[n-1].begin(), arr[n-1].end()), g(n);// 如何去理解 g(n) 作为一种备胎，滚动数组 for(int i =n-2; i&gt;=0; i--) &#123; for(int j =0; j&lt;=i; j++) &#123; g[j] =min(f[j], f[j+1])+ arr[i][j]; // 这步骤很重要 &#125; f =g; &#125; cout&lt;&lt; f[0] &lt;&lt;endl; return 0;&#125; 354. Russian Doll Envelopes 原题链接 一共有两种解法。先进行排序，然后转换成最长连续递增子序列,。时间复杂度分析： 排序最快是 $nlogn$， 如果使用dp，那么总的是$n^2$; 如果使用二分总的是$nlogn$。下面是第一种解法，使用dp 的思想。 1234567891011121314151617181920212223242526class Solution &#123;public: int maxEnvelopes(vector&lt;vector&lt;int&gt;&gt;&amp; envelopes) &#123; int n =envelopes.size(); vector&lt;int&gt; dp(n,0); dp[0] =1; int result =1; sort(envelopes.begin(), envelopes.end()); for(int i =0; i&lt;n; i++) &#123; for(int j =0;j&lt;i ;j++)// 遍历前面所有的状态的 &#123; if(envelopes[i][0]&gt; envelopes[j][0] &amp;&amp; envelopes[i][1]&gt; envelopes[j][1] ) dp[i] =max(dp[i], dp[j]+1); &#125; result =max(result, dp[i]); &#125; return result; &#125;&#125;; 单机版 1234567891011121314151617181920212223242526272829303132333435363738394041424344#include&lt;iostream&gt;#include&lt;vector&gt;#include&lt;algorithm&gt;using namespace std;// sort 然后最长递增子序列吧// dp[i] 表示长度为i 的最长递增, 转移方程 dp[i] =max(dp[i], dp[j]+1) j&lt;i, 初始化 dp[0] =0int maxEnvelopes(vector&lt;vector&lt;int&gt;&gt;&amp; arr)&#123; if(arr.empty()) return 0; int result =1; int n =arr.size(); vector&lt;int&gt; dp(n); sort(arr.begin(), arr.end());// 默认按照第一个元素进行递增排序 for(int i =0; i&lt;n ;i++) &#123; for(int j=0; j&lt;i; j++) &#123; dp[i] =max(dp[i], dp[j] +1); &#125; result =max(result, dp[i]); &#125; return result;&#125;int main()&#123; int n ; cin &gt;&gt;n; vector&lt;vector&lt;int&gt;&gt; arr(n, vector&lt;int&gt;(2)); for(int i =0; i&lt;n;i++) &#123; for(int j =0; j&lt;2;j++) cin &gt;&gt;arr[i][j]; &#125; cout &lt;&lt; maxEnvelopes(arr) &lt;&lt;endl; return 0;&#125; 第二种解法，使用二分的思想进行检索，总的时间复杂度能够达到 $O(nlog n)$.使用python 实现。 leetcode 版本 12345678910111213141516171819202122232425262728293031323334class Solution(object): def maxEnvelopes(self, envelopes): """ :type envelopes: List[List[int]] :rtype: int """ if not envelopes: return 0; envelopes.sort(key =lambda x:(x[0], -x[1])) # 按照第一个元素升序，然后第二个元素降序 # 二分查找 第二个元素的位置，当第一个元素相同的时候 #print(envelopes) lst =[] for fir, sec in envelopes: y =sec if lst ==[] or y&gt; lst[-1]: lst.append(y) else: # 二分查找y 合适的位置 left, right =0, len(lst) while left&lt;right: mid =(left +right)//2 if y&lt;= lst[mid]: right =mid else: left =mid +1 lst[left] =y return len(lst) 单机版本(c++ 中在main 或者 普通function 中对vector 申请空间之后，都是有默认的初始化的。比如对于 vector ，那么都是初始化成0)python 中sort() 函数中的自定义sort 是非常nice 的一定写法。 123456789101112131415161718192021222324252627282930313233343536373839404142434445# 先排序，然后就是 最长递增子序列的问题, 而该问题使用二分查找进行解决def maxEnvelopes(arr): if not arr: return 0 arr.sort(key =lambda x: (x[0], -x[1])) lst =[] #print(arr) for _, b in arr: import ipdb #ipdb.set_trace() y =b if lst==[] or y&gt; lst[-1]: lst.append(y) else: # 二分查找 left, right =0, len(lst)-1 while left &lt;right: mid =(left+ right)//2 if( y&lt; lst[mid]): right =mid else: left =mid+1 lst[left] =y return len(lst)if __name__ =="__main__": n =int(input()) arr =[[0]*2] *n for i in range(n): arr[i] =input().split(" ") arr[i] =[int(a) for a in arr[i]] print(arr) print(maxEnvelopes(arr)) 338. Counting Bits 原题 思想： 二进制的思想。使用dp 的时候，往往初始化 f[0] 然后在遍历循环的时候，就从i =1 的时候进行循环。 12345678910111213141516class Solution &#123;public: vector&lt;int&gt; countBits(int num) &#123; vector&lt;int&gt; f(num+1); f[0] =0; for(int i =1; i&lt;=num; i++) &#123; f[i] =f[i/2] + i&amp;1; // 表示i 的个位是不是1 &#125; return f; &#125;&#125;; 单机版本 12345678910111213141516171819202122232425#include&lt;iostream&gt;#include&lt;vector&gt;using namespace std;int main()&#123; int n; cin&gt;&gt; n; vector&lt;int&gt; f(n+1); f[0] =0; for(int i =1; i&lt;=n; i++) &#123; f[i] =f[i &gt;&gt;1] + i&amp;1;// 可以写得更加简单一点 &#125; for(auto u : f) cout &lt;&lt; u&lt;&lt; " "; cout&lt;&lt;endl; return 0;&#125; 329. Longest Increasing Path in a Matrix 原题 leetcode 版本 本题的特点，没有一个非常明确的转移顺序（枚举顺序）。因为是可以有四个方向进行选择的，这个枚举顺序是不好写的。所以可以学习以下下面的代码。写的比较nice。 这个题目很容易使用f[i][j] 表示点 (i,j) 位置的最长的路径，但是怎么转移？ 这个是没有固定的说是按照行枚举还是按照列进行枚举的。所以这个转移方程是不太好些的，所以这里用到一种方法，叫做记忆化搜索. 这种使用 dx, dy 的方式，然后得到新的坐标(a, b)。这种方式是比较通用的。 12345678910111213141516171819202122232425262728293031class Solution &#123;public: int n, m; vector&lt;vector&lt;int&gt;&gt; f, g; int dx[4] = &#123;-1, 0, 1, 0&#125;, dy[4] = &#123;0, 1, 0, -1&#125;; int dp(int x, int y) &#123; if (f[x][y] != -1) return f[x][y]; f[x][y] = 1; for (int i = 0; i &lt; 4; i ++ ) &#123; int a = x + dx[i], b = y + dy[i]; if (a &gt;= 0 &amp;&amp; a &lt; n &amp;&amp; b &gt;= 0 &amp;&amp; b &lt; m &amp;&amp; g[a][b] &lt; g[x][y]) f[x][y] = max(f[x][y], dp(a, b) + 1); &#125; return f[x][y]; &#125; int longestIncreasingPath(vector&lt;vector&lt;int&gt;&gt;&amp; matrix) &#123; if (matrix.empty()) return 0; g = matrix; n = g.size(), m = g[0].size(); f = vector&lt;vector&lt;int&gt;&gt;(n, vector&lt;int&gt;(m, -1)); int res = 0; for (int i = 0; i &lt; n; i ++ ) for (int j = 0; j &lt; m; j ++ ) res = max(res, dp(i, j)); return res; &#125;&#125;; 单机版 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748#include&lt;iostream&gt;#include&lt;vector&gt;using namespace std;int n, m;vector&lt;vector&lt;int&gt;&gt; f, arr;int dx[4] =&#123;-1, 0, 1, 0&#125;, dy[4] =&#123;0, 1, 0, -1&#125;;int dp(int i, int j)&#123; if(f[i][j] != -1) return f[i][j]; f[i][j] =1; for(int k =0; k&lt;4 ;k++) &#123; int a = i+dx[k], b =j+dy[k]; if(a &gt;=0 &amp;&amp; a&lt; n &amp;&amp; b&gt;= 0 &amp;&amp; b&lt;m &amp;&amp; arr[a][b] &gt; arr[i][j]) f[i][j] =max(f[i][j], dp(a, b)+1); &#125; return f[i][j];&#125;int main()&#123; cin&gt;&gt; n&gt;&gt;m; arr =vector&lt;vector&lt;int&gt;&gt;(n, vector&lt;int&gt;(m)); for(int i =0; i&lt;n; i++) for(int j =0; j&lt;m ;j++) cin &gt;&gt; arr[i][j]; // 初始化 f =vector&lt;vector&lt;int&gt;&gt;(n, vector&lt;int&gt;(m, -1)); int res =0; for(int i =0; i&lt;n; i++) for(int j =0; j&lt;m; j++) &#123; res =max(res, dp(i, j)); &#125; cout&lt;&lt; res&lt;&lt;endl; return 0;&#125; Coin change 原题链接 使用硬币组成给定的钱数，硬币可以无限使用，钱的总数是一定的，要求尽可能使用少的硬币。 是一种完全背包问题，从小到大进行枚举。(多重背包问题是对于完全背包问题的一种约束，后者是物品可以无限的选，前者是对于物品的数量有限制。也就是说多重背包中，某件物品只能选择几个) f[i] 表示钱数是i 需要使用最少的硬币数量。 f[i] =min(f[i], f[i-c]+1) 分成选和不选两种方案。初始化 f[0] =0 （根据实际问题含义进行初始化）。背包问题是就是先枚举物品，然后再枚举体积（这里 是金钱）的做法。 LeetCode版本 123456789101112131415161718class Solution &#123;public: int coinChange(vector&lt;int&gt;&amp; coins, int amount) &#123; vector&lt;int&gt; f(amount+1, INT_MAX/2); f[0] =0; for(auto c : coins) for(int i =c ; i&lt;= amount; i++) &#123; f[i] =min(f[i], f[i-c]+1); &#125; if(f[amount] == INT_MAX/2) return -1; else return f[amount]; &#125;&#125;; 单机版 123456789101112131415161718192021222324252627282930#include&lt;iostream&gt;#include&lt;vector&gt;using namespace std;int main()&#123; int n; cin&gt;&gt;n; vector&lt;int&gt; coins(n); int amount ; cin &gt;&gt;amount; vector&lt;int&gt; f(amount+1, INT_MAX/2); // 这个在cpp 中的常数还是经常使用的哦 f[0] =0; for(int i =0; i&lt;n ; i++) &#123; int c; cin &gt;&gt;c; for(int j =c; j&lt;= amount; j++) f[j] =min(f[j], f[j-c]+1); &#125; if(f[amount] ==INT_MAX/2) cout &lt;&lt; -1 &lt;&lt;endl; else cout &lt;&lt; f[amount] &lt;&lt;endl; return 0;&#125; Maximal Square 题目： 最大的包含全部都是1 的正方形。f[i][j] 表示右下角位置是 (i, j) 的正方形的边长。这个转移和上面increasing path 是一样的，都是没有固定的枚举方式，所以使用 dx dy 得到下一个新的坐标。根据上方，左边和左上角进行求解最小的边长。 原题链接LeetCode版本 1234567891011121314151617181920212223242526272829303132333435363738394041class Solution &#123;public: // 如何表示，dp[i][j] 表示以位置(i,j)为右下角的正方形的全部都是1的面积。 // 如何计算 dp[i][j] =min(dp[i-1][j-1], dp[i-1][j], dp[i][j-1]) 这个是转移方程 // 初始化, dp =&#123;0&#125; int maximalSquare(vector&lt;vector&lt;char&gt;&gt;&amp; matrix) &#123; if(!matrix.size() || !matrix[0].size()) return 0; int n =matrix.size(), m =matrix[0].size(); vector&lt;vector&lt;int&gt;&gt; dp(n, vector&lt;int&gt;(m, 0)); int res =0; for(int i =0; i&lt;n; i++) &#123; for(int j =0; j&lt;m; j++) &#123; //if (!i || !j) dp[i][j] =0; if(matrix[i][j] =='0') dp[i][j] =0; else &#123; dp[i][j] =1; if(i &gt;=1 &amp;&amp; j&gt;=1) &#123; // 那么这个就十分的通顺，计算的就是最小的边长 dp[i][j] += min(dp[i-1][j-1], min(dp[i-1][j], dp[i][j-1])) ; &#125; res =max(res, dp[i][j]); &#125; &#125; &#125; return res*res;// dp 得到的是边长，不是面积 &#125;&#125;; 单机版中array 的输入是 int 类型，所以在进行 array[i][j] ==’1’ 判断就是错误的， 因为 int 类型的 1 和 char 类型的 ‘1’ 这个是不一样的。所以在 c++ 或者 c 中一定要注意这个小的区别。 12345678910111213141516171819202122232425262728293031323334353637383940#include&lt;iostream&gt;#include&lt;algorithm&gt;#include&lt;vector&gt;using namespace std;int n, m;vector&lt;vector&lt;int&gt;&gt; arr, dp;int main()&#123; cin&gt;&gt;n&gt;&gt;m; // 需要申请空间的，即使是没有初始化成特定的值 /* arr =vector&lt;vector&lt;int&gt;&gt;(n, vector&lt;int&gt;(m)); for(int i =0; i&lt;n; i++) for(int j =0; j&lt;m; j++) cin&gt;&gt;arr[i][j]; */ arr =&#123;&#123;1, 0 , 1, 0, 0&#125;, &#123;1, 0, 1, 1, 1&#125;, &#123;1, 1, 1, 1,1&#125;, &#123;1, 0, 0, 1, 0&#125;&#125;; // 这种先是定义，然后再申请空间，也是昌吉nice的e dp =vector&lt;vector&lt;int&gt;&gt;(n, vector&lt;int&gt;(m, 0)); int res =0; for(int i =0;i&lt;n; i++) for(int j =0; j&lt;m; j++) &#123; if(arr[i][j] =='0') dp[i][j] =0; else &#123; dp[i][j] =1; // 这个条件很重要，因为当 arr[i][j] ==1 的时候，该位置就组成了dp[][] ==1 if(i&gt;=1 &amp;&amp; j&gt;=1) dp[i][j] += min(dp[i-1][j-1], min(dp[i][j-1],dp[i-1][j]) ); res =max(res, dp[i][j]); &#125; &#125; cout&lt;&lt; res*res &lt;&lt;endl; return 0;&#125; Out of boundary paths(到这里了) leetcode 版本 原题 f[i][j][k] 表示位置在(i, j) 并且还剩下k 步，走出边界的方案数。同样使用到了记忆化搜索，当计算过的时候，直接返回结果。 1234567891011121314151617181920212223242526272829303132333435class Solution &#123;public: // 这个应该是最为复杂的dp 了吧。dp[i][j][k] 表示 (i,j) 同时还有k 步的时候，走出边界的方案数 vector&lt;vector&lt;vector&lt;int&gt;&gt;&gt; f; int dx[4] =&#123;-1, 0, 1, 0&#125;, dy[4] =&#123;0, 1, 0, -1&#125;; int mod =1000000007 int findPaths(int m, int n, int N, int i, int j) &#123; f =vector&lt;vector&lt;vector&lt;int&gt;&gt;&gt;(m, vector&lt;vector&lt;int&gt;&gt;(n, vector&lt;int&gt;(N+1, -1))); return dp(m, n, N, i, j); &#125; int dp(int m, int n, int k, int x, int y) &#123; int &amp;v =f[x][y][k]; if(v !=-1) return v; v =0; if(!k) return v; for(int i =0; i&lt;4; i++) &#123; int a =x +dx[i], b = y+dy[i]; if(a &lt;0 || a ==m || b&lt;0 || b ==n) v++; else v += dp(m, n, k -1, a, b); v %= mod; &#125; return v; &#125;&#125;; 单机版1234567891011121314151617181920212223242526272829303132333435363738394041424344454647#include&lt;iostream&gt;#include&lt;vector&gt;using namespace std;// dp[i][j][k] 表示(i, j) 位置同时还剩下k 步，走出边界的方案书// 转移 dp[i][j][k] +=1 if 走出了边界 else += dp[a][b][k-1]// dp[i][j][k] =0vector&lt;vector&lt;vector&lt;int&gt;&gt;&gt; f;int MOD =1000000007;int dx[4] =&#123;-1, 0, 1, 0&#125;, dy[4] =&#123;0, 1, 0, -1&#125;;int dp(int m, int n, int k, int x, int y)&#123; int &amp;v =f[x][y][k]; // 记忆化搜索 if (v !=-1) return v; v =0; if (!k ) return v; // 枚举四个方向 for(int i =0; i&lt;4; i++) &#123; int a =x+dx[i], b =y+dy[i]; if(a &lt;0 || a==m || b&lt;0 || b ==n ) v ++; else v +=dp(m,n, k-1, a, b); v %=MOD; &#125; return v;&#125;int main()&#123; int m, n, N, i,j; // 简单的先通过case 再说 m=1, n =3, N =3, i =0, j=1; // 这个最后为什么手机 N+1, 测试一下边界 f= vector&lt;vector&lt;vector&lt;int&gt;&gt;&gt;(m, vector&lt;vector&lt;int&gt;&gt;(n, vector&lt;int&gt;(N+1, -1))); cout&lt;&lt; dp(m, n, N, i, j)&lt;&lt; endl; return 0;&#125; Decode Ways 存在一个英文字母到数字的映射表，给定数字，问有几种 decode 方式。 f[i] 表示前 i个数字一共的解码方式。可以分为两种情况，一种方案是当前的数字可以映射到一个字母，另一种方案是当前数字和上一个数字映射到一个字母。f[i] += f[i-1] （这种关系表示一种累加，继承的意思）。 （为什么分成了两种情况，映射表中是1 到26，数字使用两位就可以直接表示） 网站链接 LeetCode 版本 123456789101112131415161718192021222324252627282930313233class Solution &#123;public: // dp 的思路， dp[i] 表示前i 个字符串的总的方案书 // 转移 dp[i] += dp[i-1] if s[i-1] !='0' , += dp[i-2] if 10&lt;= int(s[i-2: i-1]) &lt;=26 // dp 一定是要进行初始化的 // 卧槽关于数据类型，这个是第二遍了吧， 0 和 '0' 不是一个数据类型 int numDecodings(string s) &#123; int n =s.size(); vector&lt;int&gt; f(n+1); s =' '+s; f[0] =1; for(int i =1; i&lt;=n ;i++) &#123; f[i]=0; if(s[i] !='0') f[i] +=f[i-1]; if(i &gt;1) &#123; int t =(s[i-1] -'0')*10 +s[i] -'0'; if (t &gt;=10 &amp;&amp; t&lt;= 26) f[i] += f[i-2]; &#125; &#125; //for(auto u : f) cout&lt;&lt; u&lt;&lt; " "; return f[n]; &#125;&#125;; 单机版 123456789101112131415161718192021222324252627282930313233343536373839#include&lt;iostream&gt;#include&lt;vector&gt;#include&lt;string&gt;using namespace std;/** 首先是 f[i] 表示前 i 个可以解码的总数 转移方程 f[i] += f[i-1] if s[i-1] !='0' f[i] += f[i-2] if 10&lt;=int(s[i-2,i])&lt;=26 初始化 f[i] =0 &amp;&amp; f[0] =1 **/int main()&#123; string s; cin&gt;&gt; s; int n =s.size(); vector&lt;int&gt; f(n+1); // 边界问题 s = ' '+s; f[0] =1; for(int i =1 ; i&lt;=n; i++) &#123; f[i] =0; if(s[i] !='0') f[i] += f[i-1]; if(i&gt;1) &#123; int t =(s[i-1]-'0')*10 + s[i] -'0'; if( t&lt;=26 &amp;&amp; t&gt;= 10) f[i] += f[i-2]; &#125; &#125; cout&lt;&lt; f[n]&lt;&lt;endl; return 0;&#125; ugly number leetcode 版本原题链接 vector.back() 访问返回最后一个元素。 123456789101112131415161718192021222324class Solution &#123;public: // 使用三个指针，之前使用python 使用过类似的算法才对 int nthUglyNumber(int n) &#123; vector&lt;int&gt; q; q.push_back(1); int i =0, j=0,k =0; while( --n) &#123; int t =min(q[i]*2, min(q[j]*3, q[k]*5)); q.push_back(t); // 这个是并列的if if( t == q[i]*2) i++; if(t ==q[j] *3) j++; if(t == q[k] *5) k++; &#125; return q.back(); &#125;&#125;; 单机版 123456789101112131415161718192021222324252627282930313233343536#include&lt;iostream&gt;#include&lt;vector&gt;#include&lt;algorithm&gt;using namespace std;int main()&#123; int n; cin &gt;&gt;n; //vector&lt;int&gt; arr(n+1); vector&lt;int&gt; q; //for(int i =0; i&lt;n; i++) cin &gt;&gt;arr[i]; q.push_back(1); int i =0,j =0, k =0; while( --n) &#123; int t =min(q[i] *2, min(q[j]*3, q[k]*5)); q.push_back(t); if(q[i]*2 == t) i++; if(q[j] *3 ==t) j++; if(q[k] *5 ==t) k++; &#125; cout&lt;&lt; q.back()&lt;&lt;endl; // vector.back() 是访问最后一个数字 // vector.front() 是访问第一个数字 return 0;&#125; Distinct Subsequences LeetCode 版本链接 思路： 使用dp。含义：f[i][j] 表示source 为i ，dest 为j 的情况下，最大的方案数转换： f[i][ j] = f[i-1][j] 不管什么情况下，对于source 中的i 都可以不选f[i][j] 对于 s[i] ==d[j] 的情况下， f[i][j] += f[i-1][j-1] ， 当两个相同的时候，这个是可以选择的。（不选就没有累加的方案数，选择就是累加的方案数） 子序列和子串是不同的概念， 子序列要求的不连续的index，子串要求的是连续的index。最后求的是s 能够拼凑成 t的方案数。 1234567891011121314151617181920212223class Solution &#123;public: // f[i][j] = f[i-1][j] 或者 =f[i-1][j-1] if(s[i] ==t[j]) long long numDistinct(string s, string t) &#123; int m =s.size(), n =t.size(); vector&lt;vector&lt;long long&gt;&gt; f(m+1, vector&lt;long long&gt;(n+1)); // 初始化 for(int i =0; i&lt;=m; i++) f[i][0] =1; for(int i =1; i&lt;=m; i++) for(int j =1; j&lt;=n ; j++) &#123; f[i][j] = f[i-1][j]; if(s[i-1] ==t[j-1]) f[i][j]+= f[i-1][j-1];// 这个累加的是不选的方案，然后如果相同，那么就再累加一下。 &#125; return f[m][n]; &#125;&#125;; 单机版 12345678910111213141516171819202122232425262728293031#include&lt;iostream&gt;#include&lt;vector&gt;#include&lt;string&gt;using namespace std;int main()&#123; string S; string T; cin&gt;&gt; S; cin&gt;&gt; T; int n =S.size(), m =T.size(); vector&lt;vector&lt;long long&gt;&gt; f(n+1, vector&lt;long long&gt;(m+1)); for(int i =0;i&lt;n; i++) f[i][0] =1; for(int i =1; i&lt;=n; i++) for(int j =1; j&lt;=m ; j++) &#123; f[i][j] = f[i-1][j]; if(S[i-1] == T[j-1]) f[i][j] += f[i-1][j-1]; &#125; cout&lt;&lt; f[n][m] &lt;&lt;endl; return 0; &#125; Palindrome Partitioning II 原题链接 这道题使用两次 dp。 第一次： dp[i:j]方便的判断任意两个区间组成的字符串是否是回文数。第二次： dp[i] 前i 个字符串中最少有多少个回文数(题目要求是最少的 partition的数量) 两个for 循环，保证了每次枚举的是 i( 1-n) 的left and right，二部仅仅是 1-n 的left and right。 对于dp 的debug，一般先从逻辑上（看代码）进行找错；实在不行才是每行输出。因为dp 当数据量比较大的时候，这个是不容易输出的。 leetCode-132 95:17 讲解连接 https://www.bilibili.com/video/av35161871/?p=1 123456789101112131415161718192021222324252627282930313233343536373839class Solution &#123;public: // f[i] 表示前i 个字母是多少个回文串， 数量的回文串 -1 就是 需要多少 刀 // 分成多少个 palindrome partitioning， // 使用动规 进行初始化，然后 int minCut(string s) &#123; int n =s.size(); vector&lt;vector&lt;bool&gt;&gt; c(n, vector&lt;bool&gt;(n, false)); // 这个c 表示从 [j][i] 这个子串是不是回文串, 这个操作很神奇 for(int i =1;i&lt;=n ; i++) for(int j =0; j+i-1 &lt;n; j++) &#123; int l =j, r =j+i-1; c[l][r] = s[l] ==s[r] &amp;&amp; (l+1 &gt; r-1 || c[l+1][r-1] ); &#125; for(auto u : c) &#123; for(auto v : u) cout &lt;&lt; v&lt;&lt; " "; cout &lt;&lt;endl; &#125; vector&lt;int&gt; f(n+1); f[0] =0; for(int i =1; i&lt;=n ;i++) &#123; f[i] =INT_MAX; for(int j =1; j&lt;=i; j++) if(c[j-1][i-1]) // 为甚这个是回文串，之后，然后才能+1 f[i] =min(f[i], f[j-1]+1); &#125; return f[n]-1; &#125; &#125;; 这个是单机版本 12345678910111213141516171819202122232425262728293031323334353637383940#include&lt;iostream&gt;#include&lt;vector&gt;#include&lt;string&gt;using namespace std;int main()&#123; string s; cin &gt;&gt;s; int n =s.size(); vector&lt;vector&lt;bool&gt;&gt; c(n, vector&lt;bool&gt;(n, false)); for(int i =1; i&lt;=n; i++) &#123; for(int j =0; j+i-1&lt;n; j++) &#123; int l =j, r =j+i -1; c[l][r] = s[l] ==s[r] &amp;&amp;(l+1 &gt; r-1 || c[l+1][r-1]); &#125; &#125; vector&lt;int&gt; f(n+1); f[0]= 0; for(int i =1; i&lt;=n; i++) &#123; f[i] =INT_MAX; for(int j =1; j&lt;=i; j++) if(c[j-1][i-1]) f[i] =min(f[i], f[j-1] +1); &#125; cout&lt;&lt; f[n]-1&lt;&lt;endl; return 0;&#125; Longest Common Subsequence 最长公共子序列。dp 思想，注意边界处理。时间复杂度是$O(mn)$，空间复杂度是$O(mn)$，其中$m$, $n$ 分别是字符串1和字符串2的长度。. 1234567891011121314151617class Solution &#123;public: int longestCommonSubsequence(string text1, string text2) &#123; int n =text1.size(), m =text2.size(); if(n ==0 || m ==0) return 0; vector&lt;vector&lt;int&gt;&gt; dp(n+1, vector&lt;int&gt;(m+1, 0)); for(int i =1; i&lt;= n; i++) for(int j =1; j&lt;= m; j++) if(text1[i-1] ==text2[j-1]) dp[i][j] =dp[i-1][j-1] +1; else dp[i][j] =max(dp[i-1][j], dp[i][j-1]); return dp[n][m]; &#125;&#125;; Longest Common Substring 时间复杂度 $O(mn)$，空间复杂度$O(mn)$ 其中$m$, $n$ 分别是字符串1和字符串2的长度。 1234567891011121314151617181920def longestSubstring(str1, str2): if not str1 or not str2: return 0 n, m =len(str1), len(str2) dp =[[0]*(m) for _ in range(n)] _max =0 for i in range( n): for j in range( m): if(str1[i] ==str2[j]): if i ==0 or j ==0: dp[i][j] =1 else: dp[i][j] =dp[i-1][j-1] +1 _max =max(_max, dp[i][j]) else: dp[i][j] =0 return _max str1 ="GeeksforGeeks"str2 ="GeeksQuiz"print(longestSubstring(str1, str2)) Longest Common Prefix 暴力枚举法，时间复杂度是$ O(min_{len} * n)$， 其中 $n $是字符串的个数 strs.size(). 并且其他的优化方法比如 trie 不见得能够降低时间复杂度。 在实现的时候，需要关注 str.size() 或者是 str.length() 都是需要转换成 int 然后再进行 min的计算。 1234567891011121314151617181920212223class Solution &#123;public: string longestCommonPrefix(vector&lt;string&gt;&amp; strs) &#123; int n =strs.size(); if(n ==0) return ""; int min_len= strs[0].length(); for(auto str: strs) min_len =min(min_len, int(str.size())); for(int i=1 ; i&lt;= min_len; i++) &#123; char ch =strs[0][i-1]; for(int j =1; j&lt; strs.size(); j++) &#123; if(strs[j][i-1] != ch) return strs[j].substr(0, i-1); &#125; &#125; return strs[0].substr(0, min_len); &#125;&#125;;]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>dp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Depth-first Search]]></title>
    <url>%2F2019%2F08%2F29%2Fdepth-first-search%2F</url>
    <content type="text"><![CDATA[深度优先搜索专题，题目来源于LeetCode。大多数题目使用c++ 实现，少量使用python 实现。 深度优先搜索 vs. 宽度优先搜索：dfs： 当数量非常庞大，但是解的数量非常少的时候，求解这一组解的时候，使用深搜来做。bfs 可以用来求解最短的距离，最小的数，使用一个队列进行维护 使用场景：当状态数量非常庞大的时候，解的个数非常少的时候，适合使用深度优先搜索。但搜索不等于深度优先搜索。搜索是一种算法，深搜是一种实现方式，可以使用循环来实现。 letter combinations of a phone number 原题链接其中的 state 就是最后的结果， now 是当前的结果，也是有点滚动数组的意思。 有点不断的更新前缀和后缀的意思。 c++ 语法，目前理解如果是常量string，那么使用 string dict[[] 这样形式，如果是可变的string 那么使用 vector 这种形式 1234567891011121314151617181920212223242526class Solution &#123;public: // 显示枚举数字， 然后枚举状态，再枚举字母 string dict[8] =&#123;"abc", "def", "ghi", "jkl", "mno", "pqrs", "tuv", "wxyz"&#125;; vector&lt;string&gt; letterCombinations(string digits) &#123; if (digits.empty()) return vector&lt;string&gt;(); vector&lt;string&gt; state(1, "");// 不必固定初始化，因为你也是不知道最后的size() for (auto digit: digits) &#123; vector&lt;string&gt; now; for(auto s : state) &#123; for(auto c: dict[digit-'2']) now.push_back(s+c); &#125; state=now; &#125; return state; &#125;&#125;; 单机版本123456789101112131415161718192021222324252627282930313233#include&lt;iostream&gt;#include&lt;vector&gt;#include&lt;string&gt;using namespace std;vector&lt;string&gt; dict =&#123;"abc", "def", "ghi", "jkl", "mno", "pqrs", "tuv", "wxyz"&#125;;// 深度优先搜索, 关键树顺序，按照digit 一个个进行枚举，每个digit 对应的字母都可以再次进行枚举int main()&#123; string digits; cin &gt;&gt; digits; vector&lt;string&gt; res(1, ""); //vector&lt;string&gt; now; for(auto digit: digits ) &#123; vector&lt;string&gt; now ; for(auto u: dict[digit-'2']) &#123; for(auto v : res) now.push_back(v+u); &#125; res =now; &#125; for(auto u: res) cout &lt;&lt;u&lt;&lt;" "; cout&lt;&lt;endl; return 0;&#125; Word Search 原题链接 dfs 的思路应用到该题目的做题思路： 枚举起点 从起点开始，依次搜索下一个点的位置 在枚举的过程中，需要实时判断（要保证和目标单词匹配） 时间复杂度分析$ n \times m \times 3^k$ 棋盘类的题目，一般都是需要用到搜索和动态规划。当数据比较小的时候使用搜索（暴搜），当数据比较大的时候，使用动态规划。 所谓回溯，就是需要恢复现场的。 LeetCode 版本 12345678910111213141516171819202122232425262728293031323334353637383940414243444546class Solution &#123;public: // 使用dfs 进行解决 int n_words, n, m; bool exist(vector&lt;vector&lt;char&gt;&gt;&amp; board, string word) &#123; n =board.size(); m =board[0].size(); n_words =word.size(); // 列举每个起点 bool flag =false; for(int i =0; i&lt;n ;i++) for(int j =0; j&lt;m ;j++) if( dfs(board, i, j, word, 0)) &#123; return true; &#125; return flag; &#125; // 表示 board[x][y] 和 str[u] 是否相同 // dfs 中的跳出的条件 int dx[4]=&#123;-1, 0, 1, 0&#125;, dy[4]=&#123;0, 1, 0, -1&#125;; bool dfs(vector&lt;vector&lt;char&gt;&gt;&amp; board, int x, int y, string&amp; str, int u) &#123; if(board[x][y] != str[u]) return false; if(u ==n_words-1) return true; board[x][y] ='.'; // 这个也是比较牛逼 for(int i =0; i&lt;4; i++) &#123; int a =x+dx[i], b =y +dy[i]; if(a &gt;=0 &amp;&amp; a&lt;n &amp;&amp; b&gt;= 0 &amp;&amp; b&lt;m) if(dfs(board, a, b, str, u+1)) return true; // 这个是如何去理解，如果下一个字节u 也能够走通，那么就可以返回 true &#125; board[x][y] =str[u]; return false; &#125;&#125;; 这个是单机版。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465#include&lt;iostream&gt;#include&lt;vector&gt;#include&lt;string&gt;using namespace std;#define LEN 10int n, m;int dx[4] =&#123;-1, 0, 1, 0&#125;, dy[4] =&#123;0, 1, 0, -1&#125;;// word 是一个string 类型，但是word.size() 依然是可以这样用的e// 注意这个指针的符号bool dfs(vector&lt;vector&lt;char&gt;&gt;&amp; board, int x, int y, string&amp; word, int u)&#123; //边界条件 if (board[x][y] != word[u]) return false; if(u ==word.size() -1) return true; board[x][y] ='.'; // 寻找下一步 for(int i =0; i&lt;4 ; i++) &#123; int a =x +dx[i], b =y+dy[i]; if(a &gt;=0 &amp;&amp; a&lt;n &amp;&amp; b&gt;=0 &amp;&amp; b&lt;m) if(dfs(board, a, b, word, u+1)) return true; &#125; board[x][y] =word[u]; return false; &#125;int main()&#123; cin &gt;&gt; n&gt;&gt;m; vector&lt;vector&lt;char&gt;&gt; board(n, vector&lt;char&gt;(m)); //vector&lt;string&gt; word(LEN); string word; for(int i =0; i&lt;n; i++) &#123; for(int j =0; j&lt;m; j++) cin &gt;&gt; board[i][j]; &#125; cin &gt;&gt; word; if(n ==0 || m ==0) return false; bool flag ; // 列举起点 for(int i=0; i&lt;n; i++) for(int j =0; j&lt;m; j++) &#123; if(dfs(board, i, j, word, 0 )) flag =true; &#125; flag =false; cout &lt;&lt; flag; return 0;&#125; leetcode 46: permutations 有两种思路： 枚举每个位置放哪个数字； 枚举每个数放哪些位置。 这道题目是枚举数字的思路求解，下一道题是枚举位置。 LeetCode版本： 123456789101112131415161718192021222324252627282930313233343536373839404142434445class Solution &#123;public: // 全排列的问题，有两种思路，一种是给定数字然后选择位置； 一种是给定位置选择数字。这里是给定数字然后选择位置 // 对于 dfs 在递归的过程中需要传递大量的变量，这个时候设置成全局变量更加合理 int n; vector&lt;int&gt; path; vector&lt;vector&lt;int&gt;&gt; ans; vector&lt;bool&gt; flag; vector&lt;vector&lt;int&gt;&gt; permute(vector&lt;int&gt;&amp; nums) &#123; if(nums.empty()) return vector&lt;vector&lt;int&gt;&gt;(); n =nums.size(); // 对于flag 的初始化 flag =vector&lt;bool&gt;(n); dfs(nums, 0); return ans; &#125; void dfs(vector&lt;int&gt;&amp; nums, int u) &#123; if(u ==n) &#123; ans.push_back(path); return; &#125; for(int i =0; i&lt;n; i++) &#123; if(!flag[i]) &#123; flag[i] =true; path.push_back(nums[i]); dfs(nums, u+1); // 是在原来的基础上，进行的 u+1 ， path.pop_back(); // 这个是回复现场的操作，所以path 中只会有一个解 flag[i] =false; &#125; &#125; &#125;&#125;; 个人单机版 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354#include&lt;iostream&gt;#include&lt;vector&gt;using namespace std;// permutations#define LEN 10int n;vector&lt;int&gt; path;vector&lt;vector&lt;int&gt;&gt; ans;vector&lt;bool&gt; flag;void dfs(vector&lt;int&gt;&amp; arr, int u)&#123; if( u==n) &#123; ans.push_back(path); return; &#125; for(int i =0; i&lt;n;i++) &#123; if(!flag[i]) &#123; flag[i] =true; path.push_back(arr[i]); dfs(arr, u+1); path.pop_back(); flag[i] =false; &#125; &#125;&#125;int main()&#123; vector&lt;int&gt; arr(LEN); cin &gt;&gt;n; for(int i =0; i&lt;n ;i++) cin&gt;&gt;arr[i]; flag =vector&lt;bool&gt;(n); dfs(arr, 0); for(auto u: ans) &#123; for(int i =0; i&lt; u.size(); i++) cout&lt;&lt; u[i] &lt;&lt;" "; cout &lt;&lt;endl; &#125; return 0;&#125; Permutations II LeetCode 版本 枚举位置。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647class Solution &#123;public: // 这个题目和上一个题目的不同点在于有了重复的数字， // 如果避免结果的重复呢？ 就是相同的数字的相对顺序保持不变，这样就不会发生结果的重复。 // 上一步基于一个简单的假设，相同的数字是在一块的，如何实现该假设，使用排序就可以轻松的实现 // 这个只是变量的定义，并没有开辟空间 // int n; vector&lt;vector&lt;int&gt;&gt; ans; vector&lt;int&gt; path; vector&lt;bool&gt; st; vector&lt;vector&lt;int&gt;&gt; permuteUnique(vector&lt;int&gt;&amp; nums) &#123; // 下面是开辟了空间 n =nums.size(); st =vector&lt;bool&gt;(n); path =vector&lt;int&gt;(n); sort(nums.begin(), nums.end()); // 这种是对于 nums 非常简单的实现了 dfs(nums, 0, 0); return ans; &#125; void dfs(vector&lt;int&gt; &amp;nums, int u, int start) &#123; if(u ==n) &#123; ans.push_back(path); return ; &#125; for(int i =start; i&lt;n; i++) &#123; if(!st[i]) &#123; st[i] =true; path[i] =nums[u]; // 枚举不同的位置 dfs(nums, u+1, u+1&lt; n &amp;&amp; nums[u+1] ==nums[u] ? i+1:0); st[i] =false; &#125; &#125; &#125; &#125;; 个人版123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960#include&lt;iostream&gt;#include&lt;algorithm&gt;#include&lt;cmath&gt;#include&lt;vector&gt;using namespace std;int n;vector&lt;vector&lt;int&gt;&gt; ans;vector&lt;int&gt; path;vector&lt;bool&gt; flag;void dfs(vector&lt;int&gt;&amp; arr, int u, int start )&#123; if(u ==n) &#123; ans.push_back(path); return; &#125; for(int i =start; i&lt;n; i++) &#123; if(! flag[i]) &#123; flag[i] =true; path[i] =arr[u]; dfs(arr, u+1, u+1&lt;n &amp;&amp; arr[u+1] ==arr[u] ? i+1: 0); flag[i] =false; &#125; &#125;&#125;int main()&#123; vector&lt;int&gt; arr; cin &gt;&gt;n; //初始化 path =vector&lt;int&gt;(n); flag =vector&lt;bool&gt;(n); arr =vector&lt;int&gt;(n); for(auto u: flag) cout&lt;&lt; u&lt;&lt;" "; for(int i =0;i&lt;n;i++) cin &gt;&gt; arr[i]; sort(arr.begin(), arr.end()); dfs(arr, 0, 0); for(auto u : ans) &#123; for(int i =0; i&lt;u.size(); i++) cout &lt;&lt;u[i] &lt;&lt;" "; cout &lt;&lt;endl; &#125; return 0;&#125; 上面是排列的问题，下面是组合的问题。 Subsets LeetCode 版本 123456789101112131415161718192021222324class Solution &#123;public: // 使用循环的方式解题：数字的二进制表示用来表示该位置是否出现过， // 数字i 的二进制表示中第 j位是否为1 : i&gt;&gt;j &amp;1 这样进行判断 vector&lt;vector&lt;int&gt;&gt; subsets(vector&lt;int&gt;&amp; nums) &#123; vector&lt;vector&lt;int&gt;&gt; res; //vector&lt;int&gt; now; for(int i =0; i&lt; 1&lt;&lt; nums.size(); i++) &#123; vector&lt;int&gt; now; for(int j =0; j&lt; nums.size(); j++) if(i &gt;&gt;j &amp;1) &#123; now.push_back(nums[j]); &#125; res.push_back(now); &#125; return res; &#125;&#125;; 单机版 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950#include&lt;iostream&gt;#include&lt;vector&gt;using namespace std;vector&lt;vector&lt;int&gt;&gt; fun(vector&lt;int&gt;&amp; arr)&#123; cout&lt;&lt; "he"&lt;&lt;endl; vector&lt;vector&lt;int&gt;&gt; res; int n =arr.size(); for(int i =0; i&lt; 1&lt;&lt;n; i++) &#123; vector&lt;int&gt; path; for(int j =0; j&lt;n; j++) &#123; if(i &gt;&gt;j &amp;1) path.push_back(arr[j]); &#125; res.push_back(path); &#125; return res; &#125;int main()&#123; int n; cin&gt;&gt;n; vector&lt;int&gt; arr(n); for(int i =0;i&lt;n;i++) cin&gt;&gt;arr[i]; for(int i =0;i&lt;n;i++) cout&lt;&lt; arr[i] &lt;&lt;" "; //fun(arr); vector&lt;vector&lt;int&gt;&gt; res =fun(arr); for(auto u : res) &#123; for(int i =0;i&lt; u.size(); i++) cout &lt;&lt; u[i] &lt;&lt;" "; cout &lt;&lt;endl; &#125; return 0;&#125; 90. Subsets II这个是上一道题目的扩展https://leetcode.com/problems/subsets-ii/ LeetCode版本12345678910111213141516171819202122232425262728293031323334353637class Solution &#123;public: vector&lt;vector&lt;int&gt;&gt; ans; vector&lt;int&gt; path; // 为了保证不产生重复的，先是进行排序，然后对于相同的数字，枚举的个数是和数字的次数相关 vector&lt;vector&lt;int&gt;&gt; subsetsWithDup(vector&lt;int&gt;&amp; nums) &#123; sort(nums.begin(), nums.end()); dfs(nums, 0); return ans; &#125; void dfs(vector&lt;int&gt;&amp; nums, int u) &#123; if( u ==nums.size()) &#123; ans.push_back(path); return; &#125; // u表示当前的数字，然后k 表示当前数字重复的个数 int k =0; while( u+k &lt;nums.size() &amp;&amp; nums[u+k] == nums[u]) k++; // 重复的数字可以选择 0~k 个这样的数字 for(int i =0; i&lt;=k ;i++) &#123; dfs(nums, u+k); path.push_back(nums[u]); &#125; for(int i =0; i&lt;=k ;i++) path.pop_back(); &#125; &#125;; 单机版123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566#include&lt;iostream&gt;#include&lt;algorithm&gt;#include&lt;vector&gt;using namespace std;#define LEN 10vector&lt;vector&lt;int&gt;&gt; res;vector&lt;int&gt; path;void dfs(vector&lt;int&gt;&amp; arr, int u)&#123; if(u == arr.size()) &#123; res.push_back(path); return; &#125; // 数字出现的k 次数 int k =0; while (k+u&lt; arr.size() &amp;&amp; arr[k+u] ==arr[u]) &#123; k ++; &#125; for(int i=0; i&lt;=k; i++) &#123; dfs(arr, u+k); path.push_back(arr[u]); &#125; // 回溯 for(int i =0; i&lt;=k ; i++) &#123; path.pop_back(); &#125;&#125;int main()&#123; int n; //vector&lt;int&gt; arr(LEN); cin&gt;&gt;n; vector&lt;int&gt; arr(n); for(int i =0; i&lt;n; i++) cin&gt;&gt;arr[i]; //for(int i =0; i&lt;n; i++) cout &lt;&lt; arr[i]&lt;&lt; " "; sort(arr.begin(), arr.end()); dfs(arr, 0); for(auto u: res) &#123; for(int i =0; i&lt;u.size(); i++) &#123; cout &lt;&lt;arr[i]&lt;&lt; " "; &#125; cout&lt;&lt;endl; &#125; return 0;&#125; 216. Combination Sum III 原题链接 Leetcode 版本12345678910111213141516171819202122232425262728class Solution &#123;public: //dfs(枚举数字的个数, 当前枚举的数字, 数字的总和) vector&lt;vector&lt;int&gt;&gt; ans; vector&lt;int&gt; path; vector&lt;vector&lt;int&gt;&gt; combinationSum3(int k, int n) &#123; dfs(k, 1, n); return ans; &#125; void dfs(int k, int start, int n) &#123; if(!k) &#123; if(!n) ans.push_back(path); return ; &#125; for(int i =start; i&lt;=9; i++) &#123; path.push_back(i);// 这个还是本程序的步骤 dfs(k -1, i +1, n-i);// 这个是状态的转移，是下一个步骤 path.pop_back(); &#125; &#125; &#125;; 单机版1234567891011121314151617181920212223242526272829303132333435363738394041424344#include&lt;iostream&gt;#include&lt;vector&gt;#include&lt;algorithm&gt;using namespace std;vector&lt;vector&lt;int&gt;&gt; ans;vector&lt;int&gt; path;void dfs(int k, int start, int n)&#123; if(!k) &#123; if(!n) ans.push_back(path); return; &#125; for(int i =start; i&lt;=9; i++) &#123; path.push_back(i); dfs(k -1, i+1, n-i); path.pop_back(); &#125;&#125;int main()&#123; int k,n; cin &gt;&gt;k&gt;&gt;n; dfs(k, 1, n); for(auto u: ans) &#123; for(int i=0;i&lt;u.size(); i++) cout&lt;&lt; u[i]&lt;&lt;" "; cout &lt;&lt;endl; &#125; return 0;&#125; 52. N-Queens II 视频讲解 关键是按照行进行dfs(), 然后使用 col 对列进行判断，使用 d,nd 对于对角线和斜对角线进行判断。 针对对角线和斜对角线， 是 x+y 和 x-y+n 这样的形式进行判断的。 12345678910111213141516171819202122232425262728293031323334353637383940class Solution &#123;public: int ans =0, n; vector&lt;bool&gt; col, d, nd; int totalNQueens(int _n) &#123; n =_n; // 初始化的时候，使用这样的语句 col =vector&lt;bool&gt;(n); d =nd =vector&lt;bool&gt;(n*2); dfs(0); return ans; &#125; void dfs(int row) &#123; if(row ==n) &#123; ans +=1; return ; &#125; for(int i =0; i&lt; n; i++) &#123; if(!col[i] &amp;&amp; !d[row+i] &amp;&amp; !nd[row-i +n]) &#123; col[i] =d[row+i] =nd[row-i +n] =true; dfs(row+1); col[i] =d[row+i] =nd[row-i +n] =false; &#125; &#125; &#125;&#125;; 单机版12345678910111213141516171819202122232425262728293031323334353637383940414243#include&lt;iostream&gt;#include&lt;vector&gt;using namespace std;int ans =0, n;vector&lt;bool&gt; col, d, nd;void dfs(int row)&#123; if(row ==n) &#123; ans ++; return; &#125; for(int i =0; i&lt; n; i++) &#123; if( !col[i] &amp;&amp; !d[row+i] &amp;&amp; !nd[row-i+n]) &#123; col[i] =d[row+i] =nd[row-i+n] =true; dfs(row+1); col[i] =d[row+i]=nd[row-i+n] =false; &#125; &#125; &#125;int main()&#123; cin &gt;&gt;n; col =vector&lt;bool&gt;(n); d =nd =vector&lt;bool&gt;(2*n); dfs(0); cout &lt;&lt; ans&lt;&lt;endl; return 0;&#125; leetcode 讲解版473. Matchsticks to Square 原题连接 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556class Solution &#123;public: /** (nums, 当前的第几条边，当期的边的长度，边的总长度) 剪枝： 1. 从大到小枚举（先是枚举分支少的，因为这样，如果发生了剪枝，那么效果更加明显） 2. 如果dfs() 失败，且当前边是第一条，那么剪枝 3. 如果dfs() 失败，且当期边是最后一天，那么剪枝 4. 如果当前的边失败，那么之后和其相同的边也是会失败的 */ // 剪枝是在内存和时间上的优化，即使没有剪枝，给了足够的时间和空间，那么最后也是能够得到结果、 vector&lt;bool&gt; st; bool makesquare(vector&lt;int&gt;&amp; nums) &#123; int length =0; for(auto u : nums) length += u; if( !length || length %4) return false; // 排序 这是剪枝的操作 ，时间的上的操作 sort(nums.begin(), nums.end()); reverse(nums.begin(), nums.end()); st= vector&lt;bool&gt;(nums.size()); return dfs(nums, 0, 0, length/4); &#125; bool dfs(vector&lt;int&gt;&amp; nums, int u, int cur, int sum) &#123; if(cur ==sum) cur =0, u+=1; if( u ==4) return true; for(int i =0; i&lt; nums.size(); i++) if(!st[i] &amp;&amp; cur+ nums[i] &lt;= sum) &#123; st[i] =true; // 这个判断条件是比较清奇的 if(dfs(nums, u, cur+nums[i], sum)) return true; st[i] =false; //开始进行剪枝 if(!cur) return false; if( cur+nums[i] ==sum ) return false; while(i+1 &lt;= nums.size() &amp;&amp; nums[i] ==nums[i+1]) i+=1; &#125; return false; &#125;&#125;; 单机版 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253#include&lt;iostream&gt;#include&lt;vector&gt;#include&lt;algorithm&gt;using namespace std;vector&lt;bool&gt; st;bool dfs(vector&lt;int&gt;&amp; arr, int u, int cur, int sum)&#123; if(cur ==sum ) cur =0, u +=1; if( u==4) return true; for(int i =0; i&lt; arr.size(); i++) if(!st[i] &amp;&amp; cur+arr[i] &lt;= sum) &#123; st[i] =true; if(dfs(arr, u, cur+ arr[i], sum )) return true; st[i] =false; //开始进行剪枝 if(!cur) return false; if(arr[i] +cur ==sum ) return false; // 如何去表示一条边的最后一个 while(i+1 &lt;= arr.size() &amp;&amp; arr[i+1] ==arr[i]) i++; &#125; return false;&#125;int main()&#123; int n; cin&gt;&gt;n; vector&lt;int&gt; arr(n); for(int i =0;i&lt;n; i++) cin&gt;&gt;arr[i]; int sum =0; for(auto u : arr) sum +=u; st =vector&lt;bool&gt;(n); if(!sum || sum %4) cout&lt;&lt; "false"&lt;&lt;endl; // 第一个剪枝操作，从大到小枚举 sort(arr.begin(), arr.end()); reverse(arr.begin(), arr.end()); bool flag =dfs(arr, 0, 0, sum/4); cout &lt;&lt; flag&lt;&lt;endl; return 0;&#125; 这个是很简单的操作，注意看注释，好好理解一下。 Word Search 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061class Solution &#123;public: // 时间复杂度分析， O(m *n *dfs()), 最大是 3^k ,k 表示平均的字符串长度 // dfs 一般是在很多组解中，找到少量的 或者说一个解， // bfs 一般是求解最小值 的问题，因为这个是没有办法得到所有的状态表示 // 当数据范围比较小的时候，用搜索，当数据范围比较大的时候，用动态规划 // 回溯的意义在于每一次重新开始（起点）的时候，周围的环境都是一样的呀 // 传引用就是传 指针，这个是很快的操作，而不用复制整个arr int n, m; int dx[4] =&#123;-1, 0, 1, 0&#125;, dy[4] =&#123;0, 1, 0, -1&#125;; vector&lt;vector&lt;bool&gt;&gt; st; bool exist(vector&lt;vector&lt;char&gt;&gt;&amp; board, string word) &#123; if(board.empty() || board[0].empty()) return false; n =board.size(); m =board[0].size(); st =vector&lt;vector&lt;bool&gt;&gt;(n , vector&lt;bool&gt;(m, false)); for(int i =0; i&lt;n ; i++) &#123; for(int j =0; j&lt;m; j++) &#123; bool tmp =dfs(board, i, j, word, 0); cout&lt;&lt; tmp&lt;&lt;endl; if(tmp) return true; &#125; &#125; return false; &#125; bool dfs(vector&lt;vector&lt;char&gt;&gt; &amp; board, int a, int b, string &amp; word, int u) &#123; if(board[a][b] != word[u]) return false; if(u ==word.size()-1) return true; //st[a][b] =true; // 这个就是标记一下而已 char t = board[a][b]; board[a][b] ='*'; for(int i =0; i&lt;4 ; i++) &#123; int x =a +dx[i], y =b +dy[i]; if(x &gt;= 0 &amp;&amp; x&lt;n &amp;&amp; y&gt;= 0 &amp;&amp; y&lt;m) if(dfs(board, x, y, word, u+1)) return true; &#125; //st[a][b] =false; board[a][b] =t; return false; &#125; &#125;; permutations 这个问题就是一种全排列的问题 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667class Solution &#123;public: // dfs ，枚举位置 和枚举数字，两种方式，注意这道题是比较好做在于是 distinct 的数字 /* vector&lt;vector&lt;int&gt;&gt; permute(vector&lt;int&gt;&amp; nums) &#123; dfs(nums, 0); return res; &#125; void dfs(vector&lt;int&gt;&amp; nums, int u) &#123; if( u ==nums.size() -1) &#123; tmp.push_back(nums[u]); res.push_back(tmp); tmp.clear(); return; &#125; tmp.push_back(nums[u]); if(u +1 &lt; nums.size()) dfs(nums, u+1); &#125; */ // 对于时间复杂度的分析，用到递归树， 只需要看最后的叶子结点 n!(这个可以从解的个数上来进行理解， 比如说有 3个数字，最后产生了6组解，所以是 n!,) // 一组解 所需要的时间是O(n)， 所以总的时间复杂度是 O(n * n!) 这个样子 vector&lt;bool&gt; st; vector&lt;vector&lt;int&gt;&gt; ans; vector&lt;int&gt; path; vector&lt;vector&lt;int&gt;&gt; permute(vector&lt;int&gt; &amp; nums) &#123; st =vector&lt;bool&gt;(nums.size()); dfs(nums, 0); return ans; &#125; void dfs(vector&lt;int&gt; &amp;nums, int u) &#123; if( u == nums.size()) &#123; // 这里是比较简洁的，不需要再次进行添加到path 里面了 ans.push_back(path); return ; &#125; // 我只是找到了一个解，是需要进行枚举不同的位置的 for(int i =0; i&lt; nums.size(); i++) &#123; if(!st[i]) &#123; st[i] =true; path.push_back(nums[i]); dfs(nums, u+1); st[i] =false; path.pop_back(); // vector() 中的clear() 和pop_back() 有什么区别? &#125; &#125; &#125; // vector&lt;int&gt; 中 pop_back() 是和push_back() 相对应的，都是在尾部进行操作 // erase() , Removes from the vector either a single element (position) or a range of elements ([first,last)). 包括first，不包括last。 // 比如说这个样例， vec.erase(vec.begin()+5);//erase the 6th element &#125;; Permutations II 时间复杂度同上$(n! * n)$ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546class Solution &#123;public: // 这个在于元素可能是重复的。解决方案，先进行排序，然后再选择的时候，不选相同的元素 // dfs() 的时候多一个起始点 // 是另外一种思路，上一道题是枚举的数字，这个是枚举的坑位 // 数字是重复的，但是坑位是不重复的呀，所以可以枚举坑位 vector&lt;vector&lt;int&gt;&gt; res; vector&lt;int&gt; path; vector&lt;bool&gt; st; int n ; vector&lt;vector&lt;int&gt;&gt; permuteUnique(vector&lt;int&gt;&amp; nums) &#123; n =nums.size(); st= vector&lt;bool&gt;(n, false); path= vector&lt;int&gt;(n); sort(nums.begin(), nums.end()); dfs(nums, 0, 0); return res; &#125; void dfs(vector&lt;int&gt;&amp; nums, int u, int beg) &#123; if(u ==n) &#123; res.push_back(path); return ; &#125; // 开始进行枚举 for(int i= beg ; i&lt;n; i++) &#123; if(!st[i]) &#123; st[i] =true; path[i] =nums[u]; // 这里体现的是枚举的 path[i] 表示的位置, if(u+1 &lt; n&amp;&amp; nums[u+1] != nums[u]) dfs(nums, u+1, 0); // 这个注意为什么从 0开始 else dfs(nums, u+1, i+1); st[i] =false; &#125; &#125; &#125;&#125;; Subsets 对于二维的dfs，经常出现的判断条件是是否访问过，然后标记一下。对于一维的数字，更多是回溯，如果发现不是最优解，那么是回滚到之前的状态。 12345678910111213141516171819202122232425class Solution &#123;public: // 迭代的写法，使用二进制的思想求解 // 如果有三位数，那么 000 001 010 这样表示, 如果某一位是1 那么就表示选择，否则表示没有选择 // 如果知道了时间复杂度 O(2^n *n)， 对于迭代来说，这个是不是也是基本上就知道了 vector&lt;vector&lt;int&gt;&gt; subsets(vector&lt;int&gt;&amp; nums) &#123; vector&lt;vector&lt;int&gt;&gt; res; int n =nums.size(); for(int i =0; i&lt; (1&lt;&lt;n); i++) &#123; vector&lt;int&gt; tmp; for(int j =0 ; j&lt;n ;j++) &#123; if( i&gt;&gt; j &amp;1) tmp.push_back(nums[j]); &#125; res.push_back(tmp); &#125; return res; &#125;&#125;; 如果有重复的数字，那么应该从重复的数字选择几次，这个角度进行考虑、这个时间复杂度和上面的是一样的（因为一般来说 时间复杂度就是指的最坏的情况下，除非像一些排序算法，比如说一般说时间复杂度是$O(nlogn)$， 因为 $O(n^2)$ 这个复杂度达到的概率是极小的。） 123456789101112131415161718192021222324252627282930313233343536class Solution &#123;public: // 这个题目是有重复的数字，上一道题目是每个数字只有 0 1 两种选择 // 这个题目 每个数字是有 0 1 2 ... k （其中k 表示数字在array 中出现的次数） // 对于重复的，先进行分类或者说分块（而这个分类是可以使用排序实现的） vector&lt;vector&lt;int&gt;&gt; res; int n; vector&lt;int&gt; path; vector&lt;vector&lt;int&gt;&gt; subsetsWithDup(vector&lt;int&gt;&amp; nums) &#123; n= nums.size(); sort(nums.begin(), nums.end()); dfs(nums, 0);// dfs(nums, u) u表示选择的第 几个数字 return res; &#125; void dfs(vector&lt;int&gt; &amp; nums, int u) &#123; if(u ==n) &#123; res.push_back(path); return ; &#125; // 找出k 的个数 次数 int k=0; while(k +u&lt; n&amp;&amp; nums[u+k] ==nums[u] ) k++; // 开始进行枚举 k 次 for(int i =0; i&lt;=k ;i++) &#123; dfs(nums, u+k); path.push_back(nums[u]); &#125; // 恢复现场 for(int i =0; i&lt;=k ; i++) path.pop_back(); &#125; &#125;; Combination Sum III 12345678910111213141516171819202122232425262728293031323334353637383940414243444546class Solution &#123;public: // dfs(int k, int n, int start) // 因为不能出现重复的集合，所以可以人为的规定枚举的顺序(从小到大枚举) // 时间复杂度分析 c(k,9) *k， 发现没有，就是结果数 // 选择的k 个数字中是没有重复的，因为这个是comnbination vector&lt;vector&lt;int&gt;&gt; res; vector&lt;int&gt; path; vector&lt;vector&lt;int&gt;&gt; combinationSum3(int k, int n) &#123; // 这个初始化话是从1 开始的 dfs(k, n, 1); return res; &#125; void dfs(int k, int n, int begin) &#123; // 是这里有了边界， 然后再 枚举的过程中就不需要边界，因为这个是可以return的 if(!k) &#123; if(!n) &#123; //for(auto u: path) cout&lt;&lt; u&lt;&lt;" "; res.push_back(path); &#125; return; &#125; // 知道这里k 需要做一下限制，或者说k &lt;0 的时候，是需要 return for(int i =begin; i&lt;=9; i++) &#123; path.push_back(i); dfs(k-1, n -i, i+1); path.pop_back(); &#125; &#125;&#125;; 矩阵中的路径 请设计一个函数，用来判断在一个矩阵中是否存在一条包含某字符串所有字符的路径。 深度优先搜索，时间复杂度是 $O(n^23^k)$， 枚举 $n^2$ 个起点，每个起点是有三个选择（不走重复的路），然后 $k$ 是字符串的平均长度。 1234567891011121314151617181920212223242526272829303132333435363738class Solution &#123;public: // 寻找是否存在某个字符串的路径。先是枚举起点，然后dfs 寻找路径 int n, m; int dx[4] =&#123;-1, 0, 1, 0&#125;, dy[4] =&#123;0, 1, 0, -1&#125;; bool hasPath(vector&lt;vector&lt;char&gt;&gt;&amp; matrix, string str) &#123; if( !matrix.size()) return false; // case matrix =[] n =matrix.size() , m =matrix[0].size(); for(int i =0; i&lt; n; i++) &#123; for(int j =0; j&lt; m; j++) &#123; if(dfs(matrix, str, 0, i, j)) return true; &#125; &#125; return false; &#125; bool dfs(vector&lt;vector&lt;char&gt;&gt; &amp; matrix, string &amp; str, int u, int x, int y) &#123; if(str[u] != matrix[x][y]) return false; if(u ==str.size()-1) return true;// 这个主意是 size() -1 char tmp =matrix[x][y]; matrix[x][y] ='#'; for(int i =0; i&lt; 4; i++) &#123; int a =dx[i] +x, b =dy[i] +y; if(a&gt;= 0 &amp;&amp; a&lt;n &amp;&amp; b&gt;=0 &amp;&amp; b&lt; m) &#123; if(dfs(matrix, str, u+1, a, b)) return true; &#125; &#125; matrix[x][y] =tmp; return false; &#125;&#125;; 24. 机器人的运动范围 这个是 bfs 的题目，时间复杂度是 $O(mn)$ 123456789101112131415161718192021222324252627282930313233343536373839404142class Solution &#123;public: // 这个其实都不必使用 dfs or bfs的思想，因为这个是连续的，所以必然是某一个区域是能够访问 // 某一个区域是不能访问 // 宽度优先搜索，时间复杂 O(mn) 每个点只存在遍历一遍，不存在回溯的情况， 只是标记一下 int get_single(int x) &#123; int sum =0; while(x) sum += x%10, x /=10; return sum; &#125; int get_sum(pair&lt;int, int&gt; p) &#123; return get_single(p.first) +get_single(p.second); &#125; int movingCount(int threshold, int rows, int cols) &#123; queue&lt;pair&lt;int, int&gt;&gt; q; int res =0; vector&lt;vector&lt;bool&gt;&gt; st(rows, vector&lt;bool&gt;(cols)); q.push(&#123;0,0&#125;); int dx[4] =&#123;-1, 0, 1, 0&#125;, dy[4]=&#123;0, 1, 0, -1&#125;; while(q.size()) &#123; pair&lt;int, int&gt; t =q.front(); q.pop(); int x =t.first, y =t.second; if(st[x][y] || get_sum(t) &gt; threshold) continue; st[x][y] =true; res +=1; for(int i =0; i&lt; 4; i++) &#123; int a = x+dx[i], b =y +dy[i]; if(a&gt;= 0 &amp;&amp; a&lt; rows &amp;&amp; b &gt;=0 &amp;&amp; b&lt; cols) q.push(&#123;a, b&#125;); &#125; &#125; return res; &#125;&#125;; 51. 数字排列 时间复杂度分析：搜索树中最后一层共 $n!$ 个节点，前面所有层加一块的节点数量相比于最后一层节点数是无穷小量，可以忽略。且最后一层节点记录方案的计算量是 $O(n)$，所以总时间复杂度是 $O(n×n!)$。 这个是排列问题， $n! $ 是没有毛病的。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263class Solution &#123;public: // 先排序，然后根据是否重复选择下一个枚举的起点， 使用st 表示是否枚举过 int n; vector&lt;int&gt; path; vector&lt;vector&lt;int&gt;&gt; res; vector&lt;vector&lt;int&gt;&gt; permutation(vector&lt;int&gt;&amp; nums) &#123; n =nums.size(); // 这个是不能重复的int n path.resize(n);// 神一样的操作 sort(nums.begin(), nums.end()); dfs(nums, 0, 0, 0); return res; &#125; void dfs(vector&lt;int&gt; &amp; nums, int u, int be, int st) &#123; if(u ==n) &#123; res.push_back(path); return ; &#125; // 一次dfs 判断一次就ok了 if(!u || nums[u] !=nums[u -1]) be =0; for(int i =be; i&lt; n; i++) &#123; if(!(st &gt;&gt; i &amp;1)) &#123; path[i] =nums[u]; dfs(nums, u +1, i +1, st +(1&lt;&lt; i)); &#125; &#125; &#125;&#125;;2. [784. Letter Case Permutation](https://leetcode.com/problems/letter-case-permutation/submissions/)时间复杂度每个字母有两种选择 $2^n$, 然后最后 res 的时候是 $O(n)$， 所以总的时间复杂是$O(n2^n)$```c++class Solution &#123;public: // 简单的dfs vector&lt;string&gt; res; vector&lt;string&gt; letterCasePermutation(string S) &#123; dfs(S, 0); return res; &#125; void dfs(string &amp; S, int u) &#123; if( u ==S.size()) &#123; res.push_back(S); return ; &#125; dfs(S, u +1); if(S[u] &gt;=&apos;A&apos;) &#123; S[u] ^= 32; // 英文大小写字母转换的小的tips dfs(S, u +1); &#125; &#125;&#125;; 77. Combinations 时间复杂度是$O(C^k_n)$, 是一个组合题目。 12345678910111213141516171819202122232425262728class Solution &#123;public: // 使用dfs 最重要的是枚举的顺序，这个明显是一个组合问题，没有顺序，所有是需要人为的根据样例得到一个书序 // 假定这个是递增的，所以在dfs中需要规定一个下一个的起点 vector&lt;int&gt; path; vector&lt;vector&lt;int&gt;&gt; res; vector&lt;vector&lt;int&gt;&gt; combine(int n, int k) &#123; path.resize(k); dfs(0, 1, n, k); return res; &#125; // 回溯的思想是需要有的，但是有不同的实现方法。一种是 path[u] =i 这种就是直接复制覆盖 // 另外一种 path.push_back(i) , path.pop_back() 这种也是一种回溯 void dfs(int u, int be, int n, int k) &#123; if(u == k) &#123; res.push_back(path); return; &#125; // 枚举顺序 for(int i =be; i&lt;=n ; i++) &#123; path[u] =i; dfs(u +1, i+1, n, k); &#125; &#125;&#125;; LeetCode 216. Combination Sum III 时间复杂度分析：从9个数中选 k 个总共有 $C_k^9$ 个方案，将每个方案记录下来需要 $O(k)$的时间，所以时间复杂度是 $O(C_k^9×k) $。 123456789101112131415161718192021222324252627282930class Solution &#123;public: // 这个做的是减法，那么有很多边界条件是可以优化的 vector&lt;int&gt; path; vector&lt;vector&lt;int&gt;&gt; res; vector&lt;vector&lt;int&gt;&gt; combinationSum3(int k, int n) &#123; dfs(k, n, 1); return res; &#125; // k 表示能够枚举的个数， n表示能够枚举数字的和， be 表示下一个的起点位置，需要人工的指定顺序 void dfs(int k, int n, int be) &#123; if(!k) &#123; //这里个数枚举完了，但是不一定是 符合要求的，所有有两个限制条件 if(!n) &#123; res.push_back(path); &#125; return ; &#125; for(int i =be; i&lt;=9; i++) &#123; if(i&gt;n) continue; path.push_back(i); dfs(k -1, n -i, i+1); path.pop_back(); &#125; &#125;&#125;; 79. Word Search 我也可以进行debug的，这里可以使用 修改 board[x][y] 中的值来达到记录状态的效果，并且从内存和运行时间上都是优于重新定义一个数组的。时间复杂度分析， 起点是 $n^2$，每个单词除了首字母外都是只能有三种选择（不能走重复的路），所以 $3^k$（k 表示字符串的平均长度）。所以总共的时间复杂度是$n^23^k$ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546class Solution &#123;public: // 这种思路很常见，先是枚举起点，然后再搜索路径。搜索的时候上下左右四个方向进行遍历 // 需要一个st 标志是否遍历过 vector&lt;vector&lt;bool&gt;&gt; st; int n , m; bool exist(vector&lt;vector&lt;char&gt;&gt;&amp; board, string word) &#123; n = board.size(), m =board[0].size(); st =vector&lt;vector&lt;bool&gt;&gt;(n, vector&lt;bool&gt;(m , false)); for(int i =0; i&lt; n; i++) &#123; for(int j =0; j&lt; m;j ++) &#123; if(dfs(board, i, j , word, 0)) return true; &#125; &#125; return false; &#125; int dx[4] =&#123;-1, 0, 1, 0&#125;, dy[4] =&#123;0, 1, 0, -1&#125;; bool dfs(vector&lt;vector&lt;char&gt;&gt; &amp; board, int x, int y, string &amp; str, int u) &#123; if(board[x][y] != str[u]) return false; if(u ==str.size()-1) // 这个只有在debug 的时候才难得到 &#123; return true; &#125; //cout &lt;&lt; x &lt;&lt;" "&lt;&lt; y&lt;&lt; endl; char tmp =board[x][y]; board[x][y] ='#'; //st[x][y] =true; for(int i=0; i&lt; 4; i++) &#123; int a =x +dx[i], b =y +dy[i]; if(a &gt;=0 &amp;&amp; a&lt; board.size() &amp;&amp; b &gt;=0 &amp;&amp; b &lt; board[0].size() &amp;&amp; !st[a][b]) &#123; if(dfs(board, a, b, str, u+1)) return true; &#125; &#125; board[x][y] =tmp; //st[x][y] =false; return false; &#125;&#125;; 105. Construct Binary Tree from Preorder and Inorder Traversal dfs 递归解决， 使用hash 存储inorder 中的value 和index 的关系，方便之后的查找, 在 $O(1)$ 的时间内查找到了。最终的时间复杂是 $O(n)$.， 因为每个节点只是遍历了一遍。没有重复的数字，很关键的。 123456789101112131415161718192021222324252627282930313233/** * Definition for a binary tree node. * struct TreeNode &#123; * int val; * TreeNode *left; * TreeNode *right; * TreeNode(int x) : val(x), left(NULL), right(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: // dfs 递归解决， 使用hash 存储inorder 中的value 和index 的关系，方便之后的查找。最终的时间复杂是 O(n) unordered_map&lt;int, int&gt; hash; TreeNode* buildTree(vector&lt;int&gt;&amp; preorder, vector&lt;int&gt;&amp; inorder) &#123; int n =preorder.size(); for(int i =0; i&lt; n; i++) &#123; hash[inorder[i]] =i; &#125; return dfs(preorder, inorder, 0, n -1, 0, n -1); &#125; TreeNode * dfs(vector&lt;int&gt; preorder, vector&lt;int&gt; inorder, int pl, int pr, int il, int ir) &#123; if(pl &gt; pr) return NULL; int val =preorder[pl]; int index =hash[val]; int len =index -il; TreeNode* root =new TreeNode(val); root -&gt;left = dfs(preorder, inorder, pl +1, pl +len, il, index -1); root -&gt; right =dfs(preorder, inorder, pl +len+1, pr, index +1, ir); return root; &#125;&#125;; 101. Symmetric Tree 需要满足两个条件： 两个子树根节点值相同 左子树的左孩子和右子树的右孩子是镜像， 左子树的右孩子和右子树的左孩子是镜像 因为每个节点只是被遍历一次，所以时间复杂度是 $O(n)$。 可以从这个角度理解： 深度优先遍历整个子树，每个子树只是被遍历一遍，所以是 $O(n)$。 12345678910111213141516171819202122232425/** * Definition for a binary tree node. * struct TreeNode &#123; * int val; * TreeNode *left; * TreeNode *right; * TreeNode(int x) : val(x), left(NULL), right(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: // 对于dfs 不要想 那么多，只要想清楚一层就 ok了，否则是容易混乱的 bool isSymmetric(TreeNode* root) &#123; if (!root) return true; return dfs(root -&gt; left, root -&gt; right); &#125; bool dfs(TreeNode* left, TreeNode * right) &#123; if(! left || ! right) return !left&amp;&amp; ! right; // 这种写法是非常common的，只有两个都为空，那么才返回 true if(left-&gt;val != right -&gt; val) return false; return dfs(left-&gt; left, right -&gt; right) &amp;&amp; dfs(left-&gt; right, right-&gt; left); &#125; &#125;; 98. Validate Binary Search Tree 时间复杂度是 $O(n)$，因为每个节点只是会被遍历一次。所以对于这类判别题，时间复杂度没有那么高。 12345678910111213141516171819202122232425/** * Definition for a binary tree node. * struct TreeNode &#123; * int val; * TreeNode *left; * TreeNode *right; * TreeNode(int x) : val(x), left(NULL), right(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: // 这里有很多细节，比如说二叉搜素树的左子树严格小于根节点，右子树严格大于根节点 // 需要满足三个条件， 1. 左子树小于根节点，2。右子树大于根节点，3.左右子树也是二叉搜索树。 // 使用区间的概念，当前结点在某个区间， bool isValidBST(TreeNode* root) &#123; return dfs(root, INT_MIN, INT_MAX); &#125; bool dfs(TreeNode * root, long long min_v, long long max_v) &#123; // 如果是空，那么是满足二叉搜素树的定义的 if(! root) return true; if(root-&gt;val &lt; min_v || root -&gt;val &gt;max_v) return false; return dfs(root -&gt;left, min_v, root-&gt;val -1ll) &amp;&amp; dfs(root-&gt;right, root-&gt;val +1ll, max_v); &#125;&#125;; 130. Surrounded Regions 每个结点只是被遍历一次（ 有st[a][b] 和 board[a][b] 进行联合判断） 所以总的时间复杂度是 $O(mn)$ 。 这个一种叫做 “(Flood Fill, 深度优先遍历) “的题目 123456789101112131415161718192021222324252627282930313233343536373839404142 class Solution &#123;public: vector&lt;vector&lt;bool&gt;&gt; st; int n, m ; void solve(vector&lt;vector&lt;char&gt;&gt;&amp; board) &#123; if(board.empty()) return ; n =board.size(), m =board[0].size(); st =vector&lt;vector&lt;bool&gt;&gt;(n, vector&lt;bool&gt;(m, false)); //cout &lt;&lt; n&lt;&lt; m; // 遍历四周 for (int i = 0; i &lt; n; i ++ ) &#123; if (board[i][0] == 'O') dfs(i, 0, board); if (board[i][m - 1] == 'O') dfs(i, m - 1, board); &#125; for(int i =0; i&lt; m; i++) &#123; if(board[0][i] =='O') dfs(0, i, board); if(board[n-1][i] =='O') dfs(n-1, i, board); &#125; // 最后的判断 for(int i =0; i&lt;n ; i++) &#123; for(int j =0; j&lt; m; j++) &#123; if(!st[i][j]) board[i][j] ='X'; &#125; &#125; &#125; int dx[4] =&#123;-1, 0, 1, 0&#125;, dy[4] =&#123;0, 1, 0, -1&#125;; //这个类型也是很重要的 void dfs(int x, int y, vector&lt;vector&lt;char&gt;&gt; &amp; board) &#123; st[x][y] =true; for(int i =0; i&lt; 4; i++) &#123; int a =x +dx[i], b =y +dy[i]; if(a &gt;=0 &amp;&amp; a&lt;n &amp;&amp; b&gt;=0 &amp;&amp; b&lt;m &amp;&amp; !st[a][b] &amp;&amp; board[a][b] =='O') dfs(a, b, board); &#125; &#125;&#125;; 宽度优先遍历 24. 机器人的运动范围 时间分析 $O(mn)$， 每个结点只是出栈和入栈一次，所以只需要遍历一次，那么时间复杂度就是 $O(mn)$。 123456789101112131415161718192021222324252627282930313233343536373839404142 class Solution &#123;public: // 宽度优先搜索， bfs()，时间复杂度是 O(mn) 因为每个格子只是遍历一次 // 使用队列来维护遍历的过程 int get_single(int a) &#123; int sum =0; while(a) sum += a%10, a /=10; return sum; &#125; int get_sum(pair&lt;int, int&gt; p) &#123; return get_single(p.first) + get_single(p.second); &#125; int movingCount(int threshold, int rows, int cols) &#123; if( !rows || !cols) return 0; queue&lt;pair&lt;int, int&gt;&gt; q; q.push(&#123;0, 0&#125;); int res =0; vector&lt;vector&lt;bool&gt;&gt; st(rows, vector&lt;bool&gt;(cols, false)); st[0][0] =true; while(q.size()) &#123; auto t =q.front(); q.pop(); if(get_sum(t) &gt; threshold ) continue; res +=1; for(int i =0; i&lt; 4; i++) &#123; int dx[4] =&#123;-1, 0, 1, 0&#125;, dy[4] =&#123;0, 1, 0, -1&#125;; int a = t.first +dx[i], b = t.second+dy[i]; if(a &gt;=0 &amp;&amp; a&lt; rows &amp;&amp; b &gt;=0 &amp;&amp; b &lt; cols &amp;&amp; !st[a][b]) &#123; q.push(&#123;a, b&#125;); st[a][b] =true; &#125; &#125; &#125; return res; &#125;&#125;; 不分行从上往下打印二叉树 123456789101112131415161718192021222324252627282930 /** * Definition for a binary tree node. * struct TreeNode &#123; * int val; * TreeNode *left; * TreeNode *right; * TreeNode(int x) : val(x), left(NULL), right(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: // 不分行打印，这个是比较简单的，只是需要一个vector&lt;int&gt; 存储 vector&lt;int&gt; res; // 对于树来说，只是层次遍历，所以使用一个队列就 ok vector&lt;int&gt; printFromTopToBottom(TreeNode* root) &#123; if( !root) return vector&lt;int&gt;(); queue&lt;TreeNode *&gt; q; q.push(root); while(q.size()) &#123; auto t =q.front(); q.pop(); res.push_back(t-&gt;val); if(t-&gt;left) q.push(t-&gt; left); if(t-&gt; right) q.push(t-&gt; right); &#125; return res; &#125;&#125;; 分行从上往下打印二叉树 时间复杂度是结点的个数 $O(n)$ 123456789101112131415161718192021222324252627282930313233343536373839404142/** * Definition for a binary tree node. * struct TreeNode &#123; * int val; * TreeNode *left; * TreeNode *right; * TreeNode(int x) : val(x), left(NULL), right(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: // 使用 nullptr 来标记某一行的结束, 加入的时候也是比较简单的，如果不是null 那么就和上一道题目一样 // 如果是nullptr 那么加入的还是nullptr vector&lt;vector&lt;int&gt;&gt; printFromTopToBottom(TreeNode* root) &#123; vector&lt;vector&lt;int&gt;&gt; res; if(! root) return res; // 这个是临时存储结果的 vector&lt;int&gt; level; queue&lt;TreeNode*&gt; q; q.push(root); q.push(nullptr); while(q.size()) &#123; auto t =q.front(); q.pop(); if(t) &#123; level.push_back(t-&gt; val); if(t-&gt;left) q.push(t-&gt; left); if(t-&gt; right) q.push(t-&gt; right); &#125; else &#123; if(level.empty()) break; // 这个条件很重要，因为有空指针，然后这个会陷入死循环 res.push_back(level); level.clear(); q.push(nullptr); &#125; &#125; return res; &#125;&#125;; 之字形打印二叉树 12345678910111213141516171819202122232425262728293031323334353637383940414243/** * Definition for a binary tree node. * struct TreeNode &#123; * int val; * TreeNode *left; * TreeNode *right; * TreeNode(int x) : val(x), left(NULL), right(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: // 在分层遍历的基础上，加入一个flag 表示奇数层和偶数层 vector&lt;vector&lt;int&gt;&gt; printFromTopToBottom(TreeNode* root) &#123; vector&lt;vector&lt;int&gt;&gt; res; if(! root) return res; vector&lt;int&gt; level; queue&lt;TreeNode *&gt; q; q.push(root); q.push(nullptr); bool st =false; while(q.size()) &#123; auto t =q.front(); q.pop(); if(t) &#123; level.push_back(t-&gt; val); if(t-&gt; left) q.push(t-&gt;left); if(t-&gt; right) q.push(t-&gt;right); &#125; else &#123; if(level.empty()) break; if(st) reverse(level.begin(), level.end()); st =! st; res.push_back(level); level.clear(); q.push(nullptr); &#125; &#125; return res; &#125;&#125;; Binary Tree Level Order Traversal II 123456789101112131415161718192021222324252627282930313233343536373839404142434445/** * Definition for a binary tree node. * struct TreeNode &#123; * int val; * TreeNode *left; * TreeNode *right; * TreeNode(int x) : val(x), left(NULL), right(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: //先是 分层的层序遍历，然后是reverse一下, 如果是 二维数组，那么reverse() 是对 行进行的reverse() ，是比较简单的。 vector&lt;vector&lt;int&gt;&gt; levelOrderBottom(TreeNode* root) &#123; vector&lt;vector&lt;int&gt;&gt; res; if(!root) return res; vector&lt;int&gt; level; queue&lt;TreeNode *&gt; q; q.push(root); q.push(nullptr); while(q.size()) &#123; auto t =q.front(); q.pop(); if(t) &#123; level.push_back(t-&gt; val); if(t-&gt; left) q.push(t-&gt;left); if(t-&gt; right) q.push(t-&gt;right); &#125; else &#123; if(level.empty()) break; res.push_back(level); level.clear(); q.push(nullptr); &#125; &#125; reverse(res.begin(), res.end()); return res; &#125;&#125;; Populating Next Right Pointers in Each 时间复杂度是 $O(n)$，n 表示结点的个数。 1234567891011121314151617181920212223242526272829303132333435363738394041/*// Definition for a Node.class Node &#123;public: int val; Node* left; Node* right; Node* next; Node() &#123;&#125; Node(int _val, Node* _left, Node* _right, Node* _next) &#123; val = _val; left = _left; right = _right; next = _next; &#125;&#125;;*/class Solution &#123;public: // 这个是 perfect 二叉树, 层与层之间的遍历是 root-&gt;left, // 一层之间的遍历是 root = root-&gt;next ，这样进行遍历 Node* connect(Node* root) &#123; Node* p= root, *cur; // 遍历到非叶子结点 while(p) &#123; cur =p; // 只是在一层上进行遍历 while(cur &amp;&amp; cur-&gt;left) &#123; cur-&gt;left-&gt;next =cur-&gt;right; if(cur-&gt;next) cur-&gt;right -&gt;next =cur-&gt;next -&gt;left; cur =cur -&gt;next; // 不断的往下走 &#125; p =p-&gt;left; &#125; return root; &#125;&#125;; 解法二：使用一个队列 1234567891011121314151617181920212223242526272829303132333435363738394041424344/*// Definition for a Node.class Node &#123;public: int val; Node* left; Node* right; Node* next; Node() &#123;&#125; Node(int _val, Node* _left, Node* _right, Node* _next) &#123; val = _val; left = _left; right = _right; next = _next; &#125;&#125;;*/class Solution &#123;public: Node* connect(Node* root) &#123; if(! root) return root; queue&lt;Node *&gt; q; q.push(root); q.push(nullptr); while(q.size()) &#123; auto t =q.front(); q.pop(); if(t) &#123; t-&gt;next =q.front(); if(t-&gt;left) q.push(t-&gt;left); if(t-&gt;right) q.push(t-&gt;right); &#125; else &#123; if(q.size() &gt;0) q.push(nullptr); &#125; &#125; return root; &#125;&#125;; Populating Next Right Pointers in Each Node II 时间复杂 是$O(n)$ ，但是和上一道题目不一样的是，这个是非 Perfect tree，所以万能的层序遍历。这个是可以作为一个模板的。这个模板是处理分行的层次遍历。使用$O(n)$ 的时间复杂度。 12345678910111213141516171819202122232425262728293031323334353637383940414243/*// Definition for a Node.class Node &#123;public: int val; Node* left; Node* right; Node* next; Node() &#123;&#125; Node(int _val, Node* _left, Node* _right, Node* _next) &#123; val = _val; left = _left; right = _right; next = _next; &#125;&#125;;*/class Solution &#123;public: Node* connect(Node* root) &#123; if(!root) return root; queue&lt;Node *&gt; q; q.push(root); q.push(nullptr); while(q.size()) &#123; auto t =q.front(); q.pop(); if(t) &#123; t-&gt;next =q.front(); if(t-&gt;left) q.push(t-&gt;left); if(t-&gt;right) q.push(t-&gt;right); &#125; else&#123; if(q.size()&gt; 0) q.push(nullptr); &#125; &#125; return root; &#125;&#125;; 矩阵距离 时间复杂度的分析，是$O(n^2)$ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455#include&lt;iostream&gt;#include&lt;cstring&gt;#include&lt;algorithm&gt;#include&lt;queue&gt;using namespace std;const int N =1010;int n,m;char a[N][N];int d[N][N];typedef pair&lt;int, int&gt; PAIR;void dfs()&#123; memset(d, -1, sizeof d); queue&lt;PAIR&gt; q; for(int i =0 ; i&lt; n; i++) for(int j =0; j&lt; m; j++) if(a[i][j] =='1') &#123; d[i][j] =0; q.push(&#123;i, j&#125;); &#125; while(q.size()) &#123; auto t =q.front(); q.pop(); int dx[4]=&#123;-1, 0, 1, 0&#125;, dy[4] =&#123;0, 1, 0, -1&#125;; int x =t.first, y =t.second; for(int i =0; i&lt; 4; i++) &#123; int a =x +dx[i], b =y +dy[i]; if( a&gt;=0 &amp;&amp; a&lt; n &amp;&amp; b&gt;=0 &amp;&amp; b &lt; m &amp;&amp; d[a][b] == -1) &#123; q.push(&#123;a, b&#125;); d[a][b] = d[x][ y] +1; // 这个能体现层次遍历的过程 &#125; &#125; &#125;&#125;int main()&#123; // 这个读入的时候竟然没有空格，所以需要使用字符串进行入 scanf("%d %d", &amp;n, &amp;m); for(int i =0; i&lt; n; i++) scanf("%s", a[i]); dfs(); for(int i =0; i&lt; n; i++) &#123; for(int j =0; j&lt; m; j++) printf("%d ", d[i][j]); puts(""); &#125; return 0;&#125; Dijkstra 算法 求解最短路径的一类算法。稠密图（边很多），稀疏图（边很少）。有向图和无向图。用到贪心的策略，从一点到其他点的距离。 可以处理有向图和无线图 权值不能为负数]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>dfs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[write down now]]></title>
    <url>%2F2019%2F08%2F17%2FwriteDownNow%2F</url>
    <content type="text"><![CDATA[临时文件夹，存放未成系统的知识点。 快速傅里叶变换（Fast Fourier Transform）在算法竞赛中的运用主要是用来加速多项式的乘法。快速傅里叶变换（Fast Fourier Transform，FFT）是一种可在 $O(nlogn)$时间内完成的离散傅里叶变换（Discrete Fourier transform，DFT）算法。 多项式表示的两种方法 系数表达法$$f(x)=a_{0}+a_{1} x^{1}+a_{2} x^{2}+\ldots+a_{n} x^{n}$$ 点值表达法 $$f(x) ={(x_{0}, y_{0}),(x_{1}, y_{1}),(x_{2}, y_{2}), \ldots,(x_{n-1}, y_{n-1})}$$ FFT 中的步骤，首先是点值表达法相乘( 算法复杂度是 $O(n) $)，然后是系数表达法 和 点值表达法的转换，普通的方法时间复杂度是 $O(n^2)$ 但是FFT是可以优化成 $nlogn$ HMM频率派-&gt; 统计机器学习 -&gt; 优化问题 (model strategy algorirthm) 贝叶斯派 -&gt; 概率图模型 -&gt; 后验概率-&gt; 积分问题 隐马尔科夫模型深度学习没有火起来之前， 在NLP 领域非常流行的一个模型。效果是不错的，但是时间复杂度是 O(n^3), 适合学习的，但是不适合处理海量的数据。 dynamic model 是 时间序列，样本不是独立同分布，高斯混合模型是独立同分布的 增量学习 增量学习(Incremental Learning)是指一个学习系统能不断地从新样本中学习新的知识，并能保存大部分以前已经学习到的知识。增量学习非常类似于人类自身的学习模式。 使用的场景： 数据库非常大的情形,例如Web日志记录 流数据,因为这些数据随着时间在不断的变化,例如股票交易数据. 现有的增量学习算法大多采用决策树和神经网络算法实现的 交叉验证，选择一定数量（10个）的样本作为 test 数据集，然后再训练数据集中不出现，作为一次模型的训练。 使用正则表达式也是可以切分句子，然后形成分词的（这个在英文中更加明显） 笔试题图像领域中的数据增强? 一种是收集更多的数据，一种是根据现有的数据进行增强。数据增强，常用的方式，就是旋转图像，剪切图像，改变图像色差,扭曲图像特征，改变图像尺寸大小，增强图像噪音（一般使用高斯噪音，盐椒噪音）等. 逻辑回归的目标函数：log 似然函数$ p(y)$， 先验概率：一个事件发生的概率$ p(y|x) $，后验概率：一个事件在另一个事件发生条件下的条件概率 计算三个稠密矩阵 A、B、C 的乘积 ABC，假定三个矩阵的尺寸分别为 $m \times n $, $n \times p $, $p \times q$，且 $m&lt;n&lt;p&lt;q $,以下计算效率最高的是解释：$a \times b$ 矩阵和$ b \times c $矩阵相乘，每次行乘列运算需要 b 次乘法和 b-1 次加法，即总共需要 $(2b-1) \times a \times c $ 次运算，时间复杂度为$ O(a \times b \times c) $1.AB 的时间复杂度为 $O(m \times n \times p)$ ，此时形成 $m \times p$ 的新矩阵2.(AB)C 的时间复杂度为 $O(m \times p \times q) $ 结论：首先使较小的矩阵相乘，最后乘较大的矩阵可以减少运算量。 下列哪些项所描述的相关技术是对的？A AdaGrad和L-BFGS使用的都是一阶差分B AdaGrad和L-BFGS使用的都是二阶差分C Adagrad使用的是一阶差分，L-BFGS使用的是二阶差分D Adagrad使用的是二阶差分，L-BFGS使用的是一阶差分答案： C 牛顿法不仅使用了一阶导信息，同时还利用了二阶导来更新参数,L-BFGS算法是一种在牛顿法基础上提出的一种求解函数根的算法 对于一个分类任务，如果开始时神经网络的权重不是随机赋值的，二是都设成0，下面哪个叙述是正确的？（C）C 神经网络可以训练，但是所有的神经元最后都会变成识别同样的东西令所有权重都初始化为0这个一个听起来还蛮合理的想法也许是一个我们假设中最好的一个假设了, 但结果是错误的，因为如果神经网络计算出来的输出值都一个样，那么反向传播算法计算出来的梯度值一样，并且参数更新值也一样( $w=w−α∗dw $)。更一般地说，如果权重初始化为同一个值，网络即是对称的, 最终所有的神经元最后都会变成识别同样的东西。 pca 之前是需要进行正则化，然后求解 bagging（random forest 是其的一种特例，防止过拟合的手段）bagging 中的bag 是袋子的意思，有点通过 random 然后得到一堆东西，这个是可以防止过拟合的stacking (stack 是堆 的意思)，这里以论文审稿为例，首先是三个审稿人分别对论文进行审稿，然后分别返回审稿意见给总编辑，总编辑会结合审稿人的意见给出最终的判断，即是否录用。对应于stacking，这里的三个审稿人就是第一层的模型，其输出（审稿人意见）会作为第二层模型（总编辑）的输入，然后第二层模型会给出最终的结果。boosting 算法（gbdt、adaboost 和 xgboost） BN 算法：在将所有的输入传递到下一层之前对其进行归一化（更改） 以下()属于线性分类器最佳准则？机器学习 ML模型 易A.感知准则函数B.贝叶斯分类C.支持向量机D.Fisher准则正确答案：ACD线性分类器有三大类：感知器准则函数、SVM、Fisher准则，而贝叶斯分类器不是线性分类器。 感知准则函数 ：准则函数以使错分类样本到分界面距离之和最小为原则。其优点是通过错分类样本提供的信息对分类器函数进行修正，这种准则是人工神经元网络多层感知器的基础。 支持向量机 ：基本思想是在两类线性可分条件下，所设计的分类器界面使两类之间的间隔为最大，它的基本出发点是使期望泛化风险尽可能小。（使用核函数可解决非线性问题） Fisher 准则 ：更广泛的称呼是线性判别分析（LDA），将所有样本投影到一条远点出发的直线，使得同类样本距离尽可能小，不同类样本距离尽可能大，具体为最大化“广义瑞利商”。根据两类样本一般类内密集，类间分离的特点，寻找线性分类器最佳的法线向量方向，使两类样本在该方向上的投影满足类内尽可能密集，类间尽可能分开。这种度量通过类内离散矩阵 Sw 和类间离散矩阵 Sb 实现。 以下说法中正确的是() 机器学习 ML模型 中A.SVM对噪声(如来自其他分布的噪声样本)鲁棒B.在AdaBoost算法中,所有被分错的样本的权重更新比例相同C.Boosting和Bagging都是组合多个分类器投票的方法,二都是根据单个分类器的正确率决定其权重D.给定n个数据点,如果其中一半用于训练,一般用于测试,则训练误差和测试误差之间的差别会随着n的增加而减少解释：A. SVM解决的是结构风险最小, 经验风险处理较弱, 所以对数据噪声敏感. (基于树的结构gbdt 对于噪声是具有鲁棒性的，SVM 在没有噪声的数据下效果更好) B. AdaBoost算法中, 每个迭代训练一个学习器并按其误分类率得到该学习器的权重alpha, 这个学习器的权重算出两个更新比例去修正全部样本的权重: 正样本是exp(-alpha), 负样本是exp(alpha). 所以所有被分错的样本的权重更新比例相同. C. bagging的学习器之间无权重不同, 简单取投票结果; Boosting的adaboost根据误分类率决定权重, boosting的gbdt则是固定小权重(也称学习率), 用逼近伪残差函数本身代替权重. D: 根据中心极限定律, 随着n的增加, 训练误差和测试误差之间的差别必然减少 – 这就是大数据训练的由来 关于支持向量机SVM,下列说法错误的是（）A.L2正则项，作用是最大化分类间隔，使得分类器拥有更强的泛化能力B.Hinge 损失函数，作用是最小化经验分类错误C.分类间隔为1/||w||，||w||代表向量的模D.当参数C越小时，分类间隔越大，分类错误越多，趋于欠学习 C错误。间隔应该是2/||w||才对，后半句应该没错，向量的模通常指的就是其二范数。（根据两侧的距离） SVM 分类和深度学习逻辑回归分类的区别逻辑回归是线性分类，SVM 可以应用于线性分类和非线性分类问题，取决于核函数的选取。 SVM 是凸问题，深度学习都是非凸问题正确，深度学习算法的目标函数，几乎全都是非凸的。 笔试面试中的问题总结 下列哪一种偏移，是我们在最小二乘直线拟合的情况下使用的？图中横坐标是输入 X，纵坐标是输出 Y。 线性回归模型计算损失函数，例如均方差损失函数时，使用的都是 vertical offsets。perpendicular offsets 一般用于主成分分析（PCA）中。 fork() 函数fork()函数是Linux系统中唯一可以创建一个新进程的方法。其新创建的进程称为子进程，原进程称为父进程。子进程将从父进程处继承了整个父进程的地址空间（包括进程上下文、代码段、进程堆栈、内存信息、打开的文件描述符、进程优先级、进程组号等）。由于fork()函数所产生的子进程是完全复制的父进程，因此它们会运行同一个程序，父进程的代码从fork()函数的返回值开始分别在两个地址空间中同时运行，从而时两个进程分别获得其各自的fork()函数返回值，父进程返回值为子进程的进程号，子进程的返回值为0，所以可以通过判断fork()函数的返回值来确定该进程为父进程还是子进程。 对10TB的数据文件进行排序，应使用的方法是（这个主要考察的是内存问题，而不是数据量大造成的速度快慢问题， 是内存中放不下的问题）对于10TB的海量数据，数据不可能一次全部载入内存，传统的排序方法就不适用了，需要用到外排序的方法。外排序采用分治思想，即先对数据分块，对块内数据进行排序，然后采用归并排序的思想进行排序，得到数据的一个有序序列。 请问经过表达式a = 5 ? 0 : 1的运算,变量a的最终值是? -&gt; 0表达式中的a 是一种接收变量，是bool 类型的。 假设你有以下数据：输入和输出都只有一个变量。使用线性回归模型（y=wx+b）来拟合数据。那么使用留一法（Leave-One Out）交叉验证得到的均方误差是多少？留一法，简单来说就是假设有 N 个样本，将每一个样本作为测试样本，其它 N-1 个样本作为训练样本。这样得到 N 个分类器，N 个测试结果。用这 N个结果的平均值来衡量模型的性能。题目可以参考这里 针对L1 在0点导数为0的情况如何处理？ 解决方法之一是通过周围的loss 的值或者均值来替换该点为0 的情况。比如下面的方式： $$ Smooth_{L_1}(x ) = \begin{cases}0.5x^2 &amp; |x |&lt;1 \\|x |-0.5 &amp; otherwise\end{cases}$$]]></content>
  </entry>
  <entry>
    <title><![CDATA[Model Training]]></title>
    <url>%2F2019%2F08%2F12%2Fmodel-training%2F</url>
    <content type="text"><![CDATA[介绍模型选择的两种方法：交叉检验和正则化。使用交叉检验（K-fold cross validation）进行选择模型并不难理解，但正则化和模型选择的关系，一开始还不是很理解，所以通过整理，希望能带来一些启发。 交叉检验常见的共有三种数据集的划分方式。 （1）留出法（Holdout cross validation）： 直接将原始的样本集合随机划分成训练集和测试集两部分。比方说，对于一个点击量预测模型，我们把样本按照70%~30%的比例分成两部分，70%的样本用于模型训练；30%的样本用于模型测试，包括绘制ROC曲线、计算精确率和召回率等指标来评估模型性能；该方法有两大缺点： 最终模型和参数的选取依赖于你对于训练集和测试集的划分 该方法只使用了部分数据进行模型的训练，对于训练集本身就比较小的情况，那么模型的效果会受影响 （2）留一验证（LOOV， leave one out cross validation） 每次留下一个样本作为验证集，其余所有样本作为测试集，那么产生了 $n$个模型。而测试误差的计算是将 $n$个模型取平均。\begin{equation}CV_{(n)}=\frac{1}{n} \sum_{i=1}^{n} MSE_{i}\end{equation}优点：1). 不受训练集和测试集划分影响，所有的数据都做过测试集。2). 每次训练激活使用到了所有的数据，保证了模型的偏差（bias）很小缺点： 计算量过于大，是holdout 方法的 $n-1$倍 （3）K-fold交叉验证（cross validation） 首先将全部样本划分成k个大小相等的样本子集；一次遍历这k个子集，每次把当前子集作为验证集，其余所有子集作为训练集，进行模型的训练和评估；最后把k次评估指标的平均值作为最终的评估指标。在实验中，k经常取10.比如说 $k =10$，那么就利用十折交叉验证的方法： 不重复抽样将原始数据随机分为 k 份。 每一次挑选其中 1 份作为测试集，剩余 k-1 份作为训练集用于模型训练。 重复第二步 k 次，这样每个子集都有一次机会作为测试集，其余机会作为训练集。 在每个训练集上训练后得到一个模型，用这个模型在相应的测试集上测试，计算并保存模型的评估指标， 计算 k 组测试结果的平均值作为模型精度的估计，并作为当前 k 折交叉验证下模型的性能指标。 当 K =10 的时候 ，训练数据D被分为了10 份，每次取其中9份数据作为训练集，1份作为测试集，最终将循环后所有的评估结果取平均。 其中的 cross validation（交叉），即把一部分数据当做了训练数据集，一部分当做了测试数据集，体现了交叉的思想。 （4）关于k 值的选择 K越大，每次投入的训练集的数据越多，模型的Bias越小。但是K越大，又意味着每一次选取的训练集之前的相关性越大（考虑最极端的例子，当k=N，也就是在LOOCV（叫做留一法）里，每次都训练数据几乎是一样的）。而这种大相关性会导致最终的test error具有更大的Variance。 一般来说，根据经验我们一般选择k=5或10。当数据量比较大的时候，k值可以小点；当数据量比较小的时候，k值需要大点。 （5）如何去理解交叉验证呢？ 简单的说是可以从方差和偏差的角度进行分析。当k =1，全部的数据被用于训练，容易出现过拟合，容易出现低偏差、高方差的；当k =n，也被称为留一法，偏差升高了而方差是减少了。所以说取值范围的变化可以看做是偏差和方差相互妥协的过程。 正则化角度模型训练过程中很容易出现过拟合，根本原因是模型的复杂度远小于数据量，模型的复杂度和训练误差于测试误差的关系如下图所示。 从图中可以看到， 随着模型复杂度的提高，训练误差是不断下降的，但测试误差是先下降后上升的。模型选择 的经典方法是正则化。正则化的作用是选择经验风险于模型复杂度同时较小的模型。也可以说，在所有可以选择的模型中，能够很好的解释数据并且十分简单的才是好模型。 学习方法的泛化能力是指由该方法学习到的模型对未知数据的预测能力，是学习方法本质上重要的性质。现实中采用最多的方法是通过测试误差来评价学习方法的泛化能力。 结构化风险要区分这三个概念，首先要引入一个损失函数的概念。损失函数是期望风险、经验风险和结构风险的基础。 损失函数是针对单个具体的样本而言的。表示的是模型预测的值与样本真实值之间的差距。比如对于某个样本$(x_i, y_i)$，其真实的值为Yi,而我们的模型选择决策函数为 $f $,那么通过模型预测的值为$ f(x_i)$;损失函数就是用来表示$y_i$与$ f(x_i)$之间的差距的，我们用函数 $L(f(x_),y_i) $来衡量。我们希望的是这个L函数最小化。理想的情况是我们的模型决策函数预测值$f（x_i）$刚好等于该样本的真值 $y_i$。常见的损失函数有： 平方损失函数、绝对值损失函数和对数损失函数。 通过损失函数我们只能知道模型决策函数 $f(x) $对于单个样本点的预测能力（借用损失函数 $L(y ,f(x)) $，损失函数越小，说明模型对于该样本预测越准确。），那么如果想知道模型 $f(x) $对训练样本中所有的样本的预测能力应该怎么办呢？显然只需所有的样本点都求一次损失函数然后进行累加就好了。如下式 $$ (R_{exp}(f)=\frac{1}{N} \sum_{i=1}^{N} L\left(y_{i}, f\left(x_{i}\right)\right))$$ 这就经验风险，所谓的经验风险最小化便是让这个式子最小化，注意这个式子中累加和的上标N表示的是训练样例集中样本的数目。 经验风险是对训练集中的所有样本点损失函数的平均最小化。经验风险越小说明模型f(X)对训练集的拟合程度越好，但是对于未知的样本效果怎么样呢？我们知道未知的样本数据$（x, y）$的数量是不容易确定的，所以就没有办法用所有样本损失函数的平均值的最小化这个方法，那么怎么来衡量这个模型对所有的样本（包含未知的样本和已知的训练样本）预测能力呢？熟悉概率论的很容易就想到了用期望。即假设 $x $和 $y $服从联合分布 $P(x ,y) $.那么期望风险就可以表示为： $$R_{exp}(f)=E_{P}[L(Y, f(X))]=\int_{\mathrm{x \times y}} L(y, f(x)) P(x, y) d x d y$$ 这就是期望风险，期望风险表示的是全局的概念，表示的是决策函数对所有的样本 $x,y $预测能力的大小，而经验风险则是局部的概念，仅仅表示决策函数对训练数据集里样本的预测能力。理想的模型（决策）函数应该是让所有的样本的损失函数最小的（也即期望风险最小化），但是期望风险函数往往是不可得到的，即上式中， $x $与 $ y $的联合分布函数不容易得到。现在我们已经清楚了期望风险是全局的，理想情况下应该是让期望风险最小化，但是呢，期望风险函数又不是那么容易得到的。怎么办呢？那就用局部最优的代替全局最优这个思想吧。这就是经验风险最小化的理论基础。 通过上面的分析可以知道，经验风险与期望风险之间的联系与区别。现在在总结一下： 经验风险是局部的，基于训练集所有样本点损失函数最小化的。 期望风险是全局的，是基于所有样本点的损失函数最小化的。 经验风险函数是现实的，可求的； 期望风险函数是理想化的，不可求的； 只考虑经验风险的话，会出现过拟合的现象，过拟合的极端情况便是模型f(x)对训练集中所有的样本点都有最好的预测能力，但是对于非训练集中的样本数据，模型的预测能力非常不好。怎么办呢？这个时候就引出了结构风险。结构风险是对经验风险和期望风险的折中。在经验风险函数后面加一个正则化项（惩罚项）便是结构风险了。如下式： $$R_{\mathrm{em}}(f)=\frac{1}{N} \sum_{i=1}^{N} L\left(y_{i}, f\left(x_{i}\right)\right)+\lambda J(f)$$ 相比于经验风险，结构风险多了一个惩罚项，其中是一个 $\lambda $是一个大于0的系数。$J(f) $表示的是是模型f的复杂度。结构风险可以这么理解： 经验风险越小，模型决策函数越复杂，其包含的参数越多，当经验风险函数小到一定程度就出现了过拟合现象。也可以理解为模型决策函数的复杂程度是过拟合的必要条件，那么我们要想防止过拟合现象的方式，就要破坏这个必要条件，即降低决策函数的复杂度。也即，让惩罚项 $J(f) $最小化，现在出现两个需要最小化的函数了。我们需要同时保证经验风险函数和模型决策函数的复杂度都达到最小化，一个简单的办法把两个式子融合成一个式子得到结构风险函数然后对这个结构风险函数进行最小化。 常见的说法： 极大似然估计是经验风险最小化的一个例子，当模型是条件概率分布，损失函数是对数损失函数时，经验风险最小化就等于极大似然估计。（但是，当样本量很小时，经验风险最小化学习的效果未必很好，会产生后面叙述的“过拟合”现象。） 结构风险最小化是为了防止过拟合而提出来的 策略。结构风险最小化等价于正则项或罚项。 贝叶斯估计中的最大后验概率估计（MAP,maximum posterior probability estimation）就是结构风险最小化的一个例子。当模型是条件概率分布、损失函数是对数损失函数、模型复杂度由模型的先验概率表示时，结构风险最小化就等价于最大后验概率估计。 复习笔记 经验风险和期望风险，前者是针对训练集，可以计算；后者是对于全部训练数据，不可计算。结构化风险是两者的折中，在经验风险后面加上一个正则项，对经验风险进行了惩戒。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>k-fold</tag>
        <tag>交叉验证</tag>
        <tag>结构化风险</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Union-Find (并查集算法)]]></title>
    <url>%2F2019%2F08%2F05%2Funion-find%2F</url>
    <content type="text"><![CDATA[并查集是一种树型数据结构，用于处理不相交 (Disjoint Sets) 的合并及查询问题。 定义并查集，在一些有N个元素的集合应用问题中，我们通常是在开始时让每个元素构成一个单元素的集合，然后按一定顺序将属于同一组的元素所在的集合合并，其间要反复查找一个元素在哪个集合中。 并查集属于一种数据结构算法，一般来说有半确定的模块功能： 初始化操作 合并操作 （能够体现层级关系的，数量少的集合是需要并入到数量大的集合中去） 路径压缩 （在查询的过程中，用进行路径压缩） 对于查找操作，假设需要确定x所在的的集合，也就是确定集合的代表元。判断两个元素是否属于同一集合，只需要看他们的代表元是否相同即可。并查集为了避免时间和空间上的损耗，在每一轮的查找时，都要进行一次路径压缩优化。 路径压缩 ： 在递归找到根节点的时候，把当前节点到根节点间所有节点的父节点都设置为根节点。 我们来看下图，首先我们有个这样的一棵树，现在要找到元素9所在树的根节点，在找根节点的过程中使用路径压缩，也就是说9到根的路径上的节点9，6，3，1的父节点都设置成为根节点0，所以呢，在FIND-SET(9)之后，树的形态就变成了下面的样子 我们可以看到经过路径压缩的节点及其子树到根节点的深度减小了很多，所以在以后的操作中，查找根节点的速度会快很多 这样可以将查询一个结点的根节点的时间复杂度从 O(log N) 降到 O(1) 还有一个比较有趣的图解可以说明这个优化： 并查集中的两个操作时间复杂度都是 $O(1)$ 合并两个集合 判断两个点是否在同一个集合中 并查集中有两个优化：路径压缩 和按秩合并。加上第一个优化，那么时间复杂度是 $log n$，如果加上第二个优化，那么时间复杂度是 $log log n$ 基于C语言的两种实现基于数组和结构体的两种实现（C 语言），一般来说简单问题使用数组，复杂问题使用结构体。 数组 初始化 123456789101112#define MAX 10000int set[max];//集合index的类别，或者用parent表示int rank[max];//集合index的层次，通常初始化为0int data[max];//集合index的数据类型//初始化集合void Make_Set(int i)&#123; set[i]=i;//初始化的时候，一个集合的parent都是这个集合自己的标号。没有跟它同类的集合，那么这个集合的源头只能是自己了。 rank[i]=0;&#125; 查找函数 123456789//查找集合i（一个元素是一个集合）的源头（递归实现）int Find_Set(int i)&#123; //如果集合i的父亲是自己，说明自己就是源头，返回自己的标号 if(set[i]==i) return set[i]; //否则查找集合i的父亲的源头 return Find_Set(set[i]); &#125; 合并函数 123456789101112void Union(int i,int j)&#123; i=Find_Set(i); j=Find_Set(j); if(i==j) return ; if(rank[i]&gt;rank[j]) set[j]=i; else &#123; if(rank[i]==rank[j]) rank[j]++; set[i]=j; &#125;&#125; 结构体实现 初始化 123456struct Node&#123; int data; int rank; int parent; &#125;node[MAX]; 查找函数 1234567891011/***查找集合i（一个元素是一个集合）的源头（递归实现）。 如果集合i的父亲是自己，说明自己就是源头，返回自己的标号； 否则查找集合i的父亲的源头。**/int get_parent(int x)&#123; if(node[x].parent==x) return x; return get_parent(node[x].parent);&#125; 合并函数 12345678910111213void Union(int a,int b)&#123; a=get_parent(a); b=get_parent(b); if(node[a].rank&gt;node[b].rank) node[b].parent=a; else &#123; node[a].parent=b; if(node[a].rank==node[b].rank) node[b].rank++; &#125;&#125; 应该是有三个版本的 union-find 算法的： find() union() 总的时间复杂度 quick-find O(1) O(N) O(M*N) quick-union O(logN ~ N) O(1) O(M*N) 极端 WeightedUF O(log N) O(N) O(M *log N) 这里有三个版本的实现 应用1、维护无向图的连通性。支持判断两个点是否在同一连通块内，和判断增加一条边是否会产生环。2、媒体社交（比如：向通一个社交圈的朋友推荐商品）3、数学集合（比如：判断元素p,q之后选择是否进行集合合并） 常见的一个算法题， 给出10W 条任何人之间的朋友关系，求这些朋友关系中有多少个朋友圈，并且给出算法的时间复杂度。样例：A-B, B -C, D-E, E-F 这 四对关系中存在着两个朋友圈 例题模板有三个。第一个和第二个已经练习过，但是第三个没有练习过。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768691)朴素并查集： int p[N]; //存储每个点的祖宗节点 // 返回x的祖宗节点 int find(int x) &#123; if (p[x] != x) p[x] = find(p[x]); return p[x]; &#125; // 初始化，假定节点编号是1~n for (int i = 1; i &lt;= n; i ++ ) p[i] = i; // 合并a和b所在的两个集合： p[find(a)] = find(b);(2)维护size的并查集： int p[N], size[N]; //p[]存储每个点的祖宗节点, size[]只有祖宗节点的有意义，表示祖宗节点所在集合中的点的数量 // 返回x的祖宗节点 int find(int x) &#123; if (p[x] != x) p[x] = find(p[x]); return p[x]; &#125; // 初始化，假定节点编号是1~n for (int i = 1; i &lt;= n; i ++ ) &#123; p[i] = i; size[i] = 1; &#125; // 合并a和b所在的两个集合： p[find(a)] = find(b); size[b] += size[a];(3)维护到祖宗节点距离的并查集： int p[N], d[N]; //p[]存储每个点的祖宗节点, d[x]存储x到p[x]的距离 // 返回x的祖宗节点 int find(int x) &#123; if (p[x] != x) &#123; int u = find(p[x]); d[x] += d[p[x]]; p[x] = u; &#125; return p[x]; &#125; // 初始化，假定节点编号是1~n for (int i = 1; i &lt;= n; i ++ ) &#123; p[i] = i; d[I] = 0; &#125; // 合并a和b所在的两个集合： p[find(a)] = find(b); d[find(a)] = distance; // 根据具体问题，初始化find(a)的偏移量 547. Friend Circles p[x] 表示结点 x 的祖先结点，在进行find 之后，找到该节点的祖先结点，这个过程中应用了路径压缩。使用了递归的思想。 123456789101112131415161718192021222324252627282930313233class Solution &#123;public: // 这个是并查集中的裸题 vector&lt;int&gt; p; // 找父节点的一个基本操作 int find(int x) &#123; // 根据通项公式（假设）， p[x] 的祖先结点已知 if(p[x] != x) p[x] =find(p[x]); //路径压缩 return p[x]; &#125; int findCircleNum(vector&lt;vector&lt;int&gt;&gt;&amp; M) &#123; int n =M.size(); // 初始化，每个节点都是自己的父节点 for(int i =0; i&lt;n; i++) p.push_back(i); int res =n; for(int i =0; i&lt; n; i++) &#123; for(int j =0; j&lt; i; j++) &#123; if(M[i][j] ==0) continue; if(find(i) !=find(j)) &#123; p[find(i)] =find(j); res -=1; &#125; &#125; &#125; return res; &#125;&#125;; 合并集合 123456789101112131415161718192021222324252627282930#include&lt;iostream&gt;#include&lt;string&gt;#include&lt;vector&gt;using namespace std;vector&lt;int&gt; p;int find(int x)&#123; if( x!= p[x]) p[x] =find(p[x]); return p[x];&#125;int main()&#123; int n, m; cin &gt;&gt;n&gt;&gt;m; for(int i =0; i&lt;n; i++) p.push_back(i); while(m --) &#123; string s; int a, b; cin &gt;&gt; s &gt;&gt;a&gt;&gt; b; if(s =="M") p[find(a)] =p[find(b)]; if(s =="Q") &#123; if(find(a) ==find(b)) cout &lt;&lt;"Yes"&lt;&lt;endl; else cout&lt;&lt; "No" &lt;&lt;endl; &#125; &#125; return 0;&#125; 连通块中点的数量 重点是需要维护一个 size 的数组，这样的话就能得知 每一个连通图的大小。时空分析： 总的时间复杂度 $O(m +n)$总的空间复杂度$O(n)$ init 的函数 : $O(n) $find 函数 $O(logn )$ 可以近似认为是 $O(1)$merge 函数 $O(1)$size 函数 $O(1)$ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566#include&lt;iostream&gt;#include&lt;vector&gt;using namespace std;const int N =1e5+11;int m, n;vector&lt;int&gt; p;vector&lt;int&gt; size;void init()&#123; for(int i =0; i&lt;n; i++) &#123; p.push_back(i); size.push_back(1); &#125;&#125;int find(int x)&#123; if(x != p[x]) p[x] =find(p[x]); return p[x];&#125;void merge(int x, int y)&#123; int t1, t2; t1 =find(x); t2 =find(y); if(t1 != t2) &#123; // 规定一律向 x 合并 p[t2] =t1; size[t1] += size[t2]; &#125;&#125;int main()&#123; cin &gt;&gt;n &gt;&gt;m; init(); while( m--) &#123; string op; int a, b; cin &gt;&gt; op; if(op =="C") &#123; cin &gt;&gt;a&gt;&gt;b; merge(a, b); &#125; else if(op =="Q1") &#123; cin &gt;&gt;a&gt;&gt;b; if(find(a) == find(b) ) puts("Yes"); else puts("No"); &#125; else &#123; // 就是应该单独写，这个条件下只有一个输入 cin &gt;&gt;a; cout &lt;&lt; size[find(a)] &lt;&lt;endl; &#125; &#125; return 0;&#125; 684. Redundant Connection 并查集的操作是 $O(1)$ 需要遍历一遍数组，所以总的时间是$O(n)$。其中有一点，为什么多申请了一个空间，这个是没有很明白的。 1234567891011121314151617181920212223242526272829303132333435363738394041424344class Solution &#123;public: vector&lt;int&gt; p; void init(int n) &#123; for(int i =0; i&lt; n; i++) p.push_back(i); &#125; int find(int x) &#123; if(x != p[x]) p[x] =find(p[x]); return p[x]; &#125; bool is_union(int x, int y) &#123; int t1 =find(x); int t2 =find(y); if( t1 ==t2) return true; else &#123; p[t1] =p[t2]; return false; &#125; &#125; vector&lt;int&gt; findRedundantConnection(vector&lt;vector&lt;int&gt;&gt;&amp; edges) &#123; int n =edges.size() ; init(n+1);// 这里为什么要多申请一个空间 cout &lt;&lt; n&lt;&lt;endl; vector&lt;int&gt; res(2, 0); for(int i =0; i&lt; n; i++) &#123; int x =edges[i][0]; int y =edges[i][1]; //cout &lt;&lt; x&lt;&lt;" "&lt;&lt; y&lt;&lt; " "&lt;&lt; endl; if(is_union(x, y)) &#123; res[0] =x; res[1] =y; &#125; &#125; return res; &#125;&#125;; (784. 强盗团伙)[https://www.acwing.com/problem/content/description/786/] 题解 简单的题解： 此题的朋友的朋友是很好理解的，只要是我的朋友，我们就是一个team，进行Union操作。但是敌人的敌人也是我们的朋友，怎么考量呢？我们需要为此增加一个f数组，存储我的敌人的大哥，很好理解，a和b是敌人的话，b和c是敌人的话，那么a和c应该是一个team的，此处需要对a和c进行Union操作，我们第一次存储的敌人是一个参考，判断哪些人应该合并。 时间复杂分析， sort() 函数是 $O(mlog m)$ ，然后需要遍历一遍边 $O(m)$, 并查集操作一般认为是 $O(1)$，所以最后是 $O(mlog m)$ ，m 表示边的数量。kruskal 算法求解最小生成树题解 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253#include&lt;iostream&gt;#include&lt;algorithm&gt;using namespace std;const int N =1e5+11, M =2e5+11, INF =0x3f3f3f3f;int n, m;int p[N];struct Edge&#123; int a, b, w;&#125;edges[M];int find(int x)&#123; // 这个是递归的定义，只需要一次if 就行，不要使用 while if(p[x] != x) p[x] =find(p[x ]); return p[x];&#125;bool cmp(Edge a, Edge b)&#123; return a.w&lt; b.w;&#125;int kruskal()&#123; sort(edges, edges +m, cmp); for(int i =1; i&lt;=n; i++) p[i] =i; int res =0, cnt =0; for(int i =0; i&lt;m ;i++) &#123; int a =edges[i].a, b =edges[i].b, w =edges[i].w; a =find(a), b =find(b); if(a !=b) &#123; p[a]=b; res +=w; cnt ++; &#125; &#125; if (cnt !=n -1) return INF; return res;&#125;int main()&#123; cin &gt;&gt; n&gt;&gt;m; for(int i =0; i&lt;m; i++) &#123; int a, b, w; cin &gt;&gt;a&gt;&gt;b&gt;&gt;w; edges[i] =&#123;a, b, w&#125;; &#125; int t =kruskal(); if(t ==INF) puts("impossible"); else cout&lt;&lt;t&lt;&lt;endl;&#125; 参考文献 https://www.cnblogs.com/SeaSky0606/p/4752941.html]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>union-find</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Algorithm Practice(1)]]></title>
    <url>%2F2019%2F07%2F30%2Falgorithm-practice%2F</url>
    <content type="text"><![CDATA[刷题整理笔记。 最小编辑距离 链接地址 leetcode 版本123456789101112131415161718192021222324252627282930313233343536373839404142#include&lt;iostream&gt;#include&lt;vector&gt;using namespace std;class Solution &#123;public: int minDistance(string word1, string word2) &#123; int rows = word1.length(); int cols = word2.length(); vector&lt;vector&lt;int&gt; &gt; dp(rows+1, vector&lt;int&gt;(cols+1, 0)); for(int i=1; i&lt;=rows; ++i) dp[i][0] = i; for(int j=1; j&lt;=cols; ++j) dp[0][j] = j; for(int i=1; i&lt;=rows; ++i)&#123; for(int j=1; j&lt;=cols; ++j)&#123; if(word1[i-1] == word2[j-1]) dp[i][j] = dp[i-1][j-1]; else dp[i][j] = min(dp[i-1][j-1], min(dp[i-1][j], dp[i][j-1])) + 1; &#125; &#125; return dp[rows][cols]; &#125;&#125;;int main()&#123; Solution solution; string str1 ="hello"; string str2 ="hello1"; cout &lt;&lt; solution.minDistance(str1, str2) &lt;&lt; endl; return 0;&#125; 单机版本12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849#include&lt;iostream&gt;#include&lt;vector&gt;#include&lt;string&gt;#include&lt;algorithm&gt;using namespace std;/**使用dp 的思想， f[i][j] 表示string1中第i 个位置和 string2 中第j 位置之间的最小编辑距离 转移: f[i][j] =f[i-1][j-1] if string1[i] ==string2[j] = min(f[i-1][j], f[i][j-1] ) 对应的是修改 初始化: f[0][0] =1, 然后f[i][0] 和 f[0][j] 都是0， 最后的f[m][n] 就是最后的结果 **/int main()&#123; string str1, str2; cin &gt;&gt; str1&gt;&gt; str2; int n,m; n =str1.size(), m =str2.size(); vector&lt;vector&lt;int&gt;&gt; f(n+1, vector&lt;int&gt;(m+1)); // 初始化需要根据实际意义进行 f[0][0] =0; for(int i =1; i&lt;=n; i++) f[i][0] =i; for(int j =1; j&lt;=m ;j++) f[0][j] =j; for(int i =1; i&lt;=n; i++) &#123; for(int j =1; j&lt;=m; j++) &#123; //if(!i ||!j) f[i][j] =0; if(str1[i] ==str2[j]) f[i][j] =f[i-1][j-1]; else &#123; f[i][j] =min(f[i-1][j-1], min(f[i-1][j], f[i][j-1]))+1; &#125; &#125; &#125; cout&lt;&lt;f[n][m]&lt;&lt;endl; return 0;&#125; python 语言实现 注意两种初始化的区别：12dp =[(cols+1)*[0]]*(rows+1) # 这种是不 work 的dp =[ [0] *(cols+1) for _ in range(rows+1)] # 这种是 work 的 123456789101112131415161718192021222324252627282930def minDistance(string1, string2): if not string1 or not string2 : return rows, cols =len(string1), len(string2) if rows ==0: return cols; elif cols ==0: return rows; #dp =[(cols+1)*[0]]*(rows+1) dp =[ [0] *(cols+1) for _ in range(rows+1)] for i in range(1, rows+1): dp[i][0] =i for j in range(1, cols+1): dp[0][j] =j #ipdb.set_trace() for i in range(1, rows+1): for j in range(1, cols+1): if string1[i-1] ==string2[j-1]: dp[i][j] =dp[i-1][j-1] else: dp[i][j] =1 + min(dp[i-1][j-1], dp[i-1][j], dp[i][j-1]) return dp[-1][-1]print(minDistance("string1", "string")) 从无头单链表中删除节点 删除节点的通常做法是找到该结点的前一个结点（头结点），然后 head.next =head.next.next这个题目说没有头结点，直接给出的就是应该删除的结点假设这个是头结点，那么下一个是待删除的结点，所以 current.next =current.next.next ，但是需要把current.next.value 赋值给 current.value 12345678910111213141516171819202122232425262728#include&lt;stdio.h&gt;#include&lt;stdlib.h&gt;#include&lt;assert.h&gt;typedef struct node&#123; int data; node *next;&#125;Node;void delete_node(Node *current)&#123; assert(current !=NULL); Node *next =current-&gt;next; if (next !=NULL) &#123; current-&gt;next =next-&gt;next current-&gt; data =next-&gt; data; &#125;&#125;int main()&#123; return 0;&#125; Best Time to Buy and Sell Stock Say you have an array for which the ith element is the price of a given stock on day i.If you were only permitted to complete at most one transaction (ie, buy one and sell one share of the stock), design an algorithm to find the maximum profit. Tips: 记录一个最小值 和最大的max differences 1234567891011121314151617181920class Solution: def maxProfit(self, prices): if not prices or len(prices) ==1: return 0 low, maxDiff =prices[0], 0 for i in range(1, len(prices)): if prices[i] &lt; low: low =prices[i] else: diff =prices[i] -low if diff&gt; maxDiff: maxDiff =diff return maxDiff Design an algorithm to find the maximum profit. You may complete as many transactions as you like (i.e., buy one and sell one share of the stock multiple times).Note: You may not engage in multiple transactions at the same time (i.e., you must sell the stock before you buy again). Tips： 这个是可以进行多次买卖，如同寻找增序列 1234567891011class Solution: def maxProfit(self, prices): total =0 for i in range(1, len(prices)): if prices[i] &gt;= prices[i-1]: total += prices[i]-prices[i-1] return total 最多进行两次交易。 这个返回的最后是最大值 maximum profit。好好理解一下吧 1234567891011121314151617class Solution(object): def maxProfit(self, prices): """ :type prices: List[int] :rtype: int """ one_buy = two_buy = sys.maxsize one_profit = two_profit = 0 for p in prices: one_buy = min(one_buy, p) one_profit = max(one_profit, p - one_buy) two_buy = min(two_buy, p - one_profit) # 为什么和 one_profit 进行比较呢 two_profit = max(two_profit, p - two_buy) return two_profit Design an algorithm to find the maximum profit. You may complete at most k transactions. 最多进行 k 次交易 https://leetcode.com/problems/best-time-to-buy-and-sell-stock-iv/discuss/273435/Python-1-D-DP 123456789def maxProfit(k, prices): if k &gt; len(prices) &gt;&gt; 1: return sum(prices[i+1]-prices[i] for i in range(len(prices)-1) if prices[i+1]&gt;prices[i]) cash, asset = [float('-inf')] * (k+1), [0] * (k+1) for price in prices: for i in range(1,k+1): cash[i] = max(cash[i], asset[i-1]-price) asset[i] = max(asset[i], cash[i]+price) return asset[k] 这个是比较好理解一些的https://blog.csdn.net/xx_123_1_rj/article/details/80857144 12345678910111213141516171819202122class Solution: def maxProfit(self, k, prices): n, res = len(prices), 0 if n &lt; 2: return 0 if k &gt; n //2: # 现在这个情况，就相当于题目2 for i in range(1, n): if prices[i] &gt; prices[i-1]: res += prices[i] - prices[i-1] return res hold, sold = [float('-inf')] * (k + 1), [0] * (k + 1) for price in prices: for j in range(1, k+1): hold[j] = max(hold[j], sold[j-1]-price) # hold-&gt;hold, sold-&gt;hold sold[j] = max(sold[j], hold[j]+price) # sold-&gt;sold, hold-&gt;sold return sold[k] if __name__ == '__main__': prices, k = [7, 1, 5, 3, 6, 4], 4 solu = Solution() print(solu.maxProfit(k, prices)) 38. Count and Say https://leetcode.com/problems/count-and-say/ leetcode 版本 1234567891011121314151617181920212223class Solution &#123;public: string countAndSay(int n) &#123; string s ="1"; for(int i =0; i&lt;n -1; i++) &#123; string ns; for(int j =0; j&lt; s.size();j++ ) &#123; int k =j; while(s[k] ==s[j]) k++; ns += to_string(k-j)+ s[j]; j =k -1; &#125; s =ns; &#125; return s; &#125;&#125;; 单机版本 123456789101112131415161718192021222324252627282930313233343536#include&lt;iostream&gt;#include&lt;string&gt;#include&lt;algorithm&gt;using namespace std;string count_say(int n)&#123; string s ="1"; for(int i =0; i&lt;n-1; i++) &#123; string ns; for(int j =0; j&lt;s.size(); j++) &#123; int k =j; while( s[k] ==s[j]) k++; ns += to_string(k-j)+s[j]; j = k-1; &#125; s =ns; &#125; return s;&#125;int main()&#123; int n; cin &gt;&gt;n; cout&lt;&lt; count_say(n)&lt;&lt;endl; return 0;&#125; 1234567891011121314151617181920212223242526class Solution &#123;public: // 单词不同组合的本质，然后使用dictionary 进行存储 vector&lt;vector&lt;string&gt;&gt; groupAnagrams(vector&lt;string&gt;&amp; strs) &#123; unordered_map&lt;string, vector&lt;string&gt;&gt; hash; for(auto str: strs) &#123; string tmp =str; sort(tmp.begin(), tmp.end()); hash[tmp].push_back(str); &#125; // 本质 -&gt; 现象 //for(auto item: hash) cout&lt;&lt;item&lt;&lt;endl; vector&lt;vector&lt;string&gt;&gt; res; for(auto item: hash) res.push_back(item.second); return res; &#125;&#125;; https://leetcode.com/problems/group-anagrams/submissions/ vector中push_back 类似 python 中list 的append 操作 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657#include&lt;iostream&gt;#include&lt;string&gt;#include&lt;vector&gt;#include&lt;algorithm&gt;#include&lt;unordered_map&gt;using namespace std;// 单词打错了，也是不通过的vector&lt;vector&lt;string&gt;&gt; group(vector&lt;string&gt;&amp; input)&#123; // 还原本质, unordered_map&lt;string, vector&lt;string&gt;&gt; dict; // map 的底层实现是平衡树，操作的时间复杂度是logn， 所以慢，而unordered_map(hash表) 是常数 for(auto str : input) &#123; string tmp =str; sort(tmp.begin(), tmp.end()); dict[tmp].push_back(str); &#125; // 结果 vector&lt;vector&lt;string&gt;&gt; res; for(auto key: dict) res.push_back(key.second); return res;&#125;int main()&#123; int n ; cin&gt;&gt;n; vector&lt;string&gt; input; for(int i =0; i&lt;n;i++) &#123; string tmp ; cin &gt;&gt;tmp; input.push_back(tmp); &#125; vector&lt;vector&lt;string&gt;&gt; res =group(input); for(auto u: res) &#123; for(auto v: u) cout&lt;&lt; v&lt;&lt;" "; cout&lt;&lt;endl; &#125; return 0;&#125; Two sum 系列套题 有多对解，输出一对解就可以。最简单的一种情况。 12345678910111213141516171819202122232425262728293031323334353637383940#include&lt;iostream&gt;#include&lt;unordered_set&gt;#include "vector"using namespace std;#define LEN 100vector&lt;int&gt; nums(LEN);vector&lt;int&gt; find_sum(vector&lt;int&gt; &amp; nums, int target)&#123; unordered_set&lt;int&gt; hash; for(int i =0; i&lt;nums.size(); i++) &#123; if(hash.count(target -nums[i] )) return vector&lt;int&gt;&#123;target-nums[i], nums[i]&#125;; hash.insert(nums[i]); &#125; return vector&lt;int&gt;(); &#125;int main()&#123; int n , target; cin &gt;&gt;n &gt;&gt; target; for(int i =0; i&lt;n; i++) cin&gt;&gt; nums[i]; vector&lt;int&gt; res =find_sum(nums, target); for(int i =0; i&lt;res.size(); i++) cout&lt;&lt; res[i]&lt;&lt;" "; cout&lt;&lt;endl; return 0;&#125; 这个返回的是index， 而不是数值。 123456789101112131415161718192021222324252627282930#include&lt;iostream&gt;#include&lt;unordered_set&gt;#include "vector"#include "unordered_map"using namespace std;#define LEN 100vector&lt;int&gt; nums(LEN);int main()&#123; vector&lt;int&gt; nums =&#123;1, 2, 3, 4&#125;; int target =7; unordered_map&lt;int, int&gt; hash; for(int i =0; i&lt;nums.size(); i++) &#123; auto it =hash.find(target- nums[i]); if(it != hash.end()) cout &lt;&lt;vector&lt;int&gt;&#123;i, it-&gt;second&#125;[0]&lt;&lt; " "&lt;&lt; vector&lt;int&gt;&#123;i, it-&gt;second&#125;[1]&lt;&lt;endl; hash[nums[i]] =i; &#125; return 0;&#125; 逆序对 暴力求解，时间复杂度是$O(N^2)$123456789101112131415class Solution &#123;public: int InversePairs(vector&lt;int&gt; data) &#123; int n =data.size(); // 先是暴力求解 理解一下题意 int res =0; for(int i =0; i&lt;n; i++) &#123; for(int j =i+1; j&lt;n; j++) if(data[i] &gt;data[j]) res +=1; &#125; return res; &#125;&#125;; 这个在牛客网上，使用归并的方式，也是没有办法完全通过，只能保证是 50% 的case。 12345678910111213141516171819202122232425262728293031323334class Solution &#123;public: // 归并排序 时间复杂度 O(nlogn), 对于当前的区间分成左区间的结果，右区间的结果和两个区间之间的结果 // 递归的思想就是要递归跳出的条件 int merge(vector&lt;int&gt;&amp; data, int l, int r) &#123; if(l &gt;=r) return 0; int mid =l +r &gt;&gt;1; int res= merge(data, l, mid) +merge(data, mid+1, r); int i =l, j =mid+1; vector&lt;int&gt; temp; while( i&lt;=mid &amp;&amp; j&lt;=r) &#123; if(data[i]&lt; data[j]) temp.push_back(data[i++]); else &#123; res += mid -i+1; temp.push_back(data[j++]); &#125; &#125; while(i&lt;=mid) temp.push_back(data[i++]); while(j &lt;=r) temp.push_back(data[j++]); // 这个是当前区间的起点 i =l; for(auto u: temp) data[i++] =u; return res % 1000000007; &#125; int InversePairs(vector&lt;int&gt; data) &#123; return merge(data, 0, data.size()-1); &#125;&#125;;]]></content>
      <categories>
        <category>算法</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Transfer Learning]]></title>
    <url>%2F2019%2F07%2F29%2Ftransfer-learning%2F</url>
    <content type="text"><![CDATA[介绍迁移学习 和多任务学习。 迁移学习定义： 涉及一个源领域source domain和一个目标领域（target domain），在source domain学习，并把学习到的知识迁移到target domain，提升target domain的学习效果（performance）。起源： 在一些新的领域，标注数据很难获得；目标：从一个环境中学习到的知识帮助新环境的学习任务。和传统机器学习的理论区别：传统的机器学习假设训练数据与测试数据服从相同的数据分布，迁移学习不会像传统机器学习那样作同分布假设。迁移学习是指一个学习算法可以利用不同学习任务之间的共性来共享统计的优点和在任务间迁移知识。 基本上，神经网络迁移学习主要有两个应用场景：特征提取（Feature Extraction）和微调（Fine Tuning）。第一种： 特征提取：在特征提取中，可以在预先训练好的网络结构后添加一个简单的分类器，将源任务上的预先训练好的网络作为另一个目标任务的特征提取器，只对最后增加的分类器参数进行重新学习，而预先训练好的网络参数不被修改。这使得新任务的特征提取时使用的是源任务中学习到的参数，而不用重新学习所有参数。适用场景：源任务 和目标任务保持同一分布最好，因为网络参数是更加适合的。第二种：微调固定底层的参数，调整一些顶层的参数。这样做的好处可以减少训练参数的数量，同时也有助于克服过拟合现象的发生，尤其是当目标任务的数据量不足够大的时候，该方法实践起来很有效果。实际上，微调要优于特征提取，因为它能够对迁移过来的预训练网络参数进行优化，使其更加适合新的任务。 多任务学习是迁移算法的一种。 多任务学习定义：基于共享表示，把多个相关的任务放到一起进行学习的一种机器学习方法。多任务学习是相对于单任务学习的一种机器学习的方法。 例子：我们希望有一个网络模型可以将输入的人脸图像分类为男性或女性，同时还能够预测其年龄。这个案例中有两个相关的任务：一个是二元分类任务，另一个是回归任务。显而易见，这两个任务是相关的，学习其中一个任务的同时应该增强对另一个任务的理解。 共享部分学习到的是多个任务的共享表示，共享表示具有较强的抽象能力，能够适应多个不同但相关的目标任务，通常使得多任务中的主任务获得更好的泛化能力。另一方面，针对每个不同的任务都会设计具体的顶层网络结构（头），顶层网络结构用来学习如何使用共享表示来完成每个特定的任务。 单任务学习 vs 多任务学习： 单任务学习：一次只学习一个任务（task），大部分的机器学习任务都属于单任务学习。多任务学习：把多个相关（related）的任务放在一起学习，同时学习多个任务。 从图中可以发现，单任务学习时，各个task任务的学习是相互独立的，多任务学习时，多个任务之间的浅层表示共享（shared representation） 分类： 一是不同任务之间共享相同的参数（common parameter），二是挖掘不同任务之间隐藏的共有数据特征（latent feature） 多任务学习有效性的原因：（1）多人相关任务放在一起学习，有相关的部分，但也有不相关的部分。当学习一个任务（Main task）时，与该任务不相关的部分，在学习过程中相当于是噪声，因此，引入噪声可以提高学习的泛化（generalization）效果。（2）多个任务在浅层共享表示，可能削弱了网络的能力，降低网络过拟合，提升了泛化效果。]]></content>
      <categories>
        <category>Deep learning</category>
      </categories>
      <tags>
        <tag>transfer_learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python advanced skills]]></title>
    <url>%2F2019%2F07%2F27%2Fpython-learning2%2F</url>
    <content type="text"><![CDATA[介绍 python advanced skill。 Advanced FeaturesLet’s move on to advanced features. Lambda functionsA Lambda Function is a small, anonymous function — anonymous in the sense that it doesn’t actually have a name. A lambda function can take any number of arguments, but must always have only one expression: lambda 函数是一种 内联函数 inline function，调用成本小，时空开销很小。 1234x = lambda a, b : a * b print(x(5, 6)) # prints '30' # 匿名函数也是函数，调用的时候使用这样的方式 x = lambda a : a*3 + 3 print(x(3)) # prints '12' lambda表达式的基本语法如下： lambda arg1,arg2,arg3… :&lt;表达式&gt;arg1/arg2/arg3为函数的参数（函数输入），表达式相当于函数体，运算结果是表达式的运算结果。 Python 中定义函数有两种方法，一种是用常规方式 def 定义，函数要指定名字，第二种是用 lambda 定义，不需要指定名字，称为 Lambda 函数。lambda 函数的使用场景：当逻辑比较简单的时候，没有必要起个名字或者说取一个优雅的名字是很费劲的事情。 下面两种定义是等价的。 1234567def add1(x, y): return x+yprint(add1(1, 2))add2 = lambda x, y: x+yprint(add2(1,2)) 如果是使用 匿名函数（lambda 表达式），还有有一个名字（如上所示），有点画蛇添足。通常是直接使用 lambda 表达式的。 mapmap()传入的第一个参数是f，即函数对象本身。由于结果r是一个Iterator，Iterator是惰性序列，因此通过list()函数让它把整个序列都计算出来并返回一个list。 12r = map(f, [1, 2, 3, 4, 5, 6, 7, 8, 9])list(r) reduce（找找这种 reduce的感觉）reduce把一个函数作用在一个序列[x1, x2, x3, …]上，这个函数必须接收两个参数，reduce把结果继续和序列的下一个元素做累积计算，其效果就是： 1reduce(f, [x1, x2, x3, x4]) = f(f(f(x1, x2), x3), x4) 如果要把序列[1, 3, 5, 7, 9]变换成整数13579，reduce就可以派上用场： 123def fn(x, y): return x*10 +yreduce(fn, [1, 3, 5, 7, 9]) # 输出是 13579 filter和map()类似，filter()也接收一个函数和一个序列。和map()不同的是，filter()把传入的函数依次作用于每个元素，然后根据返回值是True还是False决定保留还是丢弃该元素。 1234def is_odd(n): return n % 2 == 1list(filter(is_odd, [1, 2, 4, 5, 6, 9, 10, 15]))# 结果: [1, 5, 9, 15] 函数式编程 函数式编程（Functional Programming）是一种编程范式，和面向过程，面向对象编程相同类别的。 在计算机的层次上，CPU执行的是加减乘除的指令代码，以及各种条件判断和跳转指令，所以，汇编语言是最贴近计算机的语言。而计算则指数学意义上的计算，越是抽象的计算，离计算机硬件越远。对应到编程语言，就是越低级的语言，越贴近计算机，抽象程度低，执行效率高，比如C语言；越高级的语言，越贴近计算，抽象程度高，执行效率低，比如Lisp语言。 函数式编程特点 函数式编程就是一种抽象程度很高的编程范式，纯粹的函数式编程语言编写的函数没有变量，因此，任意一个函数，只要输入是确定的，输出就是确定的，这种纯函数我们称之为没有副作用。而允许使用变量的程序设计语言，由于函数内部的变量状态不确定，同样的输入，可能得到不同的输出，因此，这种函数是有副作用的。 函数式编程的一个特点就是，允许把函数本身作为参数传入另一个函数，还允许返回一个函数！ 所谓”副作用”（side effect），指的是函数内部与外部互动（最典型的情况，就是修改全局变量的值），产生运算以外的其他结果。函数式编程强调没有”副作用”，意味着函数要保持独立，所有功能就是返回一个新的值，没有其他行为，尤其是不得修改外部变量的值。 引用透明（Referential transparency），指的是函数的运行不依赖于外部变量或”状态”，只依赖于输入的参数，任何时候只要参数相同，引用函数所得到的返回值总是相同的。有了前面的第三点和第四点，这点是很显然的。其他类型的语言，函数的返回值往往与系统状态有关，不同的状态之下，返回值是不一样的。这就叫”引用不透明”，很不利于观察和理解程序的行为。 函数式编程的几个技术 map &amp; reduce ：这个技术不用多说了，函数式编程最常见的技术就是对一个集合做Map和Reduce操作。 pipeline：这个技术的意思是，把函数实例成一个一个的action，然后，把一组action放到一个数组或是列表中，然后把数据传给这个action list，数据就像一个pipeline一样顺序地被各个函数所操作，最终得到我们想要的结果。 递归最大的好处就简化代码，他可以把一个复杂的问题用很简单的代码描述出来。注意：递归的精髓是描述问题，而这正是函数式编程的精髓。 函数式编程的一些其他好处 lazy evaluation 惰性求值。这个需要编译器的支持。表达式不在它被绑定到变量之后就立即求值，而是在该值被取用的时候求值，也就是说，语句如x:=expression determinism 确定性.所谓确定性的意思就是像数学那样 f(x) = y ，这个函数无论在什么场景下，都会得到同样的结果，这个我们称之为函数的确定性。 函数式编程的特点一：不依赖于外部的数据，而且也不改变外部数据的值，而是返回一个新的值给你。使用下面的例子 非函数式的例子：1234int cnt;void increment()&#123; cnt++;&#125; 函数式的例子：123int increment(int cnt)&#123; return cnt+1;&#125; 特点二：把函数当成变量来用，关注于描述问题而不是怎么实现，这样可以让代码更易读。 123name_len = map(len, ["hao", "chen", "coolshell"])print name_len# 输出 [3, 4, 9] Python对函数式编程提供部分支持。由于Python允许使用变量，因此，Python不是纯函数式编程语言。 尽管 Python 算不上是一门纯函数式编程语言，但它本身提供了很多函数式编程的特性，像 map、reduce、filter、sorted 这些函数都支持函数作为参数，lambda 函数就可以应用在函数式编程中。以 sorted() 函数为例： 123list1 =[3, 4, -4, -1, 0, -2, -6]list1.sort(key =lambda x: abs(x))print(list1) 也是可以使用传统的方式进行书写， 只是看起来不够 pythonic 而已123456def my_add(n): return lambda x: x-nadd_3 =my_add(3) # add_3 是一种 lambda 函数print(add_3) # 定义的时候把 n初始化为了3print("*****")print(add_3(4)) # 4对应着x 输出是1， 1234def my_add(n): return lambda x: x+nadd_3 =my_add(3) # add_3 是一种 lambda 函数print(add_3(4)) # 4对应着n 可以换成常规的函数表示 这种函数的嵌套也是可以通过装饰器实现的。 1234567def my_add(n): def wrapper(x): return x +n return wrapperadd_5 =my_add(5)print(add_5(2)) 但是在处理比较复杂的逻辑时候，不建议使用 lambda 表达式。 顺便加上 list comprehension 的理解。函数式编程， 常用的表达是 filter, map 之类的函数。下面的式子是等价的。 12odds =map(lambda n : n*2, filter(lambda n: n %2 ==1, numbers))odds =[ n*2 for n in numbers if n %2 ==1] Nested Loops （二重循环的支持） 123456flattened =[]for row in matrix: for n in row: flattened.append(n)# 注意遍历的顺序， 只是把二重循环 flatten 的写法, not readable 但是得理解flattened =[n for row in matrix for n in row] # 如果只是遍历这样的简单的操作，那么这种写法是非常高效的 对于 set 和 dictionary 的支持123456789101112# 对于 set 的支持letters =set()for w in words: letters.add(w[0])letters =&#123; w[0] for w in words&#125;#对于dictionary 的支持 （这个是非常简单的操作， key value 倒置的操作）dict1 =&#123;'a':1, 'b':2, 'c':3&#125;#对于dictionary 的支持flipped =&#123; value: key for key,value in dict1.items()&#125;print(flipped) 闭包的概念 闭包(closure)是函数式编程的重要的语法结构。函数式编程是一种编程范式 (而面向过程编程和面向对象编程也都是编程范式)。在面向过程编程中，我们见到过函数(function)；在面向对象编程中，我们见过对象(object)。函数和对象的根本目的是以某种逻辑方式组织代码，并提高代码的可重复使用性(reusability)。闭包也是一种组织代码的结构，它同样提高了代码的可重复使用性。可以把闭包理解成轻量级的接口封装。 在计算机科学中，闭包（Closure）是词法闭包（Lexical Closure）的简称，是引用了自由变量的函数。这个被引用的自由变量将和这个函数一同存在，即使已经离开了创造它的环境也不例外。 函数内的函数以及其自由变量形成闭包。也即闭包是一种保留定义函数时存在的自由变量的绑定的函数～这样在调用函数时，绑定的自由变量依旧可用。 要形成闭包，首先得有一个嵌套的函数，即函数中定义了另一个函数，闭包则是一个集合，它包括了外部函数的局部变量，这些局部变量在外部函数返回后也继续存在，并能被内部函数引用。 按变量的作用域进行分类，Python 中的变量可分为「全局变量」、「局部变量」以及「自由变量」。一般而言，Python 中使用变量前不需要声明变量，但假定在函数体中赋值的变量为局部变量～除非显示使用 global 将在函数中赋值的变量声明为全局变量！ 而自由变量则是存在于嵌套函数中的一个概念～定义在其他函数内部的函数被称之为嵌套函数 nested function ，嵌套函数可以访问封闭范围内（外部函数）的变量。嵌套函数不可以在函数外直接访问。 在 Python 中，非本地变量默认仅可读取，在修改时必须显式指出其为非本地变量～自由变量 nonlocal，全局变量 global。 1234567def line_conf(): b = 15 def line(x): return 2*x-b return line # return a function objectmy_line = line_conf()print(my_line(10)) 在 Python 3 中引入了一个关键词 nonlocal 解决了这一个问题： 123456789101112something =0 # 这个是函数外部的变量def get_avg(): scores = 0 # 将外部临时变量由 list 改为一个 整型数值 count = 0 # 同时新增一个变量，记录个数 def inner_count_avg(val): # 内部函数，用于计算平均值 nonlocal count, scores # 这个表示想要修改外面函数外面的值 scores += val # 使用外部函数的临时变量 count += 1 return scores / count # 返回计算出的平均值 return inner_count_avg # 外部函数返回内部函数引用avg = get_avg()print(avg(10)) # 报错 浅显理解 Python 闭包 给出了一个例子说明 当从命名空间删除函数名之后，仍然是可以访问其变量。 使用闭包注意的地方 闭包只能访问，无法修改外部函数的局部变量 闭包和装饰器 事实上，装饰器就是一种的闭包的应用，只不过其传递的是函数。 闭包的作用： （从这个角度上看闭包更像是一种小型的类，实现了变量和方法的绑定）闭包的最大特点是可以将父函数的变量与内部函数绑定，并返回绑定变量后的函数（也即闭包），此时即便生成闭包的环境（父函数）已经释放，闭包仍然存在，这个过程很像类（父函数）生成实例（闭包），不同的是父函数只在调用时执行，执行完毕后其环境就会释放，而类则在文件执行时创建，一般程序执行完毕后作用域才释放，因此对一些需要重用的功能且不足以定义为类的行为，使用闭包会比使用类占用更少的资源，且更轻巧灵活。 当然，闭包在爬虫以及web应用中都有很广泛的应用，并且闭包也是装饰器的基础。 MapsMap() is a built-in Python function used to apply a function to a sequence of elements like a list or dictionary. It’s a very clean and most importantly readable way to perform such an operation.相对于 lambda, map 使用的频率更少了。 最后返回的是一个list。 这个函数的强大之处在于，对于每一个变量都是应用这个function的， 属于element-wise 的操作。 1234567891011def square_it_func(a): return a * ax = map(square_it_func, [1, 4, 7])print(x) # prints '[1, 16, 49]'def multiplier_func(a, b): return a * bx = map(multiplier_func, [1, 4, 7], [2, 5, 8])print(x) # prints '[2, 20, 56]' FilteringThe Filter built-in function is quite similar to the Map function in that it applies a function to a sequence (list, tuple, dictionary). The key difference is that filter() will only return the elements which the applied function returned as True. 只是返回符合某种条件的element 1234567891011 # Our numbersnumbers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]# Function that filters out all numbers which are odddef filter_odd_numbers(num): if num % 2 == 0: return True else: return Falsefiltered_numbers = filter(filter_odd_numbers, numbers)print(filtered_numbers)# filtered_numbers = [2, 4, 6, 8, 10, 12, 14] 123456from itertools import *def check_for_drop(x): print ('Checking: ', x) return (x &gt; 5)for i in dropwhile(check_for_drop, [2, 4, 6, 8, 10, 12]): print ('Result: ', i) Decorator简单地说，decorator就像一个wrapper一样，在函数执行之前或者之后修改该函数的行为，而无需修改函数本身的代码，这也是修饰器名称的来由。总的来说，decorator在你希望在不修改函数本身代码的前提下扩展函数的功能时非常有用。 （decorator 就类似一种嵌套函数， 执行的时候先是执行里面，后执行外面） python 中函数的”人设“, function 也是一种对象，内部函数可以访问外部的 function的变量，但是权限是”只读“。一个函数可以输入一个函数也可以返回一个函数。装饰器使得代码可以复用。在调用的使用，先调用装饰器，然后调用函数本身。 上面的过程可以简化为： 1234567891011def get_text(name): return "lorem ipsum, &#123;0&#125; dolor sit amet".format(name)def p_decorate(func): def func_wrapper(name): return "&lt;p&gt;&#123;0&#125;&lt;/p&gt;".format(func(name)) return func_wrappermy_get_text = p_decorate(get_text)print(my_get_text("John")) 输出：1&lt;p&gt;Outputs lorem ipsum, John dolor sit amet \&lt;/p&gt; 这就是我们的第一个修饰器。一个函数接收另一个函数作为参数，并且产生一个新的函数，注意观察是如何调用和 调用的顺序，体会 decorator 产生的背景，是可以简化这个调用过程的。 下面代码和上面是相同的功能，p_decorate 就像是 function get_text() 的一个外套， 其作为一种输入到 p_decorate() 中。装饰器首先执行的是装饰器部分，本函数作为一个变量使用，最后返回的是结果。 12345678910def p_decorate(func): def func_wrapper(name): return "&lt;p&gt;&#123;0&#125;&lt;/p&gt;".format(func(name)) return func_wrapper@p_decoratedef get_text(name): return "lorem ipsum, &#123;0&#125; dolor sit amet".format(name)print (get_text("John")) 输出结果： 1&lt;p&gt;lorem ipsum, John dolor sit amet&lt;/p&gt; 在给个例子，理解调用过程。 123456789def hello_decorator(original_fn): def decorator_fn(): print("Hello from new") original_fn() # original function must be invoked return decorator_fn@hello_decoratordef hello(): print("Hello from original") 输出结果：12Hello from newHello from original 另外，一个函数是可以添加多个 修饰器的，并且修饰器的顺序也是有关系的。一个函数还可以同时定义多个装饰器，比如： 12345@a@b@cdef f (): pass 它的执行顺序是从里到外，最先调用最里层的装饰器，最后调用最外层的装饰器，它等效于1f = a(b(c(f))) 执行顺序是先执行里面的，但是里面的func 作为一种输入到下一个函数中，所以最后先执行的还是“外面”函数。类似不断的递归进去的感觉。 感觉装饰器很难的原因在于没有理清它的逻辑关系，本质上装饰器也是函数，但它是对核心程序的闭包封装，在原有的基础上增加更多的功能。细细回顾几遍上面的例子能够加深对装饰器的理解。 使用修饰器实现单例模式 设计模式分成单例模式和多例模式，对于单例模式对于一个类只能实现一个实例；多例模式可以实现多个实例。单例是一种设计模式，应用该模式的类只会生成一个实例。这种方式是可以代替全局变量的。比如一些配置、日志等只需要初始化一次的文件，就可以使用这种方式。 1234567891011121314151617# 函数修饰器实现单例def singleton(cls): _instance =&#123;&#125; def inner(): if cls not in _instance: _instance[cls] =cls return _instance[cls] # 函数调用 return inner@singletonclass Cls(object): def __init__(self): passcls1 =Cls()cls2 =Cls()print(id(cls1) ==id(cls2)) 常见的自带的修饰器。 Python中 的 @classmethod 和 @staticmethod 在对象的实例方法中，self 参数是类实例对象本身，我们可以用它来对实例数据进行一些操作。@classmethod 方法也有一个强制性的第一个参数，它表示的是未实例化的类本身，而非类的实例。 1234567class Student(object): @classmethod def from_string(cls, name_str): first_name, last_name = map(str, name_str.split(' ')) student = cls(first_name, last_name) return studentscott = Student.from_string('Scott Robinson') @staticmethod 装饰器 @staticmethod 装饰类似于@classmethod ，它能够从一个非实例类对象被调用，但是没有传递 cls 参数。 1234567class Student(object): @staticmethod def is_full_name(name_str): names = name_str.split(' ') return len(names) &gt; 1Student.is_full_name('Scott Robinson') # True Student.is_full_name('Scott') # False 由于没有 self 传递任何对象，这意味着此装饰器方法无法访问任何实例数据，并且也无法在实例化对象上调用此方法。这些类型的方法通常不是为了创建/实例化对象，它们是为了处理一些与类本身有关的逻辑。 List Comprehension常见的几种形式：(An iterable is something you can loop over) list comprehensions vs loops: list comprehensions are more efficient both computationally and coding space Every list comprehension can be rewritten as a for loop, but not every for loop can be rewritten as a list comprehension. 从优化的角度 list comprehensions是优于 for loop 中的if else 操作的。因为前者是 predicatable pattern 是可以预测的。However, keep in mind that list comprehensions are faster because they are optimized for the Python interpreter to spot a predictable pattern during looping. a small code demo:在于使用功能 timeit libary 进行函数的计时比较。12345678910111213import timeitdef squares(size): result = [] for number in range(size): result.append(number * number) return resultdef squares_comprehension(size): return [number * number for number in range(size)] print(timeit.timeit("squares(50)", "from __main__ import squares", number=1_000_000))print(timeit.timeit("squares_comprehension(50)", "from __main__ import squares_comprehension", number=1_000_000)) more complex list comprehensions: 这种if 的写法 是两个进行并列的。其实可以写成 123456numbers = [1, 2, 3, 4, 5, 6, 18, 20]squares = [number for number in numbers if number % 2 == 0 if number % 3 == 0]# orsquares = [number for number in numbers if number % 2 == 0 and number % 3 == 0]print(squares)# output: [6, 18] 在 output expression 中，也是可以使用 if else 进行进一步输出筛选。12345numbers = [1, 2, 3, 4, 5, 6, 18, 20] squares = ["small" if number &lt; 10 else "big" for number in numbers if number % 2 == 0 if number % 3 == 0] print(squares)ouput: ['small', 'big'] converting nested loops into list comprehension代码功能： 都是把二维的 matrix 转成了一个 list （flattened）123456matrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]flattened = []for row in matrix: for item in row: flattened.append(item)print(flattened) 注意这个顺序，先是row in matrix 然后是 item in row.（对于这个顺序的理解，其实是可以看成二重循环的） 123matrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]flattened = [item for row in matrix for item in row] print(flattened) ouput matric from nested list comprehensions:12matrix = [[item for item in range(5)] for row in range(3)]print(matrix) 对于 dictionary 的支持： 主要是 dict1.items() 和 key, value 的使用。还有一种实现方式 zip(list1, list2)123prices = &#123;"beer": 2, "fish": 5, "apple": 1&#125;float_prices = &#123;key:float(value) for key, value in prices.items()&#125;print(float_prices) 从代码的角度，可以看出，操作和最后的返回的形式是没有很大的关系，上面是 [], 这个是 {}, 分别对应的是 list 和 set 两种不同的格式。123numbers = [10, 10, 20, 30, 12, -20, 0, 1]unique_squares = &#123;number**2 for number in numbers&#125;print(unique_squares) ContainersPython includes several built-in container types: lists, dictionaries, sets, and tuples. lists A list is the Python equivalent of an array, but is resizeable and can contain elements of different types: 12x = xs.pop() # Remove and return the last element of the listprint x, xs In addition to accessing list elements one at a time, Python provides concise syntax to access sublists; this is known as slicing: 123456789nums = range(5) # range is a built-in function that creates a list of integersprint nums # Prints "[0, 1, 2, 3, 4]"print nums[2:4] # Get a slice from index 2 to 4 (exclusive); prints "[2, 3]"print nums[2:] # Get a slice from index 2 to the end; prints "[2, 3, 4]"print nums[:2] # Get a slice from the start to index 2 (exclusive); prints "[0, 1]"print nums[:] # Get a slice of the whole list; prints ["0, 1, 2, 3, 4]"print nums[:-1] # Slice indices can be negative; prints ["0, 1, 2, 3]"nums[2:4] = [8, 9] # Assign a new sublist to a sliceprint nums # Prints "[0, 1, 8, 9, 4]" dictionary A dictionary stores (key, value) pairs, similar to a Map in Java or an object in Javascript. You can use it like this: 123d = &#123;'cat': 'cute', 'dog': 'furry'&#125; # Create a new dictionary with some dataprint d.get('monkey', 'N/A') # Get an element with a default; prints "N/A"print d.get('fish', 'N/A') # Get an element with a default; prints "wet" sets A set is an unordered collection of distinct elements. As a simple example, consider the following: 123456789101112animals = &#123;'cat', 'dog'&#125;animals.add('fish') # Add an element to a setprint len(animals) # Number of elements in a set;animals.remove('cat') # Remove an element from a setprint len(animals)animals = &#123;'cat', 'dog', 'fish'&#125;for idx, animal in enumerate(animals): print '#%d: %s' % (idx + 1, animal)# Prints "#1: fish", "#2: dog", "#3: cat" tuples A tuple is an (immutable) ordered list of values. A tuple is in many ways similar to a list; one of the most important differences is that tuples can be used as keys in dictionaries and as elements of sets, while lists cannot. Here is a trivial example: 12345d = &#123;(x, x + 1): x for x in range(10)&#125; # Create a dictionary with tuple keyst = (5, 6) # Create a tupleprint type(t)print d[t] print d[(1, 2)] 元组类似于列表，是一个基于位置的有序对象集合，但是元组一旦创建之后就不能更改，因此列表中修改元素的操作对于元组都不适用。 元组和其他不可变量类似于其他语言中“常量”的声明，它的不可变性提供了某种一致性，这样可以确保元组在程序中不会被另一个引用修改。Mark Lutz——《Learning Python》中文版 注意，dict内部存放的顺序和key放入的顺序是没有关系的。和list比较，dict有以下几个特点： 查找和插入的速度极快，不会随着key的增加而增加； 需要占用大量的内存，内存浪费多。而list相反： 查找和插入的时间随着元素的增加而增加； 占用空间小，浪费内存很少。所以，dict是用空间来换取时间的一种方法。 dict可以用在需要高速查找的很多地方，在Python代码中几乎无处不在，正确使用dict非常重要，需要牢记的第一条就是dict的key必须是不可变对象。这是因为dict根据key来计算value的存储位置，如果每次计算相同的key得出的结果不同，那dict内部就完全混乱了。这个通过key计算位置的算法称为哈希算法（Hash）。要保证hash的正确性，作为key的对象就不能变。在Python中，字符串、整数等都是不可变的，因此，可以放心地作为key。而list是可变的，就不能作为key。 使用zip函数, 把key和value的list组合在一起, 再转成字典(dict). 1234keys = ['a', 'b', 'c']values = [1, 2, 3]dictionary = dict(zip(keys, values))print dictionary 创建同样大小的list和tuple，可以看到list的时间开销几乎是tuple的10倍。对两个大表进行遍历，tuple与list速度相似。储存同样元素的list和tuple， list有更多空间开销。tuple是 immutable, 所以它是可哈希的(hashable)的。这使得tuple可以作为dict的key，或者扔进set里，list则不行。 在Python中：list、set和dictionary 都是可改变的，比如可以通过list.append()，set.remove()，dict[‘key’] = value对其进行修改，所以它们都是不可哈希的；而tuple和string是不可变的，只可以做复制或者切片等操作，所以它们就是可哈希的。 因为list 中没有 __hash__ 这个函数，所以不是hashable的，但是为什么呢？这个解释可能更加本质深入一些：Python 为什么list不能作为字典的key？ namedtuple tuple是不可变序列，当你不希望外界可以随意的改变你的函数返回值的时候，不妨将你的返回值以tuple的形式返回，tuple还可以做为字典的key，这些都是tuple的独到之处，此外，由于存储的方式不同，相同元素的tuple 要比list更快，使用的内存更少。tuple虽然有这么多优点，但是呢，在使用的时候，你不得不用下角标来访问它的元素，这样对于代码的可读性来说是一种折损。namedtuple弥补了tuple的这一缺陷，使得你可以像使用对象属性那样去访问数据. namedtuple能够用来创建类似于元祖的数据类型，除了能够用索引来访问数据，能够迭代，更能够方便的通过属性名来访问数据。这个相当于是 tuple 和dictionary 的综合体。主要这个值是只读的，不能修改。 12345from collections import namedtupleAnimals =nametuple('Animal', 'name age type')big_yellow =Animals(name ='big_yellow', age=3, type=‘dog’)// 访问的话使用 big_yellow.name numpyArrays A numpy array is a grid of values, all of the same type, and is indexed by a tuple of nonnegative integers. 1234a = np.array([1, 2, 3]) # Create a rank 1 arrayprint type(a), a.shape, a[0], a[1], a[2]a[0] = 5 # Change an element of the arrayprint a 1234567891011121314a = np.zeros((2,2)) # Create an array of all zerosprint ab = np.ones((1,2)) # Create an array of all onesprint bc = np.full((2,2), 7) # Create a constant arrayprint cd = np.eye(2) # Create a 2x2 identity matrixprint de = np.random.random((2,2)) # Create an array filled with random valuesprint e Boolean array indexing: Boolean array indexing lets you pick out arbitrary elements of an array. Frequently this type of indexing is used to select the elements of an array that satisfy some condition. Here is an example: 12345678910import numpy as npa = np.array([[1,2], [3, 4], [5, 6]])bool_idx = (a &gt; 2) # Find the elements of a that are bigger than 2; # this returns a numpy array of Booleans of the same # shape as a, where each slot of bool_idx tells # whether that element of a is &gt; 2.print bool_idx Array math Basic mathematical functions operate elementwise on arrays, and are available both as operator overloads and as functions in the numpy module: 12345678910111213141516171819202122232425262728x = np.array([[1,2],[3,4]], dtype=np.float64)y = np.array([[5,6],[7,8]], dtype=np.float64)# Elementwise sum; both produce the arrayprint x + yprint np.add(x, y)# Elementwise difference; both produce the arrayprint x - yprint np.subtract(x, y)# Elementwise product; both produce the arrayprint x * yprint np.multiply(x, y)# Elementwise division; both produce the array# [[ 0.2 0.33333333]# [ 0.42857143 0.5 ]]print x / yprint np.divide(x, y)# Elementwise square root; produces the array# [[ 1. 1.41421356]# [ 1.73205081 2. ]]print np.sqrt(x) Note that unlike MATLAB, * is elementwise multiplication, not matrix multiplication. We instead use the dot function to compute inner products of vectors, to multiply a vector by a matrix, and to multiply matrices. dot is available both as a function in the numpy module and as an instance method of array objects: 1234567891011121314151617x = np.array([[1,2],[3,4]])y = np.array([[5,6],[7,8]])v = np.array([9,10])w = np.array([11, 12])# Inner product of vectors; both produce 219print v.dot(w)print np.dot(v, w)x = np.array([[1,2],[3,4]])print np.sum(x) # Compute sum of all elements; prints "10"print np.sum(x, axis=0) # Compute sum of each column; prints "[4 6]"print np.sum(x, axis=1) # Compute sum of each row; prints "[3 7]"print xprint x.T Broadcasting Broadcasting is a powerful mechanism that allows numpy to work with arrays of different shapes when performing arithmetic operations. Frequently we have a smaller array and a larger array, and we want to use the smaller array multiple times to perform some operation on the larger array. For example, suppose that we want to add a constant vector to each row of a matrix. We could do it like this: 12345678import numpy as np# We will add the vector v to each row of the matrix x,# storing the result in the matrix yx = np.array([[1,2,3], [4,5,6], [7,8,9], [10, 11, 12]])v = np.array([1, 0, 1])y = x + v # Add v to each row of x using broadcastingprint y 12345678# Compute outer product of vectorsv = np.array([1,2,3]) # v has shape (3,)w = np.array([4,5]) # w has shape (2,)# To compute an outer product, we first reshape v to be a column# vector of shape (3, 1); we can then broadcast it against w to yield# an output of shape (3, 2), which is the outer product of v and w:print np.reshape(v, (3, 1)) * w (对于第 4 点还是挺有趣的， 多看看，其实不是很懂)Broadcasting two arrays together follows these rules: If the arrays do not have the same rank, prepend the shape of the lower rank array with 1s until both shapes have the same length. The two arrays are said to be compatible in a dimension if they have the same size in the dimension, or if one of the arrays has size 1 in that dimension. The arrays can be broadcast together if they are compatible in all dimensions. After broadcasting, each array behaves as if it had shape equal to the elementwise maximum of shapes of the two input arrays. In any dimension where one array had size 1 and the other array had size greater than 1, the first array behaves as if it were copied along that dimension 好好理解这几个例子： 123456789101112131415161718192021222324252627282930313233343536373839404142# Compute outer product of vectorsv = np.array([1,2,3]) # v has shape (3,)w = np.array([4,5]) # w has shape (2,)# To compute an outer product, we first reshape v to be a column# vector of shape (3, 1); we can then broadcast it against w to yield# an output of shape (3, 2), which is the outer product of v and w:print np.reshape(v, (3, 1)) * w# Add a vector to each row of a matrixx = np.array([[1,2,3], [4,5,6]])# x has shape (2, 3) and v has shape (3,) so they broadcast to (2, 3),# giving the following matrix:print x + v# Add a vector to each column of a matrix# x has shape (2, 3) and w has shape (2,).# If we transpose x then it has shape (3, 2) and can be broadcast# against w to yield a result of shape (3, 2); transposing this result# yields the final result of shape (2, 3) which is the matrix x with# the vector w added to each column. Gives the following matrix:print (x.T + w).T# Another solution is to reshape w to be a row vector of shape (2, 1);# we can then broadcast it directly against x to produce the same# output.print x + np.reshape(w, (2, 1)):# Multiply a matrix by a constant:# x has shape (2, 3). Numpy treats scalars as arrays of shape ();# these can be broadcast together to shape (2, 3), producing the# following array:print x * 2 Matplotlib 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647import matplotlib.pyplot as plt# Compute the x and y coordinates for points on a sine curvex = np.arange(0, 3 * np.pi, 0.1)y = np.sin(x)# Plot the points using matplotlibplt.plot(x, y)# 有 x y 轴的y_cos = np.cos(x)# Plot the points using matplotlibplt.plot(x, y_sin)plt.plot(x, y_cos)plt.xlabel('x axis label')plt.ylabel('y axis label')plt.title('Sine and Cosine')plt.legend(['Sine', 'Cosine'])# subplots 的图像# Compute the x and y coordinates for points on sine and cosine curvesx = np.arange(0, 3 * np.pi, 0.1)y_sin = np.sin(x)y_cos = np.cos(x)# Set up a subplot grid that has height 2 and width 1,# and set the first such subplot as active.plt.subplot(2, 1, 1)# Make the first plotplt.plot(x, y_sin)plt.title('Sine')# Set the second subplot as active, and make the second plot.plt.subplot(2, 1, 2)plt.plot(x, y_cos)plt.title('Cosine')# Show the figure.plt.show()]]></content>
      <categories>
        <category>CS基础</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Naive Bayes Classifier]]></title>
    <url>%2F2019%2F07%2F22%2Fnaive-bayes%2F</url>
    <content type="text"><![CDATA[从贝叶斯定理到 naive bayes 分类器，最后说明一下其应用和优缺点。 在所有的机器学习分类算法中，朴素贝叶斯和其他绝大多数的分类算法都不同。对于大多数的分类算法，比如决策树,KNN,逻辑回归，支持向量机等，他们都是判别方法，也就是直接学习出特征输出$ Y$和特征 $X $之间的关系，要么是决策函数 $𝑌=𝑓(𝑋) $（比如支持向量机）,要么是条件分布 $𝑃(𝑌|𝑋) $（比如逻辑回归）。但是朴素贝叶斯却是生成方法，也就是直接找出特征输出 $Y $和特征 $X $的联合分布 $𝑃(𝑋,𝑌) $,然后用 $𝑃(𝑌|𝑋)= \frac{𝑃(𝑋,𝑌)}{𝑃(𝑋) }$得出。 朴素贝叶斯贝叶斯学派的思想可以概括为先验概率+数据=后验概率。也就是说我们在实际问题中需要得到的后验概率，可以通过先验概率和数据一起综合得到。先验概率就是我们对于数据所在领域的历史经验，但是这个经验常常难以量化或者模型化，于是贝叶斯学派大胆的假设先验分布的模型，比如正态分布，beta分布等。（因为在没有计算之前就认为数据服从某个分布，所以被称为先验概率） 贝叶斯公式： $$P(Y | X)=\frac{P(X | Y) P(Y)}{P(X)}$$ 先验概率 $P(X) $：先验概率是指根据以往经验和分析得到的概率。 后验概率 $P(Y|X) $：事情已经发生，要求这件事情发生的原因是由某个因素引起的可能性的大小，后验分布 $P(Y|X) $表示事件 $X $已经发生的前提下，事件 $Y $发生的概率，叫做事件 $X $发生下事件 $Y $的条件概率。（执果索因） 后验概率 $P(X|Y) $：在已知Y发生后X的条件概率，也由于知道 $Y $的取值而被称为 $X $的后验概率 朴素：朴素贝叶斯算法是假设各个特征之间相互独立，也是朴素这词的意思那么贝叶斯公式中的 $P(X|Y) $可写成： $$P(X | Y)=P\left(x_{1} | Y\right) P\left(x_{2} | Y\right) \cdots P\left(x_{n} | Y\right)$$ 全概率公式：$$P(X)=\sum_{k} P\left(X | Y=Y_{k}\right)$$其中 $\sum_{k} P(Y_k) =1$ 所以，将上面的式子整合起来，得到朴素贝叶斯公式： $$P(Y | X)=\frac{P(X | Y_k) P(Y_k)}{\sum_{k} P(X | Y=Y_{k})}$$ 三种常见的贝叶斯模型上文提到了先验概率模型，这里主要介绍多项式模型（MultinomialNB）、高斯模型（GaussianNB）和伯努利模型（BernoulliNB）。 多项式模型（MultinomialNB） 多项式朴素贝叶斯常用语文本分类，特征是单词，值时单词出现的次数。多项式模型在计算先验概率 $P(Y_k) $和和条件概率 $P(X_i|Y_k) $时，会做出一些平滑处理（拉普拉斯平滑），具体公式为：$$P\left(Y_{k}\right)=\frac{N_{Y_{k}}+\alpha}{N+K \alpha}$$ $N$：样本数 $N_{Y_k} $：类别为$Y_k$的样本数 $K$：总的类别个数 $\alpha$：平滑值 $$P\left(x_{i} | Y_{k}\right)=\frac{N_{Y_{k}, x_{i}}+\alpha}{N_{Y_{k}}+n \alpha}$$ $N_{Y_{k}, x_{i}}$：类别为$Y_k $，且特征为$X_i$的样本数 $n$：特征$X_i $可以选择的数量 高斯模型（GaussianNB） 当特征是连续变量的时候，假设特征分布为正态分布，根据样本算出均值和方差，再求得概率。 可以参考这里. 伯努利模型（BernoulliNB）伯努利模型适用于离散特征的情况，伯努利模型中每个特征的取值只能是1和0。 可以参考这里 朴素贝叶斯分类的原理和流程总体的公式：$$ p(类别 | 特征) = \frac {p(类别) * p特征|类别）}{p(特征)}$$ 设特征 $x = { a _ { 1 } , a _ { 2 } , \ldots , a _ { m } }$， 其中 x 是一条数据， $a_i$ 是一个特征属性。 有类别信息 $C = { y _ { 1 } , y _ { 2 } , \ldots , y _ { n } }$ 计算 $P ( y _ { 1 } | x ) , P ( y _ { 2 } | x ) , \ldots , P ( y _ { n } | x )$ 如果 $P ( y _ { k } | x ) = \max { P ( y _ { 1 } | x ) , P ( y _ { 2 } | x ) , \ldots , P ( y _ { n } | x ) }$， 那么 $x \in y _ { k }$. 所以现在的关键步骤是如何计算第 3 步骤中的各个条件概率。可以这样做， 找到一个已知分类的待分类项集合，这个集合叫做训练样本集。 统计得到在各类别下各个特征属性的条件概率估计 $P \left( a _ { 1 } | y _ { 1 } \right) , P \left( a _ { 2 } | y _ { 1 } \right) , \ldots , P \left( a _ { m } | y _ { 1 } \right) ;P \left( a _ { 1 } | y _ { 2 } \right) , P \left( a _ { 2 } | y _ { 2 } \right) , \ldots , P \left( a _ { m } | y _ { 2 } \right) ;\ldots ; P \left( a _ { 1 } | y _ { n } \right) , P \left( a _ { 2 } | y _ { n } \right) , \ldots , P \left( a _ { m } | y _ { n } \right)$ 如果各个特征属性是条件独立的，则根据贝叶斯定理有如下推导 $$P \left( y _ { i } | x \right) = \frac { P \left( x | y _ { i } \right) P \left( y _ { i } \right) } { P ( x ) }$$ 因为分母对于所有类别为常数，因为我们只要将分子最大化皆可。又因为各特征属性是条件独立的，所以有：$$P \left( x | y _ { i } \right) P \left( y _ { i } \right) = P \left( a _ { 1 } | y _ { i } \right) P \left( a _ { 2 } | y _ { i } \right) \ldots P \left( a _ { m } | y _ { i } \right) P \left( y _ { i } \right) = P \left( y _ { i } \right) \prod _ { j = 1 } ^ { m } P \left( a _ { j } | y _ { i } \right)$$ 如果再计算过程中某个概率值为0，那么是可以考虑拉普拉斯平滑。两个概率计算公式，分子和分母都分别加上一个常数，就可以避免这个问题。 应用朴素贝叶斯的思想基础是这样的：对于给出的待分类项，求解在此项出现的条件下各个类别出现的概率，哪个最大，即认为此待分类项属于哪个类别。适用场景： 算法比较简单，在大样本下会有比较好的表现 对缺省数据不太敏感 具有支持增量式训练的能力（不借助于旧有训练数据，每一组新的训练数据都有可能引起概率值的变化，而如决策树和支持向量机，则需要我们一次性将整个数据集都传给它们。）对于一个如垃圾邮件过滤这样的应用程序而言，支持增量式训练的能力是非常重要的，因为过滤程序时常要对新到的邮件进行训练，然后必须即可进行相应的调整；更何况，过滤程序也未必有权限访问已经收到的所有邮件信息。 缺点： 不适合输入的向量有很强的特征关联的场景 无法处理基于特征值组合所产生的变化结果。例如：“在线”和“药店”分开出现时一般出现在正常邮件中，但当组合起来时“在线药店”却一般出现在垃圾邮件中，贝叶斯分类器无法理解这种特征组合。 它经常被用于文本分类中，包括互联网新闻的分类，垃圾邮件的筛选。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>naive bayes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[markdown 和latex 常见的命令]]></title>
    <url>%2F2019%2F07%2F21%2Fmarkdown-latex%2F</url>
    <content type="text"><![CDATA[主要记录总结 latex 在markdown 中的使用，防丢失。 latex 单独使用和 在markdown 中的使用还是不太一样的，所以记下常用的几个。 多行公式对齐 源码： 1234567$$\begin&#123;split&#125;a &amp;= b \\\\c &amp;= d \\\\e &amp;= f \end&#123;split&#125;\tag&#123;1.3&#125;$$ 效果如下：$$\begin{split}a &amp;= b \\c &amp;= d \\e &amp;= f\end{split}\tag{1.1}$$ 分段函数 源码： 1234$$ BP = \begin&#123;cases&#125;1 &amp; c &gt;r \\\\e ^ &#123; ( 1 - r / c ) &#125; &amp; c &lt;= r\end&#123;cases&#125;$$ 效果如下：$$ BP = \begin{cases}1 &amp; c &gt;r \\e ^ { ( 1 - r / c ) } &amp; c &lt;= r\end{cases}\tag{1.2}$$ markdown 页内跳转 源码123&lt;center&gt; &lt;span id=&apos;jump&apos;&gt; 图 1 &lt;/span&gt;&lt;/center&gt; 如[图 1](#jump) ，当 test error 增加的时候，那么模型就应该停止了。 表格 table源码：12345| Tables | Are | Cool || ------------- |:-------------:| -----:|| col 3 is | right-aligned | $1600 || col 2 is | centered | $12 || zebra stripes | are neat | $1 | 效果如下： Tables Are Cool col 3 is right-aligned $1600 col 2 is centered $12 zebra stripes are neat $1 markdown 中的图片大小的设定 源码： 12&lt;img src=&quot;https://upload.cc/i1/2019/08/09/ApFiTC.png&quot; width=&quot;50%&quot; height=&quot;50%&quot;&gt; 矩阵书写 $${\left[ \begin{matrix} v_{1}^{T} \ v_{2}^{T} \end{matrix} \right] \cdotv_{1} =\left[ \begin{matrix} v_{1}^{T} \cdot v_{1} \ v_{2}^{T} \cdot v_{1} \end{matrix} \right] =\left[ \begin{matrix} 1 \ 0 \end{matrix} \right] \quad\left[ \begin{matrix} v_{1}^{T} \ v_{2}^{T} \end{matrix} \right] \cdotv_{2} =\left[ \begin{matrix} v_{1}^{T} \cdot v_{2} \ v_{2}^{T} \cdot v_{2} \end{matrix} \right] =\left[ \begin{matrix} 0 \ 1 \end{matrix} \right]}$$对应的源码： 12345678910$$&#123;\left[ \begin&#123;matrix&#125; v_&#123;1&#125;^&#123;T&#125; \\ v_&#123;2&#125;^&#123;T&#125; \end&#123;matrix&#125; \right] \cdot v_&#123;1&#125; = \left[ \begin&#123;matrix&#125; v_&#123;1&#125;^&#123;T&#125; \cdot v_&#123;1&#125; \\ v_&#123;2&#125;^&#123;T&#125; \cdot v_&#123;1&#125; \end&#123;matrix&#125; \right] =\left[ \begin&#123;matrix&#125; 1 \\ 0 \end&#123;matrix&#125; \right] \quad\left[ \begin&#123;matrix&#125; v_&#123;1&#125;^&#123;T&#125; \\ v_&#123;2&#125;^&#123;T&#125; \end&#123;matrix&#125; \right] \cdot v_&#123;2&#125; = \left[ \begin&#123;matrix&#125; v_&#123;1&#125;^&#123;T&#125; \cdot v_&#123;2&#125; \\ v_&#123;2&#125;^&#123;T&#125; \cdot v_&#123;2&#125; \end&#123;matrix&#125; \right] =\left[ \begin&#123;matrix&#125; 0 \\ 1 \end&#123;matrix&#125; \right]&#125;$$ 关于矩阵书写更多的样式参考这里。源码在本地。 图片的压缩，hexo 中提供了对于 img 标签的支持，所以可以这样写1&lt;img src=&quot;&quot; width=&quot;80%&quot; height=&quot;80%&quot;&gt;` 效果是这样的：]]></content>
      <categories>
        <category>NOT_FOR_YOU</category>
      </categories>
      <tags>
        <tag>markdown</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Recursive & Recursion]]></title>
    <url>%2F2019%2F07%2F17%2Frecursive-recursion%2F</url>
    <content type="text"><![CDATA[递归实现指数型枚举 从 1~n 这 n 个整数中随机选取任意多个，输出所有可能的选择方案。 原题连接 递推： 符合人的思路，比如由前两项计算第三项，求解斐波那契数列，递归：从后往前算，把一个大的问题分解成小的问题，然后计算小的问题，最后把所有的结果整合起来。从递归转成非递归，这个有时候比较麻烦，但是有时候必须这么做，因为可能递归的形式可能造成栈溢出。 dfs 就是类似一种树的结构，每次分支都类似两种选择，最后的叶子节点就是方案从第0个数字开始枚举， state 表示状态位置，使用二进制表示，如果是二进制是1 那么表示当前的数字是存在的如果是0 则表示不存在的 1234567891011121314151617181920212223242526272829// 因为数据量比较小 2^15 就算是三万吧, 所以直接dfs()#include&lt;iostream&gt;using namespace std;int n; // 定义全局变量是不用在递归中传下去了void dfs(int u, int state )&#123; // 边界条件 if (u ==n) &#123; for(int i =0; i&lt;n ;i++) if( state &gt;&gt; i &amp;1) cout &lt;&lt; i+1 &lt;&lt;" "; cout &lt;&lt; endl; return ; &#125; dfs(u+1, state); dfs(u+1, state | 1 &lt;&lt;u);&#125;int main()&#123; cin &gt;&gt;n; // 枚举的当前的数字 和状态表示（这个状态是可以使用 数组来表示，但是这里二进制来表示） dfs(0 , 0); return 0;&#125; 递归实现组合型枚举 从 1~n 这 n 个整数中随机选出 m 个，输出所有可能的选择方案。 输出格式 按照从小到大的顺序输出。 第一个参数表示当前枚举到了哪个数字，第二个参数表示选了多少个数字，第三个参数表示选择了哪些数字（二进制数，是0 表示没有选，是 1表示选择了）。 1234567891011121314151617181920212223242526272829#include&lt;iostream&gt;using namespace std;int n, m;void dfs(int v, int sum, int state)&#123; //边界条件 if (sum + n-v &lt; m) return ; // 一种方案数 if (sum ==m) &#123; for (int i =0; i&lt;n ;i++) //这个是遍历总数个的，因为其中有的是选择有的是不选择的 if (state &gt;&gt;i &amp;1 ) cout &lt;&lt; i+1 &lt;&lt;" "; cout &lt;&lt;endl; return ; &#125; // 从小到大排序，所以尽可能的选择，这样前面的是比较小的 dfs(v +1, sum +1, state | 1 &lt;&lt;v); dfs(v +1, sum , state); &#125;int main()&#123; cin &gt;&gt; n&gt;&gt; m; dfs(0, 0, 0); // 初始化 return 0;&#125; 排列与组合的共同点是从 n 个不同的元素中，任取 m 个元素，不同点在于排列是按照一定的顺序排成一列，组合是无论怎样的顺序并成一组。因此，“有序” 和“无序” 是区分排列和组合的重要标志。 递归实现排列型枚举 把 1~n 这 n个整数排成一行后随机打乱顺序，输出所有可能的次序。 这个是有序的，所以不能只是一个 二进制数进行判断，应该使用数组进行保存结果。 123456789101112131415161718192021222324252627282930313233343536// 因为是有顺序的，所以使用path 进行保存结果，// 还是使用一个 二进制数字 表示是否选择过#include&lt;iostream&gt;#include&lt;vector&gt;using namespace std;int n;vector&lt;int&gt; path;void dfs(int u, int state)&#123; //跳出条件 if (u ==n) &#123; for(auto x : path) cout &lt;&lt; x&lt;&lt; " "; cout &lt;&lt;endl; return; &#125; for(int i =0 ; i&lt;n ;i++) &#123; if ( !( state &gt;&gt; i &amp;1)) // 如果没有遍历过，那么就访问 &#123; path.push_back(i+1); // 加入到path 中 dfs(u+1, state | 1 &lt;&lt;i); path.pop_back(); // 返回到现场 &#125; &#125;&#125;int main()&#123; cin &gt;&gt;n; dfs(0, 0); //从 0开始枚举，使用二进制数字进行标记是否出现过 return 0;&#125; 费解的开关 原题这个题目有点难了 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980#include&lt;iostream&gt;#include&lt;cstring&gt;using namespace std;const int INF = 1000000;char g[10][10];// 上一行的某一个位置为0 那么下一行的对应的位置就要置为1，否则上一行就不能变了。// 所以基本的思路就是固定前4行，判断第5 行如果全部是1 那么就是能够变成1，否则该方案就不成立。// 对于turn 的小的技巧// 中间， 上 右边 下面，左边// 中间 上面 右边 下面 左边// 这个只要保证当前的不变，int dx[5] =&#123; 0, 1, 0, -1, 0&#125;, dy[5] =&#123;0, 0, -1, 0, 1&#125;;void turn(int x, int y)&#123; for(int i =0; i &lt;5; i++) &#123; int a =x +dx[i], b = y+ dy[i]; if( a&gt;= 0 &amp;&amp; a&lt; 5 &amp;&amp; b&gt;=0 &amp;&amp; b&lt;5) g[a][b] ^=1; //对于01 相互转换，这种操作是可行的 &#125;&#125;int work()&#123; int ans =INF; for (int k =0; k&lt; 1&lt;&lt;5; k++) &#123; int res =0; char backup[10][10]; memcpy(backup, g, sizeof(g)); // 备份 g 到 backup for(int j =0; j&lt; 5 ; j++) if (k &gt;&gt; j &amp;1) &#123; res ++; turn(0, j); &#125; for(int i =0; i&lt;4; i++) &#123; for(int j =0; j&lt;5 ;j++) if(g[i][j] =='0') &#123; res ++; turn(i+1, j); &#125; &#125; bool is_successful =true; for(int j =0; j&lt;5 ;j++) if( g[4][j]== '0') &#123; is_successful =false; break; &#125; if(is_successful) ans =min(ans, res); memcpy(g, backup, sizeof(g)); &#125; if (ans &gt;6) ans =-1; return ans; &#125;int main()&#123; int T; cin &gt;&gt; T; while( T --) &#123; for(int i =0; i &lt;5; i++) cin &gt;&gt; g[i]; // 类似行向量的感觉,g 是一个二维的向量 cout &lt;&lt; work() &lt;&lt;endl; &#125; return 0;&#125; 代码是错误的，但是上面的注释是正确的。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889#include&lt;iostream&gt;#include&lt;cstring&gt;using namespace std;const int INF =100000;char g[10][10];// 中间， 上 右边 下面，左边// 中间 上面 右边 下面 左边int dx[5] =&#123;0, 1, 0, -1, 0&#125;, dy[5] =&#123;0, 0, -1, 0, 1&#125;;void turn(int x, int y)&#123; for(int i =0; i&lt;5 ;i++) &#123; int a =x +dx[i], b =y+dy[i]; if(a &gt;=0 &amp;&amp; a&lt;5 &amp;&amp; b&gt;=0 &amp;&amp; b&lt;5 ) g[a][b] ^=1; &#125; &#125;int work()&#123; int ans =INF; for(int k =0; k&lt; 1&lt;&lt;5; k++) &#123; int res =0; // 当前方案的操作数 char backup[10][10]; // 如果你backup 一定是在某个时间点是用于恢复的 memcpy(backup, g, sizeof(g)); // 先把第一行枚举操作完，然后固定第一行 for(int j =0; j&lt;5; j++) &#123; res ++; if(k &gt;&gt;j &amp;1) turn(0,j); // 为什么是1的时候，需要turn 一下，不是0吗？ &#125; // 固定了第一行，然后遍历前4 行 for(int i =0; i&lt;4; i++) &#123; for(int j =0; j&lt;5; j++) if( g[i][j] =='0') &#123; res ++; turn(i +1, j); &#125; &#125; bool is_successful =true; for(int j =0; j&lt;5; j++) if( g[4][j] =='0') &#123; is_successful =false; break; &#125; if( is_successful) ans =min(ans, res); memcpy(g, backup, sizeof(g)); &#125; if (ans&gt;6) ans =-1; return ans; &#125;int main()&#123; int T; cin &gt;&gt; T; // 是每组每组进行判断的额 while( T--) &#123; for (int i =0; i&lt;5; i++) cin&gt;&gt; g[i]; cout &lt;&lt; work() &lt;&lt;endl; &#125; return 0;&#125; （5）”压平”多重嵌套的list 重点使用了python 中的isinstance() 函数。 1234567891011121314151617from collections import Iterableres =[]def flatten_list(list1): for element in list1: if isinstance(element, dict): for (key, val) in element.items(): res.append((key, val)) # 这里不能直接使用Iterable，因为string 也是Iterable，但是不能这样调用 elif isinstance(element, list) or isinstance(element, tuple): flatten_list(element) # 这里体现的是递归调用 else: res.append(element)list1 = [&#123;"name": "jack", 1: 2&#125;, [[3.4j, ["this", "is", "the", "list"], (1, 2, "adf")], 3, 4], 'a', 'b', ('a', "cc", 'd'), 1, &#123;"name": "jack", 1: 2&#125;]flatten_list(list1)print(res)]]></content>
      <categories>
        <category>算法</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[LeetCode-Others]]></title>
    <url>%2F2019%2F07%2F16%2Fleetcode-others%2F</url>
    <content type="text"><![CDATA[LeetCode 刷题总结（四），使用Python 实现。该篇题目类型主要包含无法归类到上述三篇的题目。 Reverse Integer Given a 32-bit signed integer, reverse digits of an integer. Input: 120 Output: 21 Input: 123 Output: 321 https://leetcode.com/problems/reverse-integer/ Tips： 这个只是reverse 操作，注意一些细节比如负号和数字 0的处理。对于最大数的表示 pow(2, 31)。 通过 % 求余获得最小位。 12345678910111213141516class Solution(object): def reverse(self, x): """ :type x: int :rtype: int """ result = 0 if x &lt; 0: symbol = -1 x = -x else: symbol = 1 while x: result = result * 10 + x % 10 x /= 10 return 0 if result &gt; pow(2, 31) else result * symbol String to Integer (atoi) Implement atoi which converts a string to an integer. Tips: 涉及到bit 级别数字处理的一般都会用到 res =res 10 + something 这样的东西。对于能够表示的数字的判断 max(-pow(2, 31), min(ressign, pow(2, 31) -1)) 这个还是挺经典的代码的。 https://leetcode.com/problems/string-to-integer-atoi/ 1234567891011121314151617181920212223class Solution(object): def myAtoi(self, str): """ :type str: str :rtype: int """ ls =list(str.strip()) if len(ls) == 0: return 0 sign = -1 if ls[0] == '-' else 1 index=0 # 有一个index 是贯穿始终的 res =0 if ls[index] in ['-', '+']: index +=1 for i in range(index, len(ls)): if ls[i].isdigit(): res =res *10 + ord(ls[i]) -ord('0') else: # case "words and 987" 是不能有 字母的 break return max(-pow(2, 31), min(sign*res, pow(2,31) -1)) Palindrome Number Determine whether an integer is a palindrome. An integer is a palindrome when it reads the same backward as forward. Tips： 回文数。代码写的很巧妙，整体上说是通过 / 和 % 获得数字的头和尾，在实现的时候有若干细节。 1234567891011121314151617181920class Solution(object): def isPalindrome(self, x): """ :type x: int :rtype: bool """ if x &lt; 0: return False ranger = 1 while x / ranger &gt;= 10: ranger *= 10 while x: left = x / ranger right = x % 10 if left != right: return False x = (x % ranger) / 10 ranger /= 100 return True Integer to Roman Roman numerals are represented by seven different symbols: I, V, X, L, C, D and M. Tips： 基于roman 数字规则的转换。代码实现角度 tuple都是要好于 list （内存和速度方面都是），如果你想要存储的是静态的可以遍历的数据，不需要每次进行修改的话，why not 123456789101112131415161718class Solution(object): def intToRoman(self, num): if num &lt;= 0: return "" digits = &#123; "I":1, "V":5, "X":10, "L":50, "C":100, "D":500, "M":1000 &#125; nums = (1000, 900, 500, 400, 100, 90, 50, 40, 10, 9, 5, 4, 1) chs = ("M", "CM", "D", "CD", "C", "XC", "L", "XL", "X", "IX", "V", "IV", "I") len = 13 s = "" while num &gt; 0: for i in range(0, len): if num &gt;= nums[i]: num -= nums[i] s += chs[i] break return s Roman to Integer Roman numerals are represented by seven different symbols: I, V, X, L, C, D and M. Tips： 和上一道题目相似。 1234567891011121314151617181920class Solution(object): def romanToInt(self, s): """ 这个是输入时 string，所以是 index 遍历的在 dict 中进行访问，但是上一个题目是总的 number，是没有办法的 """ digits = &#123; "I":1, "V":5, "X":10, "L":50, "C":100, "D":500, "M":1000 &#125; len_s = len(s) num = 0 # 这个少遍历了一个 ，因为其中有 i+1 的存在 for i in range(0, len_s - 1): cur = digits[s[i]] next_s = digits[s[i + 1]] if cur &gt;= next_s: num += cur else: num -= cur # 处理的是最后一个 num += digits[s[len_s - 1]] return num Letter Combinations of a Phone Number Given a string containing digits from 2-9 inclusive, return all possible letter combinations that the number could represent.A mapping of digit to letters (just like on the telephone buttons) is given below. Note that 1 does not map to any letters. Input: &quot;23&quot; Output: [&quot;ad&quot;, &quot;ae&quot;, &quot;af&quot;, &quot;bd&quot;, &quot;be&quot;, &quot;bf&quot;, &quot;cd&quot;, &quot;ce&quot;, &quot;cf&quot;]. Tips：使用循环的方式表示层级的关系（使用二层循环，第一层表示外围a， 第二层表示 def 等）。 1234567891011121314151617181920212223242526272829303132333435363738class Solution: def letterCombinations(self, digits): """ :type digits: str :rtype: List[str] """ if not digits or digits == "": return [] # 命名很到位 maps =&#123; '1': (), '0': (), '2': ('a', 'b', 'c'), '3': ('d', 'e', 'f'), '4': ('g', 'h', 'i'), '5': ('j', 'k', 'l'), '6': ('m', 'n', 'o'), '7': ('p', 'q', 'r', 's'), '8': ('t', 'u', 'v'), '9': ('w', 'x', 'y', 'z') &#125; results = [""] for digit in digits: tuple1 = maps[digit] tmp =[] if len(tuple1) == 0: continue # 二重循环 for prefix in results: for suffix in tuple1: tmp.append(prefix + suffix) results = tmp return results Divide Two Integers Given two integers dividend and divisor, divide two integers without using multiplication, division and mod operator.Return the quotient after dividing dividend by divisor.The integer division should truncate toward zero. Tips： 不让使用乘除和 mod 操作，只能使用位运算符了。在python 中0 == False 这个在逻辑判断中是等价的（1 ==True）。计算的时候使用 &lt;&lt; （不断的*2）, ,maybe 是二分 12345678910111213141516171819202122class Solution(object): def divide(self, divident, divisor): sign =-1 if divident* divisor&lt;0 else 1 divident, divisor =abs(divident), abs(divisor) ans =0 while divisor &lt;= divident: div =divisor tmp =1 while (div &lt;&lt;1) &lt;= divident: div &lt;&lt;= 1 tmp &lt;&lt;= 1 divident -= div ans += tmp return max(-pow(2, 31), min(ans*sign, pow(2, 31) -1)) Pow(x, n) Implement pow(x, n), which calculates x raised to the power n (xn) Tips： 简单的递归。 1234567891011121314151617181920class Solution(object): # 递归写起来比较好些，但是有时候比较难理解这个运行的过程 def myPow(self, x,n): """ :type x: float :type n: int :rtype: float """ if n ==0: return 1 # 求解pow() 都是正数，如果n &lt;0,那么需要做的是 取导数 elif n &lt;0: return 1.0/self.myPow(x, -n) else: half =self.myPow(x, n&gt;&gt;1) if n%2 ==0: return half *half else: return x *half*half N-Queens The n-queens puzzle is the problem of placing n queens on an n×n chessboard such that no two queens attack each other. Given an integer n, return all distinct solutions to the n-queens puzzle.Each solution contains a distinct board configuration of the n-queens’ placement, where ‘Q’ and ‘.’ both indicate a queen and an empty space respectively. Input: 4Output: [ [“.Q..”, // Solution 1 “…Q”, “Q…”, “..Q.”], [“..Q.”, // Solution 2 “Q…”, “…Q”, “.Q..”]]Explanation: There exist two distinct solutions to the 4-queens puzzle as shown above. Tips： N-皇后，行列对角线是不能出现重复。中规中矩的递归解法，board 是使用的一维向量， 这样去理解 比如board=[1, 3, 0, 2]，这是4皇后问题的一个解，意思是：在第0行，皇后放在第1列；在第1行，皇后放在第3列。 check函数表示 第k 个 皇后是否能够放在第j 个位置。 123456789101112131415161718192021222324252627class Solution(object): def __init__(self): self.board =[] def check(self, k,j): for i in range(k): # 如果之前的皇后已经放到了这个位置，或者两者在一条直线上，这个abs 用的比较牛逼 if self.board[i] ==j or abs(k -i) ==abs(self.board[i] -j): return False return True def dfs(self, depth, valuelist, n, res): if depth ==n: res.append(valuelist) return for i in range(n): if self.check(depth, i): self.board[depth] =i s ='.'*n self.dfs(depth +1, valuelist+[s[:i] +'Q'+s[i+1:]], n,res) def solveNQueens(self, n): self.board =[-1 for i in range(n)] res =[] self.dfs(0, [], n, res) return res N-Queens II The n-queens puzzle is the problem of placing n queens on an n×n chessboard such that no two queens attack each other. Tips: 这个相对于上一个要简单一些，因为最后的结果要的是 counts 而不是 list of path。按照道理讲是不用记录path 的。 123456789101112131415161718192021222324252627282930313233class Solution(object): def __init__(self): self.board =[] self.count =0 def check(self, k,j): """ check if the kth queen can be put in column j :param k: :param j: :return: """ for i in range(k): if self.board[i] ==j or abs(k -i) ==abs(self.board[i] -j): return False return True def dfs(self, depth, valuelist, n, res): if depth ==n: #res.append(valuelist) self.count +=1 return for i in range(n): if self.check(depth, i): self.board[depth] =i s ='.'*n self.dfs(depth +1, valuelist+[s[:i] +'Q'+s[i+1:]], n,res) def totalNQueens(self, n): self.board =[-1 for i in range(n)] res =[] self.dfs(0, [], n, res) return self.count Add Binary Given two binary strings, return their sum (also a binary string).The input strings are both non-empty and contains only characters 1 or 0. Tips：二级制相加，从后往前走，使用 carry 位置记录进位数。 12345678910111213141516171819202122class Solution(object): def addBinary(self, a, b): """ :type a: str :type b: str :rtype: str """ i, j, carry, res =len(a) -1, len(b) -1, 0, '' while i&gt;=0 or j &gt;=0 or carry: if i &gt;=0: carry += int(a[i]) i -=1 if j&gt;=0: carry += int(b[j]) j -=1 res =str(carry%2) +res carry //=2 return res Max Points on a Line Given n points on a 2D plane, find the maximum number of points that lie on the same straight line. Tips：当使用除法的时候，因为精度问题造成的误差，所以这里使用最大的公约数进行化简。使用以下三种方式处理。 Map from (a,b,c,d) representing y=(a/b)x+(c/d) to set of indices of points that are on that line. a/b and c/d are reduced, i.e. a and b are divided by their GCD and so are c and d. Vertical lines are represented by a tuple with 1 element, the x-axis value Single points are represented by a 2-tuple (x, y). 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647import collectionsimport mathdef fraction(x, y): if x &lt; 0: x, y = -x, -y gcd = math.gcd(x, y) return x // gcd, y // gcd class Solution: def maxPoints(self, points): if not points: return 0 if len(points) == 1: return 1 aligned_points = collections.defaultdict(set) duplicates = collections.defaultdict(set) for i, p in enumerate(points): for j, q in enumerate(points[i + 1:], start=i + 1): # x 是否相同 if q[0] == p[0]: if q[1] == p[1]: duplicates[i].add(j) key = tuple(p) else: key = (q[0],) else: a, b = fraction(q[1] - p[1], q[0] - p[0]) # k斜率 c, d = fraction(p[1] * q[0] - q[1] * p[0], q[0] - p[0]) # b 位移 key = (a, b, c, d) #aligned_points[key] = aligned_points[key] or &#123;i, j&#125; # 因为之前定义的是set aligned_points[key] |= &#123;i, j&#125; for p, dups in duplicates.items(): for key in aligned_points: if p in aligned_points[key]: #aligned_points[key] = aligned_points[key] or dups # 这个or 不是选择的意思，是两者都要的意思 aligned_points[key] |= dups max_points = 0 for aliged in aligned_points.values(): max_points = max(max_points, len(aliged)) return max_points Happy Number Write an algorithm to determine if a number is “happy”.A happy number is a number defined by the following process: Starting with any positive integer, replace the number by the sum of the squares of its digits, and repeat the process until the number equals 1 (where it will stay), or it loops endlessly in a cycle which does not include 1. Those numbers for which this process ends in 1 are happy numbers. Input: 19Output: trueExplanation:12 + 92 = 8282 + 22 = 6862 + 82 = 10012 + 02 + 02 = 1 Tips: 使用set 的思路，没有重复之前就一直遍历；对于数字转成 string 逐个进行处理。 1234567891011121314class Solution(object): def isHappy(self, n): """ :type n: int :rtype: bool """ visited =set() # 这个set 每次add 都是add 到最前面一个，是规律吧 while n not in visited: visited.add(n) n =sum((int(x) **2 for x in str(n)))# () 要比 [] 使用较少的内存 return n ==1 Sum of Two Integers Calculate the sum of two integers a and b, but you are not allowed to use the operator + and -. Input: a = 1, b = 2Output: 3 Tips: 下面的add()函数只是在以下场景中 work: a*b&gt;=0 , or the negative number has a larger absolute value( a &lt; 0 and abs(a) &gt; b &gt; 0 , or b &lt; 0 and abs(b) &gt; a &gt; 0) 123456789101112131415161718192021class Solution(object): def getSum(self, a, b): """ :type a: int :type b: int :rtype: int """ def add(a, b): if not a or not b: return a or b # # ^ get different bits and &amp; gets double 1s, &lt;&lt; moves carry ， 这个可能是加法吧， return add(a^b, (a&amp;b) &lt;&lt; 1) if a*b &lt; 0: # assume a &lt; 0, b &gt; 0 if a &gt; 0: return self.getSum(b, a) if -a == b: return 0 if -a &lt; b: return -add(-a, -b) return add(a, b) Fizz Buzz * Write a program that outputs the string representation of numbers from 1 to n. n = 15,Return:[ “1”, “2”, “Fizz”, “4”, “Buzz”, “Fizz”, “7”, “8”, “Fizz”, “Buzz”, “11”, “Fizz”, “13”, “14”, “FizzBuzz”] Tips: 这是一个 for 循环就可以解决的问题 12345678910111213141516171819class Solution(object): def fizzBuzz(self, n): """ :type n: int :rtype: List[str] """ # 使用一个 result append一下 result = [] for i in xrange(1, n + 1): if i % 3 != 0 and i % 5 != 0: result.append(str(i)) elif i % 3 == 0 and i % 5 != 0: result.append("Fizz") elif i % 3 != 0 and i % 5 == 0: result.append("Buzz") else: result.append("FizzBuzz") return result Find Median from Data Stream Median is the middle value in an ordered integer list. If the size of the list is even, there is no middle value. So the median is the mean of the two middle value. For example,[2,3,4], the median is 3[2,3], the median is (2 + 3) / 2 = 2.5 Tips: 主要难点在于数据流，难在数据结构，使用最小根堆实现。这个是需要寻找 median （中位数），使用大小根堆，分别存储较小的一半 和 较大的一半。那么大根堆的堆顶就对应着较小一半的最大值，小根堆对应着较大部分的最小值。所以中位数就可以 快速的从两个堆顶元素中获得。 https://leetcode.com/problems/find-median-from-data-stream/ 简单说一下在python 中heapq -小根堆的实现。将数值转成负数，那么就可以使用小根堆来mimic 大根堆，因为堆顶是负数最小的。（对应正数最大的） heap.heappush(heap, item), 把一个item 添加到heap中 heap.heappushpop(heap, item), 先把item 放入到堆中，然后再pop() , 这样比 heappush() 然后再heappop() 快一些 push item on the heap, then pop and return the smallest item from the heap. The combined action runs more efficiently than heappush followed by a separate call to heappop() Note that the heapq in python is a min heap, thus we need to invert the values in the smaller half to mimic a &apos;max heap&apos; the add operation is O(nlogn), the find operation is O(1) 123456789101112131415161718from heapq import *class MedianFinder(object): def __init__(self): self.small =[] # max heap (the smaller half of the list), 大根堆存放的是小值，然后根存放的就是最大值, 这个转成-num 当然就是 smaller part self.large =[] # min heap (the larger half of the list), 小根堆存放的是大值，然后根就是存放的最小值 def addNum(self, num): if len(self.small) ==len(self.large): heappush(self.large, -heappushpop(self.small, -num)) else: heappush(self.small, -heappushpop(self.large, num)) def findMedian(self): if len(self.small) ==len(self.large): return float(self.large[0] -self.small[0])/2.0 else: return float(self.large[0]) Flatten Nested List Iterator Given a nested list of integers, implement an iterator to flatten it.Each element is either an integer, or a list – whose elements may also be integers or other lists. Tips: 迭代器和 generator 感觉都是差不多的操作，产生一个数字。这个看懂 API 更重要。只要数字内容，不要嵌套的关系。yield 关键字，调用完之后，程序不结束。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950# """# This is the interface that allows for creating nested lists.# You should not implement it, or speculate about its implementation# """#class NestedInteger(object):# def isInteger(self):# """# @return True if this NestedInteger holds a single integer, rather than a nested list.# :rtype bool# """## def getInteger(self):# """# @return the single integer that this NestedInteger holds, if it holds a single integer# Return None if this NestedInteger holds a nested list# :rtype int# """## def getList(self):# """# @return the nested list that this NestedInteger holds, if it holds a nested list# Return None if this NestedInteger holds a single integer# :rtype List[NestedInteger]# """class NestedIterator(object): def __init__(self, nestedList): def gen(nestedList): for x in nestedList: if x.isInteger(): yield x.getInteger() else: for y in gen(x.getList()): yield y self.gen = gen(nestedList) def next(self): return self.value def hasNext(self): try: self.value = next(self.gen) return True except StopIteration: return False# Your NestedIterator object will be instantiated and called as such:# i, v = NestedIterator(nestedList), []# while i.hasNext(): v.append(i.next()) Insert Delete GetRandom O(1) Design a data structure that supports all following operations in average O(1) time. insert(val): Inserts an item val to the set if not already present. remove(val): Removes an item val from the set if present. getRandom: Returns a random element from current set of elements. Each element must have the same probability of being returned. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546class RandomizedSet(object): # 就是一个维持list 的东西 def __init__(self): """ Initialize your data structure here. """ self.list =[] def insert(self, val): """ Inserts a value to the set. Returns true if the set did not already contain the specified element. :type val: int :rtype: bool """ if val not in self.list: self.list.append(val) return True return False def remove(self, val): """ Removes a value from the set. Returns true if the set contained the specified element. :type val: int :rtype: bool """ if val in self.list: self.list.remove(val) return True return False def getRandom(self): """ Get a random element from the set. :rtype: int """ length =len(self.list) import random index =random.randint(0,length-1) return self.list[index] # Your RandomizedSet object will be instantiated and called as such:# obj = RandomizedSet()# param_1 = obj.insert(val)# param_2 = obj.remove(val)# param_3 = obj.getRandom() Perfect Squares Given a positive integer n, find the least number of perfect square numbers (for example, 1, 4, 9, 16, …) which sum to n. Tips: 再次理解一下二重循环，如果第二层中的遍历次数和 第一层是有关系的，往往是 O(N*N/2) 的复杂度（这种记法是错误的），用于遍历前 i 个元素。 ···pythonclass Solution(object): # dp[i] 的定义表示 i 这个数字最少使用的 squares 数量 def numSquares(self, n): &quot;&quot;&quot; :type n: int :rtype: int &quot;&quot;&quot; dp = [float(&apos;inf&apos;)]*(n+1) dp[0]=0 count = 2 for i in range(1,n+1): if i&gt;=count*count: count += 1 for j in range(1,count): dp[i] = min(dp[i],dp[i-j*j]+1) return dp[n] &quot;&quot;&quot; # 这样做是不可行的，在于可能重复使用一个，如果没有最大的合适的话 square =[pow(num, 2) for num in range(1, int(math.sqrt(n) +1))] square =square[::-1] count =0 for sq in square: if n-sq &gt;0: n -= sq count +=1 return count &quot;&quot;&quot; 123456789101112131415161718192021222324** Factorial Trailing Zeroes**&gt; Given an integer n, return the number of trailing zeroes in n!.Tips: 数学问题···pythonclass Solution(object): # 这里的 += n/div 已经就表示了 5的个数， 这样是可以加快运算的 def trailingZeroes(self, n): &quot;&quot;&quot; :type n: int :rtype: int &quot;&quot;&quot; div =5 res =0 while div &lt;=n: res += n/div div =div *5 return res Count Primes Count the number of prime numbers less than a non-negative number, n. Tips: 主要是质数倍数的优化，否则是time out 123456789101112131415161718192021class Solution(object): # prime 质数， 1 既不是质数也不是 合数 # 当前数为质数时，排除掉剩下的数中该数的整倍数。遍历过所有的数之后剩下的数全是质数。提升效率的方法是减少遍历的长度。 # 还有一个优化点，可以不必从2~m-1，只需遍历2 ~ √m.因为如果m能被2 ~ m-1之间任一整数整除，其二个因子必定有一个小于或等于√m，另一个大于或等于√m。例如16能被2,4,8整除 # 质数定义为在大于1的自然数中，除了1和它本身以外不再有其他因数。 # 最后 return 的是 counter，个数 而不是具体的数字 def countPrimes(self, n): """ :type n: int :rtype: int """ if n &lt;= 2: return 0 prime = [True] * n prime[:2] = [False, False] for base in range(2, int((n ) ** 0.5) + 1): # 时间上是 [2, sqrt(m)] ，但是在python 中实现是这样的 if prime[base]: prime[base ** 2::base] = [False] * len(prime[base ** 2::base]) return sum(prime) Missing Number Given an array containing n distinct numbers taken from 0, 1, 2, …, n, find the one that is missing from the array. Tips: 使用公式 index 和前 n数的问题。 1234567891011class Solution(object): # 如果没有限制内存，那么是可以使用dict，然后根据index 和value 进行判断的 # 凡是和对应的index 发生关系，那么这个就有优化的可能，就变得比较有意思 def missingNumber(self, nums): """ :type nums: List[int] :rtype: int """ n =len(nums) return n*(n+1)/2 -sum(nums) Power of Three Given an integer, write a function to determine if it is a power of three. Tips: 求余 and /= 相结合的常见手法 12345678910111213141516171819class Solution(object): # 这个是在考察除法的运算过程 def isPowerOfThree(self, n): """ :type n: int :rtype: bool """ if n &lt;=0: return False if n ==1: return True while n &gt;1: if n %3 !=0: return False n /= 3 return True Implement Trie (Prefix Tree) Implement a trie with insert, search, and startsWith methods. Tips: 这个数据结构很有用，字典树。 123456789101112131415161718192021222324252627282930313233343536373839404142434445class TrieNode: # Initialize your data structure here. # https://www.cnblogs.com/beiyeqingteng/p/5625540.html，这里面有个图 # 还是挺形象的 def __init__(self): # 这个是什么数据结构呀 self.children = collections.defaultdict(TrieNode) # 这种设置还是理解不够深刻 self.is_word = False# 考察的是字典树，这种数据结构# 保存字母的话，是 26叉树，保存数字的话10 叉树class Trie(object): def __init__(self): self.root =TrieNode() def insert(self, word): current =self.root for letter in word: current =current.children[letter] current.is_word =True def search(self, word): current =self.root for letter in word: current =current.children.get(letter) if not current: return False return current.is_word def startsWith(self, prefix): current =self.root for letter in prefix: current =current.children.get(letter) # dictionary 操作，得到的是值 if not current: return False return True# Your Trie object will be instantiated and called as such:# obj = Trie()# obj.insert(word)# param_2 = obj.search(word)# param_3 = obj.startsWith(prefix)]]></content>
      <categories>
        <category>算法</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Knapsack]]></title>
    <url>%2F2019%2F07%2F15%2Fknapsack%2F</url>
    <content type="text"><![CDATA[01背包 有$ N $件物品和一个容量是 $V $的背包。每件物品只能使用一次。第 $i$ 件物品的体积是 $v_i$，价值是 $w_i$。求解将哪些物品装入背包，可使这些物品的总体积不超过背包容量，且总价值最大。输出最大价值。 Tips: 这个是最基础的背包问题。 背包问题属于动态规划的一种， $f[i] $表示体积为 $i $背包的最大的价值。那么转移方程就比较简单了，对于物品(v, w) 有选和不选两种方案。$ f[i] =max( f[i], f[i-v] +w) $ 这样的式子就是比较nice的选择。 01 背包问题的体积是从大到小进行枚举的。 12345678910111213141516171819202122232425262728// 01 背包问题， f[i] =max(f[i], f[i -w]+ v)#include&lt;iostream&gt;#include&lt;cstring&gt;#include&lt;algorithm&gt;using namespace std;const int N= 1010;int f[N];int v[N], w[N];int n, m;int main()&#123; cin&gt;&gt; n&gt;&gt; m; for(int i =0; i&lt; n; i++) &#123; int v, w; cin &gt;&gt; v&gt;&gt; w; for (int j =m ; j&gt;=v; j--) f[j] =max(f[j], f[j -v]+w ); &#125; cout &lt;&lt; f[m]&lt;&lt;endl; return 0;&#125; 完全背包问题 有$ N $件物品和一个容量是 $V $的背包。每件物品都有无限件可用。第 $i$ 件物品的体积是 $v_i$，价值是 $w_i$。求解将哪些物品装入背包，可使这些物品的总体积不超过背包容量，且总价值最大。输出最大价值。 Tips： 这个跟上面的不同点在于，物品是无限件可用，01 背包是只能用一个。从代码实现上看， 01背包问题的 体积是从大到小遍历的，然后完全背包问题是从小到大遍历的。 01 背包问题和 完全背包问题的区别在于前者物品只有选择与否两种方案。而完全背包对于每件物品有无限件可以选择。对于体积v 是从小到大进行枚举的。 1234567891011121314151617181920212223242526272829// 转移方程 f[i] =max(f[i], f[i -v]+w) 没有变，实现的时候有差别#include&lt;iostream&gt;#include&lt;cstring&gt;#include&lt;algorithm&gt;using namespace std;const int N =1010;int f[N];int n, m;int main()&#123; cin &gt;&gt; n&gt;&gt;m; for(int i =0; i&lt; n ;i++) &#123; int v, w; cin &gt;&gt; v&gt;&gt;w; for(int j =v; j&lt;=m ;j++) // 注意这个是 &lt;= 号; 这个起始点是假设放入了体积为v 的物品 f[j] =max(f[j], f[j -v]+w); &#125; cout&lt;&lt; f[m]&lt;&lt;endl; return 0;&#125; 多重背包问题 1 有$ N $件物品和一个容量是 $V $的背包。每件物品都有无限件可用。第 $i$ 件物品最多有 $s_i$ 件, 每件体积是 $v_i$，价值是 $w_i$。求解将哪些物品装入背包，可使这些物品的总体积不超过背包容量，且总价值最大。输出最大价值。 $$\begin{split}0&lt; &amp; N \leq 100 \\0&lt; &amp; V \leq 100 \\0&lt; &amp; v_{i}, w_{i}, s_{i} \leq 100\end{split}$$ Tips： 多重背包问题是转换成 01 背包问题进行解决的，这个大的思路是没有问题的。只是对于不同的数据规模有两种不同的实现手段。当数据规模比较小的时候，三重循环就搞定了；当数据规模比较大的时候，需要使用一个结构体进行操作。 （多重背包问题对物品的个数进行了限制，不是无限个，转换成01 背包问题，体积是从大到小进行枚举） 12345678910111213141516171819202122232425262728#include&lt;iostream&gt;#include&lt;cstring&gt;#include&lt;algorithm&gt;using namespace std;const int N =110;int n,m;int f[N];int main()&#123; cin &gt;&gt;n&gt;&gt;m; for(int i =0;i&lt; n;i++) &#123; int v, w, s; cin &gt;&gt;v &gt;&gt;w&gt;&gt;s; for (int j =m; j&gt;=0; j--) for(int k =1; k&lt;=s &amp;&amp; k*v &lt;=j ; k++) f[j] =max(f[j], f[j -k*v] +k*w); &#125; cout &lt;&lt;f[m]&lt;&lt; endl; return 0;&#125; 多重背包问题 2 有$ N $件物品和一个容量是 $V $的背包。每件物品都有无限件可用。第 $i$ 件物品最多有 $s_i$ 件, 每件体积是 $v_i$，价值是 $w_i$。求解将哪些物品装入背包，可使这些物品的总体积不超过背包容量，且总价值最大。输出最大价值。 数据范围: $$\begin{split}0&lt; &amp; N \leq 1000 \\0&lt; &amp; V \leq 2000 \\0&lt; &amp; v_{i}, w_{i}, s_{i} \leq 2000\end{split}$$ Tips: 主要区别在于数据的维度。 这个是转换成 成 01 背包问题，上一道题目是在 01 背包问题的基础上（框架上）进行修改。 这个从时间复杂度上的优化：$1000 \times 2000 \times 2000$ -&gt; $1000 \times 2000 \times \log(2000) $ 。经过优化 基本上在 $ 10^7 $，所以这个速度是可以接受的。 c++ 在 1s 中就是可以计算 $10^7 $ 这样运算。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354#include&lt;iostream&gt;#include&lt;cstring&gt;#include&lt;algorithm&gt;#include&lt;vector&gt;using namespace std;const int N =2010 ; // 这个数量是 v的最大值int f[N];int n, m;struct Good&#123; int v, w; &#125;;vector&lt;Good&gt; goods;int main()&#123; cin &gt;&gt; n&gt;&gt;m; for(int i =0; i&lt;n ; i++) &#123; int v, w, s; cin &gt;&gt; v&gt;&gt; w&gt;&gt;s; for (int k =1; k&lt;=s; k *=2) &#123; s -=k; goods.push_back(&#123;v*k, w*k&#125;); &#125; if(s &gt;0) goods.push_back(&#123;v*s, w*s&#125;); &#125; // 01 hnapsack // goods 的长度不一定是 n， 所以不能这样进行遍历 /* for(int i =0; i&lt;n; i++)// 这个 for(int j =m ; j&gt;= goods[i].v; j--) &#123; f[j] =max(f[j], f[j -goods[i].v] +goods[i].w); &#125; */ for(auto good : goods) for(int j =m ; j&gt;= good.v; j--) f[j] =max(f[j], f[j -good.v]+ good.w); cout &lt;&lt; f[m]&lt;&lt; endl; return 0;&#125; 混合背包问题 有 $ N$种物品和一个容量是 $V$的背包。 物品一共有三类： 第一类物品只能用1次（01背包）； 第二类物品可以用无限次（完全背包）； 第三类物品最多只能用 $s_i $次（多重背包）； 每种体积是 $v_i$，价值是 $w_i$。求解将哪些物品装入背包，可使物品体积总和不超过背包容量，且价值总和最大。输出最大价值。 Tips： 分类讨论，多重背包问题转成 01背包问题，所以最后是有两类。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364#include&lt;iostream&gt;#include&lt;algorithm&gt;#include&lt;vector&gt;#include&lt;cstring&gt;using namespace std;/*分成两类： 01 背包问题和完全背包问题；对于多重背包问题使用 二进制进行优化，得到01 背包问题。所以需要一个新的结构体 01 背包问题只有两个循环，一个是物品的循环，一个是商品的循环*/const int N =1010;int f[N];int n, m;struct Thing&#123; int v, w, s;&#125;;vector&lt;Thing&gt; things;int main()&#123; cin &gt;&gt;n&gt;&gt;m; for(int i =0; i&lt;n ;i++) &#123; int v, w, s; cin &gt;&gt; v&gt;&gt; w&gt;&gt;s; if(s ==-1) things.push_back(&#123;v, w, -1&#125;); else if (s ==0) things.push_back(&#123;v, w, 0&#125;); else &#123; for(int k =1; k&lt;=s; k *=2) &#123; s -=k; things.push_back(&#123;v*k, w*k, -1&#125;); &#125; if (s &gt;0) things.push_back(&#123;v*s, w*s, -1&#125;); &#125; &#125; // 01 背包问题 for(auto thing: things) &#123; if(thing.s ==-1) &#123; for( int j =m ; j&gt;= thing.v; j--) f[j] =max(f[j], f[j -thing.v]+ thing.w); &#125; else &#123; for(int j =thing.v ; j&lt;= m; j++) f[j] =max(f[j], f[j -thing.v] +thing.w); &#125; &#125; cout &lt;&lt; f[m]&lt;&lt;endl; return 0;&#125; 二维费用的背包问题 有 N 件物品和一个容量是 V 的背包，背包能承受的最大重量是 M。每件物品只能用一次。体积是$ v_i$， 重量是 $m_i$，价值是$ w_i$。求解将哪些物品装入背包，可使物品总体积不超过背包容量，总重量不超过背包可承受的最大重量，且价值总和最大。输出最大价值。 Tips: 一般来说背包问题只有一个限制条件：V。但是这种是可以扩展到多维，即有多个限制条件，比如重量。很简单的扩展是加上一个循环。同理 f 这样的函数是可以扩展成二维的。 12345678910111213141516171819202122232425262728293031#include&lt;iostream&gt;#include&lt;cstring&gt;#include&lt;algorithm&gt;using namespace std;const int N =1010;int f[N][N];int n,v, m;int main()&#123; cin &gt;&gt; n&gt;&gt; v&gt;&gt;m; for (int i =0 ; i&lt; n; i++) &#123; int a, b, c; cin &gt;&gt; a&gt;&gt; b&gt;&gt;c; for(int j =v ;j&gt;=a ; j--) for(int k =m ; k&gt;= b; k--) &#123; f[j][k] =max(f[j][k], f[j- a][k -b] +c); &#125; &#125; cout &lt;&lt; f[v][m]&lt;&lt; endl; return 0;&#125;]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>背包</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[All you need to know about seq2seq（理论部分）]]></title>
    <url>%2F2019%2F07%2F12%2Fseq2seq-theory%2F</url>
    <content type="text"><![CDATA[介绍 sequence to sequence 的定义、两种不同的结构、训练过程、注意力机制和常见的一些问题。本篇主要是理论，代码demo 可以参考另一篇。 定义和结构 定义 语言模型：对于任意的词序列，它能够计算出这个序列是一句话的概率。 Sequence-to-sequence (seq2seq) 模型，顾名思义，其输入是一个序列，输出也是一个序列. 目前来说，对于Seq2Seq生成模型来说，主要的思路是将该问题作为条件语言模型，在已知输入序列和前序生成序列的条件下,最大化下一目标词的概率，而最终希望得到的是整个输出序列的生成出现的概率最大：$$P ( Y | X ) = \sum _ { t = 1 } ^ { T } \log P \left( y _ { t } | y _ { 1 : t - 1 } , X \right)$$ seq2seq 可以用在很多方面：机器翻译、QA 系统、文档摘要生成、Image Captioning (图片描述生成器)。 Figure 1 介绍了一个标准的seq2seq模型。 其中红色的是encoder RNN, 绿色的是decoder RNN. 他们之间 有一个连线， 也就是encoder states 传给decoder RNN，当做initial state。 两种不同的结构 第一种， seq2seq 模型可简单理解为由三部分组成：Encoder、Decoder 和连接两者的 State Vector (中间状态向量) C 。 第二种结构是最简单的结构，和第一种结构相似，只是 Decoder 的第一个时刻只用到了 Encoder 最后输出的中间状态变量 ： 在两种不同的结构时候，两者的区别在 C (context) 向量是否多次使用，前一种在生成每个word的时候，都使用到了 context，而后者则没有。 训练过程 encoder 激活函数: softmax() 1234crossent = tf.nn.spase_softmax_cross_entropy_with_logits( labels=decoder_outputs, logits=logits)train_loss = (tf.reduce_sum(crossent * target_weights) / batch_size) loss 计算: 交叉熵是损失函数，针对单个样本；目标函数是针对整个数据集的损失函数+ 正则项。 decoder 当你训练你的 NMT 模型时（并且一旦你已经训练了模型），可以在给定之前不可见的源语句的情况下获得翻译。这一过程被称作推理。训练与推理之间有一个明确的区分（测试）：在推理时，我们只访问源语句，即 encoder_inputs。解码的方式有很多种，常见的有greedy 解码和束搜索解码（beam-search）。 我们知道在Seq2Seq模型的最终目的是希望生成的序列发生的概率最大，也就是生成序列的联合概率最大。 实际做的时候有两种算法进行 decoder。 greedy decoding：（贪心算法思维，得到是一种局部最优解） Fig 1 描述的是Inference 的状态， decode 输出来的每一个单词都会当做下一个时刻的输入，来进行翻译。 而在训练过程中， 因为知道翻译出来的单词是什么，就会把这个单词当做输入进行训练。 图中 inference的过程中用到了argmax, 这个函数，也就是每次都选择概率最大的那个单词当做翻译。 这个叫做greedy decoding. 这个不是optimal solution。 贪心搜索只选择了概率最大的一个，而集束搜索 (beam search)则选择了概率最大的前k个。这个k值也叫做集束宽度（Beam Width），算法复杂度是O(nKv), v 是字典的大小， n 是输出序列的长度, k 表示保留的解的个数。 Beam search decoding： 这个方法在每个翻译的步骤 都保存k 个最可能的选择， k 就是beam size, 这个方法虽然不能保证最优解， 但是效率高了很多。 k 一般就是5-10. 上图就是一个 beam size =2 的例子。 在T=0 的时候，选择概率最大的两个。 在T =1 的时候，分别将 the 和 a 输入得到两个概率模型，然后选择概率和最大的两个序列。以此类推，最终得到两个序列。然后前者的概率和大于后者，所以就选择的上面的那个，其他的路径就可以丢掉了。 注意力机制Encoder-Decoder 模型的局限性（有两点） 中间语义向量无法完全表达整个输入序列的信息。 随着输入信息长度的增加，由于向量长度固定，先前编码好的信息会被后来的信息覆盖，丢失很多信息。 Attention 模型的特点是 Decoder 不再将整个输入序列编码为固定长度的中间语义向量 Ｃ ，而是根据当前生成的新单词计算新的 $C_{i}$ ，使得每个时刻输入不同的 Ｃ，这样就解决了单词信息丢失的问题。引入了 Attention 的 Encoder-Decoder 模型如下图： 应用在英文翻译中，将英文输入到 Encoder 中，Decoder 输出中文。在图像标注中，将图像特征输入到 Encoder 中，Decoder 输出一段文字对图像的描述。在 QA 系统中，将提出的问题输入 Encoder 中，Decoder 输出对于问题的回答。 常见问题 Exposure Bias： 问题描述：Seq2Seq模型训练的过程中，编码部分的下一个时刻的输出，是需要根据上一个时刻的输出和上一个时刻的隐藏状态和语义变量Ci.此时上一个时刻的输出使用的是真实的token。而在验证Seq2Seq模型的时候，由于不知道上一个时刻的真实token，上一个时刻的输出使用的是上上个时刻的预测的输出token。这将引发Exposure Bias(曝光偏差问题)。 使用真实 token 本身的行为叫做 teacher forcing。 一种解决思路：使用Beam Search的Encoder的方式也能一定程度上降低Exposure Bias问题 （该问题可以理解为过分的依赖上一个真实的 token 或者说标签），因为其考虑了全局解码概率，而不仅仅依赖与前一个词的输出，所以模型前一个预测错误而带来的误差传递的可能性就降低了 exposure bias 的定义：RNN 在 training 时接受 ground truth input，但 testing 时却接受自己之前的 output，这两个 setting不一致会导致 error accumulate解决思路：为了解决 exposure bias 问题，提出了 scheduled sampling，即在先前的 GroundTrue 单词和先前产生的单词之间随机选择，已经成为适合基于RNN的模型的当前主要训练程序。 然而，它只能减轻 exposure bias，但不能很大程度上解决它。 OOV 和低频词 问题描述：OOV表示的是词汇表外的未登录词，低频词则是词汇表中的出现次数较低的词。在Decoder阶段时预测的词来自于词汇表，这就造成了未登录词难以生成，低频词也比较小的概率被预测生成。 一种解决思路：如果 focus 在文本摘录领域，由于其任务的特点，很多OOV 或者不常见的的词其实可以从输入序列中找到，因此一个很自然的想法就是去预测一个开关（switch）的概率 $P(si=1)=f(hi,yi-1,ci) $，如果开关打开了，就是正常地预测词表；如果开关关上了，就需要去原文中指向一个位置作为输出。 连续生成重复词的问题 问题需要从本质上进行理解，为什么生成了重复复的词语，就是在目前生成的词语上，下一个词语发现某个词一直是概率最大的。所以一种方式是通过 beam search，给出了多种不同的候选词汇，然后计算整体的句子的概率。 一种方式是判断然后惩戒，发现生成了和上一个词语相同的词语，那么拒绝这个词语。 teaching forcing其实RNN存在着两种训练模式(mode): free-running mode: 就是大家常见的那种训练网络的方式: 上一个state的输出作为下一个state的输入。问题： 如果一个unit 出现了garbage ，那么必定是会影响后面一片 unit 的学习效果；造成了不收敛 teacher-forcing mode: 使用目标语言中的实际输出作为 decoder 的输入。优点： 训练变得稳定，快速的收敛缺点：效果并不是那么好，因为在 test 的时候，是没有ground true 的支持，所以模型就效果很差。 常见的几种处理手段：集束搜索（beam search）：产生多个候选词，优化输出序列。实际上是一种剪枝后的深度优先搜索算法。计划学习（curriculum learning）：总的思想，训练开始是 teacher forcing mode，然后随着训练的过程，慢慢降低 ground true的频率。 定期采样Scheduled Sampling：（这个又是一种中和策略，需要调参） 随机的使用目标语言的输出和 decoder 预测的输出。 相对于 beam search而言，每个时刻只是选择 top1的那种，类似一种求解局部最优解，实际上是一种贪心算法。 复习 有两种encoder-decoder 结构，区别在于中间状态信息(context) 是否被使用多次。一种是在生成每个词语的时候都用到了 context 信息，一种是只有在decoder 第一个word 的时候使用context信息。 损失函数、代价函数和目标函数的区别。损失函数是针对单个样本， 代价函数是针对整个数据集，目标函数是代价函数+ 正则项。目标函数是可以最大化或者最小化，但是代价函数是需要最小化的函数。 在encoder-decoder 的模型中希望生成序列发生的概率最大，也就是生成序列的联合概率最大。有两种算法来记性decoder，一种是贪心搜索，得到的是局部最优解；一种是 beam search。前者是根据模型和数据每次选择概率最大的一个；后者是选择概率最大的前K 个，这个参数称为 beam width。算法复杂度相对前者是K 倍的。 exposure bias 是指在训练和predict 过程中模型输入不一致而导致的问题。比如说，在训练过程中，使用上一个真实的词输入（使用真实token 本身的行为叫做teacher forcing）到下一个预测的神经元中，而在预测的时候是没有真实词的输入。这会导致错误的累计。解决思路：(1) 使用 scheduled sampling，在训练的时候 真实单词和之前产生的单词随机选择，使得模型不过分依赖真实单词。(2): 使用beam search 的思想，通过多条路径而非单一路径缓解。 连续生成重复词语问题的解决思路: (1) beam search 的思路，给出多个不同的候选词 (2) 判断惩戒，如果生成了相同的上一个词语，那么拒绝这个词语。 对于decoder 中神经元的输入存在着两种模式：(1) free-running mode， 使用上一个状态的输出组委下一个状态的输入；(2) teacher forcing: 使用 真实结果进行指导，这样使得训练变得稳定，快速的收敛。]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>teacher_forcing</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[A Gentle Introduction of GBDT]]></title>
    <url>%2F2019%2F07%2F12%2Fgbdt%2F</url>
    <content type="text"><![CDATA[介绍GBDT 的定义、训练过程、优缺点和常见的问题。 Boosting、bagging和stacking是集成学习的三种主要方法。不同于bagging方法，boosting方法通过分步迭代（stage-wise）的方式来构建模型，在迭代的每一步构建的弱学习器都是为了弥补已有模型的不足，通过弱学习器提升为 强学习器的算法。Boosting族算法的著名代表是AdaBoost 和GBDT。 定义由于GBDT的学习过程是通过多轮迭代，每次都在上一轮训练结果的残差（如果是回归问题，使用平方误差作为loss function）的基础上进行学习（基函数的线性组合），来对数据进行分类或者回归。训练的过程是通过不断降低偏差来提高最后分类器的精度。 理解GBDT要分为两步，第一步是理解什么叫做用决策树去拟合当前模型的残差，第二步是理解为什么以及如何用损失函数的负梯度去替代当前模型的残差。 GDBT 处理分类问题GBDT在解决分类问题时有两种办法，一个是选择指数损失函数作为损失函数，此时GBDT退化为AdaBoost算法，另一个是选择类似于逻辑回归的对数似然损失函数。（逻辑回归使用的是对数似然函数） 使用平方损失函数时候，GBDT算法的每一步在生成决策树只是拟合前面模型的残差，（y- y^）残差是梯度的一个特例。而当损失函数是其他的形式时候，下一次迭代是使用的负梯度值。 对于一般损失函数，为了使其取得最小值，通过梯度下降算法，每次朝着损失函数的负梯度方向逐步移动，最终使得损失函数极小的方法（此方法要求损失函数可导）。 总的来说，第一颗树是由基尼系数决定的，之后所有的树的决策全是由残差来决定。 GBDT的思想可以用一个通俗的例子解释，假如有个人30岁，我们首先用20岁去拟合，发现损失有10岁，这时我们用6岁去拟合剩下的损失，发现差距还有4岁，第三轮我们用3岁拟合剩下的差距，差距就只有一岁了。如果我们的迭代轮数还没有完，可以继续迭代下面，每一轮迭代，拟合的岁数误差都会减小。 训练过程How Gradient Boosting WorksGradient boosting involves three elements: A loss function to be optimized. A weak learner to make predictions. An additive model to add weak learners to minimize the loss function. loss functionThe loss function used depends on the type of problem being solved.It must be differentiable, but many standard loss functions are supported and you can define your own.For example, regression may use a squared error and classification may use logarithmic loss.A benefit of the gradient boosting framework is that a new boosting algorithm does not have to be derived for each loss function that may want to be used, instead, it is a generic enough framework that any differentiable loss function can be used. log loss主要用来衡量二分类，cross entroy主要用来衡量多分类，前者是后者在二分类下的特例交叉熵公式 (cross entroy) $- \sum _ { i = 1 } ^ { K } p _ { i } \log q _ { i }$ 其中 $p_i$ 表示真实的分布， $q_i$ 表示预测分布, $K$ 表示分类数; 当 $K =2$ 交叉熵退化成 log loss: $- [ y \log \hat { y } + (1-y)log(1 -\hat{y}) ]$ cross entroy与logloss主要用来衡量分类算法性能，因为cross entroy意义是衡量真实分布p和预测分布q的分布差异程度；mse主要用来衡量回归算法性能； weak learner弱分类器一般选择 CART （分类回归树） Classification And Regression Tree(CART)是决策树的一种，并且是非常重要的决策树， Decision trees are used as the weak learner in gradient boosting.Specifically regression trees are used that output real values for splits and whose output can be added together, allowing subsequent models outputs to be added and “correct” the residuals in the predictions. It is common to constrain the weak learners in specific ways, such as a maximum number of layers, nodes, splits or leaf nodes.This is to ensure that the learners remain weak, but can still be constructed in a greedy manner. additive modelTrees are added one at a time, and existing trees in the model are not changed.A gradient descent procedure is used to minimize the loss when adding trees. Traditionally, gradient descent is used to minimize a set of parameters, such as the coefficients in a regression equation or weights in a neural network. After calculating error or loss, the weights are updated to minimize that error.Instead of parameters, we have weak learner sub-models or more specifically decision trees. After calculating the loss, to perform the gradient descent procedure, we must add a tree to the model that reduces the loss (i.e. follow the gradient). We do this by parameterizing the tree, then modify the parameters of the tree and move in the right direction by (reducing the residual loss.Generally this approach is called functional gradient descent or gradient descent with functions. GBDT学习过程：先使用一个初始值来学习一个决策树，叶子可以得到预测的值，以及残差，然后后面的决策树是基于前面的决策树的残差进行学习，直到残差为0.对于测试样本的预测值，就是许多决策树预测值的累加。 GBDT通过多轮迭代,每轮迭代产生一个弱分类器，每个分类器在上一轮分类器的残差基础上进行训练。对弱分类器的要求一般是足够简单，并且是低方差和高偏差的。因为训练的过程是通过降低偏差来不断提高最终分类器的精度。 损失函数：二分 log loss ，多分 交叉熵loss，回归 平方损失弱学习器（weak learner）：加法模型： 使用梯度下降的方式去减少loss， 当增加一个新的树A gradient descent procedure is used to minimize the loss when adding trees. 具体说这个弱学习器，GBDT 选择特征的过程就是 CART (分类回归树)选择特征的过程。选择特征是：遍历每个特征和每个特征的所有切分点，找到最优的特征和最优的切分点。多个CART TREE 生成过程中，选择最优特征切分较多的特征就是重要的特征。 目前GBDT 的算法比较好的库是 xgboost 优缺点优点（相对于 LR or SVM）： 可以灵活的处理各种类型的数据，包括连续型和离散型，处理分类和回归问题 继承了树模型的优点： 对异常值鲁棒，处理缺省值 即使是大量数据，也可以方便的处理，相对比SVM 来说。 缺点： 无法并行 在不带噪声的数据上，分类效果不如LR或者 SVM 常见的问题 为什么GBDT要把CART回归树树分成m棵二叉树去求（每棵树只有两个叶子节点），而不是求一棵二叉树，这棵树有m+1（最多有2m个叶子节点）层呢？这是为了解决过拟合问题，基学习器要具有简单、高偏差和低方差的特点，因此每棵CART回归树的深度不会很深。 为什么第m次学习的目标，是前m-1棵树预测值的累加和的残差？一方面通过分步求解，一步步逼近目标值，比一步到位要简单；另一方面每一步的残差计算其实变相地增大了被分错的实例的权重，因为被分错的实例其残差较大，而已经分对的实例的残差趋近于0。这样后面的树就能越来越专注于前面被分错的实例了。提升树是迭代多棵回归树来共同决策。当采用平方误差损失函数时，每一棵回归树学习的是之前所有树的结论和残差，拟合得到一个当前的残差回归树，残差的意义如公式：残差 = 真实值 - 预测值 。提升树即是整个迭代过程生成的回归树的累加。 AdaBoost 算法和 GBDT 算法的区别Boosting族算法的著名代表是AdaBoost，AdaBoost算法通过给已有模型预测错误的样本更高的权重，使得先前的学习器做错的训练样本在后续受到更多的关注的方式来弥补已有模型的不足。与AdaBoost算法不同，梯度提升方法在迭代的每一步构建一个能够沿着梯度最陡的方向降低损失（steepest-descent）的学习器来弥补已有模型的不足。经典的AdaBoost算法只能处理采用指数损失函数的二分类学习任务2，而梯度提升方法通过设置不同的可微损失函数可以处理各类学习任务（多分类、回归、Ranking等），应用范围大大扩展。另一方面，AdaBoost算法对异常点（outlier）比较敏感，而梯度提升算法通过引入bagging思想、加入正则项等方法能够有效地抵御训练数据中的噪音，具有更好的健壮性。这也是为什么梯度提升算法（尤其是采用决策树作为弱学习器的GBDT算法）如此流行的原因，有种观点认为GBDT是性能最好的机器学习算法，这当然有点过于激进又固步自封的味道，但通常各类机器学习算法比赛的赢家们都非常青睐GBDT算法，由此可见该算法的实力不可小觑。 损失函数：adaboost 使用的是指数损失函数的算法，而gbdt 可以使用不同的可微的损失函数进行分类和回归问题 adaboost 一般是用于分类，gbt 一般用于回归。 为什么是低方差？gbdt通过多轮迭代,每轮迭代产生一个弱分类器，每个分类器在上一轮分类器的残差基础上进行训练。对弱分类器的要求一般是足够简单，并且是低方差和高偏差的。因为训练的过程是通过降低偏差来不断提高最终分类器的精度，（此处是可以证明的）。通俗的理解，方差比较低就是模型表现的稳定与否。 什么是NP 问题?NP问题是指可以在多项式的时间里验证一个解的问题。NP问题的另一个定义是，可以在多项式的时间里猜出一个解的问题。 Bagging 算法?Bagging算法是这样做的：每个分类器都随机从原样本中做有放回的采样，然后分别在这些采样后的样本上训练分类器，然后再把这些分类器组合起来。简单的多数投票一般就可以。其代表算法是随机森林 AdaBoostAdaBoost，是英文”Adaptive Boosting”（自适应增强）的缩写。它的自适应在于：前一个基本分类器分错的样本会得到加强，加权后的全体样本再次被用来训练下一个基本分类器。同时，在每一轮中加入一个新的弱分类器，直到达到某个预定的足够小的错误率或达到预先指定的最大迭代次数。 具体说来，整个Adaboost 迭代算法就3步： 初始化训练数据的权值分布。如果有N个样本，则每一个训练样本最开始时都被赋予相同的权值：1/N。 训练弱分类器。具体训练过程中，如果某个样本点已经被准确地分类，那么在构造下一个训练集中，它的权值就被降低；相反，如果某个样本点没有被准确地分类，那么它的权值就得到提高。然后，权值更新过的样本集被用于训练下一个分类器，整个训练过程如此迭代地进行下去。 将各个训练得到的弱分类器组合成强分类器。各个弱分类器的训练过程结束后，加大分类误差率小的弱分类器的权重，使其在最终的分类函数中起着较大的决定作用，而降低分类误差率大的弱分类器的权重，使其在最终的分类函数中起着较小的决定作用。换言之，误差率低的弱分类器在最终分类器中占的权重较大，否则较小。 对于决策树，Adaboost分类用了CART分类树，而Adaboost回归用了CART回归树。 AdaBoost 的优点： 作为分类器，分类精度比较高 简单的二元分类器，构造简单，结果可理解 不容易发生过拟合缺点： 对异常样本敏感，异常样本在迭代过程中可能获得较高的权重，影响最终的强学习器的预测准确率。 复习总结 gbdt 的优点a. 可以灵活的处理各类数据（连续性和离散型）， 使用稠密的数据更加，对于稀疏的数据，可能LR 这些模型更加好。b. 继承了树模型的优点： 对于异常值和缺省值有很好的处理的效果。因为是在树节点分裂，那么异常值的影响是比较小的；单独把缺省值放到一边，最后处理，可以支持对缺省值的处理c. 大量数据也可以方便处理（相对于 svm 而言），如果说支持数据量最大的，还是 LR缺点：a. 如果噪声很多，那么是容易过拟合； 如果在不带噪声的数据集上，分类效果不如LR 或者 SVM；现实中的数据是有点噪声，那么使用 GBDT 还是可以的。b. 无法并行（主要是指 决策树的建立过程） gbdt 的损失函数如果是二分类一般使用log loss，多分类使用交叉熵损失函数，回归算法使用均方误差损失函数。 gbdt 核心的知识点a. 基学习器需要足够的简单，具有高偏差和低方差，这个是为了缓解过拟合的问题b. 通过多个分类器的线性累加求最后的预测结果，变相的增加了被分错的样本的权重（分对实例的残差是0） adaboost 和gbd的区别a. 首先两者都是boosting提升算法。区别在于实现方式，adaboost 是分配权重，adaboost在下一轮的循环中分错的样本得到加强，分对的分类器得到加强；gbdt 是通过梯度下降的方式实现的，定义一个loss，然后分错的loss 是比较大的，然后通过减少loss间接的重视分错的样本。b. adaboost 对于异常值比较敏感，gbdt 每一个分类器都是弱分类器，并且可以加上正则项，抵御数据中噪声c. adaboost 损失函数只能是 指数损失函数， gbdt 可以针对不同的问题，选择合适的损失函数d. adaboost 一般用于分类，gbt 一般用于回归]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>gbdt</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Summary]]></title>
    <url>%2F2019%2F07%2F10%2Fsummary_resume%2F</url>
    <content type="text"><![CDATA[something 总的大纲 面试的时候还是尽量控制一下，放慢语速，别着急，想清楚了再好好说；(已经找不上工作了，别慌) 表现欲要控制好，不要得意忘形，点到为止，不然容易挖更多的坑。 这次有一点压力面的，遇到这种情况，心态积极开放一些，反正我是全程笑嘻嘻，谦虚请教的态度，做的不好就承认，虚心接受，质疑的地方也要敢于拿实力证明，毕竟还年轻，有很大的进步空间。（就是在心态上一定要是平和的，不卑不亢） 爱奇艺实习经历面试官好，我叫贾继征，来自北京航空航天大学计算机学院。在研究生期间…总的来说之前分别在腾讯和爱奇艺实习过一段时间，分别做短文本相似度计算和文本分类、然后以第一作者身份分别发表了一篇C 类会议和EI 会议，一个涉及的nlp 文本分类问题，一个是深度学习领域中的gan 网络。做过一些关于机器学习项目和NLP项目，nlp 项目主要是处理短文本，句子的相似度计算和文章的判重，完成了一个类似faker 的开源工具。 从后往前说，爱奇艺实习经历：实习内容的背景使用nlp 和机器学习知识对推荐中query的识别，实际上做到后来不只是是低俗内容的识别，一开始是用于软色情的判断，后来模型加大了识别的范围，凡是一切不适合出现在推荐中的query，都需要进行判断。比如暴力，不符合积极的正面宣传内容都会进行识别。 如何实现？ 训练数据集300万，有标注， 分别使用 1 0 表示低俗和非低俗数据，人工标注，能有95%的准确率；两个测试数据集分别是1万（从一个月中的query 抽选出来的数据，一个是原始的数据，一个是经过规则筛选的测试数据集） 。训练数据集中正负样本1 ：1.5，但在测试集中使用的是一个月中的数据，所以正负样本比较高，达到了18：1。正常query 是低俗query 的18倍。 模型有两部分组成。一部分是基于transformer 的语义抽取，一部分是基于xgboost 进行的特征提取，训练过程中优化两个loss。 为什么使用 xgboost 机器学习进行特征提取呢？因为这里使用到了其他维度的信息，而这些信息是有助于对query 进行判别的。分成三部分：使用 fasttext 获得query 的n-gram 特征，常规word embeddingquery 和channel 的特征，对于色情这个维度，影视channel 和知识类 channel是有不同的影响的使用DSSM（获得是query 和item 之间点击的信息，得到的模型中也是计算相似度的）这些特征都是 一个transformer 没有办法获得的 然后使用xgboost +LR 模型作为这一部分的网络结构。 另一部分相对来说就比较简单，使用 transformer作为一个语义识别的模型。然后得到的两个loss，在训练过程中一块进行训练。 线下版本因为正负样本比较均匀，直接使用AUC 作为评价指标，auc 很高了，能够达到0.95左右了，有点过拟合了。在某种意义上就是想要过拟合，这可以很好的处理一些case 的问题，但是模型的优化的目标不是这样的，是希望在测试数据集上效果好的。AUC是 0.89。 线上因为正负样本的问题，使用F1 作为评价指标，使用字向量和拼音向量之后又 4%的提升。（0.79-0.83 ） 难点对于query 的理解应该从字面意思、内涵和搜索意图进行全方位的理解。字面可以使用规则进行判断，但是一些词语（比如韩国办公室沙发，公公和儿媳妇）这种很难使用单一的规则进行判断。应该根据语义信息进行判断。query 中还有一点，可能出现拼音混合汉字，错别字等现象。这种现象的存在意味着不能使用传统的分词工具进行处理，因为这个是分不准的。我们的处理方案是”字向量“+ ”拼音向量“用来补充原来的词向量。 线上版本是有要求的，是要保证precision在 0.9为底线的情况下，然后去提高recall，抱有的是宁缺毋滥的观念，绝对不能是推荐出来不好的query。 可能存在的问题：数据标准 定的标准是95%，虽然人工标注这种精度已经很高，但是还是有error的，并且这种error 在短期内是不太可能有大的提升的。线上和线下正负样本比不一致，也是误差来源之一模型依赖于别的团队的channel embedding和DSSM embedding，也可能是一个误差的来源。自己训练的fasttext 的word embeddingprecision (0.92)很高，recall(0.6~7) 的样子，说明正样本的特征并不明显，可能的一种解释是中性词，歧义词，存在二义的词语比较多（当然这种假设是可以验证的，可以看一下false positive rate ) 预测成了假的）。所以可以进一步去挖掘正样本（低俗词汇）的特征；这种特征就是上面说的可能是query 的语义和搜索意图，而不单单是quer 本身的意思。后期的改进： 后期打算，加上万能的bert，然后三个loss 进行一块训练，应该是会得到更好的query 的表示。bert 是一个预训练模型。 正负样本是一个相对的概念，对于该研究问题，那么低俗词汇就是正样本，负样本就是正常的词汇。 Bad case 分析：主要分成三类 本身label 就有问题， 比如case “爱死亡与机器人”(这个应该是电影) 被打成1，然而预测是0；”口疮咋治疗” 被label 成1，然而预测是0. 二分类错误。比如” ”武则天用的什么安慰器”” 被label 成1，然而预测为0； Query 本身不太好分类的。比如” 女人腿肥怎么办” 被label 成0，但是预测为1；”吃吐了” 被label 成1，但是预测为0. 我觉得可以对bad case再进行一下划分，不是所有的bad case 都需要处理的。具体而言，第二种情况是应该处理的。第一种情况是打标签的准确度问题，第三种情况是需要看具体语境了，并不是一种很明显地可以划分为低俗与否的问题。应该着重的看看一下第二种情况。 改进的地方：有一些中性词，比如”爱死亡与机器人“ 是一个中性词，但是在label 的时候就被标记错误了，所以对于这类词语应该怎么办？ 腾讯新闻的实习经历 绝对提高和相对提高假如之前的水平是a，然后之后的水平是b，那么绝对提高就是 (b -a); 相对提高 (b -a) /a *100\% . CTR最开始是多少？这个时间有点长了，具体忘记了，但前后模型是有增长效果的。 faiss 的介绍首先使用 index对于向量进行预处理，然后选择不同的模式. 主要讲的是三种模式，一个维度是简单模式，适合在小数据上进行计算 欧氏距离；一个维度是加快检索速度，这种模式下是需要提前的train，其基本的思路对向量进行聚类，当然文中说的是 “细胞”，建立倒排索引，然后检索的时候，搜索这个“细胞”内 和周围的“细胞” 的id 的集合，就可以返回前 K 个最相近的结果；最后一个维度是减少内存的使用，上面两种都是使用的完整的向量，这个模式下是使用的压缩向量，可以使用PCA 进行实现，当然这个模式下得到的结果也是近似解。还有两种计算的上的优化，对于向量进行分段计算，这种可以实现并行，并且支持任务在GPU 上进行运算。（一般的情况是文档id 作为索引，文档的内容作为记录；倒排索引，是文章内容关键字作为索引，文档id作为记录） 什么是 AB test?AB test 测试强调的是在同一时间维度对相似属性分组用户的测试，时间的统一性有效的规避了因为时间、季节因素带来的影响，属性的相似性（vv）则使得地域、性别、年龄等其他因素对统计的影响降为最低。对于测试的流量不宜太大，应该逐步增大流量，同时如果太小又有随机性的干扰。时间上保证是完整的一周，包括工作日和周末。AB test 也是一个迭代的过程。AB test 有两种情况，一种是界面 UI的修改，一种是后台算法的修改。在对产品进行A/B测试时，我们可以为同一个优化目标（例如优化购买转化率）制定两个方案（比如两个页面），让一部分用户使用A 方案，同时另一部分用户使用 B 方案，统计并对比不同方案的转化率、点击量、留存率等指标，以判断不同方案的优劣并进行决策，从而提升转化率。ab test 存在缺点，如果只是单纯的选择用户，那么可能存在观看视频习惯不同对最后结论产生不利的影响。所以要尽可能的选择属性相同的用户，挑选出活跃用户（vv达到某一个值的用户）。分别施以模型A和B，才能验证模型A的效果。还要一种思路，如何在不分组的情况下，进行测试呢？给用户两个算法召回的结果（等概率出现），然后记录用户的行为数据，然后判断a b 两种方案的优劣。这种思路就排除了用户本身带来的误差。summary：同一个时间维度是容易保证的，但是属性的相似性比较难保证。这里有两种策略，一种是选择属性相同的用户，比如说挑选出北京是中活跃的用户，然后分成两组进行测试；一种是在一个地区的用户分别等概率的提供两种算法，最后看效果（比如说ctr ，留存，用户时长等）。 句子向量的训练过程？word embedding 表示的句子的缺点：没有词序；没有上下文；依赖前期的处理SIF 这种句子向量 （无监督，非网络结构） 如何进行 sentence 训练?无监督模型，通过skip-though 举例说明：skip-gram 根据中心词汇预测上下文skip-thought 根据中心句 预测上下句encoder -decoderencoder 就是一个 特征提取，然后又两个decoder， 是语言模型 ，分别对应着 上一句和下一句统计语言模型是一个单词序列上的概率分布，对于一个给定长度为m的序列，它可以为整个序列产生一个概率 $P(w_1,w_2,…,w_m) $ 。 词汇扩展使用 word2vec 得到的词向量 来补充 encoder 过程中 该问题的词向量，要求前者的规模是远远大于后者的。通过这种方法补充 encoder 过程中的词汇。 数据集数据集中的句子是有衔接关系的，论文中的数据集是 google news dataset。要求有一定的逻辑的文章。 有监督模型：infersent 综述类型，找到了 nlp 领域的imagenet 和 inception network。网络结构模型是 使用一个encoder 得到向量表示，然后记性 u 相加 和点乘 三种操作，得到 fully-connection 最后是softmax。encoder 是 bi-lstm 并且使用 max pooling 进行网络结构的优化。数据集是 句子对组成的数据集，标注信息是数据间的关系（蕴含，对立和中性）数据量 570k。 具体实验对比的是 RNN lstm gru 然后搭配着 单向or 双向 max pooling or average pooling 进行做的实验。 universal sentence encoder 上面那个是 facebook 团队出的，然后这个是 google 出的，主要的区别在于使用 transformer 代替bi-lstm 作为encoder。 训练的时候，既进行了有监督的学习，又进行了无监督的训练。 图像处理和nlp 的差别：离散 or 连续； 定长 or 非定长；稠密 or 稀疏。 对于 bert transformer 的介绍？bert 是集大成者，前面还有很多重要的研究成果，比如attention，self-attention 和transformer。直接从 transformer 说起，大的结构还是 encoder、decoder 的结构。encoder 是6 个两层结构， decoder 也是 6个三层结构。transformer 中有几个技术点是需要讲解： layer normalization （主要 vs batch normalization）、position embedding （在encoder 中是结合了 word embedding 一块作为输入的）、masked attention（当predict t 时刻的word的时候，只能看到 t 之前的情况，将 t 之后的word 进行了 mask）、multi-head attention（类似 图像中的多个 filter ）、self-attention 就不多说了从 transformer 到 bert 又是一个比较复杂的模型组合的工作。 6.1 关于bert 和lstm 的比较（解答这种问题需要有一个切入点，那么就是 lstm 的缺点）lstm 是无法解决长依赖的问题，lstm 使用输入门，遗忘门和输出门可以缓解一定程度上的长依赖的问题，但是当句子更长的时候，那么也是无能为力的。 bert 是一个集大成这，其中大量使用了 transformer的结构，而transformer 结构是基于 attention机制，所以可以解决长依赖的问题。除此之外，bert是可以在 GPU 上进行fine tune，那么意味着计算效率是比较高的，可以并行运算。当然还有其他的，比如 bert 是一个预训练模型，可以在特定的任务上进行fine tune 得到比较好的结果，比如我论文中的模型就是基于 bert 进行fine tune ，然后不需要怎么训练，效果就已经很好了。 效果好并且速度比较快，这个快是指的 fine-tune 的效果，不是从头开始训练的时间。（一般都是从 速度 和效果两方面评价模型的好坏）效果主要体现在表示语义，上下文信息；给定it 能够知道指代的关系 (能多说点就多说点，因为一个面试官的面试时间有限的，那么是需要尽可能在这个时间段里面让其少问问题，自己尽量把这个充实起来) 介绍transformer的时候，但是在这两层中间还有一层attention层，帮助当前节点获取到当前需要关注的重点内容。 正弦余弦函数用来编码绝对的位置信息，也是可以用来表示位置之间的相对的距离。 batch normalization layer normalization, instance normalization, group normalization 两种mask 技术： padding mask（输入数据） 和sequence mask（decoder） decoder有三层结构 multi-head attention+ encoder-decoder attention 和前向传播公式需要掌握： $$\text { Attention }(Q, K, V)=\operatorname{softmax}\left(\frac{Q K^{T}}{\sqrt{d_{k}}}\right) V$$ lstm 相对于 rnn 是能够处理一部分的长依赖的，但是当句子更加的长的时候，效果也是不太好的；并且计算量比较大。 elmo 中使用了 n-gram（sub-words）的思想 bert 的两个训练任务，一个是随机遮蔽掉一个词，利用上下文进行预测；一个是预测下一个句子。处理句内关系和句间关系。第一个任务：随机选择15%的token，大多数的token 是被 mask 特殊字符取代；一部分是随机被其他单词取代；一部分是保留原词。目的是为了模型对上下文进行表示，减少训练集和test 集合中的数据分布的不统一。第二个任务：输入两个句子A和B，预测B 是否为A 的下一个句子，50% 是下一个句子，剩余的都不是。目标是最小化两种策略的组合函数。 输入： token embedding词向量（肯定是subword的，这个都是标配）+ segment embedding区分两个句子 + position embedding表示位置信息。 更大的数据集：使用了bookcorpus 和维基百科中的数据 改进的空间：从字模型到词模型，科大讯飞和哈工大有做这方面的研究probability这个词被切分成”pro”、”#babi”和”#lity”3个WordPiece。有可能出现的一种随机Mask是把”#babi” Mask住，但是”pro”和”#lity”没有被Mask。这样的预测任务就变得容易了，因为在”pro”和”#lity”之间基本上只能是”#babi”了。这样它只需要记住一些词(WordPiece的序列)就可以完成这个任务，而不是根据上下文的语义关系来预测出来的。类似的中文的词”模型”也可能被Mask部分(其实用”琵琶”的例子可能更好，因为这两个字只能一起出现而不能单独出现)，这也会让预测变得容易。为了解决这个问题，很自然的想法就是词作为一个整体要么都Mask要么都不Mask，这就是所谓的Whole Word Masking。 对于transformer 或者bert 本身的模型优化，这个模型还是有点复杂。 速度上计算的优势， bert 是基于transformer，而transformer是基于self-attention，self-attention 是可以连接任意两个结点，所以可以并行的运算。 bert 的 关于bert 和transformer中的输入（input）？ 在 word2vec 中使用 one-hot 方式进行表示，vector 的长度就是词汇表 vocabulary 的长度。但是在transformer中并不这样做，因为这种方式过于稀疏。如果是在pytorch 中，经常使用 nn.Embedding 来做或者使用 one-hot 和权重矩阵 $W$ 相乘得到。nn.Embedding 包含一个权重矩阵 W，对应的 shape 为 ( num_embeddings，embedding_dim )。num_embeddings 指的是词汇量，即想要翻译的 vocabulary 的长度。embedding_dim 指的是想用多长的 vector 来表达一个词，可以任意选择，比如64，128，256，512等。在 Transformer 论文中选择的是512(即 d_model =512)。处理 nn.Embedding 权重矩阵有两种选择： 使用 pre-trained 的 embeddings 并固化，这种情况下实际就是一个 lookup table。 对其进行随机初始化(当然也可以选择 pre-trained 的结果)，但设为 trainable。这样在 training 过程中不断地对 embeddings 进行改进。Transformer 选择的是后者。但是这个是没有位置信息的，所以加入了position embedding。 10分钟带你深入理解Transformer原理及实现 关键词提取技术卡方分布：卡方检验是以χ2分布为基础的一种常用假设检验方法。该检验的基本思想是：首先假设$H_0$（比如说某个特征和label 无关）成立，基于此前提计算出χ2值，它表示观察值与理论值之间的偏离程度。卡方分布中的参数F（自由度，（行数 - 1） * （列数 - 1）），有了自由度那么就可以得到对应表中的卡方值，然后得到原假设成立的概率。但是在实际应用中，只需要将最后的将最后的结果进行排序，选择最大的前K，就是最后的结果。缺点： 考虑的是出现与否，不是频率；没有考虑语意信息。 多线程，多进行python 中的实现， 多线程是 ， 使用threading， 处理的是io 响应；多进程是Concurrency， 使用multiprocessing 包，处理的是多核cpu的操作。并发和并行，并发只是用户感觉，通过快速切换，保存现场实现；并行是真正的同一时刻运行着不同的程序。对于io 绑定的任务，使用多进程可以提高性能；使用多进程也可以提高性能，但是开销往往比多线程高。比如说访问多个网站，使用多线程进行爬取网页的任务的效率是高于单个线程的效率。（大部分和io 绑定的程序 都是在等待输入输出，无所事事。比如由于网络延迟，导致来自网络 数据库 文件所花费的时间是远远大于 cpu 处理时间，那么这个时候使用多线程就比较nice）对于cpu 绑定的任务，使用多线程往往可以降低性能，使用多进程可以提高性能。比如说就算 1w 以内的所有的质数，因为现在的cpu 都是多核的，可以调用多个处理模块，但是当生成的进程数量多于cpu 的处理能力，那么就观察到了性能的下降，因为这个时候使用更多的工作来换取cpu 内核内外的进程。 新词发现这里有三个阈值（都是越大越好）：第一是最小互信息，因为互信息越大说明相关度越大，将n-gram分好的词计算互信息，如果低于阈值，则说明不能成词。第二是最小熵值，因为熵也是越大说明周边词越丰富，计算其左熵和右熵的最小值，如果最小值低于阈值，则说明不能成词。第三个是最少出现次数，为什么有这个数呢？假设前后两个词是完全相关的，出现400次，总共8000词，那么互信息=log((400/8000)/(400/8000)(400/8000))，约掉之后剩下log(8000/400)。但是一个词如果从头到尾出现了一次，但是并不是单词，则互信息为=log((1/8000)/(1/8000)(1/8000))=log(8000/1)，那么它的互信息会更大。取最少出现次数也会出现问题，就是一些低频率的词不能发现。有更加详细的公式 ner 数据集 从github 中有标注的数据集，5万的数据集，一条是20个字左右；人名，地名，机构名、专有名词和其他。在实习的时候使用旁边组的结果，但是后来有做个一个类似的项目，使用的是隐马尔科夫模型进行实体命名识别，正在做的有两个项目，一个是基于 bert + crf 的项目，一个是机器翻译 。 文章去重参加的一个课题项目，针对某个特定的行业，比如房地产、互联网，基于大数据，对网页或者文章进行分析，使用数据挖掘的手段，捕捉热点，然后为企业或者个人提供事件或者品牌的情感分析、品牌分析。这个是大的项目背景。 我参与的是在于对网页或者网页的去重工作，因为之后的文章分析的指标计算，比如流行度，都是比较依赖于数据的准确性。数据的来源：新闻（搜狐网、新浪网）电子报纸，app 新闻，分成不同类别的新闻，比如科技、教育、金融和体育。每条数据由标题、站点、url、发布时间、内容、阅读量、评论量等组成，数据量是百万级别的。其中内容部分是由500 到2000不等的字数组成。步骤的话，先是文本的预处理，提取关键词，然后使用simhash 或者minhash 进行计算，重复文章的集合，最后去掉重复的文章。 优化：多线程，字典（有一些品牌类的，人名类，需要进行人工的添加、新词发现，网络词汇） 如何去考察最后去重的效果，一般来说，simhash 中的距离函数是汉明距离，这个在3 以内，都是可以认为是相似的文章。当然对于定量的评价的话，有一个标注的小的测试集，然后可以在上面进行 baseline的验证。 Home credit default risk问题描述：Home Credit收集了申请人当前的申请表、其他机构的信用数据和之前的申请记录；用这些信息预测申请人违约概率，是一个风控模型。数据方面：原始训练集（30万*122），正负样本不均衡；进行如下操作（1）数据清洗（2）数据整合（3）数据转换（4）数据降维；模型方面：基于LR模型和XGBoost模型，两个模型进行STACK；训练方面：对数据10折交叉验证训练，使用Random Search调参；评价指标：AUC为0.798。排名：Top 2%（110/3337）。 数据方面(1) 数据清洗(2) 特征工程1). 原始特征包括原始特征和通过简单地额操作（加减乘除/分箱）构造的新特征。比如刻度收入和贷款总额的比率，客户家庭人均收入，客户年龄段，客户收入段2). 统计特征除了申请表之外，其他的表是以月为单位的数据，可以对时间序列进行聚合求解汇总统计量（mean, median, std,max, min, sum）3). 时序特征只对时间序列做统计会丢掉很多信息。从实际情况出发，客户近期的申请情况和信用情况相对比之前的更加重要，所以可以按照窗口划分：离本次申请最近的1一个月/2个月/三个月或者和6个月的信用情况。4). 交互特征对Top10的特征进行交互操作，比如说乘积、比值等，尝试是否能够交互出更加优秀的特征。Data Pre-processing 学习笔记 已经总结得比较好了，可以多分析一下。 如何处理正负样本不均匀的情况？参考这里：(Unbalanced Datasets Problems)[https://jijeng.github.io/2019/06/01/unbalanced-datasets/] 如何处理样本极端不均匀的情况？ 进行正负采样之后需要有什么操作吗，得到的是部分的数据的分布，而不是完整的？ 模型方面 xgboostlr GBDTDT中常见的有三种算法：CART算法、ID3和C4.5算法。其中CART算法使用在GBDT 中，既可以处理回归问题也可以处理分类问题，回归问题使用均方差作为损失函数，对于二分类使用gini 系数作为损失函数。树在选择分裂点的时候，先是枚举特征字段，然后是枚举单个特征中可以分裂的点，然后计算gini系数，选择gini系数最小的那个作为分裂点。 Boosting 算法在每次迭代的过程中，从弱学习器通过弥补之前的学习器的不足，变成一个强学习器。著名的代表是adaboost 和gbdt。对于gbdt，如果是回归问题并且使用平方误差作为损失函数，那么每次都是在上一步的结果的残差进行训练，然后对数据进行分类和回归，是不断降低偏差来提高最后分类器的精度。当选择指数函数作为损失函数时候，gbdt 就是adaboost。 Bagging allows multiple similar models with high variance are averaged to decrease variance. Boosting builds multiple incremental models to decrease the bias, while keeping variance small.Bagging中的单个模型方差高，然后使用多个模型来降低总体的方差；Boosting 降低的是偏差，方差能够保持比较小。 Stacking模型融合 We split the training data into K-folds just like K-fold cross-validation. A base model is fitted on the K-1 parts and predictions are made for Kth part. We do for each part of the training data. The base model is then fitted on the whole train data set to calculate its performance on the test set. We repeat the last 3 steps for other base models. Predictions from the train set are used as features for the second level model. Second level model is used to make a prediction on the test set. 训练和验证 使用10 折交叉验证划分数据集，然后进行训练。 模型融合 bagging 是相对独立的训练同质强学习器， 通过有放回的采样，得到的一系列的弱学习器，然后通过组合得到一个强学习器。该方法是可以并行，通过减少偏差提高最后的效果，其中的代表是随机森林。boosting是串行的同质学习器，在上一个模型的基础上，不断减少方差提高效果，只能串行，其中的代表是gbdt。实现是xgboost。 对于机器学习中的特征提取，可以从业务和统计学通用方法两方面入手。首先尽可能的从业务本身，提取更多可解释、稳定的特征。这个项目是一个小的风控模型，其中有一个特征是消费水平比上收入水平，一般来说消费水平对应的是人的消费观，主观意愿多花钱与否；收入水平是客观存在的，如果高收入人群，一般来说消费水平比较高。并且近几个月的消费水平和 长期的消费水平对于风控模型有着不同影响程度，一般来说近期的行为对模型影响更大。所以需要尽可能地懂数据，然后从这些维度提取可解释的特征。 人工调参是技术活，需要对模型和数据有个比价长的认识，一开始的时候常常使用Grid search 和Random search 这个也是可以展开的：domain： 所有的超参数及其值 的dictionary（键值对）Optimization algorithm: 如何去选择下一组 超参数。目标函数：这个是模型中的目标函数results history： 目标函数和 一组超参数的对应关系 关于 results history 的使用就是 random search 和基于贝叶斯方式搜索的区别，前者没有使用这种对应的关系，后者有利用这种关系。 贝叶斯，p(目标函数 | 一组超参数)。 特征工程 特征来源一部分是业务场景，一部分是常规操作。常规操作包括aggregation聚合操作（主要针对子表），特征离散化，组合特征。业务特征，就是根据不同的场景，构造在该场景下重要字段。当然还有一些骚操作，使用机器模型进行特征的构造和选择，优点是work，缺点是可解释性差，比如使用xgboost +LR 模型，前者就是一种特征提取的功能，最后叶子结点的输出，知道其是重要的，但是不知道其含义是什么。 连续特征离散化的好处： (1) 增加了模型的非线性，提升了模型表达能力 (2) 离散化特征对异常数据具有很强的鲁棒性。常用的选取离散点的方法：(1) 等距离离散 (2) 等样本离散 (3) 画图 (4) 根据实际场景，比如对于年龄的划分 特征组合( 1)基本特征的非线性组合 (2) 特征之间的差和乘积商，mean,variance，std 统计学特征 特征选择（降维）的方法: (1) 特征本身 （如果缺省值比较大或者数据的波动比较小） (2) 特征之间的关系（特征之间有较强的相关性，可以使用PCA进行降维） (3 ) 特征和最后target 的关系 (feature importance, 卡房分布， pearson 相关系数) (4) 很多常见的机器学习模型都是一种特征选择的方式，比如xgboost 连续特征离散化，离散特征one-hot 化（这个都是为了LR 使用方便，是不是很押韵） 数据挖掘中的小的trick 这个从某种角度上使用的之前的申请的记录，这个特征是非常强的。识别到user_id（根据生日，职业等信息），在训练集和测试集中发现有8000多个user 是有两个行记录，100多个user 是有3行记录，那么如果之前的申请目标是1，那么这次申请90%也是1. pygen 项目faker 中生成的信息是单列的，个人信息之间是没有联系的，所以想要用在机器学习训练的时候比较难。关键技术：中文名字有很强的性别属性。例如名字中带有“杰”“志”“宏”等字的一般为男性，带有“琬”“佩”“梅”等字的一般为女性。当然也有一些比较中性的字，例如“文”“安”“清”等，比较难猜测性别。 gan 论文D 网络在判别的时候，是从逼真程度上进行判别的，并不是从生成图像多样性上进行判别的，并且是无法从多样性上进行判别的，因为输入到D 网络中的是一个样本或者说是一个 batch的样本，是很难得到生成图像的多样性这个角度的。 同理 G 网络也只是考虑单个的生成图像，对于G 网络图像的多样性是没有考虑在内的。 有时候也会在训练过程中也会利用这种性质，比如说只是为了得到一张或者几张非常逼真好看的图像，而不会很在乎最后是否生成了多样性的图像。 孪生网络最初是用来做指纹的验证，然后扩展到 cv 和nlp 中都有用来做相似度方面的计算。在论文中的是用来作为生成图像之间相似度的计算，该网络中两个 CNN 用于提取特征，然后基于 欧式距离进行计算，输出label 是0-1 之间的数字， 0表示最相近，1表示不相近。损失函数使用的是 对比损失函数，有点类似交叉熵的感觉，分段函数， y =0 的时候是平方损失函数，y =1 的时候，是合页损失函数。 主要的是做了以下的优化：label 浮点化，Label Smoothing :这种距离的计算是通过聚类实现的，在同一个簇中或者相邻簇中的距离小，向着0 靠近；在不同的簇之间的相距大，不超过1. 网络结构上有两点优化：对于对抗生成网络的D 网路的权重进行正则化，具体来说是 谱归一化，使其符合lipschitz 约束，其中超参数 K 取1.即 D(x) -D(y) 的欧式距离是不大于 x-y 的欧式距离的。 加入了 self -attention机制。 XGboost 和GBDT 比较最后的效果提升：数据方面 Ignoring sparse inputs这个是处理缺省值（或者 0）的手段：两者在split 分裂点的时候，都是先不处理数值 0；然后找到分裂点之后，把0 放到哪边造成的loss 下降的比较大，然后就放到哪边。 模型方面 带深度限制的Leaf-wise的叶子生长策略最开始的时候xgboost 中叶子生长方式是 level-wise 的生长策略，实际上这是一种比较非常低效的算法，因为同一层的很多叶子分裂增益较低，没有必要进行搜索和分类。这种不加区分的对待同一层叶子带来的是没有必要的开销。leaf-wise 是 lightGBM 上使用的一种算法，在分裂次数相同的情况下，可以降低更多的误差，得到更好的精度。leaf-wise 的缺点可能会过拟合，所以加上了深度的限制，在保证高效率的同时防止过拟合。 loss function相对比 GBDT，目标函数加入了正则项，因为树结构的模型是很容易过拟合大，加入正则项可以减少模型的复杂度，增加模型的泛化能力。 shrinkage 和 column subsampling提出了两种防止过拟合的方法：衰减因子和采样。前者应用于上层树，后者应用于特征，选择部分特征进行建树。XGBoost利用梯度优化模型算法, 样本是不放回的，想象一个样本连续重复抽出,梯度来回踏步，这显然不利于收敛。但是，XGBoost支持子采样, 也就是每轮计算可以不使用全部样本。 计算效率的提升： find the best splitxgboost 是基于决策树，那么如何快速发现最后的分裂点。一般做法使用遍历所有的样本和所有的特征，时间复杂度是 $( n_{data}n_{features})$。xgboost 采用了一种优化手段，引入一个超参数 number of bins, 时间复杂度优化到 $n_{data}n_{bins}$。当你的 bins 的数量越大，那么进度是越高的，这是一种 trade-off。 支持并行化虽然层次树的建立是串行的，但是在一个结点选择候选结点的时候，是可以使用多进程并行运算的。 loss function是二阶求导（taylor 二阶展开式） 泰勒公式一句话描述：就是用多项式函数去逼近光滑函数。通用式子如下，$$f(x) =\sum_{n=0}^{N} \frac{f^{(n)}(0)}{n !} x^{n}$$有常见的两种写法，一种是 $f(x)$ 在 $x_0$处的基本形式，$$\begin{split}f(x) &amp;=\sum_{n=0}^{\infty} \frac{f^{(n)}(x_{0})}{n !}(x-x_{0})^{n} \\ &amp;=f(x_{0})+f^{1}(x_{0})(x-x_{0})+\frac{f^{2}(x_{0})}{2}(x-x_{0})^{2}+\cdots+\frac{f^{(n)}(x_{0})}{n !}(x-x_{0})^{n}\end{split}$$还有一种常见的写法， $x^{t +1} = x^t + \Delta x$， 将 $f (x^{t+1})$ 在 $x^t$ 处进行泰勒展开$$f\left(x^{t+1}\right)=f\left(x^{t}\right)+f^{1}\left(x^{t}\right) \Delta x+\frac{f^{2}\left(x^{t}\right)}{2} \Delta x^{2}+\cdots$$ machine-learning-xgboost 中的模型学习部分有公式推导，好好看。 fasttext &amp; faissfasttex 中有有两部分： 无监督的学习(subword)词向量的训练 和有监督分类(text classification)任务，使用的是三层网络结构：输入层，隐藏层和输出层。 使用二叉树的结构，时间复杂度从 $O(N) $ 优化到了$log_2(N)$，当使用huffman 树的时候，这种效果更加明显。层次softmax 不是fasttext 的首创，它的改进之处在实现的时候基于 huffman 树而不是普通的二叉树， 属于运算上的优化。利用了类别不均衡的特点，类别多的路径短，整体上的时间效率会提高。 N-gram 一种是基于character-level 对于不常见单词的扩充，解决的是OOV问题；一种是word-level，考虑的是词语周边的信息，加入了context 的信息，local context 的信息。 negative sampling 是解决最后softmax 层中，不更新所有的negative words，只是更新少部分单词，根据词频选择negative words，并且这种词频是经过约束，主要是使得低频词语也有出现的机会。 调参分为字典相关的参数和训练相关参数 fasttext 的和之前 CBOW的区别：网络结构中的输入层，CBOW是经过one-hot的上下文单词，而fasttext 是单词+ n-gram 的特征，在解决OOV效果比较好；另外在最后的输出层，基于huffman 树实现了层次softmax，对于类别不均衡的训练集来说，训练时间会变得更短。 fasttext 的缺点，使用文本分类的时候，当类别比较多的时候提升效果比较明显，否则是容易过拟合的。 fasttext 和 word2vec 的区别? CBOW的输入是目标单词的上下文，fastText的输入是多个单词及其n-gram特征，这些特征用来表示单个文档； CBOW的输入单词被onehot编码过，fastText的输入特征是被embedding过；（EMBEDDING_DIM表示经过embedding层输出，每个词被分布式表示的向量的维度，这里设置为100。比如对于“达观”这个词，会被一个长度为100的类似于[ 0.97860014, 5.93589592, 0.22342691, -3.83102846, -0.23053935, …]的实值向量来表示；） word2vec是一个无监督算法，而fasttext是一个有监督算法。CBOW的输出是目标词汇，fastText的输出是文档对应的类标。 word2vec 中 的训练trick（这个也不是 word2vec 的首创）： hierarchical softmax 本质是把 N 分类问题变成 log(N)次二分类 negative sampling 本质是预测总体类别的一个子集 在某些文本分类任务中类别很多，计算线性分类器的复杂度高。为了改善运行时间，fastText 模型使用了层次 Softmax 技巧。层次 Softmax 技巧建立在哈弗曼编码的基础上，对标签进行编码，能够极大地缩小模型预测目标的数量。 n-gram 是有两个维度的：字符n-gram 是缓解 oov 问题； 字维度是可以加入部分的上下文信息。 fasttext 的优势： 适合大型数据+高效的训练速度：能够训练模型“在使用标准多核CPU的情况下10分钟内处理超过10亿个词汇” fastText专注于文本分类，在许多标准问题上实现当下最好的表现（例如文本倾向性分析或标签预测）。 faiss 是稠密向量之间计算距离的开源工具。 faiss 三种模式或者说索引。一种简单模式在小的数据集上计算欧式距离；一种加快检索的速度，使用聚类算法，检索的时候只是检索id 所在的簇和周围的簇，不过这个过程是需要预训练的；一种是减少内存的时候，如果是求解近似解，那么不必存储完整的向量，使用pca 降维。还有比较通用的加快速度的方式，比如分段计算和使用gpu 进行计算。 关于k-means中选择聚类簇k的个数的算法：尝试法。如果增大k，发现并不能使得指标明显的下降，这个时候就达到了阈值。指标：一个簇内所有的点到簇类中心的距离的总和。 knn 和k-means 的区别，前者是有监督的分类算法，根据测试点周围k 个点的类别信息判断该点的信息；k-means 是无监督算法，属于聚类中的一种。 使用什么库函数机器学习中经常使用 numpy，sklearn，pandas，xgboost NLP和深度学习中中使用jieba，NLTK，tensorflow 如何评价自己缺点： （说一个真实的缺点，这个缺点必须是正在改善的缺点，最好能够成为优点的那种。）有时候自己可能过于专注于某一点，把太多的时间花费在某一方面。实习和在学校是不太一样的，公司更加看重成果，最后做出来的东西的效果，所以和实验室中整天整个月做一件事情是不太一样的，可能需要根据公司业务的需求去调整工作的重心。实习期间，自己也是在努力调整这种状态。 优点： 动手能力强，对于一个新东西，上手比较快，在ai 大多数的领域，还是从美国硅谷哪里来的，所以英语能力比较重要，而自己的英语水平比较好，看英文论文调研方便比较有优势。快速学习能力比较强，所以能够比较快的接受新鲜的事物，上手比较快。 这些任务中，你的角色是什么？ 这个问题常常被面试官这样问道“这个是你一个人做的吗？ 感觉既要体现自己的独立做事的能力又要有合作精神。实习工作基本上都是独立完成的，项目是和实验室的人合作完成的，我是主要负责人。 需要单独复习的文章如何处理 overfit 最后的发问？ 你们所在的小组主要是什么业务呢？ 如果有机会来这里工作，那么主要的工作内容是什么？ 面试结果大概什么时候出？ 如果感觉面试的比较好，那么问一下可以简单的评价我这次面试吗，因为您也面试了不少的同学了吧。]]></content>
      <categories>
        <category>NOT_FOR_YOU</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[logistics 和softmax 的公式推导]]></title>
    <url>%2F2019%2F07%2F05%2Flogistics2softmax%2F</url>
    <content type="text"><![CDATA[Logistics 和 softmax 的公式推导。总结常在面试过程中用到的公式推导。 Logistics公式推导Logistics Regression 是线性分类器。所以还是先从线性回归中开始推导。LR 模型推导分成以下四个部分： 线性回归表示，sigmoid 激活函数、优化目标极大似然估计 和使用梯度上升的方式更新权重。 线性函数 $$\begin{split}h_{w}\left(x^{i}\right) &amp;=w_{0}+w_{1} x_{1}+w x_{2}+\ldots+w_{n} x_{n} \\h_{w}\left(x^{j}\right) &amp;=w^{T} x_{i}=W^{T} X\end{split}$$ 然后分别进行向量表示： $$\begin{split}X &amp;= \left[ 1 \ x_1 \ \dots x_n \right]^T \\W &amp;= \left[ w_0 \ w_1 \ \dots \ w_n \right]^T \\\end{split}$$ 为了更好的表示分类模型，使用sigmoid 激活函数，引入了Logistics 回归。 逻辑回归是假设数据服从 Bernoulli 分布（抛硬币），因此LR属于参数模型。LR目标函数的定义：$$h_{\theta}(x)=g\left(\theta^{T} x\right)$$其中sigmoid 函数为：$g(z)=\frac{1}{1+e^{-z}}$。 sigmoid 函数的导数有很好的性质 $ g^{\prime}(z)=g(z)(1-g(z))$ 似然函数 为什么这里会出现概率呢？ 因为经过sigmoid 激活函数之后，会输出 $y_{pred}$， 范围是在 [0, 1] 之间，一般是跟军 0.5 去划分，如果大于 0.5 那么是 A 类，否则是B 类。逻辑回归的优化目标是极大化对数似然估计，采用梯度上升来学习及更新参数$ \theta $向量的值。 假设有n个独立的训练样本 ${(x_1, y_1),(x_2, y_2), \ldots,(x_n, y_n)} $, 并且$ y ={ 0, 1} $。那每一个观察到的样本 $(x_i, y_i)$ 出现的概率是： $$P(y_i, x_i) = {P(y_i = 1| x_i)}^{y_i}{(1 -P(y_i =1| x_i)}^{1-y_i)}$$ 推广到所有样本下，得到整体的似然函数表达，需要将所有的样本似然函数全部相乘。 $$L(\theta) =\Pi{P(y_i = 1| x_i)}^{y_i}({1 -P(y_i =1| x_i)})^{1-y_i}$$ 累乘的形式不利于进行优化分析，这里将似然函数取对数，得到对数似然函数，作为我们的最终优化目标，运用极大似然估计来求得最优的 $\theta$ (这个参数是来自线性相乘中的参数) $$\begin{split}l(\theta) &amp;=logL(\theta) \\&amp;= \sum_{i =1}^{m}(y^{(i)} \log h\left(x^{(i)}\right)+\left(1-y^{(i)}\right) \log \left(1-h\left(x^{(i)}\right)\right))\end{split}$$ 最优化求解 使用链式法对目标函数进行求导然后求解。 $$\frac{\partial}{\theta_{j}} J(\theta)=\frac{\partial J(\theta)}{\partial g\left(\theta^{T} x\right)} \times \frac{\partial g\left(\theta^{T} x\right)}{\partial \theta^{T} x} \times \frac{\partial \theta^{T} x}{\partial \theta_{j}}$$ 分成三部分：第一部分$$\frac{\partial J(\theta)}{\partial g\left(\theta^{T} x\right)}=y \times \frac{1}{g\left(\theta^{T} x\right)}+(y-1) \times \frac{1}{1-g\left(\theta^{T_{x} x}\right)}$$ 第二部分 $$\frac{\partial g\left(\theta^{T} x\right)}{\partial \theta^{T} x}=g\left(\theta^{T} x\right)\left(1-g\left(\theta^{T} x\right)\right)$$ 第三部分 $$\frac{\partial \theta^{T} x}{\theta_{j}}=\frac{\partial J\left(\theta_{1} x_{1}+\theta_{2} x_{2}+\cdots \theta_{n} x_{n}\right)}{\partial \theta_{j}}=x_{j}$$ 最后相乘可以得到： $$ \frac{\partial}{\partial \theta_{j}} \ell(\theta)= \left(y-h_{\theta}(x)\right) x_{j}$$ 因此总的 $\theta $ 的更新公式为：$$ J(\theta) := \theta_{j}+\alpha\left(y^{(i)}-h_{\theta}\left(x^{(i)}\right)\right) x_{j}^{(i)}$$ 关于logistics 的公式推导已经结束。 原文链接：Logistics到softmax推导整理 纯python 实现逻辑回归线性分类器：模型是参数的线性函数，分类平面是（超）平面；非线性分类器：模型分界面可以是曲面或者超平面的组合。典型的线性分类器有感知机，LDA，逻辑斯特回归，SVM（线性核）；典型的非线性分类器有 kNN，决策树，SVM（非线性核） 关键是看后面，如何使用纯python 去实现一个逻辑回归。 代码：逻辑回归.py讲解：机器学习算法 之逻辑回归以及python实现 在numpy 中，12* 和 np.multiply 都是都是数字相乘，如果是矩阵，那么就是对应位置相乘@ 和np.dot 都是矩阵相称，对应向量内积 softmax 的推导多分类中使用的是交叉熵损失函数 ( cross entropy error function)，交叉熵函数是凸函数。最后一层的网络展开，最终的结果展开成 one-hot 的形式，用 softmax 得到的概率值进行交叉熵的计算，带入公式 $$J=-\sum_{c=1}^{M} y_{c} \log \left(p_{c}\right)$$ 好好理解一下从softmax 到交叉熵计算过程。 这个里面有一部分关乎 softmax 的推导，但是感觉不是很全很详细：Logistics到softmax推导整理 SVM公式推导 如上图中只有线上的点叫做支持向量，其他的点在 $y$ 中的表示要么是 1要么是-1。支持向量可以用来进行分类和回归，这里介绍的是分类问题。支持向量的目标是最大化正负样本之间的间隔。分类问题可以分成三类： 线性可分（硬间隔支持向量） 近似线性可分（软间隔支持向量，通过引入松弛变量实现） 线性完全不可分（使用非线性核函数） 下面的公式推导都是基于线性可分条件下的，也就是硬间隔支持向量机。步骤有二： 寻找最大分隔间距（原问题可以转换成对偶问题） 通过拉格朗日求解优化问题 从第一步中得到是一个二次规划问题，可以求解，但当样本量大的时候，计算量非常大，所以可以转换成其对偶问题求解。然后使用拉格朗日公式求解，这个时候引入一个参数拉格朗日参数，求解这个参数即可。 具体公式的推导可以参考下面的文章： SVM公式推导 对于上面文章的解读 在样本空间，划分超平面可以通过以下线性方程描述：$$w^Tx+b =0$$假设超平面 $(w,b) $能将训练样本正确分类，即对于$(x_i,y_i)∈D $，若 $y_i=+1$，则有 $w^Tx+b&gt;0 $；若 $y_i=−1$ （这个条件是假设的，令） 从原来的分段函数表示成下面的公式，在于这样的结果是可以直接表示分类结果的正确与否，如果结果大于0表示 $f(x)$ 和 $label $ 是同号，那么表示预测正确，否则预测错误。$$y_{i} \cdot\left(w^{T} x_{i}+b\right) \geq 1$$公式中的 1-5 对应的是步骤一；步骤6-10 是对应求解问题。 在计算过程中支持向量的点是用来计算距离的，其他样本点是体现在约束条件中。 判别模型 反向传播的推导这个推导也是超级简单，分成三个步骤： 前向传播，注意$f_1$ 和 $y_1$ 所表示的含义的不同，前者经过激活函数 比如说sigmoid 就得到了后者 反向传播，和前向传播是一样的，只不过出发点是 $\delta$，关键是要使用到前面的weight 信息进行 $\delta$ 的重新分配 权值更新，注意涉及到原来的 weights、error、梯度和学习率四个变量。 (注意在权值更新推导的时候，一定要使用到学习率，求导，error 这几个变量，否则是没有办法基于之前的进行更新的)详细情况可以看这里 反向传播的推导 xgboost 中的理论推导xgboost 的loss 的二阶展开推导 上面式子中的 loss包含着正则项。 一个是树里面叶子节点的个数T 一个是树上叶子节点的得分w的L2模平方（对w进行L2正则化，相当于针对每个叶结点的得分增加L2平滑，目的是为了避免过拟合） 泰勒二阶展开式： $$f(x+\Delta x) \simeq f(x)+f^{\prime}(x) \Delta x+\frac{1}{2} f^{\prime \prime}(x) \Delta x^{2}$$ XGBoost与GBDT有什么不同 GBDT是机器学习算法，XGBoost是该算法的工程实现。 在使用CART作为基分类器时，XGBoost显式地加入了正则项来控制模 型的复杂度，有利于防止过拟合，从而提高模型的泛化能力。 GBDT在模型训练时只使用了代价函数的一阶导数信息（随机梯度下降），XGBoost对代 价函数进行二阶泰勒展开，可以同时使用一阶和二阶导数。（牛顿法） 传统的GBDT采用CART作为基分类器，XGBoost支持多种类型的基分类 器，比如线性分类器。 传统的GBDT在每轮迭代时使用全部的数据，XGBoost则采用了与随机 森林相似的策略，支持对数据进行采样。 传统的GBDT没有设计对缺失值进行处理，XGBoost能够自动学习出缺 失值的处理策略。 XGBoost使用了一阶和二阶偏导, 二阶导数有利于梯度下降的更快更准. 使用泰勒展开取得函数做自变量的二阶导数形式, 可以在不选定损失函数具体形式的情况下, 仅仅依靠输入数据的值就可以进行叶子分裂优化计算, 本质上也就把损失函数的选取和模型算法优化/参数选择分开了. 这种去耦合增加了XGBoost的适用性, 使得它按需选取损失函数, 可以用于分类, 也可以用于回归。 XGBoost在训练的过程中给出各个特征的评分，从而表明每个特征对模型训练的重要性。XGBoost利用梯度优化模型算法, 样本是不放回的，想象一个样本连续重复抽出,梯度来回踏步，这显然不利于收敛。XGBoost支持子采样, 也就是每轮计算可以不使用全部样本。 (XGBoost详解)[https://www.jianshu.com/p/ac1c12f3fba1] XGBoost二阶泰勒展开公式推导]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Text Summarization]]></title>
    <url>%2F2019%2F06%2F30%2Ftext-summarization%2F</url>
    <content type="text"><![CDATA[Text Summarization Text summarization is the process of distilling the most important information from a source (or sources) to produce an abridged version for a particular user (or users) and task (or tasks). There are many reasons why Automatic Text Summarization is useful: Summaries reduce reading time. Automatic summarization algorithms are less biased than human summarizers. Personalized summaries are useful in question-answering systems as they provide personalized information. Text summarization methods can be classified into different types. 虽然是可以从不同的角度进行划分，但最常见的分类角度是 based on output type: extractive and abstractive. All extractive summarization algorithms attempt to score the phrases or sentences in a document and return only the most highly informative blocks of text. Abstractive text summarization actually creates new text which doesn’t exist in that form in the document. Abstractive summarization is what you might do when explaining a book you read to your friend, and it is much more difficult for a computer to do than extractive summarization. extractive 是从源句子中找关键句的过程， abstractive 是概括，生成对于文章总结的过程。 Computers just aren’t that great at the act of creation. To date, there aren’t any abstractive summarization techniques which work suitably well on long documents. The best performing ones merely create a sentence based upon a single paragraph, or cut the length of a sentence in half while maintaining as much information as possible. Often, grammar suffers horribly. They’re usually based upon neural network models. What is ROUGE?To evaluate the goodness of the generated summary, the common metric in the Text Summarization space is called Rouge score. ROUGE stands for Recall-Oriented Understudy for Gisting Evaluation It works by comparing an automatically produced summary or translation against a set of reference summaries (typically human-produced). It works by matching overlap of n-grams of the generated and reference summary. Extractive Techniques LexRank SummarizerLexRank is an unsupervised approach that gets its inspiration from the same ideas behind Google’s PageRank algorithm. It finds the relative importance of all words in a document and selects the sentences which contain the most of those high-scoring words. NLTK Summarizer 将句子的重要程度下放到词语 word 的关键程度。先是文本的预处理，然后挑选关键词，然后挑选关键句。 Although the technique is basic, we found that it did a good job at creating large summaries. Gensim Summarizer （pagerank 的思想很简单，每一个网页都是一种投票，然后被投票的重要程度越高，那么这个网页或者网站的重要性就越高，最后的排名就越靠前）TextRank is based on PageRank algorithm that is used on Google Search Engine. In simple words, it prefers pages which has higher number of pages hitting it. TextRank is a bit more simplistic than LexRank; although both algorithms are very similar, LexRank applies a heuristic post-processing step to remove sentences with highly duplicitous. The gensim algorithm does a good job at creating both long and short summaries. Another cool feature of gensim is that we can get a list of top keywords chosen by the algorithm. This feature can come in handy for other NLP tasks, where we want to use “TextRank” to select words from a document instead of “Bag of Words” or “TF-IDF”. Gensim also has a well-maintained repository and has an active community which is an added asset to using this algorithm. Sentence Embeddings（当源文本比较短的时候，比如 review 或者email， sentence embedding ，然后使用聚类的方式，得到不同的簇之后，每个簇可以选择一个句子，类似于降维的思想） We wanted to evaluate how text summarization works on shorter documents like reviews, emails etc. We used K-means clustering to summarize the types of documents following the aforementioned structure. Then, all of the sentences in a document are clustered in k = sqrt(length of document) clusters. Each cluster of sentence embeddings can be interpreted as a set of semantically similar sentences whose meaning can be expressed by just one candidate sentence in the summary. Candidate sentences corresponding to each cluster are then ordered to form a summary for an email. The order of the candidate sentences in the summary is determined by the positions of the sentences in their corresponding clusters in the original document. Abstraction techniques Pointer — Generator Networks Compared to the sequence-to-sequence-with-attention system, the pointer-generator network does a better job at copying words from the source text. Additionally it also is able to copy out-of-vocabulary words allowing the algorithm to handle unseen words even if the corpus has a smaller vocabulary. Hence we can think of pointer generator as a combination approach combining both extraction (pointing) and abstraction (generating). Drawbacks of Abstractive summarization Firstly, training the model requires a lot of data and hence time. An inherent problem with abstraction is that the summarizer reproduces factual details incorrectly. For instance, if the article talks about Germany beating Argentina 3–2, the summarizer may replace 3–2 by 2–0 Repetition is another problem faced by the summarizer. As we can see in the second example above, some phrases are repeated in the summary To Summarize..Given the architecture of RNNs and the current computing capabilities, we observed that extractive summarization methods are faster, but equally intuitive as abstractive methods. A few other observations: The network fails to focus on the core of the source text and summarizes a less important, secondary piece of information The attention mechanism, by revealing what the network is “looking at”, shines some precious light into the black box of neural networks, helping us to debug problems like repetition and copying. To make further advances, we need greater insight into what RNNs are learning from text and how that knowledge is represented. Case Study: Text Summarization on EmailsUnsupervised Text Summarization using Sentence Embeddings Step-1: Email CleaningAs salutation and signature lines (称谓签名行) can vary from email to email and from one language to the other, removing them will require matching against a regular expression. Hi Jane, Thank you for keeping me updated on this issue. I&apos;m happy to hear that the issue got resolved after all and you can now use the app in its full functionality again. Also many thanks for your suggestions. We hope to improve this feature in the future. In case you experience any further problems with the app, please don&apos;t hesitate to contact me again. Best regards, John Doe Customer Support 1600 Amphitheatre Parkway Mountain View, CA United States Step-2: Language Detection As the emails to be summarized can be of any language, the first thing one needs to do is to determine which language an email is in. I used langdetect for my purpose and it supports 55 different languages. Step-3: Sentence TokenizationOnce the languages identification is performed for every email, we can use this information to split each email into its constituent sentences using specific rules for sentence delimiters (分隔符) for each language. Step-4: Skip-Thought EncoderWe need a way to generate fixed length vector representations for each sentence in our emails. Step-5: Clustering（对于 最后summary 的长度，这个是一个很好的baseline，给出了一个定量的结果，使用 $ \sqrt{N}$, N 表示原始段落或者文章的长度）The number of clusters will be equal to desired number of sentences in the summary. I chose the numbers of sentences in the summary to be equal to the square root of the total number of sentence in the email. Step-6: SummarizationEach cluster of sentence embeddings can be interpreted as a set of semantically similar sentences whose meaning can be expressed by just one candidate sentence in the summary. The candidate sentence is chosen to be the sentence whose vector representation is closest to the cluster center. Candidate sentences corresponding to each cluster are then ordered to form a summary for an email. PageRank 算法和TextRank 介绍 PageRank 算法 PageRank通过互联网中的超链接关系来确定一个网页的排名，其公式是通过一种投票的思想来设计的。整个互联网可以看作是一张有向图，网页是图中的节点，网页之间的链接就是图中的边。如果网页 A 存在到网页 B 的链接，那么就有一条从网页 A 指向网页 B 的有向边。构造完图后，使用下面的公式来计算网页$ i$的重要性（PR值）： $$S \left( V _ { i } \right) = ( 1 - d ) + d \cdot \sum _ { j \in I n \left( V _ { i } \right) } \frac { 1 } { \left| O u t \left( V _ { j } \right) \right| } S \left( V _ { j } \right)$$ $d$ 是阻尼系数，一般设置为0.85. $\operatorname { In } \left( V _ { i } \right)$ 是指向网页 $i$ 的链接的网页集合。 $\operatorname { Out } \left( V _ { j } \right)$ 是网页 $j$ 中的链接指向的网页的集合。 $\left| O u t \left( V _ { j } \right) \right|$ 是集合中元素的个数。 PageRank 需要多次迭代才能得到最后的结果。 (公式看着比较复杂，但是原理非常简单，如果计算网页 i的 pagerank，那么对于所有指向该网页的网站集合，进行权重的分摊。也就是说某个pageranking 很高的网站指向了某个网站，那么这个网站的pageranking 会变高，类似带有权重的投票) 假设我们有4个网页——w1，w2，w3，w4。这些页面包含指向彼此的链接。有些页面可能没有链接，这些页面被称为悬空页面。 Webpage Links w1 [w4, 22] w2 [w3, w1] w3 [] w4 [w1] 在本例中初始化成这样： 最后，这个矩阵中的值将以迭代的方式更新，以获得网页排名。 （上面的pageranking 的讲解是为了 下面的textranking 的理解） TextRank 关键词提取 两者的相似之处： In place of web pages, we use sentences Similarity between any two sentences is used as an equivalent to the web page transition probability The similarity scores are stored in a square matrix, similar to the matrix M used for PageRank 不同之处在于后者使用句子之间的相似度作为 weights. $W S \left( V _ { i } \right) = ( 1 - d ) + d \cdot \sum _ { V _ { j } \in \operatorname { In } ( V i ) } \frac { w _ { j i } } { \sum _ { V _ { k } \in O u t \left( V _ { j } \right) } w _ { j k } } W S \left( V _ { j } \right)$ $w_{ij}$ 就是图中节点 $V_i$ 到$V_j$ 的边的权值， 就是两个句子 $S_i$ 和句子 $S_j$ 的相似程度。下面的代码中使用的是 cosine 函数来表示这种相似度。 步骤： The first step would be to concatenate all the text contained in the articles Then split the text into individual sentences In the next step, we will find vector representation (word embeddings) for each and every sentence Similarities between sentence vectors are then calculated and stored in a matrix The similarity matrix is then converted into a graph, with sentences as vertices and similarity scores as edges, for sentence rank calculation Finally, a certain number of top-ranked sentences form the final summary 这里使用的 cosine similarity scores，计算句子之间的相似度，句子embedding 是通过 word embedding 相加而成。1234for i in range(len(sentences)): for j in range(len(sentences)): if i != j: sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))[0,0] Applying PageRank Algorithm123import networkx as nxnx_graph = nx.from_numpy_array(sim_mat)scores = nx.pagerank(nx_graph) TF-IDF 和 TextRank 对比总结： TextRank与TFIDF均严重依赖于分词结果——如果某词在分词时被切分成了两个词，那么在做关键词提取时无法将两个词黏合在一起（TextRank有部分黏合效果，但需要这两个词均为关键词）。因此是否添加标注关键词进自定义词典，将会造成准确率、召回率大相径庭。 TextRank的效果并不优于TFIDF。 TextRank虽然考虑到了词之间的关系，但是仍然倾向于将频繁词作为关键词。 由于TextRank涉及到构建词图及迭代计算，所以提取速度较慢。 自然语言处理有两个切入点，一个是频率一个是语义。上述两种方法本质上还是基于词频的。如何进行方法的评价，如果是基于概率 （频率）进行计算，那么一个切入点是否能够体现上下文关系，是否有语义信息。如果是基于网络，那么一个方法就是内存和训练时间上是否可以在大规模的工业界展开使用。 Referenceshttps://medium.com/jatana/unsupervised-text-summarization-using-sentence-embeddings-adb15ce83db1 https://towardsdatascience.com/data-scientists-guide-to-summarization-fc0db952e363 https://www.jiqizhixin.com/articles/2018-12-28-18 https://www.analyticsvidhya.com/blog/2018/11/introduction-text-summarization-textrank-python/ https://xiaosheng.me/2017/04/08/article49/ 复习总结 文本摘要有两种方法，一种是从源句子中抽取的过程，一种是概括的过程，生成对于文章总结的过程。 pagerank 算法和 textrank 算法对于简历上的内容相关性不大，就先不复习了]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>text-summarization</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NLP Papers Reading-Machine Translation]]></title>
    <url>%2F2019%2F06%2F29%2Fnlp-papers-reading-machine-translation%2F</url>
    <content type="text"><![CDATA[介绍机器翻译的五个发展阶段，然后是关于机器翻译的论文阅读笔记（持续更新） 机器翻译的发展过程why difficult? Machine translation is challenging given the inherent ambiguity and flexibility of human language. Statistical machine translation replaces classical rule-based systems with models that learn to translate from examples. Neural machine translation models fit a single model rather than a pipeline of fine-tuned models and currently achieve state-of-the-art results. （1）Rule-based Machine Translation Classical machine translation methods often involve rules for converting text in the source language to the target language. The rules are often developed by linguists and may operate at the lexical, syntactic, or semantic level. This focus on rules gives the name to this area of study: Rule-based Machine Translation, or RBMT. The key limitations of the classical machine translation approaches are both the expertise required to develop the rules, and the vast number of rules and exceptions required. （2）Statistical Machine Translation Statistical machine translation, or SMT for short, is the use of statistical models that learn to translate text from a source language to a target language given a large corpus of examples. Given a sentence T in the target language, we seek the sentence S from which the translator produced T. We know that our chance of error is minimized by choosing that sentence S that is most probable given T. Thus, we wish to choose S so as to maximize $P_r(S|T)$. The approach is data-driven, requiring only a corpus of examples with both source and target language text. This means linguists are not longer required to specify the rules of translation. Although effective, statistical machine translation methods suffered from a narrow focus on the phrases being translated , losing the broader nature of the target text. The hard focus on data-driven approaches also meant that methods may have ignored important syntax distinctions known by linguists. Finally, the statistical approaches required careful tuning of each module in the translation pipeline. （3）Neural Machine Translation The key benefit to the approach is that a single system can be trained directly on source and target text, no longer requiring the pipeline of specialized systems used in statistical machine learning. As such, neural machine translation systems are said to be end-to-end systems as only one model is required for the translation. （4）Encoder-Decoder Model Multilayer Perceptron neural network models can be used for machine translation, although the models are limited by a fixed-length input sequence where the output must be the same length. These early models have been greatly improved upon recently through the use of recurrent neural networks organized into an encoder-decoder architecture that allow for variable length input and output sequences. The key to the encoder-decoder architecture is the ability of the model to encode the source text into an internal fixed-length representation called the context vector. Interestingly, once encoded, different decoding systems could be used, in principle, to translate the context into different languages. The power of this model lies in the fact that it can map sequences of different lengths to each other. （5）Encoder-Decoders with Attention Although effective, the Encoder-Decoder architecture has problems with long sequences of text to be translated. The problem stems from the fixed-length internal representation that must be used to decode each word in the output sequence. The solution is the use of an attention mechanism that allows the model to learn where to place attention on the input sequence as each word of the output sequence is decoded. The encoder-decoder recurrent neural network architecture with attention is currently the state-of-the-art on some benchmark problems for machine translation. And this architecture is used in the heart of the Google Neural Machine Translation system, or GNMT, used in their Google Translate service. Although effective, the neural machine translation systems still suffer some issues, such as scaling to larger vocabularies of words and the slow speed of training the models. There are the current areas of focus for large production neural translation systems, such as the Google system. （6）Attention VS LSTM A limitation of the LSTM architecture is that it encodes the input sequence to a fixed length internal representation. This imposes limits on the length of input sequences that can be reasonably learned and results in worse performance for very long input sequences. After reading this, you will know: This (LSTM) is believed to limit the performance of these networks, especially when considering long input sequences, such as very long sentences in text translation problems. Put another way, each item in the output sequence is conditional on selective items in the input sequence. 而对于 Attention 而言: Each time the proposed model generates a word in a translation, it (soft-) searches for a set of positions in a source sentence where the most relevant information is concentrated. The model then predicts a target word based on the context vectors associated with these source positions and all the previous generated target words.… it encodes the input sentence into a sequence of vectors and chooses a subset of these vectors adaptively while decoding the translation. This frees a neural translation model from having to squash all the information of a source sentence, regardless of its length, into a fixed-length vector. And, This (Attention) increases the computational burden of the model, but results in a more targeted and better-performing model. 同样，除了在 machine translation 中有应用， 在其他领域如image description, CNN 中同样有借鉴的意义。Convolutional neural networks applied to computer vision problems also suffer from similar limitations, where it can be difficult to learn models on very large images. 参考文献 introduction-neural-machine-translationattention-long-short-term-memory-recurrent-neural-networks Sequence to Sequence Learning with Neural Network论文阅读 研究目的：CNNs需要输入、输出维度是已知和固定的。而语音识别、机器翻译、问答系统等序列到序列问题的序列长度是未知的。CNN有一个明显的缺陷：CNN只能处理输入、输出向量维度是定长的情形。对于输入、输出可变长的情况，使用RNN-Recurrent Neural Network更易求解。 论文贡献之一在于网络结构上：通过学习编码一个可变长度的序列成一个固定长度的向量表示，解码一个给定的固定长度的向量成一个可变长度的序列。实现的时候首先将source sequence通过一个encode LSTM map成一个vector，然后再通过另一个decoder LSTM进行翻译得出output，这也恰恰是image caption里的思想呀（通过CNN将输入图像conv成一个vector或者feature map，然后再输入LSTM），原来大体是这样，接着看。 另外还有一个小的策略：LSTM在长句翻译中的表现也不俗。这归功于对源序列中词序的逆转。虽然LSTM能够基于长期的相关性处理问题，但我们发现在把原句序列逆转的情况下LSMT能学习得更加出色。逆转之后，LSTM测试的复杂度从5.8降至4.7，并且在BLEU上的得分从25.9提升至30.6。 不足之处：其他方面都比较普通，或者说很多论文中都有提到过，比如LSTM可以解决vanishing的问题但没法解决gradient exploding的问题，因此採取gradient crop。模型採用了SGD without momentum。实用的LSTM结构式Grave的《Generating sequence from RNN》中的LSTM结构，等等。 总结：总体来说，这个模型还是採取了贪婪的算法，换句话说，后面的预测对前面的状态有极强的依赖，一旦前面的预测出现问题，后面的预测就不可靠了，这也是一个值得思考和改进的地方。 Effective Approaches to Attention-based Neural Machine Translation论文阅读这篇文章的核心在于 attention。 Attention 的作用可以看作是一个对齐模型，传统 SMT 我们用 EM 算法来求解对齐，这里做一个隐式的对齐，将 alignment model 用一个 feedforward neural network 参数化，和其他部分一起训练，神经网络会同时来学习 翻译模型(translation) 和 对齐模型(alignment)。 Attention 可以分成 hard and soft两种模型，简单理解 hard attention 就是从 source sentence 中找到一个能产生单词 $t^{th}$ 对齐的特定单词，把 $s_{t,i}$ 设为1，其他所有单词硬性的认为其概率为0; soft attention 对于source sentence中每个单词都给出一个对齐概率，得到一个概率分布，context vector 就是这些概率分布的一个加权和，整个模型是平滑的且处处可分。 而在该篇论文中提出了一个新的 attention 机制 local attention，在得到 context vector 时，我们不想看所有的 source hidden state，而是每次只看一个 hidden state 的子集(subset)，这样的 attention 其实更集中，也会有更好的结果。Global attention 其实就是 soft attention， local model 实际相当于 hard 和 soft attention 的一个混合或者说折中，主要是用来降低 attention 的花费，简单来说就是每次计算先用预测函数得到 source 相关信息的窗口。 soft or hard attention 还是 global or local attention是从不同的角度进行分类的，前者是在概率分布上，后者是在 context上。 这个是global attention： 这个是 local attention 总结：三种不同的attention 种类 attention 分成 hard 和soft 两种模式，简单理解 hard attention 就是从source sentence中国找到一个能够产生 $t^{th}$ 的特定单词，把这个单词设置成1，其他单词设置成0；而soft attention （global attention）是把source sentence中每个单词都给出一个对齐的概率模型，得到一个分布，然后整个模型是处处可分的。而前者不是处处可分的。local attention是相对于 global attention而言的，不是得到一个全局的attention，而是全局的子集，这样可以降低计算attention 的花费，得到一个相关信息的窗口。 BLEU论文阅读作者提出了BLEU（Bilingual Evaluation Understudy，中文读作波勒）指标用于评价句子翻译的效果。其基本假设是 The closer a machine translation is to a professional human translation, the better it is.翻译句子和专业人员的翻译越近，那么效果越好。 那么该BLEU 的计算需要由两部分组成： a numerical “translation closeness” metric a corpus of good quality human referrence translations 一个相似度计算的指标和高质量的 referrence 语料库。 BLEU 的计算分为三个步骤： 为了方便说明，沿用了论文中记号：Candidate 表示待评测的句子；Reference 是专业人员翻译的句子。 （1）n-gram These matches are position-independent. The more the matches, the better the candidate translation is. 可以发现n-gram 是统计模型，该维度主要统计句子中词的共现。该维度是准确率（precision），要求Candidate 中词语是在Reference 句子中出现的。 缺点：较少地涉及语序，虽然语序也很重要。给出的解释是 The cat is on the mat.There is a cat on the mat. 上面两个句子是同一个意思，但是语序不一样。并且n-gram 是包含了部分语序，当 $n=2$或者 $n =3$ 的时候。 （2）惩罚模型（Modified n-gram） 模型有时候会生成如下的base case： Candidate: the the the the the the the.Reference 1: The cat is on the mat.Reference 2: There is a cat on the mat. 翻译的结果具有 high precision，但是明显是不好的结果。作者提出的解决方案： To compute this, one first counts the maximum number of times a word occurs in any single reference translation. Next, one clips the to- tal count of each candidate word by its maximum reference count, adds these clipped counts up, and divides by the total (unclipped) number of candidate words.BLEU修正了这个算法，提出取机器翻译译文N-gram的出现次数和参考译文中N-gram最大出现次数中的最小值的算法，具体如下： $$Count_{clip} = \min(Count, \text{Max_Ref_Count})$$ 所以该步骤precision score, $p_n$可以表达为： $$P_n =\frac{ \sum_{C \in {Candidates} } \sum_{n-gram \in C} Count_{clip}(n-gram)}{\sum_{C \in {Candidates} } \sum_{n-gram \in C^{‘}} Count_{clip}(n-gram^{‘})} $$ （3）Sentence brevity penalty N -gram precision penalizes spurious words in the candidate that do not appear in any of the reference translations. Additionally, modified precision is penalized if a word occurs more frequently in a candidate translation than its maximum reference count.N-gram precision策略惩戒冗余长句子；但是对于短句子却没有处理，如下的bad case Candidate: of theReference 1: It is a guide to action that ensures that the military will forever heed Party commands.Reference 2: It is the guiding principle which guarantees the military forces always being under the command of the Party.Reference 3: It is the practical guide for the army always to heed the directions of the party. Candidate translations longer than their references are already penalized by the modified n-gram precision measure: there is no need to penalize them again. They consider the range of reference translation lengths in the target language.长句子已经使用 n-gram 策略处理了，所以这部分策略主要处理具有高precision但效果不好的短句子。 $$ BP = \begin{cases}1 &amp; c &gt;r \\e ^ { ( 1 - r / c ) } &amp; c &lt;= r\end{cases}$$ 这里的 $c$是机器译文的词数，$r$是参考译文的词数 所以总的 BLEU 可以表示为以下的形式： $$\mathrm { B } \mathrm { LEU } = \mathrm { BP } \cdot \exp \left( \sum _ { n = 1 } ^ { N } w _ { n } \log p _ { n } \right)$$ 或者写成 log 的形式： $$\log BLEU = \min (1- \frac{r}{c}, 0) + \sum _{n=1} ^{N} w_n \log p_n$$ 当 $N =4$的时候， $w_n = \frac{1}{N}$。 （4）结论 优点：计算速度快；容易理解；已经被广泛使用。缺点：短译句的测评精度有时会较高；它没有考虑句子意义summary：BLEU本身就不追求百分之百的准确性，也不可能做到百分之百，它的目标只是给出一个快且不差的自动评估解决方案。该指标虽然是针对机器翻译提出，但是同样适用于其他的NLP 的模型。 参考文献 1). Sequence to Sequence Learning with Neural Network2). Effective Approaches to Attention-based Neural Machine Translation3). BLEU: a Method for Automatic Evaluation of Machine Translation]]></content>
      <categories>
        <category>NLP</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Seq2Seq Translation (English to Chinese)代码]]></title>
    <url>%2F2019%2F06%2F28%2Fseq2seq-translation-codes%2F</url>
    <content type="text"><![CDATA[基于 tensorflow 的英文文本的预处理和基于 keras 的中英文本预处理。主要是代码，辅助注释。预处理比较详细的步骤： unicode to ascii normalize string tokenization (choose one-hot or not) padding (find proper length of tokenization) 需要选择一个框架 tensorflow or keras 去实现。大量数据建议选择 tensorflow，小模型使用 keras 就行。 机器学习中大多处理的是结构化数据（Excel 数据库），到了深度学习中大多处理非结构化数据。NLP 有两大核心任务：自然语言理解（Natural language understanding ）和自然语言生成（Natural language inferrence or generation）。NLP 的四大典型应用：情感分析、聊天机器人、语音识别和机器翻译。 传统机器学习的NLP 流程： 语料预处理（中文预处理有4个步骤，英文预处理有6个步骤） 特征工程 （特征提取和特征选择） 选择分类器 深度学习的NLP 流程 语料预处理 设计模型 训练模型 英文语料预处理的6 个核心步骤： 分词（tokenization） 词干提取（stemming） 词性还原（lemmatization） 词性标注（parts of speech） 命名实体识别（NER） 分块（chunking） 中文语料预处理的4 个核心步骤 分词 词性标注 命名实体识别 去停用词 热身Text data typically requires some cleanup before it can be embedded in vector space and fed to a machine learning model. Remove tags. For example, “&lt;i>Hello&lt;/i> &lt;b>World&lt;/b>!” is converted to “Hello World!” Remove repeating whitespace characters (spaces, tabs, line breaks). Convert tabs and line breaks to spaces. Remove stopwords. These include the most commonly occurring words in a language, like “the,” “on,” “is,” etc. NLP libraries like gensim provide a default list of stopwords. Convert all text to lowercase. Perform Porter stemming. Porter stemming reduces inflections like “fishing,” “fished,” and “fisher” to the root “fish.” This makes it easier for an ML model to learn how to glean meaning or intent form a sequence of words. 调用 gensim 框架实现预处理： 12345678from gensim.parsing.preprocessing import preprocess_string, strip_tags, strip_multiple_whitespaces, remove_stopwords, stem_textcustom_filters = [strip_tags, strip_multiple_whitespaces, remove_stopwords, stem_text]# 生成器函数def get_tokenized_questions(X): series = pd.Series(pd.concat([X['question1'], X['question2']]),dtype=str) series.dropna() for question in series: yield preprocess_string(question, custom_filters) All by yourself: 将英文 punctuation characters 和字母以空格隔开。 输入： s = &apos;bla. bla? bla.bla! bla...&apos; 输出： bla . bla ? bla . bla ! bla . . . python2 版本，基于库函数 re 实现。 1234567891011s = 'bla. bla? bla.bla! bla...'import re# 这个符号是可以选择的s = re.sub('([.,!?()])', r' \1 ', s) # use a regular expression to match the punctuation characters you are interested and surround them by spaces,s = re.sub('\s&#123;2,&#125;', ' ', s) # use a second step to collapse multiple spaces anywhere in the document:print(s)# replacing everything with space except (a-z, A-Z, ".", "?", "!", ",")w = re.sub(r"[^a-zA-Z?.!,¿]+", " ", w)w = w.rstrip().strip()w = '&lt;start&gt; ' + w + ' &lt;end&gt;' # 这个是可选的，在首尾加上 'start' or 'end' python3 版本，基于 translate实现。 123# 在python3 中可以使用 translate() 这个方法import stringtext = text.translate(str.maketrans(&#123;key: " &#123;0&#125; ".format(key) for key in string.punctuation&#125;)) tensorflow text preprocessing基于 tensorflow 的文本预处理， 适合大量数据，可以使用batch 输入到模型中去。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374import tensorflow as tfimport numpy as npimport unicodedataimport reraw_data = ( ('What a ridiculous concept!', 'Quel concept ridicule !'), ('Your idea is not entirely crazy.', "Votre idée n'est pas complètement folle."), ("A man's worth lies in what he is.", "La valeur d'un homme réside dans ce qu'il est."), ('What he did is very wrong.', "Ce qu'il a fait est très mal."), ("All three of you need to do that.", "Vous avez besoin de faire cela, tous les trois."), ("Are you giving me another chance?", "Me donnez-vous une autre chance ?"), ("Both Tom and Mary work as models.", "Tom et Mary travaillent tous les deux comme mannequins."), ("Can I have a few minutes, please?", "Puis-je avoir quelques minutes, je vous prie ?"), ("Could you close the door, please?", "Pourriez-vous fermer la porte, s'il vous plaît ?"), ("Did you plant pumpkins this year?", "Cette année, avez-vous planté des citrouilles ?"), ("Do you ever study in the library?", "Est-ce que vous étudiez à la bibliothèque des fois ?"), ("Don't be deceived by appearances.", "Ne vous laissez pas abuser par les apparences."), ("Excuse me. Can you speak English?", "Je vous prie de m'excuser ! Savez-vous parler anglais ?"), ("Few people know the true meaning.", "Peu de gens savent ce que cela veut réellement dire."), ("Germany produced many scientists.", "L'Allemagne a produit beaucoup de scientifiques."), ("Guess whose birthday it is today.", "Devine de qui c'est l'anniversaire, aujourd'hui !"), ("He acted like he owned the place.", "Il s'est comporté comme s'il possédait l'endroit."), ("Honesty will pay in the long run.", "L'honnêteté paye à la longue."), ("How do we know this isn't a trap?", "Comment savez-vous qu'il ne s'agit pas d'un piège ?"), ("I can't believe you're giving up.", "Je n'arrive pas à croire que vous abandonniez."),)# convert the unicode file to ascii, 主要是统一编码方式，然后去除 重音符号def unicode_to_ascii(s): return ''.join( c for c in unicodedata.normalize('NFD', s) # UCD是Unicode字符数据库（Unicode Character DataBase）的缩写。 if unicodedata.category(c) != 'Mn') # 去除 重音符号# 类似于热身中的功能def normalize_string(s): s = unicode_to_ascii(s) s = re.sub(r'([!.?])', r' \1', s) # 如果是这三个符号，那么是需要前面加上一个空格 s = re.sub(r'[^a-zA-Z.!?]+', r' ', s) # 除去不是这些符号的字符 s = re.sub(r'\s+', r' ', s) # 出现多个空格，就去除直到1个 return sraw_data_en, raw_data_fr = list(zip(*raw_data)) # 变量名称前加 *，表示传入的是一个元组，两个星号表示是一个dictionary# 从运行的结果看，由原来的 tuple of tuple 变成了两个string of tuple，并没有list 什么事情raw_data_en, raw_data_fr = list(raw_data_en), list(raw_data_fr) # from tuple to list 这个是转换了raw_data_en = [normalize_string(data) for data in raw_data_en]# 这个是decoder的输入，decoder 是有两个输入的，一个是encoder的输出，一个是 其中一个start destination sentence， 最后是一个 end# 是用来计算 loss的raw_data_fr_in = ['&lt;start&gt; ' + normalize_string(data) for data in raw_data_fr]raw_data_fr_out = [normalize_string(data) + ' &lt;end&gt;' for data in raw_data_fr] # 这种操作比较简洁哈en_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')# 默认是会把 ? . or ! 去掉, 因为我们不想让其 filter 掉 上述三个字符，所有自己进行了处理en_tokenizer.fit_on_texts(raw_data_en)data_en = en_tokenizer.texts_to_sequences(raw_data_en)# 这个padding 是为了之后创建 tf.data.Dataset object 使用的，所以还是比较nice的data_en = tf.keras.preprocessing.sequence.pad_sequences(data_en, padding='post')fr_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')# 都是先使用 fit_on_texts() 然后才使用 texts_to_sequence() ，前者相当于训练，后者是输出的结果，我的理解# A mid-way notice though, we can call fit_on_texts multiple times on different corpora and it will update vocabulary automatically.fr_tokenizer.fit_on_texts(raw_data_fr_in)fr_tokenizer.fit_on_texts(raw_data_fr_out)data_fr_in = fr_tokenizer.texts_to_sequences(raw_data_fr_in)data_fr_in = tf.keras.preprocessing.sequence.pad_sequences(data_fr_in, padding='post')data_fr_out = fr_tokenizer.texts_to_sequences(raw_data_fr_out)data_fr_out = tf.keras.preprocessing.sequence.pad_sequences(data_fr_out, padding='post') in addition： 1234567891011121314# 真正在做实验的时候需要注意的事情：# 一种常见的手段就是 limit the size of the dataset to experiment faster (optimal)# 使用tensorflow 中的dataset 的时候，有意识的 shuffle() 数据集 并且使用batch 的思想dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)# 这个和上面的语句是搭配使用的example_input_batch, example_target_batch = next(iter(dataset))example_input_batch.shape, example_target_batch.shape# 就是在真正的实验的过程中， 网络中的shape (batch_size, embedding_size, )# 超级常用的处理的手段， preprocess_sentence() 是一个函数, apply(lambda )data["eng"] = data.eng.apply(lambda w: preprocess_sentence(w))data["es"] = data.es.apply(lambda w: preprocess_sentence(w)) keras text preprocessing这个版本的代码适用于小的数据量，因为当数据量达到百万的时候，应该使用batch 去训练模型，不应一下子读入到内存中，容易爆内存。比较有特点的 filter 中文的字符使用translate 进行处理。一般从经验上讲是不建议 filter 掉 “？。，” 这三个中文字符的，其他的可以filter 掉，对应英文中的 “? , .” 这三个字符。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788import stringimport refrom numpy import array, argmax, random, takeimport pandas as pdfrom keras.models import Sequentialfrom keras.layers import Dense, LSTM, Embedding, Bidirectional, RepeatVector, TimeDistributedfrom keras.preprocessing.text import Tokenizerfrom keras.callbacks import ModelCheckpointfrom keras.preprocessing.sequence import pad_sequencesfrom keras.models import load_modelfrom keras import optimizersimport matplotlib.pyplot as pltfrom sklearn.model_selection import train_test_splitimport numpy as npdef read_text(filename): # open the file file = open(filename, mode='rt', encoding='utf-8') # read all text text = file.read() file.close() return textdef to_lines(text): sents = text.strip().split('\n') sents = [i.split('\t') for i in sents] return sentsdef to_array(path, debug): data =read_text(path) import gc eng_ch =to_lines(data) del data gc.collect() #import ipdb #ipdb.set_trace() eng_ch =np.asarray(eng_ch) # max memory #eng_ch =np.asarray(eng_ch[:5000000]) if debug: #eng_ch =eng_ch[:4, :]# just for chinese dict test eng_ch =eng_ch[:2000, :] return eng_chdef pre_process(eng_ch): import jieba cn_punctuation = "！？｡ ？。? ＃＄％＆ !（）. ＊＋－／：；＜＝＞＠［＼］＾＿｀｛｜｝～｟｠｢｣､、〃》「」『』【】〔〕〖〗〘〙〚〛〜〝〞〟〰〾 〿–—‘ ’ ‛ “ ” „ ‟ …‧﹏" eng_ch[:, 0] = [s.translate(str.maketrans('', '', string.punctuation)) for s in eng_ch[:, 0]] eng_ch[:, 1] = [s.translate(str.maketrans('', '', cn_punctuation)) for s in eng_ch[:, 1]] for i in range(len(eng_ch)): eng_ch[i, 0] =eng_ch[i, 0].lower() for i in range(len(eng_ch)): seg_list =jieba.cut(eng_ch[i, 1]) eng_ch[i, 1] =' '.join(seg_list) return eng_chdef sentence_length(eng_ch): eng_l =[] ch_l =[] # 这里需要看一下 english的数据是否前后有 空格 for i in eng_ch[:, 0]: eng_l.append(len(i.split())) for i in eng_ch[:,1]: ch_l.append(len(i)) length_df =pd.DataFrame(&#123;'eng': eng_l, 'ch':ch_l&#125;) length_df.hist(bins =50) plt.savefig('data-dist-cn.png')def tokenization(lines): tokenizer = Tokenizer() tokenizer.fit_on_texts(lines) return tokenizerdef encode_sequences(tokenizer, length, lines): seq =tokenizer.texts_to_sequences(lines) seq = pad_sequences(seq, maxlen=length, padding='post') return seq]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>seq2seq</tag>
        <tag>translation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Bairong]]></title>
    <url>%2F2019%2F06%2F27%2Fbairong%2F</url>
    <content type="text"><![CDATA[总结在百融的实习经历。侧重学到的东西。 如果需要打上tag，那么就是“机器学习+ 金融+ 风控”。 金融方面术语整个风控是分成三个阶段：要不要放贷，额度策略和定价策略。 风控的重要性：因为大部分的平台都是靠高利率来覆盖高风险，自己本身没有做好足够的风险控制。但从今往后，哪个平台的风控能够做的好，就能在长周期严监管的前提下生存下去，而不能仅仅依靠高利率了。 额度策略 目标：金额坏账率小于账户坏账率。如何实现上述的目标呢？总的策略是对于用户信用评分进行排序，给高风险的人低额度，反之成立。 模型从理论上讲，是存在极端最优解的，但是在实际中往往不是这样做的。比较常见的模型： 线性模型：理论上分得越细，效果是越好。 比较光滑的指数模型 幂函数 sigmoid 函数： sigmoid 函数形状是由两个参数进行控制的。 可以使用sigmoid 函数近似的去不断的接近最优解。最优模型的形状一部分是min，一部分是 max，中间的斜率比较大。（和sigmoid 的形状是比较像） 上面使用的是个人的信用评分，得到是一个基础额度。最后的额度还需要考虑个人资质等一系列的因素。 定价策略 这里的定价是利率的定价，还款利率的高低。主要要提供差异化定价策略。在这个讨论中提到一个专业的名词，贴水。你得到的利率是包含你的风险。基本的原则，风险越高，利率越高。 如何收益的增加？回去借贷高风险，对于利率不太敏感，适当增加的利率或者贴水。对于资质比较好的，对于利率比较敏感，那么维持较低的利率。在高风险人群得到收益。 评分模型 分成贷前、贷中和贷后三个阶段。 KS 曲线对于预测能力指标：ROC/ AUC, K-S指标 和GINI系数。 KS(Kolmogorov-Smirnov)：KS用于模型风险区分能力进行评估， 指标衡量的是好坏样本累计部分之间的差值。好坏样本累计差异越大，KS指标越大，那么模型的风险区分能力越强。数学表达式为： $K-S = max( TPR- FPR)$。 KS值的取值范围是 $[0, 1]$。 下面是图解。 计算步骤 计算每个评分区间的好坏账户数。 计算每个评分区间的累计好账户数占总好账户数比率(good%)和累计坏账户数占总坏账户数比率(bad%)。 计算每个评分区间累计坏账户占比与累计好账户占比差的绝对值（累计good%-累计bad%），然后对这些绝对值取最大值即得此评分卡的K-S值。 在完成一个模型后，将测试模型的样本平均分成10组，以好样本占比降序从左到右进行排列，其中第一组的好样本占比最大，坏样本占比最小。这些组别的好坏样本占比进行累加后得到每一组对应的累计的占比。好坏样本的累计占比随着样本的累计而变化（图中Good/Bad两条曲线），而两者差异最大时就是我们要求的K-S值（图中比较长的直线箭头的那个位置）。 这两条曲线之间的差值，就是K-S曲线。如图1所示，给定一个通过率20%（拒绝率80%），则该模型可以挑出来60%的好人，同时漏进来8%的坏人（92%的坏人都被拒绝掉了）。那么K-S曲线在这个通过率上的值，就是60%-8%=0.52。(好好看，这个坐标的对应关系) K-S曲线主要是验证模型的区分能力，通常是在模型预测全体样本的信用评分后，将全体样本按违约与非违约分为两部分，然后用K-S统计量来检验这两组样本信用评分的分布是否有显著差异。选择最大间隔对于的横轴概率阈值为最佳概率阈值。 KS值 含义 $&gt; 0.3$ 模型预测性较好 $0.2 \sim 0.3 $ 模型可用 $0 \sim 0.2$ 模型预测能力较差 $&lt; 0$ 模型错误 K-S值一般是很难达到0.6的，在0.2~0.6之间都不错。一般如果是如果负样本对业务影响极大，那么区分度肯定就很重要，此时K-S比AUC更合适用作模型评估，如果没什么特别的影响，那么用AUC就很好了。 K-S 值越大，表明分类器对正 负类的区分能力越好。并非所有的情况KS都是越高越好的，尤其在征信模型中。征信模型中，最期望得到的信用分数分布是正态分布，对于正负样本分别而言，也都期望是呈正态分布的样子。如果KS值过大，一般超过0.9，就可以认为正负样本分得过开了，不太可能是正态分布的，反而是比较极端化的分布状态（U字形，两边多，中间少），这样的分数就很不好，基本可以认为不可用。但如果模型的目的就是完美区分正负样本，那么KS值越大就表明分隔能力越突出。另外，KS值所代表的仅仅是模型的分隔能力，并不代表分隔的样本是准确的。换句话说，正负样本完全分错，但KS值可以依旧很高。(在实习的过程中，对于正太分布的执着真是非常执着的) KS曲线和ROC 曲线的区别和联系： K-S曲线其实数据来源和本质和ROC曲线是一致的，只是ROC曲线是把真正率和假正率当作横纵轴，而K-S曲线是把真正率和假正率都当作是纵轴，横轴则由选定的阈值来充当。不同的是，ROC曲线用FPR作为横轴，TPR作为纵轴，采用描点法绘制，图中总共是一条线；而KS曲线的横轴则是不同的概率判断阈值，图中一共有两条线，分别代表了FPR值和TPR值。（具体可以参考上文的图像讲解） 要弄明白ks值和auc值的关系首先要弄懂roc曲线和ks曲线是怎么画出来的。其实从某个角度上来讲ROC曲线和KS曲线是一回事，只是横纵坐标的取法不同而已。拿逻辑回归举例，模型训练完成之后每个样本都会得到一个类概率值（注意是类似的类），把样本按这个类概率值排序后分成10等份，每一份单独计算它的真正率和假正率，然后计算累计概率值，用真正率和假正率的累计做为坐标画出来的就是ROC曲线，用10等分做为横坐标，用真正率和假正率的累计值分别做为纵坐标就得到两个曲线，这就是KS曲线。AUC值就是ROC曲线下放的面积值，而ks值就是ks曲线中两条曲线之间的最大间隔距离。由于ks值能找出模型中差异最大的一个分段，因此适合用于cut_off，像评分卡这种就很适合用ks值来评估。但是ks值只能反映出哪个分段是区分最大的，而不能总体反映出所有分段的效果，因果AUC值更能胜任。 好的信用风控模型一般从准确性、稳定性和可解释性来评估模型。 参考资料：深入理解KS GINI系数 还记得经济学中那个著名的基尼系数吗？下图应该可以让你回忆起来。将一个国家所有的人口按最贫穷到最富有进行排列，随着人数的累计，这些人口所拥有的财富的比例也逐渐增加到100%，按这个方法得到图中的曲线，称为洛伦兹曲线。基尼系数就是图中A/B的比例。可以看到，假如这个国家最富有的那群人占据了越多的财富，贫富差距越大，那么洛伦茨曲线就会越弯曲，基尼系数就越大。 同样的，假设我们把100个人的信用评分按照从高到低进行排序，以横轴为累计人数比例，纵轴作为累计坏样本比例，随着累计人数比例的上升，累计坏样本的比例也在上升。如果这个评分的区分能力比较好，那么越大比例的坏样本会集中在越低的分数区间，整个图像形成一个凹下去的形状。所以洛伦兹曲线的弧度越大，基尼系数越大，这个模型区分好坏样本的能力就越强。 风控模型：ks， 基尼系数 -&gt;质量psi -&gt;稳定性PSI即Population stability index： 模型稳定性 其他经济学概念股市和债市的区别 股市和债市的关系 当股市开始走牛时，债市牛市进入最后阶段，当大部分人都知道股市牛市来临时，债市开始走熊。 当股市从牛市转为熊市时，债市跌最后一波，当大部分人都知道股市熊市来临时，债市开始走牛。 这是因为经济周期决定的。经济进入复苏期后，社会各行业平均利润率逐步升高，而贷款利率其实就是各行业平均利润率，所以银行利率停止下降，但此时银行利率还在最低点，社会流动性也充足，债市靠惯性进入牛市的最后一波。因为经济复苏，股市开始进入牛市。股市走牛一段时间后，因为资本的逐利性，大量投资者开始从债市撤退；债市经过长期上涨，其收益率也远远低于股票的分红率；利率的开始上涨导致债市基本面逆转。债市开始走熊，而股市继续走牛。随着经济由复苏进入繁荣，社会各行业平均利润率到达高峰，股市进入牛市最后阶段，债市继续走熊。当经济由繁荣进入危机期，为了应对危机，国家还在不断收紧流动性，社会各行业平均利润率也开始下降，经济滞涨，股市和债市同时下跌。当经济由危机进入萧条期，国家开始逐步放松货币，但社会各行业平均利润率仍然在下滑，贷款需求下降，大量宽松出来的银行资金首先进入债市；经过漫长下跌，债市的收益率在历史高位，具有投资价值；当大部分人都知道熊市来后，资金开始从股市向债市转移，导致债市开始走牛。去年美国国债居然上涨了18%，5年期国债年收益率只有0.65%，主要就是因为美国经济去年在萧条期，今年美国经济开始复苏，股票就成为最好的投资标的。 股市和债市的定义： 股票市场是已经发行的股票转让、买卖和流通的场所，包括交易所市场和场外交易市场两大类别。由于它是建立在发行市场基础上的，因此又称作二级市场。股票市场的结构和交易活动比发行市场（一级市场）更为复杂，其作用和影响力也更大。股票市场的前身起源于1602年荷兰人在阿姆斯特河大桥上进行荷属东印度公司股票的买卖，而正规的股票市场最早出现在美国。股票市场是投机者和投资者双双活跃的地方，是一个国家或地区经济和金融活动的寒暑表，股票市场的不良现象例如无货沽空等等，可以导致股灾等各种危害的产生。股票市场唯一不变的就是：时时刻刻都是变化的。中国有上交所和深交所两个交易市场。债券市场是发行和买卖债券的场所，是（金融市场）一个重要组成部分。债券市场是一国金融体系中不可或缺的部分。 一个统一、成熟的债券市场可以为全社会的投资者和筹资者提供低风险的投融资工具；债券的收益率曲线是社会经济中一切金融商品收益水平的基准，因此债券市场也是传导中央银行货币政策的重要载体。可以说，统一、成熟的债券市场构成了一个国家金融市场的基础。 多投: 投入了多家进行借贷之类的 对于第三方，就一个大数据公司，还是要体现数据优势，数据质量，匹配程度。贷前是风险控制， 贷中是价值挖掘，发现优质客户。维度：三方数据、工商基本信息和自有数据。 白户和纯白户：（信用记录是不全的）白户是指有申请记录，但最后申请失败没有下卡，时间半年以上。 纯白户是指，没有申请过信用卡与贷款，个人信用空白的叫纯白户。 纯白户和白户都是指没有银行信用记录的客户群体，此类人群在办理贷款时，难免会卡壳遇阻，究其原因，为了把控风险，部分银行只将受众锁定在了有信用记录且良好的人群身上。 Excel 的使用 vloopup 函数用于匹配查找。 四个参数：(查找的关键字，在哪个区域查找，返回往右边数第几列，精确查找/ 模糊查找)第一个参数就是关键字，第二个参数是关键字所在列为最左侧的列的区域全选的时候，选中标题行，ctrl shift + down +F4这样就选中了所有下面的数据相对于关键字第三参数：以关键词所在的列为第一列，然后是要的第几列的数据第四个参数使用 0 或者 false，表示精确查找 透视表/透视图 进行数据的筛选和统计 在excel 中可以使用 “=left” 进行字符串的切分 会议记录书写模板 双方参与人员：时间：地点 会议的主要内容： … … …. 后续的计划安排 也是很重要，一定要写好。 机器学习关于规则还是模型？稳定的特征是在模型中，不太稳定的是在规则中。 常常使用的工具：xgblightgbmspss 商业软件 数据分析软件 个人信息三要素：姓名、手机号和身份证号。 只要将这些信息进行加密or 缺省，那么一般是不会造成数据的缺失的。 风控的流程图：]]></content>
      <categories>
        <category>NOT_FOR_YOU</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Similarity Measures]]></title>
    <url>%2F2019%2F06%2F21%2Fsimilarity-measures%2F</url>
    <content type="text"><![CDATA[The similarity measure is the measure of how much alike two data objects are. Similarity measure in a data mining context is a distance with dimensions representing features of the objects. If this distance is small, it will be the high degree of similarity where large distance will be the low degree of similarity. The similarity is subjective and is highly dependent on the domain and application. For example, two fruits are similar because of color or size or taste. Care should be taken when calculating distance across dimensions/features that are unrelated. The relative values of each element must be normalized, or one feature could end up dominating the distance calculation. Similarity are measured in the range 0 to 1 [0,1]. Euclidean distanceEuclidean distance is the most common use of distance. In most cases when people said about distance, they will refer to Euclidean distance. Euclidean distance is also known as simply distance. When data is dense or continuous, this is the best proximity measure. Applications: Where data is continuous or numerical . Also knows as L2 Norm famously. In an n dimensional space between two vectors x and y the formula is simply the square root of the sum of the square distance: $$d \left( \left[ x _ { 1 } , x _ { 2 } , \ldots , x _ { n } \right] , \left[ y _ { 1 } , y _ { 2 } , \ldots , y _ { n } \right] \right) = \sqrt { \sum _ { i = 1 } ^ { n } \left( x _ { i } - y _ { i } \right) ^ { 2 } }$$ 123456from math import * def euclidean_distance(x,y): return sqrt(sum(pow(a-b,2) for a, b in zip(x, y))) print euclidean_distance([0,3,4,5],[7,6,3,-1]) 适用范围：通用型，在连续稠密的向量计算中相比更好。使用这个 matrix 的时候，最好是将数据规范化，一种原因在于 这个是乘方的运算，规范化之后误差是不至于太大。 Manhattan distanceThis Manhattan distance metric is also known as Manhattan length, rectilinear distance, L1 distance or L1 norm, city block distance, Minkowski’s L1 distance, taxi-cab metric, or city block distance. Applications: can be used if the dimensions are continuous or numeric. This distance measure is very similar to Euclidean but it is a sum of the absolute difference of every dimension rather than the sum of squares. $$d = \sum _ { i = 1 } ^ { n } \left| x _ { i } - y _ { i } \right|$$ 123456from math import *def manhattan_distance(x,y): return sum(abs(a-b) for a,b in zip(x,y)) print manhattan_distance([10,20,10],[10,20,20]) 适用范围：可以处理异常值、具有特征选择的功能，可以有多个解（而 Euclidean distance 只有一个最优解） Minkowski distanceMinkowski distance is the generalized distance metric. 当p =1 和2 时，恰好是 Euclidean distance 和Manhattan distance. $$\left( \sum _ { i = 1 } ^ { n } \left| x _ { i } - y _ { i } \right| ^ { p } \right) ^ { 1 / p }$$ 1234567891011121314 from math import *from decimal import Decimal def nth_root(value, n_root): root_value = 1/float(n_root) return round (Decimal(value) ** Decimal(root_value),3) def minkowski_distance(x,y,p_value): return nth_root(sum(pow(abs(a-b),p_value) for a,b in zip(x, y)),p_value) print minkowski_distance([0,3,4,5],[7,6,3,-1],3) Cosine similarityCosine similarity is particularly used in positive space, where the outcome is neatly bounded in [0,1]. One of the reasons for the popularity of cosine similarity is that it is very efficient to evaluate, especially for sparse vectors. Applications: Used in identifying document similarity, product recommendations, Information retrieval and works efficiently with high-dimensional sparse data. It is an angle between two data points in the vector space. Given a vector A and B, the cosine distance is the dot product of x and y divided by Euclidean distance. $$\text { similarity } = \cos ( \theta ) = \frac { \mathbf { A } \cdot \mathbf { B } } { | \mathbf { A } | | \mathbf { B } | } = \frac { \sum _ { i = 1 } ^ { n } A _ { i } B _ { i } } { \sqrt { \sum _ { i = 1 } ^ { n } A _ { i } ^ { 2 } } \sqrt { \sum _ { i = 1 } ^ { n } B _ { i } ^ { 2 } } }$$ 12345678910111213from math import * def square_rooted(x): return round(sqrt(sum([a*a for a in x])),3) # round(num, ndigits) 在python2 中是四舍五入 def cosine_similarity(x,y): numerator = sum(a*b for a,b in zip(x,y)) denominator = square_rooted(x)*square_rooted(y) return round(numerator/float(denominator),3) print cosine_similarity([3, 45, 7, 2], [2, 54, 13, 15]) Cosine similarity vs Euclidean distance首先从公式上说， cosine similarity 考虑的是角度 (angle)而不是 magnitude，可以排除文章长度的干扰。 Mathematically, it measures the cosine of the angle between two vectors projected in a multi-dimensional space. The cosine similarity is advantageous because even if the two similar documents are far apart by the Euclidean distance (due to the size of the document). 如果涉及到（使用 tf or tf-idf）生成了向量表示，使用 cosine similarity 比 euclidean 更好。 如果使用 word2vec 生成的向量，那么 euclidean 是不是更好的选择。Cosine is mostly used on very sparse, discrete domains such as text. Here, most dimensions are 0 and do not matter at all. 如果你想要 magnitude，那么使用 ED。Euclidean is commonly used on dense, continuous variables. There every dimension matters。 这里有一个重要的观点， Cosine is essentially the same as Euclidean on normalized data. 在很高的维度，两者都是不行的，这就是 Curse of Dimensionality. 适用范围： sparse, discrete 这个特点决定了在 nlp 中使用是比较广泛的； 而Euclidean distance 在图像中（稠密连续值）中使用比较广泛。 Jaccard similarityJaccard Distance measures how close two sets are. It is simply a ratio of the intersection of the sets to the Union. Can be used when the datatypes are categorical . Example: Products purchased/viewed by customers. Typically used in Product recommendation, Clustering customers based on purchase/engagement patterns. Please note Jaccard distance is a dissimilarity metric and Jaccard coefficient, J(A,B) is a similarity metric. $$d _ { J } ( A , B ) = 1 - J ( A , B ) = \frac { | A \cup B | - | A \cap B | } { | A \cup B | }$$ 123456789from math import * def jaccard_similarity(x,y): intersection_cardinality = len(set.intersection(*[set(x), set(y)])) # 参数前一个 *表示传入的是一个元祖 tuple union_cardinality = len(set.union(*[set(x), set(y)])) return intersection_cardinality/float(union_cardinality) print jaccard_similarity([0,1,2,5,6],[0,2,3,5,7,9]) 适用范围：set() 中计算。 Edit Distance Edit distance is used when the comparing strings. Ideal use cases would be auto spell check, meta data correction etc. The distance between two strings are smaller if the number of corrections ( insertions or deletions ) needed to perfectly match are smaller. 常见的dp 解法。 123456789101112131415161718192021222324class Solution(object): def minDistance(self, word1, word2): """ :type word1: str :type word2: str :rtype: int """ m = len(word1) n = len(word2) table = [[0] * (n + 1) for _ in range(m + 1)] for i in range(m + 1): table[i][0] = i for j in range(n + 1): table[0][j] = j for i in range(1, m + 1): for j in range(1, n + 1): if word1[i - 1] == word2[j - 1]: table[i][j] = table[i - 1][j - 1] else: table[i][j] = 1 + min(table[i - 1][j], table[i][j - 1], table[i - 1][j - 1]) return table[-1][-1] 适用范围： string 中的距离。 Kullback–Leibler divergenceKullback–Leibler divergence (KL 散度) (also called relative entropy 相对熵)， 是衡量两个分布之间的差异性指标。 可以写成这样： $$D_{\mathrm{KL}}(P | Q)=-\sum_{x \in \mathcal{X}} P(x) \log \left(\frac{Q(x)}{P(x)}\right)$$ 也可以写成这样： $$D_{\mathrm{KL}}(P | Q)=\sum_{x \in \mathcal{X}} P(x) \log \left(\frac{P(x)}{Q(x)}\right)$$ KL 散度是交叉熵和熵之差。推导如下：$$\begin{split}D_{K L}(p | q) &amp;= H(p) -H(p, q) \\ &amp; =-\int p(x) \log q(x)-\left(-\int p(x) \log p(x)\right) \\ &amp;=-\int p(x) \log \frac{q(x)}{p(x)} d x\end{split}$$（上面的式子是错误的，应该是交叉熵和熵的差） 特点： 不具有对称性 当两个分布不相交时候，距离趋向无穷，无法反应距离关系 复习总结 Euclidean distance 最常用的，适合在连续稠密的向量中进行计算。 绝对值distance， 适合处理异常值，具有特征选择的功能（是否还记得L1） cosine similarity，适合在文章相似度、商品推荐、信息召回等稀疏高纬数据中。 cosine vs. 欧氏距离，前者考虑的是角度(angle) 而不是magnitude，因此可以排除文章长度的影响；前者在nlp中使用比较广泛，后者在图像（稠密连续值）中使用比较广泛。 Jaccard similarity，交集比上并集 edit distance，字符串的编辑距离 KL 散度，衡量两个分布之间差异的指标]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>similarity-measures</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[overfit]]></title>
    <url>%2F2019%2F06%2F21%2Foverfit%2F</url>
    <content type="text"><![CDATA[分析机器学习 和深度学习中出现的过拟合现象，从不同的角度简述常用的处理手段。 什么是过拟合?训练数据集规模小是导致过拟合的原因，而网络足够的复杂（有能力）记住了所有的样本，然后在train sets 表现要远远好于 test sets。 还有一种说法是网络拟合了噪声数据。 如何处理过拟合处理该问题可以从数据和模型两个方面去考虑。 模型角度简化模型，通过不断降低模型的复杂度（比如随机森林中的估计量，神经网络中的参数），最终达到一个平衡状态：模型足够简单到不产生过拟合，又足够复杂到能从数据中学习。这样操作时一个比较方便的方法是根据模型的复杂程度查看模型在所有数据集上的误差。如 图 1 所示。 图 1 简化模型的另一个好处在于训练速度更加快. 数据角度 获取更多的数据 数据增强 获取更多的数据侧重获取得到的原始的训练数据集；而数据增强在图像处理中更加常见，主要是图像的变形，噪声方面进行考虑。 训练过程角度提前终止 (early stop) 如图 1 ，当 test error 增加的时候，那么模型就应该停止了。 正则化角度神经网络中有主要有两类实体：神经元和连接神经元的边。所以按照规范化的操作对象的不同可以分成两大类，一类是对于L 层的神经元的激活值或者说对于第 L+1 层网络神经元的输入值进行normalization 操作，比如说 batch normalization / Layer normalization 等方法都是属于这类；另一种是对于神经元之间相连的边上的权重进行规范化操作，比如说 weights normalization就属于这类。 广义上讲，一般机器学习中的损失函数中加入的 L1/ L2 等正则项 属于第二类。L1 正则项造成参数的稀疏性，使得大量的参数取得 0值， L2 正则项使得原始参数值有效的减小。通过这些规范化的手段改变参数值，已达到避免模型过拟合的目的。 （最初对于输入data 的normalization， 是属于神经元的 normalization） 虽然上述方法分别对神经元和weights 进行了规范化，但本质上都实现了对数据的规范化，只是 scale 的参数的来源是不同的。 使用L1 or L2 在 loss function (error function) 中添加正则项，对损失函数中的weights 进行限制其变大。 对于神经元的激活值来说，不管哪种方式，其目标都是一样的，将激活值规范到均值为0，方差为1 的正太分布。 BN 定义：BN针对一个minibatch的输入样本，计算均值和方差，基于计算的均值和方差来对某一层神经网络的输入X中每一个case进行归一化操作。 BN 的优点： 是一种正则化手段，加上BN 之后，学习率可以有很大的提高，可以加快模型的收敛速度 一般来说在激活函数之前比较好解释一些，效果好一些；输入激活函数之前进行了数据的归一化，防止进去到激活函数的饱和区。 所以可以得到BN 的适用场景：每个mini-batch 都比较大，数据分布比较接近。在训练之前，最好是做好了 充分的shuffle，否则效果可能不太好。 BN 的不足： 高度依赖 mini-batch 的大小，当batch size 比较小的时候，效果不好。因为数据样本少，得不到有效的统计量，也可以说噪声比较大。当然是可以通过调整 batch size 的大小规避这种问题，但是有的任务要求 batch size 不能太大；并且BN 是无法应用到 online learning 中的，因为online 都是单实例更新模型，很难组织起 mini-batch 的结构。 对于相似级别的图像生成任务，BN 效果不佳对于图片分类等任务，只要能够找出关键特征，就能正确分类，这算是一种粗粒度的任务，在这种情形下通常 BN 是有积极效果的。但是对于有些输入输出都是图片的像素级别图片生成任务，比如图片风格转换等应用场景，使用 BN 会带来负面效果，这很可能是因为在 Mini-Batch 内多张无关的图片之间计算统计量，弱化了单张图片本身特有的一些细节信息。 因为输入的 Sequence 序列是不定长的，这源自同一个 Mini-Batch 中的训练实例有长有短。对于类似 RNN 这种动态网络结构，BN 使用起来不方便 训练时和推理时统计量不一致对于 BN 来说，采用 Mini-Batch 内实例来计算统计量，这在训练时没有问题，但是在模型训练好之后，在线推理的时候会有麻烦。因为在线推理或预测的时候，是单实例的，不存在 Mini-Batch。虽说实际使用并没大问题，但是确实存在训练和推理时刻统计量计算方法不一致的问题。 Layer normalization 也是一种正则化手段 BN vs LN ： 从图中看可以知道 batch是“竖”着来的，各个维度做归一化，所以与batch size有关系。 layer是“横”着来的，对一个样本，不同的神经元neuron间做归一化。显示了同一层的神经元的情况。假设这个mini-batch一共有N个样本，则Batch Normalization是对每一个维度进行归一。而Layer Normalization对于单个的样本就可以处理。 相同点： BN 和LN 都是可以很好的一直梯度消失和梯度爆炸的。 实践证明， LN 更加适合 RNN ，BN 更加适合CNN 至于各种 Normalization 的适用场景，可以简洁归纳如下：对于 RNN 的神经网络结构来说，目前只有 LayerNorm 是相对有效的；如果是 GAN 等图片生成或图片内容改写类型的任务，可以优先尝试 InstanceNorm；如果使用场景约束 BatchSize 必须设置很小，无疑此时考虑使用 GroupNorm；而其它任务情形应该优先考虑使用 BatchNorm。 参考文献：深度学习中的Normalization模型 四种不同的normalization 四种 normalization 时间轴 BN（Batch Normalization）于2015年由 Google 提出，开创了Normalization 先河；2016年出了LN（layer normalization）和IN（Instance Normalization）；2018年也就是今年，Kaiming提出了GN（Group normalization），成为了ECCV 2018最佳论文提名。 正则化的本质是为了减少数据的复杂程度和减少模型的复杂程度，这样就可以设置更大的学习率，更快的进行收敛。 用更简单的语言来讲，各种Norm之间的差别只是针对的维度不同而已。 BN是在batch上，对N、H、W做归一化，而保留通道 C 的维度。BN对较小的batch size效果不好。BN适用于固定深度的前向神经网络，如CNN，不适用于RNN； LN在通道方向上，对C、H、W归一化，主要对RNN效果明显； IN在图像像素上，对H、W做归一化，用在风格化迁移； GN将channel分组，然后再做归一化。 BN 主要在 CNN中使用 BN 的缺点 效果依赖于batch size，如果batch size 比较小，那么效果比较差 理论上 batch size 越大，那么效果是越好的，但是batch size 大的时候，可能出现 out of memory 的问题 LN 不依赖于 batch size, 在 RNN 中效果更好。 IN IN针对图像像素做normalization，最初用于图像的风格化迁移。在三维图像中使用。 GN GN是为了解决BN对较小的mini-batch size效果差的问题。 就像作者自己比较的说法：LN和IN只是GN的两种极端形式。我们对channel进行分组，分组数为G，即一共分G组。 当G=1时，GN就是LN了；当G=C时，GN就是IN了。这也是一个有趣的比较，类似于GN是LN和IN的一个tradeoff。文章里对G的默认值是32。 深度学习中的Normalization总结常用的 Normalization 方法 深度学习中的模型Dropout 或者Dropconnect. 其理念就是在训练中随机让神经元无效（即dropout）或让网络中的连接无效（即dropconnect）。这个有点类似集成学习，提高网络模型的泛化性能，减少过拟合的问题。（类似bagging 的思想，使用不同的网络结构在不同的训练集上进行训练）.如果从集成学习角度理解dropout，那么resnet 网络是不是也有点集成学习的味道。 Dropout 的具体流程 首先随机（临时）删掉网络中一半的隐藏神经元，输入输出神经元保持不变（图3中虚线为部分临时被删除的神经元） 然后把输入x通过修改后的网络前向传播，然后把得到的损失结果通过修改的网络反向传播。一小批训练样本执行完这个过程后，在没有被删除的神经元上按照随机梯度下降法更新对应的参数（w，b）。 然后继续重复这一过程 恢复被删掉的神经元（此时被删除的神经元保持原样，而没有被删除的神经元已经有所更新） 从隐藏层神经元中随机选择一个一半大小的子集临时删除掉（备份被删除神经元的参数）。 对一小批训练样本，先前向传播然后反向传播损失并根据随机梯度下降法更新参数（w，b） （没有被删除的那一部分参数得到更新，删除的神经元参数保持被删除前的结果）。 Dropout vs Dropconnect： 它们的区别大体在于，在训练过程中dropout是随机drop掉一些节点，而dropconnect则是随机drop掉一些边。 使用多种模型Bagging： 最典型的就是 随机森林( Random Forest)， 通过不相关的决策树在不同的数据集上进行训练，最后的每个模型使用相同的权重来“融合”。Boosting: 在简单的网络上不断提升。 总结降低“过拟合”的方法：（1）获得更多的训练数据（2）降低模型复杂度（3）正则化方法（4）集成学习方法降低“欠拟合”风险的方法：（1）添加新特征（2）增加模型复杂度（3）减小正则化系数]]></content>
      <categories>
        <category>Deep learning</category>
      </categories>
      <tags>
        <tag>overfit</tag>
        <tag>dropout</tag>
        <tag>normalization</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ResNet and Inception V3 Understanding]]></title>
    <url>%2F2019%2F06%2F21%2Fresnet-understanding%2F</url>
    <content type="text"><![CDATA[本文介绍 ResNet的背景，想要解决的问题，基本思路和框架 和对于其的一种解读方式。另外，和 Resnet 思路相反的是 Inception 系列。 背景网络的深度是容易出现梯度爆炸和梯度消失，造成网络的不收敛。一些方法已经在很大程度上可以缓解这个问题，比如使用 ReLU激活函数、 良好的权值初始化方法 、还有 intermediate normalization layers(即网络中间的batch normalization)。并且对于网络过程中的过拟合问题，也提出了一些办法如，使用 regularization、权值衰减和dropout方法。 但解决了深度网络收敛问题之后，又出现了另外一个问题。 残差网络要解决的问题一般来说在没有过拟合的情况下，可以逐步增加网络的深度。但在实验中发现了这样的问题。网络退化： 网络越深，训练误差越大。（accuracy开始饱和，原文中这样说的）这种退化并不是由于过拟合造成的，并且在适当深度模型中增加更多的层会导致更多的训练误差 基本思路和结构作者基于增加层如果为恒等映射那么更深层网络不应该比浅层网络产生更高错误率的思想 如图所示左边的是传统的plain networks的结构，右边的是修改为ResNet的结构。改变前目标： 训练 F(x) 逼近 H(x)改变后目标：训练 F(x) 逼近 H(x) -x 即增加一个identity mapping（恒等映射），将原始所需要学的函数H(x)转换成F(x)+x，而作者认为这两种表达的效果相同，但是优化的难度却并不相同，作者假设F(x)的优化 会比H(x)简单的多。这一想法也是源于图像处理中的残差向量编码，通过一个reformulation，将一个问题分解成多个尺度直接的残差问题，能够很好的起到优化训练的效果 在论文中尝试了 skip 2层或者 3层 一种解读方式残差网络单元其中可以分解成右图的形式，从图中可以看出，残差网络其实是由多种路径组合的一个网络，直白了说，残差网络其实是很多并行子网络的组合，整个残差网络其实相当于一个多人投票系统（Ensembling） 从这可以看出其实ResNet是由大多数中度网络和一小部分浅度网络和深度网络组成的， 说明虽然表面上ResNet网络很深，但是其实起实际作用的网络层数并没有很深我们可以看出大多数的梯度其实都集中在中间的路径上，论文里称为effective path。 (网络越深，也是容易出现梯度消失或者梯度爆炸，这个是没有问题的)ResNet其实就是一个多人投票系统。 现在深度网络基本上分成两个方向，一个像 resnet 向着”深度“发展，一个向着”宽度“的inception network Inception V3如果 ResNet 是为了更深，那么 Inception 家族就是为了更宽。第一个见解与对层的操作有关。在传统的卷积网络中，每一层都会从之前的层提取信息，以便将输入数据转换成更有用的表征。 见解 1：为什么不让模型选择？ 这种模型架构的信息密度更大了，这就带来了一个突出的问题：计算成本大大增加。不仅大型（比如 5×5）卷积过滤器的固有计算成本高，并排堆叠多个不同的过滤器更会极大增加每一层的特征映射的数量。而这种计算成本增长就成为了我们模型的致命瓶颈。 这就涉及到了见解 2：使用 1×1 卷积来执行降维。为了解决上述计算瓶颈，Inception 的作者使用了 1×1 卷积来「过滤」输出的深度。一个 1×1 卷积一次仅查看一个值，但在多个通道上，它可以提取空间信息并将其压缩到更低的维度。比如，使用 20 个 1×1 过滤器，一个大小为 64×64×100（具有 100 个特征映射）的输入可以被压缩到 64×64×20。通过减少输入映射的数量，Inception 可以将不同的层变换并行地堆叠到一起，从而得到既深又宽（很多并行操作）的网络。 Inception Net v3 incorporated all of the above upgrades stated for Inception v2, and in addition used the following: RMSProp Optimizer. Factorized 7x7 convolutions. BatchNorm in the Auxillary Classifiers. Label Smoothing (A type of regularizing component added to the loss formula that prevents the network from becoming too confident about a class. Prevents over fitting).]]></content>
      <categories>
        <category>Deep learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Interview Questions]]></title>
    <url>%2F2019%2F06%2F21%2Finterview-questions%2F</url>
    <content type="text"><![CDATA[面试中的非技术类问题。 面试中最重要的是和面试官进行互动，互相了解的过程。如果一方说的过多，那么这个面试仍然是不成功的。在互动的过程中，也是可以学习到很多的。 “你有什么问题想要问我吗？” 不要做的事情如下： 千万不要问小白问题（凡是官网上、度娘上能够获取的信息都是小白问题）。但是你基于获得的知识，进一步提出自己的观点和看法，那么这个就是一个比较好的问题。 不要问对方无法给出你答案的问题。比如不要问 HR部门的具体业务，不要问技术面试官 关于薪酬的问题。 大的前提是根据面试官的身份，围绕着应聘职位进行提问。比如说问HR 公司是否有针对技术方面的一些培训制度，团建之类的，想要了解一下公司的氛围和企业文化 “面试中的自我评价” 主要从以下的三个方面入手： 有自己的优点，比如说自己开朗，具有合作精神，那么最好有事例进行说明 和招聘岗位匹配程度 个人的缺点，当然这个缺点不是致命的，是可以进行转折的那种 “为什么选你，而不是别人” 这本来是面试官的职责，那么既然问了，其实是创新性的帅锅给了候选人，面试官带着“证明你配得上这分工作”的心态去提问，要么是懒，要么是暗示这个岗位很热门。 既然是这种情况，我们能做就是专注于我可以怎么样… 因为别人的信息你也是无法回答的。有以下几个反套路出发点 从工作内容出发如果你已经面了一面、二面，对于自己做的事情有了一个比较清晰的认识，那么你可以说… 这个职位的工作内容，正是我比较熟悉和擅长的。需要使用具体的事实案例进行证明。 真诚反问，创造互动如果不是很清楚岗位的需求，那么上面的方式就不行，这个时候需要创造互动。简单自我介绍一下，然后说“我正想和您探讨一下， 您认为做好这个工作，候选人应该具备怎样的条件？” 如果提到的是自己的优势，那么就使用事例证明一下；如果是自己的弱项，那么简单的说一下，然后说自己是如何提高和改进，展示的好学的一面，最好是有例子证明。 最后如果聊得比较来，自己答题不错的话，那么可以在结束的时候补上一句：“我冒昧问一句，您也面试了一些候选人，您觉得我的机会大吗？” “你最大的缺点是什么“ 招聘本身就是用人之长，弥补自身的缺陷。面试官究竟想要什么？ 候选人在回答这种刁钻问题时，很喜欢避重就轻：“我最大的缺点，就是太较真，对细节要求太高”“我最大的缺点是太拼了，不注意身体”额……这恰恰中了面试官的套路：“这位候选人不够真诚。” 对于候选人来说，我推荐两种更加真诚的回答套路（对，套路也可以真诚）。正确的方式- 讲一个真实存在的缺陷，但强调你已经意识到，并已经在改善了。….. （需要有事例进行说明）这种方式，既能让面试官感受到你的真诚，也能让面试官觉得你是对自己有清醒的认识同时已经开始行动，是一个比较踏实的候选人。 说到底，企业招人是招人之长，如果你的长处是企业急需的，那么你的弱点并非致命，面试官还是很希望得到你的。 ”请简单的介绍一下自己“ 当面试官问出这种问题的时候，我的第一反应就是：这个面试官没有提前阅读我的简历。我希望面试官是阅读了的，然而，我也清楚这几率很低。 所以见证套路的时候到了，简单的自我介绍，我来自… 之前去… 做过…工作，然后在… 看到你们招聘… 岗位，觉得这个是一个不错的机会，所以投了简历。我想简单问您一个问题，可以吗？您这个职位是新设置，还是之前的同事离职呢？ ”要和面试官进行互动，面试官说的越多，那么成功的可能性是越大的，不是一昧的听，而是创造和面试官之间的有机的互动” 好的面试如同老友重逢在Central Perk，互相尊重互相理解，即使做不成同事，没准还可以做朋友。我们平时和小伙伴们谈笑风生，可以友好的提问，也可以开善意的玩笑。但到了面试时，为什么要抱着死板的心态生硬尴聊呢？在这一点上我倒是很赞赏部分互联网公司的高管， 他们抱着开放的态度和候选人聊工作甚至聊人生，也愿意给候选人提供自己的建议。 “你能在公司待多久” 对这个公司发展的前景比较感兴趣，我也希望能给公司带来点什么，能够发挥作用；只要双方都觉得有收获，做的事情有挑战有意义，就都是ok的。 “你希望这个职位的薪水是多少” 这个是一个微妙的问题，在条件允许的情况下，尽可能的拖延给出一个精确的数据来回答这个问题。你可以说，我知道这个工作的薪水大概范围是.. 到…。 或者您能否透露一下公司中对相似职位的工作的薪水大概是什么样子的呢？ 如果面试官继续追问，那么你说“我现在的薪水是… 和其他人一样，我希望能够提升这个数字，但我主要的兴趣还是在工作本身” 要记住新的工作本身并不会使得你赚到更多的钱。 在你面试过程中的最后一个阶段之前，少谈薪水的问题。因为到了那个时候你就清楚，如果公司对你有很大的兴趣，那么这个时候薪水待遇就有很大的余地。 “如何看待加班？”如果特殊时期项目需要上线之类，那么偶尔加班是可以接受的，但是如果强制长期加班，那还是算了。当然，如果自己负责的事情出了问题，即使公司没有要求加班，那么自己也是会主动加班把事情弄好，尽自己最大的努力保证项目不延期。 最好的方式是凭着工作能力说话，让别人觉得能力ok 不加班也是把工作做完做好的。 目前团队工作氛围怎么样？ 加班情况如何？之类的是可以提前问一下的。如果自己急需工作，先拿下工作之后再说，等自己缓和一下再谋出路。 最后的发问？你们小组是干什么，有什么业务产品？团队怎么样？如果我比较幸运能够来着工作的话，那么主要做哪些方面的工作？]]></content>
      <categories>
        <category>NOT_FOR_YOU</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[剑指Offer-栈、队列、链表和树]]></title>
    <url>%2F2019%2F06%2F17%2F%E5%89%91%E6%8C%87Offer-%E6%A0%88-%E9%98%9F%E5%88%97-%E9%93%BE%E8%A1%A8%E5%92%8C%E6%A0%91%2F</url>
    <content type="text"><![CDATA[这是剑指offer 系列四部曲中的第二部：栈、队列、链表和树。第一部关于字符串和数组，第三部是递归、回溯和动态规划， 最后一部分在这里。 从尾到头打印链表 输入一个链表，按链表值从尾到头的顺序返回一个ArrayList。 正向遍历之后，使用的是python 中list的特性， list[::-1] 这样进行输出的。 123456789101112131415161718# -*- coding:utf-8 -*-# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: # 返回从尾部到头部的列表值序列，例如[1,2,3] # += , -= 这个都是同一种类型的 def printListFromTailToHead(self, listNode): # write code here arraylist =[] head = listNode while head != None: arraylist += [head.val] # 这个在这里等效于 arraylist.append(head.val) head = head.next return arraylist[::-1] 在线编程中很少考察树的结构。所以就不写 main 函数版本了。 12345678910111213class Solution &#123;public: // 跟我的想法是一样的，首先遍历一遍，然后翻转，c++ 中的reverse 操作 vector&lt;int&gt; printListFromTailToHead(ListNode* head) &#123; vector&lt;int&gt; res; while(head) &#123; res.push_back(head-&gt;val); head =head-&gt;next; &#125; return vector&lt;int&gt;(res.rbegin(), res.rend()); &#125;&#125;; 重建二叉树 (经典， 多敲多背诵) 输入某二叉树的前序遍历和中序遍历的结果，请重建出该二叉树。假设输入的前序遍历和中序遍历的结果中都不含重复的数字。例如输入前序遍历序列{1,2,4,7,3,5,6,8}和中序遍历序列{4,7,2,1,5,3,8,6}，则重建二叉树并返回。 Tips: 递归，二叉树的题目大多数都是可以使用递归的思想进行解决，因为二叉树本身结构就是递归定义的。递归优点在于代码量比较少。从先序遍历中找出根节点，从中序遍历中找出左右子树 别怕，手写中如何重建二叉树，在代码中就是如何实现重建二叉树的。首先从前序list 中找到头结点，然后从中序队列中找见对应节点的index，那么前面的就是头结点的左子树，后面的就是右子树。 12345678910111213141516171819202122# -*- coding:utf-8 -*-# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: # 返回构造的TreeNode根节点 # 需要理解在前序遍历中是先遍历左子树的，并且中序和前序中左子树的个数是不会变的 def reConstructBinaryTree(self, pre, tin): # write code here if len(pre) == 0: return None root = TreeNode(pre[0]) # 这个index 函数是需要记住的 index = tin.index(pre[0]) # 这里也是需要修改的 # pre 和 tin都是需要空出一个 root.value 的位置，只不过选择空的位置是不一样的 root.left = self.reConstructBinaryTree(pre[1:index + 1], tin[:index]) root.right = self.reConstructBinaryTree(pre[index + 1:], tin[index + 1:]) return root 优化点：快速的在中序表中找见某个数的位置。使用hash 表实现。 时间复杂度是 $O(n)$ 12345678910111213141516171819202122232425262728293031323334353637/** * Definition for binary tree * struct TreeNode &#123; * int val; * TreeNode *left; * TreeNode *right; * TreeNode(int x) : val(x), left(NULL), right(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: map&lt;int, int&gt; hash; vector&lt;int&gt; preorder, inorder; TreeNode* reConstructBinaryTree(vector&lt;int&gt; _pre,vector&lt;int&gt; _vin) &#123; preorder =_pre, inorder =_vin; // 方便遍历 for(int i =0; i&lt; inorder.size(); i++) hash[inorder[i]] =i; return dfs(0, preorder.size() -1, 0, inorder.size() -1); &#125; TreeNode* dfs(int pl, int pr, int il, int ir) &#123; if(pl &gt; pr) return nullptr; auto root =new TreeNode(preorder[pl]); int k =hash[root-&gt;val]; auto left =dfs(pl +1, pl+k -il,il, k-1); // 中序和前序对于 左子树的表示 auto right =dfs(pl+k-il+1, pr, k+1, ir); root-&gt;left =left, root-&gt;right =right; return root; &#125; &#125;; 用两个栈实现队列 （经典） 用两个栈来实现一个队列，完成队列的Push和Pop操作。 队列中的元素为int类型。 Tips： 在python 中栈等同于使用list 实现。使用两个栈，意味着一个是push_stack 一个是pop_stack，使用两个栈的“后进先出”表示队列的先进先出（push and pop）从语法上讲 ，if list1 ==[], 那么 list1 ==None, 这两个条件是可以交换判断的。（在list 中） 12345678910111213141516171819class Solution: def __init__(self): self.list1 =[] self.list2 =[] def push(self, node): # write code here self.list1.append(node) def pop(self): # return if not self.list1 and not self.list2 : return None if self.list2 : return self.list2.pop() else: while self.list1: self.list2.append(self.list1.pop()) return self.list2.pop() c++ 实现。 ···c++class Solution{ // 使用一个辅助栈 cache 进行pop() 的操作public: void push(int node) { stack1.push(node); } void copy(stack&lt;int&gt; &amp;a, stack&lt;int&gt; &amp;b) { while(a.size()) { // 从一个栈到另一个栈的转换，这样就实现了 用栈表示队列 // 访问机制是 先top() 访问，然后是 pop() 进行弹出 b.push(a.top()); a.pop(); } } int pop() { copy(stack1, cache); int res =cache.top(); cache.pop(); copy(cache, stack1); return res; } private: stack stack1; stack cache;};··· 链表中倒数第k个结点 输入一个链表，输出该链表中倒数第k个结点。 Tips： 两种解法。一种是遍历存储到list 中，空间复杂度是O(N), 另外一种是两个指针p1，p2，距离相差k，当p2 到达链表尾部，p1 就在导数第k 个位置。 1234567891011121314151617181920212223242526272829303132# -*- coding:utf-8 -*-# class ListNode:# def __init__(self, x):# self.val = x# self.next = None"""尝试使用两个指针版本p1 p2 并且这种 length 在命名上是需要规范的, 并且这种指针操作，最好是拷贝出来进行操作不管怎么说，还是应该求解出来 length of listNode，这种才是正途可以使用两个指针，"""class Solution: def FindKthToTail(self, head, k): # write code here if head == None or k &lt;= 0: return None p1 = head p2 = head len1 = 0 while p1: len1 += 1 p1 = p1.next if k &gt; len1: return None p1 = head while k: p1 = p1.next k -= 1 while p1: p1 = p1.next p2 = p2.next return p2 有两种思路，一种是正向遍历一遍，存储到list 中，然后使用list 性质，返回倒数第k 个结点，这个空间复杂度是 $O(n)$。还有一种思路是 遍历一遍得到链表的长度，然后倒数第 k 个结点就是正向数 n-k +1 个结点，再次遍历一遍就可以返回。 123456789101112131415161718192021/*struct ListNode &#123; int val; struct ListNode *next; ListNode(int x) : val(x), next(NULL) &#123; &#125;&#125;;*/class Solution &#123;public: ListNode* FindKthToTail(ListNode* head, unsigned int k) &#123; int n =0; for(auto p =head ; p; p =p-&gt;next) n++; //对于复杂的数据类型，直接使用 auto 这种方式进行定义就可以了 if(k &gt;n) return nullptr; // 这个是一个细节，但是牛客网上没有显示，如果 k 是超过总的长度那么怎么办 auto p =head; for(int i =0; i&lt;n-k; i++) p =p-&gt;next; return p; &#125;&#125;; 反转链表 输入一个链表，反转链表后，输出新链表的表头。 Tips： 需要三个指针，cur，next_node, pre。 简单的链表的修改，最后新的表头就是一开始的尾节点。 1234567891011121314151617181920212223242526# -*- coding:utf-8 -*-# class ListNode:# def __init__(self, x):# self.val = x# self.next = None"""修改链表是需要三个指针的 pre, cur, next_node 如果对三个指针名进行命名好了，那么这个就是成功的一般了， 这个不容易想到的是设置pre =None ，这个是一个细节经验性的问题"""class Solution: # 返回ListNode def ReverseList(self, pHead): # write code here if pHead ==None: return None pre =None cur =pHead while cur: next_node =cur.next cur.next =pre pre, cur =cur, next_node return pre 单链表中需要记录一个前驱结点。这个是考点所在。这个是没有问题的。 123456789101112131415161718192021222324252627/*struct ListNode &#123; int val; struct ListNode *next; ListNode(int x) : val(x), next(NULL) &#123; &#125;&#125;;*/class Solution &#123;public: ListNode* ReverseList(ListNode* head) &#123; // 这个auto 的关键字 不能是在程序未知的情况下使用，应该是在“可知”的情况下使用 ListNode* pre =nullptr; auto cur =head; while(cur) &#123; auto next =cur-&gt;next; cur-&gt;next =pre; pre =cur; // 这个时候需要遍历 cur 指针 cur =next; &#125; return pre; &#125;&#125;; 合并两个排序的链表 Tips： 归并排序中的“并” 操作，只不过由原来的list 操作到现在的 linkedlist 操作。 输入两个单调递增的链表，输出两个链表合成后的链表，当然我们需要合成后的链表满足单调不减规则。 123456789101112131415161718192021222324252627282930313233# def __init__(self, x):# self.val = x# self.next = None"""就是在使用两个或者多个 index (p1 or p2) 遍历的时候，一个常见的错误就是忘记了不断更新index"""class Solution: # 返回合并后列表 def Merge(self, pHead1, pHead2): # write code here if pHead1 == None: return pHead2 if pHead2 == None: return pHead1 head = ListNode(-1) head1 = head p1 = pHead1 p2 = pHead2 while p1 and p2: if p1.val &lt; p2.val: head.next = p1 p1 = p1.next else: head.next = p2 p2 = p2.next head = head.next if p1 == None: head.next = p2 if p2 == None: head.next = p1 return head1.next 归并排序的原理，合并两个有序的数组或者链表，使用线性的复杂度就可以使用。 归并排序中while 中使用的if else， 然后两个while 判断边界条件。 快排中 while 循环里面嵌套的是两个while 排序。这两个排序算法应该是碎觉都是十分熟悉的，怎么可以这样呢？每天都是要多复习的。 1234567891011121314151617181920212223242526272829303132333435/*struct ListNode &#123; int val; struct ListNode *next; ListNode(int x) : val(x), next(NULL) &#123; &#125;&#125;;*/class Solution &#123;public: ListNode* Merge(ListNode* p1, ListNode* p2) &#123; auto dummy =new ListNode(-1); // 虚拟结点 auto cur =dummy; while(p1 &amp;&amp; p2) &#123; if(p1-&gt;val &lt; p2-&gt; val) &#123; cur -&gt; next= p1; cur =cur-&gt;next; p1 =p1-&gt;next; &#125; else &#123; cur -&gt; next =p2; cur =cur-&gt;next; p2 =p2-&gt;next; &#125; &#125; if (p1) cur-&gt;next =p1; else cur-&gt;next =p2; return dummy-&gt;next; &#125;&#125;; 树的子结构 输入两棵二叉树A，B，判断B是不是A的子结构。（ps：我们约定空树不是任意一个树的子结构） 递归定义，根节点是否相同，左右子树是否相同。 12345678910111213141516171819202122232425262728293031323334# -*- coding:utf-8 -*-# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = None"""分成两部：首先寻找两个根节点的值是否相同；然后判断子树是否完全相同subTree 这个函数就是判断子树是否完全相同的，所以函数的功能一定要搞好"""class Solution: def HasSubtree(self, pRoot1, pRoot2): if not pRoot1: return False if not pRoot2: return False result =False if pRoot1.val ==pRoot2.val: result =self.subTree(pRoot1, pRoot2) if result ==False: result = self.HasSubtree(pRoot1.left, pRoot2) or self.HasSubtree(pRoot1.right, pRoot2) return result def subTree(self, root1, root2): if not root2: return True if not root1: return False if root1.val ==root2.val: return self.subTree(root1.left, root2.left) and self.subTree(root1.right, root2.right) return False 从字符串的匹配 扩展了 树的匹配 12345678910111213141516171819202122232425262728293031/*struct TreeNode &#123; int val; struct TreeNode *left; struct TreeNode *right; TreeNode(int x) : val(x), left(NULL), right(NULL) &#123; &#125;&#125;;*/class Solution &#123;public: bool HasSubtree(TreeNode* root1, TreeNode* root2) &#123; if ( !root1 || !root2 ) return false; // 这个是寻找根节点的过程 if(isPart(root1, root2)) return true; return HasSubtree(root1-&gt; left, root2) || HasSubtree(root1-&gt; right, root2); &#125; bool isPart(TreeNode *p1, TreeNode *p2) &#123; // 找到一个根节点相同，然后不断往下遍历的过程 if (!p2) return true; if(!p1 || p1-&gt;val != p2-&gt; val) return false; return isPart(p1-&gt;left, p2-&gt;left) &amp;&amp; isPart(p1-&gt;right, p2-&gt;right); &#125;&#125;;// 先是遍历找相同的根节点，// 如果相同的话，接着去找相应的左右子树是否相同 第二遍1234567891011121314151617181920212223242526272829/*struct TreeNode &#123; int val; struct TreeNode *left; struct TreeNode *right; TreeNode(int x) : val(x), left(NULL), right(NULL) &#123; &#125;&#125;;*/class Solution &#123;public: //先是遍历根节点，然后再遍历对应根节点的左右结点 bool HasSubtree(TreeNode* root1, TreeNode* root2) &#123; if( !root1 || ! root2) return false; if(isSub(root1, root2)) return true; return HasSubtree(root1-&gt; left, root2) || HasSubtree(root1-&gt; right, root2); &#125; bool isSub(TreeNode * root1, TreeNode * root2) &#123; if( !root2) return true; if( ! root1 || root1-&gt;val != root2-&gt;val) return false; return isSub(root1-&gt; left, root2-&gt; left) &amp;&amp; isSub(root1-&gt; right, root2-&gt; right); &#125;&#125;; 二叉树的镜像 操作给定的二叉树，将其变换为源二叉树的镜像。 Tips：求解二叉树镜像，A 的左子树对应着B 的右子树。 123456789101112131415161718192021# -*- coding:utf-8 -*-# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = None"""就是在某个左（右）子树是None 的情况下，这个也是可以进行交换的，结束的标志应该是根节点是否为空"""class Solution: # 返回镜像树的根节点 def Mirror(self, root): # write code here if not root: return None root.left , root.right =root.right, root.left if root.left: self.Mirror(root.left) if root.right: self.Mirror(root.right) return root 任意一个结点的左右子树都发生互换。 12345678910111213141516171819202122/*struct TreeNode &#123; int val; struct TreeNode *left; struct TreeNode *right; TreeNode(int x) : val(x), left(NULL), right(NULL) &#123; &#125;&#125;;*/class Solution &#123;public: // 特点，任意一个结点，左右子树都是相反的 // 互换的过程应该是 从下到上进行的，所以是不断的进行递归，然后最后一个是 swap() 操作 void Mirror(TreeNode *pRoot) &#123; if ( ! pRoot) return ; Mirror(pRoot -&gt; left); Mirror(pRoot -&gt; right); swap(pRoot -&gt; left, pRoot-&gt; right); &#125;&#125;; 包含min函数的栈 定义栈的数据结构，请在该类型中实现一个能够得到栈中所含最小元素的min函数（时间复杂度应为O（1））。 Tips: 这个跟“使用两个栈表示队列” 是差不多的，就是单独使用一个list 存储min 函数调用的一个列表，这样的话能达到时间复杂度是 O(1). 在原来的基础上，stack 的基础上，使用新的 min_stack 满足这个需求，同样，原来的 push pop 这种操作还是不能少的。所以是维护了两个 list（normal_list, min_list），但是当normal_list pop() 出来的时候，这个min_list 和其pop 出来的不一定是相同的值。（我感觉） 1234567891011121314151617181920212223242526272829303132# -*- coding:utf-8 -*-"""这个栈中最小的元素是变化的，好好理解一下，如果弹出了一个比较大的元素，那么栈中最小的元素是不变的所含元素的最小元素top() and min() 操作是不需要删除元素的， pop 是删除了元素"""class Solution: def __init__(self): self.all_list = [] self.min_list = [] def push(self, node): # write code here if not self.min_list: self.min_list.append(node) else: self.min_list.append(min(node, self.min())) # 好多思想都是基于之前的结果进行求解 self.all_list.append(node) def pop(self): self.all_list.pop() self.min_list.pop() # write code here def top(self): return self.all_list[-1] # write code here def min(self): return self.min_list[-1] 123456789101112131415161718192021222324252627class Solution &#123;public: // 关键在于维护两个 stack stack&lt;int&gt; stk, stk_min; // 这个是最重要的function了 void push(int value) &#123; stk.push(value); //if( stk_min.size()) value = value&lt; stk_min.top() ? value, stk_min.top(); if(stk_min.size()) value =std::min(value, stk_min.top()); stk_min.push(value); &#125; void pop() &#123; stk.pop(); stk_min.pop(); &#125; int top() &#123; return stk.top(); &#125; int min() &#123; return stk_min.top(); &#125;&#125;; 栈的压入、弹出序列 输入两个整数序列，第一个序列表示栈的压入顺序，请判断第二个序列是否可能为该栈的弹出顺序。假设压入栈的所有数字均不相等。例如序列1,2,3,4,5是某栈的压入顺序，序列4,5,3,2,1是该压栈序列对应的一个弹出序列，但4,3,5,1,2就不可能是该压栈序列的弹出序列。（注意：这两个序列的长度是相等的） Tips: 使用一个list 来模拟压入和弹出过程，遍历弹出序列popV，如果结束，那么return True。 12345678910111213def IsPopOrder( pushV, popV): if not pushV: return False tmp =[] while popV: if tmp and popV[0] == tmp[-1]: popV.pop(0) tmp.pop() elif pushV: tmp.append(pushV.pop(0)) else: return False return True 是一种模拟题，因为选择是唯一的，对应某种情况，那么操作是一定的。使用一个 栈模拟整个操作。 1234567891011121314151617181920212223242526272829303132#include&lt;iostream&gt;#include&lt;vector&gt;#include&lt;stack&gt;using namespace std;bool isPopOrder(vector&lt;int&gt; pushV, vector&lt;int&gt; popV)&#123; if(pushV.size() != popV.size()) return false; stack&lt;int&gt; stk; int i =0; for(auto u: pushV) &#123; stk.push(u); while(stk.size() &amp;&amp; stk.top() == popV[i]) &#123; //cout&lt;&lt; stk.top()&lt;&lt;" "&lt;&lt; endl; //cout&lt;&lt; stk.top() &lt;&lt;" "&lt;&lt; i&lt;&lt; endl; stk.pop(); i ++; &#125; &#125; return stk.empty();&#125;int main()&#123; vector&lt;int&gt; pushV=&#123;1, 2, 3, 4, 5&#125;; vector&lt;int&gt; popV=&#123;4, 5, 3, 2, 1&#125;; cout&lt;&lt; isPopOrder(pushV, popV)&lt;&lt; endl; return 0;&#125; 从上往下打印二叉树 从上往下打印出二叉树的每个节点，同层节点从左至右打印。 Tips： 层次遍历，遍历根节点之后加入左右结点。 123456789101112131415161718192021222324# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: # 返回从上到下每个节点值列表，例：[1,2,3] # 层序遍历二叉树， 这个跟数据结构 队列有类似的 # nodes 装上结点，然后vlaues 装上数值 def PrintFromTopToBottom(self, root): # write code here if not root: return [] nodes =[] values = [] nodes.append(root) while nodes: node = nodes.pop(0) values.append(node.val) if node.left: nodes.append(node.left) if node.right: nodes.append(node.right) return values 好的方法，就是枚举出来的方法，大家都是这个是经典的算法，但是这种一开始的 intuition，这种 idea 是怎么出来的。一般是没有人去怎么描述的。所以，是要追根溯源的。对于一个问题，一共有哪些方法是在这个范畴，哪些是可以用的。排除一个认为不可能的，然后就开始尝试。 题目要求是一种层序遍历。对于树的遍历是有深搜 和宽搜两种方式，发现神搜不合适，宽搜正好，所以使用宽度优先搜索。对于宽度优先搜索是需要维护一个队列。 123456789101112131415161718192021222324252627282930313233/*struct TreeNode &#123; int val; struct TreeNode *left; struct TreeNode *right; TreeNode(int x) : val(x), left(NULL), right(NULL) &#123; &#125;&#125;;*/class Solution &#123;public: // 队列中的操作 queue, front() 访问， pop() 删除 vector&lt;int&gt; PrintFromTopToBottom(TreeNode* root) &#123; vector&lt;int&gt; res; if( !root) return res; queue&lt;TreeNode*&gt; qu; qu.push(root); while(qu.size()) &#123; auto t =qu.front(); qu.pop(); res.push_back(t-&gt;val); if(t-&gt;left) qu.push(t-&gt;left); if(t-&gt;right) qu.push(t-&gt;right); &#125; return res; &#125;&#125;; 层序遍历，把二叉树打印成多行（这个是输出是多行，而不是一行）c++ 实现。重点使用了 nullptr 指针进行了尾部的标记。 1234567891011121314151617181920212223242526272829303132333435363738394041/*struct TreeNode &#123; int val; struct TreeNode *left; struct TreeNode *right; TreeNode(int x) : val(x), left(NULL), right(NULL) &#123; &#125;&#125;;*/class Solution &#123;public: // 打印成多行，所以在每行打印的时候，每行的最后可以加入一个 nullptr 作为结束 vector&lt;vector&lt;int&gt; &gt; Print(TreeNode* pRoot) &#123; vector&lt;vector&lt;int&gt;&gt; res; if(! pRoot) return res; queue&lt;TreeNode*&gt; qu; qu.push(pRoot); qu.push(nullptr); vector&lt;int&gt; level; //判断 queue 是否为空的条件 while(qu.size()) &#123; auto t =qu.front(); qu.pop(); if(! t) &#123; if(level.empty()) break; res.push_back(level); qu.push(nullptr); level.clear(); continue; &#125; level.push_back(t-&gt;val); if(t-&gt;left) qu.push(t-&gt;left); if(t-&gt;right) qu.push(t-&gt;right); &#125; return res; &#125; &#125;; 二叉搜索树的后序遍历序列 输入一个整数数组，判断该数组是不是某二叉搜索树的后序遍历的结果。如果是则输出Yes,否则输出No。假设输入的数组的任意两个数字都互不相同。 Tips：二叉搜索树，按照中序遍历的话，就是一个排序的二叉树，根节点大于左子树，右子树大于根节点。后序遍历序列中最后一个是根节点，小于根节点是左子树，大于根节点的是右子树，这样进行判断。 123456789101112131415class Solution: # 后序遍历结果， 最后一个是根节点，这个是递归的思想 # 二叉搜索树， 左子树小于根节点，右子树大于根节点 def VerifySquenceOfBST(self, sequence): # write code here if not sequence: return False root = sequence[-1] for i in range(len(sequence)): if sequence[i] &gt; root: break for j in range(i, len(sequence)): if sequence[j] &lt; root: return False return True 这个边界条件也是比较好进行处理的。 1234567891011121314151617181920212223242526class Solution &#123;public: // 是判断题，需要返回的true or false 两种选择 // 搜索二叉树 ，左子树小于根节点， 右子树大于根节点，后序遍历是左子树 右子树 然后是根节点 // 那么最后一个是根节点，如果值小于根节点，那么就是该根结点的左子树；否则是该根节点的右子树，递归的进行判断 vector&lt;int&gt; seq; bool VerifySquenceOfBST(vector&lt;int&gt; sequence) &#123; if( sequence.empty()) return false; seq =sequence; return dfs(0, seq.size() -1); &#125; bool dfs(int l, int r) &#123; if(l &gt;= r) return true; int root =seq[r]; int k =l; while( k&lt; r &amp;&amp; seq[k] &lt; root) k++; for(int i =k; i&lt; r ; i++) if(seq[i] &lt; root) return false; return dfs(l, k -1) &amp;&amp; dfs(k, r-1); &#125;&#125;; 二叉树中和为某一值的路径** 输入一颗二叉树的跟节点和一个整数，打印出二叉树中结点值的和为输入整数的所有路径。路径定义为从树的根结点开始往下一直到叶结点所经过的结点形成一条路径。(注意: 在返回值的list中，数组长度大的数组靠前) Tips： 树的遍历，深度优先算法（dfs） 这个是非常典型的 dfs，是值得掌握的。 123456789101112131415161718192021222324252627# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: # 返回二维列表，内部每个列表表示找到的路径 # 深度优先 dfs() 这样的一个算法 def FindPath(self, root, expectNumber): # write code here if not root: return [] self.target = expectNumber paths = [] self.dfs(root, [root.val], paths) return pathsdef dfs(self, root, path, paths): if not root.left and not root.right and sum(path) == self.target: paths.append(path) if root.left: self.dfs(root.left, path + [root.left.val], paths) if root.right: self.dfs(root.right, path + [root.right.val], paths) c++ 写法1234567891011121314151617181920212223242526272829303132333435/*struct TreeNode &#123; int val; struct TreeNode *left; struct TreeNode *right; TreeNode(int x) : val(x), left(NULL), right(NULL) &#123; &#125;&#125;;*/class Solution &#123;public: vector&lt;vector&lt;int&gt;&gt; res; vector&lt;int&gt; path; // 深度优先的遍历 vector&lt;vector&lt;int&gt; &gt; FindPath(TreeNode* root,int expectNumber) &#123; if(! root) return res ; dfs(root, expectNumber); // 使用 sum -val 可以减少一个变量的使用 return res; &#125; void dfs(TreeNode * root, int sum) &#123; if(!root) return ; sum -= root-&gt; val; path.push_back(root-&gt; val); if( !root-&gt; left &amp;&amp; !root-&gt; right &amp;&amp; !sum) res.push_back(path); dfs(root-&gt; left, sum); dfs(root-&gt; right, sum); path.pop_back(); // 这个是 c++ 中 vector() 的操作 就是pop_back() 是没有其他的参数的 &#125;&#125;; 复杂链表的复制 （比较经典的） 输入一个复杂链表（每个节点中有节点值，以及两个指针，一个指向下一个节点，另一个特殊指针指向任意一个节点），返回结果为复制后复杂链表的head。（注意，输出结果中请不要返回参数中的节点引用，否则判题程序会直接返回空） Tips: 先是在原来的链表上进行了相同结点的copy和next 指针的指向，然后是random 指针的指向，最后是将原始链表和copy 的链表进行分离。 思路，是先复制节点和 next 指针，然后再遍历一遍复制 random 指针。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051# -*- coding:utf-8 -*-# class RandomListNode:# def __init__(self, x):# self.label = x# self.next = None# self.random = Noneclass Solution: # 返回 RandomListNode # 首先是结点的复制和 next 指针的连接， 然后是random 指针的连接，最后是选择出复制的结点def Clone(self, pHead): # write code here if not pHead: return None self.clone_nodes(pHead) self.connect_nodes(pHead) return self.select_nodes(pHead)def clone_nodes(self, head): if not head: return None while head: cloned = RandomListNode(head.label) cloned.next = head.next head.next = cloned head = cloned.nextdef connect_nodes(self, head): if not head: return None while head: cloned = head.next if head.random: cloned.random = head.random.next head = cloned.nextdef select_nodes(self, head): if not head: return None cloned =cloned_head =None # 这个if 的作用是为了保存一个 cloned_head的结点， # 一定要从这个功能出发 if head: cloned =cloned_head =head.next head.next =cloned.next head =head.next while head: cloned.next =head.next cloned =cloned.next head.next =cloned.next head =head.next return cloned_head 思路是完全正确的，但是不知道为什么在 牛客上，这个case 通过率是0.真的是太难了 123456789101112131415161718192021222324252627282930313233343536373839404142/*struct RandomListNode &#123; int label; struct RandomListNode *next, *random; RandomListNode(int x) : label(x), next(NULL), random(NULL) &#123; &#125;&#125;;*/class Solution &#123;public: // 分成三步骤，首先是复制结点插入到原来的链表中，然后处理random指针，最后是将链表挑选出来 RandomListNode* Clone(RandomListNode* pHead) &#123; // 加入一个node 是需要操作两个指针的 for( auto p=pHead; p;) &#123; auto np =new RandomListNode(p-&gt;label); auto next =p-&gt;next; p -&gt;next =np; np-&gt;next =next; p =next; &#125; // 处理random指针 for(auto p =pHead; p; p =p-&gt;next-&gt;next) &#123; if(p-&gt;random) p-&gt;next -&gt;random =p-&gt;random-&gt;next; &#125; auto dummy =new RandomListNode(-1); auto cur =dummy; for( auto p =pHead; p;p =p-&gt;next-&gt;next ) &#123; cur -&gt;next =p-&gt;next; cur =cur-&gt;next; //p =p-&gt;next; &#125; return dummy-&gt;next; &#125;&#125;; 二叉搜索树与双向链表 输入一棵二叉搜索树，将该二叉搜索树转换成一个排序的双向链表。要求不能创建任何新的结点，只能调整树中结点指针的指向。 Tips：中序遍历二叉搜索树就是一种排序的树的结点，然后树的左右指针可以作为链表中的指向使用。 12345678910111213141516171819202122232425262728293031323334# -*- coding:utf-8 -*-# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: # 使用的树的结构 表示一种双向链表 # 二叉搜索树 ，左子树小于根节点，右子树大于根节点 # 中序遍历得到就是一种排好序的结构 # 只能调整树中结点指针的指向 def Convert(self, pRootOfTree): # write code here if not pRootOfTree: return None tree = pRootOfTree res = [] self.helper(tree, res) for i in range(len(res) - 1): res[i].right = res[i + 1] res[i + 1].left = res[i] # 这个返回值也是比较鬼畜呀， 就是需要这样返回 return res[0]def helper(self, root, res): if not root: return None if root.left: self.helper(root.left, res) res.append(root) if root.right: self.helper(root.right, res) 这个代码长度有点多，所以之后再看。讲解链接 两个链表的第一个公共结点 输入两个链表，找出它们的第一个公共结点。 Tips：就是一个 m*n 的问题（m，n 分别代表两个链表的长度） 12345678910111213141516171819# -*- coding:utf-8 -*-# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: # 两个指针指向的是 一个结点，一个内存的两个指向 # 将可能不同长度的两个链表转换成相同长度的两个链表的比较，使用 def FindFirstCommonNode(self, pHead1, pHead2): # write code here if not pHead1 or not pHead2: return None p1 = pHead1 p2 = pHead2 while p1 != p2: # 这个p1 只能指向了最后一个结点，但最后一个节点不一定相同 p1 = pHead2 if not p1 else p1.next p2 = pHead1 if not p2 else p2.next return p1 该题目有一种比较巧妙的做法，如果一个指针走完之后，指向另一个list 的开头；同理，另一个指针也是可以这样进行操作。如果是可以相遇的，那么一定最后可以相遇，因为走过的路径是一样的长度的。 1234567891011121314151617181920212223242526/*struct ListNode &#123; int val; struct ListNode *next; ListNode(int x) : val(x), next(NULL) &#123; &#125;&#125;;*/class Solution &#123;public: ListNode* FindFirstCommonNode( ListNode* head1, ListNode* head2) &#123; auto p1 =head1, p2 =head2; while(p1 != p2) &#123; if(p1) p1 =p1-&gt;next; else p1 =head2; if(p2) p2 =p2-&gt;next; else p2 =head1; &#125; // 这个时候p1 和p2 已经相等了 return p1; &#125;&#125;; 二叉树的深度 输入一棵二叉树，求该树的深度。从根结点到叶结点依次经过的结点（含根、叶结点）形成树的一条路径，最长路径的长度为树的深度。 Tips：递归，相比于二叉树的路径，这个只是返回一个数值就行。 12345678910111213141516171819# -*- coding:utf-8 -*-# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: """ 分别求解 左右子树的深度，然后max(left, right) 这样的操作 """ def TreeDepth(self, pRoot): if not pRoot: return 0 left = self.TreeDepth(pRoot.left) + 1 right = self.TreeDepth(pRoot.right) + 1 # 这个return 是最后执行一次的，然后上面那个都是不断的在进行递归加深 # 这个 left right 已经完成了，最后的效果只是 返回 max(left, right) 这样子 return max(left, right) 一般二叉树的问题都是可以使用递归求解的，因为树本身就是使用递归进行定义的。思路：如果左右子树不为空，那么就返回左右子树深度 +1。这个就是递归的定义。 1234567891011121314151617/*struct TreeNode &#123; int val; struct TreeNode *left; struct TreeNode *right; TreeNode(int x) : val(x), left(NULL), right(NULL) &#123; &#125;&#125;;*/class Solution &#123;public: int TreeDepth(TreeNode* root) &#123; if(!root) return 0; return max(TreeDepth(root -&gt; left), TreeDepth(root-&gt; right)) +1; &#125;&#125;; 平衡二叉树 输入一棵二叉树，判断该二叉树是否是平衡二叉树。 Tips： 左右子树的深度差最大不超过1。两个递归，一个是计算树的深度的递归，一个是判断左右子树是否是平衡二叉树的递归。 对于平衡二叉树的第一反应，应该想到这个是二叉搜索树的一种改进。为了防止出现链表式的二叉搜索树，平衡二叉树限制了左右子树的相差的高度。通过调整左右子树，保持了二叉搜索树的性质。 直观上讲不是很偏（没有偏向一边） 123456789101112131415161718192021222324252627# -*- coding:utf-8 -*-# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: # 递归常见的都会有两个return 跳出条件，一个是异常的条件，一个是正确的返回 def get_depth(self, root): if not root: return 0 left =self.get_depth(root.left) right =self.get_depth(root.right) return max(left, right) +1 def IsBalanced_Solution(self, pRoot): if not pRoot: return True left =self.get_depth(pRoot.left) right =self.get_depth(pRoot.right) if abs(left-right) &gt;1: return False return self.IsBalanced_Solution(pRoot.left) and self.IsBalanced_Solution(pRoot.right) 在求解 树的深度的过程中，判断是否是平衡二叉树。 123456789101112131415161718192021class Solution &#123;public: // 本问题求解的过程中使用到了 求解树的深度代码 bool ans =true; bool IsBalanced_Solution(TreeNode* pRoot) &#123; dfs(pRoot); return ans; &#125; // 求解深度的过程 int dfs(TreeNode* root) &#123; if(! root) return 0; int left =dfs(root-&gt;left ), right =dfs(root-&gt; right); if(abs(left -right) &gt;1) ans =false; return max(left, right) +1; &#125; &#125;; 链表中环的入口结点 给一个链表，若其中包含环，请找出该链表的环的入口结点，否则，输出null。 Tips： 两个快慢指针，开指针在环内相遇慢指针。（两个指针一个需要再环外，一个在环内，然后同样的速度走，最后才能相遇）重置快指针到头结点，两个指针相同速度，当再次相遇时候，那就是入口结点。 123456789101112131415161718192021# -*- coding:utf-8 -*-# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: # 现在长个记性吧，在使用next 这样的时候 要先判断这个是不是存在的 def EntryNodeOfLoop(self, pHead): # write code here if not pHead or not pHead.next or not pHead.next.next: return None twoTimes =pHead.next.next oneTime =pHead.next while twoTimes != oneTime: twoTimes =twoTimes.next.next oneTime =oneTime.next twoTimes =pHead while twoTimes != oneTime: twoTimes =twoTimes.next oneTime =oneTime.next return twoTimes 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758#include&lt;iostream&gt;#include&lt;vector&gt;using namespace std;// c++ 中 单链表的定义struct ListNode&#123; int val; ListNode *next; ListNode(int x) :val(x), next(NULL)&#123;&#125; // 构造函数 &#125;;ListNode *entryNodeOfLoop(ListNode *head)&#123; auto i =head, j =head; while(i &amp;&amp; j) &#123; i =i-&gt;next; j =j -&gt;next; if(j ) j= j-&gt;next; if(i ==j) &#123; i =head; while(i != j) &#123; i =i-&gt;next; j =j-&gt;next; &#125; return i; &#125; &#125; return 0; &#125;int main()&#123; ListNode *head =new ListNode(-1); ListNode *p1 =new ListNode(1); ListNode *p2 =new ListNode(2); ListNode *p3 =new ListNode(3); ListNode *p4 =new ListNode(4); head -&gt;next =p1, p1-&gt;next =p2, p2-&gt;next =p3, p3-&gt;next =p4; p4-&gt;next =p1; //cout&lt;&lt; head-&gt;val&lt;&lt;endl; ListNode *p =entryNodeOfLoop(head); cout&lt;&lt; p-&gt;val&lt;&lt;endl; return 0;&#125; 删除链表中重复的结点 在一个排序的链表中，存在重复的结点，请删除该链表中重复的结点，重复的结点不保留，返回链表头指针。 例如，链表1-&gt;2-&gt;3-&gt;3-&gt;4-&gt;4-&gt;5 处理后为 1-&gt;2-&gt;5 12345678910111213141516171819202122232425262728# -*- coding:utf-8 -*-# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: def deleteDuplication(self, pHead): # write code here head = ListNode(-1) head.next = pHead curr = pHead last = head while curr and curr.next: # val =curr.val # 这个条件比较简单，所以可以放到前面 if curr.val != curr.next.val: curr = curr.next last = last.next else: # 这个条件 curr 还是需要注意一下的 val = curr.val # python 中 condition1 and condition2 这种是有先后顺序的 # 可能是存在短路现象的， 如果 curr 不成立，那么后面的是不会执行的 # 草拟 while curr and val == curr.val: curr = curr.next last.next = curr return head.next 凡是可能把头结点删掉的问题，一般来说我们都是会定义一个虚拟头结点。 123456789101112131415161718192021222324252627282930313233343536373839404142#include&lt;iostream&gt;#include&lt;vector&gt;using namespace std;struct ListNode&#123; int val; ListNode *next; ListNode(int x): val(x), next(NULL)&#123;&#125;&#125;;// 这个是排序之后的链表，那么可以把原来的list 看成一段一段， 使用p 和q 两个指针分别指向的两段的开头// 对于可能删除头结点的，一般使用虚拟结点，简化处理的情况ListNode * deleteDumplication(ListNode* head)&#123; auto dummy =new ListNode(-1); dummy-&gt;next =head; auto p =dummy; while(p-&gt;next) &#123; auto q =p-&gt;next; while(q &amp;&amp; p-&gt;next-&gt;val == q-&gt;val) q =q-&gt;next; if(p -&gt;next-&gt;next == q) p =p-&gt;next; else p-&gt;next =q; &#125; return dummy-&gt;next;&#125;int main()&#123; auto p1 =new ListNode(1); auto p2 =new ListNode(2); auto p3 =new ListNode(3); auto p22 =new ListNode(2); auto p33 =new ListNode(3); auto p4 =new ListNode(4); p1-&gt;next =p2,p2-&gt;next =p22, p22-&gt;next =p3, p3-&gt;next =p33, p33-&gt;next =p4; auto res =deleteDumplication(p1); cout&lt;&lt; res-&gt;val&lt;&lt;endl; return 0;&#125; 在 $O(1)时间删除链表结点$ 给定单向链表的一个节点指针，定义一个函数在 $O(1)$ 时间删除该结点。 题目视频讲解 一般删除一个节点需要知道其前驱结点，但是还有另外一种做法，就是通过下一个结点覆盖到本结点，然后删除下一个结点。 1234567891011121314151617/** * Definition for singly-linked list. * struct ListNode &#123; * int val; * ListNode *next; * ListNode(int x) : val(x), next(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: void deleteNode(ListNode* node) &#123; node-&gt;val =node-&gt;next-&gt;val; node-&gt;next =node-&gt;next-&gt;next; &#125;&#125;; 二叉树的下一个结点 给定一个二叉树和其中的一个结点，请找出中序遍历顺序的下一个结点并且返回。注意，树中的结点不仅包含左右子结点，同时包含指向父结点的指针。 Tips：中序遍历的下一个结点，如果存在右节点，那么下一个结点是右节点最左边的一个点；如果该结点是其父节点的左结点，那么下一节点是其父节点，否则一直回溯。 123456789101112131415161718192021222324252627282930# -*- coding:utf-8 -*-# class TreeLinkNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = None# self.next = Noneclass Solution: # https://blog.csdn.net/fuxuemingzhu/article/details/79723819 # 这个是求解中序遍历中某个结点的下一个结点 # 这pNode 就是一个普通的结点 def GetNext(self, pNode): # write code here if not pNode: return None # 如果存在右结点 if pNode.right: pNode = pNode.right while pNode.left: pNode = pNode.left return pNode # 如果是父节点的左子树 else: # 这里使用 pNode.next 表示父节点 while pNode.next: if pNode == pNode.next.left: return pNode.next # 这个是右结点 pNode = pNode.next return None 对称的二叉树 请实现一个函数，用来判断一颗二叉树是不是对称的。注意，如果一个二叉树同此二叉树的镜像是同样的，定义其为对称的。 Tips: 判断镜像和递归生成进行还是不太一样的哈。递归判断，根节点相同，然后左右子树是否是对称。 1234567891011121314151617181920212223242526272829# -*- coding:utf-8 -*-# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: # 镜像的概念 和递归 # isSame() 这个就是判断两个子树是否镜像的操作 def isSame(self, p, q): if not p and not q: return True # 好好思考 下面这两个跳出条件为什么是不合适的 if p and q and p.val == q.val: return self.isSame(p.left, q.right) and self.isSame(p.right, q.left) def isSymmetrical(self, pRoot): # write code here # 最开始的条件 如果都是 none 那么这个是对称的 if not pRoot: return True if pRoot.left and not pRoot.right: return False if not pRoot.left and pRoot.right: return False return self.isSame(pRoot.left, pRoot.right) 除了根结点，左右两个子树都是对称的。对称二叉树，非对称二叉树（权值不对称），非对称二叉树（结构不对称）。所以这个是对称或者镜像是有两个维度的，结构和内容。 c++ 实现 123456789101112131415161718192021222324252627282930/*struct TreeNode &#123; int val; struct TreeNode *left; struct TreeNode *right; TreeNode(int x) : val(x), left(NULL), right(NULL) &#123; &#125;&#125;;*/class Solution &#123;public: bool isSymmetrical(TreeNode* root) &#123; if(!root) return true; return dfs(root-&gt;left, root-&gt;right); &#125; // 在定义的时候，不要使用auto 了，使用具体的类型 bool dfs(TreeNode *p, TreeNode *q) &#123; // 如果有空的情况下，只有两者都为空，那么返回的是true，否则是false // 这种简洁的代码 就应该记住 if(!p || !q) return !p &amp;&amp; !q; if(p-&gt;val != q-&gt;val) return false; return dfs(p-&gt;left, q-&gt; right) &amp;&amp; dfs(p-&gt;right, q-&gt;left); &#125;&#125;; 按之字形顺序打印二叉树 对于二叉树的层序遍历有三种不同的题型。 不分行的层序遍历 不分行的层序遍历（偶数行是从左到右，奇数行是从右到左） 分行的层序遍历（每打印一行就另起一行） 请实现一个函数按照之字形打印二叉树，即第一行按照从左到右的顺序打印，第二层按照从右至左的顺序打印，第三行按照从左到右的顺序打印，其他行以此类推。 Tips：层序遍历的升级版，有两种思路，一种是使用单独 stack (list) 的思想存储偶数层数，一种是先按照原先层序遍历的思想，最后对于偶数的结果进行“翻转” 处理。选择后者，因为代码上比较简单。 1234567891011121314151617181920212223242526272829303132333435363738# -*- coding:utf-8 -*-# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: # 层序遍历 + 偶数翻转 # https://blog.csdn.net/fuxuemingzhu/article/details/79724959 def level(self, root, level, res): """ root: the root of tree level: res: result """ if not root: return if len(res) == level: res.append([]) res[level].append(root.val) if root.left: self.level(root.left, level + 1, res) if root.right: self.level(root.right, level + 1, res) def Print(self, pRoot): # write code here if not pRoot: return [] res = [] self.level(pRoot, 0, res) for level in range(1, len(res), 2): res[level] = res[level][::-1] return res 按照之字形顺序打印二叉树。 1234567891011121314151617181920212223242526272829303132333435363738394041424344/* struct TreeNode &#123; int val; struct TreeNode *left; struct TreeNode *right; TreeNode(int x) : val(x), left(NULL), right(NULL) &#123; &#125; &#125;; */class Solution &#123;public: vector&lt;vector&lt;int&gt; &gt; Print(TreeNode* pRoot) &#123; vector&lt;vector&lt;int&gt;&gt; res; if(! pRoot) return res; queue&lt;TreeNode*&gt; qu; qu.push(pRoot); qu.push(nullptr); vector&lt;int&gt; level; bool zigzag =false; //判断 queue 是否为空的条件 while(qu.size()) &#123; auto t =qu.front(); qu.pop(); if(! t) &#123; if(level.empty()) break; if(zigzag) reverse(level.begin(), level.end()); res.push_back(level); qu.push(nullptr); zigzag =!zigzag; // 这个取反的操作 level.clear(); //因为使用的是push back 操作，所以得clear() 操作 continue; &#125; level.push_back(t-&gt;val); if(t-&gt;left) qu.push(t-&gt;left); if(t-&gt;right) qu.push(t-&gt;right); &#125; return res; &#125;&#125;; 把二叉树打印成多行 从上到下按层打印二叉树，同一层结点从左至右输出。每一层输出一行。 Tips: 和上一个题目类似，在遍历二叉树的时候，关键是加入了 [level] 层数这种信息。 123456789101112131415161718192021222324252627282930# -*- coding:utf-8 -*-# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: # 返回二维列表[[1,2],[4,5]] def level(self, root, level, res): # 你这里也没有说要返回值的意思呀，这个直接是 return if not root: return if level == len(res): res.append([]) res[level].append(root.val) if root.left: self.level(root.left, level + 1, res) if root.right: # res[level] =self.level(root.right, level+1, res) # 因为这个是 传的值，所以不需要使用返回值的 self.level(root.right, level + 1, res) def Print(self, pRoot): if not pRoot: return [] res = [] self.level(pRoot, 0, res) return res Binary Tree Right Side View 是层序遍历，核心是理解使用 nullptr 作为一层结束的标志，所以当弹出的是nullptr 的时候，就应该去处理一层的遍历结果，不管是reverse 还是只是取最后一个。并且这个时候是需要加上一个nullptr，表示当前层的结束。 123456789101112131415161718192021222324252627282930313233343536373839404142/** * Definition for a binary tree node. * struct TreeNode &#123; * int val; * TreeNode *left; * TreeNode *right; * TreeNode(int x) : val(x), left(NULL), right(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: vector&lt;int&gt; rightSideView(TreeNode* root) &#123; vector &lt; int &gt; res; if (!root) return res; queue &lt; TreeNode* &gt; q; q.push(root); q.push(nullptr); vector &lt; int &gt; level; while (q.size()) &#123; auto t = q.front(); q.pop(); // 因为这里使用 nullptr 作为一层的结束，所以当前的t 为空的时候，最重要的是将 level.back() 加到 res中，然后加上 nullptr // 否则的话是不断地 push if (!t) &#123; if (level.empty()) break; res.emplace_back(level.back()); q.push(nullptr); level.clear(); continue; &#125; level.emplace_back(t-&gt;val); if (t-&gt;left) q.push(t-&gt;left); if (t-&gt;right) q.push(t-&gt;right); &#125; return res; &#125;&#125;; 序列化二叉树 请实现两个函数，分别用来序列化和反序列化二叉树 Tips：序列号和反序列化只是一种约定的存储的形式。 12345678910111213141516171819202122232425262728293031# -*- coding:utf-8 -*-# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: """ 序列化就是从树结构 转成字符串的结构；反之，也是成立的。 使用先序遍历的方法。 https://suixinblog.cn/2019/03/target-offer-serialize-binary-tree.html#%E4%BB%A3%E7%A0%81 """ def __init__(self): self.flag = -1 def Serialize(self, root): # write code here if not root: return "#" return str(root.val) + "," + self.Serialize(root.left) + "," + self.Serialize(root.right) def Deserialize(self, s): # write code here self.flag += 1 string = s.split(',') if self.flag &gt; len(string): return None root = None if string[self.flag] != '#': root = TreeNode(int(string[self.flag])) root.left = self.Deserialize(s) root.right = self.Deserialize(s) return root 序列化二叉树 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970/** * Definition for a binary tree node. * struct TreeNode &#123; * int val; * TreeNode *left; * TreeNode *right; * TreeNode(int x) : val(x), left(NULL), right(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: // 序列化的时候使用 #表示空节点，节点和结点之间使用 ,隔开 // Encodes a tree to a single string. string serialize(TreeNode* root) &#123; string res; dfs_s(root, res); return res; &#125; void dfs_s(TreeNode* root, string &amp; res) &#123; if(!root) &#123; res += "#,"; return ; &#125; res += to_string(root-&gt;val)+','; dfs_s(root-&gt;left, res); dfs_s(root-&gt;right, res); &#125; // Decodes your encoded data to tree. TreeNode* deserialize(string data) &#123; int u =0; return dfs_d(data, u); &#125; TreeNode* dfs_d(string &amp;data, int &amp;u) &#123; if(data[u] =='#') &#123; u +=2; return NULL; &#125; int t =0; bool is_minus =false; while(data[u] !=',') &#123; if(data[u] =='-') is_minus =true; else t =t*10 +data[u] -'0'; u ++; &#125; u ++; // 这个本身代表的含义是 data[u] ==','， 注意这种上下文 if(is_minus ) t =-t; auto root =new TreeNode(t); root-&gt; left =dfs_d(data, u); root-&gt;right =dfs_d(data, u); return root; &#125;&#125;; 二叉搜索树的第k个结点 给定一棵二叉搜索树，请找出其中的第k小的结点。例如， （5，3，7，2，4，6，8） 中，按结点数值大小顺序第三小结点的值为4。 Tips: 二叉搜索树，中序遍历之后有序，然后取第 k 个结点。 12345678910111213141516171819202122232425# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: def middle(self, root, result): if not root: return if root.left: self.middle(root.left, result) result.append(root) if root.right: self.middle(root.right, result) def KthNode(self, pRoot, k): # write code here if not pRoot: return result = [] self.middle(pRoot, result) if len(result) &lt; k or k &lt; 1: return return result[k - 1] 时间复杂度是 $O(K)$, 这个是最优的解法了。 12345678910111213141516171819202122232425262728293031/** * Definition for a binary tree node. * struct TreeNode &#123; * int val; * TreeNode *left; * TreeNode *right; * TreeNode(int x) : val(x), left(NULL), right(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: TreeNode* res; TreeNode* kthNode(TreeNode* root, int k) &#123; dfs(root, k); return res; &#125; // 当你传递唯一的k 的时候，一定要保证操作的是一个数字。 void dfs(TreeNode* root, int&amp; k) &#123; if(! root) return ;// 跳出条件 dfs(root-&gt; left, k); k --; if(!k) res =root; if(k &gt;0) dfs(root-&gt;right, k); &#125;&#125;;]]></content>
      <categories>
        <category>算法</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[tensorflow learning]]></title>
    <url>%2F2019%2F06%2F17%2Ftf-learning%2F</url>
    <content type="text"><![CDATA[linear_regression 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071import numpy as npimport matplotlib.pyplot as pltimport tensorflow as tfimport xlrd # 即xlrd是读excel，xlwt是写excel的库。import osfrom sklearn.utils import check_random_staten =50XX =np.arange(n)rs =check_random_state(0)YY =rs.randint(-10, 10, size=(n, ))+ 2.0* XXdata =np.stack([XX,YY], axis= 1) #沿着的纵轴num_epochs =50# 变量的初始化W =tf.Variable(0.0, name="weights")b =tf.Variable(0.0, name ="bias")# 定义数据 placeholddef inputs(): X =tf.placeholder(tf.float32, name ="X") Y =tf.placeholder(tf.float32, name ="Y") return X,Ydef inference(X): return X* W+bdef loss(X, Y): Y_predicted =inference(X) # square 是平方，平方差公式 return tf.reduce_sum(tf.squared_difference(Y, Y_predicted))(2.0* data.shape[0])# train 的过程就是 minimize loss 的过程def train(loss): learning_rate =0.0001 return tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)with tf.Session() as sess: sess.run(tf.global_variables_initializer()) X, Y =inputs() train_loss =loss(X, Y) train_op =train(train_loss) for epoch_num in range(num_epochs): loss_value, _ =sess.run([train_loss, train_op], feed_dict=&#123;X :data[:, 0], Y:data[:, 1]&#125;) print("epoch %d, loss =%f"%(epoch_num+1, loss_value)) # 已经 train之后，那么这个 run weights 和 bias 的意义 wcoeff, bias =sess.run([ W, b])# show resultsInput_values =data[:, 0]Labels =data[:, 1]Prediction_values =data[:, 0] *wcoeff +biasplt.plot(Input_values, Labels, 'ro', label ="main")plt.plot(Input_values, Prediction_values, label ="predicted")plt.legend()plt.show()plt.close() tensorflow 中图的概念 Tensorflow有图的概念，Operations会被添加到图中，作为图的节点。在添加某个Operation的时候，不会立即执行该Operation。Tensorflow会等所有Operation添加完毕，然后优化该计算图，以便决定如何执行计算。 而Tensor则是代码中的变量和常量。所有变量都需要在开始执行图计算之前进行初始化，通过调用tf.initialize_all_variables().run()来初始化所有变量。 使用Tensorflow，一般需要三个步骤： 创建Tensor； 添加Operations（Operations输入Tensor，然后输出另一个Tensor）； 执行计算（也就是运行一个可计算的图）。 Tensorflow的图必须在一个会话(Session)中来计算。Session提供了Operation执行和Tensor求值的环境 tensorflow 中name scope 和 variable scope 的区别 TensorFlow提供了通过变量名称（name）来创建或者获取一个变量的机制， 是通过加上前缀来进行变量的管理。 variable scope为了实现tensorflow中的变量共享机制：即为了使得在代码的任何部分可以使用某一个已经创建的变量，TF引入了变量共享机制，使得可以轻松的共享变量，而不用传一个变量的引用。 TensorFlow提供了两种创建变量的方法，一种是tf.Variable()，另一种是tf.get_variable()。 tf.get_variable()除了可以创建变量外，还能获取变量。 给出一个案例进行学习。 1234567891011121314151617import tensorflow as tfwith tf.name_scope('name_scope_test'): v1 = tf.get_variable('v', shape=[1], initializer=tf.constant_initializer(1.0)) v2 = tf.Variable(tf.constant(1.0, shape=[1]), name='v') v3 = tf.Variable(tf.constant(1.0, shape=[1]), name='v')with tf.Session() as sess: init_op = tf.global_variables_initializer() sess.run(init_op) print('the name of v1:', v1.name) print('the name of v2:', v2.name) print('the name of v3:', v3.name)#输出为#the name of v1: v:0#the name of v2: name_scope_test/v:0#the name of v3: name_scope_test/v_1:0 tf.ConfigProto() 创建session() 的时候，用来对 session() 进行参数配置。简单的举个例子. 123config = tf.ConfigProto(allow_soft_placement=True, allow_soft_placement=True)config.gpu_options.per_process_gpu_memory_fraction = 0.4 #占用40%显存sess = tf.Session(config=config) 常见的参数： 记录设备指派情况 : tf.ConfigProto(log_device_placement=True) 自动选择运行设备 ： tf.ConfigProto(allow_soft_placement=True) 限制GPU资源使用： 动态的申请 123config = tf.ConfigProto()config.gpu_options.allow_growth = Truesession = tf.Session(config=config) 限制GPU使用率 123config = tf.ConfigProto()config.gpu_options.per_process_gpu_memory_fraction = 0.4 #占用40%显存session = tf.Session(config=config) 或者 123gpu_options=tf.GPUOptions(per_process_gpu_memory_fraction=0.4)config=tf.ConfigProto(gpu_options=gpu_options)session = tf.Session(config=config) 设置使用哪块GPU 方法一： 在程序中设置 123os.environ['CUDA_VISIBLE_DEVICES'] = '0' #使用 GPU 0os.environ['CUDA_VISIBLE_DEVICES'] = '0,1' # 使用 GPU 0，1 方法二： 在 shell 脚本中设置 1CUDA_VISIBLE_DEVICES=0,1 python yourcode.py 保存和恢复 保存：创建 Saver（使用 tf.train.Saver()）来管理模型中的所有变量。 恢复: tf.train.Saver 对象不仅将变量保存到检查点文件中，还将恢复变量。请注意，当您恢复变量时，您不必事先将其初始化。 变量的种类和使用范围 TensorFlow 支持占位符placeholder。占位符并没有初始值，它只会分配必要的内存。在会话中，占位符可以使用 feed_dict 馈送数据。feed_dict是一个字典，在字典中需要给出每一个用到的占位符的取值。 那么，什么时候该用tf.placeholder，什么时候该使用tf.Variable之类直接定义参数呢？答案是，tf.Variable适合一些需要初始化或被训练而变化的权重或参数，而tf.placeholder适合通常不会改变的被训练的数据集。 源码也可以参考这里 logistic regression 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150import numpys as npimport matplotlib.pyplot as pltimport tensorflow as tfimport tempfileimport urllibimport pandas as pdimport osfrom tensorflow.examples.tutorials.mnist import input_data# tempfile 这个模块主要是用来创建临时文件和目录，用完后会自动删除，省的你自己去创建一个文件、使用这个文件、再删除这个过程了。max_num_checkpoint =10num_classes =2batch_size =512num_epochs =10# learning rateinitial_learning_rate =0.001learing_rate_decay_factor =0.95num_epochs_per_decay =1# statusis_training =Falsefine_tuning =Falseonline_test =Trueallow_soft_placement =Truelog_device_placement =False# Download and get MNIST dataset(available in tensorflow.contrib.learn.python.learn.datasets.mnist)# It checks and download MNIST if it's not already downloaded then extract it.# The 'reshape' is True by default to extract feature vectors but we set it to false to we get the original images.mnist = input_data.read_data_sets("MNIST_data/", reshape=True, one_hot=False)# data processing# 这种键值对还能写成这种形式，我草data =&#123;&#125;data["train/image"] =mnist.train.imagesdata["train/label"] =mnist.train.labelsdata["test/image"] =mnist.test.imagesdata["test/label"] =mnist.test.labelsdef extract_samples_Fn(data): index_list =[] for sample_index in range(data.shape[0]): label =data[sample_index] if label ==1 or label ==0: index_list.append(sample_index) return index_listindex_list_train =extract_samples_Fn(data["train/label"])index_list_test =extract_samples_Fn(data["test/label"])data["train/image"] =mnist.train.images[index_list_train]data["train/label"] =mnist.train.labels[index_list_train]data["test/image"] =mnist.test.images[index_list_test]data["test/label"] =mnist.test.labels[index_list_test]dimenionality_train =data["train/image"].shapenum_train_samples =dimenionality_train[0]num_features =dimenionality_train[1]graph =tf.Graph()with graph.as_default(): global_step =tf.Variable(0, name= ="global_step", trainable =False) # decay steps 是和 训练样本数量， batch size相关的 decay_steps = int(num_train_samples /batch_size * num_epochs_per_decay) learing_rate =tf.train.exponential_decay(initial_learning_rate, global_step, decay_steps, learing_rate_decay_factor, staircase=True, name="exponential_decay_learning_rate") # define placeholder image_place =tf.placeholder(tf.float32, shape=([None, num_features]), name= "image") label_place =tf.placeholder(tf.int32, shape=([None, ]), name = "gt") label_one_hot =tf.one_hot(label_place, depth= num_classes, axis= -1) # 这个是按照最后一个轴进行操作 dropout_param =tf.placeholder(tf.float32) # 我的理解这个不是一种常量吗， 为什么还要使用 placeholder # fully connnected logits =tf.contrib.layers.fully_connected(inputs =image_place, num_outputs =num_classes, scope ="fc") # define loss, with tf.name_scope("loss"): loss_tensor =tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits =logits, labels= label_one_hot)) # argmax(, 0) 返回行中的最大值的索引， 返回列中的最大索引 # tf.cast(predict_correct, tf.float32) 类型转换 prediction_correct =tf.equal(tf.argmax(logits, 1), tf.argmax(label_one_hot, 1)) accuracy =tf.reduce_mean(tf.cast(prediction_correct, tf.float32)) # optimizer, 上面是手动decay的，现在有了一个自动调整 learning rate 的工具 optimizer =tf.train.AdamOptimizer(learning_rate= learing_rate) with tf.name_scope("train_op"): # 加上了一个前缀 gradients_and_variables =optimizer.compute_gradients(loss_tensor) train_op =optimizer.apply_gradients(gradients_and_variables, global_step= global_step) # run the session session_conf =tf.ConfigProto( allow_soft_placement =allow_soft_placement, log_device_placement =log_device_placement, ) sess =tf.Session(graph= graph, config= session_conf) with sess.as_default(): saver =tf.train.Saver() sess.run(tf.global_variables_initializer()) checkpoint_prefix ="model" if fine_tuning: saver.restore(sess, os.path.join(checkpoint_path, checkpoint_prefix)) print(" Model restored for fine-tuning") test_accuracy =0 for epoch in range(num_epochs): # batch 的次数 是和 data 和 batch_size 相关的 total_batch_training =int(data["data/image"].shape[0] /batch_size) for batch_num in range(total_batch_training): start_idx =batch_num *batch_size end_idx =(batch_num +1) * batch_size train_batch_data, train_batch_label =data['train/image'][start_idx: end_idx], data["train/label"][start_idx: end_idx] batch_loss, _, training_step = sess.run([ loss_tensor, train_op, global_step], feed_dict=&#123;image_place: train_batch_data, label_place: train_batch_label, dropout_param: 0.5&#125;) # 使用占位符进行的输出 print("Epoch "+ str(epoch +1) +", Training loss =" + "&#123;:.5f&#125;".format(batch_loss)) # 每进行一个batch 那么是需要重新计算一下 acc的 test_accuracy = 100* sess.run(accuracy, feed_dict=&#123; image_place: data["test/image"], label_place: data["test/label"], dropout_param :1.&#125;) # 在进行test 的时候是不需要进行 dropout的 print(" Final Test Accuracy is %% %.2f" %test_accuracy) RNN 在 tensorflow 中的使用 学习单步执行的RNN ： RNNCell 每个RNNCell都有一个call方法，使用方式是：(output, next_state) = call(input, state)。借助图片来说可能更容易理解。假设我们有一个初始状态h0，还有输入x1，调用call(x1, h0)后就可以得到(output1, h1)： 再调用一次call(x2, h1)就可以得到(output2, h2)： 也就是说，每调用一次RNNCell的call方法，就相当于在时间上“推进了一步”，这就是RNNCell的基本功能。 除了call方法外，对于RNNCell，还有两个类属性比较重要： state_size output_size 前者是隐层的大小，后者是输出的大小。比如我们通常是将一个batch送入模型计算，设输入数据的形状为(batch_size, input_size)，那么计算时得到的隐层状态就是(batch_size, state_size)，输出就是(batch_size, output_size)。 一次执行多步： tf.nn.dynamic_rnn tf.nn.dynamic_rnn函数，使用该函数就相当于调用了n次call函数。即通过{h0,x1, x2, …., xn}直接得{h1,h2…,hn}。 1234# inputs: shape = (batch_size, time_steps, input_size) # cell: RNNCell# initial_state: shape = (batch_size, cell.state_size)。初始状态。一般可以取零矩阵outputs, state = tf.nn.dynamic_rnn(cell, inputs, initial_state=initial_state) 此时，得到的outputs就是time_steps步里所有的输出。它的形状为(batch_size, time_steps, cell.output_size)。state是最后一步的隐状态，它的形状为(batch_size, cell.state_size)。 tf.nn.dynamic_rnn 函数是tensorflow封装的用来实现递归神经网络（RNN）的函数。 1234567891011tf.nn.dynamic_rnn( cell, inputs, sequence_length=None, initial_state=None, dtype=None, parallel_iterations=None, swap_memory=False, time_major=False, scope=None) 重要参数介绍： （个人感觉这参数还是挺重要的，可以放任何东西）cell：LSTM、GRU等的记忆单元。cell参数代表一个LSTM或GRU的记忆单元，也就是一个cell。例如，cell = tf.nn.rnn_cell.LSTMCell((num_units)，其中，num_units表示rnn cell中神经元个数，也就是下文的cell.output_size。返回一个LSTM或GRU cell，作为参数传入。 inputs：输入的训练或测试数据，一般格式为[batch_size, max_time, embed_size]，其中batch_size是输入的这批数据的数量，max_time就是这批数据中序列的最长长度，embed_size表示嵌入的词向量的维度。 sequence_length：是一个list，假设你输入了三句话，且三句话的长度分别是5,10,25,那么sequence_length=[5,10,25]。 如果有更多的时间，那么是可以学习一下 charRNN 这个模型，还是比较有意思的。 RNN 的一个例子1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586import tensorflow as tfimport numpy as npimport matplotlib.pyplot as pltimport argparse# argparse 解析命令行参数learing_rate =0.001seed =111# trainingbatch_size =128num_epoch =10hidden_size =128# 这个是很好的方式，可以用来 重现实验结果的# Reset the graph set the random numbers to be the same using "seed"tf.reset_default_graph()tf.set_random_seed(seed)np.random.seed(seed)# divde 28*28 images to rows of data to feed to RNN as sequantial informationstep_size= 28input_size =28output_size =10X =tf.placeholder(tf.float32, [None, step_size, input_size])y =tf.placeholder(tf.int32, [None])# RNNcell =tf.nn.rnn_cell.BasicRNNCell( num_units= hidden_size)output, state =tf.nn.dynamic_rnn(cell, X, dtype =tf.float32)# dynamic_rnn(cell, inputs)# state 表示最后一层的隐状态，output 这个还是有点模糊的# forward pass and loss calculationlogits = tf.layers.dense(state, output_size)# 只有在实现的时候，才能切身的感受到 loss function 是什么cross_entropy =tf.nn.sparse_softmax_cross_entropy_with_logits(labels= y, logits =logits)loss =tf.reduce_mean(cross_entropy)optimizer =tf.train.AdamOptimizer(learning_rate=learing_rate).minimize(loss)# Prediction,prediction =tf.nn.in_top_k(logits, y, 1) # 这个语句感觉是说不通的accuracy =tf.reduce_mean(tf.cast(prediction, tf.float32))# input datafrom tensorflow.examples.tutorials.mnist import input_datamnist =input_data.read_data_sets("MNIST_data/")# process MNISTX_test =mnist.test.imagesX_test =X_test.reshape([-1, step_size, input_size])y_test =mnist.test.labelsinit =tf.global_variables_initializer()loss_train_list =[]acc_train_list =[]with tf.Session() as sess: sess.run(init) n_batches =mnist.train.num_examples / batch_size for epoch in range(num_epoch): # 迭代的次数 for batch in range(n_batches): # 这个是一个 batch的东西 X_train, y_train =mnist.train.next_batch(batch_size) X_train =X_train.reshape([-1, step_size, input_size]) # 第一个参数有时候还是不太一样的， 并不总是 loss sess.run(optimizer, feed_dict=&#123;X: X_train, y: y_train&#125;) loss_train, acc_train = sess.run([loss, accuracy],feed_dict=&#123;X: X_train, y: y_train&#125;) loss_train_list.append(loss_train) acc_train_list.append(acc_train) # 每一个 epoch 之后是进行输出的 print("Epoch :&#123;&#125;, Train loss :&#123;:.3f&#125;, Train Acc &#123;:.3f&#125;".format(epoch+1, loss_train, acc_train)) loss_test, acc_test =sess.run([loss, acc_test], feed_dict=&#123;X: X_test, y: y_test&#125;) print("Test Loss: &#123;:.3f&#125;, Test ACC: &#123;:.3f&#125;".format(loss_test, acc_test))]]></content>
      <categories>
        <category>Deep learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[注意力机制介绍]]></title>
    <url>%2F2019%2F06%2F16%2Fattention%2F</url>
    <content type="text"><![CDATA[介绍attention 机制的原理、计算流程、分类和代码实现。 Attention 机制的本质是人眼的视觉观察，当观察一个物体的时候是有重点的观察某个位置，对信息重点关注并学习的技术。在数学中中的表现形式就是加权平均和。attention机制在机器翻译、语音识别、图像标注（Image Caption）和文本摘要等领域十分流行，在序列模型是非常有效的手段。 （1）来源 很多介绍Attention机制的时候都要从，Sequence to Sequence 模型说起，这个不是没有原因的。Attention 机制的提出（不是提出，而是受到广泛的关注）就是为了解决Sequence to Sequence中的问题，准确是是为了解决机器翻译中Sequence to Sequence的问题。 经典的机器翻译的模型是使用一个Encoder + Decoder的结构，将一种语言比如说函数压缩成一个固定长度的向量，称为隐藏层，然后使用一个Decoder将隐藏层的信息映射到另外一种语言，比如英语。其中的Encoder 和Decoder 一般是基于LSTM 或者GRE等RNN的网络结构，在进行机器翻译的时候有两处缺陷：1). 源句子信息必须能够压缩到一个固定长度的向量中；2). 翻译成目标句子时候，源句子中的每个token 的权重是相同的。 对于短文本来说，上述模型是没有问题的，但是对于长文本来说，很有可能固定长度的向量表示无法有效得得到句子的特征表示。所以这个时候提出了（借用）了Attention机制。 （2）Attention 机制 在机器翻译中，Attention用于关联输出序列中每个单词与输入序列中某个特定单词的关联程度。使用attention机制还有一个好处：对齐（将原文的片段和对应的译文片段进行匹配）。当然也并不是盲目的将输出的第一个单词和输入的第一个词对齐，权重关系是学习到的。 Seq2Seq直接把最后一个时序$i$的输出 $h_i$作为上下文，作为Decoder的全部的输入。而使用了Attention机制的模型中，上下文向量（隐藏层向量）包含了各个时序输出的权重信息，也就是对于当前生成的文字，在源句子中哪部分是重要的，哪些部分是不重要的。 Attention的计算流程： 1). 准备隐藏状态2). 得到每个隐藏状态和解码器之间的score 分数（点积只是其中一种）3). 将所有的分数经过softmax 归一化4). 每一个编码器隐藏状态和上面的分数相乘5). 将对齐之后的向量相加，产生上下文向量6). 将上下文向量送到解码器中 使用动画的形式完整展示上述描述的过程： 上述过程还可以用query，key 和value进行表示： 用数学表示为： \begin{equation}a_{i}=\operatorname{soft} \max \left(f\left(Q, K_{i}\right)\right)=\frac{\exp \left(f\left(Q, K_{i}\right)\right)}{\sum_{j} \exp \left(f\left(Q, K_{j}\right)\right)}\end{equation}其中函数 $f$可以有以下几种选择 $$f(Q, K_i) = \begin{cases}Q^T K_i &amp; dot\\Q^T WK_i&amp; general \\W[Q; K_i] &amp; concat\end{cases}$$ 最后得到Attention如下所示： \begin{equation}\text { Attention }(Q, K, V)=\sum_{i} a_{i} V_{i}\end{equation} 优点：1). 当前词和全局联系 2). 并行，Attention计算不依赖上一步的结果 缺点：不能捕捉语序，是一个词和周围词关系的模型，当然是可以通过位置向量（position embedding）处理 （3）常见的Attention机制的分类 1). Soft Attention &amp; Hard Attention：上述的经典模型也被称为soft Attention，因为每个输入词的隐藏层$h_i$都参与了最后权重的计算，这样方便梯度的反向传播。对应的是 hard Attention，该类型是说在输入中中澳大某个特定的词，其权重为1，其他都是0，这种方法比较粗暴，同时因为输入和输出的一一对应关系难度很大，所以在训练的时候非常困难，需要许多技巧，所以在NLP 中不是很常见，但在图像处理中，hard attention是比较有用的。 2). Global Attention &amp; Local Attention：这两者的区别在于是否所有的Encoder 的隐状态（$h_i$）都参与了计算，如果是，那么就是Global Attention，否则就是Local Attention。 3). Self-Attention：传统的attention是基于源句的隐变量和目标句子之间计算attention，得到的结果是源句子的每个词和目标句子中每个词之间的依赖关系。但是self attention是指在一端（源句子或者目标句子）进行，得到的是自身相关的attention，捕捉的是源句子或者目标句子自身词与词之间的依赖关系，然后该依赖关系和源句子或者目标句子进行相乘，得到一端句子的依赖关系。 代码时间部分 pytorch 实现一个具有self attention 翻译模型，机器翻译的模型或者序列标注的模型 参考文献 Sequence to Sequence Learning with Neural NetworksLearning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation动图逐步讲解]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>attention</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于BERT改进的几篇论文]]></title>
    <url>%2F2019%2F06%2F16%2Fimproved_bert%2F</url>
    <content type="text"><![CDATA[介绍基于BERT进行改进的模型：ALBERT、XLNet, DistilBERT （持续更新） ALBERT在同等网络规模（隐藏层）下，BERT 拥有 12.7 亿个参数，ALBERT 仅拥有 5900 万个参数……缩小约 21.5 倍。 模型训练存在一个饱和点，一旦训练复杂性越过该饱和点，那么任何额外网络元素的加入都只会拉低增益。 对Embedding因式分解（Factorized embedding parameterization）ALBERT 的作者们指出，WordPiece 嵌入学习到的实际是与上下文无关的表示形式。而隐藏层嵌入则用于学习与上下文相关的表示形式。这两者是需要没有直接的关系，如果把这两个要素分开，可能更加合理。 投射至一个较小的低维矩阵 E 中，而后再将 E 投射至 H 隐藏空间中。 这一步操作其实就是没啥特别好说的，无外乎就是觉得词嵌入时的向量维度应该小一点，然后通过个网络扩充一下提升维度，这样一通操作就可以把参数量从$O(V×H)) $降到了$O(V×E+E×H)$ 跨层的参数共享（Cross-layer parameter sharing）全连接层与attention层都进行参数共享，也就是说共享encoder内的所有参数 参数量减少了很多，训练速度也提升了很多 句间连贯（sentence-order prediction）在ALBERT中，为了只保留一致性任务去除主题识别的影响，提出了一个新的任务 sentence-order prediction（SOP），SOP的正样本和NSP的获取方式是一样的，负样本把正样本的顺序反转即可。 SOP因为实在同一个文档中选的，其只关注句子的顺序并没有主题方面的影响。并且SOP能解决NSP的任务，但是NSP并不能解决SOP的任务，该任务的添加给最终的结果提升了一个点。 增强了模型的学习能力。 移除dropout除了上面提到的三个主要优化点，ALBERT的作者还发现一个很有意思的点，ALBERT在训练了100w步之后，模型依旧没有过拟合，于是乎作者果断移除了dropout，没想到对下游任务的效果竟然有一定的提升。这也是业界第一次发现dropout对大规模的预训练模型会造成负面影响。 总结但实际上是通过参数共享的方式降低了内存，预测阶段还是需要和BERT一样的时间。 albert_zh XLNet 讲解（1）摘要（Abstract） However, relying on corrupt- ing the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy针对BERT pretrain-finetune discrepancy 的问题，主要做了以下三点工作： enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order overcomes the limitations of BERT thanks to its autoregressive formulation XLNet integrates ideas from Transformer-XL into pretraining.其中前两点是比较重要，最后一点是在长句子数据集上预训练的改进。 （2）Introduction 自回归语言模型（Autoregressive Language Modeling, AR）和自编码语言模型（Autoencoding LM, AE）是两大成功的语言模型。自回归语言模型有个缺点，要么从左到右，要么从右到左，尽管可以类似ELMO两个都做，然后再拼接的方式。但是跟Bert比，效果明显不足够好（这里面有RNN弱于Transformer的因素，也有双向语言模型怎么做的因素）。\begin{equation}\begin{array}{l}{p(x)=\prod_{T}^{T} p\left(x_{t} | x_{&lt;t}\right)} \\ {p(x)=\prod^{T} p\left(x_{t} | x_{&gt;t}\right)}\end{array}\end{equation}1). 典型技术：GPT, ELMO（本质上是两方向的自回归简单拼接结合）。2). 优点：和下游NLP任务有关，比如生成类NLP任务，文本摘要，机器翻译等，在实际生成内容的时候，都是从左向右，自回归语言模型天然匹配这个过程。而自编码语言模型和这个过程不匹配。3). 缺点：只能利用上文或者下文的信息，或者像ELMO简单两方向结合。 同样，自编码语言模型定义：自编码语言模型是通过在输入X中随机Mask掉一部分单词，然后在预训练过程中根据上下文单词来预测这些被Mask掉的单词。该做法类似Denoising Autoencoder(DAE)的思路。那些被Mask掉的单词即为在输入侧加入的噪音。1). 典型技术：BERT。2). 优点：同时利用上下文信息，双向语言模型。3).缺点： 输入侧引入[Mask]标记，导致预训练阶段和Fine-tuning阶段不一致，因为Fine-tuning阶段看不到[Mask]标记。 独立假设问题：举个例子：比如”New York is a city”，假设我们Mask住”New”和”York”两个词，那么给定”is a city”的条件下”New”和”York”并不独立，因为”New York”是一个实体，看到”New”则后面出现”York”的概率要比看到”Old”后面出现”York”概率要大得多。所以同时Mask 15\% 的词不是好的做法。 于是作者的想法是结合 AR 模型的优点（同下游任务一致）和AE的优点（利用上下文信息），提出了XLNet ：基于AR模型使用 all possible permutations of the factorization order 来学习上下文信息。并且结合了当下AR 模型中很多训练技巧，reparameterize the Transformer(-XL) network。 一句话概括：XLNet 使用语言模型，但是为了解决双向上下文的问题，提出了排列语言模型。排列语言模型在预测的时候需要target 的位置信息，因此引入了 Two-Stream，Content 流编码到当前时刻的所有内容，而Query 流只是保留了位置信息。最后为了解决计算量过大的问题，对于一个句子，只是预测后面的 $\frac{1}{K}$的词语。 （2）排列语言模型（Permutation Language Model） 排列（乱序）语言模型跟语言模型一样，都是做条件概率分解，但排列语言模型的分解顺序是随机的： \begin{equation}\begin{aligned} p\left(x_{1}, x_{2}, x_{3}, \ldots, x_{n}\right) &amp;=p\left(x_{1}\right) p\left(x_{2} | x_{1}\right) p\left(x_{3} | x_{1}, x_{2}\right) \ldots p\left(x_{n} | x_{1}, \ldots, x_{n}\right) \\ &amp;=p\left(x_{3}\right) p\left(x_{1} | x_{3}\right) p\left(x_{2} | x_{1}, x_{3}\right) \ldots p\left(x_{n} | x_{1}, \ldots, x_{n}\right) \\ &amp;=\ldots \\ &amp;=p\left(x_{n-1}\right) p\left(x_{1} | x_{n-1}\right) p\left(x_{n} | x_{n-1}, x_{1}\right) \ldots p\left(x_{2} | x_{n-1}, x_{1}, \ldots, x_{3}\right) \end{aligned}\end{equation} 关于如何使用 attention机制实现 masked（当预测$t$时刻的词，无法看到下一个时刻的词）效果，可以看这里中乱序语言模型的解释。 XLNet仍然遵循两阶段的过程，第一个阶段是语言模型预训练阶段；第二阶段是任务数据Fine-tuning阶段。这个可以理解为在自回归LM模式下，采取什么样的具体手段，融入双向语言模型。 \begin{equation}\max _{\theta} E _{z \sim Z_T} = \sum_{t=1} ^{T} \log p_{\theta} (x_{z_t} | x_{z \le t})\label{eq-1}\end{equation}\begin{equation}\max _{\theta} E _{z \sim Z_T} = \sum_{t=1} ^{T} \log p_{\theta} (x_{z_t} | x_{z \le t})\tag{1.1}\end{equation} 从所有的排列中采样一种，然后根据这个排列来分解联合概率成条件概率的乘积，然后加起来。为了实现排列语言模型，论文采用了双流自注意力机制（Two-Stream Self-Attention）和Attention Mask。比如 $p(x_1| x_3)p(x_2|x_1 x_3)p(vx_3)$，我们可以在用Transformer编码 $x_1 $时候让它可以Attend to $x_3$，而把 $x_2$Mask掉 (意思就是算Attention score的时候只算 $x_3$对 $x_1$的影响，而 $x_2$的attention score为0)；编码 $x_3$的时候把 $x_1$, $x_2$都Mask掉。 Intuitively, if model parameters are shared across all factorization orders, in expectation, the model will learn to gather information from all positions on both sides.这个有没有类似权值共享的观点，然后权值共享能够基本表示所有的可能性。 当时上述公式$(1.1)$有个问题：没有显性表示出$t$到底是什么。所以表示成以下的公式： p_{\theta}\left(X_{z_{t}}=x | \mathbf{x}_{z_{&lt;t}}\right)=\frac{\exp \left(e(x) ^{\top} g_{\theta}\left(\mathbf{x}{\mathbf{z}&lt;t}, z{t}\right)\right)}{\sum_{x^{\prime}} \exp \left(e\left(x^{\prime}\right)^{\top} g_{\theta}\left(\mathbf{x}{\mathbf{z}{&lt;t}}, z_{t}\right)\right)} 在公式中显示表示出了 $z_t$。但是传统的transformer 第 $t$时候embedding 是既包含内容信息也包含位置信息的。所以作者提出了双流注意力机制。 （3）双流注意力机制 问题：当不同的排列组合如 $z_1 =[1, 3, 4, 2]$ 和$z_2 =[1, 3, 4, 5]$出现，$t =3$，则已知前三个状态，预测第四个状态，由于这两种排列当 $t \le 3$ 都是相同的，而 $t =4$的时候，信息是被 masked的，所以无法区别这两种信息。 为了解决上述问题，论文中引入了两个 Stream，也就是两个隐状态： 内容隐状态 $h_ \theta (X_{ z &lt; t})$，简写为 $h_{z_t}$，既可编码上下文位置也可以编码内容； 查询隐状态 $g_{\theta}\left(x_{z_{&lt;t}}, z_{t}\right)$， 简写为$g_{z_t}$，只是编码位置信息。 ` 所以在预测当前 $t$位置词语时候，$g_{\theta}\left(\mathbf{x}_{z_{&lt;t}}, z_{t}\right)$只能使用位置信息$z_t$而不能使用 $x_{z_t}$；为了预测$z_t$之后的词语，$g_{\theta}\left(\mathbf{x}_{z_{&lt;t}}, z_{t}\right)$ 必须编码了$x_{z_t}$ 的信息（语义）。 so during finetuning, we can simply drop the query stream and use the content stream as a normal Transformer(-XL).在predict的过程中，使用 content stream就可以。 （3）部分预测 排列语言模型排列很多，所以它的计算量很大，很难优化。因此我们只预测一个句子后面的一些词，为什么不预测前面的词呢？因为前面的词的上下文比较少，上下文信息相对较少。比如句子”I like New York”。预测I的时候没有任何上下文，因此可能的选择很多。而到最后一个词York的时候，如果New已经知道了，那么York的概率就非常大了。 只是对句子后面的token进行预测，节省计算时间。\begin{equation}\frac{|z|-c}{|z|}=\frac{1}{K}\end{equation} 可以得出 $K \approx \frac{|z|-c}{|z|}$ ,所以前 $c$个不用预测的token。从下图中也是可以发现：一个句子后面的词语相对于前面的词语在训练的时候可以得到更多的信息。 （4）融入Transformer-XL的思想 引入了Transformer-XL的主要思路：相对位置编码以及分段RNN机制。transformer要求输入是定长的词序列，太长的需要截断，不足的需要padding。这样的操作存在以下问题： 由于定长的要求，我们不可能让输入太长。因此虽然Self-Attention机制虽然不太受长度的约束，但是Transformer的语言模型实际能够考虑的上下文就是输入的长度。 因为语言模型是自回归的，一步错步步错。这就是所谓的context fragmentation的问题。 1). 建模多个segment 我们会把之前一个固定长度的词序列每一层的输出都放到一个cache里，比如把𝑥1,…,𝑥4的计算结果都存起来，那么在训练第二个Segment[𝑥5,…,𝑥8]的时候就可以让Self-Attention机制参考[𝑥1,…,𝑥4]的信息了。当然在反向计算梯度的时候，cache里的内容是不会参与梯度的计算的。 2). 相对Segment编码 BERT使用的是绝对的Segment编码，也就是第一个句子对于的Segment id是0，而第二个句子是1。Transformer-XL使用到了上一个 segment的信息，如果使用绝对编码，那么无法通过向量判断它是来自当前segment 中第$i$位置还是前一个segment 中第$i$位置。所以Transformer-XL 使用的是相对位置编码。 XLNet 效果好的原因： 与Bert采取De-noising Autoencoder方式不同的新的预训练目标：Permutation Language Model(简称PLM)；这个可以理解为在自回归LM模式下，如何采取具体手段，来融入双向语言模型。这个是XLNet在模型角度比较大的贡献，确实也打开了NLP中两阶段模式潮流的一个新思路。 引入了Transformer-XL的主要思路：相对位置编码以及分段RNN机制。实践已经证明这两点对于长文档任务是很有帮助的。 加大增加了预训练阶段使用的数据规模；Bert使用的预训练数据是BooksCorpus和英文Wiki数据，大小13G。XLNet除了使用这些数据外，另外引入了Giga5，ClueWeb以及Common Crawl数据，并排掉了其中的一些低质量数据，大小分别是16G,19G和78G。可以看出，在预训练阶段极大扩充了数据规模，并对质量进行了筛选过滤。这个明显走的是GPT2.0的路线。 但是和BERT不同，我们并没有增加一个预测下一个句子的Task，原因是通过实验分析这个Task加进去后并不是总有帮助。【注：其实很多做法都是某些作者的经验，后面很多作者一看某个模型好，那么所有的Follow，其实也不见得就一定好。有的时候可能只是对某个数据集有效果，或者效果好是其它因素带来的，一篇文章修改了5个因素，其实可能只是某一两个因素是真正带来提高的地方，其它3个因素可能并不有用甚至还是有少量副作用。】 UNILM参考文献 1). XLNet: Generalized Autoregressive Pretraining for Language Understanding2). XLNet原理3). 从语言模型到Seq2Seq：Transformer如戏，全靠Mask DistilBERT 讲解]]></content>
      <categories>
        <category>NLP</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[逻辑回归概念]]></title>
    <url>%2F2019%2F06%2F09%2Flr%2F</url>
    <content type="text"><![CDATA[本文主要介绍逻辑回归（logistics regression）和决策树（Decision Tree）。逻辑回归从线性回归出发到逻辑回归，然后手推公式和相关的一些特点；介绍一下决策树的特点。 逻辑回归逻辑回归是线性模型，虽然叫做”回归“，究其原因 逻辑回归从线性回归引申而来，对回归的结果进行 logistic 函数运算，将范围限制在[0,1]区间，并更改损失函数为二值交叉熵损失，使其可用于2分类问题(通过得到的概率值与阈值比较进行分类)。逻辑回归是广义上的线性模型，然后最后的sigmoid 加入了非线性。是处理线性问题的。 公式推导从线性回归问题到逻辑回归过程的推导。 线性二分模型： $$f ( x ) = \theta ^ { T } x$$ 逻辑回归决策函数是将此线性二分类嵌套一个sigmoid函数： $$ f ( x ) = \frac { 1 } { 1 + e ^ { - \theta ^ { T } x } }$$ 损失函数：如果用平方误差（MSE）作为逻辑回归的损失函数,那么函数曲线将是跳跃式的,非凸的(non-convex),原因是logistic函数将数据范围限制在[0,1]区间,而真实标签值非0即1.最小化 MSE 损失容易陷入局部极小点.逻辑回归损失是如下的分情况的凸函数(单个x与y的损失)。 $$P ( y = 1 | x ; \theta ) = h _ { \theta } ( x )$$ $$P ( y = 0 | x ; \theta ) = 1 - h _ { \theta } ( x )$$最初是上述的分段函数，合并成下面的函数，方便计算。$$p ( y | x ; \theta ) = \left( h _ { \theta } ( x ) \right) ^ { y } \left( 1 - h _ { \theta } ( x ) \right) ^ { 1 - y }$$使用最大似然的思想求解。假设我们有n个独立的训练样本{(x1, y1) ,(x2, y2),…, (xn, yn)}，y={0, 1}。那每一个观察到的样本(xi, yi)出现的概率是： 上述似然函数乘法太难算了，然后使用log 将其改为加法，变成了对数似然函数。$$J( \theta ) = \log ( L ( \theta ) ) = \sum _ { i = 1 } ^ { m } y ^ { ( i ) } \log \left( h \left( x ^ { ( i ) } \right) \right) + \left( 1 - y ^ { ( i ) } \right) \log \left( 1 - h \left( x ^ { ( i ) } \right) \right)$$ 求导优化问题sigmoid 函数的特殊性质：$$\sigma ^ { \prime } ( x ) = \sigma ( x ) ( 1 - \sigma ( x ) )$$ 分成三部分求导： 用L(θ)对θ求导，得到：$$\begin{split}\frac { d } { d \theta _ { i } } \operatorname { loss } ( \theta ) &amp;= \left( y \frac { 1 } { \sigma \left( \theta ^ { T } x \right) } - ( 1 - y ) \frac { 1 } { 1 - \sigma \left( \theta ^ { T } x \right) } \right) \frac { d } { d \theta _ { i } } \sigma \left( \theta ^ { T } x \right) \\&amp;= \left( y \frac { 1 } { \sigma \left( \theta ^ { T } x \right) } - ( 1 - y ) \frac { 1 } { 1 - \sigma \left( \theta ^ { T } x \right) } \right) \sigma \left( \theta ^ { T } x \right) \left( 1 - \sigma \left( \theta ^ { T } x \right) \right) \frac { d } { d \theta _ { i } } \theta ^ { T } x \\&amp;= \left( y \left( 1 - \sigma \left( \theta ^ { T } x \right) \right) - ( 1 - y ) \sigma \left( \theta ^ { T } x \right) \right) x _ { i } \\&amp;= \left( y - h _ { \theta } ( x ) \right) x _ { i }\end{split}$$ 注意一会儿有 $\sum$ 一会儿没有的，其实我们更倾向于不用，采用矩阵相乘的方式更加简洁。只是在表达似然函数，使用$ \sum$更加直观$$\theta _ { i } : = \theta _ { j } + \alpha \left( y ^ { ( i ) } - h _ { \theta } \left( x ^ { ( i ) } \right) \right) x _ { j } ^ { ( i ) }$$ 为什么使用 logistics function一种解释是可以。 sigmoid 函数连续，单调递增 对 sigmoid 函数求导非常的方便 可以将负无穷到正无穷的数值映射到 [0, 1] 这样的区间 另外一种维度是从伯努利分布的角度去解释。 $$f(x ; p)=p^{x}(1-p)^{1-x} \quad x=0,1$$其中 $p$ 是成功的概率。如果进行一次投掷，那么均值是 $p$ 方差是 $p (1-p)$。 然后再看二分类任务，样本的标签$y$ 服从二项分布。 $$p ( y | x ; \theta ) = \left( h _ { \theta } ( x ) \right) ^ { y } \left( 1 - h _ { \theta } ( x ) \right) ^ { 1 - y }$$详细的推导可以看这里： [img]https://ftp.bmp.ovh/imgs/2019/09/39e481f9fd958d4b.png[/img] 逻辑回归的特点 优点： LR 能以概率的形式输出结果,而非只是 0,1 判定， 可以做 ranking model； LR 的可解释性强,可控度高； 训练快 缺点： 容易欠拟合，一般准确度不太高 只能处理两分类问题. (可以应用多个逻辑回归实现多分类,类似SVM的方式; 另外对于父子类别同时分类的情况,使用逻辑回归要比Softmax等方式效果好) “海量离散特征+简单模型” 同“少量连续特征+复杂模型”的权衡。既可以离散化用线性模型，也可以用连续特征加深度学习 为什么对特征进行离散化 特征从连续变量状态到离散化的初衷在于我们认为不同的区间对于最后的结果的重要性是不同的。同样在工业界，很少直接将连续值作为逻辑回归模型的特征输入，而是将连续特征离散化为一系列0、1特征(one-hot编码)交给逻辑回归模型，这样做的优势有以下几点 离散特征的增加和减少都很容易，易于模型的快速迭代； 稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展； 离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄&gt;30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰； 单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合 离散化后可以进行特征交叉 究其原因，使用 LR+离散模型在于可控可解释。而GBDT 直接使用连续的变量，一方面的原因在于如果特征过多，那么GBDT 是跑不动的。 为什么LR模型的损失函数是交叉熵,而线性回归模型的损失函数却是最小二乘呢？能否随意确定一个损失函数作为目标呢？ 模型的损失函数由各自的响应变量y的概率分布决定，对于线性回归模型，其输出是连续值，所以我们对于该问题假设y服从正态分布；相对的，LR模型一般用来解决二分类问题，所以其输出是0/1，故而我们假设其输出服从伯努利分布；而进一步地，两者的损失函数都是通过极大似然估计推导的来的，所以模型的损失函数并非随意确定。分类模型与回归模型之间有种种联系,比如 SVM 模型可以看作逻辑回归加L2正则项, 并使用了不同的损失函数. 为什么不使用回归模型来做分类?这是一种不好的做法, 因为阈值不好确定, 随着数据集的变动, 阈值也需要有较大变化. 正则项 L2 解决过拟合 L1 解决数据稀疏性 L1和L2正则先验分别服从什么分布从信息论的角度看，向系统加入了正确先验这个信息，肯定会提高系统的性能。两者的差别感性的理解？L1是拉普拉斯分布，L2是高斯分布。拉普拉斯分布：$$f ( x | \mu , b ) = \frac { 1 } { 2 b } e ^ { - \frac { | x - \mu | } { b } }$$高斯分布：$$f \left( x | \mu , \sigma ^ { 2 } \right) = \frac { 1 } { \sqrt { 2 \pi \sigma ^ { 2 } } } e ^ { - \frac { ( x - \mu ) ^ { 2 } } { 2 \sigma ^ { 2 } } }$$ Decision tree主要介绍一下 决策树的特点。 从这次学习中明显的感受到这个 decision tree 是非常容易过拟合的。 We can make our tree more complex by increasing its size , which will result in more and more partitions trying to emulate the circular boundary. 优点在于：可以handle 非线性的变化。decision tree 给人的感觉就是线性或者离散的(category)的都是可以使用，因为decision tree 得到就是一个离散的结果，最大的缺点就是容易过拟合。 This brings us to the biggest problem associated with Decision Trees, that is, they are highly biased class of models. You can make a decision tree model on your training set which might outperform all other algorithms but it’ll prove to be a poor predictor on your test set. You’ll have to rely heavily on pruning and cross validation to get a non-over-fitting model with Decision Trees. 过拟合是可以通过剪枝或者 cross validation 进行缓解 overfit的效果的或者使用 random forest随机性进行”中和“。 This problem of over-fitting is overcome to large extent by using Random Forests, which are nothing but a very clever extension of decision trees. But random forest take away easy to explain business rules because now you have thousands of such trees and their majority votes to make things complex. Also by decision trees have forced interactions between variables , which makes them rather inefficient if most of your variables have no or very weak interactions. LR vs. SVM单独讲解SVM。这里主要是比较两种的异同。 相同点 LR和SVM都是分类算法。（svm 可以作为回归模型） LR和SVM都是监督学习算法。监督是体现在有标签。 两者都属于判别模型。（相对于bayes 模型） 判别模型会生成一个表示$P(Y|X) $的判别函数（或预测模型），而生成模型先计算联合概率$P(Y,X) $然后通过贝叶斯公式转化为条件概率。常见的判别模型有：：KNN、SVM、LR。常见的生成模型有：朴素贝叶斯，隐马尔可夫模型。 不同点 从数据和模型的角度分析。 数据 SVM 中边界附近的点会起到决策的作用，在线上的点叫做支持向量。而LR 是全局信息，全部的数据集都会参与到决策， 如果数据是严重的 unbalanced, 那么需要是对数据进行 balance 操作，因为模型是依赖于数据概率分布。 loss function 的不同 LR 的损失函数 $$J( \theta ) = \log ( L ( \theta ) ) = \sum _ { i = 1 } ^ { m } y ^ { ( i ) } \log \left( h \left( x ^ { ( i ) } \right) \right) + \left( 1 - y ^ { ( i ) } \right) \log \left( 1 - h \left( x ^ { ( i ) } \right) \right)$$ SVM的损失函数(在线性的可分的条件下) $$L ( w , b , \alpha ) = \frac { 1 } { 2 } | w | ^ { 2 } + \sum _ { i } \alpha _ { i } [ 1 - y _ { i } ( w ^ { T } x _ { i } + b ) ]$$(本质上使用拉格朗日进行最小化的求解) 数据预处理 简单来说，​逻辑回归方法基于概率理论，一个样本通过sigmoid 函数进行概率表示，然后使用极大似然的方式估计出参数的值。支持向量是基于几何间隔最大化原理，存在几何间隔最大的超平面，基于几何距离的测量函数，一般需要对数据做normalization，而LR 则不受这个因素影响。 增量训练 当训练好一个 SVM 和LR，然后来了一批新的数据，对于SVM 的决策平面变化比较小，而对于LR 就是要重新进行训练。 正则项 SVM 的损失函数中自带正则 ($ \frac { 1 } { 2 } | w | ^ { 2 }$, 这就类似L2 正则项)，这就是为什么SVM是结构风险最小化算法的原因。（所谓结构风险最小化，意思就是在训练误差和模型复杂度之间寻求平衡，防止过拟合）。而LR 想要得到更好的泛化性能，需要手动加上正则项。 选择标准（吴恩达课程） n是数据中特征的数量 m是样本数 1、如果n相对于m来说很大，则使用LR算法或者不带核函数的SVM（线性分类）n远大于m，n=10000，m=10-10002、如果n很小，m的数量适中（n=1-1000，m=10-10000）使用带有核函数的SVM算法3、如果n很小，m很大（n=1-1000，m=50000+）增加更多的feature然后使用LR算法或者不带核函数的SVMLR和不带核函数的SVM比较类似。 简单说，如果特征相对于数据量来说很大，那么使用 LR或者线性SVM 算法；如果相反，那么使用带有核函数的SVM。 总结分类模型区别LR vs. SVM vs. Bayes 数据 LR 数据处理是 balanced（依赖于概率分布）；SVM 一般是要normalization（空间距离函数） loss function LR 是log loss，使用最大似然估计求解 \theta ； SVM 是拉格朗日定量求解。 正则项 LR 需要手动加上；SVM自带，SVM是结构风险最小化算法的 增量学习LR 收到新来的数据的影响， SVM 不受（基本上只是依赖 支持向量 的点），bayes 支持增量学习。 实战LR 和不带核函数的SVM 比较类似。（基本上可以互用） m 是样本数（数据量），n 是特征数特征数n 远大于数据量 m，选择LR 模型 （n =1w，m&lt; 1000）特征n 小，m 适中 （n &lt; 1000, m ~1w）使用带有核函数的SVM 算法特征n 很小，m 很大 （m &gt; 5w） 增加特征 LR 和朴素贝叶斯的区别]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>LR</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[笔试总结]]></title>
    <url>%2F2019%2F06%2F09%2F%E7%AC%94%E8%AF%95%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[消息通信的基本方式有两种： 同步方式 两个通信应用服务之间必须要进行同步，两个服务之间必须都是正常运行的。发送程序和接收程序都必须一直处于运行状态，并且随时做好相互通信的准备。 发送程序首先向接收程序发起一个请求，称之为发送消息，发送程序紧接着就会堵塞当前自身的进程，不与其他应用进行任何的通信以及交互，等待接收程序的响应，待发送消息得到接收程序的返回消息之后会继续向下运行，进行下一步的业务处理。 异步方式 两个通信应用之间可以不用同时在线等待，任何一方只需各自处理自己的业务，比如发送方发送消息以后不用登录接收方的响应，可以接着处理其他的任务。也就是说发送方和接收方都是相互独立存在的，发送方只管方，接收方只能接收，无须去等待对方的响应。 Java中JMS就是典型的异步消息处理机制，JMS消息有两种类型：点对点、发布/订阅。 视图的操作，是最终都要转化成对基本表的操作 常见的设计模式 大致按照模式的应用目标分类，设计模式可以分为创建型模式、结构型模式和行为型模式。 创建型模式，是对对象创建过程的各种问题和解决方案的总结，包括各种工厂模式（Factory、Abstract Factory）、单例模式（Singleton）、构建器模式（Builder）、原型模式（ProtoType）。 结构型模式，是针对软件设计结构的总结，关注于类、对象继承、组合方式的实践经验。常见的结构型模式，包括桥接模式（Bridge）、适配器模式（Adapter）、装饰者模式（Decorator）、代理模式（Proxy）、组合模式（Composite）、外观模式（Facade）、享元模式（Flyweight）等。 行为型模式，是从类或对象之间交互、职责划分等角度总结的模式。比较常见的行为型模式有策略模式（Strategy）、解释器模式（Interpreter）、命令模式（Command）、观察者模式（Observer）、迭代器模式（Iterator）、模板方法模式（Template Method）、访问者模式（Visitor）。 工厂模式是我们最常用的实例化对象模式了，简单讲是用工厂方法代替new操作的一种模式。单例模式限制了类实例的创建，但采用这种模式设计的类，可以保证仅有一个实例，并可提供访问该实例的全局访问点。创建 HttpRequest 的过程，就是典型的构建器模式（Builder），通常会被实现成fluent 风格的 API，也有人叫它方法链。适配器模式 ：将一个类的接口转换成客户希望的另外一个接口。适配器模式使得原本由于接口不兼容而不能一起工作的那些类可以一起工作。装饰器模式（Decorator Pattern）允许向一个现有的对象添加新的功能，同时又不改变其结构。这种类型的设计模式属于结构型模式，它是作为现有的类的一个包装。 多态的代码题目 123456789101112131415161718192021222324252627282930313233343536373839404142#include&lt;iostream&gt;using namespace std;class Base1&#123;public: virtual void fun() &#123; cout &lt;&lt;"Base 1" &lt;&lt;endl; &#125;&#125;;class Base2&#123;public: void fun() &#123; cout &lt;&lt;"Base 2" &lt;&lt;endl; &#125;&#125;;class Derived: public Base1, public Base2&#123;public: void fun() &#123; cout &lt;&lt;"Derived" &lt;&lt;endl; &#125;&#125;;int main()&#123; Base1 bs1, *p1; Base2 bs2, *p2; Derived dv; p1 =&amp;bs1; p1-&gt; fun(); p1 =&amp;dv; p1-&gt; fun(); p2 =&amp;bs2; p2-&gt; fun(); p2 =&amp;dv; p2-&gt; fun(); return 0;&#125; 输出是1234Base 1DerivedBase 2Base 2 逻辑量和真假值是相同的意思。 常见的数据校验方法 定义：校验,是为保护数据的完整性，用一种指定的算法对原始数据计算出的一个校验值。当接收方用同样的算法再算一次校验值，如果两次校验值一样，表示数据完整。 奇偶校验： 根据被传输的一组二进制代码中“1”的个数是奇数或偶数来进行校验。eg. 数据位为 10001100 （1） -&gt; 最后一位为校验位。此时若约定好为奇校验，那么数据表示为正确的，若为偶校验，那么数据传输出错了。CRC校验（循环冗余校验码）：利用除数以及余数的原理进行错误检测，将接收到的码组进行除法运算 ，如果除尽，则说明传输无误；如果未除尽，则表明传输出现差错。 借助于多项式除法，其余数为校验字段。 例如：信息字段代码为: 1011001；对应m(x)=x6+x4+x3+1 假设生成多项式为：g(x)=x4+x3+1；则对应g(x)的代码为: 11001 x4m(x)=x10+x8+x7+x4 对应的代码记为：10110010000； 采用多项式除法: 得余数为: 1010 (即校验字段为：1010） 发送方：发出的传输字段为: 1 0 1 1 0 0 11 0 10 信息字段 校验字段 tcp 服务器端启动进程，调用Socket创建一个基于TCP协议的流套接字描述符。 其次，服务进程调用bind命名套接字，将套接字描述符绑定到本地地址和本地端口上。 再次，服务器端调用listen，开始侦听客户端的Socket连接请求。 接下来阻塞，直到收到了客户端的connect请求，调用accept进行相应。 因此，不阻塞bind和listen 单例模式（以下都是正确的）A 用户无法通过new方式实例化单例类B 违背了单一职责原则C 单例模式用于多线程应用程序D 单例模式属于创建型模式 产生式(production)描述了将终结符和非终结符组合成串的方法 一个上下文无关文法一般是由一组非终结符号，一组终结符号，一个开始符号和一组产生式构成，那么产生式是用于定义（语法成分）的一种规则 堆的存储 根据结点判别大根堆和小根堆。 一般用数组来表示堆，若根结点存在序号0处， i结点的父结点下标就为i/2。i结点的左右子结点下标分别为2i+1和2i+2。（注：如果根结点是从1开始，则左右孩子结点分别是2i和2i+1。）如第0个结点左右子结点下标分别为1和2。 常见的端口和服务 端口：21 服务：FTP 说明：FTP服务器所开放的端口，用于上传、下载。端口：23 服务：Telnet 说明：远程登录，入侵者在搜索远程登录UNIX的服务。端口：80 服务：HTTP 说明：用于网页浏览。木马Executor开放此端口。 windows 中的 在windows,线程同步的方式比较多,这篇只是简单的介绍使用内核对象互斥量进行线程同步,互斥量Mutex为内核对象,使用方式原理和临界区CriticalSection差不多,临界区是基于用户空间进行线程同步的,只能进行同一个进程中线程的同步,不能跨进程,因为不是基于内核态的. linux 中的同步和互斥 临界资源的概念：多个线程不能同时使用的资源称为临界资源CR（Critical Resource）。临界资源可以是一些独占设备，比如打印机等，也可以是一些共享变量，表格，链表等。临界区：不论硬件临界资源还是软件临界资源，多个线程必须互斥的对其进行访问。每个线程中访问临界资源的那段代码称为临界区CS（Critical Section）。每个线程在进入临界区以前，应该对欲访问的临界资源进行检查，看他是否正在被访问。若是，则该线程不能进入临界区，若否，则该线程可以进入临界区对该资源进行访问，并设置只在被访问的标志。线程互斥的概念：线程互斥是指多个线程不能同时使用同一个临界资源，即两个或两个以上线程必须互斥的使用临界资源，当然不能同时进入临界区。线程同步的概念：线程同步是指有协作关系的线程之间不断地调整他们之间的相对速度或者执行过程，以保证临界资源的合理利用和线程的顺利执行。实现线程同步的机制称为线程同步机制。 同步机制遵循的规则： 空闲让进：并发线程中某个线程不再临界区时，不阻止其它线程进入临界区。 忙则等待：只允许同一时刻一个线程进入临界区，其它欲进入临界区的线程等待。 有限等待：欲访问临界区的线程要在有限的时间内访问到临界区，避免陷入“死等”状态。 让权等待:当线程不能进入自己的临界区的时候，应立即释放CPU，以避免线程陷入“忙等”状态。 锁机制：实现互斥的一种方式是采用锁机制，即提供上锁（Lock）和开锁（Unlock）原语，以及一个锁变量w或者锁位（1bit）。信号量:（Semaphore）也叫信号灯，是在信号同步机制中用于实现线程的同步和互斥有效的数据结构。信号量机制：信号量机制中，申请和释放临界资源的两个原语操作为wait操作和signal操作，有时也称为P操作和V操作。 常见的页面置换算法 最佳置换算法（OPT）：最佳（Optimal，OPT）置换算法所选择的被淘汰页面将是以后永不适用的，或者是在最长时间内不再被访问的页面，这样可以保证获得最低的缺页率。但是由于人们目前无法预知进程在内存下的若干页面中哪个是未来最长时间不再被访问的，因而该算法无法实现。 先进先出（FIFO）页面置换算法：FIFO算法还会产生当所分配的物理块数增大而页故障数不减反增的异常现象，这时由belady于1969年发现，故称为Belady异常。只有FIFO算法可能出现Belady异常，而LRU和OPT算法永远不会出现Belady异常。 最近最久未使用（LRU）置换算法：选择最近最长时间未访问过的页面予以淘汰，它认为过去时间内一段时间内未访问过的页面，在最近的将来也不会被访问。（使用一个队列来模拟整个过程）。 LRU性能较好，但需要寄存器和栈的硬件支持。LRU是堆栈类的算法。理论上可以证明，堆栈类的算法不可能出现belady异常。FIFO基于队列实现，不是堆栈类算法。 LRU算法的性能接近于OPT，但是实现起来比较困难，且开销大；FIFO算法实现简单，但性能差。时钟（CLOCK）置换算法：如果所有帧的使用位均为1，则指针在缓冲区中完整地循环一周，将所有使用位都置为0，并且停留在最初的位置上，替换该帧中的页。由于该算法循环地检查各页面的情况，故称为CLOCK算法，又称为最近未使用（Not Recently Used，NRU）算法（这个算法其实不是很懂 过程，） 缺页率的计算 try catch的使用 只有错误可预知时才用try。所有不可预知的错误用try都是不负责任的写法 12345678910try &#123; 语句组&#125;catch(异常类型) &#123; 异常处理代码&#125;...catch(异常类型) &#123; 异常处理代码&#125; UML类图中的六大关系：关联、聚合、组合、依赖、继承、实现 关联描述两个类之间行为的一般二元关系。例如，一个学生选修一门特定的课程是学生类Student和课程类Course之间的一个关联，而一个教师教授一门课程是师资类Faculty和课程类Course之间的一个关联。Java代码中，关联可以用属性和方法来实现。 聚合是一种特殊的关联(Association)形式，表示两个对象之间的所属(has-a)关系。所有者对象称为聚合对象，它的类称为聚合类；从属对象称为被聚合对象，它的类称为被聚合类。例如，一个公司有很多员工就是公司类Company和员工类Employee之间的一种聚合关系。被聚合对象和聚合对象有着各自的生命周期，即如果公司倒闭并不影响员工的存在。 聚合是一种较弱形式的对象包含(一个对象包含另一个对象)关系。较强形式是组合(Composition). 在组合关系中包含对象负责被包含对象的创建以及生命周期，即当包含对象被销毁时被包含对象也会不复存在。例如一辆汽车拥有一个引擎是汽车类Car与引擎类Engine的组合关系。 依赖(Dependency)描述的是一个类的引用用作另一个类的方法的参数。例如，可以使用Calendar类中的setTime(Date date)方法设置日历，所以Calendar和Date之间的关系可以用依赖描述。 继承(Inheritance)模拟两个类之间的is-a关系。强是(strong is-a)关系描述两个类之间的直接继承关系。弱是(weak is-a)关系描述一个类具有某个属性。强是关系可以用类的继承表示。例如，Spring的ApplicationEvent是一个EventObject，ApplicationEvent和EventObject间就是一种强是关系，可以用继承描述。 实现(Realization)描述的是一个类实现了接口（可以是多个）。 sizeof 的使用 char,short int(short),int,long int(long),float,double, long double大小分别是：1，2，4，4，4，8, 10。 不懂函数中的那个为什么是 sizeof 是8 呀。 1234567891011121314#include&lt;iostream&gt;using namespace std;void func(char str[100])&#123; cout &lt;&lt; sizeof(str)&lt;&lt;endl; // 8&#125;int main()&#123; char str[200]; func(str); char ch; //1 cout &lt;&lt; sizeof(str)&lt;&lt;endl; //200 return 0;&#125; 常见的生成模型和判别模型 生成模型：hmm，朴素贝叶斯判别模型：条件随机场crf， 下面哪些优化算法和动量相关 momentum, adam, nesterov accelerated gradient(在rnn 中使用比较常见 )（参考博客http://localhost:4000/2019/03/26/深度网络中的碎碎念/） 四种数据存储结构—顺序存储 链接存储 索引存储 散列存储 顺序结构和链接结构适用在内存结构中。索引结构和散列结构适用在外存与内存交互结构。除建立存储结点信息外，还建立附加的索引表来标识结点的地址。索引表由若干索引项组成。其优点是检索速度快，缺点是增加了附加的索引表,会占用较多的存储空间。在数据表中，就是用索引键来进行存储与检索的。散列存储，又称hash存储，是一种力图将数据元素的存储位置与关键码之间建立确定对应关系的查找技术。 根据余数进行“聚类”的题目假定一个线性表为 (12,23,74,55,63,40) ，若按 Key % 4 条件进行划分，使得同一余数的元素成为一个子表，则得到的四个子表分别为（1） 、（2） 、（3） 和（4） 。 逻辑结构和存储结构逻辑结构分成线性结构和非线性结构；存储结构分为顺序存储 和链式存储。 运算符号表达式 (a++) +(++b) +ab 注意加法是有顺序的，所以说结果应该是 a+ b+1 + (a+1)(b+1) 。当执行完 a++，这个时候 a 已经变化了。 java 的过程 ‘’‘java中源文件的后缀为.java，经过javac.exe编译后生成字节码文件，后缀为.class，再经过java.exe编译为可执行文件，后缀为.exe。 .jar 一种压缩包格式，用来打包类库··· 重写子类中的某个方法和函数名，参数个数，参数类型和父类中的某个方法完全一样，那么就是覆盖了。]]></content>
      <categories>
        <category>CS基础</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[SVM All You Need to Know]]></title>
    <url>%2F2019%2F06%2F08%2Fsvm-all-you-need%2F</url>
    <content type="text"><![CDATA[本文主要介绍SVM 相关内容，包括理论原理、在线性可分条件下的公式推导和 SVM的应用特点。最后综合 LR 和 Decision Tree的这篇博客，给出了一些小的建议。 SVM 理论支持向量机分为三个部分，线性可分支持向量机、线性支持向量机、非线性支持向量机。 SVM 原理SVM 是一种二类分类模型。它的基本模型是在特征空间中寻找间隔最大化的分离超平面的线性分类器。 当训练样本线性可分时，通过硬间隔最大化，学习一个线性分类器，即线性可分支持向量机；当训练数据近似线性可分时，引入松弛变量，通过软间隔最大化，学习一个线性分类器，即线性支持向量机；当训练数据线性不可分时，通过使用核技巧及软间隔最大化，学习非线性支持向量机。以上各种情况下的数学推到应当掌握，硬间隔最大化（几何间隔）、学习的对偶问题、软间隔最大化（引入松弛变量）、非线性支持向量机（核技巧）。 SVM 为什么采用间隔最大化当训练数据线性可分时，存在无穷个分离超平面可以将两类数据正确分开。感知机利用误分类最小策略，求得分离超平面，不过此时的解有无穷多个。线性可分支持向量机利用间隔最大化求得最优分离超平面，这时，解是唯一的。另一方面，此时的分隔超平面所产生的分类结果是最鲁棒的，对未知实例的泛化能力最强。可以借此机会阐述一下几何间隔以及函数间隔的关系。 为什么要将求解 SVM 的原始问题转换为其对偶问题一是对偶问题往往更易求解，当我们寻找约束存在时的最优点的时候，约束的存在虽然减小了需要搜寻的范围，但是却使问题变得更加复杂。为了使问题变得易于处理，我们的方法是把目标函数和约束全部融入一个新的函数，即拉格朗日函数，再通过这个函数来寻找最优点。二是可以自然引入核函数，进而推广到非线性分类问题。 为什么 SVM 要引入核函数当样本在原始空间线性不可分时，可将样本从原始空间映射到一个更高维的特征空间，使得样本在这个特征空间内线性可分。而引入这样的映射后，所要求解的对偶问题的求解中，无需求解真正的映射函数，而只需要知道其核函数。核函数的定义：K(x,y)=&lt;ϕ(x),ϕ(y)&gt;，即在特征空间的内积等于它们在原始样本空间中通过核函数 K 计算的结果。一方面数据变成了高维空间中线性可分的数据，另一方面不需要求解具体的映射函数，只需要给定具体的核函数即可，这样使得求解的难度大大降低。 为什么SVM对缺失数据敏感这里说的缺失数据是指缺失某些特征数据，向量数据不完整。SVM 没有处理缺失值的策略。而 SVM 希望样本在特征空间中线性可分，所以特征空间的好坏对SVM的性能很重要。缺失特征数据将影响训练结果的好坏。 SVM 核函数之间的区别一般选择线性核和高斯核，也就是线性核与 RBF 核。 线性核：主要用于线性可分的情形，参数少，速度快，对于一般数据，分类效果已经很理想了。 RBF 核：主要用于线性不可分的情形，参数多，分类结果非常依赖于参数。有很多人是通过训练数据的交叉验证来寻找合适的参数，不过这个过程比较耗时。 如果 Feature 的数量很大，跟样本数量差不多，这时候选用线性核的 SVM。 如果 Feature 的数量比较小，样本数量一般，不算大也不算小，选用高斯核的 SVM。 以上是几个问题在面试中遇到 SVM 算法时，几乎是必问的问题，另外，大家一定要做到自己可以推导集合间隔、函数间隔以及对偶函数，并且理解对偶函数的引入对计算带来的优势。 支持向量 如图所示，上面只有三个点与求解的优化问题有关，它们就叫做支持向量。 SVM公式推导(线性可分条件下)假定样本空间如下$${ ( x _ { 1 } , y _ { 1 } ) , ( x _ { 2 } , y _ { 2 } ) , \ldots , ( x _ { N } , y _ { N } ) }$$ 共有N个向量，其中$x_k$是一个特征向量而不是一个单一数值。 假设超平面能够将训练样本正确分类，那么就有以下的式子成立： 这是一个二分类问题，所以$y=+1 $或者$ y=−1$。那么我们就可以得到 $$ y = \begin{cases}+1 &amp; w^Tx +b &gt; 0 \\-1 &amp; w^Tx +b &lt;0\end{cases}$$ 上面是逻辑回归的思路，没有一点的缓冲余地。如实SVM 使用下面的判别式： $$ y = \begin{cases}+1 &amp; w^Tx +b &gt;= +1 \\-1 &amp; w^Tx +b &lt;= -1\end{cases}$$ 上面距离超平面最近的几个训练样本点使上式的等号成立，这几个训练样本被称为支持向量，两个异类支持向量到超平面的距离 那么，我们可以得到$$y _ { i } \cdot ( w ^ { T } x _ { i } + b ) \geq 1 , i = 1,2 , \ldots , N$$ 因为我们现在只讨论线性可分情况下的支持向量机，那么在这个样本空间中一定存在一个超平面可以将样本集按照y的值分割城两个部分，这个超平面可以表示为$$w ^ { T } x + b = 0$$ 根据这个超平面的表达式以及第一步推到中我们得到的结果，可以得到这个样本集中任意一个样本点距离超平面的距离：$$\gamma = \frac { | w ^ { T } x + b | } { | w | } \geq \frac { 1 } { | w | }$$两个异类支持向量到超平面的距离之和，也称为间隔，为 $$\gamma = \frac { 2 } { | w | }$$ 由此，根据第一步和第三步的结果，我们可以得到最基本的目标函数： $$\arg \max _ { w , b } \frac { 2 } { | w | } , \text {s.t. } y _ { i } ( w ^ { T } x _ { i } + b ) \geq 1 , i = 1,2 , \ldots , N$$ 我们还可以对这个目标函数进一步做变化： $$\arg \min _ { w , b } \frac { 1 } { 2 } | w | ^ { 2 }, \text {s.t. } y _ { i } ( w ^ { T } x _ { i } + b ) \geq 1 , i = 1,2 , \ldots , N $$ 我们无法继续直接进行计算了，因此引入拉格朗日乘子$$L ( w , b , \alpha ) = \frac { 1 } { 2 } | w | ^ { 2 } + \sum _ { i } \alpha _ { i } [ 1 - y _ { i } ( w ^ { T } x _ { i } + b ) ]$$ 对w和b分别求L的偏导，并令其偏导数等于0：$$\frac { \partial L } { \partial w } = w - \sum _ { i } \alpha _ { i } y _ { i } x _ { i } = 0 \Rightarrow w = \sum _ { i } \alpha _ { i } y _ { i } x _ { i }$$$$\frac { \partial L } { \partial b } = \sum _ { i } \alpha _ { i } y _ { i } = 0$$ 将第七步得到的w和b代入L函数 至此，我们的目标函数已经变成了$$\arg \max _ { \alpha } ( \sum _ { i } \alpha _ { i } - \frac { 1 } { 2 } \sum _ { i } \sum _ { j } \alpha _ { i } \alpha _ { j } y _ { i } y _ { j } x _ { i } ^ { T } x _ { j } )$$$$\text { s.t. } \sum _ { i } \alpha _ { i } y _ { i } = 0$$$$\alpha _ { i } \geq 0 , i = 1,2 , \ldots , N$$ 用数值方法解出α以后，我们带入式子 7就可以得到 $$w^ { * } = \sum _ { i } \alpha _ { i } ^ { * } y _ { i } x _ { i }$$ SVM的特点SVM的优点：就是当大量的特征出现的时候，使用SVM handle large feature spaces; 然而此时 LR 不是一个很好的选择。 SVM can handle large feature spaces which makes them one of the favorite algorithms in text analysis which almost always results in huge number of features where logistic regression is not a very good choice. SVM Pros: Can handle large feature space Can handle non-linear feature interactions Do not rely on entire data SVM Cons: Not very efficient with large number of observations It can be tricky to find appropriate kernel sometimes 核函数的作用： 核函数是将原始空间线性不可分，映射到一个高纬的特征空间，使得样本在这个维度变得线性可分。 核函数的种类： 分为线性核函数和高斯核函数。前者参数少，计算量小，如果是特征数量比较大，那么是可以考虑线性核函数；如果特征数量比较小，可以考虑高斯核函数。 TakeOff首先使用 LR 进行尝试，不妨试一下 DT，然后 如果特征比较多，但是数据量不是很多，这个时候使用SVM。Always start with logistic regression, if nothing then to use the performance as baselineSee if decision trees (Random Forests) provide significant improvement. Even if you do not end up using the resultant model, you can use random forest results to remove noisy variablesGo for SVM if you have large number of features and number of observations are not a limitation for available resources and time附上链接：https://www.edvancer.in/logistic-regression-vs-decision-trees-vs-svm-part2/]]></content>
      <tags>
        <tag>SVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NLP Papers Reading-Sentence Embedding]]></title>
    <url>%2F2019%2F06%2F01%2Fnlp-papers-reading-sentence-embedding%2F</url>
    <content type="text"><![CDATA[Why Consider Sentence Embedding?One simple way you could do this is by generating a word embedding for each word in a sentence, adding up all the embeddings and divide by the number of words in the sentence to get an “average” embedding for the sentence. 句子embedding的表示 = words emebdding，可能使用 TF-IDF or SIF 作为weights 进行优化。 Alternatively, you could use a more advanced method which attempts to add a weighting function to word embeddings which down-weights common words. This latter approach is known as Smooth Inverse Frequency (SIF). These methods can be used as a successful proxy for sentence embeddings. However, this “success” depends on the dataset being used and the task you want to execute. So for some tasks these methods could be good enough 上述方法的主要缺点：语序；文字在上下文中才有意义；阅读理解，不同的句子是相同的意思，却得到不同的embedding；依赖于前期处理，如分词。 However, there are a number of issues with any of these types of approaches: They ignore word ordering. This is obviously problematic. It’s difficult to capture the semantic meaning of a sentence. The word crash can be used in multiple contexts, e.g. I crashed a party, the stock market crashed, or I crashed my car. It’s difficult to capture this change of context in a word embedding. Sentence length becomes problematic. With sentences we can chain them together to create a long sentence without saying very much, The Philadelphia Eagles won the Super Bowl, The Washington Post reported that the Philadelphia Eagles won the Super Bowl, The politician claimed it was fake news when the Washington Post reported that the Philadelphia Eagles won the Super Bowl, and so on. All these sentences are essentially saying the same thing but if we just use word embeddings, it can be difficult to discover if they are similar. They introduce extra complexity. When using word embeddings as a proxy for sentence embeddings we often need to take extra steps in conjunction with the base model. For example, we need to remove stop words, get averages, measure sentence length and so on. sentence embedding的应用场景：Similar approaches can be used to go beyond representations and semantic search, to document classification and understanding and eventually document summarizing or generation. Words Embed平均词向量与TFIDF加权平均词向量SIF加权平均词向量来自论文 A simple but tough-to-beat baseline for sentence embeddings，更多信息可以参考这里。在大家都从无监督学习走向有监督学习的时候，这个无监督的方法和神经网络的效果是旗鼓相当的。 利用n-grams embeddingfasttext 介绍。简单说 n-gram 是一种概念，可以细化成两部分：character-level 和word-level，前者是可以用来补充词汇，加强对于不常见词的表示能力，后者是对于词序的补充。 DAN（Deep Unordered Composition Rivals Syntactic Methods for Text Classification）其实DAN(Deep Averaging Networks)应该属于Bag of Words类的算法。因为比较特殊，单独列出来。 它是在对所有词语取平均后，在上面加上几层神经网络。特殊的地方在于它在sentiment analysis中表现也不错，这在BOW类方法中比较罕见。 文中提出了DAN(Deep average network)，说白了就是对于一个句子或者一个段落，把每个单词的embedding进行平均求和，得到第一层固定维度的向量，然后在套几层全连接神经网络。本质来讲，这个模型没有考虑单词之间的顺序，not在第一个位置和在最后一个位置对于DAN来讲输入都是一样的，所以自然conver不住这种情况。这是模型本身的问题，没有办法改进，除非换模型，比如textcnn就能很好的解决这种情况对于否定词敏感，比如but,not等，常常判断为negative。训练速度快，且结果较好，和Syntactic Composition性能差不多，但是消耗的计算资源少作为有监督学习任务来讲，可以试一试。但是由于全连接层，无法进行无监督学习。相反，NBOW可以无监督学习，比如文本相似度计算等。当然。对于DAN而言，可以通过迁移学习，预训练好全连接参数，实现无监督学习 总结一下：对简单的任务来说，用简单的网络结构进行处理基本就够了，但是对比较复杂的任务，还是依然需要更复杂的网络结构来学习sentence representation的。 Unsupervised Sentence Embed基于Encoder-decoder的Skip-Thought Vectorsskip-thoughts 中提及一个重要的方法是 词汇扩展。具体来说我们可以先用海量的语料训练一个Word2Vec，这样可以把一个词映射到一个语义空间，我们把这个向量叫做 $ V_{w 2 v} $。而我们之前训练的得到的输入向量也是把一个词映射到另外一个语义空间，我们记作$V_{r n n}$. 我们假设它们之间存在一个线性变换$ f: V_{w 2 v} \rightarrow V_{r n n}$。这个线性变换的参数是矩阵W，使得$V_{r n n}$= $W V_{w 2 v} $。那怎么求这个变换矩阵W呢？因为两个训练语料会有公共的词(通常训练word2vec的语料比skip vector大得多，从而词也多得多)。因此我们可以用这些公共的词来寻找 $W$。寻找的依据是：遍历所有可能的W，使得$ W V_{w 2 v} $和$V_{rnn}$尽量接近。 对于双向训练（不是使用了两个模型，而是使用正反两种不同顺序不同的训练数据集）此外还训练了bi-skip向量，它是这样得到的：首先训练1200维的uni-skip，然后句子倒过来，比如原来是”aa bb”、”cc dd”和”ee ff”，我们是用”cc dd”来预测”aa bb”以及”ee ff”，现在反过来变成”ff ee”、”dd cc”和”bb aa”。这样也可以训练一个模型，当然也就得到一个encoder(两个decoder不需要了)，给定一个句子我们把它倒过来然后也编码成1200为的向量，最后把这个两个1200维的向量拼接成2400维的向量。 模型训练完成之后还需要进行词汇扩展。通过BookCorpus学习到了20,000个词，而word2vec共选择了930,911词，通过它们共同的词学习出变换矩阵W，从而使得我们的Skip Thought Vector可以处理930,911个词。（最终得到还是语言模型） 这篇论文中的要点 训练的时候在远大于 word2vec 的训练集上展开。如 skip-thought 中使用的 BookCorpus 数据集中是千万级别的 words(984,846,357)，在word2vec 中英文的 wikicorpus 是百万 (120 million words)， 所以相差一个数量级。 objective function 是语言模型（类似 machine translation), 给定上下一个句子，然后预测中心句子的概率，不断的最大化的过程。 词汇扩展，使用了一个word2vec 中的embedding 补充在训练数据集中没有出现过的 word，从word2vec 中的word 到该模型中的 word embedding 是学习了一个matrix，转换关系。缓解了oov 问题；关于oov 问题，句子的相似度不是exact的相似，而是一种 在语法和语义上的近似相似。 Continuing the tour of older papers that started with our ResNet blog post, we now take on Skip-Thought Vectors by Kiros et al. Their goal was to come up with a useful embedding for sentences that was not tuned for a single task and did not require labeled data to train. They took inspiration from Word2Vec skip-gram (you can find my explanation of that algorithm here) and attempt to extend it to sentences. Changing a single word has had almost no effect on the meaning of that sentence. To account for these word level changes, the skip-thought model needs to be able to handle a large variety of words, some of which were not present in the training sentences. The authors solve this by using a pre-trained continuous bag-of-words (CBOW) Word2Vec model and learning a translation from the Word2Vec vectors to the word vectors in their sentences. Below are shown the nearest neighbor words after the vocabulary expansion using query words that do not appear in the training vocabulary: 论文描述了一种通用、分布式句子编码器的无监督学习方法。使用从书籍中提取的连续文本，训练了一个编码器-解码器模型，借鉴了word2vec中skip-gram模型，通过一句话来预测这句话的上一句和下一句。语义和语法属性一致的句子被映射到相似的向量表示。接着引入一个简单的词汇扩展方法来编码不再训练集内的单词，令词汇量扩展到一百万词。本文的模型被称为skip-thoughts，生成的向量称为skip-thought vector。模型采用了当下流行的端到端框架，通过搜集了大量的小说作为训练数据集，将得到的模型中encoder部分作为feature extractor，可以给任意句子生成vector。 skip-thought模型结构借助了skip-gram的思想。在skip-gram中，是以中心词来预测上下文的词；在skip-thought同样是利用中心句子来预测上下文的句子。 论文采用了GRU-RNN作为encoder和decoder，encoder部分的最后一个词的hiddenstate作为decoder的输入来生成词。这里用的是最简单的网络结构，并没有考虑复杂的多层网络、双向网络等提升效果。decoder部分也只是一个考虑了encoder last hidden state的语言模型，并无其他特殊之处，只是有两个decoder，是一个one maps two的情况，但计算方法一样。模型中的目标函数也是两个部分，一个来自于预测下一句，一个来自于预测上一句。 传说中的 objective function or loss function 是下面这个样子： 我们将构造一个类似于自编码器的序列到序列结构，但是它与自编码器有两个主要的区别。第一，我们有两个 LSTM 输出层：一个用于之前的句子，一个用于下一个句子；第二，我们会在输出 LSTM 中使用教师强迫（teacher forcing）。这意味着我们不仅仅给输出 LSTM 提供了之前的隐藏状态，还提供了实际的前一个单词。 看上去，Skip-thought和Skip-gram挺象。唯一的遗憾是Skip-thought的decoder那部分，它是作为language modeling来处理的. 从这里的讲解知道这个是不存在 ”正负“样本的， 这个的损失函数是 正确的上下句和生成的上下句之间的reconstruction error。 The end product of Skip-Thoughts is the Encoder. The Decoders are thrown away after training. The trained encoder can then be used to generate fixed length representations of sentences which can be used for several downstream tasks such as sentiment classification, semantic similarity, etc. The encoder utilises a word embedding layer that serves as a look up table. This converts each word in the input sentence to its corresponding word embedding, effectively converting the input sentence into a sequence of word embeddings. This embedding layer is also shared with both of the decoders. The model is then trained to minimise the reconstruction error of the previous and next sentences using the resulting embedding h(i) generated from sentence s(i) after it is passed through the encoder. Back propagating the reconstruction error from the decoder allows the encoder to learn the best representation of the input sentence while capturing the relation between itself and the surrounding sentences.Skip-Thoughts is designed to be a sentence encoder and the result is that the decoders are actually discarded after the training process. The encoder along with the word embedding layer is used as a feature extractor able to encode new sentences that are fed through it. Using cosine similarity on the resulting encoded sentence embeddings, provides a powerful semantic similarity mechanism, where you can measure how closely two sentences relate in terms of meaning as well as syntax. Encoder Network: The encoder is typically a GRU-RNN which generates a fixed length vector representation h(i) for each sentence S(i) in the input. The encoded representation h(i) is obtained by passing final hidden state of the GRU cell (i.e. after it has seen the entire sentence) to multiple dense layers. Decoder Network: The decoder network takes this vector representation h(i) as input and tries to generate two sentences — S(i-1) and S(i+1), which could occur before and after the input sentence respectively. Separate decoders are implemented for generation of previous and next sentences, both being GRU-RNNs. The vector representation h(i) acts as the initial hidden state for the GRUs of the decoder networks. 词汇扩展 当初有个面试官问道的训练样本不足的问题，现在给出答案，一种是大量的数据集，因为这个是无监督的学习，不需要标签，所以使用了大量的小说作为训练集；对于相似句子的定义，不是 exact的相似，只要在语法或者语义上相似，那么这个就可以看做相同的样本；然后还使用了 预训练的 模型进行 vocabulary 中词汇的补充。 作者在训练完过后用在Google News dataset上预训练的模型对Vocabulary进行了词汇扩展主要是为了弥补我们的 Decoder 模型中词汇不足的问题。具体的做法就是：(from https://www.cnblogs.com/jiangxinyang/p/9638991.html) 该思路借鉴于Tomas Mikolov的一篇文章Exploiting Similarities among Languages for Machine Translation中解决机器翻译missing words问题的思路，对训练集产生的词汇表V(RNN)进行了扩展，具体的思路可参考Mikolov的文章，达到的效果是建立了大数据集下V(Word2Vec)和本文V(RNN)之间的映射，V(Word2Vec)的规模远远大于V(RNN)，论文中V(RNN)包括了20000个词，V(Word2Vec)包括了930000多个词，成功地解决了这一问题，使得本文提出的无监督模型有大的应用价值。 评价观点这个方法只是适用于长文本，要求是至少有两个衔接的句子，思想和skip-gram 比较相近。 源码google 的实现作者的实现论文 Quick-Thought vectorsintuition: 基于生成的model， the models are trained to reconstruct the surface form of a sentence, but sometimes words are irrelevant to the meaning of the sentence as well 生成模型的计算成本比较高， 而文中的模型是一种判别模型，所以从原理上这种计算的效率就远远高于生成模型。 Viewing generation as choosing a sentence from all possible sentences, this can be seen as a discriminative approximation to the generation problem. 在实践中经验之谈， 不是要求分类器去判别 positive / negative, 只是要求分类器对于 ground-truth contexts than contrastive contexts more plausible。这个比前者在实验结果上是好的。 论文中的一些观点：encoder-decoder based sequence models 虽然效果好，但是 slow to train on large amounts of data. 另一方面, bag-of-words 虽然高效，但是无法捕捉到 word order 信息。 2018年发表的论文An efficient framework for learning sentence representations提出了一种简单且有效的框架用于学习句子表示。和常规的编码解码类模型（如skip-thoughts和SDAE）不同的是，本文采用一种分类器的方式学习句子表示。具体地，模型的输入为一个句子$s$以及一个候选句子集合$S_{cand}$，其中$S_{cand}$包含一个句子$s_{ctxt}$是$s$的上下文句子（也就是$s $)的前一个句子或后一个句子）以及其他不是$s$上下文的句子。模型通过对$s$以及$S_{cand}$中的每个句子进行编码，然后输入到一个分类器中，让分类器选出$S_{cand}$中的哪个句子是$s_{ctxt}$。实验设置候选句子集合大小为3，即$S_{cand}$包含1个上下文句子和两个无关句子。模型结构如下： 模型有如下两个细节需要注意：模型使用的分类器（得分函数）$c$非常简单，是两个向量内积，即$c(u, v)=u^Tv$，计算$s$的embedding与所有$S_{cand}$中的句子向量内积得分后，输入到softmax层进行分类。使用简单分类器是为了引导模型着重训练句子编码器，因为我们的目的是为了得到好的句子向量表示而不是好的分类器。虽然某些监督任务模型如文本蕴含模型是参数共享的，$s$的编码器参数和候选句子编码器参数是不同的（不共享），因为句子表示学习往往是在大规模语料上进行训练，不必担心参数学习不充分的问题 。测试时，给定待编码句子$s$，通过该模型得到的句子表示是两种编码器的连结 $[ f ( s ) ;g ( s ) ]$。 看上去，Skip-thought和Skip-gram挺象。唯一的遗憾是Skip-thought的decoder那部分，它是作为language modeling来处理的。QT针对这个问题，对decoder部分做了大的调整，它直接把decoder拿掉，取而代之的是一个classifier。这个classifier负责预测哪些句子才是context sentences。 QT的classifier取代了Skip-thought的Decoder。这样做的好处是运行的速度大大提升了，用判别问题取代了生成式问题（这个是才是速度提升的原因）。有趣的是，虽然QT出现的比Skip-thought更晚，但是方法更简单，也更加接近Word2Vec算法。 QT是一种新的state-of-art的算法。它不光效果好，而且训练时间要远小于其他算法。在算法方法上和效果上，都可称为是句子表征界的Word2Vec一般的存在。和前面几篇介绍的不同算法放在一起比较，同样都是为了找到好的句子表征，它们采取了不同的路径：InferSent在寻找NLP领域的ImageNet, 它的成功更像是在寻找数据集和任务上的成功，当然它成功的找到了SNLI; Concatenated p-means在寻找NLP领域的convolutional filter; QT则是直接在算法层面上，寻找句子级别的Word2Vec, 算法上的改进让它受益。我们看到不同的方法在不同的方向上都作出了努力和取得了成效，很难讲哪种努力会更有效或者更有潜力。 Supervised Sentence EmbedInferSent来自论文Supervised Learning of Universal Sentence Representations from Natural Language Inference Data，更多信息参考 这里 Multi-task learning Sentence EmbedUniversal Sentence Encoder来自论文 Universal Sentence Encoder，更多信息参考 Universal Sentence Encoder sentence embedding为什么考虑sentence embedding? 语序（语义）在word embedding 中不能体现 word embedding 的效果依赖于分词 SIF 是一种无监督的学习方式，但是最后的效果和NN 是相当的。 训练样本不足的问题 词汇扩展：使用word embedding补充 sentence embedding 集成学习：使用多个不同的模型或者大的数据集进行补充 词向量]]></content>
      <categories>
        <category>NLP</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Unbalanced Datasets Problems]]></title>
    <url>%2F2019%2F06%2F01%2Funbalanced-datasets%2F</url>
    <content type="text"><![CDATA[机器学习中出现的不平衡类别问题的分析、常用的解决方案。 定义不同类别训练数据样本的数量相差很大，导致传统意义上的衡量指标“准确率” 失去了意义。 该问题出现在多个领域中，包括： 信用欺诈 垃圾邮件过滤 疾病筛查 处理方法欠（下）采样（undersampling）对训练集中多数类随机进行下采样，取多数类中的样本使得正例、反例数目接近，然后进行学习。从$S_{majority}$ 中样本数量为 $N_1$, 使得$N_1 =N_{minority}$ 直接这种随机下采样方法是有不足的，比如采样导致原有信息的缺失。所以常常使用EasyEnsemble 算法进行优化。算法步骤： 从多数类中有放回的随机采样$n$ 次，每次选取和少数类别样本相同数目的样本个数，得到 $n$ 个模型 然后每个上述的子集和少数样本合并训练兵训练，可以得到 $n$ 个模型 最终这些模型组合形成一个集成学习系统，模型的结果是这$n$ 个模型的平均值 过（上）采样（oversampling）最简单的方法是（如果没有更多的数据，只需要复制现有的数据，并轻微的变化即可） 图像数据增强镜像翻转、旋转、平移、缩放、颜色随机扰动、非线性几何变形等；GAN生成新样本； 文本数据增强 随机过采样在少数类 $S_{minority}$ 中随机选择一些样本，然后通过复制这些选择的样本得到新的结合，这些集合作为训练中的少数样本数据集。其缺点：容易造成模型的过拟合，因为这些样本都是通过对初始样本复制采样得来，不利于提高模型的泛化性能。 SMOTE（Synthetic Minority Oversampling）算法即，合成少数类过采样技术是对随机过采样的一种改进算法。而SOMT算法的基本思想是对每个少数类样本$x_i$ ，从它的最近邻中随机选择一个样本 $\hat{x_i}$ （ $\hat{x_i}$ 是少数类中的一个样本），然后在$x_i$ 和 $\hat{x_i}$ 之间的连线上随机选择一点作为新合成的少数类样本。算法描述如下：1).对于少数类中的每一个样本 $x_i$，以欧氏距离为标准计算它到少数类样本集 $S_{minority}$中所有样本的距离，得到其k近邻。2).根据样本不平衡比例设置一个采样比例以确定采样倍率 $N$，对于每一个少数类样本 $x_i$ ，从其 $k$近邻中随机选择若干个样本，假设选择的是 $\hat{x_i}$ 。3).对于每一个随机选出来的近邻 $\hat{x_i}$ ，分别与 $x_i$ 按照如下公式构建新的样本。$$x_{new} =x_i +rand(0,1) \times (\hat{x_i} -x_i)$$参考， 有一种代码实现不足之处1). 观察到的数目是及其罕见的类别的时候，就不知所措。2). 每个少数样本都生成新样本，容易发生样本重叠的问题3). 生成机制存在一定的盲目性，可能有些少数的样本并不具有少数样本的代表性 K近邻单词替换这里用到了word embedding工具，在序列模型中，每个单词都能映射成一个词向量。所以一个单词可以看做高维空间中一个样本点，这也就可以用K近邻来得到和它语义相近的单词了。这里放一个代码链接。可以被看做是处理OOV问题的一种手段。 改变评价指标 使用auc 1from sklearn.metrics import roc_auc_score 使用F1 改变模型 基于SVM惩罚算法使用惩罚学习算法增加对少数类别分类错误的代价，一个流行的算法是惩罚性-SVM 1234from sklearn.svm import SVCclf_3 = SVC(kernel='linear', class_weight='balanced', # penalize probability=True) 基于树的算法决策树通常在不平衡的数据集上表现良好，因为这种层序结构允许其从两个类别去学习。而在目前看来，树集合（随机森林、梯度提升树）总是优于单个决策树，所以可以考虑xgboost 之类的框架。 修改损失函数 如果你用的是keras，模型训练函数中是可以调整class_weight的，可以在class_weight中适当增大正样本的权重。比较忌讳把正样本权重增大到两者损失总量一样，这么设置从来没有一次效果是好的。笔者一般把正样本权重调到负样本权重的1.1~1.5倍，可以取得比之前要好的F1 score。 Focal Loss是个值得考虑的目标函数，论文：Focal Loss for Dense Object Detection。该损失函数在目标检测领域取得了良好的处理类别不平衡效果和改善误分类的效果，笔者在文本分类的任务中用了该目标函数，也能取得较大提升。 参考如何处理机器学习中的不平衡类别分类中解决类别不平衡问题 how to handle unbalanced data对于这类问题是可以从数据和模型来进行考虑的。 Imbalanced data typically refers to a problem with classification problems where the classes are not represented equally. The accuracy paradox is the name for the exact situation in the introduction to this post. Data approachOversample minority class and Undersample majority class Over-sampling increases the number of minority class members in the training set. The advantage of over-sampling is that no information from the original training set is lost, as all observations from the minority and majority classes are kept. On the other hand, it is prone to overfitting. (You can add copies of instances from the under-represented class called over-sampling or more formally sampling with replacement) Under-sampling, on contrary to over-sampling, aims to reduce the number of majority samples to balance the class distribution. Since it is removing observations from the original data set, it might discard useful information. (You can delete instances from the over-represented class, called under-sampling.) Some Rules of Thumb Consider testing under-sampling when you have an a lot data (tens- or hundreds of thousands of instances or more) Consider testing over-sampling when you don’t have a lot of data (tens of thousands of records or less) Consider testing random and non-random (e.g. stratified) sampling schemes. Consider testing different resampled ratios (e.g. you don’t have to target a 1:1 ratio in a binary classification problem, try other ratios) Try Different Algorithms基于树的这种结构的模型还是表现比较给力的。That being said, decision trees often perform well on imbalanced datasets. The splitting rules that look at the class variable used in the creation of the trees, can force both classes to be addressed. Try Penalized ModelsPenalized classification imposes an additional cost on the model for making classification mistakes on the minority class during training. These penalties can bias the model to pay more attention to the minority class. Try Changing Your Performance Metric 使用 precision and recall curves or F1 去评价你的网络效果 Precision: A measure of a classifiers exactness. Recall: A measure of a classifiers completeness F1 Score (or F-score): A weighted average of precision and recall. GBC参数这些参数中，类似于Adaboost，我们把重要参数分为两类，第一类是Boosting框架的重要参数，第二类是弱学习器即CART回归树的重要参数。n_estimators: 也就是弱学习器的最大迭代次数，或者说最大的弱学习器的个数。learning_rate: 即每个弱学习器的权重缩减系数ν，也称作步长对于分类模型，有对数似然损失函数”deviance”和指数损失函数”exponential”两者输入选择。默认是对数似然损失函数”deviance”。一般来说，推荐使用默认的”deviance”。它对二元分离和多元分类各自都有比较好的优化。而指数损失函数等于把我们带到了Adaboost算法。对于回归模型，有均方差”ls”, 绝对损失”lad”, Huber损失”huber”和分位数损失“quantile”。默认是均方差”ls”。一般来说，如果数据的噪音点不多，用默认的均方差”ls”比较好。如果是噪音点较多，则推荐用抗噪音的损失函数”huber”。而如果我们需要对训练集进行分段预测的时候，则采用“quantile”。max_features:可以使用很多种类型的值，默认是”None”,意味着划分时考虑所有的特征数.subsample: 选择小于1的比例可以减少方差，即防止过拟合，但是会增加样本拟合的偏差，因此取值不能太低。推荐在[0.5, 0.8]之间，默认是1.0，即不使用子采样。 参考文献 GBC参数设置ROC曲线和AUC值Introduction to Python Ensembles]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>unbalanced_datasets</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机基础（2）]]></title>
    <url>%2F2019%2F05%2F24%2Fbasics-of-cs2%2F</url>
    <content type="text"><![CDATA[计算机中的基础知识，比如计算机网络、软件工程。 计算机网络常用的命令 pingping 命令式用来测试TCP/IP 网络是否畅通或者网络连接速度的命令。 网络连通性测试的“4部曲”： （1）输入“ping 127.0.0.1”命令，此命令用于检查本机的TCP/IP 协议安装是否正确，凡是以127 开头的IP地址都代表本机。（2）输入“ping 本机IP地址命令”，此命令用于检查本机的服务和网络适配器的绑定是否正确。（3）输入“ping 网关IP地址”命令，此命令用来检查本机和网关的连接是否正常。（4）输入“ping 远程主机IP地址”命令，此命令用来检查网关能否将数据包转发出去。 ipconfig 设置和查看网络接口工具 net 命令 该命令的子命令那个，可在命令提示符窗口中执行net/?命令。如果要查看子命令的相关帮助，可在命令提示符窗口中执行net help 命令即可。例如输入net help accounts 命令即可得到accouns 的相关帮助。 tracert tracert 命令可以用来跟踪数据包使用的路由（路径）。该使用程序跟踪的路径是源计算机到目的地的一条路径，不能保证或认为数据包总遵循这个路径。 netstat 命令 一般用于检测本机各端口的网络连接情况 ARP 查看和设置地址解析协议表项工具 。地址解析作用：是把目的主机的IP地址解析为目的主机的MAC地址。 route 查看和设置路由表的表项工具 nslookup （name sever lookup）域名查询 查看域名工具 ftp ftp 命令：用于文件传输的命令，是现在两台互连的机器之间传送文件，该命令与常用的FTP配置软件是一样的 talent 远程访问命令，为应用层协议远程登录访问的应用 nslookup nslookup命令用于查询DNS的记录，查看域名解析是否正常，在网络故障的时候用来诊断网络问题。 Http和Https的区别 Http协议运行在TCP之上，明文传输，客户端与服务器端都无法验证对方的身份；Https是身披SSL(Secure Socket Layer)外壳的Http，运行于SSL上，SSL运行于TCP之上，是添加了加密和认证机制的HTTP。二者之间存在如下不同： 端口不同：Http与Http使用不同的连接方式，用的端口也不一样，http是80，https是443； 资源消耗：和HTTP通信相比，Https通信会由于加减密处理消耗更多的CPU和内存资源； 开销：Https通信需要证书，而证书一般需要向认证机构购买； Https的加密机制是一种共享密钥加密和公开密钥加密并用的混合加密机制。 对称加密与非对称加密 对称密钥加密是指加密和解密使用同一个密钥的方式，这种方式存在的最大问题就是密钥发送问题，即如何安全地将密钥发给对方；而非对称加密是指使用一对非对称密钥，即公钥和私钥，公钥可以随意发布，但私钥只有自己知道。发送密文的一方使用对方的公钥进行加密处理，对方接收到加密信息后，使用自己的私钥进行解密。 由于非对称加密的方式不需要发送用来解密的私钥，所以可以保证安全性；但是和对称加密比起来，它非常的慢，所以我们还是要用对称加密来传送消息，但对称加密所使用的密钥我们可以通过非对称加密的方式发送出去。 三次握手和四次握手 第一次握手：Client将标志位SYN置为1，随机产生一个值seq=J，并将该数据包发送给Server，Client进入SYN_SENT状态，等待Server确认。 第二次握手：Server收到数据包后由标志位SYN=1知道Client请求建立连接，Server将标志位SYN和ACK都置为1，ack=J+1，随机产生一个值seq=K，并将该数据包发送给Client以确认连接请求，Server进入SYN_RCVD状态。 第三次握手：Client收到确认后，检查ack是否为J+1，ACK是否为1，如果正确则将标志位ACK置为1，ack=K+1，并将该数据包发送给Server，Server检查ack是否为K+1，ACK是否为1，如果正确则连接建立成功，Client和Server进入ESTABLISHED状态，完成三次握手，随后Client与Server之间可以开始传输数据了。 第一次挥手：Client发送一个FIN，用来关闭Client到Server的数据传送，Client进入FIN_WAIT_1状态。 第二次挥手：Server收到FIN后，发送一个ACK给Client，确认序号为收到序号+1（与SYN相同，一个FIN占用一个序号），Server进入CLOSE_WAIT状态。此时TCP链接处于半关闭状态，即客户端已经没有要发送的数据了，但服务端若发送数据，则客户端仍要接收。 第三次挥手：Server发送一个FIN，用来关闭Server到Client的数据传送，Server进入LAST_ACK状态。 第四次挥手：Client收到FIN后，Client进入TIME_WAIT状态，接着发送一个ACK给Server，确认序号为收到序号+1，Server进入CLOSED状态，完成四次挥手。 为什么是三次握手，四次挥手？ 发一次信息就是一次握手。确认位 ACK：ACK=1 表明对方发送的报文段是确认报文段。同步位 SYN：SYN=1 表明这是连接请求或者连接接受报文段。终止位 FIN；FIN=1 表明发送完毕的一方要释放连接。 三次握手的时候，第一次握手说明的是客户端想和服务器端建立连接的意愿，只有当第二次握手才证明客户端到服务器端是连通的；第三次握手的时候，说明服务器端到客户端是连通的。 连接时候，当服务器收到客户端的 SYN 报文时候，可以直接发送 SYN+ACK 报文，也就是应答+ 同步。但是当关闭连接时候，当服务器端收到FIN 报文时候，很可能需要继续向客户端发送数据，所有这个时候，先回复一个ACK 报文。只有当服务器端所有的报文都发送完了，然后才向客户端发送FIN 报文，因此是不能一起发送，所以是需要四次挥手。TCP 是支持全双工的，也就说服务器和客户端都是可以读写，当服务器端收到FIN 报文，意味着没有数据报文再发过来，但是还可以继续发送报文。 TCP协议如何来保证传输的可靠性 数据包校验：目的是检测数据在传输过程中的任何变化，若校验出包有错，则丢弃报文段并且不给出响应，这时TCP发送数据端超时后会重发数据； 对失序数据包重排序：既然TCP报文段作为IP数据报来传输，而IP数据报的到达可能会失序，因此TCP报文段的到达也可能会失序。TCP将对失序数据进行重新排序，然后才交给应用层； 丢弃重复数据：对于重复数据，能够丢弃重复数据； 应答机制：当TCP收到发自TCP连接另一端的数据，它将发送一个确认。这个确认不是立即发送，通常将推迟几分之一秒； 超时重发：当TCP发出一个段后，它启动一个定时器，等待目的端确认收到这个报文段。如果不能及时收到一个确认，将重发这个报文段； 流量控制：TCP连接的每一方都有固定大小的缓冲空间。TCP的接收端只允许另一端发送接收端缓冲区所能接纳的数据，这可以防止较快主机致使较慢主机的缓冲区溢出，这就是流量控制。TCP使用的流量控制协议是可变大小的滑动窗口协议。 Get与POST的区别 GET与POST是我们常用的两种HTTP Method，二者之间的区别主要包括如下五个方面： (1). 从功能上讲，GET一般用来从服务器上获取资源，POST一般用来更新服务器上的资源； (2). 从REST服务角度上说，GET是幂等的，即读取同一个资源，总是得到相同的数据，而POST不是幂等的，因为每次请求对资源的改变并不是相同的；进一步地，GET不会改变服务器上的资源，而POST会对服务器资源进行改变； (3). 从请求参数形式上看，GET请求的数据会附在URL之后，即将请求数据放置在HTTP报文的 请求头 中，以?分割URL和传输数据，参数之间以&amp;相连。特别地，如果数据是英文字母/数字，原样发送；否则，会将其编码为 application/x-www-form-urlencoded MIME 字符串(如果是空格，转换为+，如果是中文/其他字符，则直接把字符串用BASE64加密，得出如：%E4%BD%A0%E5%A5%BD，其中％XX中的XX为该符号以16进制表示的ASCII)；而POST请求会把提交的数据则放置在是HTTP请求报文的 请求体 中。 (4). 就安全性而言，POST的安全性要比GET的安全性高，因为GET请求提交的数据将明文出现在URL上，而且POST请求参数则被包装到请求体中，相对更安全。 (5). 从请求的大小看，GET请求的长度受限于浏览器或服务器对URL长度的限制，允许发送的数据量比较小，而POST请求则是没有大小限制的。 TCP与UDP的区别 TCP (Transmission Control Protocol)和UDP(User Datagram Protocol)协议属于传输层协议，它们之间的区别包括： TCP是面向连接的，UDP是无连接的； TCP是可靠的，UDP是不可靠的； TCP只支持点对点通信，UDP支持一对一、一对多、多对一、多对多的通信模式； TCP是面向字节流的，UDP是面向报文的； TCP有拥塞控制机制;UDP没有拥塞控制，适合媒体通信； TCP首部开销(20个字节)比UDP的首部开销(8个字节)要大； TCP的拥塞处理 慢启动：不要一开始就发送大量的数据，先探测一下网络的拥塞程度，也就是说由小到大逐渐增加拥塞窗口的大小; 拥塞避免：拥塞避免算法让拥塞窗口缓慢增长，即每经过一个往返时间RTT就把发送方的拥塞窗口cwnd加1，而不是加倍，这样拥塞窗口按线性规律缓慢增长 快重传：快重传要求接收方在收到一个 失序的报文段 后就立即发出 重复确认（为的是使发送方及早知道有报文段没有到达对方）而不要等到自己发送数据时捎带确认。快重传算法规定，发送方只要一连收到三个重复确认就应当立即重传对方尚未收到的报文段，而不必继续等待设置的重传计时器时间到期。 快恢复：快重传配合使用的还有快恢复算法，当发送方连续收到三个重复确认时，就执行“乘法减小”算法，把ssthresh门限减半，但是接下去并不执行慢开始算法：因为如果网络出现拥塞的话就不会收到好几个重复的确认，所以发送方现在认为网络可能没有出现拥塞。所以此时不执行慢开始算法，而是将cwnd设置为ssthresh的大小，然后执行拥塞避免算法。 ARP是地址解析协议 DNS（Domain Name System）域名系统 当DNS客户机需要在程序中使用名称时，它会查询DNS服务器来解析该名称。客户机发送的每条查询信息包括三条信息：包括：指定的DNS域名，指定的查询类型，DNS域名的指定类别。基于UDP服务，端口53.该应用一般不直接为用户使用，而是为其他应用服务，如HTTP，SMTP等在其中需要完成主机名到IP地址的转换。 TCP和UDP分别对应的常见应用层协议 TCP对应的应用层协议 FTP：定义了文件传输协议，使用21端口。常说某某计算机开了FTP服务便是启动了文件传输服务。下载文件，上传主页，都要用到FTP服务。 Telnet：它是一种用于远程登陆的端口，用户可以以自己的身份远程连接到计算机上，通过这种端口可以提供一种基于DOS模式下的通信服务。如以前的BBS是-纯字符界面的，支持BBS的服务器将23端口打开，对外提供服务。 SMTP：定义了简单邮件传送协议，现在很多邮件服务器都用的是这个协议，用于发送邮件。如常见的免费邮件服务中用的就是这个邮件服务端口，所以在电子邮件设置-中常看到有这么SMTP端口设置这个栏，服务器开放的是25号端口。 POP3：它是和SMTP对应，POP3用于接收邮件。通常情况下，POP3协议所用的是110端口。也是说，只要你有相应的使用POP3协议的程序（例如Fo-xmail或Outlook），就可以不以Web方式登陆进邮箱界面，直接用邮件程序就可以收到邮件（如是163邮箱就没有必要先进入网易网站，再进入自己的邮-箱来收信）。 HTTP：从Web服务器传输超文本到本地浏览器的传送协议。 UDP对应的应用层协议 DNS：用于域名解析服务，将域名地址转换为IP地址。DNS用的是53号端口。 SNMP：简单网络管理协议，使用161号端口，是用来管理网络设备的。由于网络设备很多，无连接的服务就体现出其优势。 TFTP(Trival File Transfer Protocal)：简单文件传输协议，该协议在熟知端口69上使用UDP服务。 Java程序从源文件创建到程序运行要经过两大步骤： 1、源文件由编译器编译成字节码（ByteCode）； 2、字节码由java虚拟机解释运行。因为java程序既要编译同时也要经过JVM的解释运行，所以说Java被称为半解释语言 应用层：HTTP、SMTP、FTP、Telnet、DNS、DHCP、SSH TELNET 表示层：ASCII、MPEG、JPEG、MIDI 会话层：NetBIOS、SAP、SDP、NWLink 传输层：TCP、 UDP、OSPF、SPX 网络层：IP、IPX、 ICMP、 ARP 数据链路层：Ethernet、Token Ring、FDDI、AppleTalk 数据链路层 “链接层”的功能，它在”实体层”的上方，确定了0和1的分组方式。以太网规定，一组电信号构成一个数据包，叫做”帧”（Frame）。每一帧分成两个部分：标头（Head）和数据（Data）。以太网规定，连入网络的所有设备，都必须具有”网卡”接口。数据包必须是从一块网卡，传送到另一块网卡。网卡的地址，就是数据包的发送地址和接收地址，这叫做MAC地址。每块网卡出厂的时候，都有一个全世界独一无二的MAC地址，长度是48个二进制位，通常用12个十六进制数表示。 首先，一块网卡怎么会知道另一块网卡的MAC地址？回答是有一种ARP协议，可以解决这个问题。这个留到后面介绍，这里只需要知道，以太网数据包必须知道接收方的MAC地址，然后才能发送。以太网采用了一种很”原始”的方式，它不是把数据包准确送到接收方，而是向本网络内所有计算机发送，让每台计算机自己判断，是否为接收方。 上面是处理子网内部的寻找问题，那么如何去处理子网之间的呢？这就导致了”网络层”的诞生。它的作用是引进一套新的地址，使得我们能够区分不同的计算机是否属于同一个子网络。这套地址就叫做”网络地址”，简称”网址”。每台计算机有了两种地址，一种是MAC地址，另一种是网络地址。两种地址之间没有任何联系，MAC地址是绑定在网卡上的，网络地址则是管理员分配的，它们只是随机组合在一起。 总结一下，IP协议的作用主要有两个，一个是为每一台计算机分配IP地址，另一个是确定哪些地址在同一个子网络。 网络层 传输层的由来 也就是说，我们还需要一个参数，表示这个数据包到底供哪个程序（进程）使用。这个参数就叫做”端口”（port），它其实是每一个使用网卡的程序的编号。每个数据包都发到主机的特定端口，所以不同的程序就能取到自己所需要的数据。 应用层 “应用层”的作用，就是规定应用程序的数据格式。 举例来说，TCP协议可以为各种各样的程序传递数据，比如Email、WWW、FTP等等。那么，必须有不同协议规定电子邮件、网页、FTP数据的格式，这些应用程序协议就构成了”应用层”。 用户的上网设置 1234 * 本机的IP地址 * 子网掩码 * 网关的IP地址 * DNS的IP地址 所谓”动态IP地址”，指计算机开机后，会自动分配到一个IP地址，不用人为设定。它使用的协议叫做DHCP协议。首先，它是一种应用层协议，建立在UDP协议之上。 我们知道，发送数据包，必须要知道对方的IP地址。但是，现在，我们只知道网址www.google.com，不知道它的IP地址。DNS协议可以帮助我们，将这个网址转换成IP地址。已知DNS服务器为8.8.8.8，于是我们向这个地址发送一个DNS数据包（53端口）。TCP数据包的标头长度为20字节，加上嵌入HTTP的数据包，总长度变为4980字节。 浏览网页用的是HTTP协议 TCP数据包需要设置端口，接收方（Google）的HTTP端口默认是80，发送方（本机）的端口是一个随机生成的1024-65535之间的整数，假定为51775。 这两篇文章是可以给你一个overview的。 互联网协议入门（一）互联网协议入门（二） 地址解析协议，即ARP（Address Resolution Protocol），是根据IP地址获取物理地址的一个TCP/IP协议。主机发送消息时将包含目标IP地址的ARP请求广播道网络上的所有主机，并接受返回消息，以此确定目标的物理地址；收到返回消息后将该IP地址和物理地址存入本机ARP缓存中并保留一定时间，下次请求时直接查询ARP缓存以节约资源。ARP命令可用于查询本机ARP缓存中IP地址和MAC地址的对应关系、添加或删除静态对应关系等。RARP协议：逆地址解析协议，即RARP，功能和ARP协议相对，其将局域网中某个主机的物理地址转换为IP地址 常见的路由选择协议有：RIP协议、OSRF协议RIP协议：底层是贝尔曼福特算法，它选择路由的度量标准（metric)是跳数，最大跳数是15跳，如果大于15跳，它就会丢弃数据包。OSPF协议：Open Shortest Path First开放式最短路径优先，底层是迪杰斯特拉算法，是链路状态路由选择协议，它选择路由的度量标准是带宽，延迟。 TCP/IP协议是Internet最基本的协议、Internet国际互联网络的基础，由网络层的IP协议和传输层的TCP协议组成。通俗而言：TCP负责发现传输的问题，一有问题就发出信号，要求重新传输，直到所有数据安全正确地传输到目的地。而IP是给因特网的每一台联网设备规定一个地址。使用TCP/IP 协议的包括：FTP（文件传输协议）、Telnet（远程登录协议）、SMTP（简单邮件传输协议）、POP3（和SMTP相对，用于接收邮件）、HTTP协议等。 UDP用户数据报协议，是面向无连接的通讯协议，UDP数据包括目的端口号和源端口号信息，由于通讯不需要连接，所以可以实现广播发送。UDP通讯时不需要接收方确认，属于不可靠的传输，可能会出现丢包现象，实际应用中要求程序员编程验证。使用UDP协议包括：TFTP（简单文件传输协议）、SNMP（简单网络管理协议）、DNS（域名解析协议）、NFS、BOOTP TCP与UDP的区别：TCP是面向连接的，可靠的字节流服务；UDP是面向无连接的，不可靠的数据报服务 DNS是域名系统(DomainNameSystem)的缩写，该系统用于命名组织到域层次结构中的计算机和网络服务，可以简单地理解为将URL转换为IP地址。域名是由圆点分开一串单词或缩写组成的，每一个域名都对应一个惟一的IP地址，在Internet上域名与IP地址之间是一一对应的，DNS就是进行域名解析的服务器。DNS命名用于Internet等TCP/IP网络中，通过用户友好的名称查找计算机和服务。 NAT网络地址转换(Network Address Translation)属接入广域网(WAN)技术，是一种将私有（保留）地址转化为合法IP地址的转换技术，它被广泛应用于各种类型Internet接入方式和各种类型的网络中。原因很简单，NAT不仅完美地解决了lP地址不足的问题，而且还能够有效地避免来自网络外部的攻击，隐藏并保护网络内部的计算机。 DHCP动态主机设置协议（Dynamic Host Configuration Protocol）是一个局域网的网络协议，使用UDP协议工作，主要有两个用途：给内部网络或网络服务供应商自动分配IP地址，给用户或者内部网络管理员作为对所有计算机作中央管理的手段。 超文本传输协议（HTTP，HyperText Transfer Protocol)是互联网上应用最为广泛的一种网络协议。所有的WWW文件都必须遵守这个标准。 在浏览器输入www.baidu.com后执行的全部过程（比较好的解释可以参考[这里](http://www.ruanyifeng.com/blog/2012/06/internet_protocol_suite_part_ii.html)） 1）客户端浏览器通过DNS解析到www.baidu.com的IP地址220.181.27.48，通过这个IP地址找到客户端到服务器的路径。客户端浏览器发起一个HTTP会话到220.161.27.48，然后通过TCP进行封装数据包，输入到网络层。 2）在客户端的传输层，把HTTP会话请求分成报文段，添加源和目的端口，如服务器使用80端口监听客户端的请求，客户端由系统随机选择一个端口如5000，与服务器进行交换，服务器把相应的请求返回给客户端的5000端口。然后使用IP层的IP地址查找目的端。 3）客户端的网络层不用关系应用层或者传输层的东西，主要做的是通过查找路由表确定如何到达服务器，期间可能经过多个路由器，这些都是由路由器来完成的工作，不作过多的描述，无非就是通过查找路由表决定通过那个路径到达服务器。 4）客户端的链路层，包通过链路层发送到路由器，通过邻居协议查找给定IP地址的MAC地址，然后发送ARP请求查找目的地址，如果得到回应后就可以使用ARP的请求应答交换的IP数据包现在就可以传输了，然后发送IP数据包到达服务器的地址。 互联网的本质就是一系列的网络协议 数据就是这样在计算机和网络中进行传递的。这其中做的工作就是每层进行层层解包和附加自己所要传递的信息，术语叫做报头。在四层，既传输层数据被称作段（Segments）；三层网络层数据被称做包（Packages）；二层数据链路层时数据被称为帧（Frames）；一层物理层时数据被称为比特流（Bits）。 TCP/IP 模型将 OSI 模型由七层简化为四层，传输层和网络层被完整保留，因此网络中最核心的技术就是传输层和网络层技术。TCP/IP 协议中每层技术举例：网络访问层：ARP、RARP互联网层：ICMP、IP传输层：TCP、UDP应用层：DNS、FTP、HTTP、SMTP、TELNET、IRC、WHOIS 交换机是一种基于MAC地址识别，能完成封装转发数据包功能的网络设备。交换机可以“学习”MAC地址，并把其存放在内部地址表中，通过在数据帧的始发者和目标接收者之间建立临时的交换路径，使数据帧直接由源地址到达目的地址。网络七层模型是一个标准，而非实现。 网络四层模型是一个实现的应用模型。 网络四层模型由七层模型简化合并而来。 TCP支持的应用协议主要有：Telnet、FTP、SMTP等;UDP支持的应用层协议主要有：NFS(网络文件系统)、SNMP(简单网络管理协议)、DNS(主域名称系统)、TFTP(通用文件传输协议)等。 集线器与路由器在功能上有什么不同? 首先说HUB,也就是集线器。它的作用可以简单的理解为将一些机器连接起来组成一个局域网。而交换机（又名交换式集线器）作用与集线器大体相同。但是两者在性能上有区别：集线器采用的式共享带宽的工作方式，而交换机是独享带宽。这样在机器很多或数据量很大时，两者将会有比较明显的。而路由器与以上两者有明显区别，它的作用在于连接不同的网段并且找到网络中数据传输最合适的路径。路由器是产生于交换机之后，就像交换机产生于集线器之后，所以路由器与交换机也有一定联系，不是完全独立的两种设备。路由器主要克服了交换机不能路由转发数据包的不足。 总的来说，路由器与交换机的主要区别体现在以下几个方面： （1）工作层次不同最初的的交换机是工作在数据链路层，而路由器一开始就设计工作在网络层。由于交换机工作在数据链路层，所以它的工作原理比较简单，而路由器工作在网络层，可以得到更多的协议信息，路由器可以做出更加智能的转发决策。 （2）数据转发所依据的对象不同交换机是利用物理地址或者说MAC地址来确定转发数据的目的地址。而路由器则是利用IP地址来确定数据转发的地址。IP地址是在软件中实现的，描述的是设备所在的网络。MAC地址通常是硬件自带的，由网卡生产商来分配的，而且已经固化到了网卡中去，一般来说是不可更改的。而IP地址则通常由网络管理员或系统自动分配。 （3）传统的交换机只能分割冲突域，不能分割广播域；而路由器可以分割广播域由交换机连接的网段仍属于同一个广播域，广播数据包会在交换机连接的所有网段上传播，在某些情况下会导致通信拥挤和安全漏洞。连接到路由器上的网段会被分配成不同的广播域，广播数据不会穿过路由器。虽然第三层以上交换机具有VLAN功能，也可以分割广播域，但是各子广播域之间是不能通信交流的，它们之间的交流仍然需要路由器。 （4）路由器提供了防火墙的服务路由器仅仅转发特定地址的数据包，不传送不支持路由协议的数据包传送和未知目标网络数据包的传送，从而可以防止广播风暴。 软件工程黑盒测试 白盒测试 和灰盒测试 黑盒测试：也称功能测试、数据驱动测试，它将被测软件看作一个打不开的黑盒，主要根据功能需求设计测试用例，进行测试。 常用的方法： 等价类划分法等价类划分法是一种典型的、重要的黑盒测试方法，它将程序所有可能的输入数据划分为若干个等价类。然后从每个部分中选取具有代表性的数据当做测试用例。测试用例由有效等价类和无效等价类的代表数据组成，从而保证测试用例具有完整性和代表性。使用该方法设计测试用例主要有两个步骤：(1)确定等价类；(2)生成测试用例。 边界值分析法边界值分析法是对程序输入或输出的边界值进行测试的一种黑盒测试方法。实际的测试工作证明，考虑了边界条件的测试用例比那些没有考虑边界条件的测试用例具有更高的测试回报率。这里所说的边界条件，是指输入和输入等价类中那些恰好处于边界、或超过边界、或在边界以下的状态。 因果图法因果图法也是较常用的一种黑盒测试方法，是一种简化了的逻辑图。因果图能直观地表明输入条件和输出动作之间的因果关系，能帮助测试人员把注意力集中到与程序功能有关的输入组合上。因果图法是一种适合于描述对于多种输入条件组合的测试方法，根据输入条件的组合、约束关系和输出条件的因果关系，分析输入条件的各种组合情况，从而设计测试用例的方法，它适合于检查程序输入条件的各种组合情况 错误推测法错误推测法是基于以往的经验和直觉，参照以往的软件系统出现的错误，推测当前被测程序中可能存在的缺陷和错误，有针对性地设计测试用例。 等价类划分法是一种典型的黑盒测试用例设计方法。采用等价类划分法时，完全不用考虑程序内部结构，设计测试用例的唯一依据是软件需求规格说明书。所谓等价类，是输入条件的一个子集合，该输入集合中的数据对于揭示程序中的错误是等价的。等价类又分为有效等价类和无效等价类。有效等价类代表对程序有效的输入，而无效等价类则是其他任何可能的输入（即不正确的输入值）。有效等价类和无效等价类都是使用等价类划分法设计用例时所必须的，因为被测程序若是正确的，就应该既能接受有效的输入，也能接受无效输入的考验。 白盒测试：也称结构测试或逻辑驱动测试，它是知道产品内部工作过程，可通过测试来检测产品内部动作是否按照规格说明书的规定正常进行，按照程序内部的结构测试程序，检验程序中的每条通路是否都有能按预定要求正确工作，而不顾它的功能。 白盒测试的方法有三种，一是程序结构分析，根据源代码可以首先绘制程序的流程图，然后根据流程图分析程序的结构。二是逻辑覆盖方测试，根据程序的内部结构，对所有的路径进行测试，是一种穷举路径的测试方法。三是基本路径测试，根据程序的逻辑判断，分析程序中的路径，再进行用例的设计。 灰盒测试，确实是介于二者之间的，可以这样理解，灰盒测试关注输出对于输入的正确性，同时也关注内部表现，但这种关注不象白盒那样详细、完整，只是通过一些表征性的现象、事件、标志来判断内部的运行状态，有时候输出是正确的，但内部其实已经错误了，这种情况非常多，如果每次都通过白盒测试来操作，效率会很低，因此需要采取这样的一种灰盒的方法。]]></content>
      <categories>
        <category>CS基础</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[leetcode string]]></title>
    <url>%2F2019%2F05%2F24%2Fleetcode-string%2F</url>
    <content type="text"><![CDATA[cout and say 12345678910111213141516171819202122232425262728293031#include&lt;iostream&gt;#include&lt;string&gt;using namespace std;string cout_and_say(int n)&#123; string res ="1"; // 这个初始化意味着需要少循环一次， while ( --n ) &#123; string tmp; for(int i =0; i&lt; res.size() ; i++) &#123; int j =i; // 常用的一种遍历手段 while(j &lt; res.size() &amp;&amp; res[j] ==res[i]) j +=1; tmp =to_string(j -i) +res[i]; i =j -1; &#125; res =tmp; &#125; return res;&#125;int main()&#123; int n ; cin &gt;&gt;n; string res =cout_and_say(n); for(auto u: res) cout &lt;&lt; u; cout &lt;&lt; endl; return 0;&#125; group anagrams sort 函数是 in-place() 的操作。 12345678910111213141516171819class Solution &#123;public: // 思路很简单， 先是放到一个 hash，然后再遍历一遍dictionary vector&lt;vector&lt;string&gt;&gt; groupAnagrams(vector&lt;string&gt;&amp; strs) &#123; unordered_map&lt;string, vector&lt;string&gt;&gt; hash; for(auto str : strs) &#123; string key =str;// sort 函数是 in-place() 的操作 sort(key.begin(), key.end()); hash[key].push_back(str); &#125; vector&lt;vector&lt;string&gt;&gt; res ; for (auto item : hash) &#123; res.push_back(item.second); &#125; return res; &#125;&#125;; reverse words in a sting LeetCode题目链接 c++ 中绝大部分都是左闭右开， 比如说reverse() . 1234567891011121314151617181920212223242526272829303132#include&lt;iostream&gt;#include&lt;string&gt;using namespace std;string reverse_string(string str)&#123; string res; int n =str.size() ; int k =0; for(int i =0; i&lt; n; i++) &#123; while (i &lt; n &amp;&amp; str[i] ==' ') i ++; if (i &gt; n) break; int j =i ; while(j &lt; n &amp;&amp; str[j] != ' ') j ++; reverse(str.begin() +i, str.begin() +j); if(k ) str[k ++] =' '; // 赋值回来, 这个语句是非常秒的, 把 i 的index 补充了回来 while(i &lt; j) str[k ++] =str[i ++]; &#125; str.erase(str.begin() +k , str.end()); reverse(str.begin(), str.end()); return str;&#125;int main()&#123; string input =" the sky is blue"; string res =reverse_string(input); for(auto u : res) cout&lt;&lt; u; cout &lt;&lt; endl; return 0;&#125; compare-version-numbers 123456789101112131415161718192021222324252627282930#include&lt;iostream&gt;#include&lt;string&gt;using namespace std;int cmpVersion(string str1, string str2)&#123; int n =str1.size() , m = str2.size(); int i =0, j =0; while(i &lt; n || j &lt; m) &#123; int x =i, y =j; while( x&lt; n &amp;&amp; str1[x] != '.') x ++; while( y&lt; m &amp;&amp; str2[y ] != '.') y ++; int sum1 = x ==i ? 0 : atoi(str1.substr(i, x -i).c_str()); int sum2 = x ==j ? 0: atoi(str2.substr(j, y -j).c_str()); i =x +1, j = y +1; if(sum1&lt; sum2) return -1; if (sum1 &gt; sum2) return 1; &#125; return 0;&#125;int main()&#123; string str1; // 0.1 string str2; //1.1 getline(cin, str1); getline(cin, str2); cout&lt;&lt; cmpVersion(str1, str2)&lt;&lt; endl; return 0;&#125; Unique Email Addresses 这里给出了一种如何去读入 vector，最简单的方法就是多定义一个循环，这样的话就可以完整的读入所有的string，可能再最后的结果中有 +1 或者 -1 的操作。最后看效果，稍微猜测推一下。 12345678910111213141516171819202122232425262728293031323334353637383940414243#include&lt;iostream&gt;#include&lt;unordered_set&gt;#include&lt;string&gt;#include&lt;vector&gt;using namespace std;// 比较简单 ，分成 name 和 domain 两个方面进行处理int numUniqueEmails(vector&lt;string&gt; emails)&#123; unordered_set&lt;string&gt; hash; for(auto email: emails) &#123; int at =email.find("@"); string name =email.substr(0, at); // 写成 email.begin() 吧 string domain =email.substr(at +1);// 默认是到最后的，如果只是有一个参数的话 string tmp; for(auto ch : name) &#123; if (ch =='+') break; else if(ch != '.') tmp += ch; &#125; string res =tmp+'@'+ domain; hash.insert(res); &#125; return hash.size()-1;&#125;int main()&#123; //vector&lt;string&gt; emails=&#123;"a@a.com", "b@b.com"&#125;; vector&lt;string&gt; emails; // 对于字符串 数组的输入是不不会的 int n ; cin &gt;&gt; n; n ++; while(n --) &#123; string tmp; getline(cin, tmp); emails.push_back(tmp); &#125; cout &lt;&lt; numUniqueEmails(emails)&lt;&lt; endl; return 0;&#125; longest palindromic substring 马拉车算法可以在 $O(n)$ 时间复杂度，但只是解决回文串问题，所以就不建议学，太局限。不像是kmp 算法。选定一个中心点，然后分别左右进行遍历。还需要考虑回文串中的奇偶性的问题。使用双指针算法就可以搞定的。 12345678910111213141516171819202122class Solution &#123;public: // 最长回文串，双指针算法，最后的时间复杂度是O(n^2) // 先是枚举中心对称点，然后向着左右进行扩展 string longestPalindrome(string s) &#123; string res; for(int i =0; i&lt; s.size() ; i++) &#123; // 然后双指针进行扩展, 如果有奇数个 for(int j =i, k =i; j&gt;=0 &amp;&amp; k&lt; s.size() &amp;&amp; s[k] ==s[j]; j --, k++) &#123; if( res.size() &lt; k -j +1) res =s.substr(j, k -j +1); &#125; // 如果有偶数个 for(int j =i, k =i +1; j&gt;=0 &amp;&amp; k&lt; s.size() &amp;&amp; s[k] ==s[j]; j --, k++) if(res.size() &lt; k-j +1) res =s.substr(j, k -j +1); &#125; return res; &#125;&#125;; zigzag conversion 好好理解一下，当for 循环的时候， 第三个参数就是等差数列中公差。for 循环是可以模拟 等差数列和等比数列的，初始化，判别条件和步伐是能够和公差公比相对应的。 12345678910111213141516171819202122232425262728class Solution &#123;public: // 找规律的题目 // 如果是首行和尾行，公差是 2(n -1)， // 如果是中间行, 那么是两个交错的等差数列 string convert(string s, int n) &#123; if (n ==1) return s; string res; for(int i =0; i&lt; n; i++) &#123; if(! i || i ==n-1 ) &#123; for(int j =i; j&lt; s.size(); j += 2*(n -1)) res += s[j]; &#125; else &#123; // 两个交叉的数列, 只是初始化的值需要注意一下 for(int j =i, k =2*(n-1) -i ; j &lt; s.size() || k &lt; s.size() ; j += 2*(n -1) , k += 2*(n -1) ) &#123; if(j &lt; s.size()) res += s[j]; if(k &lt; s.size() ) res += s[k]; &#125; &#125; &#125; return res; &#125;&#125;; longest substring without repeating characters 这里有一个通用的“滑动窗口 + 哈希表” 的做法。由于 $i$, $j$ 最多增加 $n$ 次，且hash 表的插入和更新时间复杂度是$O(1)$， 所以总的时间复杂度是$O(n)$。 123456789101112131415class Solution &#123;public: // 使用双指针 + hash表 int lengthOfLongestSubstring(string s) &#123; unordered_map&lt;char, int&gt; hash; int res =0; for(int i =0, j =0; j&lt; s.size() ; j++) &#123; hash[s[j]] ++; while(hash[s[j]] &gt; 1) hash[s[i++]] --; res =max(res, j -i +1); &#125; return res; &#125;&#125;; implement Trie(Predix Tree) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667class Trie &#123;public: // 使用一个结构体存储一个结点 struct Node&#123; bool is_end; Node * son[26]; // 这个类似构造函数，是对结构体进行初始化 Node() &#123; is_end =false; for(int i=0; i&lt; 26; i++) son[i] =NULL; &#125; &#125;*root; // 这个是结构体变量 /** Initialize your data structure here. */ Trie() &#123; root =new Node(); &#125; /** Inserts a word into the trie. */ void insert(string word) &#123; auto p = root; for(auto c: word) &#123; auto u =c -'a'; if(p-&gt;son[u] ==NULL) p -&gt;son[u] =new Node(); p =p-&gt;son[u]; &#125; p-&gt;is_end =true; &#125; /** Returns if the word is in the trie. */ bool search(string word) &#123; auto p =root; for(auto c : word) &#123; auto u =c -'a'; if(p -&gt;son[u] ==NULL) return false; p =p-&gt;son[u]; &#125; return p-&gt;is_end; &#125; /** Returns if there is any word in the trie that starts with the given prefix. */ bool startsWith(string prefix) &#123; auto p =root; for(auto c :prefix) &#123; auto u =c -'a'; if(p -&gt;son[u] ==NULL) return false; p =p-&gt;son[u]; &#125; return true; &#125;&#125;;/** * Your Trie object will be instantiated and called as such: * Trie* obj = new Trie(); * obj-&gt;insert(word); * bool param_2 = obj-&gt;search(word); * bool param_3 = obj-&gt;startsWith(prefix); */ 关于trie 的理论讲解可以参考这篇博客, 这里主要讲解实现。 integer to English Words 123456789101112131415161718192021222324252627282930313233343536373839404142class Solution &#123;public: string small[20] =&#123;"Zero", "One", "Two", "Three", "Four", "Five", "Six", "Seven", "Eight", "Nine", "Ten", "Eleven", "Twelve", "Thirteen", "Fourteen", "Fifteen", "Sixteen", "Seventeen", "Eighteen", "Nineteen"&#125;; string mid[10] =&#123;"", "", "Twenty", "Thirty", "Forty", "Fifty", "Sixty", "Seventy", "Eighty", "Ninety"&#125;; string big[4] =&#123;"Billion", "Million", "Thousand", ""&#125;; string numberToWords(int num) &#123; if(! num) return small[0]; string res; for(int i =1000000000,j =0; i&gt;0; i /=1000, j++) &#123; if(num &gt;=i) &#123; res += get_part(num /i) +big[j] +" "; num %= i; &#125; &#125; while(res.back() ==' ') res.pop_back(); return res; &#125; string get_part(int n) &#123; string res; if(n &gt;=100) &#123; res += small[n /100] +" Hundred "; n %= 100; &#125; if(!n) return res; if(n &gt;=20) &#123; res += mid[n/10] +' '; n %=10; &#125; if(!n) return res; res += small[n]+' '; return res; &#125;&#125;; KMP 专题首先最简单的是字符串的匹配问题， 如果暴力枚举，那么时间复杂度是 $O(mn) $，其中 $m$ 和$n$ 分别表示两个字符串的长度。如果使用kmp 算法，那么时间复杂度是 $O(m +n)$ 这个是模板：1234567891011121314151617181920求Next数组：// s[]是模式串，p[]是模板串, n是s的长度，m是p的长度for (int i = 2, j = 0; i &lt;= m; i ++ )&#123; while (j &amp;&amp; p[i] != p[j + 1]) j = ne[j]; if (p[i] == p[j + 1]) j ++ ; ne[i] = j;&#125;// 匹配for (int i = 1, j = 0; i &lt;= n; i ++ )&#123; while (j &amp;&amp; s[i] != p[j + 1]) j = ne[j]; if (s[i] == p[j + 1]) j ++ ; if (j == m) &#123; j = ne[j]; // 匹配成功后的逻辑 &#125;&#125; 这个是投机取巧的方式， 但是可以掌握关于 string:: npos 这种判断条件 1234567891011121314151617#include&lt;iostream&gt;#include&lt;string&gt;using namespace std;int main()&#123; int n, m; string p, s; cin &gt;&gt; n&gt;&gt; p&gt;&gt; m&gt;&gt; s; auto found =s.find(p); while(found != string::npos) &#123; cout &lt;&lt; found &lt;&lt; " "; found =s.find(p, found+1); &#125; return 0;&#125; 使用 kmp 算法的正解。(KMP字符串)(https://www.acwing.com/problem/content/description/833/)12345678910111213141516171819202122232425262728293031323334353637#include&lt;iostream&gt;using namespace std;const int N =1e4+11;const int N1 =1e5+11;int nex[N];char p[N];char s[N1];int n, m;void get_next()&#123; for(int i =2,j =0; i&lt;=n; i++) &#123; while(j &amp;&amp; p[i] != p[j+1]) j =nex[j]; if(p[i ] ==p[j+1]) j ++; nex[i] =j; &#125;&#125;int main()&#123; // 这种写法对于一次性的读入，还是非常有效率的，学习 cin &gt;&gt; n &gt;&gt; p+1&gt;&gt; m &gt;&gt; s+1; get_next(); for(int i =1, j =0; i&lt;=m; i++) &#123; while(j &amp;&amp; s[i] != p[j+1]) j =nex[j]; if( s[i] ==p[j+1]) j +=1; if(j ==n) &#123; printf("%d ", i-n); j =nex[j]; &#125; &#125; return 0;&#125; 周期 next[n] 的含义： 表示以 n 为结尾的后缀和以 1位起点的前缀，相同的string ， 最大长度。 n-next[n] 表示最小的循环节，那么 n/(n -next[n]) 就是最多的个数。 以i 为结点的最大的后缀和前缀相等，这个后缀的长度就是next[i] 12345678910111213141516171819202122232425262728293031323334353637#include&lt;iostream&gt;using namespace std;const int N =1e6+11;int nex[N];char str[N];int n;//在所有的前缀中 最小循环节的最大重复个数// 考察 n-next[n] 是表示最小的循环节void get_next()&#123; for(int i =2, j =0; i&lt;=n ; i++) &#123; // while 和 nex[i] 是容易出粗 while(j &amp;&amp; str[i] != str[j+1]) j =nex[j]; if(str[i] ==str[j+1]) j++; nex[i] =j; &#125;&#125;int main()&#123; int T =1; // 如果n ==0，那么就跳出了 while(scanf("%d", &amp;n), n) &#123; scanf("%s\n", str+1); // 这种 \n 在读入字符串中还是非常有用，对于cin 函数，你就知道多有用了 get_next(); printf("Test case #%d\n", T++); for(int i =1; i&lt;=n; i++) &#123; int t =i -nex[i]; if(t != i &amp;&amp; i%t ==0) printf("%d %d\n", i, i/t); &#125; puts(""); &#125; return 0;&#125; 对于string 的题目，一般可以hash 或者 kmp 算法求解。 160. 匹配统计 视频讲解 我觉得可以理解的地方是，kmp 算法求解next 数组，kmp 算法进行字符串匹配，n-next[n] 表示最大的循环节的个数 12345678910111213141516171819202122232425262728293031323334353637383940// f[i] 表示匹配长度至少是 i的情况下， 这样的后缀有多少个// 最少是 x -最少是x+1 的，那么得到的就是 x#include&lt;iostream&gt;using namespace std;const int N =2e5+11;int n, m, q;char a[N], b[N];int ne[N];int f[N];// next 数组本质上就是前缀和后缀的匹配的最大长度// next 数组不止可以操作同一个string，还是可以操作另外的数组int main()&#123; cin &gt;&gt;n &gt;&gt;m&gt;&gt; q; scanf("%s%s", a+1, b+1); // 读入了两个字符串数组 //对于 b 求解kmp 数组 for(int i =2, j =0;i &lt;=m; i++) &#123; while(j &amp;&amp; b[i] != b[j+1]) j =ne[j]; if(b[i] ==b[j+1]) j++; ne[i] =j; &#125; // 求解a 的后缀和b 的前缀的匹配, for(int i =1, j =0; i&lt;=n; i++) &#123; while(j &amp;&amp; a[i] != b[j+1]) j =ne[j]; if(a[i] ==b[j+1]) j ++; // 这里是不理解的 f[j] ++; &#125; // 这里也是不懂的 for(int i=m; i; i--) f[ne[i]] += f[i]; while( q--) &#123; int x ; cin &gt;&gt;x; cout &lt;&lt; f[x]- f[x+1] &lt;&lt;endl; &#125; return 0;&#125; 奶牛矩阵 视频讲解 太难了 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869#include&lt;iostream&gt;#include&lt;algorithm&gt;#include&lt;string.h&gt;using namespace std;const int N =10010, M =80;int n, m;char str[N][M];int ne[N];bool st[M];int main()&#123; cin &gt;&gt;n&gt;&gt;m; memset(st, true, sizeof st); for(int i =1 ;i&lt;=n; i++) &#123; scanf("%s", str[i]); for(int j =1; j&lt;=m; j++) &#123; if(st[j]) &#123; for(int k =j; k&lt;m; k+=j) &#123; for(int u =0; u&lt;j &amp;&amp; k+ u&lt; m; u++) &#123; if(str[i][u] != str[i][k+u]) &#123; st[j] =false; break; &#125; &#125; if(!st[j]) break; &#125; &#125; &#125; &#125; int width; for(int i =1; i&lt;=m; i++) &#123; if(st[i]) &#123; width =i; break; &#125; &#125; for(int i =1; i&lt;=n; i++) str[i][width] =0; // strcmp(str1, str2) 如果相等返回 0， 如果str1&lt; str2 返回-1， 如果大于 返回+1 for(int i =2, j =0; i&lt;=n; i++) &#123; while(j &amp;&amp; strcmp(str[i], str[j+1])) j =ne[j]; if(!strcmp(str[i], str[j+1])) j++; ne[i] =j; &#125; int height =n -ne[n]; //cout &lt;&lt; width &lt;&lt;" "&lt;&lt; height&lt;&lt; endl; cout &lt;&lt; width *height &lt;&lt;endl; return 0;&#125; 超市 主要是练习 小根堆和pair 的组合使用。是一个贪心算法，很经典的、时间复杂度是 $nlogn$ 1234567891011121314151617181920212223242526272829303132#include&lt;iostream&gt;#include&lt;algorithm&gt;#include&lt;vector&gt;#include&lt;queue&gt;using namespace std;typedef pair&lt;int, int&gt; PAIR;// 使用pair 这种结构是可以按照第一关键字先排序，然后按照第二关键字排序// 按照过期时间排序，维护一个小根堆，每次把利润小的给 pop出去int main()&#123; int n; while(cin &gt;&gt;n) &#123; vector&lt;PAIR&gt; products(n); // 这种写法，真是非常的简洁，使用vector&lt;pair&lt;int, int&gt;&gt; 的格式，list 中嵌套了 pair // 先是按照过期时间进行排序，如果相同，那么按照金额进行排序,都是从小到大进行排序，因为sort() 默认的按照增序进行排序的 for(int i =0; i&lt;n; i++) cin &gt;&gt; products[i].second &gt;&gt; products[i].first; sort(products.begin(), products.end()); // 定义小根堆 // 使用greater&lt;int&gt; ,less&lt;int&gt; ，头文件不写也行的 priority_queue&lt;int , vector&lt;int&gt; , greater&lt;int&gt;&gt; heap; for(auto p: products) &#123; heap.push(p.second); if(heap.size() &gt; p.first) heap.pop(); &#125; int res =0; while(heap.size() ) res += heap.top(), heap.pop(); cout &lt;&lt; res&lt;&lt; endl; &#125; return 0;&#125; 第二次写。1234567891011121314151617181920212223242526272829303132333435#include&lt;iostream&gt;#include&lt;vector&gt;#include&lt;queue&gt;#include&lt;algorithm&gt;using namespace std;typedef pair&lt;int, int&gt; PAIR;int main()&#123; int n ; while(cin &gt;&gt;n) &#123; vector&lt;PAIR&gt; products(n); // 下面是index 访问，那么就需要初始化，否则是段错误；或者使用 push_back() 进行访问 for(int i =0; i&lt; n; i++) &#123; cin &gt;&gt; products[i].second &gt;&gt; products[i].first; &#125; sort(products.begin(), products.end()); priority_queue&lt;int, vector&lt;int&gt;, greater&lt;int&gt;&gt; heap; for(auto product: products) &#123; // 先是无脑放，如果发现不满足条件，那么弹出 heap.push(product.second); if(heap.size() &gt; product.first) heap.pop(); &#125; int res =0; while(heap.size()) &#123; res +=heap.top(); heap.pop(); &#125; cout &lt;&lt; res&lt;&lt; endl; &#125; return 0;&#125; Implement strStr() 注意这个不是从 1 开始计数的，这个使用边界条件是需要记录一下的。 1234567891011121314151617181920212223242526class Solution &#123;public: int strStr(string s, string p) &#123; int n =s.size() , m =p.size(); if(m ==0) return 0; // next 数组 vector&lt;int&gt; nex(m); nex[0] =-1; // 必须要有一个初始值 for(int i =1, j =-1; i&lt; m ; i++) &#123; while( j&gt; -1 &amp;&amp; p[i] != p[j+1]) j =nex[j]; if(p[i] == p[j+1]) j++; nex[i] =j; &#125; for(int i =0, j =-1; i&lt; n; i++) &#123; while(j &gt; -1 &amp;&amp; s[i] != p[j+1]) j =nex[j]; if(s[i ] ==p[j+1]) j ++; if(j ==m -1) &#123; return i-m +1; &#125; &#125; return -1; &#125;&#125;; 459. Repeated Substring Pattern 1234567891011121314151617181920class Solution &#123;public: // 考察kmp 中 i -nex[i] 是循环节的长度。更加具体的是 在这个题目中 n -nex[n] 是最小循环节的长度， // 这个是可以用来判断是否完美切分 bool repeatedSubstringPattern(string s) &#123; int n = s.size(); vector&lt;int&gt; nex(n); nex[0] =-1; for(int i =1, j =-1; i&lt; n; i++) &#123; while(j &gt; -1 &amp;&amp; s[i] != s[j+1]) j =nex[j]; if(s[i] ==s[j+1]) j ++; nex[i] =j; &#125; int t =n-1 -nex[n-1]; return t !=n&amp;&amp; n%t ==0; // 字符串本身不能作为一个循环节 &#125;&#125;; 214. Shortest Palindrome 123456789101112131415161718192021222324252627class Solution &#123;public: // 考察kmp 的前后缀的理解 string shortestPalindrome(string s) &#123; int n =s.size(); // 这种情况专门就是指的是 s =="" 没有其他的情况了 if (n ==0) return s; string t(s.rbegin(), s.rend()); vector&lt;int&gt; nex(n); nex[0] =-1; for(int i =1, j =-1; i&lt; n; i++) &#123; while(j &gt; -1 &amp;&amp; s[i] != s[j+1]) j =nex[j]; if(s[i] ==s[j+1]) j ++; nex[i] =j; &#125; // 做的kmp 的匹配 int j =-1; // 比较的是 t 的前缀和 s 的后缀的相同的最大的长度 for(int i =0; i&lt;n; i++) &#123; while(j &gt; -1 &amp;&amp; t[i] != s[j+1]) j =nex[j]; if(t[i] ==s[j+1]) j ++; &#125; return t +s.substr(j +1, n -j -1); &#125;&#125;;]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>KMP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Beyond Word Embedding]]></title>
    <url>%2F2019%2F05%2F22%2Fbeyond-word-embedding%2F</url>
    <content type="text"><![CDATA[从one-hot 到 word2vec， 到elmo，简单介绍一下 NLP中词向量的过程。 进入正题之前，思考为什么要将词用向量来表示呢？这样可以给词语一个数学上的表示，使之可以适用于某些算法或数学模型。通常将词语表示成向量有如下两种方法: one-hot and distributed 表示法。one-hot 只有一个位置是1 其他的位置都是0，最大的特点就是数据变得稀疏，而后者属于稠密向量。前者的缺点向量是相互独立的，无法通过距离函数比如 cosine 进行相似度的比较，并且如果维度 N非常大，那么高纬度的表示也可能引发维度灾难。于是接着往下看吧… Traditional Word VectorsBefore diving directly into Word2Vec it’s worth while to do a brief overview of some of the traditional methods that pre-date neural embeddings. 这个是用来描述文章的，有一个大的dict，然后一片文章是如何进行表示、Bag of Words or BoW vector representations are the most common used traditional vector representation. Each word or n-gram is linked to a vector index and marked as 0 or 1 depending on whether it occurs in a given document. An example of a one hot bag of words representation for documents with one word. 局限性: 一方面只是一种counter，没有考虑语义信息；另一方面有些 words 是明显的 relevant than others.BoW representations are often used in methods of document classification where the frequency of each word, bi-word or tri-word is a useful feature for training classifiers. One challenge with bag of word representations is that they don’t encode any information with regards to the meaning of a given word.In BoW word occurrences are evenly weighted independently of how frequently or what context they occur. However in most NLP tasks some words are more relevant than others. 这个是可以认识是对于 bag of words “relevant” 上的改进：使得 选择的words 更加的 “representative” 文章的调性。TF-IDF, short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word or n-gram is to a document in a collection or corpus. They provide some weighting to a given word based on the context it occurs.The tf–idf value increases proportionally to the number of times a word appears in a document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently than others. 但是对于 bag of words 中“没有体现语义” 的缺陷还是没有 deal with。However even though tf-idf BoW representations provide weights to different words they are unable to capture the word meaning. 这个名字只是因为有定义而存在的名字（网络模型 or 深度网络的出现就是为了 handle 语义信息）Distributional Embeddings enable word vectors to encapsulate contextual context. Each embedding vector is represented based on the mutual information it has with other words in a given corpus.重点就是这种方式是要 predict a target word from context words，一定是要能够体现语境的。Predictive models learn their vectors in order to improve their predictive ability of a loss such as the loss of predicting the vector for a target word from the vectors of the surrounding context words. word2vec 两种类型 word2vec 是一种思想，有两种CBOW 和skip-gram 两种实现。Word2Vec is a predictive embedding model. There are two main Word2Vec architectures that are used to produce a distributed representation of words: Continuous bag-of-words (CBOW) — The order of context words does not influence prediction (bag-of-words assumption). Continuous skip-gram weighs nearby context words more heavily than more distant context words. While order still is not captured each of the context vectors are weighed and compared independently vs CBOW which weighs against the average context. word2vec 就是2 种算法+ 2种模型，总共是四种实现。 展示一下结构图： word2vec 的训练过程： 中文分词，然后保存所有的语料到一个文件中，可以使用 换行符进行分开 扫描语料库统计词频，取词频最高的V 个词，构成词汇表，one-hot 编码， 词的维度就是词典的大小，其余（出现频率很低）的词都用一个特殊符号代替掉。 词向量是从输入层到隐藏层的weights，随着初始化而存在，然后之后是不断优化的产物 训练的目标的，以skip-gram 为例，输入中心词然后最大化输出周围的词 (context )词汇。 输入层的输入：每个词存在一个one-hot向量，向量的维度是V（词典大小），如果该词在词汇表中出现过，则向量中词汇表中对应的位置为1，其他位置全为0。如果词汇表中不出现，则向量为全0 负采样（Negative Sample）和层次softmax（Hierarchical Softmax）则是两种加速训练的方法。都是优化最后的softmax 层（输出层），因为这个大小就是词典的大小，计算量太大了，如果知道softmax，是存在指数计算的。 loss function 在cbow模型中，所有的词被编码成ont-hot向量，V为总词语数。input层的one-hot vector经过 $W_{VXN} $矩阵后，被压缩为只有N个元素的向量h，之后经过W′矩阵出来，得到u。于是根据公式，有 $$p \left( w _ { t } | w _ { \text {input} } \right) = y _ { j } = \frac { \exp \left( u _ { j } \right) } { \sum \exp \left( u _ { j } \prime \right) }$$ 最大化该条件概率，得到 $$\max p \left( w _ { t } | w _ { \text {input} } \right) = \max \log y _ { j } = u _ { j } - \operatorname { log } \sum \exp \left( u _ { j } \right)$$于是得到了 词袋模型的 loss function： （关于网络中的 loss function 还是要多留意一下的）$$E = - \log p \left( w _ { t } | w _ { \text {input} } \right) = \log \sum \exp \left( u _ { j } \right) - u _ { j }$$这里，$u _ { j }$ 表示第 $j$ 个词向量， 有了 loss function，就可以进行词向量的训练了。 层次softmax 比如说一个二叉树结构，“我”肯定是第一层叶子节点，“涮羊肉”肯定是在最后一层的叶子节点。在 word2vec 中输入输出的编码都是使用的 one-hot 进行数字化表示的。 这个存储的目的是遍历的次数少了，因为是使用二分类去做多分类，如果词频高的编码少，那么最后的结果是比较少的。word2vec训练的时候按照词频将每个词语Huffman编码，由于Huffman编码中词频越高的词语对应的编码越短。所以越高频的词语在 Hierarchical Softmax过程中经过的二分类节点就越少，整体计算量就更少了。 总的特点：使用 context words 去predict 中心词 or 相反的过程，最大化这种概率关系。CBOW is faster while skip-gram is slower but does a better job for infrequent words.那么为什么快呢？ 答： cbow只要 把窗口内的其他词相加一次作为输入来预测 一个单词。不管窗口多大，只需要一次运算。而skip-gram直接受窗口影响，窗口越大，需要预测的周围词越多。在训练中，通过调整窗口大小明显感觉到训练速度受到很大影响。前者是复杂度大概是O(V)，后者的时间的复杂度为O(KV）(假设K 是窗口的大小) 为什么skip-gram 的准确率高一些，对于生僻词的效果更好一些？ 在skip-gram当中，每个词都要收到周围的词的影响，每个词在作为中心词的时候，都要进行K次的预测、调整。因此， 当数据量较少，或者词为生僻词出现次数较少时， 这种多次的调整会使得词向量相对的更加准确。因为尽管cbow从另外一个角度来说，某个词也是会受到多次周围词的影响（多次将其包含在内的窗口移动），进行词向量的跳帧，但是他的调整是跟周围的词一起调整的，grad的值会平均分到该词上， 相当于该生僻词没有收到专门的训练，它只是沾了周围词的光而已。 算法参数总体上对于效果影响不大，最重要的是语料。相对来说，比较重要的常用的参数： min-count： （这个思想有点意思呀，切词切错了，那么在计算的时候就不要了） 最小词频训练阀值，这个根据训练语料大小设置，只有词频超过这个阀值的词才能被训练。根据经验，如果切词效果不好，会切错一些词，比如 “在深圳”，毕竟切错的是少数情况，使得这种错词词频不高，可以通过设置相对大一点的 min-count 过滤掉切错的词。（这种是对于新词处理的一种补救方法） 向量维度： 如果词量大，训练得到的词向量还要做语义层面的叠加，比如 句子 的向量表示 用 词的向量叠加，为了有区分度，语义空间应该要设置大一些，所以维度要偏大。一般来说，中文 180 就差不多了。 负采样： 负采样（negative sampling）解决了这个问题，它是用来提高训练速度并且改善所得到词向量的质量的一种方法。不同于原本每个训练样本更新所有的权重，负采样每次让一个训练样本仅仅更新一小部分的权重，这样就会降低梯度下降过程中的计算量。 在论文中，作者指出指出对于小规模数据集，选择5-20个negative words会比较好，对于大规模数据集可以仅选择2-5个negative words。 任何采样算法都应该保证频次越高的样本越容易被采样出来。基本的思路是对于长度为1的线段，根据词语的词频将其公平地分配给每个词语：$$\operatorname { len } ( w ) = \frac { \operatorname { counter } ( w ) } { \sum _ { u \in \mathcal { D } } \operatorname { counter } ( u ) }$$ 在word2vec中，该“刻度尺”对应着table数组。具体实现时，对词频取了0.75次幂：$$\operatorname { len } ( w ) = \frac { [ \operatorname { counter } ( w ) ] ^ { 0.75 } } { \sum _ { u \in \mathcal { D } } [ \operatorname { counter } ( u ) ] ^ { 0.75 } }$$这个幂实际上是一种“平滑”策略，能够让低频词多一些出场机会，高频词贡献一些出场机会，劫富济贫。 （这个是一种计算上的优化，通过选取一部分结点（词汇）更新权重）负采样越低，对高频词越不利，对低频词有利。可以这么理解，本来高频词 词被迭代50次，低频词迭代10次，如果采样频率降低一半，高频词失去了25次迭代，而低频词只失去了5次。一般设置成le-5. ( 这个就是 $10^{-5}$ ) 在 fasttext 实现的时候 使用下面的超参数记性控制。 -neg number of negatives sampled [5] 窗口大小： 窗口大小影响 词 和前后多少个词的关系，和语料中语句长度有关，建议可以统计一下语料中，句子长度的分布，再来设置window大小。一般设置成8。（参数也是一个技术活，参数的设置和原始训练数据集 和其他的参数的配合是相关联的。如果句子比较长，那么window size 就不要太小） 负采样 vs 窗口大小 负采样主要是为了降低模型计算量。如果没有负采样，模型需要把词汇表中没有出现在滑动窗口的词语当作负样本。然而在实际训练过程中，并不需要这么多的负样本，过多的负样本会导致模型学偏。（窗口的大小是正采样的个数，那么 负采样的个数和窗口的大小尽量是保持了 1：1 的关系，这样是比较好的） 负采样的个数和滑动窗口的比例尽量控制在0.1-10之间，滑动窗口决定了正样本的数量，负采样的个数决定了负样本的个数，正负样本尽量不要差距太大，建议负采样的个数和滑动窗口的比例控制为1：1。 比较详细的介绍可以查看这里 如何评估 word2vec 训练的好坏？ 词聚类 （可以采用 kmeans 聚类，看聚类簇的分布） 词cos 相关性（查找cos相近的词） Analogy对比 （man-king， woman-queen） 使用tnse，pca等降维可视化展示 更多的评价方法可以参见这里. glove （g lou v） （复习到这里了） Intuition: Both CBOW and Skip-Grams are “predictive” models, in that they only take local contexts into account. word2vec does not take advantage of global context.(细节 能看懂就看)GloVe embeddings by contrast leverage the same intuition behind the co-occurrence matrix (共生矩阵) used distributional embeddings, but uses neural methods to decompose the co-occurrence matrix into more expressive and dense word vectors. 模型目标：进行词的向量化表示，使得向量之间尽可能多地蕴含语义和语法的信息。输入：语料库输出：词向量方法概述：首先基于语料库构建词的共现矩阵，然后基于共现矩阵和GloVe模型学习词向量。 下面是一个例子： sparse vectors 词-文档矩阵(Term-document matrix) 和 词共现矩阵(Term-term matrix)。Term-document matrix表示每个单词在文档中出现的次数(词频)，每一行是一个 term，每一列是一个 document两篇文档的向量相似 =&gt; 两篇文档相似，如上图 doc3 和 doc4，我们就认为它们是相似的。两个单词的向量相似 =&gt; 两个单词相似，如上图的 fool 和 clown，就是相似的。 Term-term matrix然后我们可以考虑更小的粒度，更小的上下文，也就是不用整篇文档，而是用段落(paragraph)，或者小的窗口(window of ±4 words)，所以这个时候，向量就是对上下文单词的计数，大小不再是文档长度 |D|，而是窗口长度 |V| 了，所以现在 word-word matrix 是 |V|*|V| 而 word2vec 得到的向量 dense vectors。不使用 negative sampling 的Wordvec 非常快，但准确率不高（57.4\%）,毕竟模型没有告诉什么是无关的word，模型很难对无关词汇进行惩戒，提高准确率。对于 synonym 问题，word2vec 是好于 glove，但从最终的效果上看，两者是不分彼此的。glove 使用了整体的信息，word2vec 只是使用了局部信息（local context）。 word2vec 和glove 的区别：Predictive的模型，如Word2vec，根据context预测中间的词汇，要么根据中间的词汇预测context，分别对应了word2vec的两种训练方式cbow和skip-gram。 Count-based模型，如GloVe，本质上是对共现矩阵进行降维。首先，构建一个词汇的共现矩阵，每一行是一个word，每一列是context。共现矩阵就是计算每个word在每个context出现的频率。由于context是多种词汇的组合，其维度非常大，我们希望像network embedding一样，在context的维度上降维，学习word的低维表示。该向量表示属于 sparse vectors 和word2vec 的效果比较：glove 和word2vec 相比没有 definitively better results，还是通过实验进行说话吧。While GloVe vectors are faster to train, neither GloVe or Word2Vec has been shown to provide definitively better results rather they should both be evaluated for a given dataset. fasttext这个主要是 each word + n-gram within each word， 最后的效果是好于 word2vec 的。FastText, builds on Word2Vec by learning vector representations for each word and the n-grams found within each word. The values of the representations are then averaged into one vector at each training step. While this adds a lot of additional computation to training it enables word embeddings to encode sub-word information. FastText vectors have been shown to be more accurate than Word2Vec vectors by a number of different measures. 简单说 fasttext 和word2vec 模型上的不同有两点： 模型的输出层：word2vec的输出层，对应的是每一个term，计算某term的概率最大；而fasttext的输出层对应的是 分类的label。不过不管输出层对应的是什么内容，起对应的vector都不会被保留和使用； fasttext则充分利用了h-softmax的分类功能，遍历分类树的所有叶节点，找到概率最大的label（一个或者N个） 模型的输入层：word2vec的输出层，是 context window 内的term；而fasttext对应的整个sentence的内容，包括term，也包括 n-gram的内容 更多信息可以参看博客。 overview of Neural NLP ArchitecturesDeep Feed Forward Networks 1D CNNs RNNs (LSTM/GRU) encoder- decoder 结构 attention and copy mechanisms这个是 attention 机制提出的背景：解决 句子中的长依赖；contextual impact (specific words may carry more importance at different steps)While in theory they can capture long term dependencies they tend to struggle modeling longer sequences, this is still an open problem. One cause for sub-optimal performance standard RNN encoder-decoder models for sequence to sequence tasks such as NER or translation is that they weight the impact each input vector evenly on each output vector when in reality specific words in the input sequence may carry more importance at different time steps.Attention mechanisms provide a means of weighting the contextual impact of each input vector on each output prediction of the RNN. These mechanisms are responsible for much of the current or near current state of the art in Natural language processing. attention In sum, algorithms can allocate attention, and they can learn how to do so, by adjusting the weights they assign to various inputs. Imagine a heat map over a photo. The heat is attention. 这个是要引出来 context word embeddings.One of the limits of traditional word vectors is that they presume that a word’s meaning is relatively stable across sentences.并不是物理上的二维关系能够表示词语之间的 relationship，有时候是需要高纬空间进行表示的。In fact, the strongest relationships binding a given word to the rest of the sentence may be with words quite distant from it.从 credit assignment的角度阐述了 neural networks 就是 allocating importance to input features。The fundamental task of all neural networks is credit assignment. Credit assignment is allocating importance to input features through the weights of the neural network’s model. Learning is the process by which neural networks figure out which input features correlate highly with the outcomes the net tries to predict, and their learnings are embodied in the adjusted quantities of the weights that result in accurate decisions about the data they’re exposed to.这个是传统的 LSTM （encoder -decoder） 模型，问题在于当句子过长（比如说大于20 words）之后，encoder 是无法 memory 之前的所有 words，所以效果就会变得差一些。 但是 attention 就是模仿了人翻译过程，一段作为一个单位，然后进行翻译。这样就可以持续保证较高中确率的输出。In neural networks, attention primarily serves as a memory-access mechanism. 每次的输出都是关注不同的地方，但是至于哪里更加重要，这个交给了 feedback mechanism 反向传播。下面的图片十分清晰的展示了 在翻译的过程中 “focus” 是不断地变化的。Above, a model highlights which pixels it is focusing on as it predicts the underlined word in the respective captions. Below, a language model highlights the words from one language, French, that were relevant as it produced the English words in the translation. As you can see, attention provides us with a route to interpretability. We can render attention as a heat map over input data such as words and pixels, and thus communicate to human operators how a neural network made a decision. (This could be the basis of a feedback mechanism whereby those humans tell the network to pay attention to certain features and not others.) (This could be the basis of a feedback mechanism whereby those humans tell the network to pay attention to certain features and not others.) Additionally in Machine Reading Comprehension and Summarization systems RNNs often tend to generate results, that while on first glance look structurally correct are in reality hallucinated or incorrect. One mechanism that helps mitigate some of these issues is the Copy Mechanism.copy mechanism 简单说来就是 word embedding or raw text.The copy mechanism is an additional layer applied during decoding that decides whether it is better to generate the next word from the source sentence or from the general embedding vocabulary. reading comprehension and summary 上面是说的在 machine translation，下面说的是 阅读理解 和 summary领域。Additionally in Machine Reading Comprehension and Summarization systems RNNs often tend to generate results, that while on first glance look structurally correct are in reality hallucinated or incorrect. One mechanism that helps mitigate some of these issues is the Copy Mechanism.copy mechanism 简单说来就是 decide word embedding from model or raw text.The copy mechanism is an additional layer applied during decoding that decides whether it is better to generate the next word from the source sentence or from the general embedding vocabulary. Taming Recurrent Neural Networks for Better Summarization有两种不同的 summarization:Two types of summarization：Extractive （You might think of these approaches as like a highlighter.） Abstractive（By the same analogy, these approaches are like a pen.）The great majority of existing approaches to automatic summarization are extractive – mostly because it is much easier to select text than it is to generate text from scratch.但是一个问题在于，只是使用 extrative way 可能得到相同的words，Problem 1: The summaries sometimes reproduce factual details inaccurately (e.g. Germany beat Argentina 3-2). This is especially common for rare or out-of-vocabulary words such as 2-0.Problem 2: The summaries sometimes repeat themselves (e.g. Germany beat Germany beat Germany beat…)Easier Copying with Pointer-Generator Networks。这个跟 attention 不是很相关，简单说就是In this way, the pointer-generator network is a best of both worlds, combining both extraction (pointing) and abstraction (generating). To tackle Problem 2 (repetitive summaries), we use a technique called coverage. The idea is that we use the attention distribution to keep track of what’s been covered so far, and penalize the network for attending to same parts again. elmo (e l mo)elmo 产生一个 embedding 是根据 context 产生的。ELMo is a model generates embeddings for a word based on the context it appears thus generating slightly different embeddings for each of its occurrence.（感觉理解一个概念都是 根据其 for example 进行理解的）For example, the word “play” in the sentence above using standard word embeddings encodes multiple meanings such as the verb to play or in the case of the sentence a theatre production. In standard word embeddings such as Glove, Fast Text or Word2Vec each instance of the word play would have the same representation. 参考blog:https://towardsdatascience.com/beyond-word-embeddings-part-2-word-vectors-nlp-modeling-from-bow-to-bert-4ebd4711d0ec http://www.abigailsee.com/2017/04/16/taming-rnns-for-better-summarization.html https://skymind.ai/wiki/word2vec 复习笔记 评价 word embedding的方面：是否能体现语义信息；是否比其他词语更加具有代表性； word2vec(或者说 CBOW or Skip-gram )的特点，输入和输出都是 one-hot 的表示，长度都是字典的长度，在某一个位置是1，其他的位置都是0. 训练过程：分词之后组成预料库，扫描所有的词频，取词频最高的V个词语，构成词汇表，词频低的就会被丢弃（所以词语的学习需要多次的，有上下文的）， 都被用一个特殊符号代替掉。损失函数是概率模型，给定上下文然后中心词出现的概率，概率是需要加上log，就组成了loss function，训练的过程中是最大化概率模型（最小化loss）。CBOW和Skip-gram 的区别：CBOW在训练的过程中将上下文加和成一个向量表示，所以最后的训练速度是快于skip-gram 但是效果没有skip-gram。后者是一个个单词进行训练的（当然训练时间也是比较长的）。对于中文的embedding size，一般180 就OK了。在负采样中负样本是根据词频挑选的，但公式中使用了一种平滑机制，使得低频词也有了一些机会，对高频词语进行了一些约束。 word2vec 和glove 的区别：前者是预测类的模型，后者是cout-base的模型，本质上是对共现矩阵的降维处理。每一行表示一个word，每一列表示context，共现矩阵就是计算每个word 在每个context 中出现的频率。 fasttext 和word2vec 的区别：输入层，word2vec 是一个term，而fasttext中包含的是term和对应的n-gram的特征；输出层， 在分类中fasttext的输出是label 信息。]]></content>
      <categories>
        <category>NLP</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Hyper-parameter Optimization for Machine Learning]]></title>
    <url>%2F2019%2F05%2F22%2FHyperparameter-optimization-for-machine-learning%2F</url>
    <content type="text"><![CDATA[Following are four common methods of hyper-parameter optimization for machine learning in order of increasing efficiency: Manual Grid search Random search Bayesian model-based optimization 简单说一下前两种， manual 适合比较有经验的人进行调参，整一个模型好久了，对参数的设置比较熟悉；Grid Search 最好的理解方式就是 枚举，如果时间充足那么可以找到 search space 中的最优解。但如果是 limited time and space， 那么请使用 random search 或者基于贝叶斯的开源包。 Random SearchFirst we will implement a common technique for hyper-parameter optimization: random search. Each iteration, we choose a random set of model hyper-parameters from a search space. Random search uses the following four parts: Domain: values over which to search Optimization algorithm: pick the next values at random! (yes this qualifies as an algorithm) Objective function to minimize: in this case our metric is cross validation ROC AUC Results history that tracks the hyper-parameters tried and the cross validation metric Random search can be implemented in the Scikit-Learn library using RandomizedSearchCV, however, because we are using Early Stopping (to determine the optimal number of estimators), we will have to implement the method ourselves (more practice!). This is pretty straightforward, and many of the ideas in random search will transfer over to Bayesian hyper-parameter optimization. Empirically, random search is very effective, returning nearly as good results as grid search with a significant reduction in time spent searching. However, it is still an uninformed method in the sense that it does not use past evaluations of the objective function to inform the choices it makes for the next evaluation. Case Study of random search 123456789101112131415161718192021222324252627282930313233343536373839# Load librariesfrom scipy.stats import uniformfrom sklearn import linear_model, datasetsfrom sklearn.model_selection import RandomizedSearchCV# data and model# Load datairis = datasets.load_iris()X = iris.datay = iris.target# Create logistic regressionlogistic = linear_model.LogisticRegression()# Create hyper-parameter Search Space# Create regularization penalty space# 如果比较少，那么久枚举出来penalty = ['l1', 'l2']# 如果是有规律的连续的，就使用这种方式列举出来# Create regularization hyper-parameter distribution using uniform distributionC = uniform(loc=0, scale=4)# Create hyper-parameter optionshyper-parameters = dict(C=C, penalty=penalty)# cv: cross validation, This approach involves randomly dividing the set of observations into k groups, or folds, of approximately equal size. The first fold is treated as a validation set, and the method is fit on the remaining k − 1 folds.# Create randomized search 5-fold cross validation and 100 iterationsclf = RandomizedSearchCV(logistic, hyper-parameters, random_state=1, n_iter=100, cv=5, verbose=0, n_jobs=-1)# Fit randomized searchbest_model = clf.fit(X, y)# View best hyper-parameters# 注意这种获取best params 的方式print('Best Penalty:', best_model.best_estimator_.get_params()['penalty'])print('Best C:', best_model.best_estimator_.get_params()['C'])# Predict target vectorbest_model.predict(X) Comparison between Grid Search and Random Search 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364import numpy as npfrom time import timefrom scipy.stats import randint as sp_randintfrom sklearn.model_selection import GridSearchCVfrom sklearn.model_selection import RandomizedSearchCVfrom sklearn.datasets import load_digitsfrom sklearn.ensemble import RandomForestClassifier# get some datadigits = load_digits()X, y = digits.data, digits.target# build a classifierclf = RandomForestClassifier(n_estimators=20)# Utility function to report best scoresdef report(results, n_top=3): for i in range(1, n_top + 1): candidates = np.flatnonzero(results['rank_test_score'] == i) for candidate in candidates: print("Model with rank: &#123;0&#125;".format(i)) print("Mean validation score: &#123;0:.3f&#125; (std: &#123;1:.3f&#125;)".format( results['mean_test_score'][candidate], results['std_test_score'][candidate])) print("Parameters: &#123;0&#125;".format(results['params'][candidate])) print("")# specify parameters and distributions to sample fromparam_dist = &#123;"max_depth": [3, None], "max_features": sp_randint(1, 11), "min_samples_split": sp_randint(2, 11), "bootstrap": [True, False], "criterion": ["gini", "entropy"]&#125;# run randomized searchn_iter_search = 20random_search = RandomizedSearchCV(clf, param_distributions=param_dist, n_iter=n_iter_search, cv=5, iid=False)start = time()random_search.fit(X, y)print("RandomizedSearchCV took %.2f seconds for %d candidates" " parameter settings." % ((time() - start), n_iter_search))report(random_search.cv_results_)# use a full grid over all parametersparam_grid = &#123;"max_depth": [3, None], "max_features": [1, 3, 10], "min_samples_split": [2, 3, 10], "bootstrap": [True, False], "criterion": ["gini", "entropy"]&#125;# run grid searchgrid_search = GridSearchCV(clf, param_grid=param_grid, cv=5, iid=False)start = time()grid_search.fit(X, y)print("GridSearchCV took %.2f seconds for %d candidate parameter settings." % (time() - start, len(grid_search.cv_results_['params'])))report(grid_search.cv_results_) Random search without in-built function: An example of Random Search all by yourself Bayesian hyper-parameter OptimizationThe one-sentence summary of Bayesian hyper-parameter optimization is: build a probability model of the objective function and use it to select the most promising hyper-parameters to evaluate in the true objective function. The basic idea is: spend a little more time selecting the next hyper-parameters in order to make fewer calls to the objective function. In the case of hyper-parameter optimization, the objective function is the validation error of a machine learning model using a set of hyper-parameters. The aim is to find the hyper-parameters that yield the lowest error on the validation set in the hope that these results generalize to the testing set. Evaluating the objective function is expensive because it requires training the machine learning model with a specific set of hyper-parameters. Ideally, we want a method that can explore the search space while also limiting evaluations of poor hyper-parameter choices. Bayesian hyper-parameter tuning uses a continually updated probability model to “concentrate” on promising hyper-parameters by reasoning from past results. 有很多基于这种思想的实现，hyperopt 只是其中一种There are several Bayesian optimization libraries in Python which differ in the algorithm for the surrogate of the objective function. In this article, we will work with Hyperopt, which uses the Tree Parzen Estimator (TPE) Other Python libraries include Spearmint (Gaussian Process surrogate) and SMAC (Random Forest Regression). There are four parts to a Bayesian Optimization problem: Objective Function: what we want to minimize, in this case the validation error of a machine learning model with respect to the hyper-parameters （原来model 中的 objective function） Domain Space: hyper-parameter values to search over （调参空间） Optimization algorithm: method for constructing the surrogate model and choosing the next hyper-parameter values to evaluate （loss 和调参空间的 新的关系） Result history: stored outcomes from evaluations of the objective function consisting of the hyper-parameters and validation loss （result 没有什么好说的） 其中的 Bayesian hyper-parameter Optimization using Hyperopt是可以好好学习的。 data scientists 这种东西更加贴近于 data scientist 真的。 给出两个参考代码:链接一链接二]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Introduction to Natural Language Processing for Text]]></title>
    <url>%2F2019%2F05%2F21%2FIntroduction-to-Natural-Language-Processing-for-Text%2F</url>
    <content type="text"><![CDATA[Natural Language Processing is used to apply machine learning algorithms to text and speech. For example, we can use it to create systems like speech recognition, document summarization, machine translation, spam detection, named entity recognition, question answering, autocomplete, predictive typing and so on. NLTK (Natural Language Toolkit) is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to many corpora and lexical resources. Also, it contains a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning. Best of all, NLTK is a free, open source, community-driven project. In this article, we’ll cover the following topics.这些功能都是可以使用nltk 进行实现的。text Lemmatization 比如，单词“cars”词形还原后的单词为“car”，单词“ate”词形还原后的单词为“eat”。 Sentence Tokenization 段落成句。Sentence tokenization (also called sentence segmentation) is the problem of dividing a string of written language into its component sentences. The idea here looks very simple. In English and some other languages, we can split apart the sentences whenever we see a punctuation mark.（标点符号） Word Tokenization 句子成词，颗粒度变得更小。Word tokenization (also called word segmentation) is the problem of dividing a string of written language into its component words. In English and many other languages using some form of Latin alphabet, space is a good approximation of a word divider. Text Lemmatization 词性还原 and Stemming 词干提取 这种操作如果被认为是一种 normalization，那么一个优点就是加快了运行的速度。从不同的形式到统一的形式，这可以认为减少了变量。感觉这个更加涉及语法，语法树之类的东西。For grammatical reasons, documents can contain different forms of a word such as drive, drives, driving. Also, sometimes we have related words with a similar meaning, such as nation, national, nationality. Stemming and lemmatization are special cases of normalization. However, they are different from each other. Stemming usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes.Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma. Stop Words因为 stop words往往是带了 noise rather than useful information，所以这个是要去掉的。Stop words are words which are filtered out before or after processing of text. When applying machine learning to text, these words can add a lot of noise. That’s why we want to remove these irrelevant words. stop words dictionary 可以理解成一种过滤词表，是可以根据应用的不同，然后 change的。Stop words usually refer to the most common words such as “and”, “the”, “a” in a language, but there is no single universal list of stopwords. The list of the stop words can change depending on your application. 在存储 stopword 的时候使用 set rather than list 主要原因是 much faster than search operations in a set.You might wonder why we convert our list into a set. Set is an abstract data type that can store unique values, without any particular order. The search operation in a set is much faster than the search operation in a list. For a small number of words, there is no big difference, but if you have a large number of words it’s highly recommended to use the set type. Regex A kind of search pattern. A regular expression, regex, or regexp is a sequence of characters that define a search pattern. Let’s see some basics. 12345678910. - match any character except newline\w - match word\d - match digit\s - match whitespace\W - match not word\D - match not digit\S - match not whitespace[abc] - match any of a, b, or c[^abc] - not match a, b, or c[a-g] - match a character between a &amp; g 这个解释说明了为什么在正则表达式 中使用 r”” 作为一种前缀。因为正则表达是中 ”\“ 的使用和 python 中的”\” 使用有冲突。简而言之，如果加上了 r”” 那么这个就是一种完全的 正则表达式的语法了。 Regular expressions use the backslash character (‘\’) to indicate special forms or to allow special characters to be used without invoking their special meaning. This collides with Python’s usage of the same character for the same purpose in string literals; for example, to match a literal backslash, one might have to write ‘\\‘ as the pattern string, because the regular expression must be \, and each backslash must be expressed as \ inside a regular Python string literal.The solution is to use Python’s raw string notation for regular expression patterns; backslashes are not handled in any special way in a string literal prefixed with ‘r’. So r”\n” is a two-character string containing ‘\’ and ‘n’, while “\n” is a one-character string containing a newline. Usually, patterns will be expressed in Python code using this raw string notation. An example, 1234import resentence = "The development of snowboarding was inspired by skateboarding, sledding, surfing and skiing."pattern = r"[^\w]"print(re.sub(pattern, " ", sentence)) Bag of words Machine learning algorithms cannot work with raw text directly, we need to convert the text into vectors of numbers. This is called feature extraction.The bag-of-words model is a popular and simple feature extraction technique used when we work with text. It describes the occurrence of each word within a document. 这个是 bag of words的”特点“： order or structure of words 没有体现出来。Any information about the order or structure of words is discarded. That’s why it’s called a bag of words. This model is trying to understand whether a known word occurs in a document, but don’t know where is that word in the document. The intuition is that similar documents have similar contents. Also, from a content, we can learn something about the meaning of the document. To use this model, we need to: Design a vocabulary of known words (also called tokens) Choose a measure of the presence of known words 1) 最简单的方式是 “occurrence” ，如果出现了 标为1 否则标为0；这种是最为简单的 bag of words 最的方式，这四个是一一对应的。注意体会。 The complexity of the bag-of-words model comes in deciding how to design the vocabulary of known words (tokens) and how to score the presence of known words. bag of words 中使用 “occurrence” 的方式的缺点：稀疏矩阵（当dict 很大的时候，文章的 representation中有相当成分的0）。 In some cases, we can have a huge amount of data and in this cases, the length of the vector that represents a document might be thousands or millions of elements. Furthermore, each document may contain only a few of the known words in the vocabulary.Therefore the vector representations will have a lot of zeros. These vectors which have a lot of zeros are called sparse vectors. They require more memory and computational resources.We can decrease the number of the known words when using a bag-of-words model to decrease the required memory and computational resources. We can use the text cleaning techniques we’ve already seen in this article before we create our bag-of-words model: 减少 dictionary size 的方式。 Ignoring punctuationRemoving the stop words from our documentsReducing the words to their base form (Text Lemmatization and Stemming)Fixing misspelled words n-gram 的思想是很广泛：通过 sequence of words，这个是可以增加文本的表达力的。An n-gram is a sequence of a number of items (words, letter, numbers, digits, etc.). In the context of text corpora, n-grams typically refer to a sequence of words. A unigram is one word, a bigram is a sequence of two words, a trigram is a sequence of three words etc. 关于如何去 score the presence of word： 这里是有三种方式的。We saw one very simple approach - the binary approach (1 for presence, 0 for absence).Some additional scoring methods are:2) Counts. Count the number of times each word appears in a document.3) Frequencies. Calculate the frequency that each word appears in document out of all the words in the document. TF-IDF 这个语境 是相对于 frequency 而言的，关键词是不一定有 频率所决定，而一些 rarer or domain-specific words 可能是更加常见的。One problem with scoring word frequency is that the most frequent words in the document start to have the highest scores. These frequent words may not contain as much “informational gain” to the model compared with some rarer and domain-specific words. One approach to fix that problem is to penalize words that are frequent across all the documents. This approach is called TF-IDF. TF-IDF 的关键在于体现了“语料库”。TF-IDF, short for term frequency-inverse document frequency is a statistical measure used to evaluate the importance of a word to a document in a collection or corpus. 参考资料https://towardsdatascience.com/introduction-to-natural-language-processing-for-text-df845750fb63]]></content>
      <categories>
        <category>NLP</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[LeetCode-Recursion]]></title>
    <url>%2F2019%2F05%2F16%2Fleetcode-recursion%2F</url>
    <content type="text"><![CDATA[LeetCode 刷题总结（三）， 使用Python 实现。该篇题目类型主要包括：recursion, iteration 和dynamic programming。 Regular Expression Matching Given an input string (s) and a pattern (p), implement regular expression matching with support for ‘.’ and ‘ * ‘ .‘.’ Matches any single character.‘*‘ Matches zero or more of the preceding element. Tips：典型的dp，二维数组是常见的方式。 如果看不懂注释，可以看这里 1234567891011121314151617181920212223242526272829303132333435class Solution(object): """ dp, dp[i][j] means the match status between p[:i] and s[:j] """ def isMatch(self, s, p): dp =[[False]*(len(s)+1) for _ in range(len(p) +1)] dp[0][0]= True # case, of when s is an empty string but p is not, # since each * can eliminate character before it for i in range(2, len(p)+1): dp[i][0] =dp[i-2][0] and p[i-1] =="*" for i in range(1, len(p)+1): for j in range(1, len(s)+1): if p[i-1] =='*': # elimination or propagations dp[i][j] =dp[i-2][j] or dp[i-1][j] # another case, propagations if p[i-2] ==s[j-1] or p[i-2] =='.': # 下面两种写法都是可以 # dp[i][j] = dp[i][j] or dp[i][j-1] dp[i][j] |= dp[i][j-1] else: # 对于and 这个语句就类似 if 语句, 下面两种写法都是可以的 #dp[i][j] =dp[i-1][j-1] and (p[i-1] ==s[j-1] or p[i-1] =='.') if p[i-1] ==s[j-1] or p[i-1] =='.': dp[i][j] =dp[i-1][j-1] return dp[-1][-1] Wildcard Matching Given an input string (s) and a pattern (p), implement wildcard pattern matching with support for ‘?’ and ‘*‘.‘?’ Matches any single character.‘*‘ Matches any sequence of characters (including the empty sequence). Input:s = “aa”p = “a”Output: falseExplanation: “a” does not match the entire string “aa”. Tips: Wildcard 通配符，这个和上一个基本相同啊， 1234567891011121314151617181920212223242526272829303132333435363738394041424344class Solution(object): """ 这个更加简单，while 就是能够搞定的，然后对于 特殊符号特殊判断。使用两个指针进行操作. s[i] ==p[j] 和 p[j] =='?' ，这个是可以放到同一个 if 条件下的。两者是等价的。 关键是 * 的匹配 2，在 p 中出现 * 时，记录 p 中 * 的位置，同时记录此时 s 的位置。 3，从 * 的后面的第一个字符开始匹配。如果匹配失败，返回 s 处，从 s++ 开始重新匹配。 """ def isMatch(self, s, p): """ :type s: str :type p: str :rtype: bool """ j = i = ss = 0; star = -1 # 首先把 string 中的字符比完 while i &lt; len(s): if j &lt; len(p) and (s[i] == p[j] or p[j] == '?'): i += 1; j += 1 continue # star 记录的是 j 的位置，相应的 ss 是记录的i (string) 中的位置 if j &lt; len(p) and p[j] == '*': star = j; j += 1; ss = i; continue # 如果已经有了 star 的出现， 到这里已经说明 star的下一个和 string 中的位置元素不是exact 的匹配 # 所以这里进行了 ss +=1 的操作是为了，相当于把 string 中的char 使用 * 进行了代替 # 好好理解一下 if star != -1: j = star + 1; ss += 1; i = ss continue return False # string 已经比较完了，如果只剩下 * 那么是可以行的，否则是不可行的 while j &lt; len(p) and p[j] == '*': j += 1 if j == len(p): return True return False Valid Parentheses Given a string containing just the characters ‘(‘, ‘)’, ‘{‘, ‘}’, ‘[‘ and ‘]’, determine if the input string is valid. Tips: 必须要使用 len(stack) 进行检测，因为中间的时候也可能 len(stack) 是等于0的，这时候只能是 append() ，不能访问 stack[-1] 123456789101112131415161718192021class Solution(object): def isMatch(self, l, r): return l =='[' and r==']' or l =='(' and r ==')' or l =='&#123;' and r =='&#125;' def isValid(self, s): len_s =len(s) if len_s ==0: return True stack =[] for ch in s: if len(stack) ==0 or not self.isMatch(stack[-1], ch): stack.append(ch) else: stack.pop() return len(stack) ==0 Generate Parentheses Given n pairs of parentheses, write a function to generate all combinations of well-formed parentheses. For example, given n = 3, a solution set is: [ “((()))”, “(()())”, “(())()”, “()(())”, “()()()”] Tips: dfs, left_count 表示是 ‘(‘ 的总数， left_remain 表示 left- right 的差值. 12345678910111213141516171819202122232425class Solution(object): def DogenerateParenthesis(self, n, left_count, left_remain, prefix): if n ==left_count and left_remain ==0: return [prefix] left =[] right =[] if left_count &lt;n: left =self.DogenerateParenthesis(n, left_count+1, left_remain+1, prefix+'(') if left_remain&gt;0: right =self.DogenerateParenthesis(n, left_count, left_remain-1, prefix+')') return left +right def generateParenthesis(self, n): """ :type n: int :rtype: List[str] """ if n ==0: return [] else: list =self.DogenerateParenthesis(n ,0, 0, "") return list Combination Sum Given a set of candidate numbers (candidates) (without duplicates) and a target number (target), find all unique combinations in candidates where the candidate numbers sums to target.The same repeated number may be chosen from candidates unlimited number of times. Tips: 这种找到所有符合题目要求的解，十之八九都是要使用递归。最优解（最值）一般是使用dp，减少子问题的运算。这里给出了 list 和dfs 的结合使用，通过传入 start index来解决是否遍历过的问题。 示意图： 12345678910111213141516171819202122class Solution(object): def dfs(self, candidates, target, start, intermedia, res): # target 这个变量调节了是 继续deeper or return， 每一次都是在变化的。如果 ==0，那么就return 了 if target ==0: res.append(intermedia) return for i in range(start, len(candidates)): if target &lt; candidates[i]: return self.dfs(candidates, target-candidates[i], i, intermedia+[candidates[i]], res) def combinationSum(self, candidates, target): """ :type candidates: List[int] :type target: int :rtype: List[List[int]] """ candidates.sort() res =[] self.dfs(candidates, target, 0, [], res) return res Combination Sum II Given a collection of candidate numbers (candidates) and a target number (target), find all unique combinations in candidates where the candidate numbers sums to target.Each number in candidates may only be used once in the combination. Tips： 这里给出了另外一种遍历list 和dfs 的方法，传入的是部分 list，上面那道题传入了完整的list。 123456789101112131415161718192021222324252627class Solution(object): # @param &#123;integer[]&#125; candidates # @param &#123;integer&#125; target # @return &#123;integer[][]&#125; def combinationSum2(self, candidates, target): candidates.sort() # 排序不影响 时间复杂度的，因为时间复杂度大于排序的时间复杂度 #res=set() res =[] self.findcombination(candidates,target,[],res) #return [list(i) for i in res] return res def findcombination(self,candidates,target,ls,res): if target==0 and ls not in res: # 对于 set() 中使用 add() ，list 中使用 append() #res.add(tuple(ls)) res.append(ls) return # 下面这个判断用和不用 都是相同的效果(时间和空间复杂度上) if target&lt;0: return # not use: c72 ms,11.7M for i in range(len(candidates)): if target&lt;candidates[i]: return self.findcombination(candidates[i+1:],target-candidates[i],ls+[candidates[i]],res) Permutations Given a collection of distinct integers, return all possible permutations. Input: [1,2,3]Output:[ [1,2,3], [1,3,2], [2,1,3], [2,3,1], [3,1,2], [3,2,1]] Tips : extend 是因为list of list ，而不是单独的list，这样能保证最后的结果还是 list of list 123456789101112131415161718192021class Solution(object): def permute(self, nums): return self.doPermute(nums) def doPermute(self, num_list): if len(num_list) ==1: return [num_list] res_list =[] for i in range(len(num_list)): num_list[0], num_list[i] =num_list[i], num_list[0] sub_list =self.doPermute(num_list[1:]) list_head =[num_list[0]] #new_list =list_head+ sub_list new_list = [list_head + list1 for list1 in sub_list] # 可以理解这个是 sub_list 是有一系列的解， 然后再每个解上都加上一个头元素 res_list.extend(new_list) # extend，The list.extend method extends a list by appending elements from an iterable # append 是当做一个整体进行操作 return res_list Permutations II Given a collection of numbers that might contain duplicates, return all possible unique permutations. Tips： 这个 duplicates 是通过 sort 函数，然后在选择 某个index 时候，进行判断一下是否和第一个重合，这样的方式去handle。 12345678910111213141516171819202122232425262728class Solution(object): def doPermuteUnique(self, nums): if len(nums) ==1: return [nums] res_list =[] for i in range(len(nums)): if i&gt;0 and nums[0] ==nums[i]: continue nums[0], nums[i] =nums[i], nums[0] sub_list =self.doPermuteUnique(nums[1:]) list_head =[nums[0]] new_list =[list_head +list1 for list1 in sub_list] res_list.extend(new_list) return res_list def permuteUnique(self, nums): """ :type nums: List[int] :rtype: List[List[int]] """ nums.sort() return self.doPermuteUnique(nums) Climbing Stairs You are climbing a stair case. It takes n steps to reach to the top.Each time you can either climb 1 or 2 steps. In how many distinct ways can you climb to the top?Note: Given n will be a positive integer. Tips： 数学题，斐波那契数列。 解法一： 12345678910111213141516171819202122class Solution(object): # 可以换成数学模型，发现就是 斐波那契数列 # 不使用数字，使用三个变量也是可以的额 def climbStairs(self, n): """ :type n: int :rtype: int """ if n ==1: return 1 elif n ==2: return 2 arr = [0] *(n+1) arr[1] =1 arr[2] =2 for i in range(3, n+1): arr[i] =arr[i-1] +arr[i-2] return arr[n] 解法二123456789101112def climbStairs(self, n): if n ==1: return 1 elif n ==2: return 2 a, b =1,2 c =0 for i in range(3, n+1): c = a+b a, b =b,c return c Combinations Given two integers n and k, return all possible combinations of k numbers out of 1 … n. Tips： 这个是处理的list 和 dfs()的问题，然后使用的传入 index和完整的 list 来控制进度。 12345678910111213141516171819202122class Solution(object): """ 好好理解递归这种逐渐加深的层次 """ def combine(self, n, k): res =[] self.dfs(list(range(1, n+1)), k, 0, [], res) return res def dfs(self, nums, k, index, path, res): # backtracking #if k &lt;0: #return # 这种 return 和result 结合使用的操作是经常常见的 if k ==0: res.append(path) return # 这个index 是很重要的， 在这个index 的基础上选择的 for i in range(index, len(nums)): self.dfs(nums, k-1, i +1, path+ [nums[i]], res) Subsets Given a set of distinct integers, nums, return all possible subsets (the power set). Tips: 使用的是第二种方式，传入部分list，从而由大问题转移成小问题。 12345678910111213141516class Solution(object): # 这种是最简单的深度优先的搜索了， def subsets(self, nums): res =[] self.dfs(nums, [], res) return res def dfs(self, nums, path, res): # 一般来说这个是有跳出条件，回溯的，但是这种情况是没有的，只有最后一个 # [[],[1],[1,2],[1,2,3],[1,3],[2],[2,3],[3]]， 当输出 [1, 2,3] 的时候，return，但是这个return 到了 [1, 3] 这个层次 res.append(path) for i in range(len(nums)): self.dfs(nums[i+1:], path+[nums[i]], res) Subsets II Given a collection of integers that might contain duplicates, nums, return all possible subsets (the power set). Tips： 这个含有duplicates，使用功能sort 然后在 for 循环 的时候进行判断一下。 123456789101112131415161718192021222324class Solution(object): # 递归 def subsetsWithDup(self, nums): """ :type nums: List[int] :rtype: List[List[int]] """ res =[] nums.sort() self.dfs(nums, 0, [], res) return res def dfs(self, nums, index, path, res): if path not in res: res.append(path) #res.append(path) for i in range(index, len(nums)): if i &gt; index and nums[i] ==nums[i-1]: continue self.dfs(nums, i+1, path+[nums[i]], res) # 下面的代码是错误的 memory 但是不知道为什么 Decode Ways A message containing letters from A-Z is being encoded to numbers using the following mapping: &apos;A&apos; -&gt; 1 &apos;B&apos; -&gt; 2 ... &apos;Z&apos; -&gt; 26 Given a non-empty string containing only digits, determine the total number of ways to decode it. Tips： 多少种解码方式。本质是裴波拉契数列, 感觉自己并没有get 到这个本质上是 该数列 1234567891011121314151617181920212223242526class Solution(object): """ DP[i] = DP[i-1] + DP[i-2] \ \___________(if str[i-2] exists and 10&lt;= int(str[i-1] + str[i]))&lt;=26 ) \___________(If str[i-1] exists and str[i] != '0' ) """ def numDecodings(self, s): """ :type s: str :rtype: int """ if not s: return 0 if s =='10': return 1 dp =[0] *(len(s) +1) dp[0] =1 for i in range(1, len(s)+1): if s[i-1] !='0': dp[i] +=dp[i-1] if i &gt;1 and '10' &lt;=s[i-2:i] &lt;='26': dp[i] += dp[i-2] return dp[-1] Binary Tree Inorder Traversal Given a binary tree, return the inorder traversal of its nodes’ values. Tips： 递归。 1234567891011121314151617# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution(object): # inorder 中序遍历, recursive 递归， iterative 迭代 # 这个是递归的 recursively def helper(self, root, res): if root: self.helper(root.left, res) res.append(root.val) self.helper(root.right, res) Tips： 递归的容易写，循环的也好会，使用的stack 先保存左子树，然后不断的node 其右子树。 12345678910111213141516def inorderTraversal(self, root): # 如果使用 迭代，那么就是 stack结构了 res, stack =[], [] while True: while root: stack.append(root) root =root.left if not stack: return res node =stack.pop() res.append(node.val) root =node.right Validate Binary Search Tree Given a binary tree, determine if it is a valid binary search tree (BST).Assume a BST is defined as follows: The left subtree of a node contains only nodes with keys less than the node’s key. The right subtree of a node contains only nodes with keys greater than the node’s key. Both the left and right subtrees must also be binary search trees. Tips： 二叉搜索树的特点，中序遍历，先得到遍历结果，然后判断是否是不减的（只是需要O(N)）. 1234567891011121314151617181920212223242526272829303132# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution(object): def isValidBST(self, root): """ :type root: TreeNode :rtype: bool """ output =[] self.inOrder(root, output) for i in range(1, len(output)): if output[i-1] &gt;= output[i]: return False return True def inOrder(self, root, output): if not root: return self.inOrder(root.left, output) output.append(root.val) self.inOrder(root.right, output) Same Tree Given two binary trees, write a function to check if they are the same or not.Two binary trees are considered the same if they are structurally identical and the nodes have the same value. Tips: 对应的值相同，对应的结构相同。 1234567891011121314151617181920212223242526# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution(object): # 根据上一一个题目的要求，这个是也是可以先进行遍历，然后再比较最后的遍历结果吗 def isSameTree(self, p, q): """ :type p: TreeNode :type q: TreeNode :rtype: bool """ if not p and not q: return True elif not p or not q: return False if p.val ==q.val: return self.isSameTree(p.left, q.left) and self.isSameTree(p.right, q.right) else: return False Symmetric Tree Given a binary tree, check whether it is a mirror of itself (ie, symmetric around its center). Tip： 对称和 same 是在于比较的方式是不一样的。 1234567891011121314151617181920212223242526272829303132# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution(object): # 该题目和 isSameTree 是有点相似的，只是修改部分代码就可以 def isSymmetric(self, root): """ :type root: TreeNode :rtype: bool """ # 这个如果是 [] 或者 None， 是返回true， 因为输入的形式是 list ，所以判断条件是 if root ==[], 这样形式 if not root: return True return self.dfs(root.left, root.right) def dfs(self, p, q): if not p and not q: return True elif not p or not q: return False if p.val == q.val: return self.dfs(p.left, q.right) and self.dfs(p.right, q.left) else: return False Binary Tree Zigzag Level Order Traversal Given a binary tree, return the zigzag level order traversal of its nodes’ values. (ie, from left to right, then right to left for the next level and alternate between). For example:Given binary tree [3,9,20,null,null,15,7], 3 / \ 9 20 / \ 15 7结果是这样的：[ [3], [20,9], [15,7]] Tips： 一行是从左往右，一行是从右往左。层序遍历的变体。从左向右使用 append() ，然后从右向左使用 insert()，这个是没有问题的。 12345678910111213141516171819202122232425262728# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution(object): # 层次遍历 + 奇偶性来决定是否 reverse def zigzagLevelOrder(self, root): """ :type root: TreeNode :rtype: List[List[int]] """ res = [] self.dfs(root, 0, res) return res def dfs(self, root, level, res): if root: if len(res) &lt; level + 1: res.append([]) if level % 2 == 0: res[level].append(root.val) else: res[level].insert(0, root.val) self.dfs(root.left, level+1, res) self.dfs(root.right, level+1, res) Maximum Depth of Binary Tree Given a binary tree, find its maximum depth.The maximum depth is the number of nodes along the longest path from the root node down to the farthest leaf node.Note: A leaf is a node with no children. Tips: 左右子树的max+1，这个是树的深度。 12345678910111213141516171819# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution(object): # 递归， 最简单的方式 def maxDepth(self, root): """ :type root: TreeNode :rtype: int """ if not root: return 0 # 这个是最简单的代码了 return 1 +max(self.maxDepth(root.left), self.maxDepth(root.right)) Binary Tree Level Order Traversal II Given a binary tree, return the bottom-up level order traversal of its nodes’ values. (ie, from left to right, level by level from leaf to root). Tips: 层序遍历，但是 res 需要存储成list of list，这样最后进行reverse，能够表示出 层数的信息。 1234567891011121314151617181920212223242526272829# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution(object): # 需要有一个 level的index， 然后翻转就行了 def levelOrderBottom(self, root): """ :type root: TreeNode :rtype: List[List[int]] """ res =[] self.dfs(root, 0, res) #res 这个是整体的导致，一层 element的倒置，不涉及 element内部的倒置 return res[::-1] def dfs(self, root, level, res): if root: if len(res) &lt; level+1: res.append([]) # 这个是append 一个空的 [] 这种结构，然后下面使用该 list；否则的话 直接进行append res[level].append(root.val) # 这个很重要哦 self.dfs(root.left, level+1, res) self.dfs(root.right, level +1, res) Convert Sorted Array to Binary Search Tree Given an array where elements are sorted in ascending order, convert it to a height balanced BST.For this problem, a height-balanced binary tree is defined as a binary tree in which the depth of the two subtrees of every node never differ by more than 1. Tips： 不减的array 就是 binary search tree 中的中序遍历的结果。递归思想，先要找到 root，然后划分左右子树。递归进行。 1234567891011121314151617181920212223242526# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution(object): # balance tree 这种是递归进行定义的，左右子树相差最多为1 # 主要是不太清楚 如何保证这种 balanced tree def sortedArrayToBST(self, nums): """ :type nums: List[int] :rtype: TreeNode """ if not nums: return None mid =len(nums)//2 root =TreeNode(nums[mid]) root.left =self.sortedArrayToBST(nums[:mid]) root.right =self.sortedArrayToBST(nums[mid+1:]) return root Balanced Binary Tree Given a binary tree, determine if it is height-balanced.For this problem, a height-balanced binary tree is defined as:a binary tree in which the depth of the two subtrees of every node never differ by more than 1. Tip：左右子树的差值不能大于1 为balanced，这个盘别题目，比较容易，重点是 getHeight() 的实现、根节点是 balanced，左右子树也是balanced 1234567891011121314151617181920212223242526# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution(object): # 使用了之前的 求解 树的height 的东西，然后使用定义进行做题 def isBalanced(self, root): """ :type root: TreeNode :rtype: bool """ if not root: return True return abs(self.getHeight(root.left) -self.getHeight(root.right))&lt;2 and self.isBalanced(root.left) and self.isBalanced(root.right) def getHeight(self, root): if not root: return 0 return 1 +max(self.getHeight(root.left), self.getHeight(root.right)) Minimum Depth of Binary Tree Given a binary tree, find its minimum depth.The minimum depth is the number of nodes along the shortest path from the root node down to the nearest leaf node.Note: A leaf is a node with no children. Tip：求 height的变形，如果是叶子节点，那么返回 1+max(left, right) 否则的话，返回左右子树中较小的高度。如果是求高度，那么就不管了什么情况下都是返回 max()+1 1234567891011121314151617181920212223# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution(object): # 求解根节点的左右子树的最小高度 # 如果左右子树都有，那么就是调用该函数， 如果有一个又，那么直接求解高度就行了 def minDepth(self, root): """ :type root: TreeNode :rtype: int """ if not root: return 0 if not root.left or not root.right: # 这个是求解高度的 return 1 +max(self.minDepth(root.left), self.minDepth(root.right)) else: return min(self.minDepth(root.left), self.minDepth(root.right))+1 Path Sum Given a binary tree and a sum, determine if the tree has a root-to-leaf path such that adding up all the values along the path equals the given sum.Note: A leaf is a node with no children. Tips: 有条件的dfs(), 有条件的进行树的路径，树在加深的同时，target 数字也是不断的减少，最后如果相等，那么就是一个合适的解。 12345678910111213141516171819202122232425262728293031323334353637383940# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution(object): # 树的路径 # 总结一下 any all 这种到自己的博客 """ any: Returns true if any of the items is True. It returns False if empty or all are false. Any can be thought of as a sequence of OR operations on the provided iterables. all: Returns true if all of the items are True (or if the iterable is empty). All can be thought of as a sequence of AND operations on the provided iterables. It also short circuit the execution i.e. stop the execution as soon as the result is known. """ def hasPathSum(self, root, sum): """ :type root: TreeNode :type sum: int :rtype: bool """ res =[] self.dfs(root, sum, res) return any(res) def dfs(self, root, target, res): if not root: return False # 对于叶子结点的定义 if not root.left and not root.right: if root.val == target: res.append(True) if root.left: self.dfs(root.left, target-root.val, res) if root.right: self.dfs(root.right, target-root.val, res) Path Sum II Given a binary tree and a sum, find all root-to-leaf paths where each path’s sum equals the given sum.Note: A leaf is a node with no children. Tips： 这个和上面的区别在于，一个是 true or false，一个find all paths，所以需要有一个变量去存储正确的路径。 12345678910111213141516171819202122232425262728293031# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution(object): # 上一道题目是 return true or false，这个是找到所有的路径 def pathSum(self, root, sum): """ :type root: TreeNode :type sum: int :rtype: List[List[int]] """ res =[] self.dfs(root, sum, [], res) return res def dfs(self, root, target, path, res): if not root: return [] if not root.left and not root.right: if root.val == target: res.append(path+[root.val]) if root.left: #这种条件是可以减少迭代的次数 self.dfs(root.left, target-root.val, path+[root.val], res) if root.right: self.dfs(root.right, target-root.val, path+[root.val], res) Flatten Binary Tree to Linked List Given a binary tree, flatten it to a linked list in-place. Tips： 比较有意思，将 tree 的左右子树 flatten 成 linked list的左右结点。其中的 self.pre 就类似一种全局变量，将整个遍历， 1234567891011121314151617181920212223242526272829# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution(object): # 这种 flatten 就是 拉平（先序遍历）， 然后转成linkedlist # 并且这种操作是要求 in-place的 def __init__(self): self.pre =TreeNode('dummy') def flatten(self, root): """ :type root: TreeNode :rtype: None Do not return anything, modify root in-place instead. """ if not root: return tmp =root.right # 这个保存下来，是为了下面的flatten 使用 self.pre.right =root self.pre.left =None self.pre =root self.flatten(root.left) self.flatten(tmp) Populating Next Right Pointers in Each Node You are given a perfect binary tree where all leaves are on the same level, and every parent has two children. The binary tree has the following definition: Tip，属于树的结构的优化，多了一个next 指针指向的是同层的右节点。这个树的操作一般是 in-place，所以在某个递归过程中 return 是不必return value，本生就是在修改。 123456789101112131415161718192021222324252627282930"""# Definition for a Node.class Node(object): def __init__(self, val, left, right, next): self.val = val self.left = left self.right = right self.next = next"""class Solution(object): # perfect binary tree, # 题目的要求， populate each next pointer to its next right node def helper(self, left, right): if not left or not right: return left.next = right # 三种关系，先后顺序是没有关系的 self.helper(left.left, left.right) self.helper(left.right, right.left) self.helper(right.left, right.right) def connect(self, root): if not root: return self.helper(root.left, root.right) return root Populating Next Right Pointers in Each Node II Given a binary tree struct Node { int val; Node left; Node right; Node *next; }Populate each next pointer to point to its next right node. If there is no next right node, the next pointer should be set to NULL. Tips：注意从图片上观察这一题和上一题的区别，这图中表民一个子树的左子树是可以指向另一个子树的右子树，说明这个明显类似层次遍历，而不像上一题那样。 123456789101112131415161718192021222324252627282930313233"""# Definition for a Node.class Node(object): def __init__(self, val, left, right, next): self.val = val self.left = left self.right = right self.next = next"""class Solution(object): # 这个是不太明白的 # 这个相对于上一道题目，只是少了 perfect binary tree, 翻译成中文，满二叉树（完美二叉树），包括最后一层都是满的 def connect(self, root): if root is None: return None queue = [root] while queue: prev,curr = None,None size = len(queue) # 有点类似层次遍历的意思 for i in range(size): curr = queue.pop(0) # 这个 if 只有在for 之内才是有效的，第一次是无效的 if prev : prev.next = curr if curr.left: queue.append(curr.left) if curr.right: queue.append(curr.right) prev = curr curr.next = None return root Binary Tree Maximum Path Sum Given a non-empty binary tree, find the maximum path sum.For this problem, a path is defined as any sequence of nodes from some starting node to any node in the tree along the parent-child connections. The path must contain at least one node and does not need to go through the root. Input: [-10,9,20,null,null,15,7] -10 / \ 9 20 / \ 15 7Output: 42 Tips： 这个的难点在于，可以从任意点开始，然后再任意点结束，并且过不过根节点都是可以的。 分制到底部，在返回的时候传入左右任意一遍最大值加上目前root.val:cur = max(left, right) + root.val 这种情况处理了从Root到左右任意一边的最大值，也就是 root.val + left 和 root.val + right； 还有一种情况就是当最大值 = root.val + left + right， 我们在放入global变量的时候何其比较。 对于最底部叶子节点传上来的值，我们将其设置成0: return cur if cur &gt; 0 else 0 12345678910111213141516171819202122232425262728# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution(object): # 根据以往的经验，树的递归解法一般都是递归到叶节点，然后开始边处理边回溯到根节点。 # 但是这个题目不是， 这个是可以任意 start， 任意 end，然后不一定要经过根节点 def maxPathSum(self, root): """ :type root: TreeNode :rtype: int """ # 使用 self 标志 就意味这个是一种全局的变量， 类似在 init 中进行初始化的 self.res = - float('inf') self.dfs(root) return self.res def dfs(self, root): if not root: return 0 left = self.dfs(root.left) right = self.dfs(root.right) self.res = max(self.res, left + right + root.val) cur = max(left, right) + root.val return cur if cur &gt; 0 else 0 Sum Root to Leaf Numbers Given a binary tree containing digits from 0-9 only, each root-to-leaf path could represent a number.An example is the root-to-leaf path 1-&gt;2-&gt;3 which represents the number 123.Find the total sum of all root-to-leaf numbers.Note: A leaf is a node with no children. Input: [1,2,3] 1 / \ 2 3Output: 25Explanation:The root-to-leaf path 1-&gt;2 represents the number 12.The root-to-leaf path 1-&gt;3 represents the number 13.Therefore, sum = 12 + 13 = 25. Tips: 路径组成的数字代表一个数字，然后所有的路径和相加起来。关键代码只要 cur =pre*10 + root.val， 还是树的路径的遍历吧。 123456789101112131415161718192021222324252627282930313233343536# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution(object): # 对于树的 类型，大概就是这样了， 递归，找出递归的跳出的条件，然后处理保存结果 def sumNumbers(self, root): """ :type root: TreeNode :rtype: int """ self.result =0 self.sumNum(root, 0) return self.result def sumNum(self, root, pre): if not root: return cur = pre *10 +root.val if not root.left and not root.right: self.result += cur return if root.left: self.sumNum(root.left, cur) if root.right: self.sumNum(root.right, cur) Binary Tree Preorder Traversal Given a binary tree, return the preorder traversal of its nodes’ values. Tips：非递归版本（迭代），使用栈（递归的思想就是栈的思想）。 1234567891011121314151617181920212223242526272829# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution(object): # Recursive solution is trivial, could you do it iteratively? def preorderTraversal(self, root): """ :type root: TreeNode :rtype: List[int] """ if not root: return [] res, queue =[], [root] while queue: cur =queue.pop() if cur: res.append(cur.val) queue.append(cur.right) queue.append(cur.left) #queue.append(cur.right) return res LRU Cache Design and implement a data structure for Least Recently Used (LRU) cache. It should support the following operations: get and put.get(key) - Get the value (will always be positive) of the key if the key exists in the cache, otherwise return -1.put(key, value) - Set or insert the value if the key is not already present. When the cache reached its capacity, it should invalidate the least recently used item before inserting a new item.The cache is initialized with a positive capacity. Tips：这个题目纯粹手解，太麻烦了，在python3 中有 collections.OrderedDict() 的实现，这是作弊的写法。特点在于dict +队列（不完全是队列，因为访问之后还会放到队列的最后，而不是弹出）。因为一般的dict 存储的时候是无序（不是按照放入的先后书序），ordereddict 是按照放入的先后顺序进行存储的。题目本身就是先进先出的队列，只不过存储的是 (key, value) 这样的键值对。使用get 的时候，get到一个不能删除，应该放到最后；put的时候在 OrderedDict.popitem()有一个可选参数last（默认为True），当last为True时它从OrderedDict中删除最后一个键值对并返回该键值对，当last为False时它从 OrderedDict中删除第一个键值对并返回该键值对。 123456789101112131415161718192021222324252627282930class LRUCache(object): # python3 environment def __init__(self, capacity): self.size =capacity self.cache = collections.OrderedDict() def get(self, key): if key not in self.cache: return -1 val =self.cache[key] self.cache.move_to_end(key) # Python &gt;= 3.2 return val def put(self, key, val): if key in self.cache: del self.cache[key] self.cache[key] =val if len(self.cache) &gt; self.size: self.cache.popitem(last= False) # Your LRUCache object will be instantiated and called as such:# obj = LRUCache(capacity)# param_1 = obj.get(key)# obj.put(key,value) Insertion Sort List Sort a linked list using insertion sort. Tips： linkedlist 擅长于修改元素（直接修改指向），其中的 if while 是经常搭配使用，发现.. 然后就处理… 1234567891011121314151617181920212223242526272829303132# Definition for singly-linked list.# class ListNode(object):# def __init__(self, x):# self.val = x# self.next = Noneclass Solution(object): # 前插 def insertionSortList(self, head): """ :type head: ListNode :rtype: ListNode """ p =dummy =ListNode(0) cur =dummy.next =head while cur and cur.next: next_val =cur.next.val if cur.val &lt;= next_val: cur =cur.next continue # the sequence is not sorted inorder # find the proper situation # 从头开始找 if p.next.val &gt; next_val: p =dummy while p.next.val &lt;= next_val: p =p.next p.next, cur.next.next, cur.next =cur.next, p.next, cur.next.next return dummy.next Sort List Sort a linked list in O(n log n) time using constant space complexity. Input: 4-&gt;2-&gt;1-&gt;3 Output: 1-&gt;2-&gt;3-&gt;4 Tips: mergesort 的思想，显示把list 分成left and right（分），然后最后merge 算法 1234567891011121314151617181920212223242526272829303132333435# Definition for singly-linked list.# class ListNode(object):# def __init__(self, x):# self.val = x# self.next = Noneclass Solution(object): def merge(self, h1,h2): dummy =tail =ListNode(-1) while h1 and h2: if h1.val &lt; h2.val: tail.next, h1 =h1, h1.next else: tail.next, h2 =h2, h2.next tail =tail.next tail.next =h1 or h2 return dummy.next def sortList(self, head): if not head or not head.next: return head pre, slow, fast =None, head, head # slow fast 直接是两种快慢的不影响的index 遍历方式，slow 是下一个链表的结点 while fast and fast.next: pre, slow, fast =slow, slow.next, fast.next.next pre.next =None # 下面的两种写法是等价的 return self.merge(self.sortList(head), self.sortList(slow)) # return self.merge(*map(self.sortList, (head, slow))) Number of Islands Given a 2d grid map of ‘1’s (land) and ‘0’s (water), count the number of islands. An island is surrounded by water and is formed by connecting adjacent lands horizontally or vertically. You may assume all four edges of the grid are all surrounded by water. Tips: 这个dfs 跟之前的不一样之处在于，需要对于每个点进行 dfs() ，其他的还好。提供了两种解法，第一种比较代码比较少。比较喜欢第一种代码的风格，这样两个函数看起来比较均衡。 https://leetcode.com/problems/number-of-islands/ 解法一：1234567891011121314151617181920212223242526272829class Solution(object): def numIslands(self, grid): """ :type grid: List[List[str]] :rtype: int """ if not grid: return 0 count =0 for i in range(len(grid)): for j in range(len(grid[0])): if grid[i][j] =='1': self.dfs(grid, i,j) # 写法比较巧妙 count +=1 return count def dfs(self, grid, i,j): if i &lt;0 or j&lt;0 or i&gt;=len(grid) or j&gt;= len(grid[0]) or grid[i][j] !='1': return grid[i][j] ='0' self.dfs(grid, i+1, j) self.dfs(grid, i-1, j) self.dfs(grid, i, j+1) self.dfs(grid, i, j-1) 解法二：两种思想一样，写法不一样。 12345678910111213141516171819202122232425262728293031323334353637383940class Solution(object): def numIslands(self, grid): """ :type grid: List[List[str]] :rtype: int """ if not grid: return 0 used =[ [False]* len(grid[0]) for _ in range(len(grid))] count =0 for i in range(len(grid)): for j in range(len(grid[0])): num =self.dfs(grid, used, len(grid)-1, len(grid[0])-1, i, j) if num&gt;0: count +=1 return count def dfs(self, grid, used, row, col, x, y): if grid[x][y] =='0' or used[x][y]: return 0 used[x][y] =True num =1 if x!=0: num += self.dfs(grid, used, row, col, x -1, y) if x !=row: num += self.dfs(grid, used, row, col, x +1,y) if y!=0: num += self.dfs(grid, used, row, col, x, y-1) if y !=col: num += self.dfs(grid, used, row, col, x, y+1) return num Super Egg Drop You are given K eggs, and you have access to a building with N floors from 1 to N.Each egg is identical in function, and if an egg breaks, you cannot drop it again.You know that there exists a floor F with 0 &lt;= F &lt;= N such that any egg dropped at a floor higher than F will break, and any egg dropped at or below floor F will not break. 1234567891011121314151617181920212223class Solution(object): def gameOfLife(self, board): # 纯粹的count，之后的判断是下面决定的 def count(x, y): res =0 # 遍历 点的四周 for r in range(x-1, x+2): for c in range(y-1, y+2): if (r!= x or c!=y) and 0&lt;= r &lt; len(board) and 0&lt;= c &lt; len(board[0]) and board[r][c] &gt;0: res +=1 return res for x in range(len(board)): for y in range(len(board[0])): board[x][y] =count(x, y) +1 if board[x][y] ==1 else -count(x, y) # if board[x][y] == 1, change its value to count(x,y) + 1, the reason I add 1 is to keep it positive for x in range(len(board)): for y in range(len(board[0])): board[x][y] = 1 if board[x][y] in &#123;3, 4, -3&#125; else 0 # &#123;2, 3, -3&#125; Kth Smallest Element in a BST Given a binary search tree, write a function kthSmallest to find the kth smallest element in it. Tips: 二分查找树，中序遍历就是不减的list .有递归，迭代两个版本，共三种实现。倾向于使用第二个版本。迭代，然后使用k 进行及时的跳出。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475# Definition for a binary tree node.class TreeNode(object): def __init__(self, x): self.val = x self.left = None self.right = Noneclass Solution(object): # BST 中序遍历 得到一个不减的list，然后就可得第k 小的元素 # 下面是递归版本 ''' def kthSmallest(self, root, k): """ :type root: TreeNode :type k: int :rtype: int """ if not root: return res =[] self.dfs(root, res) if len(res) +1&lt;k: return return res[k-1] def dfs(self, root, res): if not root: return #res.append(root.val) self.dfs(root.left, res) res.append(root.val) self.dfs(root.right, res) ''' ''' # 相比于第一种方式，时间上是有减少的 def kthSmallest(self, root,k ): stack =[] node =root while True: if node: stack.append(node) node =node.left else: node =stack.pop() # 使用计数的方式进行访问，减少了空间复杂度 k -=1 if not k: break node =node.right return node.val ''' # 这个代码就是有点 抖机灵的那种，如果使用了 try ..catch.. 那么exception 就不会报错 def kthSmallest(self, root, k): def inorder(root, k): if root: inorder(root.left, k) if k ==1: raise Exception(root.val) inorder(root.right, k-1) #return k try: inorder(root, k) except Exception as e: return e.message Lowest Common Ancestor of a Binary Tree Given a binary tree, find the lowest common ancestor (LCA) of two given nodes in the tree.According to the definition of LCA on Wikipedia: “The lowest common ancestor is defined between two nodes p and q as the lowest node in T that has both p and q as descendants (where we allow a node to be a descendant of itself).” Given the following binary tree: root = [3,5,1,6,2,0,8,null,null,7,4] Tips: 这道Follow Up没有BST的特性，所以要对几种case一个一个进行测试。Condition为两种：如果没找到，返回None，找到则返回当前的root(因为找到一个root就不需要继续深入)比对方式： 如果parent的左右孩子都有返回，说明parent就是LCA 如果左边没有返回：则右边返回的就是LCA 如果右边没有返回：则左边返回的就是LCA 讲解 https://leetcode.com/problems/lowest-common-ancestor-of-a-binary-tree/ 1234567891011121314151617181920212223242526272829# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution(object): def lowestCommonAncestor(self, root, p, q): """ :type root: TreeNode :type p: TreeNode :type q: TreeNode :rtype: TreeNode """ if not root: return None if p ==root or q ==root: return root left =self.lowestCommonAncestor(root.left, p, q) right =self.lowestCommonAncestor(root.right, p, q) if left and right: return root if not left: return right if not right: return left Serialize and Deserialize Binary Tree Serialization is the process of converting a data structure or object into a sequence of bits so that it can be stored in a file or memory buffer, or transmitted across a network connection link to be reconstructed later in the same or another computer environment. Tips: 序列化主要是用在 存储和传输上吧. 基于 队列进行实现。队列可以两边进行修改。先序遍历 https://leetcode.com/problems/serialize-and-deserialize-binary-tree/ 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Codec: # 先序遍历 def serialize(self, root): if not root: return "" queue =collections.deque([root]) res =[] # 这个就是一种循环先序遍历二叉树 while queue: # 使用 pop 和 append() 操作右边 node =queue.popleft() # 在队列中使用 popleft 和appendleft() 直接操作队列的左边的增减 if node: queue.append(node.left) queue.append(node.right) res.append(str(node.val) if node else '#') # 使用 # 表示是一种none return ','.join(res) # 使用, 隔开每个node def deserialize(self, data): if not data: return None nodes =data.split(',') root =TreeNode(int(nodes[0])) queue =collections.deque([root]) index =1 # 作为string 的index while queue: node =queue.popleft() if nodes[index] != "#": # nodes[index] is not '#' 这样写也是可以的 node.left =TreeNode(int(nodes[index])) queue.append(node.left) index +=1 if nodes[index] != "#": node.right =TreeNode(int(nodes[index])) queue.append(node.right) index +=1 return root # Your Codec object will be instantiated and called as such:# codec = Codec()# codec.deserialize(codec.serialize(root)) Binary Tree Maximum Path Sum Given a non-empty binary tree, find the maximum path sum.For this problem, a path is defined as any sequence of nodes from some starting node to any node in the tree along the parent-child connections. The path must contain at least one node and does not need to go through the root. Tips: 这个不是树的路径，可以从任意非根节点出发。 1234567891011121314151617181920212223242526272829# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution(object): # 根据以往的经验，树的递归解法一般都是递归到叶节点，然后开始边处理边回溯到根节点。 # 但是这个题目不是， 这个是可以任意 start， 任意 end，然后不一定要经过根节点 def maxPathSum(self, root): """ :type root: TreeNode :rtype: int """ # 这种全局变量的设置确实是必须的，当携带变量的时候就出错了 self.res = - float('inf') self.dfs(root) return self.res def dfs(self, root): if not root: return 0 left = self.dfs(root.left) right = self.dfs(root.right) self.res = max(self.res, left + right + root.val) cur = max(left, right) + root.val return cur if cur &gt; 0 else 0 Number of Islands Given a 2d grid map of ‘1’s (land) and ‘0’s (water), count the number of islands. An island is surrounded by water and is formed by connecting adjacent lands horizontally or vertically. You may assume all four edges of the grid are all surrounded by water. Tips: dfs 12345678910111213141516171819202122232425262728293031class Solution(object): # dfs 是一个中规中矩的算法 # 这种方式更加简洁一点，直接使用 grid[i][j] 是否等于1 进行操作， # 然后如果能返回，在主程序中进行计数，最后的结果比较nice def numIslands(self, grid): """ :type grid: List[List[str]] :rtype: int """ if not grid: return 0 count =0 for i in range(len(grid)): for j in range(len(grid[0])): if grid[i][j] =='1': self.dfs(grid, i,j) count +=1 return count def dfs(self, grid, i, j): if i&lt;0 or j&lt;0 or i&gt;=len(grid) or j &gt;=len(grid[0]) or grid[i][j]!='1': return grid[i][j] ='#' self.dfs(grid, i+1, j) self.dfs(grid, i-1, j) self.dfs(grid, i,j +1) self.dfs(grid, i, j-1) Course Schedule There are a total of n courses you have to take, labeled from 0 to n-1.Some courses may have prerequisites, for example to take course 0 you have to first take course 1, which is expressed as a pair: [0,1]Given the total number of courses and a list of prerequisite pairs, is it possible for you to finish all courses? Tips: dfs 先修课程 12345678910111213141516171819202122232425262728293031323334353637383940414243class Solution(object): """ 这种解释是比较nice的，使用 0 -1 和1 分别表示初始化，正在访问和已经完成 if node v has not been visited, then mark it as 0. if node v is being visited, then mark it as -1. If we find a vertex marked as -1 in DFS, then their is a ring. if node v has been visited, then mark it as 1. If a vertex was marked as 1, then no ring contains v or its successors. """ def canFinish(self, numCourses, prerequisites): """ :type numCourses: int :type prerequisites: List[List[int]] :rtype: bool """ graph = [[] for _ in range(numCourses)] visited = [0 for _ in range(numCourses)] # create graph for pair in prerequisites: x, y = pair graph[x].append(y) # visit each node for i in range(numCourses): if not self.dfs(graph, visited, i): return False return True def dfs(self, graph, visited, i): # if ith node is marked as being visited, then a cycle is found if visited[i] == -1: return False # if it is done visted, then do not visit again if visited[i] == 1: return True # mark as being visited visited[i] = -1 # visit all the neighbours for j in graph[i]: if not self.dfs(graph, visited, j): return False # after visit all the neighbours, mark it as done visited visited[i] = 1 return True Course Schedule II There are a total of n courses you have to take, labeled from 0 to n-1.Some courses may have prerequisites, for example to take course 0 you have to first take course 1, which is expressed as a pair: [0,1]Given the total number of courses and a list of prerequisite pairs, return the ordering of courses you should take to finish all courses.There may be multiple correct orders, you just need to return one of them. If it is impossible to finish all courses, return an empty array. Tips: 和上一题相似，dfs 求解的是路径问题，而不是最值。 1234567891011121314151617181920212223242526272829303132333435363738class Solution(object): # 上一题是true or false 这个题目要求给个能够完成的路径，哎 # 不得不说这个是图的知识点呀 def findOrder(self, numCourses, prerequisites): """ :type numCourses: int :type prerequisites: List[List[int]] :rtype: List[int] """ def dfs(i, visited, graph, res): if visited[i] ==1: return True if visited[i] ==-1: return False visited[i] =-1 for n in graph[i]: if not dfs(n, visited, graph, res): return False res.append(i) visited[i] =1 return True visited =[0] * numCourses graph =&#123;x :[] for x in range(numCourses)&#125; # 注意这个顺序，因为最后要的是路径，所以这样是更加合理的 for p in prerequisites: graph[p[1]].append(p[0]) res =[] for i in range(numCourses): if not dfs(i, visited, graph, res): return [] return res[::-1]]]></content>
      <categories>
        <category>算法</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Stack Algorithm]]></title>
    <url>%2F2019%2F05%2F06%2Fstack_algorithm%2F</url>
    <content type="text"><![CDATA[栈数据结构的介绍，并以LeetCode为例实现。 在 计算机中， 栈是一种抽象的数据结构，后进先出是栈中元素的特点。在深度优先算法中需要栈的实现。支持两种基本的操作 push，添加元素到容器（collection）中 pop，去除最近添加进去的 无论是在C++ 还是在python 中，为了高效实现栈的操作，往往添加以下的判别函数： peek() ，获取栈顶元素，没有弹出 isFull()， 判断栈是否满 isEmpty() ，判断栈是否空 不同语言实现有所差异。 参考文献 1). Score of Parentheses C++ 实现，使用了 stack 数据结构，时间复杂度是 $O(n)$， $n$ 表示字符串的长度。 1234567891011121314151617181920212223class Solution &#123;public: int scoreOfParentheses(string S) &#123; stack&lt;int&gt; st; st.push(0); for (auto &amp; ch : S) //在遍历的时候为了节省内存，使用引用 &#123; if(ch =='(') st.push(0); else &#123; int t =st.top(); st.pop(); if (t ==0) st.top() ++; else st.top() += 2* t; &#125; &#125; # 因为这个是 balanced parentheses 对称的，所以最后一定剩下了一个 return st.top(); &#125;&#125;; python 中没有 栈这个数据结构，直接使用 list实现就ok。 12345678910111213141516class Solution: def scoreOfParentheses(self, S: str) -&gt; int: stack =[] stack.append(0) for ch in S: if ch =='(': stack.append(0) else: # 这个相当于遇到了右括号，那么肯定是要弹出的 t =stack[-1] stack.pop() if t ==0: stack[-1] +=1 else: stack[-1] += t* 2 return stack[-1] 2). Next Greater Element I nums1 是子集，nums2 是总的数字。这个是可以暴力求解，然后时间复杂度是是$O(mn)$。但是可以使用 hash 表和栈进行优化，优化后的时间复杂度是$O(m)$，其中$m$ 是nums2 的长度，空间复杂度是$O(m)$。hash表中key 表示nums2 中的每个数字，value 表示第一个比起大的元素。使用的栈是一个单调栈。 1234567891011121314151617181920212223class Solution &#123;public: // 栈实际上是递减的（从栈底到栈顶） vector&lt;int&gt; nextGreaterElement(vector&lt;int&gt;&amp; nums1, vector&lt;int&gt;&amp; nums2) &#123; unordered_map&lt;int, int&gt; hash; stack&lt;int&gt; st; vector&lt;int&gt; res; for(auto num : nums2) &#123; while( !st.empty() &amp;&amp; st.top() &lt; num) &#123; hash[st.top()] = num; st.pop(); &#125; st.push(num); &#125; for(auto num : nums1) &#123; res.emplace_back(hash.count(num) ? hash[num] : -1); // 这个三木判断还是有点意思的 &#125; return res; &#125;&#125;; 使用python 实现。 其中的 while 重复体现了单调栈， 栈中的元素可以理解为未找到比其大的元素。123456789101112131415class Solution: def nextGreaterElement(self, nums1: List[int], nums2: List[int]) -&gt; List[int]: dic =&#123;&#125; stack =[] res =[] for num in nums2: while stack and stack[-1] &lt; num: dic[stack[-1]] =num stack.pop() stack.append(num) print(dic) for num in nums1: res.append( dic[num] if num in dic else -1) return res 3). Next Greater Element II 将原数组复制一份放在原数组之后，可以处理环形问题。和上一题的思路一样，还是 $O(n)$ 的时间复杂度。在实现上需要注意，这里的stack 存放的是 index 而不是上一题的数字。 123456789101112131415161718192021class Solution &#123;public: vector&lt;int&gt; nextGreaterElements(vector&lt;int&gt;&amp; nums) &#123; int n =nums.size(); vector&lt;int&gt; res(n, -1); stack&lt;int&gt; st; for(int i =0; i&lt; 2*n ; i++) &#123; int num =nums[i %n]; while ( ! st.empty() &amp;&amp; nums[st.top() ]&lt; num) &#123; res[st.top()] = num; st.pop(); &#125; if(i &lt; n) st.push(i); &#125; return res; &#125;&#125;; 4). Remove Outermost Parentheses 去除最外层的括号，对于嵌套的括号，最后保留一个括号进行。成对出现的括号问题,大多数都可以使用 stack 进行求解，并且当 ) 时是弹出队列，当( 的时候是弹入队列。这两个是不变的操作，其他的可以根据题目进行求解。 123456789101112class Solution: def removeOuterParentheses(self, S: str) -&gt; str: stack =[] ans ="" for ch in S: if ch ==')': stack.pop() if stack : ans += ch if ch =="(": stack.append(ch) return ans Remove All Adjacent Duplicates In String 如果当前的元素和栈顶元素相同，那么弹出栈顶元素，否则入栈。最后将栈中所有元素输出，翻转。时间复杂度是$O(N)$ 12345678910111213class Solution: def removeDuplicates(self, S: str) -&gt; str: stack =[] for ch in S: if stack and stack[-1] == ch: stack.pop() else: #当不满足条件之后添加 和 无论如何都要添加 还是两个不同的方式 stack.append(ch) res ="" while stack: res += stack[-1] stack.pop() return res[::-1] 12345678910111213141516171819202122class Solution &#123;public: string removeDuplicates(string S) &#123; stack&lt;char&gt; st; for(auto ch : S) &#123; if(st.size() &amp;&amp; st.top() ==ch) # 这种是常见的操作， st.size() 判断是否为空 &amp;&amp; st.top() 是逻辑判断 st.pop(); else st.push(ch); &#125; string res ; while (st.size()) &#123; auto t = st.top(); res += t; st.pop(); &#125; reverse(res.begin(), res.end()); return res; &#125;&#125;; Backspace String Compare 12345678910111213141516171819202122232425262728293031323334353637383940class Solution &#123;public: // 在想一个问题： 什么时候是用到栈的数据结构? // 栈是有一定顺序的，如果有逆序访问的习惯，栈是一个很好的开始 vector&lt;int&gt; asteroidCollision(vector&lt;int&gt;&amp; asteroids) &#123; int n =asteroids.size(); stack&lt;int&gt; st; vector&lt;int&gt; res; for(int aster : asteroids) &#123; // 逆向 怼 if (aster &lt; 0) &#123; while(st.size() &amp;&amp; st.top() &lt; -aster) &#123; st.pop(); &#125; if(st.empty()) res.emplace_back(aster); else &#123; if (st.top() == -aster) st.pop(); &#125; &#125; else st.push(aster); &#125; // 现在st 中保存的是 幸存的结果 vector&lt;int&gt; t; while(st.size()) &#123; t.emplace_back(st.top()); st.pop(); &#125; for(auto it =t.rbegin(); it != t.rend(); it ++) res.emplace_back(*it); return res; &#125;&#125;; Decoded String at Index 使用 cnt统计变形之后字符串的总长度，$K$ 是总的长度， $i$ 表示string 的长度。 123456789101112131415161718192021222324class Solution &#123;public: string decodeAtIndex(string S, int K) &#123; long i =0, cnt =0; for(; cnt &lt; K; i++) &#123; cnt =isdigit(S[i]) ? cnt *(S[i] -'0') : (cnt +1); &#125; while(i --) &#123; if(isdigit(S[i])) &#123; cnt /= (S[i] -'0'); K %= cnt; &#125; else &#123; if (K %cnt ==0) return string(1, S[i]); cnt --; &#125; &#125; return ""; &#125;&#125;; Minimum Add to Make Parentheses Valid 括号匹配题一般想到用栈 该题目中 stack 中永远存放的是左括号，遇到右括号，如果栈中有左括号（只可能是左括号），那么就弹出；否则加上右括号的个数。最后加上左括号的个数。 1234567891011121314151617class Solution(object): def minAddToMakeValid(self, S): """ :type S: str :rtype: int """ res =0 stack =[] for ch in S: if ch =='(': stack.append('(') else: if stack: stack.pop() else: res +=1 return res +len(stack) Sum of Subarray Minimums 计算每一个数字的贡献值，当前数字是自己本身的最小值，然后是左边某个区间的最小值，是右边某个区间的最小值，这三部分相乘就是当前数字总的贡献值。具体参考讲解1 和讲解2 123456789101112131415161718192021222324252627282930313233343536373839404142434445class Solution &#123;public: int sumSubarrayMins(vector&lt;int&gt;&amp; A) &#123; int n =A.size(); stack&lt;int&gt; st; vector&lt;int&gt; l(n), r(n); long long ans =0; const int mod =1e9+7; for(int i =0; i&lt; n; i++) &#123; while (st.size() &amp;&amp; A[i] &lt; A[st.top()]) &#123; r[st.top()] =i; // r[] 中存储的是比其小的元素的index st.pop(); &#125; st.push(i); // 如果是”一样“的数字，那么不会加上 else 语句，stack 中每个都会给其一次机会 &#125; // 如果找不到，那么就置为 n while(st.size()) &#123; r[st.top()] =n; st.pop(); &#125; // 栈就保持了一种可以往回退的结构 for(int i =n -1; i&gt;=0 ; i--) &#123; while(st.size() &amp;&amp; A[i] &lt;= A[st.top()]) &#123; l[st.top()] =i; st.pop(); &#125; st.push(i); &#125; while(st.size()) &#123; l[st.top()] =-1; st.pop(); &#125; for(int i =0; i&lt;n; i++) &#123; ans =(ans + (long long)A[i] *(r[i] -i) *(i -l[i])) %mod; &#125; return ans; &#125;&#125;; python 中双向队列使用这种方式进行实现；如果是栈的话，直接使用 stack实现就行。 1stack = collections.deque([]) Validate Stack Sequences 123456789101112131415161718192021class Solution &#123;public: // 使用一个stack 模拟过程, push进去 pushed 元素，然后和popped 中的元素做对比 // 如果是可以 pop，那么就一直pop bool validateStackSequences(vector&lt;int&gt;&amp; pushed, vector&lt;int&gt;&amp; popped) &#123; int n =pushed.size(); int i =0, j =0; stack&lt;int&gt; st; while (i &lt; n &amp;&amp; j &lt; n) &#123; st.push(pushed[i]); while(st.size() &amp;&amp; st.top() == popped[j]) &#123; st.pop(); j ++; &#125; i ++; &#125; return j ==n; &#125;&#125;; Minimum Remove to Make Valid Parentheses stack更像是一种过滤机制， 最后的 v[i] 记录了默写去掉的字符 123456789101112131415161718192021222324252627282930class Solution &#123;public: // 使用 stack 做了一个标记 string minRemoveToMakeValid(string s) &#123; int n =s.size(); stack&lt;int&gt; st; vector&lt;bool&gt; v(n, true); for(int i =0; i&lt;n; i++) &#123; if(s[i] ==')') &#123; if ( !st.size() ) v[i] =false; else st.pop(); &#125; else if(s[i] == '(') st.push(i); // 存储的是index 下标，所以类型是int 而不是char &#125; while(st.size()) &#123; v[st.top()] =false; st.pop(); &#125; string ans ; for(int i =0; i&lt; n; i++) if(v[i]) ans += s[i]; return ans; &#125;&#125;; Remove All Adjacent Duplicates in String II 12345678910111213141516171819202122232425class Solution &#123;public: string removeDuplicates(string s, int k) &#123; string res("#"); stack&lt;int&gt; cnt; cnt.push(1); int n =s.length(); for(int i =0; i&lt;n ; i++ ) &#123; if(res.back() == s[i]) cnt.push(cnt.top() +1); // 这个是累加 push，后面也就是累加 pop的 else cnt.push(1); res += s[i]; if(cnt.top() &gt;= k) &#123; for(int j =0; j&lt; k ; j++) &#123; cnt.pop(); res.pop_back(); &#125; &#125; &#125; return stk.substr(1); &#125;&#125;; stack 数据结构 有点类似是来回访问, 但是能够保证只是访问一次，因为对于某个元素而言，只是经过一次的push 和pop class Solution: def reverseParentheses(self, s: str) -&gt; str: stack =[] for ch in s: if ch ==")": inner =[] while True: last =stack.pop() if last =="(": break else: inner.append(last) stack.extend(inner) else: stack.append(ch) return "".join(stack) 参考文献leetcode]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>stack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++ 编程语言（2）]]></title>
    <url>%2F2019%2F05%2F06%2Fcpp2%2F</url>
    <content type="text"><![CDATA[C++ 语言学习笔记2 字符串拼接问题 大量拼接字符串的效率从高到低依次为：+=、append()、stringstream、sprintf()。 常用的方法 operator += 操作符 append() 操作符 reserve &amp;&amp; operator += stringstream 的用法 针对较短字符串，使用reserve提前分配空间对性能提升意义不大，当字符串的长度很长是，使用reserve方法提前分配空间可以带来比较大的性能提升。operator+= 和 append 方法在进行字符串拼接时性能表现几乎一致。原因是stl 实现的operator+= 方式实际是直接调用了append 方法。综上，拼接长字符串时最优方式是 reserve &amp;&amp; append。 vector 中两个和内存相关的两个函数： resize()函数和容器的size息息相关。调用resize(n)后，容器的size即为n。reserve()函数和容器的capacity息息相关。reserve(n)预分配n个元素的存储空间。从两个函数的用途可以发现，容器调用resize()函数后，所有的空间都已经初始化了，所以可以直接访问。而reserve()函数预分配出的空间没有被初始化，所以不可访问。 123456789101112vector&lt;int&gt; a;a.reserve(100);a.resize(50);cout&lt;&lt;a.size()&lt;&lt;" "&lt;&lt;a.capacity()&lt;&lt;endl;a.resize(150);cout&lt;&lt;a.size()&lt;&lt;" "&lt;&lt;a.capacity()&lt;&lt;endl;a.reserve(50);cout&lt;&lt;a.size()&lt;&lt;" "&lt;&lt;a.capacity()&lt;&lt;endl;a.resize(50);cout&lt;&lt;a.size()&lt;&lt;" "&lt;&lt;a.capacity()&lt;&lt;endl;a.resize(210);cout&lt;&lt;a.size()&lt;&lt;" "&lt;&lt;a.capacity()&lt;&lt;endl; string中有两个函数：reserve()和resize()，和vector 中两个函数也是类似的。自动增长”的过程很耗时，并且会导致所有的指针、迭代器和引用失效。所以避免频繁的内存重新分配就显得很重要。 简单的实现 一直输入元素，如果是以”\n” 结尾，那么就跳出。 12345678910111213141516171819#include&lt;iostream&gt;#include&lt;vector&gt;#include&lt;stdio.h&gt;using namespace std;const int N =1000010;int main()&#123; int n; char ch; while(scanf("%d%c", &amp;n, &amp;ch)) &#123; cout&lt;&lt; n&lt;&lt; " "; if(ch =='\n') break; &#125; return 0; &#125; 类型转换C语言中有两种类型转换方式：(1). 隐性类型转换 (2). 显性类型转换。 隐式类型转化是编译器默默地、隐式地、偷偷地进行的类型转换，这种转换不需要程序员干预，会自动发生。赋值转换123float f = 100;int f = 100.1;int n = f; 运算类型转换 12 下面的类型都是可以自动转换类型。 强制类型转换 123456789#include &lt;stdio.h&gt;int main() &#123; int sum = 100; int count = 9; double average; average = (double)sum / count; printf("average is %lf\n", average); return 0;&#125; C++ 中的类型转换风格 1cast-name&lt;type&gt;(expression) 其中 type是转换的目标类型； expression 是被转换的值；cast-name有四种方式。 static_cast 静态类型转换 reinterpret_cast 重解析类型转换 dynamic_cast 动态类型转换 const_cast 去只读属性转换 1static_cast&lt;type&gt;(expression) 任何编写程序时能够明确的类型转换都可以使用static_cast（static_cast不能转换掉底层const，volatile和__unaligned属性）。由于不提供运行时的检查，所以叫static_cast，因此，需要在编写程序时确认转换的安全性。 dynamic_cast转换仅适用于指针或引用。相比static_cast，dynamic_cast会在运行时检查类型转换是否合法，具有一定的安全性。由于运行时的检查，所以会额外消耗一些性能。dynamic_cast使用场景与static相似，在类层次结构中使用时，上行转换和static_cast没有区别，都是安全的；下行转换时，dynamic_cast会检查转换的类型，相比static_cast更安全。其它三种都是编译时完成的,dynamic_cast是运行时处理的,运行要进程类型检查. const_cast用于移除类型的const、volatile和__unaligned属性。常量指针被转换成非常量指针，并且仍然指向原来的对象；常量引用被转换成非常量引用，并且仍然引用原来的对象。 12const char *pc;char *p = const_cast&lt;char*&gt;(pc); reinterpret_cast 1reinterpret_cast&lt;type&gt;(expression) 非常激进的指针类型转换，在编译期完成，可以转换任何类型的指针，所以极不安全。非极端情况不要使用。 12int *ip;char *pc = reinterpret_cast&lt;char*&gt;(ip);]]></content>
      <categories>
        <category>CS基础</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[新词发现]]></title>
    <url>%2F2019%2F05%2F06%2Fhmm_bak%2F</url>
    <content type="text"><![CDATA[介绍隐马尔科夫模型（HMM） 和新词发现。 HMM网络新词充分应用了英语、汉语、数字和符号等相互结合的方式，结构新颖自由。 基于规则 基于规则的方法是从语言学的角度对新词的构词规则进行归纳总结并建立正则表达式规则库。该方法对于高频新词有很好的识别效果，对特定的领域有很好的准确率，但是人工制定规则需要大量人力成本，存在规则领域适应能力下降等问题。 比如在微博中的新词发现的一种规则： 可以提取相应的规则： 来源 基于信息熵 使用词频,内部凝固程度,自由程度三个考察纬度进行新词筛选. 词频很好理解，词的出现频率一般比较高，需要设置一个阈值，如果高于这个阈值那么就判定为一个新词。 内部凝固度（互信息）用来衡量候选子串之间的结合程度。主要是用来提高对于低频新词的识别精度。当低频新词的子串出现频率也较低，子串之间的额结合程度紧密时，其互信息仍然是较高的，从而达到精确识别该类新词的效果。 自由程度利用信息熵来衡量候选新词的左邻字符和右临字符的不确定性，候选新词的邻接熵越大，说明邻接字符的不确定性越大，成为新词边界的可能性就越大。可以很好的解决新词边界的问题。 在人人网用户状态中，“的电影”出现了 389 次，“电影院”只出现了 175 次，然而我们却更倾向于把“电影院”当作一个词，因为直觉上看，“电影”和“院”凝固得更紧一些。经过计算“的电影” 和“电影院”的联合概率，计算结果表明，“电影院”更可能是一个有意义的搭配，而“的电影”则更像是“的”和“电影”这两个成分偶然拼到一起的。可以想到，凝合程度最高的文本片段就是诸如“蝙蝠”、“蜘蛛”、“彷徨”、“忐忑”、“玫瑰”之类的词了，这些词里的每一个字几乎总是会和另一个字同时出现，从不在其他场合中使用。 光看文本片段内部的凝合程度还不够，我们还需要从整体来看它在外部的表现。考虑“被子”和“辈子”这两个片段。如果一个文本片段能够算作一个词的话，它应该能够灵活地出现在各种不同的环境中，具有非常丰富的左邻字集合和右邻字集合。 在实际运用中你会发现，文本片段的凝固程度和自由程度，两种判断标准缺一不可。只看凝固程度的话，程序会找出“巧克”、“俄罗”、“颜六色”、“柴可夫”等实际上是“半个词”的片段；只看自由程度的话，程序则会把“吃了一顿”、“看了一遍”、“睡了一晚”、“去了一趟”中的“了一”提取出来，因为它的左右邻字都太丰富了。 我们把文本中出现过的所有长度不超过 d 的子串都当作潜在的词（即候选词，其中 d 为自己设定的候选词长度上限，我设定的值为 5 ），再为出现频数、凝固程度和自由程度各设定一个阈值，然后只需要提取出所有满足阈值要求的候选词即可。为了提高效率，我们可以把语料全文视作一整个字符串，并对该字符串的所有后缀按字典序排序。下表就是对“四是四十是十十四是十四四十是四十”的所有后缀进行排序后的结果。实际上我们只需要在内存中存储这些后缀的前 d + 1 个字，或者更好地，只储存它们在语料中的起始位置。 12345678910111213141516 十十十四是十四四十是四十十是十十四是十四四十是四十十是四十十四是十四四十是四十十四四十是四十是十十四是十四四十是四十是十四四十是四十是四十是四十是十十四是十四四十是四十四十四十是十十四是十四四十是四十四十是四十四是十四四十是四十四是四十是十十四是十四四十是四十四四十是四十 这样的话，相同的候选词便都集中在了一起，从头到尾扫描一遍便能算出各个候选词的频数和右邻字信息熵。将整个语料逆序后重新排列所有的后缀，再扫描一遍后便能统计出每个候选词的左邻字信息熵。另外，有了频数信息后，凝固程度也都很好计算了。这样，我们便得到了一个无需任何知识库的抽词算法，输入一段充分长的文本，这个算法能以大致 O(n · logn) 的效率提取出可能的词来。 更多详细的信息可以查看这篇博客 中”新词发现“部分讲解。 jieba分词会依照这个字典,将出现在词典中的词生成一个无向图DAG: 统计方法有很强的领域适应能力和可扩展性，但存在需要大规模语料库和数据稀疏等问题。 基于分词的热词挖掘 一个问题摆在了我们面前：我们如何去量化一个词的“当日热度”？第一想法当然是简单地看一看每个词的当日频数和昨日频数之间的倍数关系，不过细想一下你就发现问题了：它不能解决样本过少带来的偶然性。 忽略所有样本过少的词？这似乎也不太好，样本少的词也有可能真的是热词。 让计算机也能聪明地排除偶然因素，这是我们在数据挖掘过程中经常遇到的问题。我们经常需要对样本过少的项目进行“平滑”操作，以避免分母过小带来的奇点。这里，我采用的是一个非常容易理解的方法：一个词的样本太少，就给这个词的热度打折扣。 怎么做呢？我们把每个词的得分都和全局平均分取一个加权平均！首先计算出这四个词的平均总频数，为 313.75 ；再计算出这四个词的平均得分，为 0.719 。接下来，我们假设已经有 313.75 个人预先给每个词都打了 0.719 分，换句话说每个词都已经收到了 313.75 次评分，并且所有这 313.75 个评分都是 0.719 分。“下雪”这个词则还有额外的 125 个人评分，其中每个人都给了 0.736 分。因此，“下雪”一词的最终得分就是： 容易看出，此时样本越大的词，就越有能力把最终得分拉向自己本来的得分，样本太小的词，最终得分将会与全局平均分非常接近。经过这么一番调整，“下雪”一词的得分便高于了“李宇春”。实际运用中， 313.75 这个数也可以由你自己来定，定得越高就表明你越在意样本过少带来的负面影响。这种与全局平均取加权平均的思想叫做 Bayesian average ，从上面的若干式子里很容易看出，它实际上是最常见的平滑处理方法之一——分子分母都加上一个常数——的一种特殊形式。 利用之前的抽词程序抽取出人人网每一天内用户状态所含的词，把它们的频数都与前一天的作对比，再利用刚才的方法加以平滑，便能得出每一天的热词了。 注意，由于我们仅仅对比了相邻两天的状态，因而产生了个别实际上是由工作日/休息日的区别造成的“热词”，比如“教室”、“老师”、“星期二”等。把这样的词当作热词可能并不太妥。结合上周同日的数据，或者干脆直接与之前整个一周的数据来对比，或许可以部分地解决这一问题。 事实上，有了上述工具，我们可以任意比较两段不同文本中的用词特点。更有趣的是，人人网状态的大多数发布者都填写了性别和年龄的个人信息，我们为何不把状态重新分成男性和女性两组，或者 80 后和 90 后两组，挖掘出不同属性的人都爱说什么？要知道，在过去，这样的问题需要进行大规模语言统计调查才能回答！然而，在互联网海量用户生成内容的支持下，我们可以轻而易举地挖掘出答案来。 不仅如此，不少状态还带有地理位置信息，因而我们可以站在空间的维度对信息进行观察。这个地方的人都爱说些什么？爱说这个词的人都分布在哪里？借助这些包含地理位置的签到信息，我们也能挖掘出很多有意思的结果来。例如，对北京用户的签到信息进行抽词，然后对于每一个抽出来的词，筛选出所有包含该词的签到信息并按地理坐标的位置聚类，这样我们便能找出那些地理分布最集中的词。结果非常有趣：“考试”一词集中分布在海淀众高校区，“天津”一词集中出现在北京南站，“逛街”一词则全都在西单附近扎堆。北京首都国际机场也是一个非常特别的地点，“北京”、“登机”、“终于”、“再见”等词在这里出现的密度极高。 从全国范围来看，不同区域的人也有明显的用词区别。我们可以将全国地图划分成网格，统计出所有签到信息在各个小格内出现的频数，作为标准分布；然后对于每一个抽出来的词，统计出包含该词的签到信息在各个小格内出现的频数，并与标准分布进行对比（可以采用余弦距离等公式），从而找出那些分布最反常的词。程序运行后发现，这样的词还真不少。一些明显具有南北差异的词，分布就会与整个背景相差甚远。例如，在节假日的时候，“滑雪”一词主要在北方出现，“登山”一词则主要在南方出现。地方特色也是造成词语分布差异的一大原因，例如“三里屯”一词几乎只在北京出现，“热干面”一词集中出现在武汉地区，“地铁”一词明显只有个别城市有所涉及。这种由当地人的用词特征反映出来的真实的地方特色，很可能是许多旅游爱好者梦寐以求的信息。另外，方言也会导致用词分布差异，例如“咋这么”主要分布在北方地区，“搞不懂”主要分布在南方城市，“伐”则非常集中地出现在上海地区。当数据规模足够大时，或许我们能通过计算的方法，自动对中国的方言区进行划分。 其实，不仅仅是发布时间、用户年龄、用户性别、地理位置这四个维度，我们还可以对浏览器、用户职业、用户活跃度、用户行为偏好等各种各样的维度进行分析，甚至可以综合考虑以上维度，在某个特定范围内挖掘热点事件，或者根据语言习惯去寻找出某个特定的人群。或许这听上去太过理想化，不过我坚信，有了合适的算法，这些想法终究会被一一实现。 上面的称述只是抛转引玉，详细的基于SNS的文本数据挖掘 基于HMM 对于未登录词,词典中没有,所以使用上述方法会被分为单字词,还是上面的例子: 结婚／的／和／尚未／结婚／的 对于jieba分词来说,这里的连续单字词”的和”就很有可能是一个新词,会被送入隐马尔科夫模型来做进一步的识别. 实际上当我们把dict.txt中的词语全部删除，jieba依然能够进行分词，其实这个时候使用的就是HMM来进行分词了。 自然语言处理中的序列标注问题, 在目前, 比较主流的技术是语言模型(如LSTM, BERT)+CRF(条件随机场)。为什么单单使用语言模型是不可以的，后面需要加上概率图模型？比如说”自贸区”对应的标注是: 自(B-LOC)贸(I-LOC)区(I-LOC), 这三个字都对应一个”地名”的标签, 但是第一个字属于实体开头的字, 所以使用”B”开头的标签, 后面两个字的标签都是”I”开头. 比如说搭建好模型之后，使用交叉熵来训练模型，很有可能得到argmax 是上面的组合；上面的原因就是我们要从语言模型(例如BERT, LSTM)后面再加上概率图模型, 例如条件随机场, 用来约束模型的输出, 防止出现不合规的标注输出. 如果使用条件随机场，那么条件随机场的损失是可以反传到模型中去，帮助模型做更好的建立序列之间的依赖关系。 什么是HMM？ HMM模型是概率图模型的一种, 属于生成模型, 笼统的说, 我们上面说的”BIO”的实体标签, 就是一个不可观测的隐状态, 而HMM模型描述的就是由这些隐状态序列(实体标记)生成可观测状态(可读文本)的过程. HMM模型有两个基本假设(非常重要): 第$t$个隐状态(实体标签)只跟前一时刻的$t-1$隐状态(实体标签)有关, 与除此之外的其他隐状态(如$t-2,\ t+3$)无关.例如上图中: 蓝色的部分指的是$i_t$只与$i_{t-1}$有关, 而与蓝色区域之外的所有内容都无关, 而$P(i_{t}|i_{t-1})$指的是隐状态$i$从$t-1$时刻转向$t$时刻的概率, 具体转换方式下面会细讲. 观测独立的假设, 我们上面说过, HMM模型中是由隐状态序列(实体标记)生成可观测状态(可读文本)的过程,观测独立假设是指在任意时刻观测$o_t$只依赖于当前时刻的隐状态$i_t$, 与其他时刻的隐状态无关.例如上图中: 粉红色的部分指的是$i_{t+1}$只与$o_{t+1}$有关, 跟粉红色区域之外的所有内容都无关. 我们现在已经了解了HMM的三大参数$A, \ B, \ \pi$, 假设我们已经通过建模学习, 学到了这些参数, 得到了模型的概率, 我们怎么使用这些参数来解决序列标注问题呢?设目前在时刻$t$, 我们有当前时刻的观测到的一个汉字$o_t=v_k$(指的第$t$时刻观测到$v_k$), 假设我们还知道在$t-1$时刻(前一时刻)对应的实体标记类型$i_{t-1} = \hat{q}^{t-1}_i$(指的$t-1$时刻标记为$\hat{q}^{t-1}i$). 我们要做的仅仅是列举所有$i{t}$可能的实体标记$\hat{q}^{t}{j}$, 并求可以使下式输出值最大的那个实体类型$q^{t}{j}$(也就是隐状态类型):$$\hat{q}j^{t} = argmax{\hat{q}j^{t} \in Q{hidden}}P(i_t = \hat{q}j^{t} | i{t-1} = \hat{q}^{t-1}_i) P(o_t=v_k| i_t = \hat{q}_j^{t})$$将所有$t$时刻当前可取的实体标签带入下式中, 找出一个可以使下式取值最大的那个实体标签作为当前字的标注:$$P(当前可取实体标签|上一时刻实体标签)P(测到的汉字|当前可取实体标签)$$注意: 我们这里只讲到了怎样求第$t$时刻的最优标注, 但是在每一时刻进行这样的计算, 并不一定能保证最后能得出全局最优序列路径, 例如在第$t$时刻最优实体标签是$q_j$, 但到了下一步, 由于从$q_j$转移到其他某些实体标签的转移概率比较低, 而降低了经过$q_j$的路径的整体概率, 所以到了下一时刻最优路径就有可能在第$t$时刻不经过$q_j$了, 所以每一步的局部最优并不一定可以达成全局最优, 所以我们之后会用到维特比算法来找到全局最优的标注序列, 这个后面会有详细讲解. HMM参数学习(监督学习): 我们今天要用HMM解决的是序列标注问题, 所以我们解决的是监督学习的问题. 也就是说我们现在有一些文本和与之对应的标注数据, 我们要训练一个HMM来拟合这些数据, 以便之后用这个模型进行数据标注任务, 最简单的方式是直接用极大似然估计来估计参数: 初始隐状态概率$\pi$的参数估计:$$\hat{\pi}_{q_i}=\frac{count(q^{1}_{i})}{count(o_1)}$$上式指的是, 计算在第$1$时刻, 也就是文本中第一个字, $q^{1}_{i}$出现的次数占总第一个字$o_1$观测次数的比例, $q^{1}_{i}$上标1指的是第1时刻, 下标$i$指的是第$i$种标签(隐状态), $count$是的是记录次数. 转移概率矩阵$A$的参数估计:我们之前提到过$transition \ matrix$里面$A_{ij}$(矩阵的第i行第j列)指的是在$t$时刻实体标签为$q_i$, 而在$t+1$时刻实体标签转换到$q_j$的概率, 则转移概率矩阵的参数估计相当与一个二元模型$bigram$, 也就是把所有的标注序列中每相邻的两个实体标签分成一组, 统计他们出现的概率:$$\hat{A}{ij}=P(i{t+1}= q_j | i_{t} = q_i)=\frac{count(q_i后面出现q_j的次数)}{count(q_i的次数)}$$ 联合概率 除以边缘概率 发射概率矩阵$B$的参数估计:我们提到过$emission \ matrix$中的$B_{jk}$(矩阵第j行第k列)指的是在$t$时刻由实体标签(隐状态)$q_j$生成汉字(观测结果)$v_k$的概率.$$\hat{B}{jk}=P(o{t}= v_k | i_{t} = q_j)=\frac{count(q_j与v_k同时出现的次数)}{count(q_j出现的次数)}$$到此为止, 我们就可以遍历所有语料, 根据上面的方式得到模型的参数$A, \ B, \ \pi$的估计. 视频讲解 基于隐马尔科夫模型的新词发现 互联网中经常会出现很多网红词,比如最近的”佛系青年”,“食草男”.这些词并不在词典中,被称为未登录词,前面写过一篇文章信息熵在新词发现中的运用利用简单的概率知识就可以识别这种新词,不过jieba分词并没有采用这种方法,而是用更为强大的隐马尔科夫模型进行新词发现. 对于未登录词,词典中没有,所以使用上述方法会被分为单字词,还是上面的例子: 结婚／的／和／尚未／结婚／的 对于jieba分词来说,这里的连续单字词”的和”就很有可能是一个新词,会被送入隐马尔科夫模型来做进一步的识别. 实际上当我们把dict.txt中的词语全部删除，jieba依然能够进行分词，其实这个时候使用的就是HMM来进行分词了。 马尔科夫模型用来分词时,使用BMES,作为每个字的状态,B代表词头,M代表词中,E代表词尾词,S代表单个字的词.根据预先训练好的起始状态,转移概率和观测概率,就可以估计出每个单词状态: 新词发现和分词同样的道理,估计每个字的隐状态就可以判断是否为一个词. 挖掘新词的传统方法是，先对文本进行分词，然后猜测未能成功匹配的剩余片段就是新词。这似乎陷入了一个怪圈：分词的准确性本身就依赖于词库的完整性，如果词库中根本没有新词，我们又怎么能信任分词结果呢？此时，一种大胆的想法是，首先不依赖于任何已有的词库，仅仅根据词的共同特征，将一段大规模语料中可能成词的文本片段全部提取出来，不管它是新词还是旧词。然后，再把所有抽出来的词和已有词库进行比较，不就能找出新词了吗？ 互联网时代的社会语言学：基于SNS的文本数据挖掘 hmm 的过程是由隐状态生成可观测状态的过程。比如是由标签生成文本。 hmm 的假设 两大假设 HMM模型有两个基本假设(非常重要): 第$t$个隐状态(实体标签)只跟前一时刻的$t-1$隐状态(实体标签)有关, 与除此之外的其他隐状态(如$t-2,\ t+3$)无关.例如上图中: 蓝色的部分指的是$i_t$只与$i_{t-1}$有关, 而与蓝色区域之外的所有内容都无关, 而$P(i_{t}|i_{t-1})$指的是隐状态$i$从$t-1$时刻转向$t$时刻的概率, 具体转换方式下面会细讲. 观测独立的假设, 我们上面说过, HMM模型中是由隐状态序列(实体标记)生成可观测状态(可读文本)的过程,观测独立假设是指在任意时刻观测$o_t$只依赖于当前时刻的隐状态$i_t$, 与其他时刻的隐状态无关.例如上图中: 粉红色的部分指的是$i_{t+1}$只与$o_{t+1}$有关, 跟粉红色区域之外的所有内容都无关. hmm 的参数 三大参数 转移概率 是 $n * n$ 的矩阵， $n$ 表示标签的数量。 发射概率 是 $n * m$ 的概率，其中 $n$ 表示标签的数量， $m$ 表示字典的大小，共有$m $ 中可能性。 HMM的初始隐状态概率: 又称为$initial \ probabilities$, 我们通常用$\pi$来表示, 注意这里可不是圆周率:$$\pi=P(i_1=q_i) \quad q_i \in Q_{hidden} = { q_0, q_1, … , q_{N-1}}$$上式指的是自然语言序列中第一个字$o_1$的实体标记是$q_i$的概率, 也就是初始隐状态概率. 有监督的学习，使用hmm 来拟合这些数据，然后使用这个模型进行数据标注任务，最简单的方式是直接用极大似然估计来估计这些参数。 使用 argmax 得到的每一步的局部最优解不一定是 lead to 全局最优解。 训练很快，2 -3s，因为做的是一个概率统计，极大似然，很简单的。 维特比算法是用来求解全局最优的标注序列。 实际的运算，都是求解log，从乘法转成加法，防止因为概率过小而造成下溢。 维特比算法使用了动态规划算法来解决类似HMM 和CRF 的预测问题。使用维特比算法可以找到概率最大路径，也就是最优路径。 一个表格记录最大的概率， 另一个表格记录最大概率是从哪个隐状态转移过来的。 最优路径的特点：最优路径有以下特性: 假设我们有一条最优路径在$t$时刻通过一个隐状态$i_t$, 那么这一路径从$i_t$到最优路径的终点$i_T$相对于在这段距离里所有可能出现的路径里, 也必须是最优的. 否则从$i_t$到$i_T$就会有更优的一条路径, 如果把他和从$i_1$到$i_t$的路径(最优路径$i_t$之前的部分)连起来, 等于我们又有一条更优路径, 这是矛盾的. 时间复杂度分析：假设我们有 $N$ 中隐状态，在每个时刻之间，一共可能的路径有 $N^2$ 中，假设我们有$T$ 个时刻，那么维特比算法的时间复杂度是 $O(TN^2)$ ner HMM 可以使用在 有监督的学习中（知道了隐状态 和观测状态， 未知变量是模型的参数），和无监督学习中（只是知道观测状态，需要求解 模型的参数和隐状态，这个时候需要使用类似 EM 算法的思想） 命名实体识别（Name Entity Recognition,NER） 命名实体识别（Name Entity Recognition,NER）,也称作“专名识别”，是指识别文本中具有特定意义的实体，包括人名、地名、机构名、专有名词等。 NER 的评价指标， acc precision, recall 和F1值 命名实体识别是将文本中的元素分成预先定义的类，如人名、地名、 机构名、时间、货币等等。作为自然语言的承载信息单位，命名实体识别 属于文本信息处理的基础的研究领域，是信息抽取、信息检索、机器翻译、 问答系统等多种自然语言处理技术中必不可少的组成部分。 命名实体识别主要分类，一般包括 3 大类（实体类、时间类和数字类）和 7 小类（人名、地名、组织名、机构名、时间、日期、货币和百分比）。但随着 NLP 任务的不断扩充，在特定领域中会出现特定的类别，比如医药领域中，药名、疾病等类别。 主流的方法： 这个是之前一个人回答的，大概谈了NER 中模型发展的历史：有一些，大致如下：MLP-&gt;LSTM-&gt;LSTM/CNN+CRF-&gt;BiLSTM+CRF- &gt;BiLSTM+CNN+CRF。提到技术，是不是大家都是用的 CRF，除了目前最新的深度学习。（CRF+ 深度学习） 词性需要有一定的规范，如将词分为名词、形容词、动词，然后用’n’ ‘adj’ ‘v’来表示。北大词性标注集部分标注词性如下表所示： 123456789101112131415161718192021222324252627282930313233343536373839Ag 形语素 形容词性语素。形容词代码为a，语素代码ｇ前面置以A。 a 形容词 取英语形容词adjective的第1个字母。 ad 副形词 直接作状语的形容词。形容词代码a和副词代码d并在一起。 an 名形词 具有名词功能的形容词。形容词代码a和名词代码n并在一起。 b 区别词 取汉字“别”的声母。 c 连词 取英语连词conjunction的第1个字母。 Dg 副语素 副词性语素。副词代码为d，语素代码ｇ前面置以D。 d 副词 取adverb的第2个字母，因其第1个字母已用于形容词。 e 叹词 取英语叹词exclamation的第1个字母。 f 方位词 取汉字“方” 的声母。 g 语素 绝大多数语素都能作为合成词的“词根”，取汉字“根”的声母。 h 前接成分 取英语head的第1个字母。 i 成语 取英语成语idiom的第1个字母。 j 简称略语 取汉字“简”的声母。 k 后接成分 l 习用语 习用语尚未成为成语，有点“临时性”，取“临”的声母。 m 数词 取英语numeral的第3个字母，n，u已有他用。 Ng 名语素 名词性语素。名词代码为n，语素代码ｇ前面置以N。 n 名词 取英语名词noun的第1个字母。 nr 人名 名词代码n和“人(ren)”的声母并在一起。 ns 地名 名词代码n和处所词代码s并在一起。 nt 机构团体 “团”的声母为t，名词代码n和t并在一起。 nz 其他专名 “专”的声母的第1个字母为z，名词代码n和z并在一起。 o 拟声词 取英语拟声词onomatopoeia的第1个字母。 p 介词 取英语介词prepositional的第1个字母。 q 量词 取英语quantity的第1个字母。 r 代词 取英语代词pronoun的第2个字母,因p已用于介词。 s 处所词 取英语space的第1个字母。 Tg 时语素 时间词性语素。时间词代码为t,在语素的代码g前面置以T。 t 时间词 取英语time的第1个字母。 u 助词 取英语助词auxiliary 的第2个字母,因a已用于形容词。 Vg 动语素 动词性语素。动词代码为v。在语素的代码g前面置以V。 v 动词 取英语动词verb的第一个字母。 vd 副动词 直接作状语的动词。动词和副词的代码并在一起。 vn 名动词 指具有名词功能的动词。动词和名词的代码并在一起。 w 标点符号 x 非语素字 非语素字只是一个符号，字母x通常用于代表未知数、符号。 y 语气词 取汉字“语”的声母。 z 状态词 取汉字“状”的声母的前一个字母。 NER 的应用场景： 新闻标注：和文本分类不同, 这里可以使用NER技术将与文章相关的人物, 地点都以标签的形式标注出来, 方便用户对某个人物或地点进行索引。 搜索引擎：可以通过使用命名实体识别来抽取web页面中的实体, 后续可以使用这些信息来提高搜索效率和准确度。 从商品描述中自动提取商品类别, 品牌等信息, 提高货物上架效率, 在咸鱼等应用上已经实现了类似功能。 工具易用性提升, 例如从短信息或邮件中提取时间和地点等实体, 从而实现点击时间直接创建日历, 点击地址直接跳转到地图App等便捷操作。 一般来说 NER 是不使用在文本分类领域的。 视频讲解 HMM与CRF隐形马尔可夫链与条件随机场-attention is all you need CRF (conditional random field )定义和intuitionCRF 利用了label (在HMM 中的hidden layer) 的信息，因为上一个标签是有助于下一个标签的预测。比如上一个是动词，那么下一个词语的label 也是动词的概率是非常小的。 we should incorporate the labels of nearby photos, and this is precisely what a conditional random field does. 特征函数 特征函数的输入： a sentence s the position $i $of a word in the sentence the label $l_i$ of the current word the label $l_i−1$ of the previous word 这里约束的是 linear-chain CRF (Note: by restricting our features to depend on only the current and previous labels, rather than arbitrary labels throughout the sentence, I’m actually building the special case of a linear-chain CRF. For simplicity, I’m going to ignore general CRFs in this post.) 特征的概率 计算某个 feature function的条件下得到的score：$$score (l | s)=\sum_{j=1}^{m} \sum_{i=1}^{n} \lambda_{j} f_{j}\left(s, i, l_{i}, l_{i-1}\right)$$ 正则化到区间 $[0, 1]$$$p(l | s)=\frac{\exp [ {scorells})]}{\sum_{l^{\prime}} \exp \left[\operatorname{score}\left(l^{\prime} | s\right)\right]}=\frac{\exp \left[\sum_{j=1}^{m} \sum_{i=1}^{n} \lambda_{i} f_{j}\left(s, i, l_{i-1}\right)\right]}{\sum_{l^{\prime}} \exp \left[\sum_{j=1}^{m} \sum_{i=1}^{n} \lambda_{i} f_{j}\left(s, i_{l}^{\prime} l_{i-1}^{\prime}\right)\right]}$$ feature functions例子（这个就类似人工的提取特征） $f1(s,i,l_i,l_{i−1})=1 $if$ l_i= ADVERB $and the ith word ends in “-ly”; 0 otherwise. ** If the weight λ1 associated with this feature is large and positive, then this feature is essentially saying that we prefer labelings where words ending in -ly get labeled as ADVERB.如果以 -ly 结尾，并且权重比较大，那么就是 adj 如果句子的结尾是 ?，那么开头的单词就可能是一个动词 和 HMM 的比较 CRF 更加强大，任何一个 HMM 都是可以看做是某个 CRF。这个是因为： CRFs can define a much larger set of features. CRFs can have arbitrary weights. learning weights 当然是使用 gradient descent 的思想。 Finding the optimal labeling 和HMM 一样， 使用 Viterbi algorithm。 理论讲解Introduction to Conditional Random Fields To identify entities in text, one must be able to identify the pattern. For example, if we need to identify the claim number, we can look at the words around it such as “my id is” or “my number is”, etc. Let us examine a few approaches mentioned below for identifying the patterns. Regular expressions: Regular expressions (RegEx) are a form of finite state automaton. They are very helpful in identifying patterns that follow a certain structure. For example, email ID, phone number, etc. can be identified well using RegEx. However, the downside of this approach is that one needs to be aware of all the possible exact words that occur before the claim number. This is not a learning approach, but rather a brute force one Hidden Markov Model (HMM): This is a sequence modelling algorithm that identifies and learns the pattern. Although HMM considers the future observations around the entities for learning a pattern, it assumes that the features are independent of each other. This approach is better than regular expressions as we do not need to model the exact set of word(s). But in terms of performance, it is not known to be the best method for entity recognition MaxEnt Markov Model (MEMM): This is also a sequence modelling algorithm. This does not assume that features are independent of each other and also does not consider future observations for learning the pattern. In terms of performance, it is not known to be the best method for identifying entity relationships either Conditional Random Fields (CRF): This is also a sequence modelling algorithm. This not only assumes that features are dependent on each other, but also considers the future observations while learning a pattern. This combines the best of both HMM and MEMM. In terms of performance, it is considered to be the best method for entity recognition problem The bag of words (BoW) approach works well for multiple text classification problems. This approach assumes that presence or absence of word(s) matter more than the sequence of the words. However, there are problems such as entity recognition, part of speech identification where word sequences matter as much, if not more. Conditional Random Fields (CRF) comes to the rescue here as it uses word sequences as opposed to just words. Broadly speaking, there are 2 components to the CRF formula: Normalization: You may have observed that there are no probabilities on the right side of the equation where we have the weights and features. However, the output is expected to be a probability and hence there is a need for normalization. The normalization constant Z(x) is a sum of all possible state sequences such that the total becomes 1. You can find more details in the reference section of this article to understand how we arrived at this value. Weights and Features: This component can be thought of as the logistic regression formula with weights and the corresponding features. The weight estimation is performed by maximum likelihood estimation and the features are defined by us. Complete tutorial on Text Classification using Conditional Random Fields Model (in Python) 发展的脉络 隐马尔可夫模型（Hidden Markov Model，HMM） NER本质上可以看成是一种序列标注问题（预测每个字的BIOES标记），在使用HMM解决NER这种序列标注问题的时候，我们所能观测到的是字组成的序列（观测序列），观测不到的是每个字对应的标注（状态序列）。 解码问题，我们使用的是维特比（viterbi）算法。 条件随机场（Conditional Random Field, CRF) 上面讲的HMM模型中存在两个假设，一是输出观察值之间严格独立，二是状态转移过程中当前状态只与前一状态有关。也就是说，在命名实体识别的场景下，HMM认为观测到的句子中的每个字都是相互独立的，而且当前时刻的标注只与前一时刻的标注相关。但实际上，命名实体识别往往需要更多的特征，比如词性，词的上下文等等，同时当前时刻的标注应该与前一时刻以及后一时刻的标注都相关联。由于这两个假设的存在，显然HMM模型在解决命名实体识别的问题上是存在缺陷的。 而条件随机场就没有这种问题，它通过引入自定义的特征函数，不仅可以表达观测之间的依赖，还可表示当前观测与前后多个状态之间的复杂依赖，可以有效克服HMM模型面临的问题。 解码的时候与HMM类似，也可以采用维特比算法。 Bi-LSTM 除了以上两种基于概率图模型的方法，LSTM也常常被用来解决序列标注问题。和HMM、CRF不同的是，LSTM是依靠神经网络超强的非线性拟合能力，在训练时将样本通过高维空间中的复杂非线性变换，学习到从样本到标注的函数，之后使用这个函数为指定的样本预测每个token的标注。 LSTM比起CRF模型最大的好处就是简单粗暴，不需要做繁杂的特征工程，直接训练即可，同时比起HMM，LSTM的准确率也比较高。 Bi-LSTM+CRF 简单的LSTM的优点是能够通过双向的设置学习到观测序列（输入的字）之间的依赖，在训练过程中，LSTM能够根据目标（比如识别实体）自动提取观测序列的特征，但是缺点是无法学习到状态序列（输出的标注）之间的关系，要知道，在命名实体识别任务中，标注之间是有一定的关系的，比如B类标注（表示某实体的开头）后面不会再接一个B类标注，所以LSTM在解决NER这类序列标注任务时，虽然可以省去很繁杂的特征工程，但是也存在无法学习到标注上下文的缺点。 相反，CRF的优点就是能对隐含状态建模，学习状态序列的特点，但它的缺点是需要手动提取序列特征。所以一般的做法是，在LSTM后面再加一层CRF，以获得两者的优点。 NLP实战-中文命名实体识别]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>crf</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Word2vec 介绍]]></title>
    <url>%2F2019%2F05%2F04%2Fword2vec%2F</url>
    <content type="text"><![CDATA[介绍Word2vec 的两种训练模式（DBOW和Skip-gram）和两种优化策略（Hierarchical Softmax 和Negative Sampling）。 两种训练模式（1）Context-Based: Continuous Bag-of-Words (CBOW) For example, the target word “swing” in the above case produces four training samples: (“swing”, “sentence”), (“swing”, “should”), (“swing”, “the”), and (“swing”, “sword”). （2）Skip-Gram Model The Continuous Bag-of-Words (CBOW) is another similar model for learning word vectors. It predicts the target word (i.e. “swing”) from source context words (i.e., “sentence should the sword”). Because there are multiple contextual words, we average their corresponding word vectors, constructed by the multiplication of the input vector and the matrix W. （3）损失函数 skip-gram 和CBOW 都是采用的交叉熵损失函数。 \begin{equation}L_{\theta}=-\sum_{i=1}^{V} y_{i} \log p\left(w_{i} | w_{I}\right)=-\log p\left(w_{O} | w_{I}\right)\end{equation} 其中 $y_i$表示真实的数据的label， $p$是网络输出的概率值，具体表示为下式子：\begin{equation}p\left(w_{O} | w_{I}\right)=\frac{\exp \left(v_{w_{o}}^{\prime} \top_{v_{w_{i}}}\right)}{\sum_{i=1}^{V} \exp \left(v_{w_{i}}^{\prime} \tau_{v_{w_{i}}}\right)}\end{equation} 其中$p\left(w_{O} | w_{I}\right)$ 表示给定了 $w_l$ 的条件下 $w_O$的概率。 （4）超参数windows size One heuristic is that smaller window sizes (2-15) lead to embeddings where high similarity scores between two embeddings indicates that the words are interchangeable (notice that antonyms are often interchangable if we’re only looking at their surrounding words – e.g. good and bad often appear in similar contexts). Larger window sizes (15-50, or even more) lead to embeddings where similarity is more indicative of relatedness of the words.经验结论：小的window size得到的词向量是 interchangeable，比如说bad 和good 词向量相近；大的window size得到的词向量更加具有解释性。当然时间成本也更加大。 优化策略（1）Hierarchical Softmax 层次softmax （Hierarchical softmax）是在最后一层softmax中计算上的优化，从原来的$O(V)$ 优化成$O(log_2V)$，其中 $V$表示字典的大小。 当词典 V = {this, battle, will, be, my, masterpiece, the, unseen, blade, is, deadliest}, ||V|| = 11，那么最后的softmax层可以表示为以下的结构： 每个叶子节点表示一个单词。那么 $p(unseen) = p(left) * p(right) * p(right) * p(right)$。其中 $sigmoid(x * w +b) $ 得到了相应概率。 并且由于左右子树的概率相加之和为1，所以当已知左子树的概率时候，右子树的概率是不用重新计算的，即 $p(left) + p(right) =1$。 （2）Negative Sampling (NEG) 1). Simple Sampling该采样方式是根据数据随机采样，那么出现频率高的数据被采样中的次数多，而频率少的那么被计算的机会少。 2). Adjusted Sampling \begin{equation}p(w_i) =\frac{fre(w_i)^c }{ \sum_{j}^{V} fre(w_j) ^c}\end{equation} 其中 $c =\frac{3}{4}$ 是实验中的经验值，一般使用该值效果比较好。 参考文献： (1). Learning Word Embedding(2). The Illustrated Word2vec]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>word2vec</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从RNN到LSTM]]></title>
    <url>%2F2019%2F05%2F04%2Flstm%2F</url>
    <content type="text"><![CDATA[从 RNN到LSTM，重点介绍LSTM 的网络结构和LSTM是如何缓解RNN 中出现的梯度消失。 循环神经网络(RNN)一个RNN可以看作是同一个网络的多份副本，每一份都将信息传递到下一个副本。如果我们将环展开的话： 这种链式结构展示了RNN与序列和列表的密切关系。RNN的这种结构能够非常自然地使用这类数据。 RNN 的主要应用如下： 文本相关。主要应用在自然语言处理方面（NLP）、对话系统、情感分析、机器翻译 时序相关。就是在做时序预测问题，比如预测天气、温度，包括有很多人使用其在做预测股票价格的问题 长期依赖(Long Term Dependencies)的问题对于RNN 来说，可以处理非常短的文本序列（比如下文第一种情况）但是不可以处理比较长的序列（比如下文第二种情况） 有时候，我们只需要看最近的信息，就可以完成当前的任务。比如，考虑一个语言模型，通过前面的单词来预测接下来的单词。如果我们想预测句子“the clouds are in the sky”中的最后一个单词，我们不需要更多的上下文信息——很明显下一个单词应该是sky。 RNN 是可以被用来进行这样问题的训练学习。 然而，有时候我们需要更多的上下文信息。比如，我们想预测句子“I grew up in France… I speak fluent French”中的最后一个单词。不幸的是，随着距离的增大，RNN对于如何将这样的信息连接起来无能为力。 LSTM 中的基本概念LSTM 是用来解决RNN 中的梯度消失/ 梯度爆炸问题的，可以处理 long-term sequence了。 门（gate ）定义： gate 实际上就是一层全连接层，输入是一个向量，输出是一个 0到1 之间的实数向量。公式如下：$$g ( \mathbf { x } ) = \sigma ( W \mathbf { x } + \mathbf { b } )$$ 遗忘门（forget gate）它决定了上一时刻的单元状态 $c_{t-1} $有多少保留到当前时刻$ c_t$输入门（input gate）它决定了当前时刻网络的输入 $x_t$ 有多少保存到单元状态 $c_t$输出门（output gate）控制单元状态$ c_t $有多少输出到 LSTM 的当前输出值 $h_t$ LSTM网络在普通的RNN中，重复模块结构非常简单，例如只有一个tanh层。 LSTM也有这种链状结构，不过其重复模块的结构不同。LSTM的重复模块中有4个神经网络层，并且他们之间的交互非常特别。 LSTM分步详解LSTM的第一步是决定我们将要从元胞状态中扔掉哪些信息。遗忘门观察$h_{t−1}$和 $x_t$，对于元胞状态 $C_{t−1} $中的每一个元素，输出一个0-1之间的数。1表示“完全保留该信息”，0表示“完全丢弃该信息”。 下一步是决定我们将会把哪些新信息存储到元胞状态中。这步分为两部分。首先，有一个叫做“输入门(Input Gate)”的Sigmoid层决定我们要更新哪些信息。接下来，一个tanh层创造了一个新的候选值，$\tilde { C } _ { t }$，该值可能被加入到元胞状态中。在下一步中，我们将会把这两个值组合起来用于更新元胞状态。 现在我们该更新旧元胞状态 $C_{t−1} $到新状态 $C_t$了。上面的步骤中已经决定了该怎么做，这一步我们只需要实际执行即可。 最后，我们需要决定最终的输出。输出将会基于目前的元胞状态，并且会加入一些过滤。首先我们建立一个Sigmoid层的输出门(Output Gate)，来决定我们将输出元胞的哪些部分。然后我们将元胞状态通过tanh之后（使得输出值在-1到1之间），与输出门相乘，这样我们只会输出我们想输出的部分。 优点： 解决了RNN 中的梯度消失的问题，可以处理 长依赖 缺点： 计算复杂度高，运行时间长 LSTM中参数的计算 首先参数的个数和 时间steps 无关 $h_t$ 和 $c_t$ 的维度是相同的 总共四组 [w, b] 参数 直接给出公式$$ 4(n(m +n) +n)$$其中 $m$ 表示输入 $x$ 的维度， $n$ 表示 hidden 或者说 context 的维度。 $(m+n)$ 表示在处理下一层的 输入时候，把当前层数据 $x$ 的维度 $m$ 和 hidden 中维度 $n$ 给链接起来，具体可以看一下 lstm 中的示意图。 LSTM参数计算的例子 GRUGRU (gated recurrent unit) 是对于 LSTM 速度上的提升，但是相应的表达能力也受到了限制 GRU 中一共有两个门。GRU 把LSTM 中遗忘门(forget gate) 和输入门(input gate) 使用 更新门(update gate) 进行代替。还有一个重置门(reset gate)， 重置门主要决定了多少过去的信息需要遗忘。GRU 不会保存内部记忆 context，而且没有输出门。 LSTM缓解梯度消失（1） RNN 中为什么会出现梯度消失 RNN 中的传播公式 \begin{equation}\begin{array}{l}{s_{t}=\phi\left(U x_{t}+W s_{t-1}\right) } \\ {o_{t}=f\left(V s_{t}\right) } \end{array}\end{equation} 其中 $s_t$表示隐藏层的状态值，$W$ 表示$s$的权重矩阵， $U$ 表示 $x$的权重矩阵。 第一个公式是隐藏层的计算公式，第二层是输出层的计算。 假设时间序列 $t =3$，那么可得到：$t=1$的时候的状态和输出， \begin{equation}\begin{array}{l}{s_{1}=\phi\left(U x_{1}+W s_{0}\right) } \\(o_{1}=f\left(V \phi\left(V x_{1}+W s_{0}\right)\right.) \end{array}\end{equation}当 $t =2$ 的状态和输出：\begin{equation}\begin{array}{l}{s_{2}=\phi\left(U x_{2}+W s_{1}\right)} \\ {o_{2}=f\left(V \phi\left(V x_{2}+W s_{1}\right)\right)=f\left(V \phi\left(V x_{2}+W \phi\left(U x_{1}+W s_{0}\right)\right)\right)}\end{array}\end{equation} 当 $t =3$的状态和输出：\begin{equation}\begin{array}{l}{s_{3}=\phi\left(U x_{3}+W s_{2}\right)} \\o_{3}=f\left(V \phi\left(U x_{3}+W s_{2}\right)\right)=\ldots=f\left(V \phi\left(U x_{3}+W \phi\left(U x_{2}+W \phi\left(U x_{1}+W s_{0}\right)\right)\right)\right)\end{array}\end{equation} 所以对于RNN 而言，所谓的无法解决长依赖是因为 $s_0$， $x_1$经过了太多的激活层和权重相乘。而常见的激活函数sigmoid 或者tanh其最大值是1，不可能是一直是1，那么很容易等于0。如 $0.8^{50}=0.00001427247$。 这就是RNN 中出现梯度消失的原因。 使用Relu 是可以解决梯度消失，因为 $x&gt;0$ 情况下梯度恒为0。但是容易发生梯度爆炸（虽然可以通过设置适当的阈值）。 如果说通过修改网络结构来解决梯度消失或者梯度爆炸，那么就是LSTM 了。 （2）LSTM 是如何缓解梯度消失的？ \begin{equation}h_{t}=o_{t} \odot \phi \left(f_{t} \odot c_{t-1}+i_{t} \odot \phi \left(W_{x c} x_{t}+W_{h c} h_{t-1}+b_{c}\right)\right)\end{equation} 在隐藏层中 LSTM 相对于普通的RNN 有了很多加和，从而保证了在$c$(context) 这个路径上是有梯度的。但是其他路径上梯度流与普通 RNN 类似，照样会发生相同的权重矩阵反复连乘。梯度爆炸相对于梯度消失是更容易解决的。 参考文献Understanding LSTM Networks]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>rnn</tag>
        <tag>lstm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NLP Papers Reading- BERT]]></title>
    <url>%2F2019%2F04%2F27%2Fpaper-reading-bert%2F</url>
    <content type="text"><![CDATA[总结 GPT, ELMO 和BERT 模型。 attention is all you needSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed cy a compatibility function of the query with the corresponding key. 中文的理解：深度学习里的Attention model其实模拟的是人脑的注意力模型，举个例子来说，当我们观赏一幅画时，虽然我们可以看到整幅画的全貌，但是在我们深入仔细地观察时，其实眼睛聚焦的就只有很小的一块，这个时候人的大脑主要关注在这一小块图案上，也就是说这个时候人脑对整幅图的关注并不是均衡的，是有一定的权重区分的。这就是深度学习里的AttentionModel的核心思想。所谓注意力机制，就是说在生成每个词的时候，对不同的输入词给予不同的关注权重。通过注意力机制，我们将输入句子编码为一个向量序列，并自适应地选择这些向量的一个子集，同时对译文进行译码，例如where are you——&gt;你在哪？现在我们在翻译“你”的时候给”you”更多的权重，那么就可以有效的解决对齐问题。 Background: 主要是面临的三个问题。 Transformer 的结构示意图:(transformer 就是讨论了如何实现上述的 self-attention 结构) Encoder: encoder由6个相同的层堆叠而成，每个层有两个子层。第一个子层是多头自我注意力机制(multi-head self-attention mechanism)，第二层是简单的位置的全连接前馈网络(position-wise fully connected feed-forward network)。在两个子层中会使用一个残差连接，接着进行层标准化(layer normalization)。也就是说每一个子层的输出都是LayerNorm(x + sublayer(x))。网络输入是三个相同的向量q, k和v，是word embedding和position embedding相加得到的结果。为了方便进行残差连接，我们需要子层的输出和输入都是相同的维度。 Decoder: decoder也是由N（N=6）个完全相同的Layer组成，decoder中的Layer由encoder的Layer中插入一个Multi-Head Attention + Add&amp;Norm组成。输出的embedding与输出的position embedding求和做为decoder的输入，经过一个Multi-HeadAttention + Add&amp;Norm（（MA-1）层，MA-1层的输出做为下一Multi-Head Attention + Add&amp;Norm（MA-2）的query（Q）输入，MA-2层的Key和Value输入（从图中看，应该是encoder中第i（i = 1,2,3,4,5,6）层的输出对于decoder中第i（i = 1,2,3,4，5,6）层的输入）。MA-2层的输出输入到一个前馈层（FF），经过AN操作后，经过一个线性+softmax变换得到最后目标输出的概率。 对于decoder中的第一个多头注意力子层，需要添加masking，确保预测位置i的时候仅仅依赖于位置小于i的输出。 层与层之间使用的Position-wise feed forward network。 transformer 的结构谈及 transformer，首先应该提到是 计算效率的大大提高，从原先的RNN 的线性O(N)提升的很多，这个的实现是基于多线程的。而后者是因为是有顺序的线性模型，所以是无法使用并行运算的。 对于 RNN 来说，句首的信息要传递到句尾，需要经过 n 次 RNN 的计算；而 Self-Attention 可以直接连接任意两个节点. 从整体上来看，Transformer依旧是一个“Sequence to Sequence”框架，拥有Encoder和Decoder两部分： transformer 的结构 论文中encoder层由6个encoder堆叠在一起，decoder层也一样。 每一个 encoder 和 decoder 的内部简图如下： encoder 部分 对于encoder，包含两层，一个self-attention层和一个前馈神经网络，self-attention能帮助当前节点不仅仅只关注当前的词，从而能获取到上下文的语义。decoder也包含encoder提到的两层网络，但是在这两层中间还有一层attention层，帮助当前节点获取到当前需要关注的重点内容。 self-attention 先说一下 attention 机制的实现： 当使用 self的时候，query, key and value 这三个就都是相同的。经过softmax() 得到就是一个权重，用于标记和 当前处理的词语的关系。self-attention是Transformer用来将其他相关单词的“理解”转换成我们正在处理的单词的一种思路，attention 就是一种加权平均数，self-attention 可以进一步下放，当前句子中对当前处理的词语最重要的是哪些部分。 Multi-Headed Attention 我的理解就是 在CNN中使用多个filter 的类似产物。该机制理解起来很简单，就是说不仅仅只初始化一组Q、K、V的矩阵，而是初始化多组，tranformer是使用了8组，所以最后得到的结果是8个矩阵。 这样做的主要目的是从不同的语义空间投射原文本，能够从更多的角度表征，并且能够拓展模型对不同位置的关注能力。 这给我们留下了一个小的挑战，前馈神经网络没法输入8个矩阵呀，这该怎么办呢？所以我们需要一种方式，把8个矩阵降为1个，首先，我们把8个矩阵连在一起，这样会得到一个大的矩阵，再随机初始化一个矩阵和这个组合好的矩阵相乘，最后得到一个最终的矩阵。这个就是 multi-head attention 机制的全部的流程了。 Positional Encoding transformer给encoder层和decoder层的输入添加了一个额外的向量Positional Encoding，维度和embedding的维度一样，这个向量采用了一种很独特的方法来让模型学习到这个值，这个向量能决定当前词的位置，或者说在一个句子中不同的词之间的距离。这个位置向量的具体计算方法有很多种，论文中的计算方法如下： $$P E ( p o s , 2 i ) = \sin \left( p o s / 10000 ^ { 2 i } / d _ { m } \text {odel} \right)$$ $$P E ( p o s , 2 i + 1 ) = \cos \left( p o s / 10000 ^ { 2 i } / d _ { m } o d e l \right)$$其中pos是指当前词在句子中的位置，i是指向量中每个值的index，可以看出，在偶数位置，使用正弦编码，在奇数位置，使用余弦编码. 最后把这个Positional Encoding与embedding的值相加，作为输入送到下一层。 layer normalization Normalization有很多种，但是它们都有一个共同的目的，那就是把输入转化成均值为0方差为1的数据。我们在把数据送入激活函数之前进行normalization（归一化），因为我们不希望输入数据落在激活函数的饱和区。 batch normalization 和layer normalization 的区别，简单来说前者依赖于 batch size，是在不同的样本的同一个特征上进行归一化，在CNN 上的效果更好，后者在一个样本上进行归一化， 在 RNN的网络结果中效果更好。更多详细的内容可以参考这篇博客. BN的主要思想就是：在每一层的每一批数据上进行归一化。我们可能会对输入数据进行归一化，但是经过该网络层的作用后，我们的数据已经不再是归一化的了。随着这种情况的发展，数据的偏差越来越大，我的反向传播需要考虑到这些大的偏差，这就迫使我们只能使用较小的学习率来防止梯度消失或者梯度爆炸。可以看到，右半边求均值是沿着数据 batch_size的方向进行的 不过 LN 是在每一个样本上计算均值和方差，而不是BN那种在批方向计算均值和方差！ decoder 部分 decoder部分其实和encoder部分大同小异，不过在最下面额外多了一个masked mutil-head attetion，这里的mask也是transformer一个很关键的技术。 Transformer 模型里面涉及两种 mask，分别是 padding mask 和 sequence mask。其中 padding mask 在所有的 scaled dot-product attention 里面都需要用到，而 sequence mask 只有在 decoder 的 self-attention 里面用到。前者就是一种填充技术，使得 不定长的sequence 变成定长的sequence之后做出的一些处理。 Padding Mask 什么是 padding mask 呢？因为每个批次输入序列长度是不一样的也就是说，我们要对输入序列进行对齐。具体来说，就是给在较短的序列后面填充 0。但是如果输入的序列太长，则是截取左边的内容，把多余的直接舍弃。因为这些填充的位置，其实是没什么意义的，所以我们的attention机制不应该把注意力放在这些位置上，所以我们需要进行一些处理。具体的做法是，把这些位置的值加上一个非常大的负数(负无穷)，这样的话，经过 softmax，这些位置的概率就会接近0！ Sequence masksequence mask 是为了使得 decoder 不能看见未来的信息。也就是对于一个序列，在 time_step 为 t 的时刻，我们的解码输出应该只能依赖于 t 时刻之前的输出，而不能依赖 t 之后的输出。因此我们需要想一个办法，把 t 之后的信息给隐藏起来。那么具体怎么做呢？也很简单：产生一个上三角矩阵，上三角的值全为1。把这个矩阵作用在每一个序列上，就可以达到我们的目的。 缺点： 问题一： 长输入 在文本只要等篇章级别的任务重， transformer 因为计算量的复杂性，所以速度回急速变慢。所以短期内，这些方面仍然是RNN 或者CNN的应用场景（虽然两者做的也不是很好）。 transformer 的改进思路： 比如可以把长输入切断分成K份，强制把长输入切短，再套上Transformer作为特征抽取器，高层可以用RNN或者另外一层Transformer来接力，形成Transformer的层级结构，这样可以把n平方的计算量极大减少。（分而治之的思路是真的比较常见呀） 问题二： 网络结构过于复杂 如何更深刻认识它的作用机理，然后进一步简化它，这也是一个好的探索方向。 上面在做语义特征抽取能力比较时，结论是对于距离远与13的长距离特征，Transformer性能弱于RNN 分界线 - - - - - – - - - - - - - - - - - 分 界线（另外的解读方式） Encoder和Decoder的内部结构： 模型的特点：Positional embedding；（位置嵌入向量——其实类似word2vec，处理的语序的信息）。multi-head attention; (多头注意力机制——点乘注意力的升级版本， 这个就类似ensemble的思想，不同的子空间的attention 进行融合）Position-wise Feed-Forward Networks（位置全链接前馈网络——MLP变形） 有两种常用的注意力函数，一种是加法注意力(additive attention)，另外一种是点乘注意力(dot-productattention)，论文所采用的就是点乘注意力，这种注意力机制对于加法注意力而言，更快，同时更节省空间。 加法注意力还是以传统的RNN的seq2seq问题为例子，加性注意力是最经典的注意力机制，它使用了有一个隐藏层的前馈网络（全连接）来计算注意力分配： 公式:$$\alpha _ { i j } = \frac { \exp \left( e _ { i j } \right) } { \sum _ { k = 1 } ^ { L } e _ { i k } }$$ Scaled Dot-Product这篇论文计算query和key相似度使用了dot-product attention，即query和key进行点乘（内积）来计算相似度。 Multi-Head Attention:（将单个计算组成矩阵运算，有利于并行运算）在实际中为了并行计算，可以在一组queries上计算注意力函数，将多个query堆叠成Q，同理keys和values也被堆叠成K和V，通过下面的公式来计算矩阵输出:self-attention 模型就是自己对自己求attention，即𝑄=𝐾=𝑉$$\text { Attention } ( Q , K , V ) = \operatorname { softmax } \left( \frac { Q K ^ { T } } { \sqrt { d _ { k } } } \right) V$$之所以用内积除以维度的开方，论文给出的解释是：假设Q和K都是独立的随机变量，满足均值为0，方差为1，则点乘后结果均值为0，方差为dk。也即方差会随维度dk的增大而增大，而大的方差导致极小的梯度(我认为大方差导致有的输出单元a（a是softmax的一个输出）很小，softmax反向传播梯度就很小（梯度和a有关））。为了避免这种大方差带来的训练问题，论文中用内积除以维度的开方，使之变为均值为0，方差为1。 除了计算一个单独的注意力函数，论文提出对queries，keys和values做h次不同的投影, 然后都经过Scaled Dot-Product Attention，将结果拼接在一起，最后通过一个线性映射输出，通过多头注意力，模型能够获得不同子空间下的位置信息。如下图所示，公式如下:$$\text {MultiHead} ( Q , K , V ) =Concat(head_1, head_2, …, head_h) W ^ { o }$$ Self-Attention那么首先要明白什么是Attention。从语言学的角度，它是表示词与词之间的关联关系（这种关系是通过反向传播学习到的）。而 self-attention 表示句子内部词于词之间的关联关系，如下图中的it 和其他位置词的关系，颜色越深表示关系越紧密， 从图中可以看到 it 正确的关联到了 animal 它所指代的一个词。 Positional Encodingtransformer是使用 positional encoding 加入了位置信息，保持了词语之间的上下文关系。实现的的时候，在偶数位置，使用正弦编码，在奇数位置，使用余弦编码. Residual connection和layer-normalization 对于学习CV的人估计对这个结构一点也不陌生，Residual connection是对于较为深层的神经网络有比较好的作用，比如网络层很深时，数值的传播随着weight不断的减弱，Residual connection是从输入的部分，就是图中虚线的部分，实际连到它输出层的部分，把输入的信息原封不动copy到输出的部分，减少信息的损失。 layer-normalization这种归一化层是为了防止在某些层中由于某些位置过大或者过小导致数值过大或过小，对神经网络梯度回传时有训练的问题，保证训练的稳定性，这是神经网络设计比较常用的case。 结论：self-attention层的好处是能够一步到位捕捉到全局的联系，解决了长距离依赖，因为它直接把序列两两比较（代价是计算量变为 O(n2)，当然由于是纯矩阵运算，这个计算量相当也不是很严重），而且最重要的是可以进行并行计算，因为这个操作是可以使用矩阵运算的。相比之下，RNN 需要一步步递推才能捕捉到，并且对于长距离依赖很难捕捉。而 CNN 则需要通过层叠来扩大感受野（感受野的概念，更像是 最后经过CNN 的一个点在原始的图像中是多大的面积，这种管中窥豹的感觉），这是 Attention 层的明显优势。 Deep Contextualized Word Representations（可以得到有上下文关系的词向量， 这个特点是相对于 word2vec 或者 glove 的）这篇论文的想法其实非常非常简单，但是取得了非常好的效果。它的思路是用深度的双向RNN(LSTM)在大量未标注数据上训练语言模型，如下图所示。然后在实际的任务中，对于输入的句子，我们使用这个语言模型来对它处理，得到输出的向量，因此这可以看成是一种特征提取。但是和普通的Word2Vec或者GloVe的pretraining不同，ELMo得到的Embedding是有上下文的。比如我们使用Word2Vec也可以得到词”bank”的Embedding，我们可以认为这个Embedding包含了bank的语义。但是bank有很多意思，可以是银行也可以是水边，使用普通的Word2Vec作为Pretraining的Embedding，只能同时把这两种语义都编码进向量里，然后靠后面的模型比如RNN来根据上下文选择合适的语义——比如上下文有money，那么它更可能是银行；而如果上下文是river，那么更可能是水边的意思。但是RNN要学到这种上下文的关系，需要这个任务有大量相关的标注数据，这在很多时候是没有的。而ELMo的特征提取可以看成是上下文相关的，如果输入句子有money，那么它就(或者我们期望)应该能知道bank更可能的语义，从而帮我们选择更加合适的编码。 我们把这两个方向的RNN合并起来就得到Bi-LSTM。我们优化的损失函数是两个LSTM的交叉熵加起来是最小的： 主要贡献： 提出了一个双向训练的 language model，使用前K-1 个词语训练 第K 个词语，然后使用后 N-K+1 个词语训练第K 个词语，所以第 K 个词语是combine 了上下文的信息的。 word embedding 的表示是不同layer 累加的结果，weights 的设定是学习而得。 Why do we need contextualized representations? 词语的意思是由上下文所决定的。所以一个固定的 word embedding 不能准确的表示不同场景下 word 的含义。 As an illustrative example, take the following two sentences: “The bank on the other end of the street was robbed”“We had a picnic on the bank of the river” Both sentences use the word “bank”, but the meaning of the word differs completely between them. This phenomenon where two identical words change meaning depending on the context is known as “polysemy“, and has been an issue in the NLP deep learning community ever since word embeddings really took off. Most current neural networks are bad at handling polysemy because they use a single vector to represent the meaning of the word “bank”, regardless of the context. In reality, the vector representing any word should change depending on the words around it. 之前的做法的缺点是对于每一个单词都有唯一的一个embedding表示, 而对于多义词显然这种做法不符合直觉, 而单词的意思又和上下文相关, ELMo的做法是我们只预训练language model, 而word embedding是通过输入的句子实时输出的, 这样单词的意思就是上下文相关的了, 这样就很大程度上缓解了歧义的发生. 这种算法的特点是：每一个word representation都是整个输入语句的函数。具体做法就是先在大语料上以language model为目标训练出bidirectional LSTM模型，然后利用LSTM产生词语的表征。ELMo故而得名(Embeddings from Language Models)。为了应用在下游的NLP任务中，一般先利用下游任务的语料库(注意这里忽略掉label)进行language model的微调,这种微调相当于一种domain transfer; 然后才利用label的信息进行supervised learning。 ELMo表征是“深”的，就是说它们是biLM的所有层的内部表征的函数。这样做的好处是能够产生丰富的词语表征。高层的LSTM的状态可以捕捉词语意义中和语境相关的那方面的特征(比如可以用来做语义的消歧)，而低层的LSTM可以找到语法方面的特征(比如可以做词性标注)。如果把它们结合在一起，在下游的NLP任务中会体现优势。 所以，最后的 embedding 使用不同层进行weights 的累加，这种理论上是站得住脚的。上面的描述和 CV 是惊人的相似。 Salient featuresELMo representations are: Contextual: The representation for each word depends on the entire context in which it is used. Deep: The word representations combine all layers of a deep pre-trained neural network. Character based: ELMo representations are purely character based, allowing the network to use morphological clues to form robust representations for out-of-vocabulary tokens unseen in training. related work: 针对传统词向量是固定的，与上下文语境无关的缺点，先前的工作多通过两种方式来解决： (1) 通过引入字符级(subword)信息丰富词向量表达； (2) 学习每个单词不同含义的独立向量； ELMo也利用了字符卷积（Character-Convolutions）引入字符级信息，并同时结合了深度双向语言模型的各层隐状态来丰富词向量表达。 P.s.：基于字符的模型不仅能够通过引入字符级信息丰富词向量表达，也能够在很大程度上解决NLP领域的OOV（Out-Of-Vocabulary）问题。 ELMo用到上文提到的双向的language model, 给定N个tokens (t1, t2,…,tN), language model通过给定前面的k-1个位置的token序列计算第k个token的出现的概率:$$p \left( t _ { 1 } , t _ { 2 } , \ldots , t _ { N } \right) = \prod _ { k = 1 } ^ { N } p \left( t _ { k } | t _ { 1 } , t _ { 2 } , \ldots , t _ { k - 1 } \right)$$后向的计算方法与前向相似:$$p \left( t _ { 1 } , t _ { 2 } , \ldots , t _ { N } \right) = \prod _ { k = 1 } ^ { N } p \left( t _ { k } | t _ { k + 1 } , t _ { k + 2 } , \ldots , t _ { N } \right)$$biLM训练过程中的目标就是最大化:$$\sum _ { k = 1 } ^ { N } \left( \log p \left( t _ { k } | t _ { 1 } , \ldots , t _ { k - 1 } ; \Theta _ { x } , \vec { \Theta } _ { L S T M } , \Theta _ { s } \right) + \log p \left( t _ { k } | t _ { k + 1 } , \ldots , t _ { N } ; \Theta _ { x } , \overline { \Theta } _ { L S T M } , \Theta _ { s } \right) \right)$$ELMo对于每个token $t_k$, 通过一个L层的biLM计算出2L+1个表示:$$R_{ k } = { x _ { k } ^ { L M } , \vec { h } _ { k , j } ^ { L M } , h _ { k , j } ^ { L M } | j = 1 , \ldots , L } = { h _ { k , j } ^ { L M } | j = 0 , \ldots , L }$$其中$h _ { k , 0 } ^ { L M }$是对token进行直接编码的结果(这里是字符通过CNN编码), $h _ { k , j } ^ { L M } = \left[ \vec { h } _ { k , j } ^ { L M } ; \overline { h } _ { k , j } \right]$ 是每个biLSTM层输出的结果. 在实验中还发现不同层的biLM的输出的token表示对于不同的任务效果不同. 应用中将ELMo中所有层的输出R压缩为单个向量, ELMok=E(Rk;Θϵ), 最简单的压缩方法是取最上层的结果做为token的表示:$E \left( R _ { k } \right) = h _ { k , L } ^ { L M }$ 更通用的做法是通过一些参数来联合所有层的信息:$$E L M o _ { k } ^ { t a s k } = E \left( R _ { k } ; \Theta ^ { t a s k } \right) = \gamma ^ { t a s k } \sum _ { j = 0 } ^ { L } s _ { j } ^ { t a s k } h _ { k , j } ^ { L M }$$ 其中$s_j$是一个softmax出来的结果, $γ$是一个任务相关的scale参数, 我试了平均每个层的信息和学出来$s_j$发现学习出来的效果会好很多. 文中提到$γ$在不同任务中取不同的值效果会有较大的差异, 需要注意, 在SQuAD中设置为0.01取得的效果要好于设置为1时. ELMo: Context Matters Instead of using a fixed embedding for each word, ELMo looks at the entire sentence before assigning each word in it an embedding. It uses a bi-directional LSTM trained on a specific task to be able to create those embeddings. ELMo provided a significant step towards pre-training in the context of NLP. The ELMo LSTM would be trained on a massive dataset in the language of our dataset, and then we can use it as a component in other models that need to handle language. What’s ELMo’s secret? ELMo gained its language understanding from being trained to predict the next word in a sequence of words - a task called Language Modeling. This is convenient because we have vast amounts of text data that such a model can learn from without needing labels. We can see the hidden state of each unrolled-LSTM step peaking out from behind ELMo’s head. Those come in handy in the embedding proecss after this pre-training is done. ELMo actually goes a step further and trains a bi-directional LSTM – so that its language model doesn’t only have a sense of the next word, but also the previous word.ELMo comes up with the contextualized embedding through grouping together the hidden states (and initial embedding) in a certain way (concatenation followed by weighted summation). lstm-based language modelIn case you are unfamiliar with language models, a language model is simply a model that can predict how “likely” a certain sequence of words is to be a real piece of text. This is generally done by training a model to take a part of sentence (say, the first n words) and predict the next word – or more precisely, output the probability of each word in the vocabulary being the next word (In this blog post, we’ll focus on LSTM-based language models which are the focus of this paper). One trick that this paper uses is to train a language model with reversed sentences that the authors call the “backward” language model.这种模型：上一个模型的输出到下一个模型输入Furthermore, instead of using a single-layer LSTM, this paper uses a stacked, multi-layer LSTM. Whereas a single-layer LSTM would take the sequence of words as input, a multi-layer LSTM trains multiple LSTMs to take the output sequence of the LSTM in the previous layer as input (of course, the first layer takes the sequence of words as input). This is best illustrated in the following illustration: 最后的embedding 是是将不同的层 combination起来，这个系数是通过学习出来的。In ELMo, the part that is task specific is the combination of the task-agnostic representations. The weight is learned for each task and normalized using the softmax function. The parameter $\gamma$ is a task-dependent value that allows for scaling the entire vector, which is important during optimization. 优缺点： 在ELMo中，嵌入基于一个双层的双向语言模型（biLM）的内部状态计算，ELMo也是因此得名的：Embeddings from Language Models（来自语言模型的嵌入）。ELMo的特性：ELMo的输入是字符而不是单词。这使得它可以利用子字（sub-word）单元为词汇表以外的单词计算有意义的表示（和FastText类似）。ELMo是biLM的多层激活的连接（concatenation）。语言模型的不同层编码了单词的不同信息。连接所有层使得ELMo可以组合多种词表示，以提升下游任务的表现。 OpenAI GPT它的思想其实也很简单，使用Transformer来学习一个语言模型，对句子进行无监督的Embedding，然后根据具体任务对Transformer的参数进行微调。 这篇论文中的 多任务学习是如何体现的呢？首先是无监督的pretraining 中有一个语言模型，需要优化一个最大似然估计 L1，然后再监督的fine-tuning 中有一个交叉熵损失函数，这里也是有一个loss ，记为L2。正常情况下，我们应该调整参数最大化L2， 但是我们使用的是多任务学习，同时让它最大似然L1 和L2。$$L_{3}(\mathcal{C})=L_{2}(\mathcal{C})+\lambda \times L_{1}(\mathcal{C})$$ A simple but tough-to-beat baseline for sentence embeddingsTaking the average of the word embeddings in a sentence tends to give too much weight to words that are quite irrelevant, semantically speaking. Smooth Inverse Frequency tries to solve this problem in two ways: Weighting: like our tf-idf baseline above, SIF takes the weighted average of the word embeddings in the sentence. Every word embedding is weighted by a/(a + p(w)), where a is a parameter that is typically set to 0.001 and p(w) is the estimated frequency of the word in a reference corpus. (使用新的词权重计算方法，不是tf-idf, 频率越高，权重越低，抑制高频词) Common component removal: next, SIF computes the principal component of the resulting embeddings for a set of sentences. It then subtracts from these sentence embeddings their projections on their first principal component. This should remove variation related to frequency and syntax that is less relevant semantically.As a result, SIF downgrades unimportant words such as but, just, etc., and keeps the information that contributes most to the semantics of the sentence. 第一步中的$p(w) $ 是在语料中的词频，第二步中对整个句子集合进行一次PCA，然后对每个句子上面得到的向量减去它在第一奇异向量或者说主成分上的投影。最后初步的句子向量减去对应句子向量的共性成分(起到平滑作用),得到最后的独有的句子向量(使得各个句子向量间的耦合度降低,增强句子的鲁棒性).耦合性越低（模块之间的关联性越小） 作用： 第一步骤中的超参数 $a $ 是一种平滑项，对于低频词的支持，出现的次数少，反而权重是比较大的，降低常见词的权重。$ ( \alpha p(w)) $,其中$(p(w))$是单词 $(w) $在整个语料中出现的概率(词频角度), $ (\alpha) $是一个超参数. 这样, 即使和 $ (c_s) $的内积很小, 这个单词也有概率出现. 第二步骤中的减去 主成分，可以理解为让各个词向量更好的分开，减去公共的部分，减少耦合性，使得相似的句子聚类在一起。因为这个主成分是整个语料库中的主成分. 另外论文中还提到了这种方法的鲁棒性: 使用不同语料(多种领域)训练得到的不同的word embedding, 均取得了很好的效果, 说明了对各种语料的友好. 使用不同语料得到的词频, 作为计算词权重的因素, 对最终的结果影响很小. 对于方法中的超参数, 在很大范围内, 获得的结果都是区域一直的, 即超参数的选择没有太大的影响. 尽管长期以来句子的无监督表示学习是主流，最近几个月（2017年末/2018年初），我们看到了许多非常有趣的工作，显示了向监督学习和多任务学习（不同的任务学习到不同的维度，然后组合）转向的趋势。 强力/迅速的基线：FastText、词袋（Bag-of-Words） 当前最先进模型：ELMo、Skip-Thoughts、Quick-Thoughts、 InferSent、MILA/MSR的General Purpose Sentence Representations、Google的Universal Sentence Encoder 关于nlp 中的word embedding 是可以有 phrases, sentences, and paragraphs 三个不同类别的 embedding，所以还是挺好的。 优点： 程序的运行只需要十几分钟，效果和神经网络是相当的 属于无监督的学习，可以对大规模的语料进行利用，相对于有监督的学习方式，这个是优势 缺点： 缺点就是没有考虑句子的语序,导致不能辨别(“我爱你”还是”你爱我”), 只是字意的表达，并没有体现了句意 对于短文本上的word2vec， SIF 效果很好，但是涉及到语意理解的时候，这种方式效果就一般了，而这个时候就应该使用 elmo，transformer or bert 等模型了 Supervised Learning of Universal Sentence Representations from Natural Language Inference Data文章成功的找到了NLP领域的ImageNet — SNLI (Stanford Natural Language Inference dataset), 并且试验了不同的深度学习模型，最终确定bi-LSTM max pooled 为最佳模型。 域 数据 任务 模型(编码器) CV ImageNet image classification Le-Net, VGG-Net, Google-Net, ResNet, DenseNet NLP SNLI NLI ? 基于监督学习方法学习sentence embeddings可以归纳为两个步骤：第一步选择监督训练数据，设计相应的包含句子编码器Encoder的模型框架；第二步选择（设计）具体的句子编码器，包括DAN、基于LSTM、基于CNN和Transformer等。 数据集： 本文采用的是Stanford Natural Language Inference Datasets，简称SNLI （NLP领域的ImageNet ）。SNLI包含570K个人类产生的句子对，每个句子对都已经做好了标签，标签总共分为三类：蕴含、矛盾和中立（Entailment、contradiction and neutral）。下面是这些数据集的一个例子： 从上图可以看出，每个句子对为（text, hypothesis）,中间的judgments为它们的标签。可以看到标签是综合了5个专家的意见，根据少数服从多数的原则得到的。 7种不同的architectures： standard recurrent encoders with LSTM ，取最后一个隐状态 standard recurrent encoders with GRU ，取最后一个隐状态上述两种是基础的recurrent encoder，在句子建模中通常将网络中的最后一个隐藏状态作为sentence representation； conncatenation of last hidden states of forward and backward GRU这种方法是将单向的网络变成了双向的网络，然后用将前向和后向的最后一个状态进行连接，得到句子向量； Bi-directional LSTMs (BiLSTM) with mean pooling Bi-directional LSTMs (BiLSTM) with max pooling这两种方法使用了双向LSTM结合一个pooling层的方法来获取句子表示，具体公式如下： self-attentive network这个网络在双向LSTM的基础上加入了attention机制，具体网络结构如下： hierarchical convolutional networks Now that we have discussed the various sentence encoding architectures used in the paper, let’s go through the part of the network which takes these sentence embeddings and predicts the output label. After the sentence vectors are fed as input to this model, 3 matching methods are applied to extract relations between the text, u and hypothesis, v – concatenation of the two representations (u, v) element-wise product u * v and, absolute element-wise difference |u – v | The resulting vector captures information from both the text, u and the hypothesis, v, and is fed into a 3-class classifier consisting of multiple fully connected layers followed by a softmax layer. Universal Sentence Encoder这篇文章基于InferSent， 也是想找到一个universal encoder。不同之处在于文章把InferSent的bi-lstm换成了DAN（或者Transformer)，而使用DAN这样“简单”的encoder的效果竟然相当好（尤其是时间和内存消耗和其他算法比小很多。） The Google Sentence Encoder is Google’s answer to Facebook’s InferSent. It comes in two forms: an advanced model that takes the element-wise sum of the context-aware word representations produced by the encoding subgraph of a Transformer model. a simpler Deep Averaging Network (DAN) where input embeddings for words and bigrams are averaged together and passed through a feed-forward deep neural network.The Transformer-based model tends to give better results, but at the time of writing, only the DAN-based encoder was available. In contrast to InferSent, the Google Sentence Encoder was trained on a combination of unsupervised data (in a skip-thought-like task) and supervised data (the SNLI corpus). DAN其实DAN(Deep Averaging Networks)应该属于Bag of Words类的算法。因为比较特殊，单独列出来。 它是在对所有词语取平均后，在上面加上几层神经网络。特殊的地方在于它在sentiment analysis中表现也不错，这在BOW类方法中比较罕见。 新方法 类型 基于的旧算法 贡献 SIF 无监督 BOW 一个简单而有效的baseline算法 InferSent 监督 NA 找到了NLP领域的ImageNet – SNLI， 并给出了一个state-of-art 算法 P-mean 无监督 BOW 比SIF更简单且有效的一个算法且适用于cross-lingual Universal-sentence-encoder 监督 InferSent 更加简单的encoder 文章共提出两种基于不同网络架构的Universal Sentence Encoder：Transformer and Deep Averaging Network (DAN).Our two encoders have different design goals. One based on the transformer architecture targets high accuracy at the cost of greater model complexity and resource consumption. The other targets efficient inference with slightly reduced accuracy. NLP 是如何体现了多任务训练的？ 该篇论文在前人的研究基础上，综合利用无监督训练数据和有监督训练数据，进行多任务训练，从而学习一个通用的句子编码器。无监督训练数据包括问答(QA)型网页和论坛，Wikipedia, web news，有监督训练数据为SNLI。多任务模型设计如下图所示，其中灰色的encoder为共享参数的句子编码器。 论文对比了DAN和Transfomer这两种编码器。得出如下结论： Transformer 模型在各种任务上的表现都优于简单的 DAN 模型，且在处理短句子时只稍慢一些。（更高的精度） DAN模型也能具有很不错的表现，并且相较于Transformer模型，训练时间和内存的开销都更小，尤其是当句子较长时。（更快的速度） 总结： Sentence Embedding的质量往往由训练数据和Encoder共同决定。Encoder不一定是越复杂越好，需要依据下游任务、计算资源、时间开销等多方面因素综合考虑。 BERT的理解词汇扩展： 对于词向量中 OOV 问题的处理方法： 如果使用词向量分词的话，一种常见的是字节维度 n-gram 模型，也就是把一个单词分成多个部分，比如说playing 分成play he ##ing 两个token，这种更加细粒度的划分是一种常见的处理oov 的方式。 但是在工业界，经常使用多个不同的语言模型得到word2vec， 可以分成两类，一类是针对该任务训练的word2vec，一类是在通用的模型下进行训练的word2vec， 并且当这种训练方法不同的时候，最后得到的结果也是不同的。通过不同的任务进行补充。在一定程度上是可以缓解 oov 问题的。 在句子向量中进行词汇扩展的方式：常见的是使用 word2vec的词向量来进行扩展句子向量中的词向量。 ELMo 思路是使用双向RNN 在大量未标注数据上训练语言模型，对于之后特定的任务，我们使用这个语言模型进行特征提取得到输出的向量。和 word2vec 不同的是，这个embedding 是有上下文的。比如说 bank的embedding 的上下文如果有river 那么就是水边的意思；如果上下文有money 那么更可能是银行的意思。 实现： 基于lstm 进行实现的，总的loss 是前后两个loss的相加，优化的时候，两个lstm的交叉熵加起来是最小的。 openai 出的 GPT (generative pre-training)， 得到的语言模型中的参数不是固定的，是可以根据特定的任务进行微调，使得词向量更加匹配特定的任务。思想也是很简单，使用transformer学习一个语言模型，对句子进行无监督的embedding，然后根据特定的任务对transformer的参数进行微调。 无监督的预训练：最初的时候 transformer是用来进行机器翻译的，encoder 得到的输出输入到decoder中去。但是在GPT 中的模型，encoder 是用来预测下一个词的。但是基于self-attention的基本结构，它是能够看中心词汇左右两边的上下文，这个特点和想要达到的任务是不符合要求的。所以这里使用到了 mask的原理，将中心词后面的词汇遮住，然后进行训练。 有监督的fine-tuning当只有一个句子（分类问题） 使用简单的分类问题作为一个例子，给定一个句子(x1, …xn)，然后给定标签。然后再最上层加上一个softmax，使用交叉熵损失函数计算loss，从而根据数据调整之前 transformer 和softmax 中weights 的参数。 本来按照elmo 的思想，我们可以fixed（固定）encoder 这个语言模型中的参数，然后只是训练最后softmax 中weights 的参数，但是这里同时去优化 encoder 和最后softmax 的参数，就类似一种多任务学习，同时优化了两个loss，并且这两个loss 中是可以设置权重的，所以从理论上讲模型是具有更好的泛化性能的。（对于前一个loss 的训练，因为前者是无监督的，所以这里只是使用了其中的x，而没有使用其中的y）并且训练速度也会提高，为什么这么说呢？因为是基于大的已经训练好的数据集上进行fine tune，得到的结果可以用不会太差来进行描述，那么你的loss 也会相应的不会太大，所以需要的迭代的次数也不会很多。 当有两个句子的时候（比如相似度的计算或者问答系统）。需要使用特殊的技巧将两个序列变成一个输入序列。（上面有个图是可以非常清楚的展示如何处理多种不同输入） 对于只有一个序列的任务，可以在前后加上两个特殊token，”start” 和”extract”，分别表示开始和结束；对于两个序列，可以在中间加上一个特殊的token, “delim”，输出是三分类标签中的一个。如果是相似度计算，因为对称性，可以把他们交换顺序，然后输入两个transformer。 好终于进入了bert 的学习： 问题： 传统的ELMo 或者GPT 最大的问题是语言模型是单向，不同同时得到前后两个方向的信息。注意transformer中的self-attention 从理论上是可以同时handle 前后上下文的，但是这里使用了mask 机制，所以这种方式也是不行的。防止过拟合的方法：通过对网络结构的约束，比如CNN的局部特效，RNN的时序特效，多层网络的层次结构，对它进行了很多约束，从而使得能够收敛到最佳的参数。 解决方案：在BERT之前，LM 通常是单向的，常见的做法是分别训练正向和反向的LM，然后再做一个ensemble得到的上下文相关表示。这样的做法是会有信息缺失的问题的。 BERT 是 “Bidirectional Encoder Representations from Transformers” 的缩写，B表示模型能够同时利用前后两个方向的信息，而ELMo和GPT 只能是单个方向的。 而bert 仍然使用的是 transformer模型，那么是如何解决语言模型中的只利用一个方向的问题呢？因为bert 不是普通的语言模型，而是一种mask 语言模型。 bert 的输入表示：输入是两个句子，然后是对于每个token 进行3 个embedding：词的embedding， 位置的embedding和segment 的embedding。词语的embedding 是非常常见的，位置embedding引入了词语的顺序信息，segment 的embedding可以学习到不同的segment的信息。位置向量是因为transformer 不像传统的RNN 那样能够很好的处理时序，所以人为加入了表示位置的向量。 这种海量数据还是很重要的。 bert 模型是需要有一个固定的sequence的长度，比如说是128，如果不够了会padding，如果多了会进行裁剪。 Mask LM 和NSP 分别对应的是词级别和句子级别的任务，效果很好。bert也是一种语言模型，在语料训练过程中，是把这两个任务的损失函数相加，同时学习这两个任务。所以这个就是一种多任务学习方式。BERT是通过两个辅助任务训练语言模型。 Mask LM（bert 的第一个重点终于来了） mask语言模型类似完形填空，给定一个句子，然后把其中的某个词遮挡起来，让人猜测可能的词语。这个会随机mask 15%的词， 然后让bert 来预测这些mask 的词，同归调整模型的参数使得模型预测正确的概率尽可能的大，这个等价于交叉熵的损失函数。这样的transformer在编码一个词的时候（必须）参考上下文的信息。 但是还有一个问题，就是在pretraineing mask LM 时候会出现一些特殊的token，但是在fine-tuning 时候并不会出现，这个时候就出现了mismatch 的问题。因此在bert中，如果某个token 被选中之后， 会随机按照以下的方式随机的执行： 80%随机替换成mask 10%替换成随机的一个词 10%概率替换成单词本身 因此，当他看到了 [mask或者apple 的时候，强迫模型在编码的时候不能太依赖当期的词，而是要考虑上下文，甚至进行上下文的“纠错”。 预测句子关系（bert模型中第二个训练任务） 在问答中，前后两个句子有一定的关联关系，我们希望bert pretraining 的模型能够学习到这种关系。因此bert 增加了一种新的额学习任务–预测两个句子是否有关联关系。这个训练集要求是文章（有上下文关系的句子）。对于这个任务，bert 以50%的概率随机抽取两个无关的句子，50%的概率抽取有关联的句子。（这个句子是经过token处理的句子） 实验证明该项任务是可以明显给QA和NLI 类任务带来提升的。 fine-tuning 共有四种任务， （使用过的是两种任务，a和b，分别进行 sentence pair classification 和single sentence classification）对于普通的分类任务，输入是一个序列，如图右上所示，所有的token 都是属于同一个segment，然后再模型的最后一层接上一个softmax进行分类， 用分类数据进行fine tuning 对于相似度计算等输入为两个序列的任务，过程如左上所示；两个序列的token 是对应着不同的segment(id =0/1)。在最后一层加上softmax 进行分类，然后使用分类数据进行fine-tuning 第三类任务是序列标注，比如命名实体识别，使用右下的方式进行训练。 第四类是问答类问题，输入是一个问题和一段很长包含答案文字（paragraph），输出在这段文字里找到的问题的答案。 在参数设置上，作者建议大部分的参数不用变，只是修改batch size， learning rate 和 number of epochs 就可以了：batch size: 16,32learning rate(adam): 5e-5, 3e-5, 2e-5number of epochs: 3, 4并且训练数据集越大，对超参数就越不敏感，而且fine tune 一般来说收敛的是比较快的。 对于中文来说，bert对中文提供的模型是基于字的，而word2vec 是基于词的，所以当word2vec的词向量效果越好，那么这个差距是越大的。 有监督的模型效果好，但是有标签的数据获取非常难。一种有效的解决方案是采用多任务学习(multi task learning MLT), 一方面可以在数据集标注较少的情况下利用其它相似任务的标注数据，另一方面可以降低针对特定任务的过拟合，起到正则化的作用。 Resnet, BERT都告诉我们： 更大的数据规模，更多样性的数据和更高的数据质量。数据还是比较关键的。 bert 模型的缺点： 对于篇章级别的任务，transformer的计算量复杂，速度是变得很慢。解决方案是进行长输入的切分 网络结构的过于复杂 对于中文的改进（从字到词语 的mask） 这几篇文章都是对BERT模型的Pretraining阶段的Mask进行了不同方式的改进。为了解决OOV的问题，我们通常会把一个词切分成更细粒度的WordPiece(不熟悉的读者可以参考机器翻译·分词和WordpieceTokenizer)。BERT在Pretraining的时候是随机Mask这些WordPiece的，这就可能出现只Mask一个词的一部分的情况。 简单说原来的bert 模型对于中文是基于字的， 比如如何mask 掉琵琶中的一个字，那么模型是很容易预测下一个字的。所以为了解决这个问题，很自然的想法就是把词作为一个整体，要么都mask 掉，要么不mask。当然前不久哈工大和科大讯飞是做了这方面的工作的。 但是对于BERT模型本身(基于Mask LM的Pretraining、Transformer模型和Fine-tuning)没有做任何修改。 原文链接 transformer 的理解Transformer 其采用 Self Attention 来学习序列的表示, 具体的是: Scaled Dot-Product Attention. 为解决位置信息 (Position Information) 丢失问题, 模型将 Positional Encpding 与 Input Embedding 结合；为防止 decoder 中后续位置 (模型可并行计算) 对前面位置的影响, 模型在 decoder 中使用了 Mask 以使位置 ii 处的预测只依赖于前面的输出. Transformer由且仅由self-Attenion和Feed Forward Neural Network组成。一个基于Transformer的可训练的神经网络可以通过堆叠Transformer的形式进行搭建.作者的实验是通过搭建编码器和解码器各6层，总共12层的Encoder-Decoder(这个只是一个通用的框架，实际上是可以根据自己的需求进行不同的层的增减) Multi-Head Attention相当于 多个不同的self-attention的集成（ensemble）。 如何表示位置信息？ 常见的模式有：a. 根据数据学习；b. 自己设计编码规则。在这里作者采用了第二种方式。编码公式如下： $$P E(\text { pos, } 2 i)=\sin \left(\frac{\text { pos }}{10000^{\frac{2 i}{\text { model }}}}\right)$$ $$P E(p o s, 2 i+1)=\cos \left(\frac{p o s}{10000^{\frac{2 i}{d_{m o d e l}}}}\right)$$ 公式中： pos 表示这个word 在句子中的位置 $i $表示 embedding维度，比如 $d_{model}$ 是512，那么 $i$ 就从 1 到512. 上式中，$pos$ 表示当前单词在句子中的位置，可以看出对于偶数位，使用正弦编码，对于奇数位使用余弦编码.。$d$ 表示模型的维度。除了单词的绝对位置，单词的相对位置也非常重要。这就是为什么使用正弦和余弦函数。根据公式$\sin (\alpha+\beta)=\sin \alpha \cos \beta+\cos \alpha \sin \beta$ 和$\cos (\alpha+\beta)=\cos \alpha \cos \beta-\sin \alpha \sin \beta$，这表明位置$k+p$ 的位置向量可以表示为位置 $k$的特征向量的线性变化，这为模型捕捉单词之间的相对位置关系提供了非常大的便利。 谷歌还特意将这种方式构造的向量和学习得到的向量作对比，发现效果接近，然后谷歌就用这个构造式的，因为虽然效果接近，但这种构造式的更能在使用中适应不同长度序列。 将位置向量和词向量进行加和得到最终输入向量，所以前面我们看到词向量和位置向量维度是相同的。 两种mask 技术 padding mask：mask对某些值进行掩盖，使其不产生效果。我们每次批处理序列的长度是不一样的，所以我们需要对齐，具体来说是在较短序列中填充0. 而attention机制不应该把注意力放在这些位置上。具体做法是这些位置上加上一个非常大的负数，这样经过softmax，这些位置的概率就会接近0. 在 encoder 和decoder 中都使用。 sequence mask：是为了decoder 不能看见未来的信息，进行预测的时候只是依赖前$i$ 个单词的信息。具体做法，产生一个上三角形，上三角形值全部为1，下三角形和对角线都是0，作用在序列上就可以达到目的。只是在decoder 中使用。 对于 attention 机制的分类 可以从多角度对 Attention 进行分类，如从信息选择的方式上，可以分为 Soft attention 和 Hard attention。从信息接收的范围上可分为 Global attention 和 Local attention。 global attention 中所有的信息都要参与计算，这样计算的开销就比较大，而别当encoder 的句子比较长时，如一段话或一篇文章。所以提出了 local attention的概念 transformer中是有三种attention 机制 Encoder 由 6 个相乘的 Layer 堆叠而成（6并不是固定的，可以基于实际情况修改）。 从图中可以知道 decoder 是有三种网络结构的， Diff_1：Decoder SubLayer-1 使用的是 “masked” Multi-Headed Attention 机制，防止为了模型看到要预测的数据，防止泄露。Diff_2：SubLayer-2 是一个 encoder-decoder multi-head attention。Diff_3：LinearLayer 和 SoftmaxLayer 作用于 SubLayer-3 的输出后面，来预测对应的 word 的 probabilities 。 encoder-decoder multi-head attention 中 123456789101112131415class DecoderLayer(nn.Module): "Decoder is made of self-attn, src-attn, and feed forward (defined below)" def __init__(self, size, self_attn, src_attn, feed_forward, dropout): super(DecoderLayer, self).__init__() self.size = size self.self_attn = self_attn self.src_attn = src_attn self.feed_forward = feed_forward self.sublayer = clones(SublayerConnection(size, dropout), 3) def forward(self, x, memory, src_mask, tgt_mask): m = memory x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask)) x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask)) return self.sublayer[2](x, self.feed_forward) 重点在于 x = self.sublayer1 self.src_attn 是 MultiHeadedAttention 的一个实例。query = x，key = m, value = m, mask = src_mask，这里x来自上一个 DecoderLayer，m来自 Encoder的输出。（m 是encoder的输出， x 树decoder 中的输出，主线还是跟着m 走，attention是求解的 m 和x 的相关性） 到目前位置 transformer 中三种不同的attention 都已经介绍完毕。 最后还有一个全连接加上一个softmax 求probably，用来看哪些词出现的概率是最大的。 beam search or greedy search： 前者是保留k 个候选集，后者只保留一个。 teacher forcing or scheduled sampling： 前者在下一个的输入使用真实的样本；后者是开始的时候使用真实的样本，到后来加上了生成的样本。 RNN，CNN 和Self-attention 时间效率的比较 网络结构 时间复杂度 Self-Attention $O(length^2 \cdot dim^2) $ RNN(LSTM) $O(length \cdot dim^2)$ Convolution $O(length \cdot dim^2 \cdot kernel_width)$ 其中 $length$ 是处理的句子的长度， $dim$ 是隐藏层的长度，$kernel_width$ 表示CNN 中kernel 的宽度。可以发现当隐藏层的长度远远大于处理的句子长度的时候，RNN 的计算效率非常低；相反，当处理的句子长度（文章）远远大于隐藏层的长度时候，选择RNN 可能是更好的选择。]]></content>
      <categories>
        <category>NLP</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[pygen]]></title>
    <url>%2F2019%2F04%2F24%2Fpygen%2F</url>
    <content type="text"><![CDATA[pygen功能：有关联的随机生成人名，邮箱，ID Card (ssn)，电话，地址等信息，并且可以选择保存为 pandas dataframe格式, 数据库”.db” 文件, Excel 文件和csv 文件格式，用于机器学习训练。项目地址github。 随机生成虚假个人信息具有很大的应用空间。首先，虚假的生成数据可以用于机器学习模型的“准备数据”，当真实的数据比较少或者难以获得的时候，可以使用生成数据进行训练模型，待模型调通之后，然后使用真实的模型。并且，当真实的数据集中缺少某些特征时候，可以使用这种方法进行特征的填充。比如大的数据集中缺少现居城市地址的时候，可以调用该库中的 “city_real” 进行填充。 当前使用最为广泛的是 Faker 开源库用于个人信息的生成，对于中文姓名、邮箱电话等信息支持有限。并且生成的数据是单列的，数据之间没有联系。比如生成的身份证 (ssn) 和姓名所能体现的性别是不匹配(了解更多可以参考这里)、生成的姓名中缺少复姓和电话邮箱等信息不符合我们的使用习惯等等。所以我将从以下几点改进： 增强数据之间相关性 生成名字的多样性 符合国人使用习惯的邮箱电话 提供保存多种保存文件格式，更加适合机器学习的训练 中文名字有很强的性别属性。例如名字中带有“杰”“志”“宏”等字的一般为男性，带有“琬”“佩”“梅”等字的一般为女性。当然也有一些比较中性的字，例如“文”“安”“清”等，比较难猜测性别，关于这点会在另一个博客中展开，请期待。 faker 对中文的支持有限，比如下面这种情况。 1234from faker import Fakerfake = Faker('zh_CN')for _ in range(10): print(fake.name(),fake.ssn(),fake.phone_number()) 从图中可以明显的看出 “王玉梅”和 “李桂花”都是两个女性，但是这种身份证信息（ssn）都没有体现这点。关于身份证的科普信息可以从这里获得。简单来说倒数第二位表示性别信息，如果是男性就是奇数如果是女性就是偶数。faker 生成的数据是不具有数据之间的相关性的。 基于此，我们进行了改进。首先是姓名的生成，然后是性别的判断，最后再生成相应性别的身份证号码。 123from pygen import pygendb =pygen()db.gen_dataframe(fields =['name', 'ssn', 'phone', 'email']) 效果如下： 红色线条表示姓名和性别对应一致，蓝色线条表示结果不确定（“镜阳炎” 像是一个中性的名字），绿色表示生成了含有复姓的名字，增强了数据的多样性。 从上图的 “mail” 一列可以看出邮箱前缀的命名基本上是中文名字中“姓” 和“民”的拼音组合，加强了数据之间的相关性和真实度。 另外，电话号码按照运营商分为三类：0 表示移动，1表示联通，2表示电信。 print(&apos;移动字段:&apos;) for _ in range(5): print(db.simple_ph_num(types =0)) print(&apos;联通字段:&apos;) for _ in range(5): print(db.simple_ph_num(types =1)) print(&apos;电信字段：&apos;) for _ in range(5): print(db.simple_ph_num(types =2)) 输出： 移动字段: 15023689929 16771753917 16790223946 15950129353 15271129554联通字段: 13869739303 13786227031 13950354445 15137578545 15240836142电信字段： 17172983067 15658567011 18562313243 17073127396 15543448286 最后提供了多种文件保存格式，包括”.csv”, “.db” 和”.xlsx”等格式。可以使用如下：12345from pygen import pygendb =pygen()db.gen_table(filename =filename, fields =[&apos;name&apos;, &apos;ssn&apos;, &apos;phone&apos;, &apos;email&apos;])db.gen_excel(filename =filename, fields =[&apos;name&apos;, &apos;ssn&apos;, &apos;phone&apos;, &apos;email&apos;])db.gen_csv(filename =filename, fields =[&apos;name&apos;, &apos;ssn&apos;, &apos;phone&apos;, &apos;email&apos;])]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Data Enhancement</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mode Collapse in GANs]]></title>
    <url>%2F2019%2F04%2F18%2Fmode-collapse-in-gan%2F</url>
    <content type="text"><![CDATA[Mode collapse, a failure case for GANs where the generator generate a limited diversity of samples, regardless of the input. But what causes the mode collapse? There are four reasons for that. The objective of GANs The generator, generates new data, while the discriminator evaluates it for authenticity but not for the diversity of generated instances. the generator can win by producing a polynomial number of training examples. And a low capacity discriminator cannot detect this process, thus, it cannot guide the generator to approximate the target distribution. Even if a high discriminator identifies and assigns the collapse part a low probability, then the generator will simply move from its collapsed output to focus on another fixed output point. Generator No matter the objective function is, if it only considers individual samples (without looking forward or backward) then the generator is not directly incentivised to produce diverse examples. From [1], standard GAN training corresponds exactly to updating the generator parameters using only the first term in this gradient because of a fixed discriminator during GAN training. Therefore, in standard GAN training, each generator update step is a partial collapse towards a delta function. $$\frac { \mathrm { d } f _ { K } \left( \theta _ { G } , \theta _ { D } \right) } { \mathrm { d } \theta _ { G } } = \frac { \partial f \left( \theta _ { G } , \theta _ { D } ^ { K } \left( \theta _ { G } , \theta _ { D } \right) \right) } { \partial \theta _ { G } } + \frac { \partial f \left( \theta _ { G } , \theta _ { D } ^ { K } \left( \theta _ { G } , \theta _ { D } \right) \right) } { \partial \theta _ { D } ^ { K } \left( \theta _ { G } , \theta _ { D } \right) } \frac { \mathrm { d } \theta _ { D } ^ { K } \left( \theta _ { G } , \theta _ { D } \right) } { \mathrm { d } \theta _ { G } }$$ Some methods have been proposed. Multiple generators and weight-sharing generators are developed to capture more modes of the distribution. Discriminator The mode collapse is often explained as gradient exploding of discriminator, which comes from the imbalance between the discriminator and the generator. For example, the technique of TTUR could help discriminator to keep its optimality. But some researchers believe that this is a desirable goal since a good discriminator can give good feedback and ignore the fact. In addition, the discriminator process each example independently, the generator depends on discriminator, thus no mechanism to tell the outputs of the generator to become more similar to each other. The idea from [2], that we could use mini-batch discrimination to help generator give better feedback A straightforward approach to handle multimodality is to take random noise vectors along with the conditional contexts as inputs, where the contexts determine the main content and noise vectors are responsible for variations.The noise vectors are ignored or of minor impacts, since cGANs pay more attention to learn from the high-dimensional and structured conditional contexts. Another question Mode collapse may happen only partially?since training is stochastic progress, the input of generator network will vary and the sample drawn from the real distribution will also vary But sometimes mode collapse is not all bad news. In style transfer using GAN, we are happy to convert one image to just a good one, rather than finding all variants. Indeed, the specialization in the partial mode collapse sometimes creates higher quality images. referrences [1]. Section 2.4 of Unrolled Generative Adversarial Networks[2]. Section 3.2 of Improved Techniques for Training GANs[3]. Mode Seeking Generative Adversarial Networks for Diverse Image Synthesis[4]. Improving Generalization and Stability of Generative Adversarial Networks]]></content>
      <categories>
        <category>Deep learning</category>
      </categories>
      <tags>
        <tag>mode_collapse</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[A Not-So-Gentle Introduction to Hyper-parameters Tuning]]></title>
    <url>%2F2019%2F04%2F17%2Fa-not-so-gentle-introduction-to-hyperparameters-tuning%2F</url>
    <content type="text"><![CDATA[Setting the hyper-parameters seems like a black art that requires years of experience to acquire. Currently, there are no simple and easy ways to set hyper-parameters, especifically, batch size, learning rate, momentum, and weight decay. A grid search or random search maybe sounds like a good idea. In this blog, I’d like to share you my idea from reading papers and my projects. Hyper-parametersBatch SizeLearning rate is maybe the most important hyper-parameters, but we choose batch size firstly because large batch size needs a large learning rate in most circumstances. A general principle is: use as a large batch size as possible to fit your CPU memory or/both GPU memory. There are several reasons: larger batch sizes permit the use of larger learning rates A constant number of iterations favors larger batch sizes However, small batch sizes add regularization while large batch sizes add less. So utilize it while balancing the proper amount of regularization. Learning RateWe will introduce the idea from [Cyclical Learning Rates for Training Neural Networks][1]: Cyclical Learning Rates. Instead of monotonically decreasing the learning rate, this method lets the learning rate cyclically vary between reasonable boundary values. The essence of this learning rate policy comes from the observation that increasing the learning rate might have a short term negative effect and yet achieve a longer term beneficial effect. This observation leads to the idea of letting the learning rate vary within a range of values rather than adopting a stepwise fixed or exponentially decreasing value. That is, one sets minimum and maximum boundaries and the learning rate cyclically varies between these bounds.%From Cyclical Learning Rates for Training Neural Networks An intuitive understanding of why CLR methods work comes from considering the loss function topology. Dauphin et al. argue that the difficulty in minimizing the loss arises from saddle points rather than poor local minima. Saddle points have small gradients that slow the learning process. However, increasing the learning rate allows for more rapid traversal of saddle point plateaus. But the question is that how can we find the Minimum bound and Maximum bound. There is a simple way to estimate the reasonable minimum and maximum boundary values with one training run of the network for a few epochs. It is a “LR range test”; run your model for several epochs while letting the learning rate increase linearly between low and high LR values. For example, set both the step size and maxiter to the same number of iterations. In this case, the learning rate will increase linearly from the minimum value to the maximum value during this short run. Next, plot the accuracy versus learning rate. Note the learning rate value when the accuracy starts to increase and when the accuracy slows, becomes ragged, or starts to fall. These two learning rates are good choices for bounds; that is, set $ lr_{base}$ to the first value and set $ lr_{max} $ to the latter value. MomentumSince learning rate is regarded as the most important hyper-parameter to tune then momentum is also important. Like learning rates, it is valuable to set momentum as large as possible without causing instabilities during training. The large learning rate can deal with local minimum but works fail when it comes to saddle point where momentum comes to rescue. The local minimum is like the following picture.In mathematics, a saddle point or minimax point is a point on the surface of the graph of a function where the slopes (derivatives) in orthogonal directions are all zero (a critical point), but which is not a local extremum of the function. Your first step from the very top would likely take you down, but then you’d be on a flat rice terrace. The gradient would be zero, and you’d have nowhere to go. To remedy this, we employ momentum - the algorithm remembers its last step and adds some psroportion of it to the current step. This way, even if the algorithm is stuck in a flat region, or a small local minimum, it can get out and continue towards the true minimum. In summary: when performing gradient descent, learning rate measures how much the current situation affects the next step, while momentum measures how much past steps affect the next step. Weights DecayWhen training neural networks, it is common to use “weight decay,” where after each update, the weights are multiplied by a factor slightly less than 1. This prevents the weights from growing too large and can be seen as gradient descent on a quadratic regularization term. But why? Large weights might correlate with certain patterns in the input data (x), this means that the model almost hard codes certain values. This then makes our training data fit well but our test data fit less well. The idea of weight decay is simple: to prevent overfitting, every time we update a weight $w$ with the gradient $∇J$ in respect to $w$, we also subtract from it $λ∙w$. This gives the weights a tendency to decay towards zero, hence the name. L2 is a type of weights decay.$$J ( W ; X , y ) + \frac { 1 } { 2 } \lambda \cdot | W | ^ { 2 }$$ But weights decay is not necessarily true for all gradient-base algorithms and was recently shown to not be the case for adaptive gradient algorithms, such as Adam. In addition, weight decay is not the only regularization technique. In the past few years, some other approaches have been introduced such as Dropout, Bagging, Early Stop, and Parameter Sharing which work very well in NNs. Takeoff Batch Size Use as a large batch size as possible to fit your memory Learning Rate Perform a learning rate range test to identify a “large” learning rate. Momentum Test with short runs of momentum values 0.99, 0.97, 0.95, and 0.9 to get the best value for momentum. If using the 1-cycle learning rate schedule, it is better to use a cyclical momentum (CM) that starts at this maximum momentum value and decreases with increasing learning rate to a value of 0.8 or 0.85. Weights Decay A grid search to determine the proper magnitude but usually does not require more than one significant figure accuracy.A more complex dataset requires less regularization so test smaller weight decay values, such as $10^{−4} $, $10^{−5} $, $10^{−6} $, 0.A shallow architecture requires more regularization so test larger weight decay values, such as $10^{−2} $, $10^{−3} $, $10^{−4} $. References[1]. Cyclical Learning Rates for Training Neural Networks[2]. A disciplined approach to neural network hyper-parameters]]></content>
      <categories>
        <category>Deep learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Weights Initialization]]></title>
    <url>%2F2019%2F04%2F17%2Fweights-initialization%2F</url>
    <content type="text"><![CDATA[weights initialization 知识点 和分享两个图像领域的架构（resnet 和 inception v3）。 Training a neural network consists of four steps: initialize weights and biases, forward propagation, compute the loss function and backward propagation. This blog mainly focuses on the first part: weights initialization. After completing this tutorial, you will know: Four main types of weights initialization How to choose between Xavier /zivier/ initialization and He initialization Types of Weights Initialization Initializing weights with zero When you set all weights in a neural network to zero, the derivative with respect to loss function is the same for every $ w$ in the same layer, thus all the weights have the same values in the subsequent iteration, which makes your model equivalent to a linear model. Initializing weights randomly You can get weights like this (Python): w =np.random.randn(layer_size[l],layer_size[l-1]) The weighs follows standard normal distribution while it can potentially lead to two issues: vanishing gradients and exploding gradients.下面的情况是很容易发生，因为网络中特征足够多（网络结构足够宽），所以 random 得到数值有足够的 coverage，所以就会出现 weights too small or too large 这种情况。 If the weights start too small, then the signal shrinks as it passes through each layer until it’s too small to be useful. If the weights start too large, then the signal grows as it passes through each layer until it’s too massive to be useful (big value in sigmoid function). Thus there are two necessary conditions to consider: The values of each activation layer won’t be zero The values of each activation layer won’t go into the area of saturation Xavier/Glorot Initialization For deep networks, we can use a heuristic to initialize the weights depending on the non-linear activation function. This applies to Xavier and He initialization. Xavier/Glorot Initialization initializes the weights in your network by drawing them from a distribution with zero mean and a specific variance.$$ { var } ( w _ { i } ) = \frac { 1 } { layer_{l-1}}$$ w=np.random.randn(layer_size[l],layer_size[l-1])*np.sqrt(1/layer_size[l-1]) In practice, it works better for layers with sigmoid or tanh function. He Initialization Using RELU or Leaky RELU is relatively robust to the vanishing/ exploding gradient issues compared with sigmoid function especially for networks that are not too deep. And it the case of Leaky RELU, it never has zero gradients. For RELU, we multiply the randomly generated values of $w$ by: $$\sqrt { \frac { 2 } { layer _ { [ l - 1 ] } } }$$ w=np.random.randn(layer_size[l],layer_size[l-1])*np.sqrt(2/layer_size[l-1]) Sometimes, we combine the idea of Xavier initialization and He initializaiton so the variance becomes the following: $$\sqrt { \frac { 2 } { layer _ { [ l - 1 ] } + \operatorname { layer } _ { [ l ] } } }$$ w=np.random.randn(layer_size[l],layer_size[l-1])*np.sqrt(2/(layer_size[l-1]+layer_size[l])) The idea behind this is that we set the weights neither too much bigger than 1 nor too much less than 1 so the gradients do not vanish or explode too quickly. TakeoffIn summary, the main difference in machine learning is the following: He initialization works better for layers with ReLu(s) activation. Xavier initialization works better for layers with sigmoid activation. Referrence:He initialization Xavier initialization]]></content>
      <categories>
        <category>Deep learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Hadoop And Spark]]></title>
    <url>%2F2019%2F04%2F15%2Fhadoop-spark%2F</url>
    <content type="text"><![CDATA[主要介绍 Hadoop 和 Spark 的关系和联系。 Hadoop的框架最核心的设计就是：HDFS和MapReduce。HDFS为海量的数据提供了存储，则MapReduce为海量的数据提供了计算。所以使用Hadoop则可以抛开spark，而直接使用Hadoop自身的mapreduce完成数据的处理。Spark是不提供文件管理系统的，但也不是只能依附在Hadoop上，它同样可以选择其他的基于云的数据系统平台，但spark默认的一般选择的还是hadoop。 HDFS是一个采用Master/Slave模式的高度容错的分布式文件系统，DataNode节点用于存储数据，NameNode节点维护集群内的元数据，Map负责对数据进行打散，Reduce负责对数据进行聚合，核心步骤为：首先将计算机任务拆分成若干个Map任务，然后分配到不同的节点上执行，每一个Map任务处理数据中的一部分，完成后生产的的中间结果，保存在磁盘中，Reduce将前面的若干个Map的输出汇总到一起输出。 Hadoop集群由一个Master主节点和若干个Slave节点组成。其中，Master节点上运行NameNode和JobTracker守护进程；Slave节点上运行DataNode和TaskTracker守护进程。 Namenode是整个文件系统的管理节点。它维护着1.整个文件系统的文件目录树，2.文件/目录的元信息和每个文件对应的数据块列表。3.接收用户的操作请求。 dataNode提供真实文件数据的存储服务。 文件块（block）：最基本的存储单位。对于文件内容而言，一个文件的长度大小是size，那么从文件的０偏移开始，按照固定的大小，顺序对文件进行划分并编号，划分好的每一个块称一个Block。 HDFS默认Block大小是128MB，以一个256MB文件，共有256/128=2个Block. 不同于普通文件系统的是，HDFS中，如果一个文件小于一个数据块的大小，并不占用整个数据块存储空间。 (这样设置可以减轻namenode压力，因为namonode维护者文件与数据块列表的对应大小) Replication。多复本。默认是三个。（hdfs-site.xml的dfs.replication属性） 首先了解一下Mapreduce，它最本质的两个过程就是Map和Reduce，Map的应用在于我们需要数据一对一的元素的映射转换，比如说进行截取，进行过滤，或者任何的转换操作，这些一对一的元素转换就称作是Map；Reduce主要就是元素的聚合，就是多个元素对一个元素的聚合，比如求Sum等，这就是Reduce。 spark 的优势 每一个作业独立调度，可以把所有的作业做一个图进行调度，各个作业之间相互依赖，在调度过程中一起调度，速度快。（形成了一个有向无环图） 所有过程都基于内存，所以通常也将Spark称作是基于内存的迭代式运算框架。 spark提供了更丰富的算子，让操作更方便。 更容易的API：支持Python，Scala和Java 其实spark里面也可以实现Mapreduce，但是这里它并不是算法，只是提供了map阶段和reduce阶段，但是在两个阶段提供了很多算法。如Map阶段的map, flatMap, filter, keyBy，Reduce阶段的reduceByKey, sortByKey, mean, gourpBy, sort等。 Spark 算子大致可以分为以下两类:1.Transformation 变换/转换算子：这种变换并不触发提交作业，完成作业中间过程处理。2.Action 行动算子：这类算子会触发 SparkContext 提交 Job 作业。怎么区分transformation算子和action算子呢?transformation算子一定会返回一个rdd,action大多没有返回值,也可能有返回值,但是一定不是rdd. Spark 中的RDD就是为了解决这种问题而开发出来的，Spark使用了一种特殊设计的数据结构，称为RDD。RDD的一个重要特征是，分布式数据集可以在不同的并行环境当中被重复使用，这个特性将Spark和其他并行数据流模型框架(如MapReduce)区别开。 RDD 的算子主要分成2类，action 和 transformation。这里的算子概念，可以理解成就是对数据集的变换。action 会触发真正的作业提交，而 transformation 算子是不会立即触发作业提交的。每一个 transformation 方法返回一个新的 RDD。只是某些 transformation 比较复杂，会包含多个子 transformation，因而会生成多个 RDD。这就是实际 RDD 个数比我们想象的多一些 的原因。通常是，当遇到 action 算子时会触发一个job的提交，然后反推回去看前面的 transformation 算子，进而形成一张有向无环图。 列举一些 Transformation 和 ActionTransformantion: Map, Filter, FlatMap, Sample, GroupByKey, ReduceByKey, Union, Join, Cogroup, MapValues, Sort, PartionBy Action: Collect, Reduce, Lookup, Save RDD 的转化操作是返回一个新的 RDD 的操作，比如 map() 和 filter() ，而行动操作则是向驱动器程序返回结果或把结果写入外部系统的操作，会触发实际的计算，比如 count() 和 first() 。Spark 对待转化操作和行动操作的方式很不一样，因此理解你正在进行的操作的类型是很重要的。如果对于一个特定的函数是属于转化操作还是行动操作感到困惑，你可以看看它的返回值类型：转化操作返回的是 RDD，而行动操作返回的是其他的数据类型。 RDD 中所有的 Transformation 都是惰性的，也就是说，它们并不会直接计算结果。相反的它们只是记住了这些应用到基础数据集（例如一个文件）上的转换动作。只有当发生一个要求返回结果给 Driver 的 Action 时，这些 Transformation 才会真正运行。 这个设计让 Spark 更加有效的运行。 区别 数据处理速度 spark 是支持许多机器学习的模型(分类、聚类、回归、协同过滤)，而 hadoop 中的MapReduce 是不支持的. 数据安全恢复：Hadoop每次处理的后的数据是写入到磁盘上，所以其天生就能很有弹性的对系统错误进行处理；spark的数据对象存储在分布于数据集群中的叫做弹性分布式数据集中，这些数据对象既可以放在内存，也可以放在磁盘，所以spark同样可以完成数据的安全恢复。 基于内存的Spark和基于磁盘的Hadoop也是一个区别。 Spark的核心在于RDD，可以理解为包含许多操作接口的数据集合，主要有Transformation（lazy）和action两类算子。spark根据RDD之间的依赖关系切分成不同的阶段stage,RDD之间的转换的思想是 lazy的，也就是说不是实际发生的，而是以有向无环图的方式记录，通过一个Action算子，将积累的所有算子一次性执行。 spark 和mapreduce 的区别： 性能 Spark在内存中处理数据，而MapReduce是通过map和reduce操作在磁盘中处理数据。所以从这方面讲Spark的性能是超过MapReduce的。但是当数据量比较大，无法全部读入内存时，MapReduce就比较有优势。当涉及需要重复读取同样的数据进行迭代式计算的时候，Spark比较有优势；但是当涉及到单次读取，类似ETL操作任务时，适合用MapReduce进行处理。 容错 当执行中途失败时，MapReduce会从失败处继续执行，因为它是依赖于硬盘驱动器的。但是Spark就必须从头开始执行，这样MapReduce相对节省了时间。 应用场景 MapReduce主要是进行离线计算处理，计算一些已存在的数据，比如对已存在的订单或者日志进行分析。而Spark可以应用在一些实时查询和迭代分析的场景，比如像推荐系统。 spark的有几种部署模式，每种模式特点？ 本地模式 Spark不一定非要跑在hadoop集群，可以在本地，起多个线程的方式来指定。方便调试，本地模式分三类local：只启动一个executorlocal[k]: 启动k个executorlocal[*]：启动跟cpu数目相同的 executor standalone模式 分布式部署集群，自带完整的服务，资源管理和任务监控是Spark自己监控，这个模式也是其他模式的基础 Spark on yarn模式 分布式部署集群，资源和任务监控交给yarn管理粗粒度资源分配方式，包含cluster和client运行模式cluster 适合生产，driver运行在集群子节点，具有容错功能client 适合调试，dirver运行在客户端 spark on yarn 的支持两种模式1）yarn-cluster：适用于生产环境；2）yarn-client：适用于交互、调试，希望立即看到app的输出 YARN 架构学习总结 Spark核心技术原理透视二（Spark运行模式谈谈Spark运行模式 Spark On Mesos模式 spark有哪些组件？ master：管理集群和节点，不参与计算。worker：计算节点，进程本身不参与计算，和master汇报。Driver：运行程序的main方法，创建spark context对象。spark context：控制整个application的生命周期，包括dagsheduler和task scheduler等组件。client：用户提交程序的入口。 spark架构与生态 Spark Core Spark SQL Spark Streaming：对实时数据流进行处理和控制。Spark Streaming允许程序能够像普通RDD一样处理实时数据。 MLlib：一个常用机器学习算法库，算法被实现为对RDD的Spark操作。这个库包含可扩展的学习算法，比如分类、回归等需要对大量数据集进行迭代的操作。 GraphX hadoop分为 MapReduce 与 分布式文件系统(HDFS) MapReduce: 仅支持Map和Reduce两种操作Map中间结果需要写磁盘任务调度和启动开销大无法充分利用内存Map和Reduce都需要排序不适合迭代计算 Spark： 丰富的API（Java、Scala、Python、R四种语言，sort、join等高效算子）DAG执行引擎，中间结果不落盘线程池模型减少task启动开销充分利用内存，减少磁盘IO避免不必要的排序操作适合迭代计算，比如机器学习算法 RDD spark 涉及的核心概念就是resilient distributed dataset (RDD)，rdd是具有容错性的数据集合，并可以并行数据计算。有两种方法可以创建rdd,第一种就是parallelizing 方法：序列化存在driver program 中的集合，见下方代码 12val data = Array(1, 2, 3, 4, 5)val distData = sc.parallelize(data) 并parallelize 方法中可以指定数据分区参数，并每个分区对应一个task 如下面代码12val data = Array(1, 2, 3, 4, 5)val distData = sc.parallelize(data,10) 窄依赖是指每个父RDD的Partition最多被子RDD的一个Partition所使用，例如map、filter，见上左图宽依赖是指一个父RDD的Partition会被多个子RDD的Partition所使用，例如groupByKey、reduceByKey等 RDD（Resilient Distributed Dataset）叫做弹性分布式数据集，是Spark中最基本的数据抽象，它代表一个不可变、可分区、元素可并行计算的集合。 RDD的弹性表现:1、弹性之一：自动的进行内存和磁盘数据存储的切换；2、弹性之二：基于Lineage的高效容错（第n个节点出错，会从第n-1个节点恢复，血统容错）；3、弹性之三：Task如果失败会自动进行特定次数的重试（默认4次）；4、弹性之四：Stage如果失败会自动进行特定次数的重试（可以只运行计算失败的阶段）；只计算失败的数据分片； RDD的持久化是spark的一个重要的特性，当你把RDD持久化，每个Node会存储RDD的分区在内存，在其他action中用到此rdd的时候，就不用从头转化，而是直接使用。你可以用persist或者cache方法持久化rdd，Spark的 缓 存是一个容 错 的技 术 -如果RDD的任何一个分区 丢 失，它 可以通 过 原有的 转换 （ transformations ）操作自 动 的重复 计 算并且 创 建出 这 个分区。另外，每一个RDD可以选择不同的持久化级别. RDD 的创建方式主要有2种: 并行化(Parallelizing)一个已经存在与驱动程序(Driver Program)中的集合如set、list; 读取外部存储系统上的一个数据集，比如HDFS、Hive、HBase,或者任何提供了Hadoop InputFormat的数据源.也可以从本地读取 txt、csv 等数据集 RDD 的操作函数(operation)主要分为2种类型 Transformation 和 Action 类别 函数 区别 Transformation Map,filter,groupBy,join, union,reduce,sort,partitionBy 返回值还是 RDD,不会马上 提交 Spark 集群运行 Action count,collect,take,save, show 返回值不是 RDD,会形成 DAG 图,提交 Spark 集群运行 并立即返回结果 Spark运行模式？ （这四种运行模式是需要好好看看的）Spark On LocalSpark On Local Cluster（Spark Standalone）Spark On YarnSpark Cluster模式 常见的Spark的性能瓶颈有哪些？ 影响性能的主要因素是Shuffle，可以优化代码减少不必要的Stage数量及Shuffle数据量以改进Shuffle性能。由于Shuffle的中间结果数据都是要落磁盘的，所以可以考虑给服务器加SSD（一般100G左右就够了），将Spark的tmp目录设置为SSD目录，性能提升20%左右。 其次是IO，网络IO建议上万兆网络（这也是影响Shuffle性能的一个因素），对于磁盘IO，一般是给服务器设置多块单盘，不要做RAID！挂载磁盘时设置noatime，提高磁盘IO性能。 最后是CPU，Spark做计算如果数据已经加载到内存了，CPU就比较容易成为计算瓶颈，代码方面主要是优化计算，减少计算量，同时也要关注计算任务是否有数据倾斜的现象；硬件方面只能换性能更强劲的CPU了。 12345678910111213141516正常的数据分布理论上都是倾斜的，就是我们所说的20-80原理：80%的财富集中在20%的人手中, 80%的用户只使用20%的功能 , 20%的用户贡献了80%的访问量 , 不同的数据字段可能的数据倾斜一般有两种情况。什么是数据倾斜简单的讲，数据倾斜就是我们在计算数据的时候，数据的分散度不够，导致大量的数据集中到了一台或者几台机器上计算，这些数据的计算速度远远低于平均计算速度，导致整个计算过程过慢。以Hadoop和Spark是最常见的两个计算平台，下面就以这两个平台说明：Hadoop中的数据倾斜- Hadoop中的数据倾斜主要表现在ruduce阶段卡在99.99%，一直99.99%不能结束。Spark中的数据倾斜的表现- 单个Executor执行时间特别久，整体任务卡在某个阶段不能结束- Executor lost，OOM，Shuffle过程出错数据倾斜的原理我们以Spark和Hive的使用场景为例。他们在做数据运算的时候会设计到，countdistinct、group by、join等操作，这些都会触发Shuffle动作，一旦触发，所有相同key的值就会拉到一个或几个节点上，就容易发生单点问题。 大部分数据倾斜的原理就类似于下图，很明了，因为数据分布不均匀，导致大量的数据分配到了一个节点。 如何解决这种问题1.业务逻辑，我们从业务逻辑的层面上来优化数据倾斜，比如上面的例子，我们单独对这两个城市来做count，最后和其它城市做整合。2.程序层面，比如说在Hive中，经常遇到count（distinct）操作，这样会导致最终只有一个reduce，我们可以先group 再在外面包一层count，就可以了。3.调参方面，Hadoop和Spark都自带了很多的参数和机制来调节数据倾斜，合理利用它们就能解决大部分问题。 硬盘有机械硬盘(HDD)和固态硬盘(SSD) 针对不同的作业类型，在对集群进行良好配置的提前下，spark作业最耗时的部分往往是因为集群资源的限制，主要体现在三个方面：CPU、DISK IO、NETWORK IO。spark将作业拆分成的单元是stage，不同的stage内部执行不同的逻辑。stage内部既有io操作，也有cpu计算操作，还会有network（主要是shuffle引起的），当这三个部分其中某一部分所占的比例较大时，在资源占用上就会体现出这一部分的bottleneck。例如， 这样一个stage，就是简单的从hdfs中读取文本数据计算有多少行。几乎没有cpu操作，shuffle的量也很小，主要的操作在DISK IO上，因此，这个stage耗时最长的部分就是在磁盘读写上。因此，判断一个作业最耗时的部分，需要实际的去分析stage的执行逻辑，结合实际的资源占用情况，这样才能的到准确完整的答案 对于不同的计算场景，io，shuffle，cpu都有可能成为计算瓶颈。一般来说，做统计的时候io是最大的瓶颈，做数据挖掘的时候比较慢的是shuffle和cpu。 Shuffle的作用是什么？ Shuffle的中文解释为“洗牌操作”，可以理解成将集群中所有节点上的数据进行重新整合分类的过程。其思想来源于hadoop的mapReduce,Shuffle是连接map阶段和reduce阶段的桥梁。由于分布式计算中，每个阶段的各个计算节点只处理任务的一部分数据，若下一个阶段需要依赖前面阶段的所有计算结果时，则需要对前面阶段的所有计算结果进行重新整合和分类，这就需要经历shuffle过程。在spark中，RDD之间的关系包含窄依赖和宽依赖，其中宽依赖涉及shuffle操 filter map flatMap等操作属于transform，rdd经过若干的transform，直到action（如take count isEmpty foreach foreachPartition）才会真正执行。 spark 中常见的action 算子Action类算子也是一类算子（函数）叫做行动算子，如foreach,collect，count等。Transformations类算子是延迟执行，Action类算子是触发执行。一个application应用程序（就是我们编写的一个应用程序）中有几个Action类算子执行，就有几个job运行。 reduce 通过函数func聚集数据集中的所有元素，这个函数必须是关联性的，确保可以被正确的并发执行 collect在driver的程序中，以数组的形式，返回数据集的所有元素，这通常会在使用filter或者其它操作后，返回一个足够小的数据子集再使用 count返回数据集的元素个数 first返回数据集的第一个元素(类似于take(1)) take返回一个数组，由数据集的前n个元素组成。 takeSample(withReplacement,num,seed) withReplacement:结果中是否可重复num:取多少个seed:随机种子 返回一个数组，在数据集中随机采样num个元素组成，可以选择是否用随机数替换不足的部分，seed用于指定的随机数生成器种子 saveAsTextFile saveAsTextFile用于将RDD以文本文件的格式存储到文件系统中 hadoop: javahive: javestorm: clojure, jstorm javakafka: scalaspark: scalaflink: scala Spark is a fast and general engine for large-scale data processing. speed, run programs up to 100x faster than hadoop mapreduce in memory, or 10x faster on disk. ease of use, Java, Scala, Python, R combine SQL(离线任务), streaming（实时任务）, and complex analytics. run everywhere, Spark runs on Hadoop, Mesos, standalone(这个是spark 自己的), or in the cloud. It can access diverse data sources including HDFS, Cassandra, HBase, and S3. 有三种下载方式 spark 官网 github 托管 arkive.apache.cn 安装指南 collect是收集起来，然后展示。spark 中的方法通常是被称为算子 什么是 RDD? RDD( Resilient distributed dataset) 叫做分布式数据集，是Spark 中最基本的数据抽象，它代表一个不可变，可分区，里面的位置可并行计算的集合。RDD 具有数据流模型的特点：自动容错，位置感知性调度和可伸缩性，RDD 允许用户执行多个查询时显式将工作集缓存在内存中，后序的查询能够重用工作集，极大的提供了查询的速度。 RDD 的属性？（多看） 生成RDD的两种方式 一种是使用textFile()一种使用parallelize() 两种类型的算法主要是从运行效率角度考虑 transformation（转换类型），仅仅记录一个运算过程，只有当 action类型的动作产生，那么才会运行。action（动作类型），生成一个任务就提交了集群上进行计算，所以一定要记住 action 类型的算子，基于优化的角度，慎用。 查找官方 spark programming guide, 官网中的 transformation 是常用的类型，不是全部的转换类型。 存储mysql 数据库，使用 mappartitions 而不是 map进行操作，减少调用的次数。 常见的transformation 类型 map (func) 每一个元素 一个个根据 func 进行处理 filter 需要给定一个filter 的逻辑 flatmap 先是压平，然后处理 mappartitions 取出数据的分区进行遍历，一个个分区进行处理 sample 抽样 union intersection distinct 去重 groupbykey 使用key 进行分组，相同key 就分到一块 reducebykey 使用key 来做聚合 aggregatebykey sortbykey 使用key 来做排序 join 左连接 和右连接 repartition 重新分区 常见的action reduce 聚合 collect count first 拿到第一个元素 take (n) 拿到前 n 个元素 take Sample 抽样 takeOrdered 排序之后再去取元素 saveAsTextFile(path) 存储 foreach 没有返回值（和map 不一样，map 处理之后是有返回值的）， 比如说拿到数据并且存储到数据库 MapReduce 6个过程 如何去优化 shuffle 过程？这个博客讲解的很好hadoop 中的shuffle]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机基础]]></title>
    <url>%2F2019%2F04%2F10%2Fbasics_of_cs%2F</url>
    <content type="text"><![CDATA[计算机中的基础知识，比如操作系统、计算机组成、数据库、计算机网络、软件工程。 操作系统 并发和并行并行(parallel)：指在同一时刻，有多条指令在多个处理器上同时执行。就好像两个人各拿一把铁锨在挖坑，一小时后，每人一个大坑。所以无论从微观还是从宏观来看，二者都是一起执行的。并发(concurrency)：指在同一时刻只能有一条指令执行，但多个进程指令被快速的轮换执行，使得在宏观上具有多个进程同时执行的效果，但在微观上并不是同时执行的，只是把时间分成若干段，使多个进程快速交替的执行。 常见的页面置换算法 最佳置换算法 (opt)虽然这个算法不可能实现，但是最佳页面置换算法可以用于对可实现算法的性能进行衡量比较。 先进先出置换算法（FIFO）这种算法只是在按线性顺序访问地址空间时才是理想的，否则效率不高。因为那些常被访问的页，往往在主存中也停留得最久，结果它们因变“老”而不得不被置换出去。FIFO的另一个缺点是，它有一种异常现象（belady），即在增加存储块的情况下，反而使缺页中断率增加了。当然，导致这种异常现象的页面走向实际上是很少见的。 LRU (least recently used) 最近最少使用算法思想：LRU（Least recently used，最近最少使用）算法根据数据的历史访问记录来进行淘汰数据，其核心思想是“如果数据最近被访问过，那么将来被访问的几率也更高”。实现： 最常见的实现是使用链表保存缓存数据。常见的操作：a. 新数据插入到链表头部；b. 每当缓存命中（即缓存数据被访问），则将数据移到链表头部；c. 当链表满的时候，将链表尾部的数据丢弃。 最少使用（LFU）置换算法在采用最少使用置换算法时，应为在内存中的每个页面设置一个移位寄存器，用来记录该页面被访问的频率。该置换算法选择在最近时期使用最少的页面作为淘汰页。LRU的淘汰规则是基于访问时间，而LFU是基于访问次数的。 进程的状态进程一般有3种基本状态：运行、就绪和阻塞。就绪：当一个进程获得了除处理机以外的一切所需资源，一旦得到处理机即可运行，则称此进程处于就绪状态。运行：当一个进程在处理机上运行时，则称该进程处于运行状态。阻塞：也称为等待或睡眠状态，一个进程正在等待某一事件发生（例如请求I/O而等待I/O完成等）而暂时停止运行，这时即使把处理机分配给进程也无法运行，故称该进程处于阻塞状态。 CPU调度算法引入多程序设计，目的是提高计算机资源利用率，尤其是CPU利用率（CPU utilization）。分为 非抢占式：进程自愿交出CPU，引起新一轮的调度。抢占式：进程被迫交出CPU，引起新一轮的调度。CPU调度器追求目标CPU利用率（CPU utilization）吞吐率（Throughput）周转时间（Turnaround time）等待时间（Waiting time）响应时间（Response time）常用的调度算法：a. FCFS调度算法（First-Come,First-Served Scheduling）b. Shortest-Job-First（SJF）调度算法（最短作业优先算法）c. 优先权法（Priority Scheduling）d. 轮转法（Round Robin，RR）常说的时间片轮转，如果时间片用完了，那么就被迫交出 CPU资源可以通过一个例题讲解：缺页率的计算 IO控制方式 I/O控制方式主要有程序查询方式、中断方式、DMA方式和通信方式。 程序查询方式也称程序轮询方式，用户程序直接控制主机和外部设备之间输入和输出操作。cpu 通过不断循环检测IO 设备状态的端口，当发现设备ready 的时候，cpu 就可以与I/O 设备进行数据存取操作。 中断方式：当I/O 设备完成时候，就会想CPU 发出中断请求信息，CPU 收到信号之后就采取相应的措施。与程序查询方式相比，该方式大大提高了CPU 的利用率，但是在中断方式下，也是以字节或者字为单位进行的，而系统是以”块“为存储单位，效率低下。 DMA（直接内存存储）：允许主存储器和I/O 设备设备通过DMA 直接进行数据交换，整个过程无须CPU的干预。 IO 通道，通道也被称为外围设备处理器、输入输出处理机，相对于CPU 而言，是一个处理器。与DMA 控制相比，通道需要的CPU 控制更少，一个通道是可以控制多个设备，并且能够一次进行多个不连续的数据块的存储交换，大大提高了计算系统效率 计算机组成原理 原码, 反码, 补码 原码就是符号位加上真值的绝对值, 即用第一位表示符号, 其余位表示值. 比如如果是8位二进制:12$[+1]_原$ = 0000 0001$[-1]_原 $= 1000 0001 第一位是符号位. 所以8位二进制数的取值范围就是:1[1111 1111 , 0111 1111] 即：1[-127 , 127] 反码：正数的反码是其本身负数的反码是在其原码的基础上, 符号位不变，其余各个位取反.12[+1] = [00000001]原 = [00000001]反[-1] = [10000001]原 = [11111110]反 补码的表示方法是:正数的补码就是其本身负数的补码是在其原码的基础上, 符号位不变, 其余各位取反, 最后+1. (即在反码的基础上+1)12345补码的表示方法是:正数的补码就是其本身负数的补码是在其原码的基础上, 符号位不变, 其余各位取反, 最后+1. (即在反码的基础上+1)[+1] = [00000001]原 = [00000001]反 = [00000001]补[-1] = [10000001]原 = [11111110]反 = [11111111]补 数据库 数据库事务的四大特性（ACID）以及事务的隔离级别什么是数据库的事务？事务其实就是单个数据逻辑单元组成的对象操作集合，而数据库的终极目标就是使数据库从一个一致的状态转换到另一个一致的状态，这就是ACID中的一致性（Consistency），而原子性（Atomicity）、隔离性（Isolation）、持久性（Durability）是为了实现这个目标的手段。 原子性（Atomicity）原子性是指事务是一个不可再分割的工作单位，事务中的操作要么都发生，要么都不发生。 一致性（Consistency）一致性是指在事务开始之前和事务结束以后，数据库的完整性约束没有被破坏。这是说数据库事务不能破坏关系数据的完整性以及业务逻辑上的一致性。 隔离性（Isolation）多个事务并发访问时，事务之间是隔离的，一个事务不应该影响其它事务运行效果。 这指的是在并发环境中，当不同的事务同时操纵相同的数据时，每个事务都有各自的完整数据空间。由并发事务所做的修改必须与任何其他并发事务所做的修改隔离。事务查看数据更新时，数据所处的状态要么是另一事务修改它之前的状态，要么是另一事务修改它之后的状态，事务不会查看到中间状态的数据。 持久性（Durability）意味着在事务完成以后，该事务所对数据库所作的更改便持久的保存在数据库之中，并不会被回滚。 https://blog.csdn.net/qq_25448409/article/details/78110430https://blog.csdn.net/ranran0224/article/details/78427541 数据库的乐观锁和悲观锁是什么？数据库管理系统（DBMS）中的并发控制的任务是确保在多个事务同时存取数据库中同一数据时不破坏事务的隔离性和统一性以及数据库的统一性。乐观并发控制(乐观锁)和悲观并发控制（悲观锁）是并发控制主要采用的技术手段。悲观锁：假定会发生并发冲突，屏蔽一切可能违反数据完整性的操作乐观锁：假设不会发生并发冲突，只在提交操作时检查是否违反数据完整性。 数据库三范式第一范式（1NF）：数据库表中的字段都是单一属性的，不可再分。这个单一属性由基本类型构成，包括整型、实数、字符型、逻辑型、日期型等。第二范式（2NF）：数据库表中不存在非关键字段对任一候选关键字段的部分函数依赖（部分函数依赖指的是存在组合关键字中的某些字段决定非关键字段的情况），也即所有非关键字段都完全依赖于任意一组候选关键字。第三范式（3NF）：在第二范式的基础上，数据表中如果不存在非关键字段对任一候选关键字段的传递函数依赖则符合第三范式。所谓传递函数依赖，指的是如 果存在”A → B → C”的决定关系，则C传递函数依赖于A。因此，满足第三范式的数据库表应该不存在如下依赖关系： 关键字段 → 非关键字段 x → 非关键字段y 事务的并发问题 未提交读(Read Uncommitted)：允许脏读，也就是可能读取到其他会话中未提交事务修改的数据 提交读(Read Committed)：只能读取到已经提交的数据。Oracle等多数数据库默认都是该级别 (不重复读) 可重复读(Repeated Read)：可重复读。在同一个事务内的查询都是事务开始时刻一致的，InnoDB默认级别。在SQL标准中，该隔离级别消除了不可重复读，但是还存在幻象读 串行读(Serializable)：完全串行化的读，每次读都需要获得表级共享锁，读写相互都会阻塞 事务的四种隔离级别在数据库操作中，为了有效保证并发读取数据的正确性，提出的事务隔离级别。我们的数据库锁，也是为了构建这些隔离级别存在的。 隔离级别 脏读（Dirty Read） 不可重复读（NonRepeatable Read） 幻读（Phantom Read） 未提交读（Read uncommitted） 可能 可能 可能 已提交读（Read committed） 不可能 可能 可能 可重复读（Repeatable read） 不可能 不可能 可能 可串行化（Serializable ） 不可能 不可能 不可能 增删改查操作 1). 增加有两种操作使用insert插入单行数据：123 语法：insert [into] &lt;表名&gt; [列名] values &lt;列值&gt; 例：insert into Strdents (姓名,性别,出生日期) values (&apos;王伟华&apos;,&apos;男&apos;,&apos;1983/6/15&apos;) 注意：如果省略表名，将依次插入所有列 使用insert,select语句将现有表中的 数据添加到已有的新表中123 语法：insert into &lt;已有的新表&gt; &lt;列名&gt; select &lt;原表列名&gt; from &lt;原表名&gt; 例：insert into addressList (&apos;姓名&apos;,&apos;地址&apos;,&apos;电子邮件&apos;)select name,address,email from Strdents 注意：查询得到的数据个数、顺序、数据类型等，必须与插入的项保持一致 2). 删除有两种操作使用delete删除数据某些数据 123 语法：delete from &lt;表名&gt; [where &lt;删除条件&gt;] 例：delete from a where name=&apos;王伟华&apos;（删除表a中列值为王伟华的行） 注意：删除整行不是删除单个字段，所以在delete后面不能出现字段名 使用truncate table 删除整个表的数据123 语法：truncate table &lt;表名&gt; 例：truncate table addressList 注意：删除表的所有行，但表的结构、列、约束、索引等不会被删除；不能用于有外建约束引用的表 3). 修改数据123语法：update &lt;表名&gt; set &lt;列名=更新值&gt; [where &lt;更新条件&gt;] 例：update addressList set 年龄=18 where 姓名=&apos;王伟华&apos; 注意：set后面可以紧随多个数据列的更新值（非数字要引号）；where子句是可选的（非数字要引号），用来限制条件，如果不选则整个表的所有行都被更新 4). 查询数据 1234567891011121314普通查询 语法：select &lt;列名&gt; from &lt;表名&gt; [where &lt;查询条件表达试&gt;] [order by &lt;排序的列 &gt;模糊查询 使用like进行模糊查询 (注意：like运算副只用语字符串) 例：select * from a where name like &apos;赵%&apos; 说明：查询显示表a中，name字段第一个字为赵的记录分组查询 使用group by进行分组查询 例：select studentID as 学员编号, AVG(score) as 平均成绩 (注释:这里的score是列名) from score (注释:这里的score是表名) group by studentID多表联接查询 例：select a.name,b.mark 具体到MySQL 数据库中的增删改查（其实就是关键字的改变，其他的没有什么变化） 创建表1cerate table if not exists（判断是否存在） 表名（xsb） 向表中插入记录 123insert into 表名(xsl) values(‘081101’,’王琳’,’计算机’,’女’,’1990-2-10’,50,null,null);insert into xsl(学号，姓名，总学分)values(‘王燕’，50)；insert into xsl set 学号=’081104’,姓名=’韦言平’,性别=’男’,出生日期=’1989-3-12’; 修改表结构（比如说修改字段、修改主键、默认值等等） 1234567891011121314151617添加字段：alter table xs2 add 家庭住址 varchar(100) after(指定放在哪个字段后面) 总学分;//向xs2表中添加字段“家庭住址”。删除字段：Alter table xs2 drop 家庭住址;将xs2表中的家庭住址字段删除。添加主键：Alter table xs3 add primary key(学号);//在xs3表的学号字段上添加一个主键。删除主键：Alter table xs3 drop primary key;//删除xs3表中的主键；注意：一个表中只有一个主键。添加默认值：Alter table xs3 alter 专业 set default ‘汽车维修’;//为专业字段设置一个默认值为“汽车维修”。6． 删除默认值：Alter table xs3 alter 专业 drop default;//删除xs3表中专业字段的默认值。7．修改字段的类型、字符集：Alter table xs3 modify 姓名 varchar(100) character set utf8;//将xs3表中的姓名字段类型改为varchar(100),字符集改为utf8。8．修改字段的名称、类型：Alter table xs3 change 专业 专业名 varchar(100);//将专业字段改名为专业名。 删除表 1drop database if exists 数据库名 (xsgl) 主键、外键和索引的区别？ 定义： 主键–唯一标识一条记录，不能有重复的，不允许为空外键–表的外键是另一表的主键, 外键可以有重复的, 可以是空值索引–该字段没有重复值，但可以有一个空值 作用： 主键–用来保证数据完整性外键–用来和其他表建立联系用的索引–是提高查询排序的速度 个数： 主键–主键只能有一个外键–一个表可以有多个外键索引–一个表可以有多个唯一索引 什么是主键？ 表中的每一行都应该具有可以唯一标识自己的一列(或一组列)。而这个承担标识作用的列称为主键。主键的两个作用： 惟一地标识一行 作为一个可以被外键有效引用的对象。 主键和索引的区别？ 主键一定是唯一性索引，唯一性索引并不一定就是主键。 一个表中可以有多个唯一性索引，但只能有一个主键。3.主键列不允许空值，而唯一性索引列允许空值（不过最好都不要有null）。 索引可以提高查询的速度。 索引可以提高查询速度,它就相当于字典的目录,可以通过它很快查询到想要的结果,而不需要进行全表扫描. union 和union all的区别 union会对结果集进行处理排除掉相同的结果union all 不会对结果集进行处理，不会处理掉相同的结果 preparedStatement和statement的区别 Statement的作用为一条Sql语句生成执行计划， 建立连接数据库之类的。 statement每次执行sql语句，相关数据库都要执行sql语句的编译，preparedstatement是预编译得,preparedstatement支持批处理 PreparedStatement是预编译的,对于批量处理可以大大提高效率。使用 Statement 对象。在对数据库只执行一次性存取的时侯，用 Statement 对象进行处理。PreparedStatement对象的开销比Statement大，对于一次性操作并不会带来额外的好处。 任何时候使用preparedStatement而不是statementPreparedStatement预编译，防止SQL注入PreparedStatement多次使用可提高效率 drop、truncate和delete的区别 执行速度 drop &gt; truncate &gt; deletedelete一行一行删除，truncate 删除表全部数据，drop 连表数据和表结构一起删除delete 是 DML 语句，没提交事务还可以回滚，truncate 和 drop 是 DDL 语句，操作完马上生效，不能回滚有FOREIGN KEY 约束引用的表，不能使用 TRUNCATE TABLE，而应使用不带 WHERE 子句的 DELETE 语句delete删除行会出现id不联系，truncate是id重新开始 sql 语句 join 操作 inner join 内连接, 两者都有的数据 返回 left join 以左边数据作为返回的行数，右边的数据如果没有则使用 null 填充 right join 和上面的相反 full outer join 一般被译作外连接、全连接 详解的细节可以参考这里，图解 SQL 里的各种 JOIN 查询数据 a. 查询语句的基本语法 1234567891011SELECT column_1, column_2, ...FROM table_1[INNER | LEFT |RIGHT] JOIN table_2 ON conditionsWHERE conditionsGROUP BY column_1HAVING group_conditionsORDER BY column_1LIMIT offset, length; 该SELECT语句由几个子句组成，如下面的列表所述： SELECT后面以逗号分隔的字段名称或星号（*），表示要返回的列。 FROM指定要查询数据的表或视图。 JOIN基于某些连接条件从其他表获取数据。 WHERE过滤结果集中的行。 GROUP BY将数据分组并对每个组应用聚合函数。 HAVING基于GROUP BY子句定义的组筛选组。 ORDER BY指定用于排序字段。 LIMIT约束返回的行数。 SELECT和FROM查询语句中必须的。其他部分是可选的。 b. 在单个表上查询数据 123456select * from STUDENT where STU_AGE&gt;13;select * from STUDENT where STU_AGE in(11,12);select * from STUDENT where STU_AGE between 13 and 15;select * from STUDENT where STU_AGE NOT IN(13,14,16);select * from STUDENT where STU_ID&lt;2005 OR STU_ID&gt;2015; 模糊查询 like 关键字 12select * from STUDENT where STU_NAME LIKE &apos;%王&apos;; #表示匹配任何以王结尾的select * from STUDENT where STU_NAME LIKE &apos;阿%&apos;; # 表示匹配任何以阿开头的 “字符串”参数的值可以是一个完整的字符串，也可以是包含百分号(%)或者下划线()的通配字符。二者有很大区别“%”可以代表任意长度的字符串，长度可以为0;“”只能表示单个字符。 c. 使用聚合函数查询数据 mysql 中的 max min sum avg cout 五个统计函数（聚合函数） 12345select * from STUDENT group by STU_SEX;select STU_SEX,group_concat(STU_NAME) from STUDENT group by STU_SEX; #如果想看分组的内容，可以加groub_concat select EMP_DEP,count(*) from EMPLOYEES where EMP_SALARY&gt;=500 group by EMP_DEP; 带条件的groub by 字段 having，利用HAVING语句过滤分组数据 求每个部门的工资平均值 1select EMP_DEP,avg(EMP_SALARY),group_concat(EMP_NAME)from EMPLOYEES group by EMP_DEP HAVING avg(EMP_SALARY) &gt;=6000; 查找平均工资大于6000的部门，并把部门里的人全部列出来 一般 groupby 和 统计函数一块使用，这样才是比较有意义的， 这个才是有内容的 d. 多表上联合查询 多表上联合查询分为内连接查询和外连接查询 隐式内连接查询 1select STUDENT.STU_ID,STUDENT.STU_NAME,STUDENT.STU_AGE,STUDENT.STU_SEX,GRADE.STU_SCORE from STUDENT,GRADE WHERE STUDENT.STU_ID=GRADE.STU_ID AND GRADE.STU_SCORE &gt;=90; 显式内连接查询1select STUDENT.STU_ID,STUDENT.STU_NAME,STUDENT.STU_AGE,STUDENT.STU_SEX,GRADE.STU_SCORE from STUDENT inner join GRADE on STUDENT.STU_ID=GRADE.STU_ID AND GRADE.STU_SCORE &gt;=90; e. 子查询 以一个查询select的结果作为另一个查询的条件 查找大于85分的学生信息1select * from STUDENT where STU_ID IN(select STU_ID from GRADE where STU_SCORE&gt;85); EXISTS和NOT EXISTS操作符只测试某个子查询是否返回了数据行。如果是，EXISTS将是true，NOT EXISTS将是false。 如果有学生成绩大于100，才查询所有的学生信息1select * from STUDENT where EXISTS (select STU_ID from GRADE where STU_SCORE&gt;=100); ALL、ANY和SOME子查询any和all的操作符常见用法是结合一个相对比较操作符对一个数据列子查询的结果进行测试。它们测试比较值是否与子查询所返回的全部或一部分值匹配。比方说，如果比较值小于或等于子查询所返回的每一个值，&lt;=all将是true，只要比较值小于或等于子查询所返回的任何一个值，&lt;=any将是true。some是any的一个同义词。 123select STU_ID from GRADE where STU_SCORE &lt;67;select * from STUDENT where STU_ID &gt;= any (select STU_ID from GRADE where STU_SCORE &lt;67); f. 合并查询结果 union 是去重的结果， union all 是不去重的结果 1234SELECT语句1UNION | UNION ALLSELECT语句2UNION | UNION ALL …. g. 排序和取数 order by price //默认升序排列 order by price desc //降序排列 order by price asc //升序排列，与默认一样 order by rand() //随机排列，效率不高 123select * from GRADE where STU_SCORE &gt;80 order by STU_SCORE;select * from GRADE where STU_SCORE &gt;80 order by STU_SCORE ASC;select * from GRADE where STU_SCORE &gt;80 order by STU_SCORE desc; g. limit limit [offset,] Noffset 偏移量，可选，不写则相当于limit 0,N；N 取出条目 取分数排名在10-15之间的5条1select * from GRADE order by STU_SCORE desc limit 10,5 g. 为表和字段重命名 1select STU_ID as &apos;学号&apos;,STU_SCORE as &apos;分数&apos; from GRADE; h. 使用正则表达式查询 正则表达式是用某种模式去匹配一类字符串的一个方式。例如，使用正则表达式可以查询出包含A、B、C其中任一字母的字符串。正则表达式的查询能力比通配字符的查询能力更强大，而且更加的灵活。正则表达式可以应用于非常复杂查询。MySQL中，使用REGEXP关键字来匹配查询正则表达式。其基本形式如下： 12345678910111213141516171819202122232425使用字符“^”可以匹配以特定字符或字符串开头的记录。select * from STUDENT where STU_NAME REGEXP &apos;^阿&apos;;以数字开头select * from STUDENT where STU_NAME REGEXP &apos;^[0-9]&apos;;以数字结尾select * from STUDENT where STU_NAME REGEXP &apos;[0-9]$&apos;;以w开头,以数字结束，中间有4个select * from STUDENT where STU_NAME REGEXP &apos;^w....[0-9]$&apos;;查询所有包含有数字和小写字母的select * from STUDENT where STU_NAME REGEXP &apos;[0-9a-z]&apos;;o出现2次select * from STUDENT where STU_NAME REGEXP &apos;o&#123;2&#125;&apos;;fa至少出现一次select * from STUDENT where STU_NAME REGEXP &apos;(fa)+&apos;; 复习的时候，一定要看看这个帖子MySql基本查询、连接查询、子查询、正则表达查询讲解 MySQL 查询语句select讲解与练习 数学 等差数列 通项公式 $$1, 5, 9, 13, … d=4, \quad a_{n}=1+(n-1) \times 4=4 n-3$$所以一般化$$a_n=a_1+ (n-1)d$$等差中项如果三个数字 $a, b, c$成等差数列，那么 $b$ 是 $a$ 和$c$ 的等差中项。 等差求和 数列： $$S_n = a_1 + a_2 + a_3 + …… + a_n$$那么可以得到, $$\begin{split}S_n &amp;= a_1 + (a_1 + d) + (a_1 + 2d) + …… + [ a_1 + (n – 1)d ] \\S_n &amp;= a_n + (a_n – d) + (a_n – 2d) + …… + [ a_n – (n – 1)d ]\end{split}$$上面两个式子相加就可以得到$$2S_n = n (a_1 + a_n)$$如果再加上通项公式，那么可以得到两个常见的求解公式。 $$\begin{split}S_n &amp;= \frac{n (a_1 + a_n)}{ 2} \\S_n &amp;= \frac{na_1 + n (n – 1) d }{2}\end{split}$$ 等比数列 通项公式$$a_n =a_1q^{n-1}$$ 举个栗子 $$ 4，–8，16，–32，64 … … q = –2， an = 4 (–2) n–1 $$等比中项 如果 $a$, $b$, $c$ 是等比数列，那么 $b$ 就是该数列的等比中项. 等比求和 $$\begin{split}S_n &amp;= a_1 + a_1q + a_1q^2 + …… + a_1q^{n – 1} \\qS_n &amp;= a_1q + a_1q^2 + …… + a_1q^{n – 1} + a_1q^n\end{split}$$上式相减，那么可以得到$$(1 – q) S_n = a_1 (1 – q^n )$$整理得$$S_{n}=\frac{a_{1}\left(1-q^{n}\right)}{1-q}=\frac{a_{1}-a_{n} q}{1-q} \quad(q \neq 1)$$ 数列求和 分组求和 通项虽不是等差或等比数列，但通项是可以由等差或者等比数列和的形式得到，那么可以进行拆分，利用最基本的数列公式求和计算。比如说 裂项相消 把数列和中各项裂开之后，可以消除一部分从而计算数列和，适用于通项是 $\frac{1}{a_n.a_{n+1}}$ 的前 $n$ 项和，其中 $a_n$ 是等差数列，那么$\frac{1}{a_n a_{n+1}} = \frac{1}{d}(\frac{1}{a_n} -\frac{1}{a_{n+1}})$ 为等差数列。比如说，常见的拆项的方法有： $\frac{1}{n(n+1)}=\frac{1}{n}-\frac{1}{n+1}$ $\frac{1}{\sqrt{a}+\sqrt{b}}=\frac{1}{a-b}(\sqrt{a}-\sqrt{b})$ 错位相减 利用等比数列求和公式推导，一般可以解决等差乘等比数列的求和 $$= \begin{cases}S_n = a_1 + a_2 + a_3 + …… + a_n&amp; \\qS_n = a_2 + a_3 + …… + a_n+ a_{n+1}&amp;\end{cases}$$ 然后得到$$(1-q)S_n =a_1-a_{n+1}$$ 那么$$S_n= \begin{cases}na_1&amp; (q =1)\\\frac{a_{1}\left(1-q^{n}\right)}{1-q}=\frac{a_{1}-a_{n} q}{1-q} &amp; (q \neq 1)\end{cases}$$ 编程语言中的取模和取余 在matlab中，关于取余和取模是这么定义的：当y≠0时：取余：rem(x,y)=x-y.fix(x./y)取模：mod(x,y)=x-y.floor(x./y)其中，fix()函数是向0取整，floor()函数是向负无穷取整以前边的运算为例：7/（-3）=-2.3，在这个运算中，x为7，y为-3，分别调用fix()和floor()两个函数，得到结果是：fix（-2.3）=-2floor（-2.3）=-3所以，rem（7，-3）=1，mod（7，-3）=-2 对于整数 a，b 来说，取模运算或者求余运算的方法要分如下两步：1、求整数商：c=a/b2、计算模或者余数：r=a-(c*b) 求模运算和求余运算在第一步不同取余运算在计算商值向0方向舍弃小数位取模运算在计算商值向负无穷方向舍弃小数位 比如说 12345678910111213141516&gt;&gt; mod(5,2)ans =1 % 除数是正，余数就是正&gt;&gt; mod(-5,2)ans =1&gt;&gt; mod(5,-2)ans =-1 % 除数是负，余数就是负&gt;&gt; mod(-5,-2)ans =-1 % 用 rem 时，不管除数是正是负，余数的符号与被除数的符号相同&gt;&gt; rem(5,2)ans =1 % 被除数是正，余数就是正&gt;&gt; rem(5,-2); ans =1&gt;&gt; rem(-5,2)ans =-1 % 被除数是负， 余数就是负&gt;&gt; rem(-5,-2)ans =-1 当两个操作数是同号的时候，两者是没有区别的。异号时候，这两者是不一样的结果。一般计算机中使用求模运算，数学中使用求余运算。 在C语言中，%符号表示的是求余运算，在python脚本中，%表示的是取模。（通常取模运算中b不允许是负数，但是在python 2.5.1里可以在%后面跟负数，因为python语言中除法的结果是向0舍入，因此计算结果是取模！） 概率题 伯努利实验事件E只有两种可能结果：发生和不发生，概率分别为p和（1-p）。E独立重复进行n次可以称为n次伯努利试验。 二项分布n次伯努利试验发生k次的可能性服从二项分布： 几何分布举例：一个六面的骰子，平均需要投掷多少次可以掷出数字6:p = 1/6, 1-p = 5/6E(X) = 1/p = 6 次 排列 $$A_{n}^{m}=\frac{n !}{(n-m) !}$$ 组合 C(\begin{array}{c}{n} \ {m}\end{array})=\frac{n !}{m !(n-m) !} 排列组合的关系$$A_{n}^{m}=m ! C_{n}^{m}$$ 古典概型 在一个有限的集合 S 中随机抽取一个元素，求该元素属于子集 T 的概率；概率 p = 子集 T 中元素的数量 / 集合 S 中元素的数量 几何概型 在一个集合形状 S 中随机选取一点，求该点属于子形状 T 的概率；概率 p = T 的面积 / S 的面积 54 张牌，平均分成 6 份，求大小王在一起的概率？ 12345将 54 张牌放入 1-54 的方法数：a = 54!每份 9 张牌，大小王在一起的方法数：b = 6 * 9 * 8 * 52!大小王同在一堆的概率：9 * 8 * 52!共 6 堆概率 p = b/a = 8/53 有一对夫妇，先后生了两个孩子，其中一个孩子是女孩，问另一个孩子是男孩的概率是多大？ 12答案是2/3.两个孩子的性别有以下四种可能：（男男）（男女）（女男）（女女），其中一个是女孩，就排除了（男男），还剩三种情况。其中另一个是男孩的占了两种，2/3. 之所以答案不是1/2是因为女孩到底是第一个生的还是第二个生的是不确定的。一个国家人们只想要男孩，每个家庭都会一直要孩子，只到他们得到一个男孩。如果生的是女孩，他们就会再生一个。如果生了男孩，就不再生了。那么，这个国家里男女比例如何？ 10个人出去玩，集合时间有10分钟，每个人都在该时间内到达，概率均匀分布，彼此独立，那么最后一个人最有可能到达的时间是? 1234遇到这种想不明白，最好的方法就是枚举。若最后一个人在10分钟到达（概率1/10），其他人也都已经到达了（概率是1），总概率是 1919 * (1/10)若最后一个人在9分钟到达（概率1/10），其他人到达的概率是(9/10)9(9/10)9，总概率是 (9/10)9(9/10)9 * (1/10)依此类推。可见概率最大的是第10分钟。 随机数题目 已知一随机发生器，产生0的概率是p，产生1的概率是1-p，现在要你构造一个发生器，使得它产生0和1的概率均为1/2 12345678910111213由题目有：0 : p1 : 1-p连续产生两个数，其组合以及概率如下：00 : p201 : p*(1-p)10 : (1-p)*p11 : (1-p)2- 可以发现 01 和 10 组合的概率是相等的，只需要将其分别映射到0和1即可。即每次随机产生两个数，如果组合为00或11则丢弃，若为01则映射到1，若为10则映射到0，这样一来产生0和1的概率均为 1/2 。答案：1/4 易知，当A、B、C三点都在同一个半圆内时，三角形ABC必是直角或钝角三角形；只有当三点不在同一个半圆内，才可以组成锐角三角形。于是问题等价于“在圆周上任取三个不同的点，求它们不在同一半圆内的概率”。 过圆心任取两条直线，并在圆上任取一点作为点 A； 1接着在候选的 P1~P4 中选择 B 和 C，有四种情况：&#123;P1, P2&#125;、&#123;P1, P2&#125;, &#123;P2, P3&#125;, &#123;P3, P4&#125;。当且仅当选中 &#123;P3, P4&#125; 时，能够成锐角三角形（或者说包含圆心），概率为 1/4. 在 6*9 的方格中，以左上角为起点，以右下角为终点，每次只能向下走或者向右走，请问一共有多少种不同的走法 解法：一共走13步，其中必然有5步向下，剩下的8步向右，所以$C_{13}^{5} = 1287$ ABCDEFG 七个人排队，要求A 必须在B 的左边，但不要求一定相邻，请问共有多少种排法？第二问人如果要求A 必须在B 左边，并且相邻，请问共有多少种排法。 1第一问答案 : 7! /2 =2520 种，第二问 6! =720 种。详解看[这里](https://www.cnblogs.com/aishanyishi/p/10915291.html) 六个人排成一排，要求甲和乙不相邻，并且甲和丙不相邻的排法是多少？ 6! =720 ， 720 -240 -240 +48 =288 种。详解看这里 给出从n个数中随机选择m个数的方法。n很大，可以认为是亿级别。m可以很小，如接近1；也可以很大，如接近n。 抛一个6面的骰子，连续抛直到6为止，问期望的抛的次数是多少 设期望次数为E,那么有：[1]1次抛出6的概率为1/6，那么期望次数为1*1/6[2]本次抛出非6数字的概率为5/6，因为没有抛出6，因此期待抛出6还需要执行试验的次数仍为E，需要注意加上本次（1次）失效的抛掷，即期望次数为(1+E)(5/6) 综合可得：E = 1*(1/6) + (1+E)(5/6) 一根木棒，截成三截，组成三角形的概率是多少？画图可知，(x, y) 必须在单位正方形的左下角的半个直角三角形里，面积为 1 / 2。 然后考虑能形成三角形的截法。首先要满足刚才的三个条件： 0 &lt; x &lt; 1 0 &lt; y &lt; 1 0 &lt; 1 - x - y &lt; 1 假设一个木棒长为1，三截的长度分别为 x, y , 1-x-y，由此可以转换成线性规划问题。 x + y &gt; 1 - x - y x + 1 - x - y &gt; y y + 1 - x - y &gt; x 化简即得： 0 &lt; x &lt; 1/2 0 &lt; y &lt; 1/2 1/2 &lt; x + y &lt; 1 画图可知，此时 (x, y) 必须在边长为 1/2 的三角形的右上角的半个直角三角形里，面积为 1/8。于是最终概率为 (1/8) / (1/2) = 1/4。 有一苹果，两个人抛硬币来决定谁吃这个苹果，先抛到正面者吃。问先抛这吃到苹果的概率是多少？ 题目一看似乎答案就是 1/2，但其实认真细想并不是这么回事。 给所有的抛硬币操作从 1 开始编号，显然先手者只可能在奇数（1，3，5，7…）次抛硬币得到苹果，而后手只可能在偶数次（2，4，6，8…）抛硬币得到苹果。 设先手者得到苹果的概率为 p，第 1 次抛硬币得到苹果的概率为 p = 1/2，在第 3 次（3，5，7…）以后得到苹果的概率为 p/4（这是因为这种只有在第1次和第2次抛硬币都没有抛到正面，概率为 1/4 = 1/2 * 1/2 的时候才有可能发生，而且此时先手者此刻面临和开始相同的局面）；所以可以列出等式 p = 1/2 + p /4，p = 2/3（注意 p 表示先手者得到苹果的概率）。 一个三角形， 三个端点上有三只蚂蚁，蚂蚁可以绕任意边走，问蚂蚁不相撞的概率是多少？ 首先，每个蚂蚁在方向的选择上有且只有 2 种可能，共有 3 只蚂蚁，所以共有 2 的 3 次方种可能，而不相撞有有 2 种可能，即全为顺时针方向或全为逆时针方向。不相撞概率 = 不相撞 / 全部 = 2/8 = 1/4。]]></content>
      <categories>
        <category>CS基础</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[倒排索引实现搜索功能]]></title>
    <url>%2F2019%2F04%2F09%2Finverted_index%2F</url>
    <content type="text"><![CDATA[首先介绍了信息检索中两种匹配方式：精确匹配和最好匹配，然后介绍在实现过程中涉及到的boolean retrieval 和倒排索引，实现了基于set 的搜索引擎和boolean retrieval的算法。 精确匹配（exact match）精确匹配（exact match）的特点如下： query specifies precise retrieval criteria every document either matches or fails to match query result is a set of documents (Unordered in pure exact match) （1）优点 Can be very efficiently implemented Predictable, easy to explain （2）缺点 Difficulty increases with collection size Indexing vocabulary same as query vocabulary Acceptable precision generally means unacceptable recall （3）代码实现 基于set 实现的搜索引擎 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576import nltkfrom collections import defaultdictfrom nltk.stem.snowball import EnglishStemmerclass Index: # init 里面更多的是全局变量的定义；函数才是操作的定义 def __init__(self, tokenizer, stemmer =None, stopwords =None): self.tokenizer =tokenizer self.stemmer =stemmer # 词干提取，将词还原成 词本来的形式 self.index =defaultdict(list) self.documents = &#123;&#125; self.__unique_id =0 if not stopwords: self.stopwords =set() else: self.stopwords =set(stopwords) def lookup(self, word): word =word.lower() if self.stemmer: word =self.stemmer.stem(word) return [ self.documents.get(id, None) for id in self.index.get(word)] # add 是维护两个操作， 一个是documents 的操作，一个是 index的操作 def add(self, document): # 维护(关键词 , index列表) 的dictionary；对于每一个关键词，都需要加上当前的document 的id for token in [t.lower() for t in nltk.word_tokenize(document)]: if token in self.stopwords: continue if self.stemmer: token =self.stemmer.stem(token) # 这个不是很理解 逻辑，想要干什么 if self.__unique_id not in self.index[token]: self.index[token].append(self.__unique_id) # 下面是documents 维护，每一来一个文档，就加了进去 self.documents[self.__unique_id] =document self.__unique_id +=1index =Index(nltk.word_tokenize, EnglishStemmer(), nltk.corpus.stopwords.words('english'))#TOP10 Dire straitsindex.add('Industrial Disease')index.add('Private Investigations')index.add('So Far Away')index.add('Twisting by the Pool')index.add('Skateaway')index.add('Walk of Life')index.add('Romeo and Juliet')index.add('Tunnel of Love')index.add('Money for Nothing')index.add('Sultans of Swing')# TOP10 Led Zeppelinindex.add('Stairway To Heaven')index.add('Kashmir')index.add('Achilles Last Stand')index.add('Whole Lotta Love')index.add('Immigrant Song')index.add('Black Dog')index.add('When The Levee Breaks')index.add('Since I\'ve Been Lovin\' You')index.add('Since I\'ve Been Loving You')index.add('Over the Hills and Far Away')index.add('Dazed and Confused')# Let's make some queries:print( index.lookup('loves'))# ['Tunnel of Love', 'Whole Lotta Love', "Since I've Been Loving You"]print( index.lookup('loved'))# ['Tunnel of Love', 'Whole Lotta Love', "Since I've Been Loving You"]print (index.lookup('daze'))# ['Dazed and Confused']print (index.lookup('confusion'))# ['Dazed and Confused'] python 中set 操作是两两相交进行的，所以执行顺序会对操作时间造成很大的影响。尽可能使得前面的结果产生的结果小，这样之后的结果只会更小。1234timeit set.intersection(*(inverted_index[c] for c in "aje"))1000 loops, best of 3: 366 µs per looptimeit set.intersection(*(inverted_index[c] for c in "aej"))10 loops, best of 3: 23.1 ms per loop 所以，如果是基于set 结构实现的，那么优化的原则是：每一次操作的中间结果越小越好。最开始选择最小的，可以先遍历一遍所有的query 命中的doc的数量，然后选择命中数量最少的进行set 操作。 最好匹配（best match） Best-match or ranking models are now more commonBoolean or structured queries can be part of a best-match retrieval model 最好匹配（best match）的特点 Query describes good or “best” matching document Every document matches query to some degree Result is ranked list of documents （1）优点 Significantly more effective than exact match Similar efficiency (based on inverted file implementations) Easier to use (supports full text queries) （2）缺点 More difficult to convey an appropriate cognitive model (“control”) Full text does not mean natural language understanding (no “magic”) （3）Boolean Retrieval Many users prefer Boolean. On Google, the default interpretation of a query $[w_1, w_2, \dots , w_n ]$ is $w_1$ AND $w_2$ AND … AND $w_n$。如果没有得到你想要的结果（在Google 上，不是百度），那么可能是以下原因： 搜索结果中含有 variant of $w_i$ 很长的query 以至于无法解析 bool expression generates very few hits Simple Boolean vs. Ranking of result set Simple Boolean retrieval returns matching documents in no particular order Google (and most well designed Boolean engines) rank the result set - they rank good hits higher than bad hits Boolean Retrieval伪代码如下： ` 上面的问题是两个有序表的合并算法，使用双指针，时间复杂度是$O(m +n)$。使用c语言实现。 12345678910111213141516// p1,p2 是指向两个posting 链的指针Posting intersect(p1, p2)&#123; Posting answer; while(p1 != null &amp;&amp; p2!= null) &#123; if(p1 ==p2) &#123; add(answer, p1-&gt;docID); p1 =p1-&gt;next; p2 =p2-&gt;next; &#125; else if (p1 &gt;p2) p2 =p2 -&gt;next; else p1 =p1-&gt;next; &#125;&#125; （4）Inverted Index In computer science, an inverted index (also referred to as a postings file or inverted file) is a database index storing a mapping from content, such as words or numbers, to its locations in a table, or in a document or a set of documents (named in contrast to a forward index, which maps from documents to content). 倒排索引是一种数据库索引方式，存储了从 内容（关键字或者数字）到其位置的映射关系。该种索引方式和正排索引相反。该种数据结构是典型的搜索引擎检索算法重要的部分。 索引构建过程 1). 词条序列： 从文档文件到 （关键字，DocID）的转换2). 按照词项排序：每个关键词按照DocID 排序3). 词典和倒排记录表： 某关键词在单片文档中出现就会被合并 拆分为词典和倒排记录表两部分 每个关键词出现的文档数目（docFrequency, DocIDs） 被加入 ` 所以说如果以关键词 BRUTUS 和CALPURNIA 得到记录表，那么如何得到最后的包含两者的文档列表呢？这里就用到了上面所说的Bool Retrieval。 ` 第三种方式将精确匹配和最好匹配结合起来。 项目中出现的一些好的代码，总结整理。 1234567891011121314151617181920212223242526272829303132333435363738def assert_dir(path): if not os.path.exists(path): print("ERROR : &#123;&#125; does not exists".format(path)) sys.exit(1) if not os.path.isdir(path): print("Error: &#123;&#125; is not a directory".format(path)) sys.exit(1)def preprocess_text(text): processed_text =text.lower() non_words =re.compile(r"[^A-Za-z]+") processed_text =re.sub(non_words, ' ', processed_text) for doc_file in os.listdir(doc_path): filename =os.path.join(doc_path, doc_file) text = get_text_from_file(filename) return processed_textdef writeDown(docs_path, data_path): with open(doc_path, mode ='w') as f: for word in inverted_index.keys(): f.write(word +'\n') # python 中序列化数据存储（二进制） # pickle模块只能在python中使用，python中几乎所有的数据类型（列表，字典，集合，类等）都可以用pickle来序列化， with open(inverted_index_file, mode ='wb') as f: pickle.dump(inverted_index, f)result =Nonefor word in words: if result is None: result =inverted_index.get(word) else: result.intersection_update(inverted_index.get(word))x =&#123;'a', 'b', 'c'&#125;y =&#123;'c', 'd', 'e'&#125;z= &#123;'f', 'g', 'c'&#125;x.intersection_update(y, z) # output: c 有时候需要进行多个文件的合并或者拆分，此时需要打开多个文件。python 中有种简洁的写法 12345with open(file1) as f1, open(file2) as f2, open(file3) as f3: for i in f1: j =f2.readline() k =f3.readline() print(i, j, k) 或者还可以写成123456from contextlib import nestedwith nested(open(file1), open(file2), open(file3)) as (f1, f2,f3): for i in f1: j =f2.readline() k =f3.readline() pirnt(i, j, k) 参考文献 Inverted indexIntroduction to Information Retrieval]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>inverted_index</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[对抗生成网络简介]]></title>
    <url>%2F2019%2F04%2F09%2Fgan%2F</url>
    <content type="text"><![CDATA[从损失函数角度分析GAN和WGAN等经典的网络模型；分析GAN评价的两个指标Inception Score 和Frechet Inception Distance（FID）。 损失函数角度分析GAN模型对抗生成网络是尝试使用无监督的方式学习原数据的概率分布。在GAN的训练过程中，使用损失函数反映真实图像分布和生成图像分布之间的差异。本文主要介绍两种GAN损失函数： 最大最小化损失函数（minmax loss） Wasserstein损失函数（Wasserstein loss） 除了本文介绍的两个损失函数，还有许多其他不同的损失函数。 GAN的训练过程中有两个损失函数：一个作为网络$G$的损失函数，一个作为网络$D$的损失函数。问题是，如果使用两个损失函数反映真实图像分布和生成图像分布之间的差异呢？ （1） 最大最小化损失函数 \begin{equation}\label{eq1}\min_G \max_D V(D,G) =E_{x \sim p_{data}(x)}[ \log D(x)] + E_{z \sim p_z (z)}[log (1- D(G(z)))] \tag{1}\end{equation} 其中，$D(x)$ 是$D$网络对真实样本$x$ 预测为真的概率估计， $E_x$是所有的样本的期望值，$G(z)$ 是网络$G$的生成结果， $D(G(z))$是$D$网络对生成样本判别为真的概率估计， $E_z$ 是对所有生成样本判别为真的概率估计。 $E_{x \sim p_{data}(x)}[ \log D(x)]$ 表示网络$D$判别真实的样本是真，$D(G(z))$表示网络$D$判别网路$G$生成的样本为真，$E_{z \sim p_z (z)}[log (1- D(G(z)))]$表示网络$D$判别网络$G$生成的样本为假。在网络$D$训练的过程中，需要最大化两个子式子；在网络$G$训练的过程中是需要最小化右边的式子，即$ E_{z \sim p_z (z)}[log (1- D(G(z)))]$。上述的计算都需要进行log运算。$D$网络承担的是分类器的功能，网络的输出是在$[0, 1]$之间，一般来说以0.5为界限，如果大于0.5，那么$D$网络认为该样本是真实的样本，否则是虚假的样本。 该损失函数来源于交叉熵。如果两个分布$p$ 和$q$，那么两者之间的交叉熵为：$$H(p, q) =-\sum_{i} p_{i} \log q_{i}$$其中$p$ 和$q$分别是真实和预测的分布。两者都是离散的分布，如果是连续的话，那么应该使用积分符号而非求和符合。最初GAN的设计是一个二分类，判别图像是真实的$1$还是虚假的$0$。GAN的训练目标可以表示为以下形式： $$H((x_{1}, y_{1}), D)=-y_{1} \log D(x_{1})-(1-y_{1}) \log (1-D(x_{1}))$$ 对于真实样本$x$，网络$D$可以映射到 $[0,1]$区间，对于生成样本的判别，需要转化一下 $1-D(z)$，所以输出就约束于$[0, 1]$。上述是一个样本，然后将所有的样本加起来求期望得 $$H((x _{i}, y _{i}) _{i=1}^{N}, D)=-\sum _{i=1}^{N} y _{i} \log D(x _{i})-\sum _{i=1}^{N}(1-y _{i}) \log (1-D(x _{i}))$$ 输入样本到$D$网络中的样本一般是来自真实数据集，一半是来自$G$网络，并且把求和符号转换成期望，写成如下的形式： $$H((x _{i}, y _{i}) _{i=1}^{\infty}, D)=-\frac{1}{2} E _{x \sim p _{\text {data }}}[\log D(x)]-\frac{1}{2} E _{z}[\log (1-D(G(z)))]$$是不是和公式$(1)$很类似。 （2）修正后的最大最小化损失函数 最初的论文发现，对抗生成网路训练的开始几个阶段，最大最小化损失函数容易停止不动，这可能是$D$网络判别太容易。所以论文中修改原来损失函数中的$G$网络为最大化$ \log D(G(z))$。 （3）Wasserstein损失函数 在Wasserstein GAN（WGAN）中网络$D$不再承担分类样本的功能。这里的$D$只要能使得真实的样本的输出大于$G$生成样本就可以。所以这里的额$D$网路更像是一种”critic”的角色而不是一个”discriminator”的角色。损失函数就变得异常简单 Critic Loss: $D(x) - D(G(z))$ 在训练过程中，$D$网络去最大化上述式子。也就是说，$D$网络努力最大化真实样本输出和生成样本输出之间的差距。 Generator Loss: $D(G(z))$ 训练过程中，$G$网络最大化上述式子。也就是说，$G$网络的输出要被$D$网络认为是“真”。$D$网络的输出不必约束于$[0, 1]$之间。 两个评价指标两个指标的计算都是基于Inception network预训练模型，然后得到某一层的信息，最后进行计算。Inception Score是基于最后一层softmax层，Frechet Inception Distance是基于中间的某个特征层（v3 pool3）得到的特征向量。 （1）Inception Score 在最后softmax层中计算两个分布之间的距离\begin{equation} IS=exp[E_xD_{KL}(p(y|x)||p(y))]\end{equation} 原作者认为$p(y|x)$ 是图像真实度的衡量，$p(y)$是由前者积分得到， 是图像多样性的衡量。所以IS是计算的$p(y|x)$和$p(y)$两个分布之间的KL 散度。 123456789101112131415161718import mathimport torchimport torch.nn.functional as Ffrom torch.autograd import Variablefrom torchvision.models import inception_v3net = inception_v3(pretrained=True).cuda()def inception_score(images, batch_size=5): scores = [] for i in range(int(math.ceil(float(len(images)) / float(batch_size)))): batch = Variable(torch.cat(images[i * batch_size: (i + 1) * batch_size], 0)) s, _ = net(batch) # skipping aux logits scores.append(s) p_yx = F.softmax(torch.cat(scores, 0), 1) p_y = p_yx.mean(0).unsqueeze(0).expand(p_yx.size(0), -1) KL_d = p_yx * (torch.log(p_yx) - torch.log(p_y)) final_score = KL_d.mean() return final_score 使用Inception Score进行评价的时候，只需要输入一个生成图像路径即可。 （2）Frechet Inception Distance 在特征维度计算两个分布的距离 \begin{equation}{d}^{2}((m, C),(m_{w}, C_{w}))=|m-m_{2}|+{Tr}(C+C_{w}-2(C C_{w})^{1 / 2})\end{equation} 其中$m$ 和$m_w$表示真实图像和生成图像在特征空间的均值，$C$和$C_w$表示真实图像和生成图像在特征空间向量的协方差矩阵中的协方差（在二维中就是方差）。$Tr$表示矩阵中的迹（主对角线元素之和）。 基于numpy的代码实现。两个正太分布之间的距离。 1234567891011# calculate frechet inception distancedef calculate_fid(act1, act2): # calculate mean and covariance statistics mu1, sigma1 = act1.mean(axis=0), cov(act1, rowvar=False) mu2, sigma2 = act2.mean(axis=0), cov(act2, rowvar=False) ssdiff = numpy.sum((mu1 - mu2)**2.0) covmean = sqrtm(sigma1.dot(sigma2)) if iscomplexobj(covmean): covmean = covmean.real fid = ssdiff + trace(sigma1 + sigma2 - 2.0 * covmean) return fid 使用FID进行GAN的评价时候，需要给定两个路径分别指向真实的图像和生成的图像。 参考文献Understanding Generative Adversarial NetworksLoss Functions]]></content>
      <categories>
        <category>Deep learning</category>
      </categories>
      <tags>
        <tag>GANs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[The Evaluation of Sentence Similarity]]></title>
    <url>%2F2019%2F04%2F06%2FThe-evaluation-of-sentence-similarity%2F</url>
    <content type="text"><![CDATA[I am trying to write my first english blog based on two reasons: First, the data set used in this blog is english; Second, I’d like to expand my reach and attract more audiences, although I should admit that nobody cares. DataInitially I want to use chinese corpus, but I cannot find a proper one. The data should sound like this one: word1 word2 similarity score阿拉伯人 阿拉伯 7.2畜产 农业 5.6垂涎 崇敬 3.4次序 秩序 4.7定心丸 药品 4.3房租 价格 5.2翡翠 宝石 6.7高科技 技术 7.5购入 购买 8.5观音 菩萨 8.2归并 合并 7.7 not like this: 为何我无法申请开通花呗信用卡收款 支付宝开通信用卡花呗收款不符合条件怎么回事 1花呗分期付款会影响使用吗 花呗分期有什么影响吗 0为什么我花呗没有临时额度 花呗没有临时额度怎么可以负 0能不能开花呗老兄 花呗逾期了还能开通 0我的怎么开通花呗收钱 这个花呗是个什么啥？我没开通 我怎么有账单 0蚂蚁借呗可以停掉么 蚂蚁借呗为什么给我关掉了 0我想把花呗功能关了 我去饭店吃饭，能用花呗支付吗 0为什么我借呗开通了又关闭了 为什么借呗存在风险 0支付宝被冻了花呗要怎么还 支付功能冻结了，花呗还不了怎么办 1 If you can find the dataset where ‘similarity score’ is double, please donot hesitate to email me. So, the choice has to be enlgish corpus. The dataset used in this experiment are STSbenchmark and SICK data. The SICK data contains 10,000 sentence paris labeled with semantic relatedness and entailment relation. Similarity MethodsBaselineAs the baseline, we just take the embedding of the words in sentence, and compute the average, weighted by frequency of each word. 1234567891011121314151617181920212223242526272829303132def run_avg_benchmark(sentences1, sentences2, model=None, use_stoplist=False, doc_freqs=None): if doc_freqs is not None: N = doc_freqs["NUM_DOCS"] sims = [] for (sent1, sent2) in zip(sentences1, sentences2): tokens1 = sent1.tokens_without_stop if use_stoplist else sent1.tokens tokens2 = sent2.tokens_without_stop if use_stoplist else sent2.tokens tokens1 = [token for token in tokens1 if token in model] tokens2 = [token for token in tokens2 if token in model]l if len(tokens1) == 0 or len(tokens2) == 0: sims.append(0) continue tokfreqs1 = Counter(tokens1) tokfreqs2 = Counter(tokens2) weights1 = [tokfreqs1[token] * math.log(N / (doc_freqs.get(token, 0) + 1)) for token in tokfreqs1] if doc_freqs else None weights2 = [tokfreqs2[token] * math.log(N / (doc_freqs.get(token, 0) + 1)) for token in tokfreqs2] if doc_freqs else None embedding1 = np.average([model[token] for token in tokfreqs1], axis=0, weights=weights1).reshape(1, -1) embedding2 = np.average([model[token] for token in tokfreqs2], axis=0, weights=weights2).reshape(1, -1) sim = cosine_similarity(embedding1, embedding2)[0][0] sims.append(sim) return sims Smooth Inverse FrequencyThe baseline, like we did before, is very simple and crude of computing sentence embedding. Word frequency cannot reliably reflect its importance to sentence, semantically speaking. Smooth Inverse Frequency (SIF) tries to solve this problem. SIF is very similar to the weighted average we used before, with the difference that it’s weighted by this formular.$$\operatorname { SIF } ( w ) = \frac { a } { ( a + p ( w ) )}$$where $a$ is a hyper-parameter (set to 0.001 by default) and $ p(w)$ is the estimated word frequency in the corpus. (这个权重和 TF或者 IDF 都是不相同的) we need to perform common component removal: subtract from the sentence embedding obtained above the first principal component of the matrix. This corrects for the influence of high-frequency words that have syntactic or dicourse function, such as ‘but’, ‘and’, etc. You can find more information from this paper. 因为这个的输入直接是句子，没有经过分词的处理，所以不免有 but and 这类的词汇出现。 12345678910111213141516171819202122232425262728293031323334353637def remove_first_principal_component(X): svd = TruncatedSVD(n_components=1, n_iter=7, random_state=0) svd.fit(X) pc = svd.components_ XX = X - X.dot(pc.transpose()) * pc return XXdef run_sif_benchmark(sentences1, sentences2, model, freqs=&#123;&#125;, use_stoplist=False, a=0.001): total_freq = sum(freqs.values()) embeddings = [] # SIF requires us to first collect all sentence embeddings and then perform # common component analysis. for (sent1, sent2) in zip(sentences1, sentences2): tokens1 = sent1.tokens_without_stop if use_stoplist else sent1.tokens tokens2 = sent2.tokens_without_stop if use_stoplist else sent2.tokens tokens1 = [token for token in tokens1 if token in model] tokens2 = [token for token in tokens2 if token in model] weights1 = [a / (a + freqs.get(token, 0) / total_freq) for token in tokens1] weights2 = [a / (a + freqs.get(token, 0) / total_freq) for token in tokens2] embedding1 = np.average([model[token] for token in tokens1], axis=0, weights=weights1) embedding2 = np.average([model[token] for token in tokens2], axis=0, weights=weights2) embeddings.append(embedding1) embeddings.append(embedding2) embeddings = remove_first_principal_component(np.array(embeddings)) sims = [cosine_similarity(embeddings[idx * 2].reshape(1, -1), embeddings[idx * 2 + 1].reshape(1, -1))[0][0] for idx in range(int(len(embeddings) / 2))] return sims Google Sentence EncoderInferSent is a pre-trained encoder that produces sentence embedding, which opensourced by Facebook. The Google Sentence Encoder is Google’s answer to Facebook’s InferSent. In contrast to InferSent, the Google Sentence Encoder was trained on a combination of unsupervised data and supervised data (SNLI corpus), which tends to give better results. The codes can be used in Google Jupyter Notebook 12345678910111213141516171819202122232425import tensorflow_hub as hubtf.logging.set_verbosity(tf.logging.ERROR)embed = hub.Module("https://tfhub.dev/google/universal-sentence-encoder/1")def run_gse_benchmark(sentences1, sentences2): sts_input1 = tf.placeholder(tf.string, shape=(None)) sts_input2 = tf.placeholder(tf.string, shape=(None)) sts_encode1 = tf.nn.l2_normalize(embed(sts_input1)) sts_encode2 = tf.nn.l2_normalize(embed(sts_input2)) sim_scores = tf.reduce_sum(tf.multiply(sts_encode1, sts_encode2), axis=1) with tf.Session() as session: session.run(tf.global_variables_initializer()) session.run(tf.tables_initializer()) [gse_sims] = session.run( [sim_scores], feed_dict=&#123; sts_input1: [sent1.raw for sent1 in sentences1], sts_input2: [sent2.raw for sent2 in sentences2] &#125;) return gse_sims Experiments1234567891011121314def run_experiment(df, benchmarks): sentences1 = [Sentence(s) for s in df['sent_1']] sentences2 = [Sentence(s) for s in df['sent_2']] pearson_cors, spearman_cors = [], [] for label, method in benchmarks: sims = method(sentences1, sentences2) pearson_correlation = scipy.stats.pearsonr(sims, df['sim'])[0] print(label, pearson_correlation) pearson_cors.append(pearson_correlation) spearman_correlation = scipy.stats.spearmanr(sims, df['sim'])[0] spearman_cors.append(spearman_correlation) return pearson_cors, spearman_cors Helper function: 1234567891011import functools as ftbenchmarks = [ ("AVG-GLOVE", ft.partial(run_avg_benchmark, model=glove, use_stoplist=False)), ("AVG-GLOVE-STOP", ft.partial(run_avg_benchmark, model=glove, use_stoplist=True)), ("AVG-GLOVE-TFIDF", ft.partial(run_avg_benchmark, model=glove, use_stoplist=False, doc_freqs=doc_frequencies)), ("AVG-GLOVE-TFIDF-STOP", ft.partial(run_avg_benchmark, model=glove, use_stoplist=True, doc_freqs=doc_frequencies)), ("SIF-W2V", ft.partial(run_sif_benchmark, freqs=frequencies, model=word2vec, use_stoplist=False)), ("SIF-GLOVE", ft.partial(run_sif_benchmark, freqs=frequencies, model=glove, use_stoplist=False)),] Results123import matplotlib.pyplot as pltplt.rcParams['figure.figsize'] = (20,13)spearman[['AVG-GLOVE', 'AVG-GLOVE-STOP','AVG-GLOVE-TFIDF', 'AVG-GLOVE-TFIDF-STOP','GSE']].plot(kind="bar").legend(loc="lower left") Take Off Smooth Inverse Frequency methods are better than baseline, no matter with word2vec or Glove embeddings. Google Sentence Encoder has the similar performance as Smooth Inverse Frequency. Using tf-idf weights does not help and using a stoplist looks like a reasonable choice. Pearson CorrelationSpearman Correlation Full codes can be found in here. 复习笔记 TF-IDF 和 SIF三者的差别 SIF的计算公式：$$\operatorname { SIF } ( w ) = \frac { a } { ( a + p ( w ) )}$$$a$ 是超参数，一般设置为0.001，保证…; $p(w)$ 是word 在预料中出现的频数。 TF 的计算公式： $$ 词频(TF) = 某个词在文章中出现的次数( 频数) $$ 可以进一步标准化（减少文章长度的影响） $$ 词频( TF) = \frac{某次在文中出现的次数}{文章的总词语数} $$ $$ 逆文档频率 (IDF) = log(\frac{语料中的文档总数}{ 包含该词的文档数 +1}) $$]]></content>
      <categories>
        <category>NLP</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Data Structure]]></title>
    <url>%2F2019%2F04%2F05%2Fdata_structure%2F</url>
    <content type="text"><![CDATA[介绍面试过程中的数据结构，树、Hash表和图。 树树中常见的概念 节点的度：指的是结点拥有的子树的个数，二叉树的度不大于2 高度：叶子节点的高度是1， 根结点的高度最高 节点的层次：从根结点开始，根结点为第一层，根结点的叶子节点为第二层，以此类推。 三种树的比较 full binary tree vs. complete binary tree vs. perfect binary tree 中文翻译的时候常常容易翻译不准，所以使用英文更加容易说清楚。 Full Binary Tree A Binary Tree is full if every node has 0 or 2 children. Following are examples of a full binary tree. We can also say a full binary tree is a binary tree in which all nodes except leaves have two children.full binary tree 限制每个节点要么是有两个孩子好么是没有孩子。 Complete Binary Tree: A Binary Tree is complete Binary Tree if all levels are completely filled except possibly the last level and the last level has all keys as left as possiblecomplete binary tree限制最后一层只能是在最后一层的左边有左子树。 Perfect Binary Tree A Binary tree is Perfect Binary Tree in which all internal nodes have two children and all leaves are at the same level.perfect binary tree 最后一层一定是满的。 满二叉树 一棵深度为 $k$ 且有 $2^{k+1} -1$个结点的二叉树被称为完美二叉树。 特点： 每一层上的结点都是最大结点数（每层都是满） 数学表达为，$2^{k-1}$ 叶子结点全部都是在最底层 最后一层是 2^{k-1}， 总的结点个数是 $2^{k+1} -1$ 对满二叉树结点位置进行编号（编号的定义对于完全二叉树的理解有重要的意义） 从根节点开始，自上而下，自左而右 每一结点位置上都有元素 完全二叉树 深度为 $k$ 的具有 $n$ 个结点的二叉树，当且仅当其每一个结点都与深度为 $k$ 的满二叉树中编号为 $1 \sim n$ 的结点一一对应，称之为完全二叉树。（满二叉树在实际中不是那么常见，但是完全二叉树就比较常见，因为没有约束最后一层必须是满的，但是存在的结点却保留这满二叉树的性质，存在的结点都是和满二叉树一一对应的） 大根堆和小根堆 特点： 堆是一个完全二叉树( 如果有 h 层，那么 1- h-1 层都是满的，在h 层缺失的若干个 右叶子) 小根堆的根节点的值是最小值，大根堆的根节点是最大值 堆的结构适合采用顺序存储 堆的存储结构一般都用数组来表示堆，$i $结点的父结点下标就为 $(i–1)/2 $。它的左右子结点下标分别为 $2 i + 1 $和 $2 i + 2$。如第0个结点左右子结点下标分别为1和2。在二叉排序树中，某结点的右孩子结点的值一定大于该结点的左孩子结点的值；在堆中却不一定，堆只是限定了某结点的值大于（或小于）其左右孩子结点的值，但没有限定左右孩子结点之间的大小关系。 堆的结构和完全二叉树的区别：堆是一个完全二叉树，并且每个结点的值都大于或等于其左右孩子结点的值（这里的讨论以大根堆为例），所以，堆是结点之间满足一定次序关系的完全二叉树。具有n个结点的堆，其深度即为堆所对应的完全二叉树的深度$logn $。 接下来的树结构都是一种高效的查找结构。 二叉搜索树（二叉排序树） 定义： 是空树或者满足以下的性质 特点： 若左子树不为空，那么左子树上所有结点的值小于（不大于）根结点的值 若右子树不为空，那么右子树上所有结点的值大于（不小于）结点的值 查找、插入、删除操作的最坏时间复杂度 二叉查找树 平衡二叉树 红黑树 查找 $O(N)$ $O(logn)$ $O(logn)$ 插入 $O(N)$ $O(logn)$ $O(logn)$ 删除 $O(N)$ $O(logn)$ $O(logn)$ 查找、插入、删除操作的平均时间复杂度 二叉查找树 平衡二叉树 红黑树 查找 $O(logn)$ $O(logn)$ $O(logn)$ 插入 $O(logn)$ $O(logn)$ $O(logn)$ 删除 $O(logn)$ $O(logn)$ $O(logn)$ 二叉查找树出现 $O(N)$ 的时间复杂度在于树形结构变成了单链表的形式。 平衡树和红黑树的区别：平衡二叉树的插入/删除操作带来的旋转操作可能会达到logn次，而红黑树的插入/删除操作带来的旋转操作最多为2 or 3次。 平衡二叉树（AVL树，俄国两个人名的首写字母） 可以介绍一个概念：平衡因子。该节点左子树的高度- 该节点的右子树高度，即左右子树高度之差，被称为平衡因子。 平衡二叉树的特点 在AVL树中任意节点的两个子树的高度最大差为1. 查找、删除、插入的平均和最坏情况下都是$O(N)$ 使用左旋（left rotation）和右旋（right rotation）来调整树的平衡。 如果在AVL树中进行插入或删除节点，可能导致AVL树失去平衡，这种失去平衡的二叉树可以概括为四种姿态：LL（左左）、RR（右右）、LR（左右）、RL（右左）。它们的示意图如下： 红黑树 是平衡二叉树的一种，被称为自平衡二叉树。红黑树的每个节点上都存储着表示节点的颜色，可以是红色或者是黑色，所以称为红黑树。相对于 avl 来说，红黑树并不是完整符合平衡条件（任意两个子树的高度最大差为1），因此查找性能从理论上稍微不如 AVL 树。但是插入和删除的效率更高。 红黑树的特性： 节点是红色或者黑色 根结点是黑色 每个叶节点（包括空节点）是黑色 每个红色节点的两个子节点都是黑色（从每个叶子到根的所有路径上不能有两个连续的红色节点） 从任一节点到其每个叶子节点的所有路径都包含相同数目的黑色节点 自平衡策略 对于一棵红黑树的操作最基本的无外乎增删改查，其中查和改都不会改变树的结构，所以与普通平衡二叉树操作无异。剩下的就是增删操作，插入和删除都会破坏树的结构，不过借助一定的平衡策略能够让树重新满足定义。平衡策略可以简单概括为三种：左旋转、右旋转，以及 变色。在插入或删除结点之后，只要我们沿着结点到根的路径上执行这三种操作，就可以最终让树重新满足定义。 三种基本操作： 左旋转对于当前结点而言，如果右子结点为红色，左子结点为黑色，则执行左旋转，如下图： 右旋转对于当前结点而言，如果左子、左孙子结点均为红色，则执行右旋转，如下图： 变色 场景：搜索 B 树 B 树概括来说是一个更加一般化的二叉搜索树，即一个节点可以拥有2 个以上的子节点。这种树结构又被称为平衡多路（即不止两个子树）查找树。 首先介绍两个简单类似的树形结构。下面是 2-3 树。其中的2，3表示的可以有2个或者3 个子树。（这种就是一种多路平衡树） 然后是2-3-4 树。 上面的两种可以看做是B 树的特例。B 树中最重要的特性是下面图片中的第三点：除根节点外的非叶子结点都至少有 $\left \lceil m/2 \right \rceil $棵子树。这个性质是控制着要不要进一步的分裂。第五个性质所有的叶子节点出现在同一层次上，不带信息。（参考上面的2-3树） B 树最重要的是查找，相当于是二叉排序树的扩展。 使用场景：适合于读写相对大的数据块的存储系统，比如硬盘， 常常被用在数据块和文件系统中。 哈夫曼树（最优二叉树） 这个树和上面有点差别。如何根据节点不同的查找频率构造更加有效的搜索树？ 总的原则：查询频率高的路径是比较短，查询频率低的路径比较长。最后的平均查找方式最低。 这里引入了一个概念：带权值路径长度（WPL）：设二叉树有 $n$ 个叶子节点，每个叶子节点带有权值 $w_k$， 从根结点到每个叶子节点的长度为$l_k$，则每个叶子节点的带权路径长度纸盒就是 $ WPL = \sum{w_kl_k}$ 构造方式：每次把权值最小的两棵树合并。从这种构造方式中可以得到以下的特点： 没有度为1 的节点 $n$ 个叶子节点的哈夫曼树总共有 $2n-1$ 个结点 对同一组权值 ${w_1, w_2, w_3… , w_n}$， 存在着不同构的两棵哈夫曼树 最小生成树算法 生成树的特点： 没有环 连接所有的顶点 $N $个顶点， $N-1$ 边 (这种数量关系是很重要的) 还有一个要求是边的权重相加是最小。 这里介绍两种算法，kruskal 算法和prim算法。都是贪婪思想。方式不同。 Kruskal 算法 （克鲁斯卡尔） 此算法可以称为“加边法”，初始最小生成树边数为0，每迭代一次就选择一条满足条件的最小代价边，加入到最小生成树的边集合里。 把图中的所有边按代价从小到大排序； 把图中的n个顶点看成独立的n棵树组成的森林； 按权值从小到大选择边，所选的边连接的两个顶点$u_i$, $v_i$应属于两颗不同的树（这个是防止生成环的条件），则成为最小生成树的一条边，并将这两颗树合并作为一颗树。 重复(3) ，直到所有顶点都在一棵树内或者有 n-1 条边为止 （其中的判断条件 所选的边属于不同的树，也可以理解为选择的边不能构成环） Prim 算法（普里姆算法） 此算法可以称为“加点法”，每次迭代选择代价最小的边对应的点，加入到最小生成树中。算法从某一个顶点s开始，逐渐长大覆盖整个连通网的所有顶点。 图的所有顶点集合为 $V$；初始令集合 $u={s} $,$v=V−u$; 在两个集合$u$, $v$能够组成的边中，选择一条代价最小的边$(u_0,v_0)$，并把$v_0$并入到集合 $u$中。 重复上述步骤，直到最小生成树有 $n-1$条边或者 $n$个顶点为止。 在prim 算法中，没有判断是否为环的过程，跳出的条件是第一个列表全部已选，那么就形成了一个最小生成树。 在实现的时候，需要维持三个列表：顶点是否选择列表，顶点之间的最小距离列表和顶点之间的信息列表。 应用场景： 考虑城市之间间隔的距离，建设通信线路的难度等各种因素，将这些因素综合成一个数值表示，然后可以计算最小的建设成本。 写作参考 看视频理解 完全二叉树中叶子节点数量的计算 解法一：叶子节点为 n/2 （向上取整），其中n 个是节点的总个数解法二：节点总数=n0+n1+n2对于任意一个为空的子树 n0 =n2+1当节点总数为偶数时候，n1 为1；当结点总数为奇数，n1 为0.所以连方程组，求解n2 和n0 Trie 树 Trie树，又叫字典树、前缀树（Prefix Tree）、单词查找树 或 键树，是一种多叉树结构。如下图： a. 根节点不包含字符，除根节点外每一个节点都只包含一个字符b. 从根节点到某一节点，路径上经过的字符连接起来，为该节点对应的字符串c. 每个节点的所有子节点包含的字符都不相同 可以看出，Trie树的关键字一般都是字符串，而且Trie树把每个关键字保存在一条路径上，而不是一个结点中。另外，两个有公共前缀的关键字，在Trie树中前缀部分的路径相同，所以Trie树又叫做前缀树（Prefix Tree） Trie树的核心思想是空间换时间，利用字符串的公共前缀来减少无谓的字符串比较以达到提高查询效率的目的。 优点 插入和查询的效率很高，都为O(m)O(m)，其中 mm 是待插入/查询的字符串的长度。（关于查询，会有人说 hash 表时间复杂度是O(1)O(1)不是更快？但是，哈希搜索的效率通常取决于 hash 函数的好坏，若一个坏的 hash 函数导致很多的冲突，效率并不一定比Trie树高。） Trie树只有在允许一个关键字关联多个值的情况下才有类似hash碰撞发生。 Trie树可以对关键字按字典序排序。 缺点 当 hash 函数很好时，Trie树的查找效率会低于哈希搜索。 空间消耗比较大。 应用场景 a. 前缀匹配例如：找出一个字符串集合中所有以 “五分钟” 开头的字符串。我们只需要用所有字符串构造一个 trie树，然后输出以 五−&gt;分−&gt;钟 开头的路径上的关键字即可。trie树前缀匹配常用于搜索提示。如当输入一个网址，可以自动搜索出可能的选择。当没有完全匹配的搜索结果，可以返回前缀最相似的可能 b. 字符串检索给出 N 个单词组成的熟词表，以及一篇全用小写英文书写的文章，按最早出现的顺序写出所有不在熟词表中的生词。检索/查询功能是Trie树最原始的功能。给定一组字符串，查找某个字符串是否出现过，思路就是从根节点开始一个一个字符进行比较： 实现 可以参考这里 看动画轻松理解「Trie树」 Hash 表一个hash 函数 $f(x) $ hash 函数 单向算法（不能从f(x) 得到x, 只能是从x 得到 f(x)） 唯一性 输出长度固定 那么第三条会出现hash 碰撞。常用的处理方法： 开放地址法（总的原则是在冲突的位置查找下一个位置，可以使用线性探查法，平方探查法等方式） 再hash 法（同时使用多个hash 函数，如果第一个hash 函数发生冲突，那么接着使用第二个hash 函数） 链地址法（将同一个hash 值相同的元素使用一个单链表进行存储） 图连通图 对于无向图 $G$而言，若 $V(G)$中任一两个不同的顶点 $V_i$ 和$V_j$都连通（即有路径），则称$G$为连通图。 对于有向图$G$中， 如果对于两个顶点 $V_i$和$V_j$有一条从 $V_i$到 $V_j$ 的有向路径，同时有一条从 $V_j$ 到$V_i$的有向路径，那么称这两个顶点强连通。如果有向图中两个顶点都强连通，那么称这个图为强连通图。 连通分量 在无向图中，连通分量即为连通子图。 上图中，总共有四个连通分量。顶点A、B、C、D构成了一个连通分量，顶点E构成了一个连通分量，顶点F，G和H，I分别构成了两个连通分量。 强连通分量：在有向图中，尽可能多的若干顶点组成的子图中，这些顶点都是相互可到达的，则这些顶点称为一个强连通分量。 上图中有三个强连通分量，分别是a、b、e以及f、g和c、d、h。 并查集：判断图上是否存在环。可以参考之前的一个博客。 KMP 算法next数组的含义：以 i 为终点的后缀 和以1 为起点的前缀相等，并且满足长度最长，那么这个就是 next[i] 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950#include&lt;iostream&gt;using namespace std;/** s 表示source， p表示pattern，n和m 分别是对应的长度 get_next是计算了 next数组， 主程序中写了匹配的模板（逻辑结构） */const int N =1e6+10;int n, m;char s[N], p[N];int nex[N];// 因为存储和计算的时候都是从 1开始的，所以 j+1 才是真正有意义比较项，这个模板其实很好记的。void get_next()&#123; for(int i =2, j =0; i&lt;=m; i++) &#123; while(j &amp;&amp; p[i] !=p[j+1]) j=nex[j]; if(p[i] ==p[j+1]) j++; nex[i] =j; &#125; &#125;// abcd, abcdd// ddint main()&#123; cin&gt;&gt; n&gt;&gt;m; scanf("%s", s+1); scanf("%s", p+1); get_next(); //匹配 //for(int i =1; i&lt;=m; i++) cout&lt;&lt; nex[i] &lt;&lt;" "; for(int i =1, j =0; i&lt;=n; i++) &#123; while(j &amp;&amp; s[i] != p[j+1]) j =nex[j]; if(s[i] ==p[j+1]) j++; if(j ==m) &#123; j =nex[j]; cout&lt;&lt;"true"&lt;&lt;endl; &#125; &#125; return 0;&#125; 时空复杂度分析复习之后，这两个视频是需要再快速过一遍的： 视频一：https://www.bilibili.com/video/av32548823/?p=1视频二：https://www.bilibili.com/video/av32548823/?p=2 快速排序的时间复杂度最坏是 $O(N^2)$， 但是这种情况是很难达到的。hash 表中时间复杂度最坏是 $O(N)$， 但由于不知道这种hash 函数是什么，所以很难达到这种情况。 做笔试的时候的步骤 首先看数据范围，然后根据数据范围约莫着使用什么算法 下面介绍的是根据数据范围反推算法复杂度和算法内容： 一般ACM或者笔试题的时间限制是1秒或2秒。在这种情况下，C++代码中的操作次数控制在 $10^7$ 为最佳。 下面给出在不同数据范围下，代码的时间复杂度和算法该如何选择： $n &lt;= 30$, 指数级别, dfs+剪枝，状态压缩dp $n &lt;= 100 –&gt; O(n^3)$，floyd，dp $n≤1000 =&gt; O(n^2)$，$O(n^2logn)$，dp，二分 $n≤10000 =&gt; O(n \times \sqrt(n)) $，块状链表 $n≤100000 =&gt; O(nlogn) $=&gt; 各种sort，set/map、heap、dijkstra+heap，二分 $n≤1000000 =&gt; O(n) $, 以及常数较小的 $O(nlogn)$ 算法 =&gt; hash、双指针扫描、kmp，常数比较小的 $O(nlogn)$ 的做法：sort、树状数组、heap、dijkstra $n≤10000000 =&gt; O(n)$，双指针扫描、kmp $n≤10^9 =&gt; O( \sqrt(n)) $，判断质数 $n≤10^{18} =&gt; O(logn)$，最大公约数 简单版本 $n &lt;= 100 =&gt; O(n^3)$，floyd，dp $n≤1000 =&gt; O(n^2)$，$O(n^2logn)$，dp，二分 $n≤100000 =&gt; O(nlogn) $=&gt; 各种sort，树状数组、set/map、heap、dijkstra+heap、二分 $n≤1000000 =&gt; O(n) $, 以及常数较小的 $O(nlogn)$ 算法 =&gt; hash、双指针扫描、kmp 常数比较小的 $O(nlogn)$ 的做法：sort、heap、dijkstra $n≤10000000 =&gt; O(n)$，双指针扫描、kmp $n≤10^9 =&gt; O( \sqrt{n}) $，判断质数 $n≤10^{18} =&gt; O(logn)$，最大公约数 总的原则：最好是 1s 能够进行 $10^7$ 次运算。 $ 10^2 -&gt; O( n^3) $ dp， floyd$ 10^3 -&gt; O( n^2) $, dp， 二分, 枚举$ 10^5 -&gt; O( nlogn) $ 各种 sort 函数， set/map, heap, 二分$ 10^6 -&gt; O( n) $， 常数较小的 $O(n log n)$算法 -&gt; hash， 双指针，kmp，$10^7 -&gt; O(n)$，双指针， kmp 算法$ 10^18 -&gt; O( logn) $ 比较一下 $O(n^2)$ 和 $O(nlogn)$ 的时间复杂度， 当n =100000 的话 $O(n) = 10^5$ $O(n^2) = 10^{10}$ $O(nlogn ) = 20n = 2 \times 10^6$ 在c++ 中分析递归的空间复杂度是不容易分析的，因为需要进行 $logn$ 级别的递归，这个栈空间是需要加到内存的申请中去的。快排只有递归写法，所以空间复杂度上至少是$log n$的。 链表的时间和空间分析是比较简单的。 这个题目是可以好好看看，为什么不能使用快排但可以使用归并。因为前者不能写成非递归，后者可以写成非递归形式。 主定理一般用于算dfs 中的时间复杂度对于dfs +剪枝的题目，一般是不好分析时间复杂度的，因为有的剪枝效率是很高的，那么久变得不可预测了。 树的时间度分析使用的例题： leetcode 192 动态规划： 时间复杂度两种计算方法： 状态 数量$O(n^2)$, 状态转移复杂度 $O(1)$ ， 所以总的是两者的相乘 看循环的个数，两个for 循环叠加，那么就是 $O(N^2)$ dp 中会用到 memory 的思想 （记忆化搜索），有可能是递归的形式，实际上时间复杂度并不是dfs 的形式。 二分查找的时间复杂度是 $O(log n)$（常识） 字符串leetcode 245 是比较nice 的题目，好好看看。kmp 算法是 214 ，可以看看 单调队列（这个对于两个例题是需要再看看） 丑数，时间复杂度最优的是O(n) hash 表专题 hash表的最坏时间复杂度不用考虑，一般考虑均摊时间复杂度（$O(1)$）就可以。 149 这个题目是比较困难的，可以看看的哦 实现 two sum hash 表含义是存储之前的数字。hash 表的增删改查时间复杂度都是 $O( 1)$。下面的程序总的时间复杂度是 $O(n)$，空间是 $O(n)$ repeated DNA sequences unorderedc_map&lt;string, int&gt;使用 string.substr() 得到子串，然后将子串放到 hash 表中，如果 count()大于2，那么这个就是最后的结果。 design hashmap 定义常量表示大数组的时候，应该多加几个数字，防止边界情况，并且尽量使用质数。1int N =20011 ; 对待冲突是有两种方法：拉链法和开放寻址法。前者加一个链表，后者是发生冲突的时候，向着周围的空间进行寻找。？ 空间大小和原数据大小的关系？查找一下。 使用到了 迭代器 概念，这个题目是比较难的。 design hashmap， 这个题目特别好。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263class MyHashMap &#123;public: /** Initialize your data structure here. */ // 实现自己的hashmap // 关键在于处理 冲突，有两种方式，链表法 和开放寻址法； 这里实现的时候，使用链表法 // int N =20011; vector&lt;list&lt;pair&lt;int, int&gt;&gt;&gt; h; MyHashMap() &#123; h =vector&lt;list&lt;pair&lt;int, int&gt;&gt;&gt;(N); &#125; // 快速的 find 一个key的位置, 首先是一个 list，然后是一个链表 // 查找的时候，已经包含了处理链表的情况 list&lt;pair&lt;int, int&gt;&gt; :: iterator find(int key) &#123; int t =key %N; for(auto it =h[t].begin() ; it != h[t].end(); it ++) &#123; if(it -&gt; first ==key) return it; &#125; return h[t].end(); &#125; // 这种pair 的结构是非常容易操作key,value 这样的数字 /** value will always be non-negative. */ void put(int key, int value) &#123; auto it =find(key); int t =key %N; if(it ==h[t].end()) h[t].push_back(&#123;key, value&#125;); else it -&gt; second= value; &#125; /** Returns the value to which the specified key is mapped, or -1 if this map contains no mapping for the key */ int get(int key) &#123; auto it =find(key); int t =key %N; if( it ==h[t].end()) return -1; return it-&gt;second; &#125; /** Removes the mapping of the specified value key if this map contains a mapping for the key */ void remove(int key) &#123; auto it =find(key); int t =key %N; if(it != h[t].end()) h[t].erase(it); &#125;&#125;;/** * Your MyHashMap object will be instantiated and called as such: * MyHashMap* obj = new MyHashMap(); * obj-&gt;put(key,value); * int param_2 = obj-&gt;get(key); * obj-&gt;remove(key); */ Subarray Sum Equals K 题解思路也是非常的惊奇。时间复杂度是 $O(1)$ 123456789101112131415161718192021222324class Solution &#123;public: // 前缀和 + hash的思路 // hash 处理的是插入和查询的问题 int subarraySum(vector&lt;int&gt;&amp; nums, int k) &#123; unordered_map&lt;int, int&gt; hash; int res =0; int sum =0; //hash[] hash[0] =1; // 因为后面用到的 sum ==k 这样的前缀和 for(int i =0; i&lt;nums.size() ;i++) &#123; sum += nums[i]; res += hash[sum -k]; hash[sum] +=1; &#125; return res; &#125;&#125;; 并查集常用的两种操作 合并两个集合 判断两个点是否在同一个集合中 有两种优化，路径压缩和按秩合并。前者优化之后时间复杂度变成 $O(log n)$ 后者进一步优化变成 $loglog n$。因为后者优化之后收益不大，所以一般使用前者。 leetcode题目 12345678910111213141516171819202122232425262728293031323334353637class Solution &#123;public: vector&lt;int&gt; p; int find(int x) &#123; if( p[x] !=x) p[x] =find(p[x]); return p[x]; &#125; int findCircleNum(vector&lt;vector&lt;int&gt;&gt;&amp; M) &#123; int n= M.size(); int res =n; // 初始化 for(int i =0; i&lt;n; i++) p.push_back(i); for(int i =0; i&lt;n ; i++) for(int j =0; j&lt;i; j++) &#123; if(M[i][j] ==0) continue; if(find(i) != find(j)) &#123; // find(x) 函数是找x 的父节点 // find(i) 的父节点指向了 find(j) p[find(i)] =find(j); res -=1; &#125; &#125; return res; &#125;&#125;; 自己手写堆的话，可以实现以下四个功能： 查找最大值 $O( 1) $ 插入一个数 $O (log n)$ 删除一个数 $ O(log n)$ 修改一个数 $O(log n)$ 如果使用 c++ 中 LST 中 priority_queue 那么只能使用上面 1. 2. 3. 种功能，并且删除一个数，只能删除堆顶，而不是删除任意一个。默认是大根堆的实现。 Top K Frequent Words 小根堆有很多实现方式，C++ 中默认是大根堆，所以可以存储相反数。hash 表存储单词出现的次数。 heap 表示堆的单词。 在result 存储的时候，实际上最先出来的是最差的，所以是从后往前遍历。 pair 是双关键字比较大小，如果第一关键字不同，那么比较出来大小，如果第一关键字相同，那么比较第二关键字大小。题目中要求出现频率大且字典序小的在前面。如果使用 负数表示，那么就转换成了负数小（正数大）且字典序小的在前。所以使用大根堆比较好处理。 123456789101112131415161718192021222324252627282930313233343536373839#include&lt;iostream&gt;#include&lt;vector&gt;#include&lt;string&gt;#include&lt;unordered_map&gt;#include&lt;queue&gt;// 注意 priorty_queue 是定义在 queue 里面的using namespace std;vector&lt;string&gt; topK_frequent(vector&lt;string&gt; arr, int k )&#123; typedef pair&lt;int, string&gt; PAIR; unordered_map&lt;string ,int&gt; hash; priority_queue&lt;PAIR&gt; heap; for(auto word: arr) hash[word] ++; for(auto item : hash) &#123; PAIR t(-item.second, item.first); heap.push(t); if(heap.size() &gt; k) heap.pop(); &#125; vector&lt;string&gt; res(k); for(int i =k-1; i&gt;=0 ; i--) &#123; res[i] =heap.top().second; heap.pop(); &#125; return res;&#125;int main()&#123; vector&lt;string&gt; arr=&#123;"i", "love", "leetcode", "i", "love", "coding"&#125;; int k =2; vector&lt;string&gt; res =topK_frequent(arr, k); for(auto u: res) cout &lt;&lt; u&lt;&lt; " "; cout &lt;&lt;endl; return 0;&#125; 在python 中默认是小根堆，也是(-val, key) 这样保持一致，如果是都小，那么排在前面。最后的结果是先弹出来的，那么就是出现频率最大，并且字母序在前面的那种。 1234567891011121314151617181920class Solution(object): def topKFrequent(self, words, k): """ :type words: List[str] :type k: int :rtype: List[str] """ import collections, heapq has =collections.defaultdict(int) for word in words: has[word] +=1 heap =[] for (key, val) in has.items(): heapq.heappush(heap, (-val, key)) res =[] while k: val, key = heapq.heappop(heap) res.append(key) k -=1 return res 类似的题目 347. Top K Frequent Elements1234567891011121314151617181920212223class Solution &#123;public: vector&lt;int&gt; topKFrequent(vector&lt;int&gt;&amp; nums, int k) &#123; unordered_map&lt;int, int&gt; hash; typedef pair&lt;int, int&gt; PAIR; priority_queue&lt;PAIR&gt; heap; for(auto num : nums) hash[num] +=1; for(auto item : hash) &#123; PAIR t(-item.second, item.first); heap.push(t); if(heap.size() &gt; k) heap.pop(); &#125; vector&lt;int&gt; res(k); for(int i =k -1; i &gt;=0; i--) &#123; res[i] =heap.top().second; heap.pop(); &#125; return res; &#125;&#125;; 123456789101112131415161718192021222324class Solution(object): """ 是出现频率最大的k 个数字，那么想到的是使用小根堆的思想，维护 k 大的小根堆 """ def topKFrequent(self, nums, k): """ :type nums: List[int] :type k: int :rtype: List[int] """ import collections, heapq has =collections.defaultdict(int) heap =[] for num in nums: has[num] +=1 # 取反 这样保证是根据 次数进行选择， for (key, val) in has.items(): heapq.heappush(heap, (-val, key)) res =[] while k: _, key =heapq.heappop(heap) res.append(key) k -=1 return res LeetCode 295动态维护有序序列 -&gt; 手写平衡树 使用对顶堆的思想。找中位数是 $O(1)$， 插入操作是 $O(log n)$ 的。 模板课总结排序算法（快排和归并排序） 二分（整数二分，浮点数二分） 学习方法：理解只有，多写 3-5遍。重复才是王道。 对于边界问题非常复杂的问题，建议大家背过模板，这样是比较nice的。如果对于数据量很大的情况，建议使用 scanf() 进行读入，因为 cin 相对来说是比较慢的。稳定性，两个相同的数字，如果再排序前后能够保持相对的顺序，那么就是稳定的。快排是不稳定的，归并排序是稳定的。快排中的数字变成 pair&lt;num, index&gt;类型，那么就可以成为稳定的排序。 （背诵一个固定的模板就行）快排的思想： 分治。算法步骤： 确定分界点， q[l], q[r], q[l(l +r) /2] 随机都是可以的 调整区间， 使一侧是小于等于x，另外一侧是大于等于x 就是ok的 递归处理左右两段 快排的平均时间复杂度是$n log_{2} n$， 最坏是 $O(n^2)$。 归并排序- 分治 确定分界点，mid =(l +r) /2 递归排序, left 和 right 归并，合二为一 快排是先操作后递归，归并是先递归然后其他操作。时间复杂度是(平均和最坏) $O(nlog_2n)$，总共是有 $logn$ 层，然后每层计算是$O(n)$。归并排序相对于快排是需要一个额外的$O(n)$ 的空间去存储排序之后的结果。 if (l &gt;= r) 如果没有数字或者只有一个数字的时候，做的操作.在常用的模板中，因为使用了一个临时数组，所以是需要把结果拿回来，所以最后是需要一个循环进行操作的。 二分 二分的本质不是单调性（如果具有单调性，那么是可以二分，但如果没有单调性，也是有可能二分的）。二分的本质是边界，可以找到一个分界点，左边是满足这个条件，右边是不满足这个条件。这个时候就可以二分。每次二分的时候都保证答案是在这个区间内的。 整数二分版本一：当我们将区间[l, r]划分成[l, mid]和[mid + 1, r]时，其更新操作是r = mid或者l = mid + 1;，计算mid时不需要加1。版本二：当我们将区间[l, r]划分成[l, mid - 1]和[mid, r]时，其更新操作是r = mid - 1或者l = mid;，此时为了防止死循环，计算mid时需要加1。可以通过这个题目好好理解两者的区别。在排序数组中查找元素的第一个和最后一个位置 cpp 中list 封装是链表，vector 封装是数组。 该题目的讲解可以从这里找找 1234567891011121314151617181920212223242526class Solution &#123;public: vector&lt;int&gt; searchRange(vector&lt;int&gt;&amp; nums, int target) &#123; if (nums.empty()) return vector&lt;int&gt;(&#123;-1, -1&#125;); vector&lt;int&gt; res; int l = 0, r = nums.size() - 1; while (l &lt; r) &#123; int mid = l + r &gt;&gt; 1; if (nums[mid] &gt;= target) r = mid; else l = mid + 1; &#125; if (nums[r] != target) return vector&lt;int&gt;(&#123;-1, -1&#125;); res.push_back(r); l = 0, r = nums.size() - 1; while (l &lt; r) &#123; int mid = l + r + 1 &gt;&gt; 1; if (nums[mid] &lt;= target) l = mid; else r = mid - 1; &#125; res.push_back(r); return res; &#125;&#125;; 浮点数二分问题。浮点数不存在 +1，-1 操作，因为这个是可分的。但是存在精度问题，如果题目要求是 6位小数，那么边界值判断是使用 $1e-8$ 就可以保证。（要保证比要求的位数多两位，一般是没有问题的） 123456789101112131415161718192021#include&lt;iostream&gt;using namespace std;int main()&#123; double x ; cin &gt;&gt;x; double l =0, r =0; while( r -l &gt; 1e-8) &#123; double mid =( l+r) /2; if(mid *mid &gt;=x) r =mid; else l =mid; &#125; printf("%lf\n", l); return 0;&#125;]]></content>
      <categories>
        <category>CS基础</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[深度网络中的碎碎念]]></title>
    <url>%2F2019%2F03%2F26%2F%E6%B7%B1%E5%BA%A6%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E7%A2%8E%E7%A2%8E%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[介绍深度网络中小的基本概念，比如权重初始化、激活函数和优化器 和常见的技术比如 dropout。 Weights Initializationweights 的初始化在网络的训练起到重要的作用，初始化的好坏能够直接影响到网络是否可以正常收敛。这里的初始化都是指的是weights初始化。bias 表示偏差，噪声，作用在于企图去描述真实的分布（高斯分布），通过引入随机性来表示这个是具有推广性的。主要介绍常见的三种初始化方法和选择方法。 Here’s another trick — before squishing our scalar value (called an activation) into the sigmoid function, we can add a little number called the bias, b. There will be two biases, one for each user and one for each movie. Random Initialization总结来说就是容易出现梯度消失和梯度爆炸，尤其是在layer_size（特征数量） 比较大的时候。从均值方差的角度进行分析。 a) If weights are initialized with very high values the term np.dot(W,X)+becomes significantly higher and if an activation function like sigmoid() is applied, the function maps its value near to 1 where slope of gradient changes slowly and learning takes a lot of time.b) If weights are initialized with low values it gets mapped to 0, where the case is same as above. 1w =np.random.randn(layer_size[l],layer_size[l-1]) 另外的表述方式： If the weights start too small, then the signal shrinks as it passes through each layer until it’s too small to be useful.If the weights start too large, then the signal grows as it passes through each layer until it’s too massive to be useful (big value in sigmoid function). Xavier initialization /ˈzeɪvjər/sFor deep networks, we can use a heuristic to initialize the weights depending on the non-linear activation function. This applies to Xavier and He initialization. Xavier/Glorot Initialization initializes the weights in your network by drawing them from a distribution with zero mean and a specific variance.$$ { var } ( w _ { i } ) = \frac { 1 } { layer_{l-1}}$$ 1w=np.random.randn(layer_size[l],layer_size[l-1])*np.sqrt(1/layer_size[l-1]) In practice, it works better for layers with sigmoid or tanh function. 总的思想原则：They set the weights neither too much bigger that 1, nor too much less than 1.就是本来就是在 (0,1) 标准正太分布出来，然后进行了进一步的约束条件。思想当特征数量越大的时候，weights 的波动情况是成反比的，最后的weights 数值越接近于均值附近。 He InitializationUsing RELU or Leaky RELU is relatively robust to the vanishing/ exploding gradient issues compared with sigmoid function especially for networks that are not too deep. And it the case of Leaky RELU, it never has zero gradients. For RELU, we multiply the randomly generated values of $w$ by: $$\sqrt { \frac { 2 } { layer _ { [ l - 1 ] } } }$$1w=np.random.randn(layer_size[l],layer_size[l-1])*np.sqrt(2/layer_size[l-1]) Sometimes, we combine the idea of Xavier initialization and He initializaiton so the variance becomes the following: $$\sqrt { \frac { 2 } { layer _ { [ l - 1 ] } + \operatorname { layer } _ { [ l ] } } }$$ 12w=np.random.randn(layer_size[l],layer_size[l-1])*np.sqrt(2/(layer_size[l-1]+layer_size[l]))# 代码中 random() 中的两个参数是 shape，最后的np.sqrt 标准差 The idea behind this is that we set the weights neither too much bigger than 1 nor too much less than 1 so the gradients do not vanish or explode too quickly. 所以上述的初始化从数学的角度去理解： 从(0,1) 标准正太分布 转换成了 (0, np.sqrt(2/ size_l -1)) 这样的分布，就是你网络结构是越宽，那么这个方差就是越小的，最后的结果是越集中的，就越集中的 均值 u 左右。从图像的角度看，方差越大，图像越矮胖；方差越小，图像越瘦高。 Takeoff In summary, the main difference in machine learning is the following: He initialization works better for layers with ReLu(s) activation. Xavier initialization works better for layers with sigmoid activation. Activation function总的说可以分为线性和非线性的激活函数， activation function 的作用 就是对于网络的输出 说yes or no. It maps the resulting values in between 0 to 1 or -1 to 1 etc. Sigmoid function (Logistic Activation)the only reason why we use sigmoid is because it exists between 0 to 1. 这个非常有利于 predict probability. 因为自然映射到 0 是不存在 然后1 是存在。而当多分类的时候，使用softmax。 Tanh function The advantage is that the negative inputs will be mapped strongly negative and the zero inputs will be mapped near zero in the tanh graph. Relu (Rectified Linear Unit) Activation本质上是分段函数。 range: [0, infinity]. The function and its derivative both are monotonic. Leaky Relu每当一个伟大的东西产出，总会伴随着比较明显的错误，然后紧跟着就是有一个 rectified(improved) 这种版本。这个相比之前就是修正了 当输入是负数的时候，怎么办的问题。 Softmax这个需要从数学的角度去理解，数学公式就是下面。有一个真实的例子： 如果一张”4” 的图片，输出一个网络，最后是softmax 激活函数，那么最后得到的 “4” 的概率是最大的，但是也是有其他的可能性存在的，这个就是softmax 的效果。 最主要的功能是 convert number into probabilities. 这种效果，不像sigmoid 那样有很明确的数学公式。 $$\sigma ( z ) _ { j } = \frac { e ^ { z _ { j } } } { \sum _ { k = 1 } ^ { K } e ^ { z _ { k } } }$$ Softmax function, a wonderfulactivation function that turns numbers aka logits into probabilities that sum to one.Remember the takeaway is: the essential goal of softmax is to turn numbers into probabilities. 卷积网络一个卷积神经网络由若干卷积层、Pooling层、全连接层组成。 卷积层$Input \rightarrow [Conv] \times N \rightarrow [Pool] \times M \rightarrow [FK] \times K$ 下面的动画显示了包含两个filter的卷积层的计算。我们可以看到 $7 \times 7 \times 3$ 输入，经过两个$3 \times 3 \times 3 $filter的卷积(步幅为2)，得到了$ 3 \times 3 \times 2 $的输出。另外我们也会看到下图的Zero padding是1，也就是在输入元素的周围补了一圈0。Zero padding对于图像边缘部分的特征提取是很有帮助的。 100×100×3，3×3 卷积核，输出是 50×50×10，算进行了多少次乘-加操作？输出的每个像素点都要经过 3×3×3 = 27 次乘-加操作，因此总共需要进行 50×50×10×27 次乘-加操作。 对于包含两个 $333 $的fitler的卷积层来说，其参数数量仅有 $(3 \times 3 \times3+1) \times 2 =56 $个，且参数数量与上一层神经元个数无关。与全连接神经网络相比，其参数数量大大减少了。 卷积层输出大小计算：输入大小： $W 1 \times H 1 \times D 1$超参数（filter信息 +是否填充）： filter 个数( K), filter 大小( F), 步长 (S )，边界填充( P)输出：$$\begin{split}W _ { 2 } &amp; = \left( W _ { 1 } - F + 2 P \right) / S + 1 \\H _ { 2 } &amp;= \left( H _ { 1 } - F + 2 P \right) / S + 1 \\D _ { 2 } &amp;= K\end{split}$$ 注意计算的是宽和高的两个维度。 $$\begin{split}\text {output} _ { w } &amp;= \left\lfloor \frac { i m a g e _ { w } + 2 p a d d i n g - k e r n e l _ { s i z e } } { s t r i d e } \right\rfloor + 1 \\\text {output} _ { h } &amp;= \left\lfloor \frac { i m a g e _ { h } + 2 p a d d i n g - k e r n e l _ { s i z e } } { s t r i d e } \right\rfloor + 1 \\\end{split}$$卷积层参数量计算： 权值共享： 给定一张图，用一个 filter 去扫描这张图， filter 里面的数字叫做权重，这张图么个位置是被同样的filter 扫描的，所以权重是一样的，也就是共享。（从减少参数量的角度去理解）这个概念是和全连接层中的权值进行比较的，简单来说就是降低了权重的使用。权重共享即 filter 值的共享。对于 三维图片来说，每个filter需要FFD1个权重值，总共K个filter，需要FFD1*K权重值。和一维一样，整个滑动过程中filter W0和W1值保持不变，可称作权值共享。而且，补充一句，对于三维的input，权值只是在input的每个depth slice上共享的。对于一层的 filter 只是有一个bias。 for example:Filter个数：32原始图像shape：$224 \times 224 \times 3$卷积核大小为：$2 \times 2$一个卷积核的参数：$ 2 \times 2 \times 3=12 $16个卷积核的参数总额：$ 16 \times 12 + 16 =192 + 16 = 208 $$ weight \times x + bias $根据这个公式，即可算的最终的参数总额为：208 Pooling 层Pooling层主要的作用是下采样，主要有两点作用，一个是提取重要特征，一个是简化网络的计算。Pooling的方法很多，最常用的是Max Pooling。Max Pooling实际上就是在n*n的样本中取最大值，作为采样后的样本值。下图是 max pooling： 除了Max Pooing之外，常用的还有Mean Pooling——取各样本的平均值。 池化层往往在卷积层后面，通过池化来降低卷积层输出的特征向量，同时改善结果（不易出现过拟合）。 池化层参数个数计算，并且池化层明显是没有参数的。 卷积特征往往对应某个局部的特征。要得到global的特征需要将全局的特征执行一个aggregation（聚合）。池化就是这样一个操作，对于每个卷积通道，将更大尺寸（甚至是global）上的卷积特征进行pooling就可以得到更有全局性的特征。这里的pooling当然就对应了cross region。 卷积层 vs pooling 层主要比较两者在降维、特征提取方面的差别。 pooling 层常见的 max pooling 和 average (mean ) pooling两种。该层是没有参数的。 pooling 的作用主要体现在减少模型去拟合的难度，防止过拟合，节省计算力方面。max pooling 相比于 mean pooling 更加有 提取特征的感觉。抓住比计较显著的特征，AVE pooling 可以带来一定意义上的平滑，可以减小图像尺寸变化的干扰。从效果上讲，前者一半是要好于后者的。 从上面左图可以看到，使用了pool操作其实就是降低图片的空间尺寸。右图使用一个 2 × 2的 池化核（filter），以2为步长（stride），对图片进行max pooling，那么会图片就会尺寸就会减小一半。需要注意，这里是因为 stride = 2，所以图片尺寸才会减少一半的。 不同点：pooling 是没有 weights 或者 parameter 更新，仅仅是下采样convolution layer 则不一样，提取了特征并且进行了下采样。 全连接层连接所有的特征，将输出值送给分类器（如softmax分类器） 比如说上一层（池化层）的输出为：$(111 \times 111 \times 16) $，从第一层到第二层，只是图片大小发生了变化，深度没有发生变化，而Dense对应的神经元个数为133个，那么还是根据公式：$weight \times x + bias$，计算得：$133 \times16+133=2261$ Dropout 概念 dropout 是指在深度学习网络的训练过程中，按照一定的概率将一部分神经网络单元暂时从网络中丢弃，相当于从原始的网络中找到一个更瘦的网络。 why 我们在训练神经网络的时候，会遇到两大缺点： 容易过拟合 费时 dropout 主要是为了在一定程度上减少过拟合。 工作原理 首先随机（临时）删掉网络中一半的隐藏神经元，输入输出神经元保持不变（图中虚线为部分临时被删除的神经元） 然后把输入x通过修改后的网络前向传播，然后把得到的损失结果通过修改的网络反向传播。一小批训练样本执行完这个过程后，在没有被删除的神经元上按照随机梯度下降法更新对应的参数（w，b） 继续重复这一过程：恢复被删掉的神经元（此时被删除的神经元保持原样，而没有被删除的神经元已经有所更新）。从隐藏层神经元中随机选择一个一半大小的子集临时删除掉（备份被删除神经元的参数）。 不断的重复着一过程 怎么理解测试时权重参数w要乘以概率p？ 假设总共有100个神经元，训练的时候我们加上dropout，p=0.5，那么我们就有50个神经元参与训练，那么我们每次50个神经元训练出来的模型参数w是要比直接100个神经元要小的，因为它更新的次数会更少。我们测试的时候100个神经元是都会参与计算的，这就跟训练的时候我们使用50个神经元产生差异了，如果要保证测试的时候每个神经元的关联计算不能少，只能从通过改变w来达到跟训练时一样输出，所以才会有权重参数w乘以p。 为什么 dropout 可以有效的减少过拟合？（类似取平均的活动）因为不同的网络可能产生不同的过拟合，取平均则有可能让一些“相反的”拟合互相抵消。dropout掉不同的隐藏神经元就类似在训练不同的网络，随机删掉一半隐藏神经元导致网络结构已经不同，整个dropout过程就相当于对很多个不同的神经网络取平均。 梯度消失/ 梯度爆炸 首先一个观点，梯度消失和梯度爆炸本质上一回事。 理由：sigmoid 导数的最大值为0.25，通常 abs(w) &lt; 1,则上述分析中的激活函数的导数与权重的积小于0.25，前面的层比后面的层梯度变化更小，故变化更慢，从而引起了梯度消失问题。当权值过大，前面层比后面层梯度变化更快，则引起梯度爆炸问题。所以后面的梯度消失和梯度爆炸只是前面初始化值的一种蝴蝶效应，只是数值问题。 解决方法 重新设计网络结构 使用比较浅的网络结构 使用残差结构, 这种方式在图像处理中更加常见 激活函数使用 relu or leaky relu 而不是 sigmoid or tanh 关于weights 方面 使用梯度截断（Gradient Clipping），检查误差梯度的值是否超过阈值，如果超过，则截断梯度，将梯度设置为阈值。 使用权重正则化（Weight Regularization），常用的是 ，通常使用的是 L1 惩罚项（权重绝对值）或 L2 惩罚项（权重平方）这个是在损失函数上的操作 batch normalization （关于BN 的位置是可以再查一下的，现在有两个位置，一个是在激活函数之前一个是 激活函数之后，个人倾向于激活函数之前，因为这样才有可以减少梯度消失和梯度爆炸的发生呀） dropout一定在激活函数之后 ps: CONV / FC - &gt; BatchNorm - &gt; ReLu（或其他激活） - &gt; Dropout - &gt; CONV / FC对于 cnn 还有一种常见的结构：先卷积，再batchnorm, 然后激活函数，最后pooling在fully connection中的应用，用在全连接层之后激活函数之前 理解 正则化如何减少模型过拟合程度 High Bias（高偏差）就是欠拟合，High Variance（高方差）就是过拟合。 简单来说，正则化就是在原来的 cost function 中添加 正则项。 正则化项能减少模型的非线性程度，从而降低模型的过拟合。从图中来看，正则化项能将过拟合的模型（蓝色）变为Just Right的模型（粉红色）。 为什么正则化是有效的？ 对于线性模型，其添加了正则化项的Cost Function如下图。 现在目标函数有两个目标，第一个是我们想要训练的，使假设更好地去拟合训练数据，第二个目标是我们想要保持参数较小。 $ \lambda$ 用来调节两者之间的平衡，这样理解如果 该值设置的很大，那么前面的参数 $\theta 1 \theta 2 \theta 3 \theta 4$ 就会被非常大的惩罚，这些值就会接近0。如果假设第一个目标是多项式组成的，那么当 $\theta $ 数值变小的时候，这个式子就没有了，就减少了模型的复杂度。 对于神经网络，其激活函数（以tanh为例）如下图 直观的理解，如果我们的正则化系数（lambda）无穷大，则权重w就会趋近于0。权重变小，激活函数输出z变小。z变小，就到了激活函数的线性区域，从而降低了模型的非线性化程度。 感受野的计算在卷积神经网络中，感受野（Receptive Field）的定义是卷积神经网络每一层输出的特征图（feature map）上的像素点在输入图片上映射的区域大小。 这个是一个反向的过程。这个和 CNN 的 不同在于，如果是2 两层，那么这个是连续（持续）的对同一个图像进行采样。接着上一个的结果进行操作。 使用例子说明：两层$3 \times 3$ 卷积操作的有效区域(感受野)是 $5 \times 5 $ (所有filter的stride=1,pad=0)。 卷积和池化操作的计算（例题）卷积和池化的计算方式是一样的，具体可以参考上面小结关于卷积操作的公式。 输入图片大小为200×200，依次经过一层卷积（kernel size 5×5，padding 1，stride 2），pooling（kernel size 3×3，padding 0，stride 1），又一层卷积（kernel size 3×3，padding 1，stride 1）之后，输出特征图大小为：A. 95B. 96C. 97D. 98E. 99F. 100 解答： 第一次卷积后大小：$$\frac { 200 + 2 - 5 } { 2 } + 1 = 99$$ 第一次池化后大小： $$\frac { 99 + 0 - 3 } { 1 } + 1 = 97$$ 第二次卷积后大小： $$\frac { 97 + 2 - 3 } { 1 } + 1 = 97$$ 所以最后的结果是 97 为什么使用奇数小尺寸的卷积核 For an odd-sized filter, all the previous layer pixels would be symmetrically around the output pixel. Without this symmetry, we will have to account for distortions across the layers which happens when using an even sized kernel. Therefore, even sized kernel filters are mostly skipped to promote implementation simplicity.选择奇数个数的filter 是因为前后的filter 是有对称性的。下图所示。 $3x3$ 是最小的能够捕获像素八邻域信息的尺寸。 1). 参数量多个 $3x3 $的卷积层比一个大尺寸的 filter 有更少的参数，假设卷基层的输入和输出的特征图大小相同为$ C$，那么三个 $3x3$ 的卷积层参数个数 $3x（3x3xCxC）=27C^2$；一个$7x7 $的卷积层参数为 $49C^2$；所以可以把三个 $3x3 $的filter看成是一个$7x7 $filter的分解。2). 增加非线性多个 $3x3 $的卷基层比一个大尺寸 filter卷基层有更多的非线性（更多层的非线性函数），使得判决函数更加具有判决性。 $1 * 1$的卷积核特点1). 降维/升维 2). 增加非线性 $1 *1 $卷积核，可以在保持feature map尺度不变的（即不损失分辨率）的前提下大幅增加非线性特性（利用后接的非线性激活函数），把网络做的很deep。 3). 从全连接层的角度理解 $1 * 1$卷积操作 左边6个神经元，分别是a1—a6，通过全连接之后变成5个，分别是b1—b5 左边6个神经元相当于输入特征里面的channels：6 右边5个神经元相当于$1 *1$卷积之后的新的特征channels：5 左边 $W *H *6$ 经过 $1 *1 *5 $的卷积核就能实现全连接]]></content>
      <categories>
        <category>Deep learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[siamese network]]></title>
    <url>%2F2019%2F03%2F26%2Fsiamese-network%2F</url>
    <content type="text"><![CDATA[主要是介绍自己论文中的网络结构：siamese network。 但凡优化，无非两条路：在好的基础上更快，在快的基础上效果更好。 Siamese network训练速度快，所以只是需要其在训练效果上更好。 先来分析一下为什么训练速度快。那么不得不分析该网络结构。整个网络的输入是 (img1, img2, y) 这样的三元组，img 表示图片，y表示label。图片可以是同一类别的图片，也可以是不同类别的，y表示两张图片之间的相似程度，y的取值 (0,1)，0表示相似（同一类别），1 表示不相似（不同类别）。并且y 是double 类型，属于0-1 区间任意的数字。网路结构最后的输出是 0-1 区间的任意数字。通常是以0.5 作为分界线，如果小于0.5 那么认为两种图片是属于同一类别，或者说更相似；反之也成立。重要的一点是中间的weights 实现了权值共享，这样可以加快网络的训练速度。 loss function这个是属于经典的 contrastive loss function (对比损失函数)。当y 接近于0的时候，右半部分消失，这个是表示两张图片很是相似，然后就不断使得 欧氏距离减少；当y 接近于1的时候，左半部分消失，这个时候两张图片很不相似，然后右边就是 hinge loss （合页损失函数）。参数m 作为一种margin 是是可以调节，我的实验中 m 取1.总的思想：就是使得相近的图像距离相近，不想近的图像距离变远。 $L ( W , ( Y , X _ { 1 } , X _ { 2 } ) ) = ( 1 - Y ) \frac { 1 } { 2 } ( D _ { W } ) ^ { 2 } + ( Y ) \frac { 1 } { 2 } { \max ( 0 , m - D _ { W } ) } ^ { 2 }$ Spectral Normalization图像输入到网络之前使用正则化，然后输入到激活函数之前也是使用正则化，所以这种效果也是扩展到 weights，直接对 weights 进行正则化使其符合 Lipschitz 约束，避免使用大的gradients。在GAN 网络中的 discriminator 或者 generator 都发现了其可以稳定训练的过程。在实验中，我们扩大了这种使用范围，把其应用到所有的网络的 layer上。 Lipschitz Continuity 在 GAN 中，假设我们有一个判别器 $\mathrm { D } : 1 \rightarrow \mathrm { R }$， 其中 I 是图像空间. 如果判别器是 K-Lipschitz continuous 的, 那么对图像空间中的任意 x 和 y，有 $$| D ( x ) - D ( y ) | \leq K | x - y |$$ 其中 $ | \cdot | $ 为L2 norm，如果K 取到最小值，那么K 被称为 Lipschitz constant。 直观来说，Lipschitz 条件限制了函数变化的剧烈程度，即函数的梯度。在一维空间中，很容易看出 y=sin(x) 是 1-Lipschitz 的，它的最大斜率是 1。 self-attention mechanismAttention 机制自从 “Attention Is All You Need” 开始火爆，并且实验的效果也是很好的，然后在图像领域也开始尝试使用 attention 机制来解决长依赖的问题。应用到图像领域主要是 explore spatial locality information, 说白了就是细节的信息。 If we look at the DCGAN model, we see that regular GANs are heavily based on convolution operations, which use a local receptive field (convolutional kernel) to learn representations. Simple features like edges and corners are learned in the first few layers. Also, ConvNets are able to use these simple representations to learn more complex ones. However, long-range dependency might be hard to learn. Long-range dependency (long-term dependency) is from RNN, which we can say anything larger than trigram as a long term dependency. Thus, most of the image content does not exhibit elaborated shape such as sky or the ocean looks fine. The task of creating geometrically complex forms, such as four-legged animals, is far more challenging. This is where attention comes into play. 而 self-attention 中QKV 三个部分是相同的，对于这种处理方法和Res_block 还是有点相似的。 结果训练数据集使用是 Cifar-10，记录了训练过程中 acc 和loss 的变化情况。除了训练的效果比较好外，训练速度也是非常快的，可以清楚的看到model acc 在接近25 epoches的时候就开始收敛。]]></content>
      <categories>
        <category>Deep learning</category>
      </categories>
      <tags>
        <tag>siamese network</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[fastText & faiss]]></title>
    <url>%2F2019%2F03%2F25%2FfastText-faiss%2F</url>
    <content type="text"><![CDATA[主要介绍 fastText、faiss 两个常用的工具，然后顺带介绍一下容易混淆的概念 k-means 和knn。 fastTextfastText结合了自然语言处理和机器学习中最成功的理念。这些包括了使用词袋以及n-gram袋表征语句，还有使用子字(subword)信息，效果上的提升。另外采用了一个softmax层级(利用了类别不均衡分布的优势)来加速运算过程。 fastText 主要是用来解决 word representations和 sentence classification. 有趣的是前者是无监督的学习方式，后者是有监督的学习方式。分别主要来自 ”Enriching Word Vectors with Subword Information“ 和 “Bag of Tricks for Efficient Text Classification” 两篇论文。并且使用的是 shallow neural network 而不是深度网络。 FastText is a library created by the Facebook Research Team for efficient learning of word representations and sentence classification. Take off:fastText 方法包含三部分：模型架构、层次 Softmax 和 N-gram 特征。fasttext 有两个用处： text classification 和 word embedding 。使用场景：大型数据，高效计算 下面进行细说： 模型架构这个是总的框架图。分为两个部分介绍这个网络结构：从input -&gt; hidden:输入层输入的是一个已经分词后短文本。短文本中每个词的词向量是由该短文本的one-hot矩阵乘以一个初始化的矩阵w得到的。（原理图：下图是fasttext 运行的时候，这个分词是再处理成单词和n-gram 组成的特征，这个是不需要我们进行显性的操作的）从 hidden -&gt; output：插播一句，我们经常使用的预训练模型中的weights 是从input-&gt; hidden。 Hierarchical SoftmaxHierarchical Softmax 不是fasttext 首创，它的改进之处在于实现结构上基于 huffman 树而不是普通的二叉树，属于运算上的优化。具体说来：利用了类别（class）不均衡这个事实（一些类别出现次数比其他的更多），通过使用 Huffman 算法建立用于表征类别的树形结构。对标签进行编码，能够极大地缩小模型预测目标的数量。 这个是softmax 的原始的计算公式：$$p \left( w _ { j } | w _ { I } \right) = y _ { j } = \frac { \exp \left( u _ { j } \right) } { \sum _ { j ^ { \prime } = 1 } ^ { V } \exp \left( u _ { j ^ { \prime } } \right) }$$ 采用二叉树的结构之后，时间上优化不少。$O ( N) \rightarrow O \left( \log _ { 2 } N \right)$。见下图。 和之前的神经网络模型相比，这里的huffmax树的所有内部节点就类似之前神经网络隐藏层的神经元。其中，根节点的词向量对应我们的投影后的词向量，而所有的叶子节点就类似于之前神经网softmax输出层的神经元。叶子节点的个数就是词汇表的大小. 和之前的相比，从隐藏层到输出层的softmax映射不是一下就完成的，而是沿着 huffman树一步步完成的，因此这种 softmax取名为”Hierarchical softmax”. N-gram 特征N-gram是基于这样的思想：某个词的出现依赖于其他若干个词；我们获得的信息越多，预测越准确。我想说，我们每个人的大脑中都有一个N-gram模型，而且是在不断完善和训练的。我们的见识与经历，都在丰富着我们的阅历，增强着我们的联想能力。 N-gram 是一种思想，可以有两种level 的实现，一种是基于 character-level，一种是基于 word-level，前者是扩充了对于”不常见“单词，后者是考虑了部分的词的顺序，都是考虑了”周边“ 信息,用流行的话就是 context 的信息。所以比较难界定 fasttext 训练出来的是不是有比较强的词序。 N-gram模型是一种语言模型（Language Model，LM），语言模型是一个基于概率的判别模型，它的输入是一句话（单词的顺序序列），输出是这句话的概率，即这些单词的联合概率（joint probability）。 这样的作用，使用N-gram来给文本添加额外的特征获得关于局部词顺序的部分信息。举个栗子：对于句子：“我 喜欢 喝 咖啡”, 如果不考虑顺序，那么就是每个词，“我”，“喜欢”，“喝”，“咖啡”这五个单词的word embedding求平均。如果考虑2-gram, 那么除了以上五个词，还有“我喜欢”，“喜欢喝”，“喝咖啡”等词。“我喜欢”，“喜欢喝”，“喝咖啡”这三个词就作为这个句子的文本特征。我们经常见到的场景：输入法的预选词汇。就是可以通过这种方式实现的。 当然使用了更多的特征意味着计算量的增加，计算效率下降，于是该作者提出了两种解决方法： 过滤掉低词频 使用词粒度代替字粒度。 还是使用上面的句子”我喜欢喝咖啡“，如果使用子粒度的2-gram，那么产生的特征是“我喜”，“喜欢”，“欢喝”，“喝咖”，“咖啡”。如果使用词粒度为2-gram，那么产生的特征是“我喜欢”，“喜欢喝”，“喝咖啡”。 补充一句，subwords就是一个词的character-level的n-gram。比如单词”hello”，长度至少为3的char-level的ngram有”hel”,”ell”,”llo”,”hell”,”ello”以及本身”hello”。 Negative Sampling该 technique 主要是减轻计算量的角度考虑的，每次让一个训练样本仅仅更新一部分的权重参数，这个技术不是 fastText 首创的，但是本着总结知识点的，也就放在这里了。 CBOW / Skip-gram模型 （这个论文中）提出了两种方法，一种是Hierarchical Softmax，另一种是Negative Sampling。论文中提出的两种方法都是用来提高计算效率的，下面说一下负采样。 在训练神经网络时，每当接受一个训练样本，然后调整所有神经单元权重参数，来使神经网络预测更加准确。换句话说，每个训练样本都将会调整所有神经网络中的参数。而 Negative Sampling 每次让一个训练样本仅仅更新一小部分的权重参数，从而降低梯度下降过程中的计算量。如果 vocabulary 大小为1万时， 当输入样本 ( “fox”, “quick”) 到神经网络时， “ fox” 经过 one-hot 编码，在输出层我们期望对应 “quick” 单词的那个神经元结点输出 1，其余 9999 个都应该输出0。在这里，这9999个我们期望输出为0的神经元结点所对应的单词我们称为 negative word，随机选择一小部分的 negative words，比如选 10个 negative words 来更新对应的权重参数。 解决的问题，在最后一层 softmax 的计算量太大，相当于每一次word 都是需要整个dict 量的级别的更新。然后选择 k 个negative words，只是计算这些softmax 的值。 Training a neural network means taking a training example and adjusting all of the neuron weights slightly so that it predicts that training sample more accurately. In other words, each training sample will tweak all of the weights in the neural network.As we discussed above, the size of our word vocabulary means that our skip-gram neural network has a tremendous number of weights, all of which would be updated slightly by every one of our billions of training samples!Negative sampling addresses this by having each training sample only modify a small percentage of the weights, rather than all of them. Here’s how it works.When training the network on the word pair (“fox”, “quick”), recall that the “label” or “correct output” of the network is a one-hot vector. That is, for the output neuron corresponding to “quick” to output a 1, and for all of the other thousands of output neurons to output a 0.With negative sampling, we are instead going to randomly select just a small number of “negative” words (let’s say 5) to update the weights for. (In this context, a “negative” word is one for which we want the network to output a 0 for). We will also still update the weights for our “positive” word (which is the word “quick” in our current example).The paper says that selecting 5-20 words works well for smaller datasets, and you can get away with only 2-5 words for large datasets.Recall that the output layer of our model has a weight matrix that’s 300 x 10,000. So we will just be updating the weights for our positive word (“quick”), plus the weights for 5 other words that we want to output 0. That’s a total of 6 output neurons, and 1,800 weight values total. That’s only 0.06% of the 3M weights in the output layer!In the hidden layer, only the weights for the input word are updated (this is true whether you’re using Negative Sampling or not). 对应的参数 123-wordNgrams 最大ngram 1-minn 字符ngram最小长度 0-maxn 字符ngram最大长度 0 其中 wordNgrams 是对应语序，字符minn 和maxn 是解决oov 问题。 只用unigram的话会丢掉word order信息，所以通过加入N-gram features进行补充 用hashing来减少N-gram的存储由于n-gram的量远比word大的多，完全存下所有的n-gram也不现实。Fasttext采用了Hash桶的方式，把所有的n-gram都哈希到buckets个桶中，哈希到同一个桶的所有n-gram共享一个embedding vector。如下图所示： Positive samples and Negative samplesOne little detail that’s missing from the description above is how do we select the negative samples.（下面说的是如何进行选择negative sample的问题：基本思路是根据出现频率进行选择）The negative samples are chosen using the unigram distribution. Essentially, the probability of selecting a word as a negative sample is related to its frequency, with more frequent words being more likely to be selected as negative samples. Instead of using the raw frequency in the original word2vec paper, each word is given a weight that’s equal to it’s frequency (word count) raised to the 3/4 power. The probability for selecting a word is just it’s weight divided by the sum of weights for all words.$$P \left( w _ { i } \right) = \frac { f \left( w _ { i } \right) ^ { 3 / 4 } } { \sum _ { i = 0 } ^ { n } \left( f \left( w _ { j } \right) ^ { 3 / 4 } \right) }$$ 上述中的函数是幂函数，图像的形状和log 函数差不多，都是从 $y =x$ 进行了一下约束，函数变得更加的平缓。对于高频词进行了约束，对于低频次也有机会出现。 This decision to raise the frequency to the 3/4 power appears to be empirical; as the author claims it outperformed other functions (e.g. just using unigram distribution).Side note: The way this selection is implemented in the original word2vec C code is interesting. They have a large array with 100M elements (which they refer to as the unigram table). They fill this table with the index of each word in the vocabulary multiple times, and the number of times a word’s index appears in the table is given by Then, to actually select a negative sample, we just generate a random integer between 0 and 100M, and use the word at that index in the table. Since the higher probability words occur more times in the table, we’re more likely to pick those. 这个也是有讲 任何进行negative sample的选择http://jalammar.github.io/illustrated-word2vec/一般来说在 word2vec 中context 是会选择到 5，然后这个 positive / negative sample 会是(1/6), 然后 nagative sample 是随机在 dictionary里面选的（所以有可能选到 positive sample）， 这个是这个dictionary 是根据频率，出现次数越多的，被选中的可能性也越大。The number of negative samples is another factor of the training process. The original paper prescribes 5-20 as being a good number of negative samples. It also states that 2-5 seems to be enough when you have a large enough dataset. The Gensim default is 5 negative samples. To address this, we need to introduce negative samples to our dataset – samples of words that are not neighbors. Our model needs to return 0 for those samples. Now that’s a challenge that the model has to work hard to solve – but still at blazing fast speed.This idea is inspired by Noise-contrastive estimation. We are contrasting the actual signal (positive examples of neighboring words) with noise (randomly selected words that are not neighbors). This leads to a great tradeoff of computational and statistical efficiency. Essentially, the probability for selecting a word as a negative sample is related to its frequency, with more frequent words being more likely to be selected as negative samples. They fill this table with the index of each word in the vocabulary multiple times, and the number of times a word’s index appears in the table is given by $P(wi)*P(wi)$ table_size. Then, to actually select a negative sample, you just generate a random integer between 0 and 100M, and use the word at that index in the table. Since the higher probability words occur more times in the table, you’re more likely to pick those. (ps 这种数量比不是 1：1，常常是 positive ： negative =1：5， 这个是经验值，在传统机器学习中可能认为是 data unbalanced)It’s now time to build out our skip-gram generator which will give us pair of words and their relevance (word, word in the same window), with label 1 (positive samples). (word, random word from the vocabulary), with label 0 (negative samples). 使用第一个应用场景：词向量。fastText作为训练词向量认为可以有两种模式，一种是根据周围词语预测中心词汇的CBOW （continuous bag-of-words）模型，另一种是根据中心词汇预测上下文的 skip-gram 模型。 ./fasttext – It is used to invoke the FastText library. skipgram/cbow – It is where you specify whether skipgram or cbow is to be used to create the word representations. -input – This is the name of the parameter which specifies the following word to be used as the name of the file used for training. This argument should be used as is. data.txt – a sample text file over which we wish to train the skipgram or cbow model. Change this name to the name of the text file you have. -output – This is the name of the parameter which specifies the following word to be used as the name of the model being created. This argument is to be used as is. model – This is the name of the model created.Running the above command will create two files named model.bin and model.vec. model.bin contains the model parameters, dictionary and the hyperparameters and can be used to compute word vectors. model.vec is a text file that contains the word vectors for one word per line. 最后生成有两个文件，一个 xxx.bin 文件，一个是 xxx.vec 文件，前者是预训练模型，后者是词向量。这两个可能是最重要的格式了。 The most important parameters of the model are its dimension and the range of size for the subwords. 常见的代码格式： ./fasttext skipgram -input data/fil9 -output result/fil9 -minn 2 -maxn 5 -dim 300 跑偏一下说一下shell的小技巧。使用echo 或者 &lt; 这样进行单个词或者多个词的词向量的查询。 ./fasttext print-word-vectors model.bin &lt; queries.txtecho “word” | ./fasttext print-word-vectors model.bin Finding simialr words: ./fasttext nn model.bin 最重要的几个参数： The most important parameters of the model are its dimension and the range of size for the subwords. The dimension (dim) controls the size of the vectors, the larger they are the more information they can capture but requires more data to be learned. But, if they are too large, they are harder and slower to train. By default, we use 100 dimensions, but any value in the 100-300 range is as popular. The subwords are all the substrings contained in a word between the minimum size (minn) and the maximal size (maxn). By default, we take all the subword between 3 and 6 characters, but other range could be more appropriate to different languages: 1$ ./fasttext skipgram -input data/fil9 -output result/fil9 -minn 2 -maxn 5 -dim 300 The following arguments for the dictionary are optional: -minCount 词出现的最少次数 [5] -minCountLabel 标签出现的最少次数 [0] -wordNgrams 单词 ngram 的最大长度 [1] -bucket 桶的个数 [2000000] -minn char ngram 的最小长度 [3] -maxn char ngram 的最大长度 [6] The following arguments for training are optional -dim 字向量的大小 [100] -ws 上下文窗口的大小 [5] -epoch 迭代次数 [5] -neg 负样本个数 [5] -loss 损失函数 {ns, hs, softmax} [ns] 第二个应用场景：文本分类。 Sentiment analysis and email classification are classic examples of text classification （BERT 也是采用的这种label 的格式）在训练数据集中label 默认是使用 “__label__” 进行表示的，当然也是可以进行自定义的。 ./fasttext supervised -input train.ft.txt -output model_kaggle -label __label__ -lr 0.5 就是进行predict的时候，有时候并不是很能想起来只是predict top 3 这样的东西。 # Predicting on the test dataset ./fasttext predict model_kaggle.bin test.ft.txt # Predicting the top 3 labels ./fasttext predict model_kaggle.bin test.ft.txt 3 fasttext VS. CBOW在标准的多核CPU上， 能够训练10亿词级别语料库的词向量在10分钟之内，能够分类有着30万多类别的50多万句子在1分钟之内。 n-gram n-gram 是一种基于语言模型的算法，基本思想是将文本内容按照字节顺序进行大小为N的滑动窗口操作，最终形成长度为N的字节片段序列。 CBOW 是和词序无关的，实现 n-gram 作为额外的特征可以捕捉一些部分的词序。fastText是一种基于skip-gram模型的新扩展，它会使用subword的信息，将每个词被表示成一个字符级n-gram词袋(a bag of character n-grams)。每个向量表示与每个字符级n-gram相关联，而词(word)则可以看成是这些n-gram向量表示的求和(sum)。fastText在大语料上训练很快。 网络结构方面 输入层：CBOW 的输入层是由目标词汇 $y$ 的上下文单词 ${ x _ { 1 } , \ldots , x _ { c } }$ 组成， $\boldsymbol { x } _ { i }$ 是被 onehot 编码过的 V 维向量，其中 V 是词汇量。而fasttext 的输入是多个单词及其n-gram特征。比如说，对于单词“apple”，假设n的取值为3，则它的trigram有: “&lt;ap”, “app”, “ppl”, “ple”, “le&gt;”其中，&lt;表示前缀，&gt;表示后缀。于是，我们可以用这些trigram来表示“apple”这个单词，进一步，我们可以用这5个trigram的向量叠加来表示“apple”的词向量。这样做有两点好处： 对于低频词生成的词向量效果会更好。因为它们的n-gram可以和其它词共享。 对于训练词库之外的单词，仍然可以构建它们的词向量。我们可以叠加它们的字符级n-gram向量。 从输入层到隐藏层，CBOW会将上下文单词向量叠加起来并经过一次矩阵乘法（线性变化）并应用激活函数，而fastText省略了这一过程，直接将embedding过的向量特征求和取平均； 两者都是使用的层次softmax，word2vec 最后的叶子节点是词典中的单词；而fasttext 针对多类有监督训练，将最后的叶子节点改为标签，并且基于哈夫曼树，类别多的标签的路径比较短。 使用 fasttext 进行文本分类的时候，其核心思想是 将整篇文档的词及n-gram向量叠加平均得到文档向量，然后使用文档向量做softmax多分类。 层次softmax softmax 是在 逻辑回归 （logistic regression） 在多分类任务上的推广，是网络中的最后一层。当 词汇数量V 较大时候，softmax 的计算代价是很大的， O(v) 量级。层次softmax 是将全局多分类转化成了若干个二分类问题，从而将时间复杂度从O(V) 转化成了 O(log V)。 缺点：fastText适用与分类类别非常大而且数据集足够多的情况，当分类类别比较小或者数据集比较少的话，很容易过拟合。 faiss用途：相似度检测和稠密向量的聚类。 Faiss is a library for efficient similarity search and clustering of dense vectors. 之前的实习经历主要是用faiss 处理文本的representation，但是这个是有偏差的，凡是能够打成词向量，都是可以使用faiss 进行计算的，当然这词向量需要满足：相近内容在相近的空间。 Once the vectors are extracted by learning machinery (from images, videos, text documents, and elsewhere), they’re ready to feed into the similarity search library. faiss的实现过程首先使用 index对于向量进行预处理，然后选择不同的模式. 主要讲的是三种模式，一个维度是简单模式，适合在小数据上进行计算 欧氏距离；一个维度是加快检索速度，这种模式下是需要提前的train，其基本的思路对向量进行聚类，当然文中说的是 “细胞”，建立倒排索引，然后检索的时候，搜索这个“细胞”内 和周围的“细胞” 的id 的集合，就可以返回前 K 个最相近的结果；最后一个维度是减少内存的使用，上面两种都是使用的完整的向量，这个模式下是使用的压缩向量，可以使用PCA 进行实现，当然这个模式下得到的结果也是近似解。还有两种计算的上的优化，对于向量进行分段计算，这种可以实现并行，并且支持任务在GPU 上进行运算。 牺牲了一些精确性来使得运行速度更快。 Similarity search can be made orders of magnitude faster if we’re willing to trade some accuracy; that is, deviate a bit from the reference result. For example, it may not matter much if the first and second results of an image similarity search are swapped, since they’re probably both correct results for a given query. Accelerating the search involves some pre-processing of the data set, an operation that we call indexing. ( 下面这句话的观点是什么，感觉不知道逻辑在哪里啊)向量的比较有两种metric：一种是L2 一种是基于consine (点乘)进行检索。前者是求解最小的值，后者是通过inner——product 求解maximum. 并且是支持GPU的，在原来CPU上建立的index，然后很好的迁移到 GPU上。 faiss 中的三种基本索引 IndexFlatL2 基于brute-force计算向量的L2距离，就是暴搜。检索速度慢，适用于小数据量。 在计算上进行了优化，比如使用堆存储结构，寻找最接近的 K 个元素时候后，进行分段计算，把 d 维向量分成几段分别进行计算；建立倒排索引( id -contents) ，先使用聚类，然后再类内和相近的类进行寻找而非整个空间。 12345678910111213141516171819202122232425262728293031323334353637383940import numpy as npd = 64 # 维度nb = 100000 # 数据库大小nq = 10000 # 要搜索的querynp.random.seed(1234) # 确定种子，使随机数可重现xb = np.random.random((nb, d)).astype('float32')xb[:, 0] += np.arange(nb) / 1000. # 每一行的第一个列增加一个等差数列的对应项数xq = np.random.random((nq, d)).astype('float32')xq[:, 0] += np.arange(nq) / 1000.print(xq.shape) # (10000, 64)print(xb.shape) # (100000, 64)import faiss # make faiss availableindex = faiss.IndexFlatL2(d) # 构建FlatL2索引print(index.is_trained)print(index.ntotal)index.add(xb) # 向索引中添加向量。add操作如果没有提供id，则使用向量序号作为id。print(index.ntotal)k = 4 # 搜索多少个临近向量D, I = index.search(xb[:5], k) # 用xb的前五行本身自己搜索自己，完整性检查，用于测试print("I=")print(I)#I=#[[ 0 393 363 78 924]# [ 1 555 277 364 617]# [ 2 304 101 13 801]# [ 3 173 18 182 484]# [ 4 288 370 531 178]]# I输出类似于上面，每行对应着相应向量的搜索结果。k为多少就有多少列，distance低的排在前面。# 可以看到前五行的第一列确实是0~4print("D=")print(D)#[[0. 7.1751733 7.207629 7.2511625]# [0. 6.3235645 6.684581 6.7999454]# [0. 5.7964087 6.391736 7.2815123]# [0. 7.2779055 7.5279865 7.6628466]# [0. 6.7638035 7.2951202 7.3688145]]# 可以看到第一行第一列都是0，意思是向量与自己本身的距离为0D, I = index.search(xq, k) # 搜索print(I[:5]) # 最初五个向量查询的结果print(I[-5:]) # 最后五个向量查询的结果 IndexIVFFlat (加速搜索) 对于暴搜来说，海量数据搜索速度太慢，那么需要预训练把向量都聚类。这里使用IndexIVFFlat来加快搜索速度。IndexIVFFlat是faiss的倒排索引，把数据构成的向量空间切割为Voronoi细胞，每个向量落入其中一个Voronoi细胞中。在搜索时，只有查询x所在细胞中包含的数据库向量y与少数几个相邻查询向量进行比较。 训练的时候还需要有一个量化器，用于决定以什么方式将向量分配给Voronoi细胞。每个细胞由一个质心定义，找到一个向量所在的Voronoi细胞包括在质心集中找到该向量的最近邻居。 搜索方法有两个参数： nlist 划分Voronoi细胞的数量 nprobe 执行搜索访问的单元格数(不包括nlist)，该参数调整结果速度和准确度之间折中的一种方式。如果设置nprobe=nlist则结果与暴搜一致。 加快索引的方式之一，与暴搜对比就是需要train，把向量空间下的数据切割为Voronoi细胞，检索只对向量所在细胞和周围细胞进行检索。 123456789101112131415161718192021222324252627import numpy as npd = 64 # dimensionnb = 100000 # database sizenq = 10000 # nb of queriesnp.random.seed(1234) # make reproduciblexb = np.random.random((nb, d)).astype('float32')xb[:, 0] += np.arange(nb) / 1000.xq = np.random.random((nq, d)).astype('float32')xq[:, 0] += np.arange(nq) / 1000.import faissnlist = 100k = 4quantizer = faiss.IndexFlatL2(d) # 内部的索引方式index = faiss.IndexIVFFlat(quantizer, d, nlist, faiss.METRIC_L2)# here we specify METRIC_L2, by default it performs inner-product searchprint("before train")assert not index.is_trainedindex.train(xb)assert index.is_trainedprint("before add")index.add(xb) # add may be a bit slower as wellD, I = index.search(xq, k) # actual searchprint(I[-5:]) # neighbors of the 5 last queriesindex.nprobe = 10 # default nprobe is 1, try a few moreD, I = index.search(xq, k)print(I[-5:]) # neighbors of the 5 last queries IndexIVFPQ (减少内存使用) 上面两种索引都是存储的完整向量，下面介绍一种压缩向量的方法。IndexIVFPQ基于PQ (Product Quantizer)算法压缩向量。在这种情况下，由于向量没有精确存储，搜索方法返回的距离也是近似值。上面我们看到的索引IndexFlatL2和IndexIVFFlat都会全量存储所有的向量在内存中，为满足大的数据量的需求，faiss提供一种基于Product Quantizer(乘积量化)的压缩算法编码向量大小到指定的字节数。此时，存储的向量时压缩过的，查询的距离也是近似的。 原理：简单来说就是通过PCA将高纬空间转换成低维空间。 原来的数据 train 得到一个转换矩阵P，然后这个矩阵和原来的数据X得到新的降维之后的Y ($PX =Y$)。这样转换过程中信息损失的更少，在faiss 中使用 train() 函数进行实现。 123456789101112131415161718192021222324252627282930313233343536373839import numpy as npd = 64 # dimensionnb = 100000 # database sizenq = 10000 # nb of queriesnp.random.seed(1234) # make reproduciblexb = np.random.random((nb, d)).astype('float32')xb[:, 0] += np.arange(nb) / 1000.xq = np.random.random((nq, d)).astype('float32')xq[:, 0] += np.arange(nq) / 1000.import faissnlist = 100m = 8k = 4quantizer = faiss.IndexFlatL2(d) # 内部的索引方式index = faiss.IndexIVFPQ(quantizer, d, nlist, m, 8)# 每个向量都被编码为8个字节大小index.train(xb)index.add(xb)D, I = index.search(xb[:5], k) # sanity checkprint(I)print(D)#[[ 0 78 714 372]# [ 1 1063 555 277]# [ 2 304 134 46]# [ 3 773 64 8]# [ 4 288 531 827]]#[[1.6675376 6.1988335 6.4136653 6.4228306]# [1.4083313 6.023788 6.025648 6.284443 ]# [1.6988016 5.592166 6.139589 6.6717234]# [1.7987373 6.625978 6.7166452 6.865783 ]# [1.5371588 5.7953157 6.38059 6.4141784]]# 可以看到确实搜索到了正确的结果，但是第一行第一列的distance不为零，属于有损压缩。# 虽然与接下来的几列（其他几个搜索结果）对比还是有几倍的优势。index.nprobe = 10 # 与以前的方法相比D, I = index.search(xq, k) # searchprint(I[-5:]) 在涉及index使用考虑速度，acc和内存大小三个不同的维度。然后不同的index 是有不同的侧重的。 安装参考 Take Off(这个是有三方面需要权衡的： query time、 query accuracy and preprocessing time) As with anything, there is a tradeoff between improving query time versus query accuracy versus preprocessing/index build time versus data storage: no build time, high query time, high storage, exact accuracy: Faiss IndexFlat low build time, med query time, high storage, high accuracy: Faiss IndexIVFFlat med build time, low query time, low-med storage, med-high accuracy: Faiss IndexIVFPQ very high build time, low query time, low-high storage (whether stored as a k-NN graph or raw data), high accuracy: NN-Descent by Dong et al. (e.g., nmslib) IndexIVFPQ with perhaps IMI is typically what we concentrate on, seems to be a reasonable sweet spot for billion-scale datasets. product quantization 算法这里的乘积是指笛卡尔积（Cartesian product），意思是指把原来的向量空间分解为若干个低维向量空间的笛卡尔积，并对分解得到的低维向量空间分别做量化（quantization）。这样每个向量就能由多个低维空间的量化code组合表示。 The idea is to decomposes the space into a Cartesian product of low dimensional subspaces and to quantize each subspace separately. A vector is represented by a short code composed of its subspace quantization indices. Image Vector Dataset: 存储的是离 embedding 最近的centroid (质心) 的编号 而非向量本身。 Let’s say you have a collection of 50,000 images, and you’ve already performed some feature extraction with a convolutional neural network, and now you have a dataset of 50,000 feature vectors with 1,024 components each. The first thing we’re going to do is compress our dataset. The number of vectors will stay the same, but we’ll reduce the amount of storage required for each vector. Note that what we’re going to do is not the same as “dimensionality reduction”! This is because the values in the compressed vectors are actually symbolic rather than numeric, so we can’t compare the compressed vectors to one another directly. Two important benefits to compressing the dataset are that (1) memory access times are generally the limiting factor on processing speed, and (2) sheer memory capacity can be a problem for big datasets. Here’s how the compression works. For our example we’re going to chop up the vectors into 8 sub-vectors, each of length 128 (8 sub vectors x 128 components = 1,024 components). This divides our dataset into 8 matrices that are [50K x 128] each. These centroids are like “prototypes”. They represent the most commonly occurring patterns in the dataset sub-vectors. We’re going to use these centroids to compress our 1 million vector dataset. Effectively, we’re going to replace each subregion of a vector with the closest matching centroid, giving us a vector that’s different from the original, but hopefully still close. Doing this allows us to store the vectors much more efficiently—instead of storing the original floating point values, we’re just going to store cluster ids. For each subvector, we find the closest centroid, and store the id of that centroid. Each vector is going to be replaced by a sequence of 8 centroid ids. I think you can guess how we pick the centroid ids–you take each subvector, find the closest centroid, and replace it with that centroid’s id. Note that we learn a different set of centroids for each subsection. And when we replace a subvector with the id of the closest centroid, we are only comparing against the 256 centroids for that subsection of the vector. Because there are only 256 centroids, we only need 8-bits to store a centroid id. Each vector, which initially was a vector of 1,024 32-bit floats (4,096 bytes) is now a sequence of eight 8-bit integers (8 bytes total per vector!). 总的来说faiss 高效实现了PCA 算法, k-means 算法 和PQ 算法。 ref 1ref 2ref 3ref 4 K-means 算法K-means 是一种聚类算法，属于无监督学习算法，先说一下什么是聚类：聚类分析是在数据中发现数据对象之间的关系，将数据进行分组，组内的相似性越大，组间的差别越大，则聚类效果越好。 K-Means算法思想：对给定的样本集，事先确定聚类簇数K （超参数），让簇内的样本尽可能紧密分布在一起，使簇间的距离尽可能大。该算法试图使集群数据分为n组独立数据样本，使n组集群间的方差相等，数学描述为最小化惯性或集群内的平方和。K-Means作为无监督的聚类算法，实现较简单，聚类效果好，因此被广泛使用。 算法步骤 创建k个点作为k个簇的起始质心（经常随机选择）。 分别计算剩下的元素到k个簇中心的相异度（距离），将这些元素分别划归到相异度最低的簇。 根据聚类结果，重新计算k个簇各自的中心，计算方法是取簇中所有元素各自维度的算术平均值。 将D中全部元素按照新的中心重新聚类。 重复第4步，直到聚类结果不再变化。 最后，输出聚类结果。 K-Means算法优缺点 优点 原理易懂、易于实现； 当簇间的区别较明显时，聚类效果较好； Trains quickly 缺点 当样本集规模大时，收敛速度会变慢； 对噪声或者离群点比较敏感 k的取值十分关键，对不同数据集，k选择没有参考性，需要大量实验 初始质心的选择，不同的质心选择可能得到完全不同的结果，因为k-means 可以保证的是局部最优解，而不是全局最优解。针对该缺陷，可以使用 kmeans++ 算法解决。k-means++ 算法选择seeds的基本思路是：初始化聚类中心之间的相互距离要尽可能的远。 时间复杂度$O(mnkt)$， 其中$t$ 为迭代刺手， $k$ 为簇的数目， $m$ 为数据量, $n$ 为数据的维度。对于大数据来说，$t$，$k$和$n$ 都是可以看做常数，那么时间复杂度近似就是 $O(n)$，所以该算法是相当高效的。 空间复杂度$O(m+k)n$ 其中 $k$ 为簇的数目， $m$为数据量， $n$ 为数据的维度。同理对于空间复杂度也可以这样分析。最后的结论可以简化为：时空复杂度是$O(n)$ Choosing K The algorithm explained above finds clusters for the number k that we chose. So, how do we decide on that number? 尝试法： 计算每个点到最近的簇的距离的总和，如果增加 k 导致的总和下降不明显，那么就接近临界点了。 To find the best k we need to measure the quality of the clusters. The most traditional and straightforward method is to start with a random k, create centroids, and run the algorithm as we explained above. A sum is given based on the distances between each point and its closest centroid. As an increase in clusters correlates with smaller groupings and distances, this sum will always decrease when k increases; as an extreme example, if we choose a k value that is equal to the number of data points that we have, the sum will be zero. The goal with this process is to find the point at which increasing k will cause a very small decrease in the error sum, while decreasing k will sharply increase the error sum. This sweet spot is called the “elbow point.” In the image below, it is clear that the “elbow” point is at k-3.­ 使用场景：一般在数据分析的前期使用，选择适当的 $K$，分析不同聚类数据下的特点。 k-means 是以欧式距离作为相似度测量，要求某一初始化聚类中心分类效果最好。算法采用误差平方和作为准侧函数，优化的是该函数。 欧式距离和曼哈顿距离（L1距离）都可以表示最近邻居之间的距离，如何进行选择？前者适合在归一化无量纲的情况下进行使用，两者不具有相互替代性。 代码实现 自己实现了一个 k-means 计算过程；然后使用 sklearn 中实现好的框架。没有比这个写得更好的了。代码链接 代码摘要1234567891011121314151617181920212223242526272829## Initialisationimport pandas as pdimport numpy as npimport matplotlib.pyplot as plt%matplotlib inlinedf = pd.DataFrame(&#123; 'x': [12, 20, 28, 18, 29, 33, 24, 45, 45, 52, 51, 52, 55, 53, 55, 61, 64, 69, 72], 'y': [39, 36, 30, 52, 54, 46, 55, 59, 63, 70, 66, 63, 58, 23, 14, 8, 19, 7, 24]&#125;)np.random.seed(200)k = 3# centroids[i] = [x, y]# 学习开源代码，在于向别人学习，看人家是如何写代码的，怎么写得简单高效，漂亮centroids = &#123; i+1: [np.random.randint(0, 80), np.random.randint(0, 80)] for i in range(k)&#125; fig = plt.figure(figsize=(5, 5))plt.scatter(df['x'], df['y'], color='k')colmap = &#123;1: 'r', 2: 'g', 3: 'b'&#125;for i in centroids.keys(): plt.scatter(*centroids[i], color=colmap[i])plt.xlim(0, 80)plt.ylim(0, 80)plt.show() 1234567891011121314151617def assignment(df, centroids): for i in centroids.keys(): # sqrt((x1 - x2)^2 - (y1 - y2)^2) df['distance_from_&#123;&#125;'.format(i)] = ( np.sqrt( (df['x'] - centroids[i][0]) ** 2 + (df['y'] - centroids[i][1]) ** 2 ) ) centroid_distance_cols = ['distance_from_&#123;&#125;'.format(i) for i in centroids.keys()] df['closest'] = df.loc[:, centroid_distance_cols].idxmin(axis=1)#得到的是id 标识 df['closest'] = df['closest'].map(lambda x: int(x.lstrip('distance_from_'))) # 这个转换成数字 df['color'] = df['closest'].map(lambda x: colmap[x]) # 注意lambda 函数 x 是输入参数，: 之后是处理的函数 return dfdf = assignment(df, centroids)print(df.head()) 1234567891011121314151617181920212223242526## Update Stageimport copyold_centroids = copy.deepcopy(centroids)def update(k): for i in centroids.keys(): centroids[i][0] = np.mean(df[df['closest'] == i]['x']) centroids[i][1] = np.mean(df[df['closest'] == i]['y']) return kcentroids = update(centroids)# 这个可视化的箭头也是做得相当的棒fig = plt.figure(figsize=(5, 5))ax = plt.axes()plt.scatter(df['x'], df['y'], color=df['color'], alpha=0.5, edgecolor='k')for i in centroids.keys(): plt.scatter(*centroids[i], color=colmap[i])plt.xlim(0, 80)plt.ylim(0, 80)for i in old_centroids.keys(): old_x = old_centroids[i][0] old_y = old_centroids[i][1] dx = (centroids[i][0] - old_centroids[i][0]) * 0.75 dy = (centroids[i][1] - old_centroids[i][1]) * 0.75 ax.arrow(old_x, old_y, dx, dy, head_width=2, head_length=3, fc=colmap[i], ec=colmap[i])plt.show() 12345678910111213141516# Continue until all assigned categories don't change any morewhile True: closest_centroids = df['closest'].copy(deep=True) centroids = update(centroids) df = assignment(df, centroids) # 不同的语言之间是相同的，要么是值比较，那么是引用比较，显然这里是值比较 if closest_centroids.equals(df['closest']): breakfig = plt.figure(figsize=(5, 5))plt.scatter(df['x'], df['y'], color=df['color'], alpha=0.5, edgecolor='k')for i in centroids.keys(): plt.scatter(*centroids[i], color=colmap[i]) # 如意可以传入一个list，表示两个参数plt.xlim(0, 80)plt.ylim(0, 80)plt.show() KNNK最近邻(k-Nearest Neighbor，KNN) 是有监督分类学习，根据K 个最近邻的类别信息，通过投票的方式决定刚进来的数据点的类别。和KNN 容易相混淆的是K-means算法，具体可以参考上面的描述。KNN 中的K 表示K个最近邻是有投票权的，根据K 个最近邻的投票然后决定新加入的点类别信息。 In short, the algorithms are trying to accomplish different goals. K-nearest neighbor is a subset of supervised learning classification (or regression) algorithms (it takes a bunch of labeled points and uses them to learn how to label other points). It is supervised because you are trying to classify a point based on the known classification of other points. In contrast, K-means is a subset of unsupervised learning clustering algorithms (it takes a bunch of unlabeled points and tries to group them into clusters). It is unsupervised because the points have no external classification. The $ k $ in each case mean different things. In K-NN, the $ k $ represents the number of neighbors who have a vote in determining a new player’s position. The $ k $ in K-means, determine the number of clusters we want to end up. In a K-NN algorithm, a test sample is given as the class of majority of its nearest neighbours. For example, if we have three classes and the goal is to find a class label for the unknown example $ x_j $ then, by using the Euclidean distance and a value of $ k=5 $ neighbors, the unknown sample is classified to the category of the most voted neighbors. How it works?Step 1: Determine the value for KStep 2: Calculate the distances between the new input (test data) and all the training data. The most commonly used metrics for calculating distance are Euclidean, Manhattan and MinkowskiStep 3: Sort the distance and determine k nearest neighbors based on minimum distance valuesStep 4: Analyze the category of those neighbors and assign the category for the test data based on majority voteStep 5: Return the predicted class The situation with K-means is that, given some data you will cluster them in k-groups or clusters. The initial step of the algorithm is to randomly spawn $ k $ centroids (centers). At every iteration the center of each cluster is moved slightly to minimize the objective function. The algorithm will terminate if the iterations are maximized or if the centroids stop to move. The objective function of K-means is $ J = \sum_{j=1}^{k}\sum_{i=1}^{n}\left |x_i^{j}-c_j \right |^{2} $ How it works?Step 1: Determine K value by Elbow method and specify the number of clusters KStep 2: Randomly assign each data point to a clusterStep 3: Determine the cluster centroid coordinatesStep 4: Determine the distances of each data point to the centroids and re-assign each point to the closest cluster centroid based upon minimum distanceStep 5: Calculate cluster centroids againStep 6: Repeat steps 4 and 5 until we reach global optima where no improvements are possible and no switching of data points from one cluster to other. 上图a表达了初始的数据集，假设k=2。在图b中，我们随机选择了两个k类所对应的类别质心，即图中的红色质心和蓝色质心，然后分别求样本中所有点到这两个质心的距离，并标记每个样本的类别为和该样本距离最小的质心的类别，如图c所示，经过计算样本和红色质心和蓝色质心的距离，我们得到了所有样本点的第一轮迭代后的类别。此时我们对我们当前标记为红色和蓝色的点分别求其新的质心，如图4所示，新的红色质心和蓝色质心的位置已经发生了变动。图e和图f重复了我们在图c和图d的过程，即将所有点的类别标记为距离最近的质心的类别并求新的质心。最终我们得到的两个类别如图f。 EM 算法先验概率和后验概率 事件发生前的预判概率。可以是基于历史数据的统计，可以由背景常识得出，也可以是人的主观观点给出。一般都是单独事件概率，如 $P(x) $, $P(y)$。事件发生后求的反向条件概率；或者说，基于先验概率求得的反向条件概率。条件概率：一个事件发生后另一个事件发生的概率。 先验概率是指根据以往经验和分析得到的概率,它往往作为”由因求果”问题中的”因”出现，比如全概率公式。 后验概率是指依据得到”结果”信息所计算出的最有可能是那种事件发生,如贝叶斯公式中的,是”执果寻因”问题中的”因”， 比如贝叶斯公式。 极大似然估计 极大似然估计是用来求解概率分布中参数值。为什么要求解参数值？因为大多数概率分布都是由关键的参数值， 得到分布就可以计算概率值，那么进行分类和回归就没有什么问题。常见的概率分布比如二项分布是由 $p $控制，正太分布是由 $\mu$ 和$\sigma$控制。极大似然估计理论认为，概率越大，发生的可能性就越大。 该算法背景： 有一个独立同分布的样本集 D，且样本从数据分布 $p(x | \theta)$ 中抽取 想要计算 $\theta$ 解决思路： 假设 D 中所有的样本都是独立从 $ p(x | \theta)$ 中抽取，那么：$$P({X=x^{1}, X=x^{2}, \cdots, X=x^{n}})=\prod_{i=1}^{n} p(x^{i} | {\theta})$$记等号后面的式子为似然函数 因为乘积不方便处理，所以上面式子中左右两边求对数 $$\ln l(\boldsymbol{\theta})=\ln \prod_{i=1}^{n} p\left(x^{i} | \boldsymbol{\theta}\right)=\sum_{i=1}^{n} \ln p\left(x^{i} | \boldsymbol{\theta}\right)$$ 根据算法的思想，我们的目标就是最大化 $L(\theta)$， 最大化的 $\theta$ 就是我们要求的。如何最大化 $L(\theta)$？一般来说，如果如果只有一个参数，比如二项分布，那么求导令导数为0 就可以求$\theta$。如果有多个参数，比如正太分布，那么就需要求解偏导， 令偏导数为0. 极大似然估计的局限性 需要事先假定数据分布 假定的数据分布和真实的数据分布不一致的时候，容易出现较大的误差 介绍完了最大似然估计，那么后面是真正 EM （Expectation Maximization Algorithm）算法。 EM算法解决这个的思路是使用启发式的迭代方法，既然我们无法直接求出模型分布参数，那么我们可以先猜想隐含数据（EM算法的E步），接着基于观察数据和猜测的隐含数据一起来极大化对数似然，求解我们的模型参数（EM算法的M步)。由于我们之前的隐藏数据是猜测的，所以此时得到的模型参数一般还不是我们想要的结果。不过没关系，我们基于当前得到的模型参数，继续猜测隐含数据（EM算法的E步），然后继续极大化对数似然，求解我们的模型参数（EM算法的M步)。以此类推，不断的迭代下去，直到模型分布参数基本无变化，算法收敛，找到合适的模型参数。从上面的描述可以看出，EM算法是迭代求解最大值的算法，同时算法在每一次迭代时分为两步，E步和M步。一轮轮迭代更新隐含数据和模型分布参数，直到收敛，即得到我们需要的模型参数。一个最直观了解EM算法思路的是K-Means算法，见之前写的K-Means聚类算法原理。在K-Means聚类时，每个聚类簇的质心是隐含数据。我们会假设$𝐾$个初始化质心，即EM算法的E步；然后计算得到每个样本最近的质心，并把样本聚类到最近的这个质心，即EM算法的M步。重复这个E步和M步，直到质心不再变化为止，这样就完成了K-Means聚类。 EM算法能保证收敛吗？有充分的理论说明，这个是能够保证收敛的。 EM算法如果收敛，那么能保证收敛到全局最大值吗？不能保证，这个取决于初始化。初始值不同，那么最后的结果不同。所以这个是给定初始值，经过循环迭代，最终逼近真实值。 如果要讲解 EM 算法，可以认为 K-means 是其一个特例，那么进行理解。 复习笔记 使用二叉树的结构，时间复杂度从 $O(N) $ 优化到了$log_2(N)$，当使用huffman 树的时候，这种效果更加明显。层次softmax 不是fasttext 的首创，它的改进之处在实现的时候基于 huffman 树而不是普通的二叉树， 属于运算上的优化。利用了类别不均衡的特点，类别多的路径短，整体上的时间效率会提高。 N-gram 一种是基于character-level 对于不常见单词的扩充，解决的是OOV问题；一种是word-level，考虑的是词语周边的信息，加入了context 的信息，local context 的信息。 negative sampling 是解决最后softmax 层中，不更新所有的negative words，只是更新少部分单词，根据词频选择negative words，并且这种词频是经过约束，主要是使得低频词语也有出现的机会。 调参分为字典相关的参数和训练相关参数 fasttext 的和之前 CBOW的区别：网络结构中的输入层，CBOW是经过one-hot的上下文单词，而fasttext 是单词+ n-gram 的特征，在解决OOV效果比较好；另外在最后的输出层，基于huffman 树实现了层次softmax，对于类别不均衡的训练集来说，训练时间会变得更短。 fasttext 的缺点，使用文本分类的时候，当类别比较多的时候提升效果比较明显，否则是容易过拟合的。 faiss 三种模式或者说索引。一种简单模式在小的数据集上计算欧式距离；一种加快检索的速度，使用聚类算法，检索的时候只是检索id 所在的簇和周围的簇，不过这个过程是需要预训练的；一种是减少内存的时候，如果是求解近似解，那么不必存储完整的向量，使用pca 降维。还有比较通用的加快速度的方式，比如分段计算和使用gpu 进行计算。 关于k-means中选择聚类簇k的个数的算法：尝试法。如果增大k，发现并不能使得指标明显的下降，这个时候就达到了阈值。指标：一个簇内所有的点到簇类中心的距离的总和。 knn 和k-means 的区别，前者是有监督的分类算法，根据测试点周围k 个点的类别信息判断该点的信息；k-means 是无监督算法，属于聚类中的一种。]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>k-means</tag>
        <tag>knn</tag>
        <tag>fasttext</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NLP中的碎碎念(1)]]></title>
    <url>%2F2019%2F03%2F25%2FNLP%E4%B8%AD%E7%9A%84%E7%A2%8E%E7%A2%8E%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[顾名思义，主要整理一下自己在文本处理中遇到的小的知识点，比如关键词提取技术，分词软件包的原理。 关键词提取TF-IDF这个是可以参看之前自己写的一个博客 卡方分布卡方检验是以χ2分布为基础的一种常用假设检验方法。该检验的基本思想是：首先假设$H_0$成立，基于此前提计算出χ2值，它表示观察值与理论值之间的偏离程度。 官方定义： 若k 个随机变量$Z_1$、……、$Z_k $相互独立，且数学期望为0、方差为 1(即服从标准正态分布)，则随机变量X$$X = \sum _ { n = 1 } ^ { k } Z _ { n } ^ { 2 }$$被称为服从自由度为 k 的卡方分布，记作$$X \sim \chi ^ { 2 } ( k )$$ 从直观的角度感受一下自由度k 和图像形状的关系，自由度越大卡方分布越接近正太分布。 用于特征选择 如果文章是否包含“篮球“与文章是否属于体育类别是独立无关的。且一个新闻文章属于体育类别的概率是0.609，那么可以得到下面的表格。假设文章是否包含“篮球“与文章是否属于体育类别是独立无关的，所以不管文章中是不是包含”篮球“，其属于体育类别的概率都是0.609。 列联表 组别 体育 非体育 包含”篮球“ 44 * 0.609 = 26.8 44 * 0.391 = 17.2 不包含”篮球“ 43 * 0.609 = 26.2 43 * 0.391 = 16.8 如果两个分类变量真的是独立无关的，那么四格表的实际值与理论值得差值应该非常小（有差值的原因是因为抽样误差）。那么如何衡量实际值与理论值得差值呢？ 步骤 统计每个词的正文档出现频率（A）、负文档出现频率（B）、正文档不出现频率）、负文档不出现频率。-计算每个词的卡方值 将每个词按卡方值从大到小排序，选取前k个词作为特征，k即特征维数。$$\operatorname { CHI } ( \mathrm { x } , \mathrm { y } ) = \chi ^ { 2 } ( x , y ) = \sum \frac { ( A - T ) ^ { 2 } } { T }$$该公式可以进一步简化成 （其中 x 表示一个特征，y 表示的target）$$\mathrm { CHI } ( \mathrm { x } , \mathrm { y } ) = \chi ^ { 2 } ( x , y ) = \frac { N ( A D - B C ) } { ( A + B ) ( A + C ) ( B + D ) ( C + D ) }$$ 组别 体育 非体育 合计 包含”篮球“ 34 (A) 10 (B) 44 (A+B) 不包含”篮球“ 19 (C) 24 (D) 43 (C+D) 合计 53(A +C) 34 (B+D) 87 (N) 卡方分布的临界值 自由度F = （行数 - 1） * （列数 - 1） = 1，对于四格表，F = 1。 由于自由度F = 1，所以只需要看分布表的第一行。可以看到，随着CHI的增大，原假设成立的概率就越小。因为10.10 &gt; 6.64，所以原假设成立是概率是小于1%。反之，也就是说，原假设不成立（即两个分类变量不是独立无关）的概率大于99% 如何应用于特征选择 CHI值越大，说明两个变量越不可能是独立无关的，也就是说X2越大，两个变量的相关程序也就越高。对于特征变量x1,x2,…,xn，以及分类变量y。只需要计算CHI(x1, y)、CHI(x2, y)、…、CHI(xn, y)，并按照CHI的值从大到小将特征排序，然后选择阈值，大于阈值的特征留下，小于阈值的特征删除。这样就筛选出一组特征子集了，接着使用这组特征子集去训练分类器，然后评估分类器的性能。 因为只要比较CHI值得相对大小，所以上述的分布表就没用了。 使用范围：一般是在离散变量上进行使用。 另外一个例子 In the case of classification problems where input variables are also categorical, we can use statistical tests to determine whether the output variable is dependent or independent of the input variables. If independent, then the input variable is a candidate for a feature that may be irrelevant to the problem and removed from the dataset. The Pearson’s chi-squared statistical hypothesis is an example of a test for independence between categorical variables. Contingency TableFor example, the Sex=rows and Interest=columns table with contrived counts might look as follows:下面是一个列联表123 Science, Math, ArtMale 20, 30, 15Female 20, 15, 30 The Pearson’s Chi-Squared test, or just Chi-Squared test for short, is named for Karl Pearson, although there are variations on the test. 如何去解读这种信息？ We can interpret the test statistic in the context of the chi-squared distribution with the requisite number of degress of freedom as follows: If Statistic &gt;= Critical Value: significant result, reject null hypothesis (H0), dependent.If Statistic &lt; Critical Value: not significant result, fail to reject null hypothesis (H0), independent.The degrees of freedom for the chi-squared distribution is calculated based on the size of the contingency table as: degrees of freedom: (rows - 1) * (cols - 1) In terms of a p-value and a chosen significance level (alpha), the test can be interpreted as follows: If p-value &lt;= alpha: significant result, reject null hypothesis (H0), dependent.If p-value &gt; alpha: not significant result, fail to reject null hypothesis (H0), independent.For the test to be effective, at least five observations are required in each cell of the contingency table. case study： Chi-square Test for feature selection $$X ^ { 2 } = \frac{ {(Observed frequency - Expected frequency)} ^ 2 } { Expected frequency }$$ 12345678910111213141516171819202122# Load libraries from sklearn.datasets import load_iris from sklearn.feature_selection import SelectKBest from sklearn.feature_selection import chi2 # Load iris data iris_dataset = load_iris() # Create features and target X = iris_dataset.data y = iris_dataset.target # Convert to categorical data by converting data to integers X = X.astype(int) # Two features with highest chi-squared statistics are selected chi2_features = SelectKBest(chi2, k = 2) X_kbest_features = chi2_features.fit_transform(X, y) # Reduced features print('Original feature number:', X.shape[1]) print('Reduced feature number:', X_kbest.shape[1]) Original feature number: 4Reduced feature number : 2 而实际应用到特征选择中的时候，我们不需要知道自由度，不要知道卡方分布，我们只需要根据算出来的χ2 进行排序就好了，越大我们就越喜欢！挑选最大的一堆，于是就完成了利用卡方检验来进行特征提取。卡方分布的缺点：没有考虑词频，它只统计文档是否出现词，而不管出现了几次。这会使得他对低频词有所偏袒（因为它夸大了低频词的作用）。 参考一参考二 CBOW和skip-gram举一个简单的小例子说明 CBOW和skip-gram的区别：skip-gram 是根据中心词汇然后预测上下文词汇，这个不是一下子输入上下文词汇的，而是一个过程，中心词汇和上下文词汇1 ，中心词汇和上下文词汇2，这样的进行输入。 cbow 和其的区别，在于简单相加了上下文词汇作为一个整体，然后和中心词汇进行输入，所以最后是这样的结果。 使用skip gram训练的时间更长，但是对于出现频率不高的词汇，效果比较好。但CBOW的训练速度是相对来说比较快一些。 分词软件常见的中文分词服务 分词服务 开源/商业 支持语言 词性标注 命名实体识别 jieba 开源 Python, Java, C++ 无 无 HanLP 开源 Python，Java， C++ 有 有 StandFord CoreNLP 开源 Java 百度NLP 商业 阿里NLP 商业 Stanford NLP是由斯坦福大学的 NLP 小组开源的 Java 实现的 NLP 工具包。可以使用python 调用 Stanford NLP 进行中文分词 HanLP是由一系列模型与算法组成的Java工具包。也可以使用Python调用HanLP进行中文分词。 结巴分词的算法策略 基于前缀词典实现高效的词图扫描，生成句子中汉字所有可能成词情况所构成的有向无环图 (DAG) 采用了动态规划查找最大概率路径, 找出基于词频的最大切分组合 对于未登录词，采用了基于汉字成词能力的 HMM 模型，使用了 Viterbi 算法 如果想更加详细的了解这种策略，可以参考这里. 中文分词主要掌握规则的分词方法（逆向最大匹配法（该方法是优于正向的）、有个词频-内连-外 的分词算法， 总共两种算法就可以了） 中文分词的困难主要体现在：歧义词的切分和未登录词识别。其中后者的影响要远远大于前者的影响。 下面是常见的分词方式，但是并没有说明哪个软件是什么分词方法。（需要掌握两种分词的方式，一种是基于词典的逆向最大匹配，一种是后面新词发现中的基于统计的信息熵算法） 基于规则的分词方法 这种方法又叫作机械分词方法、基于字典的分词方法，它是按照一定的策略将待分析的汉字串与一个“充分大的”机器词典中的词条进行匹配，比如说正向最大匹配法、逆向最大匹配法 基于统计的分词方法 该方法的主要思想：词是稳定的组合，因此在上下文中，相邻的字同时出现的次数越多，就越有可能构成一个词。因此字与字相邻出现的概率或频率能较好地反映成词的可信度。可以对训练文本中相邻出现的各个字的组合的频度进行统计，计算它们之间的互现信息。互现信息体现了汉字之间结合关系的紧密程度。当紧密程 度高于某一个阈值时，便可以认为此字组可能构成了一个词。该方法又称为无字典分词。 比如说CRF 算法。 基于语义的分词方法 语义分词法引入了语义分析，对自然语言自身的语言信息进行更多的处理，如扩充转移网络法、知识分词语义分析法、邻接约束法、综合匹配法、后缀分词法、特征词库法、矩阵约束法、语法分析法等。 逆向最大匹配分词是中文分词基本算法之一，因为是机械切分，所以它也有分词速度快的优点，且逆向最大匹配分词比起正向最大匹配分词更符合人们的语言习惯。逆向最大匹配分词需要在已有词典的基础上，从被处理文档的末端开始匹配扫描，每次取最末端的i个字符（分词所确定的阈值i）作为匹配字段，若匹配失败，则去掉匹配字段最前面的一个字，继续匹配。而且选择的阈值越大，分词越慢，但准确性越好。这种方法经常被用来解决歧义词的问题，所以单独说一下。 算法：事先设置一个k值，下面的程序k值设为5，然后从最后一个字开始向前截取k个字，先把这k个字和字典匹配，看能否找到匹配的词语，若不能，则剔除这k个字最左边的字，然后再把这k-1个字与字典匹配…一直到匹配成功，或者前k-1个字都没匹配成功，那就把第k个字当成一个独立的词，然后再向前移动分出来的词的长度，再截取k个字……一直到全部分好词为止。“我爱北京天安门” 先从后面开始截取k(这里是5)个字，然后把”北京天安门”五个字与字典匹配，字典中没有这个词，然后就去掉”北”字，把剩下的”京天安门”与字典匹配，字典中还是没有这个词，再去掉”京”，然后再把”天安门”与字典匹配，发现匹配到了这个词，于是就把”天安门”划为一个词语，然后指针向前移动三个字。再截取k个字，这里因为就剩下4个字了，所以就截取4个字，把”我爱北京”与字典匹配，没成功，去掉”我”，再把”爱北京”与字典匹配，还是没成功，再去掉”爱”，然后发现”北京”匹配成功，把”北京”划为一个词语，再把指针向前移动两个字， CRF算法基本思路是对汉字进行标注训练，不仅考虑了词语出现的频率，还考虑上下文，具备较好的学习能力，因此其对歧义词和未登录词的识别都具有良好的效果。 近年来，随着硬件计算能力的发展以及词的分布式表示（word embedding）的提出，神经网络可以有效处理许多NLP任务。这类方法对于序列标注任务（如CWS、POS、NER）的处理方式是类似的：将token从离散one-hot表示映射到低维空间中成为稠密的embedding，随后将句子的embedding序列输入到RNN中，用神经网络自动提取特征，Softmax来预测每个token的标签。这种方法使得模型的训练成为一个端到端的过程，而非传统的pipeline （管道，指的是一个包含多步骤的流水式的工作），不依赖于特征工程，是一种数据驱动的方法，但网络种类繁多、对参数设置依赖大，模型可解释性差。 推荐博客讲解 CRF算法，反正我是没有看懂，哈哈哈。【中文分词】条件随机场CRF, 浅谈分词算法（4）基于字的分词方法（CRF） 和 CRF算法学习——自己动手实现一个简单的CRF分词 分词粒度目前，分词主要包含细粒度分词和粗粒度分词两种，在不同的应用场景需要用到不同的粒度。细粒度分词是指将原始语句切分成最基本的词语，而粗粒度分词是指将原始语句中的多个基本词组合起来切成一个词，进而组成语义相对明确的实体。 原始串：浙江大学坐落在西湖旁边 细粒度：浙江/大学/坐落/在/西湖/旁边 粗粒度：浙江大学/坐落/在/西湖/旁边 举例说明分词粒度和应用场景：对于query为“耐克鞋”来说，词典中是包含这个实体词的，分词的切分结果就是“耐克鞋”。但是，考拉用来建索引的商品描述中，这三个字是没有连续出现的，自然就没有“耐克鞋”对应的文档，这个时候就无法召回结果。那么，这时候你会说那就使用最小粒度的分词就解决这个问题了。相反，某一款商品可能有“参数表”这三个字，如果有最小粒度的分词策略，分词的结果为“参数/表”。很不幸，当query为“表”时，你会发现会召回莫名其妙的结果。一般来说，是使用词频表来进行粒度控制，基本可以解决绝大多数问题。 搜索引擎展现与粒度：显而易见，粒度越小，展现就越多，建立倒排索引时，索引的长度就越长;粒度的层次越多，索引的数量就越多。一个多，一个长，就对搜索系统的性能构成了极大的考验。搜索引擎并不会对所有小粒度词都建索引，而是选择“更有可能展现相关结果”的小粒度词。所以在一般情况下，切分文本粒度越大，索引越多，相关性越好，但展现越少;切分文本粒度越小，索引越少，相关性越差，但展现越好。 词频决定分词粒度：具体算法是，当发现一个由多个短词组成的长词时，判断每个短词中最小的词频，如果这个词频还是大于长词的词频，则按该组合进行拆分。如果多种组合，按词频最大的组合拆分。如上面例子，”中央饭店”，中央的词频为1000，饭店为900,饭为200,店为600，而中央饭店为500 。 OOV问题由来：由于词频过小被替换成了 UNK token 或者是在原始的训练数据集中没有出现而在测试集中出现了。 常见的解决方案： subword (n-gram ) 分字和词语两个维度。西欧语系中是有字维度，在中文中没有。对于西欧语系的词汇，是可以通过 n-gram；如果推广到中文，那么更加合适使用分词之后的结果作为一个 word，然后每个字作为一个gram在训练过程中，每个n-gram都会对应训练一个向量，而原来完整单词的词向量就由它对应的所有n-gram的向量求和得到。所有的单词向量以及字符级别的n-gram向量会同时相加求平均作为训练模型的输入。优点：解决了 低频词 oov 问题；缺点：需要估计的参数量变多 transfer learning (fine tune) 使用 context embedding 去表示缺省的 中心词（oov） 文献 新词发现该算法最主要是基于互信息和左右熵的计算规则，而这两个概念都是出自信息论的范畴，其一者称内部凝和度，其二者称外部自由度或者边界自由度。在中文分词的世界里，最主要的挑战有两个：歧义词识别，未登录词（新词）识别。对于歧义词简单说一下，比如“乒乓球拍卖完了”，切分为以下两种情况都是合理的，“乒乓球拍/卖/完了”，“乒乓球/拍卖/完了”。这个就是典型的歧义词。而处理这种问题常用的一种手段是使用逆向最大匹配法去处理歧义词，由于汉语中偏正结构较多，若从后向前匹配，可以适当提高精确度。比如取出“南京市长江大桥”的后四个字“长江大桥”，发现词典中有匹配，切割下来；对剩余的“南京市”进行分词，整体结果为：“南京市/长江大桥”。 新词对最后结果的影响程度是大于歧义词（20倍左右吧）。 so，这里主要讲的是新词的发现 而非处理歧义词的识别。 中英文分词的区别 分词 / 词干提取和词形还原。 中文和英文的nlp各有各的难点，中文的难点在于需要进行分词，将一个个句子分解成一个单词数组。而英文虽然不需要分词，但是要处理各种各样的时态，所以要进行词干提取和词形还原。比如 词干提取主要采取“缩减”的方式，“cats” 处理为 “cat”， 将“effective” 处理为“effect”； 词性还原主要采取转变的方式，“drove” 处理成“drive”， 将“driving” 处理成“drive”。 对于中文，一个Word可以是一个单词，也可以是一个词。 分词的重要性 在处理文本对象时，非常关键的问题在于“切词”这个环节，几乎所有的后续结果都依赖第一步的切词。因此切词的准确性在很大程度上影响着后续的处理，切词结果的不同，也就影响了特征的提取，跟数据挖掘一样，特征提取的好坏特别重要，不论用什么算法，特征好数据好结果才会好。 分词的目的 当模型的记忆和拟合能力足够强（或者简单点，足够智能）的时候，我们完全可以不用分词的，直接基于字的模型就可以做，比如基于字的文本分类、问答系统等，早已有人在研究。但是，即便这些模型能够成功，也会因为模型复杂而导致效率下降，因此，很多时候（尤其是生产环境中），我们会寻求更简单、更高效的方案。比如之前我们一直是word embedding，但是 sentence embedding 也是在学术界很流行的，最后没有大规模的采用，无非就是工业级要求更加高效的方式，有时候会牺牲一些精度。 目前很多的切词模块可以处理大部分的通用语料，然而有两类文本集仍然处理的不是很好，就是： 网络文档 领域文档 后者可以有对应的专家 handle，但涉及到商用这种人力成本也是比较高的了，所以一般使用基础词汇+ 各领域的常用词汇。即使这样的方案仍然是无法可持续的，所以需要一种算法去判断是否是新词。在当下的互联网时代，人们还会不断的创造出一些新词出来，比如：“神马”、“不明觉厉”等。未登录词辨别未登录词包括是种类繁多，形态组合各异，规模宏大的一个领域。对这些词语的自动辨识，是一件非常困难的事。比如，“美的”、“快的”、“英雄联盟”应该被作为一个词，却被切成了两个词，失去了原有的语义。未登录词（out-of-vocabulary, OOV）笼统地之未在词典中出现的词， 人工标注可以解决很好识别，比如最典型的未登录词就是人名，尤其是明星，然后最简单的是手动的维护，但是人力成本也是比较高昂的。 从分词的角度来看，新词一般表现为细粒度切分后相邻词的组合。 基于统计的新词发现 这里有三个阈值（都是越大越好）：第一是最小互信息，因为互信息越大说明相关度越大，将n-gram分好的词计算互信息，如果低于阈值，则说明不能成词。第二是最小熵值，因为熵也是越大说明周边词越丰富，计算其左熵和右熵的最小值，如果最小值低于阈值，则说明不能成词。第三个是最少出现次数，为什么有这个数呢？假设前后两个词是完全相关的，出现400次，总共8000词，那么互信息=log((400/8000)/(400/8000)(400/8000))，约掉之后剩下log(8000/400)。但是一个词如果从头到尾出现了一次，但是并不是单词，则互信息为=log((1/8000)/(1/8000)(1/8000))=log(8000/1)，那么它的互信息会更大。取最少出现次数也会出现问题，就是一些低频率的词不能发现。 基于信息熵的新词发现算法，从互信息和左右信息熵入手，成词的标准有两个： 内部凝固度 自由运用程度 内部凝固度和自由运用程度分别考虑是词语内部的紧密程度和外部搭配的丰富性。所谓内部凝固度，用来衡量词搭配（collocation）是否合理。比如，对于“的电影”、“电影院”这两个搭配，直观上讲“电影院”更为合理，即“电影”和“院”凝固得更紧一些。在计算语言学中，PMI (Pointwise mutual information)被用来度量词搭配与关联性，定义如下： $$p m i ( x , y ) = \log \frac { P ( x , y ) } { P ( x ) P ( y ) }$$ 若PMI高，即两个词共现（co-occurrence）的频率远大于两个词自由拼接的乘积概率，则说明这两个词搭配更为合理一些。针对一个词有多种搭配组合，比如“电影院”可以由“电影”+“院”构成，也可以由“电”+“影院”构成，那么取其所有pmi最小值（去掉log）作为内部凝固度：$$\operatorname { solid } \left( c _ { 1 } ^ { m } \right) = \min \frac { P \left( c _ { 1 } ^ { m } \right) } { \prod P \left( c _ { i } ^ { j } \right) } = \frac { P \left( c _ { 1 } ^ { m } \right) } { \max \prod P \left( c _ { i } ^ { j } \right) }$$ 其中， $c _ { 1 } ^ { m } = c _ { 1 } c _ { 2 } \cdots c _ { m }$表示长度为 $m$ 的字符串，$P \left( c _ { 1 } ^ { m } \right)$ 表示$c _ { 1 } ^ { m }$ 的频率。 光看文本片段内部的凝合程度还不够，我们还需要从整体来看它在外部的表现。考虑“被子”和“辈子”这两个片段。我们可以说“买被子”、“盖被子”、“进被子”、“好被子”、“这被子”等，在“被子”前面加各种字；但“辈子”的用法却非常固定，除了“一辈子”、“这辈子”、“上辈子”、“下辈子”，基本上“辈子”前面不能加别的字了。“辈子”这个文本片段左边可以出现的字太有限，以至于直觉上我们可能会认为，“辈子”并不单独成词，真正成词的其实是“一辈子”、“这辈子”之类的整体。 先简单的介绍熵的概念，熵是一种表示信息量的指标，熵越高就意味着信息含量越大，不确定性越高，越难以预测，信息也就越丰富。$$H ( X ) = - \sum _ { x \in X } p ( x ) \log _ { 2 } p ( x )$$ 所以，提出了自由运用程度，用以衡量一个词的左邻字与右邻字的丰富程度。正好信息熵可以完美地诠释了这种丰富程度，熵越大则丰富程度越高。“被子”和“辈子”这两个片段的左邻字熵le与右邻字熵re分别如下 le(被子) = 3.67453re(被子) = 3.8740le(辈子) = 1.25963re(辈子) = 4.11644 可以看出，“被子”的左邻字熵与右邻字熵都较高，而“辈子”的左邻字熵较小，即左邻字非常贫乏。因此，“被子”较“辈子”更有可能成词。自由运用程度的定义如下： 给频数、内部凝固度与自由运用程度设定一个阈值，提取出来符合阈值的候选词，去掉词典中存在的词即为新词了。所以两者都高于某个对应的阈值，那么说明这个是一个新词。 实现： 使用射雕英雄传txt 作为文本，然后词频、内部凝固度 和自由程度进行新词识别。代码。 词频这点很好理解,因为不是词的话出现的频率一般比较低,词的出现频率会比较高.所以可以设置一个词频阀值,高于这个阀值的判断为词,否则判定为不是词. 凝固度 基于上一步词频的挑选。 自由度 基于上述分词进行挑选。 NLP 中的三类特征提取器 NLP 和图像中数据的特征 NLP的输入往往是一句话或者一篇文章，所以它有几个特点：首先，输入是个一维线性序列，这个好理解；其次，输入是不定长的，有的长有的短，而这点其实对于模型处理起来也会增加一些小麻烦；再次，单词或者子句的相对位置关系很重要，两个单词位置互换可能导致完全不同的意思。 NLP 中的四大任务 序列标注： 分词/ POS Tag /NER /语义标注 分类任务： 文本分类/ 情感计算 句子关系判断： Entailment /QA / 自然语言推理 生成式任务： 机器翻译/ 文本摘录 一类是序列标注，这是最典型的NLP任务，比如中文分词，词性标注，命名实体识别，语义角色标注等都可以归入这一类问题，它的特点是句子中每个单词要求模型根据上下文都要给出一个分类类别。第二类是分类任务，比如我们常见的文本分类，情感计算等都可以归入这一类。它的特点是不管文章有多长，总体给出一个分类类别即可。第三类任务是句子关系判断，比如Entailment，QA，语义改写，自然语言推理等任务都是这个模式，它的特点是给定两个句子，模型判断出两个句子是否具备某种语义关系；第四类是生成式任务，比如机器翻译，文本摘要，写诗造句，看图说话等都属于这一类。它的特点是输入文本内容后，需要自主生成另外一段文字。 深度学习最大的优点是 “端到端” NLP中的任务很多，哪些任务是最具有代表性的呢？答案是机器翻译。 回归主题，特征提取器 RNNRNN模型结构参考上图，核心是每个输入对应隐层节点，而隐层节点之间形成了线性序列，信息由前向后在隐层之间逐步向后传递。 RNN也存在问题，它采取线性序列结构不断从前往后收集输入信息，但这种线性序列结构在反向传播的时候存在优化困难问题，因为反向传播路径太长，容易导致严重的梯度消失或梯度爆炸问题。为了解决这个问题，后来引入了LSTM和GRU模型，通过增加中间状态信息直接向后传播，以此缓解梯度消失问题，获得了很好的效果，于是很快LSTM和GRU成为RNN的标准模型。 （Attention机制最早是在视觉图像领域提出来的，但是真正火起来应该算是2014年google mind团队的论文《Recurrent Models of Visual Attention》，他们在RNN模型上使用了attention机制来进行图像分类） 为什么RNN能够这么快在NLP流行并且占据了主导地位呢？主要原因还是因为RNN的结构天然适配解决NLP的问题，NLP的输入往往是个不定长的线性序列句子，而RNN本身结构就是个可以接纳不定长输入的由前向后进行信息线性传导的网络结构，而在LSTM引入三个门后，对于捕获长距离特征也是非常有效的。所以RNN特别适合NLP这种线形序列应用场景，这是RNN为何在NLP界如此流行的根本原因。 那么为什么有衰弱了？RNN本身的序列依赖结构对于大规模并行计算来说相当之不友好。通俗点说，就是RNN很难具备高效的并行计算能力。如果适合在学术界发论文，那么不太可能在工业界广泛的使用。 CNN 特征提取器卷积层本质上是个特征抽取层，可以设定超参数F来指定卷积层包含多少个卷积核（Filter）。对于某个Filter来说，可以想象有一个d*k大小的移动窗口从输入矩阵的第一个字开始不断往后移动，其中k是Filter指定的窗口大小，d是Word Embedding长度。对于某个时刻的窗口，通过神经网络的非线性变换，将这个窗口内的输入值转换为某个特征值，随着窗口不断往后移动，这个Filter对应的特征值不断产生，形成这个Filter的特征向量。这就是卷积核抽取特征的过程。卷积层内每个Filter都如此操作，就形成了不同的特征序列。Pooling 层则对Filter的特征进行降维操作，形成最终的特征。一般在Pooling层之后连接全联接层神经网络，形成最后的分类过程。 CNN 捕捉到的是什么信息？ 关键在于卷积核覆盖的那个滑动窗口，CNN能捕获到的特征基本都体现在这个滑动窗口里了。大小为k的滑动窗口轻轻的穿过句子的一个个单词，荡起阵阵涟漪，那么它捕获了什么?其实它捕获到的是单词的k-gram片段信息，这些k-gram片段就是CNN捕获到的特征，k的大小决定了能捕获多远距离的特征。 卷积操作是通过加深层数，然后获得远距离的特征的。所以有两种解题思路： 一种是增加窗口大小k 增大；一种是加深深度。 简单谈一下CNN的位置编码问题和并行计算能力问题。CNN的卷积层其实是保留了相对位置信息的，只要你在设计模型的时候别手贱，中间层不要随手瞎插入Pooling层，问题就不大，不专门在输入部分对position进行编码也行。至于CNN的并行计算能力，那是非常强的，这其实很好理解。我们考虑单层卷积层，首先对于某个卷积核来说，每个滑动窗口位置之间没有依赖关系，所以完全可以并行计算；另外，不同的卷积核之间也没什么相互影响，所以也可以并行计算。CNN的并行度是非常自由也非常高的，这是CNN的一个非常好的优点。 Transformer看看 Transformer 对于NLP 任务中的解决方案： 不定长的输入：Transformer 一般设置最大的长度，不够了 就padding，然后多了就 去尾。 单词之间的相对位置： Transformer是用位置函数来进行位置编码的，而Bert等模型则给每个单词一个Position embedding，将单词embedding和单词对应的position embedding加起来形成单词的输入embedding 长依赖问题： self attention机制 对于Transformer来说，Multi-head attention的head数量严重影响NLP任务中Long-range特征捕获能力：结论是head越多越有利于捕获long-range特征。（对标filter 的个数） 我个人意见是：这说明Transformer之所以能够效果这么好，不仅仅multi-head attention在发生作用，而是几乎所有构件都在共同发挥作用，是一个小小的系统工程。 参考文献 放弃幻想，全面拥抱Transformer 复习记录： 分词两大问题：歧义词和oov。后者对于最后的效果影响更大。处理OOV 常用的方法：subword；使用多个word embedding（transfer learning）； 使用context embedding进行分词常用的算法：基于规则的逆向最大匹配（中文的特点），频数；基于信息熵的内部凝固度和自由运用程度（分别考虑词语内部的紧密程度和外部搭配的丰富程度）；深度学习+NER进行的词性标注，条件随机场CRF之类的。 中英文语料预处理的区别：中文需要分词，英文需要处理时态，对于名词和形容词处理成标准名词，对于动词进行词性还原。 NLP中数据的特征： 输入是一维线性 输入是不定长 单词或者子句的相对位置很重要 RNN 的缺点，首先是捕捉长距离依赖，可以被LSTM 比较有效的解决；另一个缺点，RNN 这种序列的线性结构对于并行运算是不利的。CNN 的特点关键在于滑动窗口]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>关键词提取</tag>
        <tag>oov</tag>
        <tag>中文分词</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python中的多线程和多进程]]></title>
    <url>%2F2019%2F03%2F25%2FPython%E4%B8%AD%E7%9A%84%E5%A4%9A%E7%BA%BF%E7%A8%8B%E5%92%8C%E5%A4%9A%E8%BF%9B%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[多线程和多进程问题是可以对应到 并发 （cncurrency）和并行(parallelism)上的。 并发，就是一个单核cpu同时开始了多个任务，但是这个任务并不是同时独立进行的，而是通过cpu的不断切换，保存现场，然后重启这样的快速的切换，给用户的感觉是并发，但是实际上是cpu的计算能力受到了限制，用户体验比较好一些。如果在多核cpu （比如我的mac 是一cpu 6核）这样的话完全是可以达到并行的，这个是真正的独立操作(parallelism)，对应的是多进程的。对应python 中的实现多线程是使用threading，处理的是io 响应；多进程是Concurrency，使用multiprocessing包，处理的是多核cpu的操作。 Take off: 如果处理io 响应，那么使用多线程；如果是计算，那么使用多进程。 So, before we go deeper into the multiprocessing module, it’s worthwhile ensuring you know the advantages of using multiprocessing over multithreading. The general rule of thumb is that, if you are trying to improve the performance of CPU-bound tasks, multiprocessing is what you want to use. However, if your particular task is Input/Output bound, then you’ll generally want to use multithreading to improve the performance of your applications. 这个是多线程的demo 响应的io 请求。 1234567891011121314151617181920212223242526272829import threadingclass Worker(threading.Thread): # Our workers constructor, note the super() method which is vital if we want this # to function properly def __init__(self): super(Worker, self).__init__() def run(self): for i in range(10): print(i)def main(): thread1 = Worker() thread1.start() thread2 = Worker() thread2.start() thread3 = Worker() thread3.start() thread4 = Worker() thread4.start()if __name__ == "__main__": main() 下面是多进程的demo响应的计算请求。 123456789101112131415161718import multiprocessing as mpdef my_func(x): print(mp.current_process()) return x ** xdef main(): pool = mp.Pool(mp.cpu_count()) # 这个还是很好的 pool 这个的个数和你的cpu count 是保持一致的 result = pool.map(my_func, [4, 2, 3, 5, 3, 2, 1, 2]) result_set_2 = pool.map(my_func, [4, 6, 5, 4, 6, 3, 23, 4, 6]) print(result) print(result_set_2)if __name__ == "__main__": main()]]></content>
      <categories>
        <category>CS基础</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Data Pre-processing 学习笔记]]></title>
    <url>%2F2019%2F03%2F25%2FData-Pre-processing%2F</url>
    <content type="text"><![CDATA[主要介绍机器学习中的数据预处理（这里主要讲的是数值型数据的预处理，而非中文数据预处理），包括 data cleaning、data integration、 data transformation、data reduction、data imbalanced 和一些概念。 data normalization 是经常用到的。 数据预处理的时候需要注意 missing values、noisy data (outlier) ，对于这种现象应该分成要不要处理和如何处理两个问题。 数据清洗 (缺省值、异常值的处理)、数据整合 (子表 合并到 or 成主表)、数据转换 (one-hot or label encoding， 连续数值离散化)、和数据降维 (可以单独的成一章)。这个几个步骤应该是熟记于心的。 Data Cleaning这个步骤主要处理 missing values 和 noisy data (outlier).对于missing values ，可以分成两个问题，要不要处理 和如何处理，具体说来有以下处理手段： ignore the tuple; fill in the missing value manually use a global constant to fill in the missing value use the attribute mean to fill in the missing value (均值) use the most probable value to fill in the missing value (mode 众数) 有时候就是根据某几个特征然后弄一个简单的回归模型，根据模型进行predict 关于这几种方法如何去选择，我如果说 “it depends”，那么其他人不认为这是一个具有说服力的答案，他们更像知道 it depends what, and when and why to use specific method? 我认为应该是根据缺省值程度和重要性进行经验性的选择，这也去就是 empirical study吧。 接着是 noisy data (outlier)，我的观点是首先得认识到这个是错误的数据（使用 $3\sigma$定理去筛选异常值），不是真实的数据来源，可能是来自人为的笔误 或者仪器记录的问题，这个是需要修改的。可以使用聚类 (clustering) 进行noisy data 的检测，找到之后这个就类似 missing value了，可以采取以上的手段进行操作，应该注意到的这个 noisy data 所占比例不会很高，否则就成了主要的数据分布了。 Data Integration处理数据库数据，经常是需要处理子表信息的，那么必然存在着主表，而子表系信息往往是主表信息的某一方面的细化。所以有必要将两者连接起来。如何连接？一般使用聚合函数（mean, variance, std, max, min，median, mode ）等方式得到子表的统计信息，然后将两者连接。 Data TransformationIn data transformation, the data are transformed or consolidated into forms appropriate for mining.这里想要澄清的是很多相同的内容都可以用不同的方式表达，并且可以放在数据处理的不同阶段，并且这种工作不是一次性完成的，而是迭代的 until you run out your patience and time.首先我接触的最常见的就是 discrete variables -&gt; continuous variables. 当然对于 discrete variables，基于树结构的机器学习模型是可以处理的，这里想说的是有这种方式。这种 transformation 常见的处理方式: one-hot 或者 label encoding. 如果按照 data transformation的预设，那么 normalization 就也属于该模块的内容。 不论是在 machine learning 还是在 图像处理的时候，对于原始的数据经常采取 normalization. 一方面这个是可以预防梯度消失 或者 gradient exploding, 如果你采用了 Sigmoid的激活函数的话。另一方面我认为更加重要的原因是将 不同的数据放在了同一个尺度下，如果你采取了 normalization之后。 Data Reduction一般来说很少提及到到 data reduction的必要性，如果非要给出原因，那么可以从时间和空间的角度进行考虑。更加需要关注的是如何做的问题。 我的理解reduction 可以从两个维度进行考虑，假设一个 matrics A 是 m*n，这个是一个二维的矩阵，那么可以从 行列两方面入手。映射到机器学习中一般这样描述 从dimension 和 data两个角度去描述，分别称之为 dimension reduction 和 data compression. 前者指的是特征的选取，后者是数据size的减少。dimension reduction: where irrelevant, weakly relevant, or redundant attributes or dimensions may be detected and removed.data compression: PCA 线性降维 to reduce the data set size. 这个是针对某一个特征展开的。 Data Normalization 是什么？ 中心化： 均值为0，对方差没有要求 $$x ^ { \prime } = x - \mu$$ 标准化：服从正太分布 (0, 1) $$x ^ { \prime } = \frac { x - \overline { x } } { \sigma }$$ 归一化有两种： mean normalization 和 min-max normalization mean normalization$$x ^ { \prime } = \frac { x - \operatorname { mean } ( x ) } { \max ( x ) - \min ( x ) }$$ min-max normalization $$x ^ { \prime } = \frac { x - \operatorname { min } ( x ) } { \max ( x ) - \min ( x ) }$$ 为什么？ 提高模型的准确率比如说两个特征，一个特征的范围是0-100， 另一个是-2000 到2000， 这个使用 欧式距离进行计算的时候，两个特征的差值很大，特征并没有站在同一个维度上。 提高模型的速度还是上面的例子，两个特征x1 x2的取值范围比较大，那么学习率上变得波动，所以学习时间会变长。 深度学习中数据归一化可以防止模型梯度爆炸以sigmoid 函数为例解释就行 怎么做？定义就是表示怎么做 适用范围 概率模型不需要归一化，因为它们不关心变量的值，而是关心变量的分布和变量之间的条件概率，如决策树、rf。而像adaboost、svm、lr、KNN、KMeans之类的最优化问题就需要归一化。 映射到 N(0,1) 的这种行为，叫做归一化。Feature scaling is the method to limit the range of variables so that they can be compared on common grounds. 有三个主要的原因。 Because most of the Machine Learning models are based on Euclidean Distance. Age- 40 and 27Salary- 72000 and 48000 这两个特征，这两个距离相差很大；但是这个并不是我们想要的，我们想要的是相对值，而不是绝对值。 即使最后的loss function不是 euclidean distance，比如说decision tree，实践证明经过正则化的之后的数据的训练速度是快于 没有经过正则化的数据的。 经过归一化之后，数据是不容易出现梯度消失或者梯度爆炸的。 很多模型的基本假设 就是 N(0,1) 高斯分布。 实现的三种手段： rescaling (min-max normalization) $$x ^ { \prime } = \frac { x - \min ( x ) } { \max ( x ) - \min ( x ) }$$ mean normalization $$x ^ { \prime } = \frac { x - \operatorname { average } ( x ) } { \max ( x ) - \min ( x ) }$$ standardization $$x ^ { \prime } = \frac { x - \overline { x } } { \sigma }$$ data imbalanced机器学习中的特征工程是有一定技巧可言，其中我觉得最为有趣的是: generation or you can call it abstraction. 对于特征的泛的提取才是对于问题本身或者特征的理解，这不仅需要积累，更需要对于该问题领域的专业知识， that’s all.举个栗子，在 “Home Credit Default Risk” (kaggle 竞赛)中，原始的训练数据有信贷金额和客户的年收入，这个时候 “credit_income_percent” 就是类似这种性质的提取特征。 Data Exploration using Numpy，Matplotlib and Pandas（1）柱状图（Histogram） 柱状图需要有两个参数，一个是数据；一个是分成多少块（bins）123456789101112import matplotlib.pyplot as pltimport pandas as pddf=pd.read_excel("E:/First.xlsx", "Sheet1")fig=plt.figure()ax = fig.add_subplot(1,1,1)ax.hist(df['Age'],bins = 5)#Labels and Titplt.title('Age distribution')plt.xlabel('Age')plt.ylabel('#Employee')plt.show() （2）散点图（scatter plot） 需要给定两个数据， $x$和$y$ 的数据。12345678ax = fig.add_subplot(1,1,1)#Variableax.scatter(df[&apos;Age&apos;],df[&apos;Sales&apos;])#Labels and Titplt.title(&apos;Sales and Age distribution&apos;)plt.xlabel(&apos;Age&apos;)plt.ylabel(&apos;Sales&apos;)plt.show() （3）Box-plot 箱形图使用seaborn 中的库函数。主要从图形上识别出 四分位线，二分位线。 123import seaborn as sns sns.boxplot(df['Age']) sns.despine() 参考文献 1). Ultimate guide for Data Exploration in Python using NumPy, Matplotlib and Pandas2). Ultimate guide for Data Exploration in Python using NumPy, Matplotlib and Pandas]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Finding Similar Quora Questions]]></title>
    <url>%2F2019%2F03%2F25%2Fquora-questions%2F</url>
    <content type="text"><![CDATA[介绍 kaggle 上 finding similar quora quesitons 竞赛，作为一次总结。 问题描述判断一对问题是否重复。这种问题的研究是非常有价值，如果发现用户提出了一个重复的已经回答过的问题，那么可以直接返回给用户答案，提高了用户体验。 简要说明一下 kaggle 上总的评分机制：在计算得分的时候，Public Leaderboard (LB)和 Private LB 之分。具体而言，参赛选手提交整个测试集的预测结果，Kaggle 使用测试集的一部分计算得分和排名，实时显示在 Public LB上，用于给选手提供及时的反馈和动态展示比赛的进行情况；测试集的剩余部分用于计算参赛选手的最终得分和排名，此即为 Private LB，在比赛结束后会揭晓。 针对于该问题，使用的log loss 作为评分依据，提交的文件中每一对问题并不是给出 {0, 1} 这样的二值，而是给出区间 [0,,1] 的浮点数，然后predict 的结果和真实的结果，计算 log loss。作为最后的得分，该loss 的取值范围是 [0, 1] ，如果结果越小，说明预测越准确。 我的情况： 总共是3000多个队伍吧，最后的排名是在 5% （在150名之内） 数据特点 数据规模 总体： training data: 40 万， 400,000 , 63M test data: 其中有机器生成的虚假的数据，不作为最后的评分，但是存在, 314 M 这个从数量上充分说明了冗余的数据，防止人工作弊的。 training data 的字段： 1234id: Looks like a simple rowIDqid&#123;1, 2&#125;: The unique ID of each question in the pairquestion&#123;1, 2&#125;: The actual textual contents of the questions.is_duplicate: The label that we are trying to predict - whether the two questions are duplicates of each other. 句子的频数 ：使用 qid 统计在训练集中出现的次数。 发现大多数句子的重复率比较低。 1234567891011121314print('Total number of question pairs for training: &#123;&#125;'.format(len(df_train)))print('Duplicate pairs: &#123;&#125;%'.format(round(df_train['is_duplicate'].mean()*100, 2)))qids = pd.Series(df_train['qid1'].tolist() + df_train['qid2'].tolist())print('Total number of questions in the training data: &#123;&#125;'.format(len( np.unique(qids))))print('Number of questions that appear multiple times: &#123;&#125;'.format(np.sum(qids.value_counts() &gt; 1)))plt.figure(figsize=(12, 5))plt.hist(qids.value_counts(), bins=50)plt.yscale('log', nonposy='clip')plt.title('Log-Histogram of question appearance counts')plt.xlabel('Number of occurences of question')plt.ylabel('Number of questions')print() In terms of questions, everything looks as I would expect here. Most questions only appear a few times, with very few questions appearing several times (and a few questions appearing many times). One question appears more than 160 times, but this is an outlier. 训练数据集中句子word 的个数 的分布，可以看出一般是在20 左右的长度 word 的句子是最多的。 训练数据集中 character 个数的分布 注意到两个分布都是右偏态分布（正偏态），从数学的角度理解 mean &gt; median. 同理也有左偏态分布。偏态分布又是相对于正太分布而言的。这种偏态的定义，是以尾部命名，右偏态或者正偏态的尾部，集中在右侧。 由上面的例子可以看出，不管是正偏态分布还是负偏态分布，由于受偏态分布的影响，众数与均值值发生变化，而均值的影响更大。 中位数仅与样本总数有关，即在整个样本的中间位置，而在正偏态分布，大部分值在峰值（众数）的右边，故中位数在众数的右边（众数＜中位数） 中位数与均值的比较了？由于均值受极大值的影响变大而中位数不受影响，故均值大于中位数（中位数＜均值） 所以 众数 &lt; 中位数 &lt; 均值 首先众数与中位数的比较 这个分布是左偏的，大部分值是在峰值（众数）的左边，故众数大于中位数 均值与中位数的比较 由于是负偏态分布，极小值对均值的影响大（极小值对均值拉低），故均值小于中位数 所以 均值＜中位数＜众数 training data 和 test data 中的正负样本有很大的差别 However, before I do this, I would like to rebalance the data that XGBoost receives, since we have 37% positive class in our training data, and only 17% in the test data. 采取的措施 Rebalancing the Data (这里所谓的 rebalance data 就是将原始的一部分数据集去掉，减少 正样例的个数)。 原因有两点： 机器学习算法的基本假设就是独立同分布，如果 train data 和test data 不能保持同分布，那么泛化性能不能要求很高 因为对于 log loss 的计算保持正负样例的比例，对于最后的结果是重要的。 特征主要分成两类：传统的数据挖掘的特征+ NLP embedding 向量特征 传统数据挖掘特征 character of length of questions 1 and 2 number of words in question 1 and 2 number of character in question 1 and question 2 difference of length 字符串的精确匹配和部分匹配（fuzzywuzzy 工具） numbers of capital letters, questions marks, 数字 normalized word share count (非常具有 prediction 的特征)计算实现： 12345678910111213141516def word_match_share(row): q1words = &#123;&#125; q2words = &#123;&#125; for word in str(row['question1']).lower().split(): if word not in stops: q1words[word] = 1 for word in str(row['question2']).lower().split(): if word not in stops: q2words[word] = 1 if len(q1words) == 0 or len(q2words) == 0: # The computer-generated chaff includes a few questions that are nothing but stopwords return 0 shared_words_in_q1 = [w for w in q1words.keys() if w in q2words] shared_words_in_q2 = [w for w in q2words.keys() if w in q1words] R = (len(shared_words_in_q1) + len(shared_words_in_q2))/(len(q1words) + len(q2words)) return R 我认为这个定义应该是两个句子的交集/ 两个句子的并集。 和上面的实现不知道有什么区别？反正这个特征是非常 preditable 的 NLP 方面的处理 基于词向量得到得到的句子向量的相关距离函数的计算 cosine distance cityblock distance jaccard distance euclidean distance 直接是句子向量的上述距离的计算 可以做到的是对比试验，在只是使用 词向量得到的特征最后的test acc 是87%， 而加上了句子向量没有提升，应该是86.3% 之类的。 最好的模型是acc 是 0.88， 当然一切模型都是需要提到线上的public 进行计算的，对应的 log loss 是0.14658。排名是 5% （150~ /3000~） 好的代码pandas 对于数据的常用操作 实用的数据预分析 123456789101112131415# Check for any null valuesprint(train.isnull().sum())print(test.isnull().sum())# Add the string 'empty' to empty stringstrain = train.fillna('empty')test = test.fillna('empty')# 数据的分类统计temp = df.column.value_counts() # 显示整体的df 的信息df.info()# 直接就产生了一个图像，不敢想象， 使用groupby()，然后选择 column 可以学习df.groupby("is_duplicate")['id'].count().plot.bar() 常见的表格的操作 123456789test_cp.rename(columns=&#123;'test_id':'id'&#125;,inplace=True)comb = pd.concat([train_cp,test_cp])ques = pd.concat([train_orig[['question1', 'question2']], \ test_orig[['question1', 'question2']]], axis=0).reset_index(drop='index') # 筛选操作train_comb = comb[comb['is_duplicate'] &gt;= 0][['id','q1_hash','q2_hash','q1_freq','q2_freq','is_duplicate']]test_comb = comb[comb['is_duplicate'] &lt; 0][['id','q1_hash','q2_hash','q1_freq','q2_freq']] 小心使用 12train_questions.drop_duplicates(subset = ['question1'],inplace=True)train_questions.reset_index(inplace=True,drop=True) 其他 1234567891011eng_stopwords = set(stopwords.words('english'))# 主要掌握set.intersection() 交集的使用def q1_q2_intersect(row): return(len(set(q_dict[row['question1']]).intersection(set(q_dict[row['question2']]))))# series 是可以直接 tolist() 的qids = pd.Series(df_train['qid1'].tolist() + df_train['qid2'].tolist())from nltk import word_tokenize, ngrams# word_tokenize 就是将句子打散，成为一个个单词 手写 TF-IDF 12345678910111213141516171819202122232425262728293031323334from collections import Counter# If a word appears only once, we ignore it completely (likely a typo)# Epsilon defines a smoothing constant, which makes the effect of extremely rare words smallerdef get_weight(count, eps=10000, min_count=2): if count &lt; min_count: return 0 else: return 1 / (count + eps)eps = 5000 words = (" ".join(train_qs)).lower().split()counts = Counter(words)weights = &#123;word: get_weight(count) for word, count in counts.items()&#125;def tfidf_word_match_share(row): q1words = &#123;&#125; q2words = &#123;&#125; for word in str(row['question1']).lower().split(): if word not in stops: q1words[word] = 1 for word in str(row['question2']).lower().split(): if word not in stops: q2words[word] = 1 if len(q1words) == 0 or len(q2words) == 0: # The computer-generated chaff includes a few questions that are nothing but stopwords return 0 shared_weights = [weights.get(w, 0) for w in q1words.keys() if w in q2words] + [weights.get(w, 0) for w in q2words.keys() if w in q1words] total_weights = [weights.get(w, 0) for w in q1words] + [weights.get(w, 0) for w in q2words] R = np.sum(shared_weights) / np.sum(total_weights) return R 123# 在代码中使用 ls 的操作，也是比较 666 的from subprocess import check_outputprint(check_output([&quot;ls&quot;, &quot;../input&quot;]).decode(&quot;utf8&quot;)) 在验证 word2vec 时候， 可以使用t-sne 进行可视化 训练得到的model 可以 most_similar() ,定住一个变量，去验证好坏就行 参考资料 kaggle kernel 1kaggle kernel 2代码 https://towardsdatascience.com/finding-similar-quora-questions-with-word2vec-and-xgboost-1a19ad272c0d 复习总结 数据分析 数据：训练数据集是40万，测试数据集是5 倍 training data（这里有生成的虚假数据，防止作弊）数据特点：（1）句子对在训练集的频数（使用pid 在训练集中出现的次数，发现大多数句子重复率是比较低的10-50之间），并且在训练数据集中这种重复度是随着时间递减， 测试集中没有标注pid （2）句子中单词的长度是在20个词左右（正太分布中的u） 特征工程 数据挖掘特征 + NLP embedding特征。简单举例说明，前者是两个句子的共现词；后者根据 word embedding 计算的字符串的匹配之类的，比如cosine distance 或者jaccard distance之类的。 模型 xgboost 其他众数、均值和中位数和偏态分布的关系（相对于正太分布而言）(1) 对于正太分布，中位数、众数和均值是一个。偏态的定义是根据尾部命名，右偏态表示尾部在右边。中位数是样本总数中间位置，如果是右偏态，那么中位数在偏向于右边；众数是偏向于左边，因为尾部是在右边，所以头部是在左边。 所以 众数&lt;中位数&lt; 均值。（对于均值可能不太好理解，可以看上面的解释）]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Linear Algebra in ML]]></title>
    <url>%2F2019%2F03%2F25%2FLinear-Algebra-in-ML%2F</url>
    <content type="text"><![CDATA[我觉得到 ML 中的一个难点：就是由原来简单的 linear equations 直接过渡到了 matrics and vectors。这个过程是没有人跟你说的。网络结构可以当作是一个complicated 并且是无法表示的函数，于是很多使用者把它当作黑匣子，关心于输入和输出，中间过程 don’t care. 变量（特征个数）和解的关系多变量和最后target的关系是可以使用 matrices 进行表示的，这就是一种数学公式化。 Broadly speaking, in linear algebra data is represented in the form of linear equations. These linear equations are in turn represented in the form of matrices and vectors. 先直观的感受一下变量和图形（可视化）的关系。两个变量组成的equations 是两条线的相交情况。而三个变量在空间中有三种情况： 相交，平行，不在一个平面上。三个变量组成的equations 是三个面的相交情况。有四种情况 (try hard to figure it out)：No intersection at all.Planes intersect in a line.They can intersect in a plane.All the three planes intersect at a point. 当到达4 dims 的时候，it’s impossible to visulize it. terms in related to matrix这些词汇 (terms) 经常在文献中出现，需要对于其含义有个比较好的认识。Order of matrix – If a matrix has 3 rows and 4 columns, order of the matrix is 34 i.e. rowcolumn. (翻译成 矩阵的阶)Square matrix – The matrix in which the number of rows is equal to the number of columns.Diagonal matrix – A matrix with all the non-diagonal elements equal to 0 is called a diagonal matrix.Upper triangular matrix – Square matrix with all the elements below diagonal equal to 0.Lower triangular matrix – Square matrix with all the elements above the diagonal equal to 0.Scalar matrix – Square matrix with all the diagonal elements equal to some constant k.Identity matrix – Square matrix with all the diagonal elements equal to 1 and all the non-diagonal elements equal to 0.Column matrix – The matrix which consists of only 1 column. Sometimes, it is used to represent a vector.Row matrix – A matrix consisting only of row.Trace – It is the sum of all the diagonal elements of a square matrix.Rank of a matrix – Rank of a matrix is equal to the maximum number of linearly independent row vectors in a matrix.Determinant of a matrix - 矩阵的行列式转置 -在图形 matrix中还是很常见的。$$\mathrm { A } _ { \mathrm { ij } } ^ { \mathrm{T}} = \mathrm { A } _ { \mathrm { ji } }$$ 这个矩阵乘法和元素相称的区别，后者是element-wise 进行的。可以从另外一个角度去列及矩阵相称： This operation on a vector is called linear transformation. 就是后面的vector 映射到了前面的矩阵空间。 特征值和奇异值着两个是分别对应着PCA 和SVD。Eigenvalues and Eigenvectors如公式所示，特征值和特征向量的乘积就是方阵和特征向量的乘积，原先的方阵是可以降维表示成特征向量和特征值的。$ A x = \lambda x $ 特征值和特征向量的计算过程，如果矩阵计算比较简单，那么还是容易计算的。 对于奇异值分解，最常见的就是这种表达：$A = U \Sigma V ^ { T }$特征值分解和奇异值分解都是给一个矩阵(线性变换)找一组特殊的基，特征值分解找到了特征向量这组基，在这组基下该线性变换只有缩放效果。而奇异值分解则是找到另一组基，这组基下线性变换的旋转、缩放、投影三种功能独立地展示出来了。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[剑指offer-递归、回溯和动态规划]]></title>
    <url>%2F2019%2F03%2F17%2F%E5%89%91%E6%8C%87offer-%E9%80%92%E5%BD%92-%E5%9B%9E%E6%BA%AF%E5%92%8C%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%2F</url>
    <content type="text"><![CDATA[这是剑指offer 系列四部曲中的第三部：递归、回溯和动态规划。第一部关于字符串和数组，第二部是栈、队列、链表和树， 最后一部分在这里。 斐波那契数列 大家都知道斐波那契数列，现在要求输入一个整数n，请你输出斐波那契数列的第n项（从0开始，第0项为0）。n&lt;=39 Tips: 简单的递归，可以转换成循环。 1234567891011121314class Solution: # python 中list 的初始化，最开始的是从0 开始，所以是需要多进行一个初始化的 def Fibonacci(self, n): # write code here if n == 0: return 0 if n == 1: return 1 arr = [0] * (n + 1) arr[0] = 0 arr[1] = 1 for i in range(2, n + 1): arr[i] = arr[i - 1] + arr[i - 2] return arr[n] 这个是可以进一步优化到时间 O(N)，空间是 O(1)12345678910111213141516171819202122232425262728293031#include&lt;iostream&gt;#include&lt;algorithm&gt;using namespace std;//0, 1, 1, 2, 3, 5,// 第0项是0，第1项是1int fiber(int n)&#123; if (n ==1) return 1; else if(n ==2) return 1; int a =1, b =1; for(int i =3; i&lt;=n; i++) &#123; a =a+b; b =a; &#125; return b;&#125;int main()&#123; int n; cin &gt;&gt;n; cout&lt;&lt; fiber(n)&lt;&lt;endl; return 0;&#125; 跳台阶 一只青蛙一次可以跳上1级台阶，也可以跳上2级。求该青蛙跳上一个n级的台阶总共有多少种跳法（先后次序不同算不同的结果）。 Tips： 同上。 python 版本123456789101112131415class Solution: def jumpFloor(self, number): # write code here if number == 1: return 1 if number == 2: return 2 arr = [0] * (number + 1) arr[1] = 1 arr[2] = 2 for i in range(3, number + 1): arr[i] = arr[i - 1] + arr[i - 2] return arr[number] c++ 版本12345678910111213141516171819202122232425262728293031#include&lt;iostream&gt;#include&lt;algorithm&gt;#include "vector"using namespace std;//0, 1, 1, 2, 3, 5,// 第0项是0，第1项是1vector&lt;int&gt; f;int jump(int n)&#123; if(n ==1) return 1; if (n ==2) return 2; f[1] =1,f[2] =2; for(int i =3; i&lt;=n;i++) f[i] =f[i-1] +f[i-2]; return f[n]; &#125;int main()&#123; int n; cin &gt;&gt;n; f =vector&lt;int&gt;(n+1); cout&lt;&lt;jump(n)&lt;&lt;endl; return 0;&#125; 跳台阶2 一只青蛙一次可以跳上1级台阶，也可以跳上2级……它也可以跳上n级。求该青蛙跳上一个n级的台阶总共有多少种跳法。 Tips：同上。 12345678910111213class Solution: """ 在使用 for循环的时候，注意 range() 这种取值，究竟是使用 range() 作为次数的计量； 还是要使用range 中的index 。两者是不相同的操作，尤其是对于前后的取值。 """ def jumpFloorII(self, number): # write code here if number == 1: return 1 nums = 1 for i in range(number - 1): nums = nums * 2 return nums 矩形覆盖 我们可以用2*1的小矩形横着或者竖着去覆盖更大的矩形。请问用n个2*1的小矩形无重叠地覆盖一个2*n的大矩形，总共有多少种方法？ Tips: math, 找出递归方程。 1234567891011121314151617"""既然结果只是最后一个解，那么就没有必要保存中间变量，所以只是，所以空间复杂度从O（n） -&gt; O(1) ，这个是超级nice的"""class Solution: def rectCover(self, number): # write code here if number &lt;= 0: return 0 if number &lt;= 2: return number # 只是两个变量罢了 a, b = 1, 2 while number &gt; 2: a, b = b, a + b # python 中独有的比较神奇的操作， 是计算了右边的 b, a+b 然后再统一赋值给 a, b number -= 1 return b 注意在c++ 中是需要使用三个变量的。这样才能一步步的进行转移。 123456789101112131415161718192021class Solution &#123;public: int rectCover(int number) &#123; if(number &lt;=0) return 0; int&amp; n =number; if (n ==1) return 1; if (n ==2) return 2; int a =1, b =2, c; for(int i =3; i&lt;=n; i++) &#123; c =a+b; a =b; b=c; &#125; return c; &#125;&#125;; 机器人的运动范围 (c++ 中一秒能够遍历 $10^8$，遍历一亿个数字) 地上有一个m行和n列的方格。一个机器人从坐标0,0的格子开始移动，每一次只能向左，右，上，下四个方向移动一格，但是不能进入行坐标和列坐标的数位之和大于k的格子。 例如，当k为18时，机器人能够进入方格（35,37），因为3+5+3+7 = 18。但是，它不能进入方格（35,38），因为3+5+3+8 = 19。请问该机器人能够达到多少个格子？ Tips：递归，转移方程不难，在上下左右四个方向进行尝试，需要判断的条件比较多，比如是否访问过，数位之和等一些条件。 123456789101112131415161718192021222324252627282930class Solution: def movingCount(self, threshold, rows, cols): # visited 不一定是二维的，只要是能够"自圆其说" 就行。 visited = [False] * (rows * cols) count = self.movingCountCore(threshold, rows, cols, 0, 0, visited) return count def movingCountCore(self, threshold, rows, cols, row, col, visited): count = 0 # 就是这个访问记录是需要进行变化的， 如果是false ，然后访问之后 是需要设置为 true的 if self.check(threshold, rows, cols, row, col, visited): visited[row * cols + col] = True count = 1 + self.movingCountCore(threshold, rows, cols, row, col - 1, visited) + \ self.movingCountCore(threshold, rows, cols, row, col + 1, visited) + \ self.movingCountCore(threshold, rows, cols, row + 1, col, visited) + \ self.movingCountCore(threshold, rows, cols, row - 1, col, visited) return count def check(self, threshold, rows, cols, row, col, visited): if row &gt;= 0 and row &lt; rows and col &gt;= 0 and col &lt; cols and self.judge(threshold, row, col) and not visited[row * cols + col]: return True else: return False def judge(self, threshold, i, j): if sum(map(int, str(i) + str(j))) &lt;= threshold: return True else: return False 当数据量比较大的时候，一般使用宽度优先遍历，因为深度优先遍历每进入一层，就会申请一定的栈空间，如果层数很多，那么是容易栈溢出的。所以可以使用bfs （宽度优先遍历），bfs 的时间复杂度就是所有格子的个数。 扩展新节点时候需要满足以下条件： 之前没有遍历过，可以使用 bool 数组进行判断 没有走出边界 横纵坐标的各位数字之和小于 $k$ 使用BFS(宽度优先搜索)时间复杂度分析：每个节点最多只会入队一次，所以时间复杂度不会超过方格中的节点个数。最坏情况下会遍历所有的方格的点，所以时间复杂度是 $O(mn)$。这个能不能进入某个格子是确定的，不是依赖于从上面到下面不能进入，但是从左面到右面就能够进入。所以对于每个节点，遍历一次就足以。 c++ 实现的 BFS 代码 1234567891011121314151617181920212223242526272829303132333435363738394041class Solution &#123;public: int get_sum(pair&lt;int, int&gt; p) &#123; int s = 0; while (p.first) &#123; s += p.first % 10; p.first /= 10; &#125; while (p.second) &#123; s += p.second % 10; p.second /= 10; &#125; return s; &#125; int movingCount(int threshold, int rows, int cols) &#123; if (!rows || !cols) return 0; queue&lt;pair&lt;int,int&gt;&gt; q; // 这种定义还是要多熟悉一下的 vector&lt;vector&lt;bool&gt;&gt; st(rows, vector&lt;bool&gt;(cols, false)); int dx[4] = &#123;-1, 0, 1, 0&#125;, dy[4] = &#123;0, 1, 0, -1&#125;; int res = 0; q.push(&#123;0, 0&#125;); while (q.size()) &#123; auto t = q.front(); q.pop(); if (st[t.first][t.second] || get_sum(t) &gt; threshold) continue; res ++ ; st[t.first][t.second] = true; for (int i = 0; i &lt; 4; i ++ ) &#123; int x = t.first + dx[i], y = t.second + dy[i]; if (x &gt;= 0 &amp;&amp; x &lt; rows &amp;&amp; y &gt;= 0 &amp;&amp; y &lt; cols) q.push(&#123;x, y&#125;); &#125; &#125; return res; &#125;&#125;; c++ 实现的 DFS 代码 时间复杂度分析，因为有 visit 数组判重，所以每个点只会遍历一次，时间复杂度是 $O(mn)$ 12345678910111213class Solution &#123; public int movingCount(int k, int rows, int cols)&#123; boolean[][] vis = new boolean[rows][cols] ; return dfs(k,0,0,vis) ; &#125; public int dfs(int k,int i,int j,boolean[][] vis)&#123; if(i&lt;0 || i &gt;= vis.length || j&lt;0 || j&gt;=vis[0].length || vis[i][j] || (i/10+i%10+j/10+j%10)&gt;k)&#123; return 0; &#125; vis[i][j] = true ; return dfs(k,i+1,j,vis) + dfs(k,i-1,j,vis) + dfs(k,i,j-1,vis) + dfs(k,i,j+1,vis) + 1; &#125;&#125; 这个不需要回溯，因为最后的结果是一个值，标记某个点能否被访问，是不需要回溯的，不是路径问题。宽度优先遍历需要维护一个队列，如果能够被访问，那么就可以继续扩展，否则不能。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152class Solution &#123;public: // 宽度优先搜索， 使用队列 int get_single_sum(int x) &#123; int s =0; while (x) &#123; s += x%10; x =x/10; &#125; return s; &#125; int get_sum(pair&lt;int, int&gt; p) &#123; return get_single_sum(p.first) +get_single_sum(p.second); &#125; int movingCount(int threshold, int rows, int cols) &#123; int res =0; if(!rows || !cols) return 0; vector&lt;vector&lt;bool&gt;&gt; st(rows, vector&lt;bool&gt;(cols)); int dx[4] =&#123;-1, 0, 1, 0&#125;, dy[4]=&#123;0, 1, 0, -1&#125;; queue&lt;pair&lt;int, int&gt;&gt; q; q.push(&#123;0, 0&#125;); while (q.size()) &#123; auto t =q.front(); q.pop(); if(get_sum(t)&gt; threshold || st[t.first][t.second]) continue; res ++; st[t.first][t.second] =true; for(int i =0; i&lt;4; i++) &#123; int a= t.first+ dx[i], b =t.second+ dy[i]; if(a &gt;=0 &amp;&amp; a&lt;rows &amp;&amp; b&gt;=0 &amp;&amp; b&lt;cols) q.push(&#123;a, b&#125;); &#125; &#125; return res; &#125;&#125;; 矩阵中的路径 请设计一个函数，用来判断在一个矩阵中是否存在一条包含某字符串所有字符的路径。路径可以从矩阵中的任意一个格子开始，每一步可以在矩阵中向左，向右，向上，向下移动一个格子。如果一条路径经过了矩阵中的某一个格子，则之后不能再次进入这个格子。 例如 a b c e s f c s a d e e 这样的3 X 4 矩阵中包含一条字符串”bcced”的路径，但是矩阵中不包含”abcb”路径，因为字符串的第一个字符b占据了矩阵中的第一行第二个格子之后，路径不能再次进入该格子。 Tips： 在二维数组中每个点上都进行尝试，每个点上同样是上下左右进行尝试，返回符合条件的。 1234567891011121314151617181920212223242526class Solution: # 递归 这个是 true or false 判断类型的。 # 思路：先是 rows* cols 这样的全部遍历 def hasPath(self, matrix, rows, cols, path): # 如果使用 [ for _in range(rows) ] for _ in range(cols) ， 这个是有结构的 rows* cols assist = [True] * rows * cols for i in range(rows): for j in range(cols): if self.rightPath(matrix, rows, cols, i, j, path, assist): return True return False def rightPath(self, matrix, rows, cols, i, j, path, assist): if not path: return True index = i * cols + j if i &lt; 0 or i &gt;= rows or j &lt; 0 or j &gt;= cols or matrix[index] != path[0] or assist[index] == False: return False assist[index] = False if (self.rightPath(matrix, rows, cols, i + 1, j, path[1:], assist) or self.rightPath(matrix, rows, cols, i - 1, j, path[1:], assist) or self.rightPath(matrix, rows, cols, i, j - 1, path[1:], assist) or self.rightPath(matrix, rows, cols, i, j + 1, path[1:], assist)): return True assist[index] = True return False 对于深度优先搜索(DFS) 中最重要的是考虑搜索顺序。在这里，先枚举单词的起点，然后枚举单词的每个字母。时间复杂度分析，起点一共是有 $n^2$ 个，然后每个单词是有 四个方向，但是不能走回头路，所以除了首字母外，只有三种选择。所以总的时间复杂度是 $O(n^23^k)$, 其中n 表示matrix 的长或者宽，k 表示寻找字符串的长度。这个代码中明显是需要使用到回溯的，因为每个节点都是有 3中选择的。 题目链接 12345678910111213141516171819202122232425262728293031323334353637383940414243class Solution &#123;public: // dfs ,首先是遍历网格，然后对于每个网格进行dfs() 遍历 bool hasPath(vector&lt;vector&lt;char&gt;&gt;&amp; matrix, string str) &#123; for(int i =0; i&lt; matrix.size(); i++) &#123; for(int j =0; j&lt; matrix[i].size(); j++) &#123; if(dfs(matrix, str, 0, i, j)) return true; &#125; &#125; return false; &#125; bool dfs(vector&lt;vector&lt;char&gt;&gt;&amp; matrix, string str, int u, int i, int j) &#123; if(str[u] != matrix[i][j]) return false; // 注意这个边界条件 if( u ==str.size()-1) return true; char t =matrix[i][j]; // 表示一种标记，访问过了 matrix[i][j] ='#'; //然后访问下一个点 int dx[4] =&#123;-1, 0, 1, 0&#125;, dy[4] =&#123;0, 1, 0, -1&#125;; for(int k=0; k&lt; 4; k++ ) &#123; int a =i +dx[k], b = j +dy[k]; if(a&gt;=0 &amp;&amp; a&lt;matrix.size() &amp;&amp; b&gt;= 0 &amp;&amp; b&lt; matrix[a].size()) if(dfs(matrix, str, u +1, a, b)) return true; // 这里是有return 函数的，关键步骤注意一下 &#125; matrix[i][j] =t; return false; &#125;&#125;;]]></content>
      <categories>
        <category>算法</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[剑指Offer-字符串和数组]]></title>
    <url>%2F2019%2F03%2F17%2F%E5%89%91%E6%8C%87Offer-%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%92%8C%E6%95%B0%E7%BB%84%2F</url>
    <content type="text"><![CDATA[题目都是来自牛客网在线刷题中的剑指offer。刷题记录，顺便从考察知识点的角度分类整理。主要分成以下四大类： 字符串、数组 链表、树 递归、回溯、动态规划 其他, 比如位运算、正则匹配等 这是剑指offer 系列四部曲中的第一部。第一部关于字符串和数组，第二部是栈、队列、链表和树，第三部递归、回溯和动态规划， 最后一部分在这里。 （1）二维数组中的查找 在一个二维数组中（每个一维数组的长度相同），每一行都按照从左到右递增的顺序排序，每一列都按照从上到下递增的顺序排序。请完成一个函数，输入这样的一个二维数组和一个整数，判断数组中是否含有该整数。 列的递增不是基于上一行中最大的（最右边）进行的递增，而是基于上一行对应的列的递增。所以不是左上角是最小，右下角是最大的。 双指针问题，对于任意的一点，下面的都是比其大，左面的都是比其小，这就决定了做好的出发点是从右上角出发。这样可以不断的进行if … else 的判断，然后找到 target。 12345678910111213141516171819# -*- coding:utf-8 -*-class Solution: # array 二维列表 def Find(self, target, array): # write code here if not array: return True n, m =len(array), len(array[0]) i , j =0, m-1 while i&lt;n and j&gt;=0: if(target ==array[i][j]): return True elif (target &gt; array[i][j]): i+=1 else: j-=1 return False c++ 版本123456789101112131415161718class Solution &#123;public: // 时间 O(m+n), 空间O(1) bool Find(int target, vector&lt;vector&lt;int&gt; &gt; array) &#123; if(array.empty()) return false; int n =array.size(), m =array[0].size(); for(int i =0, j =m-1; i&lt;n &amp;&amp;j&gt;=0 ; ) &#123; if(target == array[i][j]) return true; else if(target &gt; array[i][j]) i+=1; else j-=1; &#125; return false; &#125;&#125;; （2）替换空格 请实现一个函数，将一个字符串中的每个空格替换成“\%20”。例如，当字符串为We Are Happy.则经过替换之后的字符串为We\%20Are\%20Happy。 Tips： 字符串的遍历对于 python 而言是比较简单的。 考察的就是字符串的遍历。如果发现了空格，那么替换成 “%20”，否则就是原字符。 12345678910111213# -*- coding:utf-8 -*-class Solution: # s 源字符串 # 时间 O(n), 空间 O(n), n =len(s) def replaceSpace(self, s): # write code here if not s: return "" res ="" for ch in s: if ch ==" ": res +="%20" else: res += ch return res 在 c++ 读入和读出 sring，不能使用 cin，因为cin 读入是以空格分割的。但是读入的一个字符串不能这样。 1234567891011121314151617181920212223242526#include&lt;iostream&gt;#include&lt;string&gt;using namespace std;const int N =10010;string replaceSpace(string str)&#123; string res; for(auto x: str) &#123; if (x ==' ') res += "%20"; else res += x; &#125; return res;&#125;int main()&#123; char str[N]; gets(str); cout&lt;&lt; str&lt;&lt;endl; string res =replaceSpace(str); cout&lt;&lt; res&lt;&lt;endl; return 0;&#125; （3）旋转数组的最小数字 把一个数组最开始的若干个元素搬到数组的末尾，我们称之为数组的旋转。 输入一个非减排序的数组的一个旋转，输出旋转数组的最小元素。 例如数组{3,4,5,1,2}为{1,2,3,4,5}的一个旋转，该数组的最小值为1。 NOTE：给出的所有元素都大于0，若数组大小为0，请返回0。 时间复杂度最坏是$0(N)$， 但是对于一般情况下，是可以优化的。因为这个数据特点是存在重复的元素。本题中的比较对象是 arr[0]。 1234567891011# 注意python 中判断条件是需要有 :, 而在c++中需要有 &#123;&#125;def minNumber(arr): n =len(arr)-1 if n &lt;0: return 0 while(n and arr[0] ==arr[n]): n -=1 l, r =0, n while(l &lt;r): mid = l+r &gt;&gt;1 if(arr[0] &gt; arr[mid]): r =mid else: l =mid +1 return arr[l] c++ 版本。时间复杂度最坏是 $O(n)$, 但是一般的时间复杂度是 $O(logn)$ 12345678910111213141516171819202122232425262728293031323334#include&lt;iostream&gt;#include&lt;vector&gt;using namespace std;const int MAX =1010;int n;vector&lt;int&gt; arr(MAX);int minNumber(vector&lt;int&gt; arr)&#123; while(n &amp;&amp; arr[n] ==arr[0]) n--; int l =0, r =n; while(l &lt;r) &#123; int mid = l+r &gt;&gt;1; if(arr[0] &gt; arr[mid]) r =mid; else l =mid +1; &#125; return arr[l];&#125;int main()&#123; cin&gt;&gt;n; for(int i =0; i&lt;n;i++) cin &gt;&gt; arr[i]; cout&lt;&lt; minNumber(arr)&lt;&lt; endl; return 0;&#125; （4）调整数组顺序使奇数位于偶数前面 输入一个整数数组，实现一个函数来调整该数组中数字的顺序，使得所有的奇数位于数组的前半部分，所有的偶数位于数组的后半部分，并保证奇数和奇数，偶数和偶数之间的相对位置不变。 快排是根据阈值分别左右挑选了大于和小于阈值的数组。这道题是可以分别左右分别挑选偶数和奇数，然后交换位置，思路是一样的。双指针问题，使用 while语句查找，如果找到，使用 swap() 函数交换。代码格式和快排很像。快排的实现和双指针基本上是等号的。 1234567891011121314151617181920212223242526#include&lt;iostream&gt;#include&lt;vector&gt;using namespace std;const int N =1000;// 奇数在前 偶数在后void reOrder(vector&lt;int&gt; &amp;arr)&#123; int i =0, j =arr.size() -1; while( i&lt;=j) &#123; while(i &lt;=j &amp;&amp; arr[j] %2 ==0) j-=1; while(i &lt;=j &amp;&amp; arr[i] %2 ==1) i+=1; if(i &lt;=j) swap(arr[i], arr[j]); // 在上面的执行过程中，有可能i 和j 已经不满足相对的关系，所以是需要进行判断 &#125; &#125;int main()&#123; vector&lt;int&gt; arr(N); int n; cin &gt;&gt;n; for(int i =0; i&lt;n; i++) cin&gt;&gt; arr[i]; reOrder(arr); for(int i=0; i&lt;n; i++) cout&lt;&lt; arr[i] &lt;&lt; " "; cout&lt;&lt;endl; return 0;&#125; 123456789101112131415def partition(arr): if not arr or len(arr) ==0: return left, right =0, len(arr)-1 while left &lt; right: print(arr[left]) while arr[left] %2 ==0:# 奇偶判断的时候，尽量不要使用整除，python2 python3 的语法环境是不一样的 left +=1 while arr[right] % 2 ==1: right -=1 if (left&lt; right): arr[left], arr[right] =arr[right], arr[left]if __name__ =="__main__": arr =[1,2, 3, 4, 5] partition(arr) print(arr) 上面版本保留了原始数字相对的顺序，下面这个没有保留相对的顺序。前者的空间复杂度是 $O(N)$, 后者的空间复杂度是 $O(1)$. 1234567891011121314151617class Solution: def reOrderArray(self, array): if len(array) ==0: return [] left =0 right =len(array)-1 while left &lt; right: # 如果是奇数 key =array[left] while left &lt; right and array[right] &amp; 1 == 0: right -= 1 array[left] = array[right] # 如果是偶数 while left &lt; right and array[left] &amp; 1 == 1: left += 1 array[right] = key return array （5）字符串的全排列 输入一个字符串,按字典序打印出该字符串中字符的所有排列。例如输入字符串abc,则打印出由字符a,b,c所能排列出来的所有字符串abc,acb,bac,bca,cab和cba。 N 表示字符串的长度。时间复杂度$ O(N * N!)$ 空间复杂度 $O(N *N!)$ python 具有更加简单的写法。实现过程中主要是用到python 的字符串的切片。 1234567891011121314# -*- coding:utf-8 -*-class Solution: def dfs(self, ss, path, res): if not ss: res.append(path) for i in range(len(ss)): self.dfs(ss[:i] +ss[i+1:], path+ss[i], res) def Permutation(self, ss): # write code here if not ss: return ss res =[] self.dfs(ss, "", res) return sorted(list(set(res))) c++ 实现的时候是通过 swap() 函数形成不同的组合，需要注意的是如果没有返回值的话，那么 res是引用，而不是每次调用的时候都重新申请一个栈空间。 12345678910111213141516171819202122class Solution &#123;public: void dfs(string str, int u, vector&lt;string&gt;&amp; res) &#123; if(u == str.size()) res.emplace_back(str); for(int i =u; i&lt; str.size(); i++) &#123; if (i != u &amp;&amp; str[u] ==str[i]) continue; swap(str[i], str[u]); dfs(str, u +1, res); swap(str[i], str[u]); &#125; &#125; vector&lt;string&gt; Permutation(string str) &#123; vector&lt;string&gt; res; if (str.empty()) return res; dfs(str, 0,res); sort(res.begin(), res.end()); return res; &#125;&#125;; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849#include&lt;iostream&gt;#include&lt;vector&gt;using namespace std;/* 每一个递归都是对应着一个递归树，所以好好思考这个过程。 有两种思路，一种是枚举每个位置上放哪些数字；一种是枚举每个数字放到哪些位置上。差别在于同一层中数字的不同的摆放形式。 */vector&lt;int&gt; arr;int n;vector&lt;vector&lt;int&gt;&gt; ans;vector&lt;int&gt; path;vector&lt;bool&gt; st;// u 表示底基层, 这个代码是枚举位置（每个位置可以放什么数字）状态就是位置void dfs(vector&lt;int&gt; arr, int u)&#123; if(u ==n) &#123; ans.push_back(path); return; &#125; for(int i =0;i&lt; n; i++ ) &#123; if(!st[i]) &#123; st[i] =true; path.push_back(arr[i]); dfs(arr, u+1); st[i] =false; path.pop_back(); &#125; &#125;&#125;int main()&#123; cin&gt;&gt;n; arr =vector&lt;int&gt;(n); for(int i =0; i&lt;n; i++) cin&gt;&gt;arr[i]; st =vector&lt;bool&gt;(n); dfs(arr, 0); for(auto u: ans) &#123; for(auto v: u) cout&lt;&lt; v&lt;&lt;" "; cout&lt;&lt;endl; &#125; return 0;&#125; （7）数组中出现次数超过一半的数字 数组中有一个数字出现的次数超过数组长度的一半，请找出这个数字。例如输入一个长度为9的数组{1,2,3,2,2,2,5,4,2}。由于数字2在数组中出现了5次，超过数组长度的一半，因此输出2。如果不存在则输出0。 Tips： 如果某个数字出现的次数多于一半，那么其他所有非该数字的出现的频数是小于该数字的，所以形成一个二分类。 12345678910111213141516171819class Solution:# 如果存在这样的数字，那么这个数字的频数一定是大于其他所有的频数def MoreThanHalfNum_Solution(self, numbers): # write code here if not numbers: return 0 target = numbers[0] nums = 0 # 统计出现次数最多的数字 for i in numbers: if target == i: nums += 1 elif nums == 0: target = i nums = 1 else: nums -= 1 res = target if numbers.count(target) &gt; len(numbers) // 2 else 0 return res 12345678910111213141516171819202122232425class Solution &#123;public: int MoreThanHalfNum_Solution(vector&lt;int&gt; numbers) &#123; int target =numbers[0]; int counts =1; for(int i =1; i&lt; numbers.size(); i++) &#123; if(target == numbers[i]) counts +=1; else counts -=1; if(counts ==0) &#123; target =numbers[i]; counts =1; &#125; &#125; int c =0; for(auto num : numbers) &#123; if(num ==target) c ++; &#125; if(c &gt; numbers.size() &gt;&gt;1) return target; return 0; &#125;&#125;; （8）连续子数组的最大和 HZ偶尔会拿些专业问题来忽悠那些非计算机专业的同学。今天测试组开完会后,他又发话了:在古老的一维模式识别中,常常需要计算连续子向量的最大和,当向量全为正数的时候,问题很好解决。但是,如果向量中包含负数,是否应该包含某个负数,并期望旁边的正数会弥补它呢？例如:{6,-3,-2,7,-15,1,2,2},连续子向量的最大和为8(从第0个开始,到第3个为止)。给一个数组，返回它的最大连续子序列的和，你会不会被他忽悠住？(子向量的长度至少是1) 连续子数组的最大和和数组中超过一半的数字，这种问题都是属于数组的遍历问题。 最笨的方法先是枚举子数组，然后计算；但是问题是不要求求解 该子数组，只是要求最大和。时间复杂度 $O(N) $, 空间 $O(1) $ 123456789101112131415class Solution &#123;public: int FindGreatestSumOfSubArray(vector&lt;int&gt; array) &#123; int max_v =array[0]; int val =0; for(auto num : array) &#123; if (val &lt;0) val =0; val += num; // 这个操作是无脑加上，后面的max 进行判断 max_v =max(max_v, val); &#125; return max_v; &#125;&#125;; （9）第一个只出现一次的字符 在一个字符串(0&lt;=字符串长度&lt;=10000，全部由字母组成)中找到第一个只出现一次的字符,并返回它的位置, 如果没有则返回 -1（需要区分大小写）. 如果使用了 dictionary，那么这个只是数据结构上的问题，不存在算法上的问题了。 1234567891011121314151617181920212223242526272829303132class Solution: """ Three ways to get dictionary of string s """ def FirstNotRepeatingChar(self, s): if not s: return -1 # get dictionary from collections import defaultdict dict1 =defaultdict(int) for string in s: dict1[string] += 1 # or this way # from collections import Counter # dict1 =Counter(s) # or do it yourself #dict1 =self.Counter_self(s) for index, val in enumerate(s): if dict1[val] == 1: return -1 def Counter_self(self, s): dict1 = &#123;&#125; for val in s: if val not in dict1: dict1[val] = 1 else: dict1[val] = dict1[val] + 1 return dict1 C++ 实现。本质上还是dictionary 的思想，统计每个字母出现的个数，然后遍历，时间复杂度是$O(n)$，空间是$O(n)$。当字符串比较长的时候，可以进一步优化，辅助的数组记录字符出现的最小的index。这要求在预处理的时候，进行判断一下。 12345678910111213class Solution &#123;public: int firstUniqChar(string s) &#123; vector&lt;int&gt; arr(26, 0); for(int i =0; i&lt; s.size(); i++) &#123; arr[s[i] -'a'] ++; &#125; for(int i =0; i&lt; s.size(); i++) if(arr[s[i] -'a'] ==1) return i; return -1; &#125;&#125;; （10）数组中的逆序对 在数组中的两个数字，如果前面一个数字大于后面的数字，则这两个数字组成一个逆序对。输入一个数组,求出这个数组中的逆序对的总数P。并将P对1000000007取模的结果输出。 即输出P%1000000007 1234567891011121314151617181920212223242526272829303132333435363738394041# -*- coding:utf-8 -*-class Solution: def InversePairs(self, data): # write code here if not data: return 0 temp = [i for i in data] return self.mergeSort(temp, data, 0, len(data ) -1) % 1000000007 def mergeSort(self, temp, data, low, high): if low &gt;= high: temp[low] = data[low] return 0 mid = (low + high) / 2 left = self.mergeSort(data, temp, low, mid) right = self.mergeSort(data, temp, mid +1, high) count = 0 i = low j = mid +1 index = low while i &lt;= mid and j &lt;= high: if data[i] &lt;= data[j]: temp[index] = data[i] i += 1 else: temp[index] = data[j] count += mid - i +1 j += 1 index += 1 while i &lt;= mid: temp[index] = data[i] i += 1 index += 1 while j &lt;= high: temp[index] = data[j] j += 1 index += 1 return count + left + right c++ 写法。逆序对的个数等于左边的逆序对个数、右边逆序对的个数和交叉（一个在左边一个在右边）逆序对的个数，对于前两种是可以通过递归求解，对于第三种在递归的过程中是可解的。时间复杂度是$O(nlogn)$ 空间复杂度是$O(n)$。$log n $的时间复杂度和 $n$ 是有很大区别的， 当 $n=10^6 $时候，$logn =20$. 123456789101112131415161718192021222324252627282930class Solution &#123;public: int merge(vector&lt;int&gt; &amp; nums, int l, int r) &#123; if(l &gt;= r) return 0; int mid =(l +r) &gt;&gt;1; int res = merge(nums, l, mid) +merge(nums, mid+1, r); int i =l, j =mid+1; // 别归并排序都不会写了 vector&lt;int&gt; temp; while(i &lt;= mid &amp;&amp; j&lt;= r) &#123; if(nums[i] &lt;= nums[j]) temp.push_back(nums[i++]); // 这个写的是非常的简洁的 else &#123; temp.push_back(nums[j++]); res += mid -i +1; &#125; &#125; while(i &lt;=mid) temp.push_back(nums[i++]); while(j &lt;= r) temp.push_back(nums[j++]); // 下面将 temp 数组返回到nums中, temp 是已经排好序的 i =l; for(auto x : temp) nums[i++] =x; return res; &#125; int InversePairs(vector&lt;int&gt; data) &#123; return merge(data, 0, data.size() -1); &#125;&#125;; （11）数字在排序数组中出现的次数 统计一个数字在排序数组中出现的次数。 对于有序数组的查找，统计某个数字的次数，所以就找到其左右边界left 和right，最后次数就是 right- left +1。那么时间复杂度是 $O(logn)$。注意处理边界问题。 123456789101112131415161718192021222324252627class Solution &#123;public: int GetNumberOfK(vector&lt;int&gt; data ,int k) &#123; if( data.empty()) return 0; int l =0, r =data.size() -1; while( l&lt;r) &#123; int mid = (l+r )&gt;&gt;1; if(data[mid] &gt;= k) r =mid ; else l =mid +1; &#125; if (data[l] != k) return 0; int left =l; l =0, r =data.size() -1; while(l&lt;r) &#123; int mid = (l +r +1) &gt;&gt;1; if(data[mid] &gt; k) r =mid-1; else l =mid; &#125; return l -left +1; &#125;&#125;; （12）只有一个数字出现了一次，其他的数字都是出现了偶次，那么一遍遍历数（进行异或）就可以得到这个数字。现在是由两个单独的数字。 思路一：最常规的做法是使用dictionary 统计数字出现的次数，key 是数字，vaue 是数字的个数。时间复杂度是$O(n)$，空间复杂度是$O(n)$。思路二：除了字典思路外，对于数字而言，还可以使用异或的操作。根据异或的性质，两两异或最后剩下的数字一定是出现一次的数字。时间复杂度是$O(n)$，空间复杂度是$O(1)$。 2^4 # 43^4 # 740^42 # 2 （12）数组中只出现一次的数字 一个整型数组里除了两个数字之外，其他的数字都出现了两次。请写程序找出这两个只出现一次的数字。 使用index 得到两个不同的数字二进制形式下的位置，然后从该位置将原来的数组分成两类，那么每类中只含有一个出现一次的数字，接着使用异或操作。 12345678910111213141516171819202122class Solution:# 返回[a,b] 其中ab是出现一次的两个数字# 使用异或的性质，如果只有一个不同，其他的偶次出现，那么全部异或的结果# 就是那个单一的数字def FindNumsAppearOnce(self, array): # write code here remain, index =0, 1 for num in array: remain = remain ^ num # 找出第一个是1 的位置 # index 都是 while (remain &amp; index) ==0: index = index &lt;&lt;1 res1, res2 =0,0 for num in array: # 这个条件必须是0, 表示两个在这个位数是相同的， # 234 ^0 =234 , if num &amp; index ==0: res1 =res1 ^ num else: res2 =res2 ^ num return [res1, res2] 这个是基于 c++ 的实现。很简洁 1234567891011121314151617181920class Solution &#123;public: // 思路：找到位置为1 的位置，然后从该位置分开，分别处理得到两个单独的数字 vector&lt;int&gt; findNumsAppearOnce(vector&lt;int&gt;&amp; nums) &#123; int x =0; for(auto num : nums) x ^= num; int i =0; while((x &gt;&gt;i &amp; 1) ==0) i ++; int a=0; int b =0; for(auto num : nums) if(num &gt;&gt; i &amp; 1) a ^= num; else b ^= num; return vector&lt;int&gt;&#123;a, b&#125;; &#125;&#125;; （13）和为S的连续正数序列 小明很喜欢数学,有一天他在做数学作业时,要求计算出9~16的和,他马上就写出了正确答案是100。但是他并不满足于此,他在想究竟有多少种连续的正数序列的和为100(至少包括两个数)。没多久,他就得到另一组连续正数和为100的序列:18,19,20,21,22。现在把问题交给你,你能不能也很快的找出所有和为S的连续正数序列? 这个是数学问题，使用等差数列求和。还有一种解法。 12345678910111213141516def FindContinuousSequence(self, tsum): # write code here if tsum &lt; 2: return [] left = 1 right = left + 1 res = [] while left &lt; tsum // 2 + 1: if sum(range(left, right)) == tsum: res.append(range(left, right)) left += 1 elif sum(range(left, right)) &lt; tsum: right += 1 else: left += 1 return res 双指针问题，可以在 O(n) 时间内解决。 如果是条件判断，那么最好从定义出发使用 while 进行判断。 1234567891011121314151617181920class Solution &#123;public: vector&lt;vector&lt;int&gt; &gt; findContinuousSequence(int sum) &#123; vector&lt;vector&lt;int&gt;&gt; res; int i =1, j =1, s =1; while (i &lt; sum) // 注意这个条件 &#123; while(s &lt; sum ) s += ++j; // 这个操作也是比较骚 if(s ==sum &amp;&amp; j -i &gt;0) &#123; vector&lt;int&gt; tmp; for(int k =i; k&lt;=j ; k++) tmp.push_back(k); res.push_back(tmp); &#125; s -= i; i ++; &#125; return res; &#125;&#125;; （14）和为S的两个数字 输入一个递增排序的数组和一个数字S，在数组中查找两个数，使得他们的和正好是S，如果有多对数字的和等于S，输出两个数的乘积最小的。 有序序列，查找两个数字，可以使用二分。二分查找和双指针算法有比较大的相关性。如果使用二分，那么二分的跳出条件和二分的判断条件是重点需要关注的。 12345678910111213141516class Solution: def FindNumbersWithSum(self, array, tsum): # write code here if len(array) &lt; 2: return [] left = 0 right = len(array) - 1 while left &lt; right: if array[left] + array[right] == tsum: return [array[left], array[right]] elif array[left] + array[right] &lt; tsum: left += 1 else: right -= 1 return [] 时间复杂度是$O(N)$, 但是空间复杂度是 $O(1)$ 1234567891011121314151617class Solution &#123;public: // 使用二分的思想去做， O(logn) vector&lt;int&gt; FindNumbersWithSum(vector&lt;int&gt; array,int sum) &#123; int l =0, r =array.size()-1; while( l&lt;r) &#123; int tmp =array[l] + array[r]; if(tmp == sum) return vector&lt;int&gt;&#123;array[l], array[r]&#125;; else if( tmp &gt; sum) r -=1; else l +=1; &#125; return vector&lt;int&gt;(); &#125;&#125;; 还有一种思路使用hash 表。这种时间复杂度也是 $O(n)$，但是空间上使用了 hash 表的操作。（这里使用的set ，因为只是判断是否存在） 123456789101112 class Solution &#123;public: vector&lt;int&gt; FindNumbersWithSum(vector&lt;int&gt; array,int sum) &#123; unordered_set&lt;int&gt; hash; for(auto a : array) &#123; if (hash.count(sum -a)) return vector&lt;int&gt;&#123;a, sum -a&#125;; hash.insert(a); &#125; return vector&lt;int&gt;(); &#125;&#125;; 所以总结来说，使用二分（双指针）思路，那么时间复杂度是$O(n)$，空间复杂度是$O(1)$； 使用hash表（unordered_set），时间复杂度是$O(n)$，空间复杂度是$O(n)$。所以hash 表这种预处理是一种可行解，不是最优解。 （15）左旋转字符串 汇编语言中有一种移位指令叫做循环左移（ROL），现在有个简单的任务，就是用字符串模拟这个指令的运算结果。对于一个给定的字符序列S，请你把其循环左移K位后的序列输出。例如，字符序列S=”abcXYZdef”,要求输出循环左移3位后的结果，即“XYZdefabc”。是不是很简单？OK，搞定它！ python 中直接可以使用切片的思想。 123456class Solution: def LeftRotateString(self, s, n): # write code here if len(s) &lt; n: return '' return s[n:] + s[:n] C++ 中时间复杂度是 $O(1)$ 空间复杂度是 $O(N)$。 1234567891011121314151617181920212223242526272829#include&lt;iostream&gt;#include&lt;string&gt;using namespace std;string reversek(string str, int k)&#123; reverse(str.begin(), str.end()); reverse(str.begin(), str.begin() +str.size() -k); reverse(str.begin() +str.size() -k, str.end()); return str;&#125;// 这个getline() 不能同时输入 int 和 string 两个变量int main()&#123; int k =3 ; //cin &gt;&gt;k; string str; getline(cin, str); string res = reversek(str, 3); cout&lt;&lt; res&lt;&lt; endl; return 0;&#125; （16）翻转单词顺序 牛客最近来了一个新员工Fish，每天早晨总是会拿着一本英文杂志，写些句子在本子上。同事Cat对Fish写的内容颇感兴趣，有一天他向Fish借来翻看，但却读不懂它的意思。例如，“student. a am I”。后来才意识到，这家伙原来把句子单词的顺序翻转了，正确的句子应该是“I am a student.”。Cat对一一的翻转这些单词顺序可不在行，你能帮助他么？ 这个是通过两次翻转就可以解决，第一次是字符串的完全翻转，第二次是单词的完全翻转。start, end 分别标记一个单词的开始和结束。 12345678910111213141516171819202122232425262728class Solution: def Reverse(self, s, left, right): while left &lt;right: s[left], s[right] = s[right], s[left] left +=1 right -=1 def ReverseSentence(self, s): # write code here if not s: return s # from immutable string to mutable list # s =list(s) self.Reverse(s, 0, len(s) - 1) start, end = 0, 0 # 这个小于号 是python 中特有的坑，真正能够访问的区间是 [0, len(s)-1] 这样的区间 while start &lt; len(s): if s[start] == " ": start += 1 end += 1 elif end == len(s) or s[end] == " ": self.Reverse(s, start, end - 1) # update 操作 end += 1 start = end else: end += 1 return "".join(s) 其中 string 的判断操作是非常经典的， i 和j 的操作，这个就是一个模板。 12345678910111213141516class Solution &#123;public: // 整个字符串翻转，然后每个单词进行翻转 时间O(1) 空间是 o(N) string ReverseSentence(string str) &#123; reverse(str.begin(), str.end()); for(int i =0; i&lt; str.size(); i++) &#123; int j =i; while( j&lt; str.size() &amp;&amp; str[j] != ' ') j++; reverse(str.begin() +i, str.begin() +j); i =j; &#125; return str; &#125;&#125;; （17）扑克牌顺子 LL今天心情特别好,因为他去买了一副扑克牌,发现里面居然有2个大王,2个小王(一副牌原本是54张^_^)…他随机从中抽出了5张牌,想测测自己的手气,看看能不能抽到顺子,如果抽到的话,他决定去买体育彩票,嘿嘿！！“红心A,黑桃3,小王,大王,方片5”,“Oh My God!”不是顺子…..LL不高兴了,他想了想,决定大/小 王可以看成任何数字,并且A看作1,J为11,Q为12,K为13。上面的5张牌就可以变成“1,2,3,4,5”(大小王分别看作2和4),“So Lucky!”。LL决定去买体育彩票啦。 现在,要求你使用这幅牌模拟上面的过程,然后告诉我们LL的运气如何， 如果牌能组成顺子就输出true，否则就输出false。为了方便起见,你可以认为大小王是0。 模拟题分成三步骤 删去所有的0 如果存在重复的元素，那么一定不是顺子 如果最大值和最小值之间的差距大于4 那么不是；如果小于4 那么就是。 12345678910111213141516class Solution &#123;public: bool IsContinuous( vector&lt;int&gt; numbers ) &#123; if (!numbers.size() ) return false; // 首先是要sort() sort(numbers.begin(), numbers.end()); // 统计0 的个数 int i =0; while (!numbers[i] ) i++; // 如果有重复的，那么一定不能构成顺子 for(int j =i+1; j&lt; numbers.size(); j++) if(numbers[j] ==numbers[j-1]) return false; // 只有最大的数字和最小的数字之间的间隔小于4，那么才有可能构成顺子 return numbers.back() -numbers[i] &lt;=4; &#125;&#125;; （18）孩子们的游戏(圆圈中最后剩下的数) 每年六一儿童节,牛客都会准备一些小礼物去看望孤儿院的小朋友,今年亦是如此。HF作为牛客的资深元老,自然也准备了一些小游戏。其中,有个游戏是这样的:首先,让小朋友们围成一个大圈。然后,他随机指定一个数m,让编号为0的小朋友开始报数。每次喊到m-1的那个小朋友要出列唱首歌,然后可以在礼品箱中任意的挑选礼物,并且不再回到圈中,从他的下一个小朋友开始,继续0…m-1报数….这样下去….直到剩下最后一个小朋友,可以不用表演,并且拿到牛客名贵的“名侦探柯南”典藏版(名额有限哦!!^_^)。请你试着想下,哪个小朋友会得到这份礼品呢？(注：小朋友的编号是从0到n-1) Tips: 约瑟夫环的问题， 要求求解的是最后胜利者的编号，所以应用数学技巧就可以了。 $ f(x) = (f( x-1) + m ) \% (x) $ , 共有 m 个编号，n 个人 12345678910111213class Solution:# mod 求余 的操作， a mod b ==c ,说明 a除以b 之后余数是c# https://blog.csdn.net/gatieme/article/details/51435055， 从做题思路上讲解的比较好# n 个小朋友，然后是m 个编号def LastRemaining_Solution(self, n, m): # write code here if n&lt; 1 or m&lt;1: return -1 last =0 for i in range(2, n+1): # 这个相当于 是一个 “挑选人” 的逆过程， 因为使用的 mod 操作就是取余的操作 last =(last +m) %i return last 约瑟夫问题使用递推的方式来做。相邻两项之间的关系，推导足够简单，那么就可以之间计算了。当去掉第m 个人之后，重新进行编号。需要寻找的是新旧编号之间的关系。 $f(n, m) = (f(n-1, m) +m ) \% n$ 就是新旧编码中递推关系式。 其中 n 表示总共有n 个人，m 表示去掉m （m 表示数到的数字）个人。当n ==1 的时候（剩下一个人），那么可以跳出了。 （19）求1+2+3+…+n 之和 求1+2+3+…+n，要求不能使用乘除法、for、while、if、else、switch、case等关键字及条件判断语句（A?B:C）。 有一种思路使用位运算， 然后结合递归进行求解。 12345678910111213141516class Solution: # 如果你想使用全局变量，那么放在 __init__ 中就是一个很好的方式 def __init__(self): self.ans =0 def Sum_Solution(self, n): # write code here self.recur(n) return self.ans # n&gt;0 就是一个短路条件，这个直接决定了后面递归会不会继续执行下去，也就是跳出的条件 # 至于会不会回到原来最初的状态，这个是不重要的，最后的结果是 self.ans ，false之后直接使用这个就行了 def recur(self, n): self.ans += n n -= 1 return n &gt; 0 and self.Sum_Solution(n) （19）不用加减乘除做加法 写一个函数，求两个整数之和，要求在函数体内不得使用+、-、*、/四则运算符号。 对于两个数字（带有正负号），可以有四种组合同号（同为正数，同为负数）异号（正大负小，正小负大），可以归结为两个函数，fun1处理同号，fun2处理异号（正大负小）。 在二进制表示中，都是一样的（原码，反码和补码），不用分成正负数进行处理。使用异或和与操作分成三步进行 进行二进制加法（没有进位） 计算进位，如果进位不为0，那么继续 返回的是num1 12345678910111213class Solution&#123;public: int Add(int num1, int num2) &#123; while (num2 !=0) &#123; int tmp =num1 ^ num2; num2 = num1 &amp; num2 &lt;&lt;1; num1 =tmp; &#125; return num1; &#125;&#125;; （20）把字符串转换成整数 将一个字符串转换成一个整数(实现Integer.valueOf(string)的功能，但是string不符合数字要求时返回0)，要求不能使用字符串转换整数的库函数。 数值为0或者字符串不是一个合法的数值则返回0。 Tips：还是python 中处理 string， 使用 dictionary 处理字符串和 数字的匹配。这个有好多个版本，是需要问清题目要求，然后根据进行作答。比如说这个是整数比较简单，如果加上小数，该如何处理。如果是中文，的该如何处理。 使用python 处理字符串问题。其实就是分情况一个个进行处理。 正负号 非整数类型字符 最后的边界问题 （python 中使用 sys.maxint判断） 1234567891011121314151617181920# -*- coding:utf-8 -*-class Solution: def StrToInt(self, s): required =['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '+', '-'] n =len(s) flag =1 res =0 for i in range(n): if s[i] not in required: return 0 elif s[i] =='-': flag =-1 elif s[i] =='+': continue else: res =res *10 + required.index(s[i]) res =res *flag if res &gt;=2147483648 or res &lt;= -2147483649: return 0 return res c++ 实现 123456789101112131415161718192021222324252627282930313233343536373839#include&lt;iostream&gt;#include&lt;vector&gt;#include&lt;string&gt;using namespace std;// 处理空格，处理正负号，处理数字字母int str2int(string str)&#123; int k =0; while(k &lt; str.size() &amp;&amp; str[k] ==' ') k++; long long number =0; // 这种对于 bool值的名称的定义 bool is_minus =false; if(str[k] =='+') k ++; else if(str[k] =='-') k++, is_minus =true; for(; k&lt; str.size() &amp;&amp; str[k] &gt;'0' &amp;&amp; str[k] &lt; '9'; k++) &#123; number = number *10 + (str[k] -'0'); &#125; if(is_minus) number *= -1; if(number &gt; INT_MAX) return INT_MAX; else if(number &lt; INT_MIN) return INT_MIN; return (int)number;&#125;int main()&#123; string str; // 这个语句处理一个 string 的输入还是ok 的 getline(cin, str); cout&lt;&lt;str2int(str)&lt;&lt;endl; return 0;&#125; （21）数组中重复的数字 在一个长度为n的数组里的所有数字都在0到n-1的范围内。 数组中某些数字是重复的，但不知道有几个数字是重复的。也不知道每个数字重复几次。请找出数组中任意一个重复的数字。 例如，如果输入长度为7的数组{2,3,1,0,2,5,3}，那么对应的输出是第一个重复的数字2。 解法一：使用dictionary 存储，key 表示数字，value 表示频数，时间复杂度是$O(n)$，空间复杂度是$O(n)$。解法二：因为数组中存储的是 $[0, n-1]$的数字，和index 是相对应的。所以如果不相同，那么修改其相等为止，如果发现 numbers[i] 和 numbers[numbers[i]] 相等了，那么就找到了重复的数字；最后返回false。 1234567891011121314# -*- coding:utf-8 -*-class Solution: # 这里要特别注意~找到任意重复的一个值并赋值到duplication[0] # 函数返回True/False def duplicate(self, numbers, duplication): len_ =len(numbers) for i in range(len_): while i != numbers[i]: if numbers[numbers[i]] ==numbers[i]: duplication[0] =numbers[i] return True else: numbers[numbers[i]], numbers[i] =numbers[i], numbers[numbers[i]] return False （22）构建乘积数组 给定一个数组A[0,1,…,n-1],请构建一个数组B[0,1,…,n-1],其中B中的元素B[i]=A[0] *A[1] *…*A[i-1] *A[i+1] *… *A[n-1]。不能使用除法。 Tips: 分成上下三角形进行计算，不能每个B[i] 都进行单独重复计算。下三角形是从上往下遍历，上三角形是从下往上遍历。ans 存储各个不同的结果。时间复杂度是$O(n)$，空间复杂度是$O(n)$。 1234567891011121314151617class Solution: # A 是一个list ，只是自己构建的是一个矩阵 def multiply(self, A): # write code here ans =[] tmp =1 length =len(A) # 值得是 rows # 首先是下三角形 各个部分的数值的相乘， 从上往下遍历 for i in range(length): ans.append(tmp) tmp *= A[i] tmp =1 # 上三角形 从下往上进行遍历 for i in range(length-1, -1, -1): ans[i] *= tmp tmp *= A[i] return ans 这个题目的限制条件。 只能开辟一个数组空间的长度 不能使用除法 仔细观察两个for 循环，好的代码可读性也是很强的。 1234567891011121314151617181920212223242526272829303132#include&lt;iostream&gt;#include&lt;string&gt;#include&lt;vector&gt;using namespace std;// 处理空格， 处理正负号，处理string 类型的数字const int N =100;vector&lt;int&gt; multiply(vector&lt;int&gt; arr, int n)&#123; vector&lt;int&gt; res(n); for(int i =0, p =1; i&lt; n; i++) &#123; res[i] =p; p *= arr[i]; &#125; for(int i =n-1, p =1; i&gt;=0; i--) &#123; res[i] =p * res[i]; p *= arr[i]; &#125; return res;&#125;int main()&#123; int n; vector&lt;int&gt; arr(N); cin&gt;&gt;n; for(int i =0; i&lt;n;i++) cin&gt;&gt; arr[i]; vector&lt;int&gt; res =multiply(arr, n); for(auto u: res) cout&lt;&lt; u&lt;&lt; " "; cout&lt;&lt; endl; return 0;&#125; 加深 对于 “~” 运算的理解 和对于 if 判断的理解。当 if 中条件是0 （对应bool 中的false）的时候，那么才会不执行，如果是 -1 那么也是可以执行的。但是这里没有必要写的那么简洁深邃，可以简单点写。 12345678910int main()&#123; cout&lt;&lt; ~-1&lt;&lt; endl; cout&lt;&lt; ~0 &lt;&lt;endl; cout&lt;&lt; ~2 &lt;&lt;endl; if( -3) cout&lt;&lt; "niubi "&lt;&lt; endl; return 0;&#125; （23）表示数值的字符串 请实现一个函数用来判断字符串是否表示数值（包括整数和小数）。例如，字符串”+100”,”5e2”,”-123”,”3.1416”和”-1E-16”都表示数值。 但是”12e”,”1a3.14”,”1.2.3”,”+-5”和”12e+4.3”都不是。 这个是一道判别题而不是 string to int 的题目。考察的是细节，从正负号，小数点， Ee和数字进行分类讨论。 12345678910111213141516171819202122# -*- coding:utf-8 -*-class Solution: # s字符串 def isNumeric(self, s): s =s.strip() dots =e =digit =False # 这个都是flag for i, char in enumerate(s): if char in '+-': if i &gt; 0 and s[i-1] not in 'eE': return False elif char =='.': if dots or e: return False dots =True elif char in 'eE': if e or not digit: return False e, digit =True, False elif char.isdigit(): digit =True else: return False return digit （24）字符流中第一个不重复的字符 请实现一个函数用来找出字符流中第一个只出现一次的字符。例如，当从字符流中只读出前两个字符”go”时，第一个只出现一次的字符是”g”。当从该字符流中读出前六个字符“google”时，第一个只出现一次的字符是”l”。 流操作一般在笔试中不会考察。 12345678910111213141516171819class Solution: # 返回对应char # 这个没有了dict 那么依赖于 count 函数 # 主要差别在于有了一个 字符流，是动态的，所以需要有一个大的存储的list def __init__(self): self.list1 =[] def FirstAppearingOnce(self): # write code here for string in self.list1: if self.list1.count(string) == 1: return string return "#" def Insert(self, char): # write code here self.list1.append(char) 对于 $O(n^2)$ 算法复杂度，优化成 $O(n)$， 一般在面试的过程中，有两种优化算法，一种是双指针算法，一种是单调队列算法。 1234567891011121314151617181920212223242526272829class Solution&#123;public: //Insert one char from stringstream // 双指针算法，但在实现的时候不一定适用双指针 // 使用队列，可以节省内存 unordered_map&lt;char, int&gt; dic; // 这个维护的是一个dictionary queue&lt;char&gt; q; // 这个维持的是一个 first appearing once 的队列 void Insert(char ch) &#123; if( ++dic[ch] &gt;1) &#123; while(q.size() &amp;&amp; dic[q.front()] &gt;1) q.pop(); &#125; else q.push(ch); &#125; //return the first appearence once char in current stringstream char FirstAppearingOnce() &#123; if(q.empty()) return '#'; return q.front(); &#125;&#125;;]]></content>
      <categories>
        <category>算法</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Collections]]></title>
    <url>%2F2019%2F03%2F07%2Fcollections%2F</url>
    <content type="text"><![CDATA[收藏夹。 （1）中文数据集来源 大规模中文自然语言处理语料 Large Scale Chinese Corpus for NLP 中文自然语言处理数据集]]></content>
      <categories>
        <category>NOT_FOR_YOU</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[leetcode tree]]></title>
    <url>%2F2019%2F03%2F01%2Fleetcode-tree%2F</url>
    <content type="text"><![CDATA[判断是不是一个二叉搜索树？ 解题思路有二。一方面是可以递归的进行判断，如果根节点大于（严格）左子树，小于右子树。那么是一个二叉搜索树。另一方可以使用区间的思想。就是下面的解法。 1234567891011121314151617181920212223/** * Definition for a binary tree node. * struct TreeNode &#123; * int val; * TreeNode *left; * TreeNode *right; * TreeNode(int x) : val(x), left(NULL), right(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: // 判断是否是二叉搜索树， 左子树&lt; 根节点&lt; 右子树 // 使用区间的概念，根节点 [INT_MIN, INT_MAX] bool isValidBST(TreeNode* root) &#123; return dfs(root, INT_MIN, INT_MAX); &#125; bool dfs(TreeNode * root, long long minv, long long maxv) &#123; if(!root) return true; if(root-&gt; val &lt; minv || root-&gt;val &gt; maxv) return false; return dfs(root-&gt; left, minv, root-&gt;val- 1ll) &amp;&amp; dfs(root-&gt; right, root-&gt;val+1ll, maxv); &#125;&#125;; Binary Tree Inorder Traversal 中序非递归遍历。 a. 将整个树的最左边一条链压入栈中b. 每次取出栈顶元素，如果有右子树，那么将右子树压入栈中 1234567891011121314151617181920212223242526272829303132333435/** * Definition for a binary tree node. * struct TreeNode &#123; * int val; * TreeNode *left; * TreeNode *right; * TreeNode(int x) : val(x), left(NULL), right(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: vector&lt;int&gt; inorderTraversal(TreeNode* root) &#123; vector&lt;int&gt; res; stack&lt;TreeNode *&gt; stk; auto p =root; // 这个条件也是比较nice的， p || stk.size() while( p || stk.size()) &#123; while(p ) &#123; stk.push(p); p =p-&gt; left; &#125; // 这个p 是一个遍历指针，类似i，是可以重复的赋值的 p = stk.top(); stk.pop(); //auto tmp =stk.top(); //stk.pop(); res.push_back(p-&gt; val); // 然后就转向了右子树 p =p-&gt;right; &#125; return res; &#125;&#125;; Symmetric Tree 递归判断是否是 symmetric tree。 这个还是非常nice的。 这种思路比较简单的，根节点不用比较，然后左右子树，左子树的左结点和右子树的右节点，左子树的右节点和右子树的左结点。 a. 两个根节点的值要相等b. 左边的左子树和右边的右子树对称c. 左边的右子树和右边的左子树对称 1234567891011121314151617181920212223242526/** * Definition for a binary tree node. * struct TreeNode &#123; * int val; * TreeNode *left; * TreeNode *right; * TreeNode(int x) : val(x), left(NULL), right(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: // 对称， 左子树的左结点 和右子树的右节点； 左子树的右节点和右子树的左结点 // bool isSymmetric(TreeNode* root) &#123; // 如果为空 那么 return true if (! root) return true; return dfs(root-&gt; left, root -&gt; right); &#125; bool dfs(TreeNode * left, TreeNode * right) &#123; if(! left || ! right) return !left &amp;&amp; ! right; // 只有两者都为空，那么才是true if(left-&gt; val != right -&gt; val) return false; return dfs(left-&gt; left, right -&gt; right) &amp;&amp; dfs(left-&gt; right, right -&gt; left); &#125;&#125;; 重建二叉树 根据前序遍历和中序遍历重塑二叉树。在一个无序的list 中去查找二叉树，时间是 $O(n)$，这个是可以使用hash 优化成 $O(1)$。重点是可以通过长度进行划分 左子树序列和右子树序列。 1234567891011121314151617181920212223242526272829303132333435/** * Definition for a binary tree node. * struct TreeNode &#123; * int val; * TreeNode *left; * TreeNode *right; * TreeNode(int x) : val(x), left(NULL), right(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: unordered_map&lt;int, int&gt; hash; TreeNode* buildTree(vector&lt;int&gt;&amp; preorder, vector&lt;int&gt;&amp; inorder) &#123; int n =preorder.size(); for(int i =0; i&lt; n; i++) hash[inorder[i]] =i; return dfs(preorder, inorder, 0, n -1, 0, n -1); &#125; TreeNode * dfs(vector&lt;int&gt;&amp; preorder, vector&lt;int&gt; &amp; inorder, int pl, int pr, int il, int ir) &#123; // dfs 一定是有return 的 if(pl &gt; pr) return NULL ; int val =preorder[pl]; int index =hash[val]; int len =index -il; TreeNode* root =new TreeNode(val); root -&gt; left =dfs(preorder, inorder, pl+1, pl +len, il, index -1); root -&gt; right =dfs(preorder, inorder, pl+len+1, pr, index +1, ir); return root; &#125; &#125;; 层序遍历，非递归写法。 1234567891011121314151617181920212223242526272829303132333435/** * Definition for a binary tree node. * struct TreeNode &#123; * int val; * TreeNode *left; * TreeNode *right; * TreeNode(int x) : val(x), left(NULL), right(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: // 层次遍历，先进先出，队列的东西 queue&lt;TreeNode *&gt; q; vector&lt;vector&lt;int&gt;&gt; levelOrder(TreeNode* root) &#123; vector&lt;vector&lt;int&gt;&gt; res; if (!root) return res; q.push(root); while(q.size()) &#123; vector&lt;int&gt; level; int n = q.size(); // 这个是需要划分成一层的 for(int i =0; i&lt; n; i++) &#123; auto tmp =q.front(); q.pop(); level.push_back(tmp-&gt;val ); if(tmp -&gt; left) q.push(tmp -&gt; left); if(tmp -&gt; right) q.push(tmp -&gt; right); &#125; res.push_back(level); &#125; return res; &#125;&#125;; lowest common ancestor of a binary tree 二叉树的最小公共父节点。 12345678910111213141516171819202122/** * Definition for a binary tree node. * struct TreeNode &#123; * int val; * TreeNode *left; * TreeNode *right; * TreeNode(int x) : val(x), left(NULL), right(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: // 最短公共祖先 TreeNode* lowestCommonAncestor(TreeNode* root, TreeNode* p, TreeNode* q) &#123; if(! root || p ==root || q ==root) return root; // 否则就需要进行寻找 auto left =lowestCommonAncestor(root -&gt;left, p, q); auto right =lowestCommonAncestor(root -&gt; right, p, q); if( !left) return right; if(! right) return left; return root; &#125;&#125;; serialize and deserialize binary tree 一般来说，单独的前序遍历是不能唯一确定一个树的结构。（使用前序遍历和中序遍历才能确定一个二叉树的结构，如之前的题目）但是这里可以唯一确定，因为前序遍历中把空节点加入了整个序列中。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071/** * Definition for a binary tree node. * struct TreeNode &#123; * int val; * TreeNode *left; * TreeNode *right; * TreeNode(int x) : val(x), left(NULL), right(NULL) &#123;&#125; * &#125;; */class Codec &#123;public: // 编码的格式 1,2,#,#, // Encodes a tree to a single string. // 使用string 的地址表示 exactly 的那种结果 string serialize(TreeNode* root) &#123; string res; dfs1(root, res); return res; &#125; void dfs1(TreeNode * root, string &amp; res) &#123; if(! root) &#123; res +="#,"; return; &#125; res += to_string(root-&gt;val)+','; dfs1(root-&gt;left, res); dfs1(root-&gt;right, res); &#125; // Decodes your encoded data to tree. TreeNode* deserialize(string data) &#123; int u=0 ; return dfs2(data, u); &#125; TreeNode* dfs2(string &amp;data, int &amp; u) &#123; if(data[u] =='#') &#123; u +=2; // 一个 # 一个, return NULL ; &#125; bool is_minus =false; if(data[u] =='-') &#123; is_minus =true; u ++; &#125; int val =0; while(data[u] != ',') &#123; val = val *10 +data[u] -'0'; u ++; &#125; u ++; if(is_minus) val =-val; auto root =new TreeNode(val); root-&gt;left =dfs2(data, u); root -&gt;right =dfs2(data, u); return root; &#125;&#125;;// Your Codec object will be instantiated and called as such:// Codec codec;// codec.deserialize(codec.serialize(root)); diameter of binary tree 先是枚举节点，然后求解左右子树深度之和。因为左右子树是没有联系的，是可以单独求解左右子树的最大深度。（ 和下一道题目相比，这个所有的权重都是 1) 时间复杂度是 $O(n)$， 因为这个一定是从根节点出发 的。 12345678910111213141516171819202122232425262728293031/** * Definition for a binary tree node. * struct TreeNode &#123; * int val; * TreeNode *left; * TreeNode *right; * TreeNode(int x) : val(x), left(NULL), right(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: // 这个题目和最长路径不一样，在于这个是求解边的长度，而不是结点的个数 // 所以最长的是一定是从根节点出发的， 然后dfs进行 // 长度是 left + right // 时间复杂度是 O(n) int res =0; int diameterOfBinaryTree(TreeNode* root) &#123; dfs(root); return res; &#125; // 这个不是dfs ，这个是从root向下走的最大的路径 int dfs(TreeNode * root) &#123; if( !root) return 0; auto left =dfs(root -&gt; left); auto right =dfs( root -&gt; right); res =max(res, left +right); return max(left, right) +1; &#125; &#125;; binary tree maximum path sum 这个权重有正有负。时间复杂度是 $O( n^2)$， 首先枚举的是点 $O(n)$ ，然后每个点上计算， 那么也是 $O(n)$， 最后的结果是 $O(n ^2)$。这里实际上是有三种路径的：a. root -&gt; val + Lb. root-&gt;val + Rc. root -&gt;val 所以看一下最后是三种选择的. 123456789101112131415161718192021222324252627282930/** * Definition for a binary tree node. * struct TreeNode &#123; * int val; * TreeNode *left; * TreeNode *right; * TreeNode(int x) : val(x), left(NULL), right(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: // 权重可以看成 root -&gt;val int res =INT_MIN; int maxPathSum(TreeNode* root) &#123; dfs(root); return res; &#125; // 从根节点遍历 int dfs(TreeNode* root) &#123; if(! root) return 0; // 左右子树的最大值 int left =dfs(root -&gt; left); int right =dfs(root -&gt; right); res =max(res, left +right + root -&gt;val); // 返回当前节点的最大值 return max(0, max(left, right) + root -&gt; val); &#125;&#125;; binary search tree iterator 其中的 average 是均摊的意思， $O(n) $ 除以 $n$ 那么最后的结果是 $O(1)$。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849/** * Definition for a binary tree node. * struct TreeNode &#123; * int val; * TreeNode *left; * TreeNode *right; * TreeNode(int x) : val(x), left(NULL), right(NULL) &#123;&#125; * &#125;; */class BSTIterator &#123;public: // 迭代器 就是有next 函数，每次都是可以得到下一个结点 // 这里因为限制 O(h) 空间，使用栈进行中序遍历的（不能使用 vector，因为那样是 O(n), n 表示全部结点的个数） // 使用栈中序遍历，就是每次把 结点的左子树全部放到里面 左中右遍历方式， stack&lt;TreeNode*&gt; stk; BSTIterator(TreeNode* root) &#123; while(root) &#123; stk.push(root); root = root-&gt;left; &#125; &#125; /** @return the next smallest number */ int next() &#123; TreeNode* p =stk.top(); stk.pop(); int res =p -&gt;val ; p =p-&gt;right; while(p) &#123; stk.push(p); p =p -&gt;left; &#125; return res; &#125; /** @return whether we have a next smallest number */ bool hasNext() &#123; return ! stk.empty(); &#125;&#125;;/** * Your BSTIterator object will be instantiated and called as such: * BSTIterator* obj = new BSTIterator(root); * int param_1 = obj-&gt;next(); * bool param_2 = obj-&gt;hasNext(); */]]></content>
      <categories>
        <category>算法</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[linux commands]]></title>
    <url>%2F2019%2F02%2F18%2Flinux-commands%2F</url>
    <content type="text"><![CDATA[介绍linux 中常见的命令 和github 中常用的命令 Linux 常见命令awk、grep、sed是linux操作文本的三大利器，也是必须掌握的linux命令之一。三者的功能都是处理文本，但侧重点各不相同，其中属awk功能最强大，但也最复杂，awk更适合格式化文本，对文本进行较复杂格式处理。grep更适合单纯的查找或匹配文本，sed更适合编辑匹配到的文本。 awk 取列数据之所以叫AWK是因为其取了三位创始人 Alfred Aho，Peter Weinberger, 和 Brian Kernighan 的Family Name的首字符。AWK是一种处理文本文件的语言，是一个强大的文本分析工具。特点是处理灵活，功能强大。可实现统计、制表以及其他功能。 awk 基本语法：1awk &apos;BEGIN&#123;&#125; &#123;command&#125; END&#123;&#125;&apos; filename awk 是逐行处理文本内容的；BEGIN{} 是初识化代码块，在处理文件第一行内容之前，定义一些变量；{command} 为一些命令，对文件内容的每一行进行相应地处理；END{} 为结束代码块，在 {command}运行结束后执行。 格式1234命令行格式awk [options] &apos;command&apos; file(s)脚本格式awk -f awk-script-file file(s) 常用内置参数 12345$0，$1，$2... 表示整个当前行$1 每行第一个字段NF 字段数量变量NR 每行的记录号，多文件记录递增FILENAME 文件名 awk -F’\t’ ‘{print $1;}’ all-test.txt &gt; all_query.inc AWK是一种处理文本文件的语言，是一个强大的文本分析工具。之所以叫AWK是因为其取了三位创始人 Alfred Aho，Peter Weinberger, 和 Brian Kernighan 的 Family Name 的首字符。 awk 非常擅长处理结构化数据和生成表单.和 sed 和 grep 很相似.由于 awk 具备各种及哦啊本语言的特点,所以可以把它看做是一种脚本语言. 语法（有三种方式） 第一种方式: 1awk [-F 分隔符] ‘commands’ input-file(s) 在 awk 中默认使用空格间隔； 如果文件中使用冒号作为分隔符，那么必须使用 -F 选项:1awk -F : &apos;commands&apos; input-file 第二种方式 将所有 awk 命令插入一个单独文件，然后调用1awk -f awk-script-file input-file 常见的案例学习 1234567891011121314151617181920统计/etc/passwd:文件名，每行的行号，每行的列数，对应的完整行内容:awk -F &apos;:&apos; &apos;&#123;print &quot;filename:&quot; FILENAME &quot;,linenumber:&quot; NR &quot;,columns:&quot; NF &quot;,linecontent:&quot;$0&#125;&apos; /etc/passwd除了 awk 的内置变量,awk 还可以自定义变量.例如:统计/etc/passwd 的行数:awk &apos;&#123;count++&#125;END&#123;print count&#125;&apos; /etc/passwdcount 是自定义变量,这里没有初始化 count,虽然默认是 0,但是妥当的做法还是初始化为 0.awk &apos;BEGIN&#123;count=0&#125;&#123;count=count+1&#125;END&#123;print count&#125;&apos; /etc/passwd例如:统计某个文件夹下的文件占用的字节数ls -l |awk &apos;BEGIN &#123;size=0;&#125; &#123;size=size+$5;&#125; END&#123;print &quot;[end]size is &quot;, size&#125;&apos;如果按照 M 为单位显示ls -l |awk &apos;BEGIN &#123;size=0;&#125; &#123;size=size+$5;&#125; END&#123;print &quot;[end]size is &quot;, size/1024/1024,&quot;M&quot;&#125;&apos; shell 学习十七天—awk 命令 解释：打印包含 MA 的行中的第一个单词。(使用 echo 关键字可以模拟读入文件的操作) 1echo &apos;this is one world\nthat is another world&apos; | awk &apos;&#123;print $1&#125;&apos; 关于 -F 的使用 （设置分割符）默认使用空格分割；如果是passwd 文件，那么使用冒号作为分割符。其中的 $1 表示第一个字段， $0 表示所有的字段。 1awk -F : &apos;commands&apos; input_file 按照 [] 进行切分，然后打印第1，2，和3 个字段。1awk -F &apos;[\[\]]&apos; &apos;&#123;print $1, $2, $3&#125;&apos; log.txt 在输出的时候，可以加上字符串和转义字符之类的， 注意加上的这些东西是使用双引号表示的。1awk &apos;&#123;print $1 &quot;\t&quot; $2 &quot;\t&quot; $3&#125;&apos; fruit.txt 同时处理多个文件 1awk &apos;&#123;print FILENAME &quot;\t&quot; $0&#125;&apos; demo1.txt demo2.txt 在 awk 中使用变量 1awk &apos;&#123;msg=&quot;hello world&quot;; print msg &quot;\t&quot; $0&#125;&apos; log.txt 如果要判断文件的第 3 列数据，也就是平均工资小于 5500 的公司，然后将其打印输出 1awk &apos;$3 &lt; 5500 &#123;print $0&#125;&apos; company.txt 在 awk 中使用正则表达式 使用正则表达式匹配字符串 “There” ，将包含这个字符串的行打印并输出 1awk &apos;/There/&#123;print $0&#125;&apos; poetry.txt 使用正则表达式配一个包含字母 t 和字母 e ，并且 t 和 e 中间只能有任意单个字符的行 12345awk &apos;/t.e/&#123;print $0&#125;&apos; poetry.txtThere is nothing either good or bad, but thinking makes it soThere’s a special providence in the fall of a sparrowNo matter how dark long, may eventually in the day arrival 这个cat 命令也是不太懂得 cat base_meta | python segmentor.py ~/default_query/data &gt; base_meta.seg 常见的变量 变量NF表示当前行有多少个字段，因此$NF就代表最后一个字段。 12$ echo &apos;this is a test&apos; | awk &apos;&#123;print $NF&#125;&apos;test 1$(NF-1)代表倒数第二个字段。 print命令里面，如果原样输出字符，要放在双引号里面。 awk还提供了一些内置函数，方便对原始数据的处理。 函数toupper()用于将字符转为大写。 1234567tolower()：字符转为小写。length()：返回字符串长度。substr()：返回子字符串。sin()：正弦。cos()：余弦。sqrt()：平方根。rand()：随机数。 完整的built-in function 手册 12345678910# 输出奇数行$ awk -F &apos;:&apos; &apos;NR % 2 == 1 &#123;print $1&#125;&apos; demo.txtrootbinsync# 输出第三行以后的行$ awk -F &apos;:&apos; &apos;NR &gt;3 &#123;print $1&#125;&apos; demo.txtsyssync 下面的例子输出第一个字段等于指定值的行。 12345$ awk -F &apos;:&apos; &apos;$1 == &quot;root&quot; &#123;print $1&#125;&apos; demo.txtroot$ awk -F &apos;:&apos; &apos;$1 == &quot;root&quot; || $1 == &quot;bin&quot; &#123;print $1&#125;&apos; demo.txtrootbin if else 语句 1234$ awk -F &apos;:&apos; &apos;&#123;if ($1 &gt; &quot;m&quot;) print $1&#125;&apos; demo.txtrootsyssync 上面代码输出第一个字段的第一个字符大于m的行。 awk 处理文本 awk、grep、sed是linux操作文本的三大利器，也是必须掌握的linux命令之一。三者的功能都是处理文本，但侧重点各不相同，其中属awk功能最强大，但也最复杂。grep更适合单纯的查找或匹配文本，sed更适合编辑匹配到的文本，awk更适合格式化文本，对文本进行较复杂格式处理。 grep 命令grep（global search regular expression(RE) and print out the line，全面搜索正则表达式并把行打印出来）是一种强大的文本搜索工具，它能使用正则表达式搜索文本，并把匹配的行打印出来。 12345678910111213-d&lt;进行动作&gt; 当指定要查找的是目录而非文件时，必须使用这项参数，否则grep命令将回报信息并停止动作。-h 当搜索多个文件时，不显示匹配文件名前缀-i 忽略字符大小写的差别。-l 列出文件内容符合指定的范本样式的文件名称。-n 列出所有的匹配的文本行，并显示行号-r 递归搜索，搜索当前目录和子目录,此参数的效果和指定“-d recurse”参数相同。-v 反转查找。只显示不匹配的文本行 history | grep “ssh” # 在返回的结果中使用 grep 进行查找 sed 行编辑器sed是一种流编辑器，它是文本处理中非常重要的工具，能够完美的配合正则表达式使用，功能不同凡响。处理时，把当前处理的行存储在临时缓冲区中，称为“模式空间”（pattern space），接着用sed命令处理缓冲区中的内容，处理完成后，把缓冲区的内容送往屏幕。接着处理下一行，这样不断重复，直到文件末尾。文件内容并没有 改变，除非你使用重定向存储输出。Sed主要用来自动编辑一个或多个文件；简化对文件的反复操作；编写转换程序等。 常用的选项 12-e&lt;script&gt;或--expression=&lt;script&gt;：以选项中的指定的script来处理输入的文本文件；-n或--quiet或——silent：仅显示script处理后的结果； 常用的命令12345678910a\ 在当前行下面插入文本。i\ 在当前行上面插入文本。c\ 把选定的行改为新的文本。d 删除，删除选择的行。n 读取下一个输入行，用下一个命令处理新的行而不是用第一个命令。s 替换指定字符p 打印模板块的行。q 退出Sed。r file 从file中读行。w file 写并追加模板块到file末尾。 更多详细的例子 最好是动手试一下 sed 是以行为单位处理信息的，比如下面的命令可以输出第二行的信息：1ifconfig enp9s0f1 | sed -n &apos;2p&apos; inet 10.2.7.140 netmask 255.255.255.128 broadcast 10.2.7.255 同样使用 awk 也可以达到同样的效果， 使用NR 关键词。所以awk 相比于sed 是更加强大处理工具。1ifconfig em1 | awk NR==2 关于上述三个命令的练习awk 的练习题看上面的例子就行 强大的grep，sed和awk–用案例来讲解更多sed 详细的例子关于sed 的练习题 软链接 硬链接 和cp ln -s 源文件 目标文件 做软链接 软链接：通过软链接建立的链接文件与原文件并不是同一个文件，相当于原文件的快捷方式。 硬链接：硬链接的两个文件是独立的两个引用计数文件，他们共用同一份数据，所以他们- 的inode节点相同删除硬链接中的任意一个文件，另外一个文件不会被删除。 ln -s ~/sag_common/production/search/nlp/query_quality/src/libglog.so.0 /usr/lib6 ln -s log2013.log link2013 ssh 远程登录的命令 ssh -i .ssh/qiso_id_rsa qiso@10.39.22.30 使用 exit 退出当前账户的ssh 登录 ssh -i .ssh/qiso_id_rsa zhlu@10.52.80.185 # 这个是有gpu 的机器 ssh -i .ssh/qiso_id_rsa qiso@10.39.22.30 # 这个应该是hadoop 的机器 所以现在一共有三个机器，一个是我账户下的机器，一个是 gpu机器，一个是qiso 的机器 scp -i 表示使用ssh 进行传输文件 scp -r -i ~/.ssh/qiso_id_rsa qiso@10.39.22.30:/home/qiso/jarvis* ~/ scp -r -i ~/.ssh/qiso_id_rsa qiso@10.39.22.30:/home/qiso/data/production//search/nlp/default_query/data/traditi /home/qiso/data/production/search/nlp/query_quality/data/ scp -i Selects the file from which the identity (private key) for public # 使用 private key 进行测试 tar 命令 解包：tar zxvf FileName.tar打包：tar czvf FileName.tar DirName 两个命令连用： ps -ax 查看进程的， To see every process on the system using BSD syntax: ps -ax | grep vulgar_kill -9 pid 杀死进程 ps 是显示进程的 下面两个都是显示进程用的ps -eps -ef 执行shell 脚本 123sh -x vulgar_query_preprocessor.shscp -r -i ~/.ssh/qiso_id_rsa qiso@10.39.22.30:/home/qiso/data/production/search/nlp/query_quality/data/vulgar_query/model/ .//home/qiso/data/production/search/nlp/query_quality/data/vulgar_query/embedding/query_ngram_represent diff 命令 讲解： http://www.ruanyifeng.com/blog/2012/08/how_to_read_diff.html 比较两个文件 diff &lt;a.file&gt; &lt;b.file&gt; 最简单的形式，没有参数，得到的是3四行 4c4 #第一行是提示，表示变动的位置, c表示 change , a 表示 addition, d 表示 deletion &lt; a #表示 a.file 去除 a 内容 —- # 分隔符 > b #表示b.file 添加该行 alias 用来设置指令的别名 在机器上使用vim 而不要使用 vi alias vi=’/usr/bin/vim’ # 设置 alias alias vi=’vim’ # 设置指向 source, sh, bash, ./ 执行 shell 文件的区别 source source a.sh # 在当前的shell 中读取，a.sh 不需要有执行权利 . a.sh # 也可以写成这样，注意中间是有空格的 #这个是在当前的 shell 中，逐行读入的脚本，然后创建的变量都保存在当前的shell 中 sh/bash sh a.sh bash a.sh # 这两种方式都是打开一个 subshell 去执行， a.sh 不需要有执行权限 sh -x a.sh # 这个是 boss 常用的执行方式 ./ ./a.sh #打开一个subshell 执行， a.sh 是需要有执行权限的，如果没有可以通过 chmod +x 进行添加 环境变量 - set、env、export set命令显示当前shell的变量，包括当前用户的变量; env命令显示当前用户的变量; export命令显示当前导出成用户变量的shell变量。 查看命令-more, less, cat cat 是一次性显示整个文件的内容，还可以将多个文件连接起来显示，它常与重定向符号配合使用，适用于文件内容少的情况； more命令，功能类似 cat。more命令只能前向后读取文件，因此在启动时就加载整个文件。 less可以前后看，比more 功能更加强大。 linux 中 sort 123456sort file1.txt file2.txt &gt; sorted.txtsort file1.txt file2.txt -o sorted.txt #上面两种手工写法都是一样的cat sorted_file.txt | uniq &gt; uniq_lines.txt # 找出已排序文件中不重复的行：sort -k 2 data.txt # k表示的列数 依据第二列进行排序sort -nrk 1 data.txt #依据第一列进行逆序排序, -r 表示逆序 1依据第二列进行排序 查看cpu信息的方法 使用命令 lscpu 查看系统文件 less /proc/cpuinfo 使用三方的软件包 查看GPU 的信息： nvidia-smi 这个是查看NVIDIA 显卡的命令 linux 中的脚本调试 常用是以下的三种方式。 echo 方式输出 最简单的调试方法， 比如： echo $VAR 命令选项 -n 功能：读取shell脚本，但是不执行。 比如 bash –n script.sh 命令选项 -x 功能： 提供跟踪执行信息,将执行脚本的过程中把实际执行的每个命令显示出来，行首显示+, +后面显示经过替换之后的命令行内容，有助于分析实际执行的是什么命令. 其中”+” 表示执行的某个命令人，然后基于上一个”+” 使用”++” 表示更近一步的执行。很好理解的方式。 特点： 是shell 首选的调试方式。比如： 在命令行提供参数: sh -x script.sh 在脚本开头提供参数: #!/bin/sh -x 在脚本中用 set 命令启用or 禁用参数： set -x 表示启用， set +x 表示禁用 管道linux 中有些命令是可以带参数，接受“标准输入”作为参数1cat /etc/passwd | grep root 其中的 | 是管道命令。左侧的命令(cat /etc/passwd)的标准输出作为后面命令的输入。其中grep 是可以接受标准输入作为参数。上面的命令等同于1grep root /etc/passwd 但是大多数的命令是不接受标准输入作为参数，比如说 echo 这个使用后就出现了 xargs 命令, 该命令将标准输入转换成命令行参数。 12echo &quot;hello world&quot; | xargs echo# hello world 就是输出 该命令的格式如下1$ xargs [-options] [command] xargs的作用在于，大多数命令（比如rm、mkdir、ls）与管道一起使用时，都需要xargs将标准输入转为命令行参数。 默认情况下， xargs 将换行符和空格作为分隔符，把标准输入分解成一个个命令行参数。1echo &quot;one two three&quot; | xargs mkdir 使用 -d 可以更改分隔符1echo -e &quot;a\tb\tc&quot; | xargs -d &quot;\t&quot; echo 参数 -p，可以确认到底执行的是什么命令 -p参数打印出要执行的命令，询问用户是否要执行。 12$ echo &apos;one two three&apos; | xargs -p touchtouch one two three ?... -t参数则是打印出最终要执行的命令，然后直接执行，不需要用户确认。 12$ echo &apos;one two three&apos; | xargs -t rmrm one two three examples xargs 和cp 命令结合使用过，返回前 5 个结果作为 cp 的输入。1234ls | head -n 5| xargs -i cp &#123;&#125; /home/jijeng/projects/test/src# 回到上一个目录（最近的目录）cd - 其他小命令查看端口在mac 上使用 lsof: list open files 12lsof -i # 查看某个端口是否被占用， i 表示ip tcp 表示tcp 协议lsof -i tcp:8888 比如说查看服务器8000 端口(port )的占用情况 linux 中可以使用 lsof 和netstat 两个命令 netstat用来查看系统当前系统网络状态信息，包括端口，连接情况等，常用方式如下： netstat -atunlp,各参数含义如下:-t : 指明显示TCP端口-u : 指明显示UDP端口-l : 仅显示监听套接字(LISTEN状态的套接字)-p : 显示进程标识符和程序名称，每一个套接字/端口都属于一个程序-n : 不进行DNS解析-a 显示所有连接的端口 1234567lsof -i:端口号netstat -nltp | grep 进程号netstat -tunlp | grep 22netstat -tunlp | grep 端口号ps -ef | grep 进程名 xargs该命令和管道的命令是类似的，不同点在于， xargs 是把 | 左边的输出当做后面命令的一个参数来运行。而如果只是使用|， 是把其左边的输出当做右边命令的输入。xargs命令是给其他命令传递参数的一个过滤器，也是组合多个命令的一个工具。它擅长将标准输入数据转换成命令行参数，xargs能够处理管道或者stdin并将其转换成特定命令的命令参数。 Linux 中脚本执行的四种方式（1）切换到shell脚本所在的目录（此时，称为工作目录）执行shell脚本 1./hello.sh 其中命令行中的.表示当前工作目录。不是什么执行之类的符号（2）使用绝对路径执行1/data/shell/hello.sh 上述两种方法实际上是一类，文件执行时候的bash 路径是文件第一行声明的bash路径；需要执行权限。（3）使用命令bash或者 sh执行脚本12bash hello.shsh hello.sh 这种方式不需要设定执行权限；不需要执行bash路径，因为使用的是当前bash 的路径。 上述三种（(1),(2), (3)）都是在父shell中开启了一个子shell，执行的脚本是在子shell 中执行，当执行完成之后，子shell就关闭，然后回到父shell 中。（4）使用source命令执行 1source hello.sh source命令是作用于整个父进程，这个是和上述三者的最大区别。比如说，如果不想注销系统的，但是想让全局配置文件生效，那么就使用source命令。1source /etc/profile git 常见的命令git命令 git 命令清单 http://www.ruanyifeng.com/blog/2015/12/git-cheat-sheet.html git checkout # 检出分支， 创建分支和切换分支 git checkout [branch-name] # 切换到指定分支 git checkout -b dev origin/dev git checkout -b [branch] #新建一个分支 并切换到该分支 git branch [branch] # 新建一个分支 并停留在当前的分支 git checkout dev git checkout master git fetch 命令用于从另一个存储库下载对象和引用。 git fetch &lt;远程主机名&gt; &lt;分支名&gt; git fetch #更新所有分支，命令可以简写为： git fetch origin git fetch [remote] # 下载远程仓库所有的变动 git push origin dev git push [remote] [branch] # 上传本地指定分支到远程仓库 git commit -am “update” -a append, 追加修改过的文件然后提交到仓库里面 -m message 添加message 在每次执行 git commit之前先使用git status检查文件状态是一个很好的习惯, 这样能防止你不小心提交了您不想提交的东西。 git status Git命令详解 bash相关快捷键bash 指得是命令行。 123456789101112131415ctrl键组合ctrl+a:光标移到行首。ctrl+c:杀死当前进程。ctrl+d:退出当前 Shell。ctrl+e:光标移到行尾。esc组合esc+d: 删除光标后的一个词esc+f: 往右跳一个词， 不能连续跳，需要按一下然后停一下。esc+b: 往左跳一个词esc+t: 交换光标位置前的两个单词。 vi 常见命令和配置 第一阶段 123456789i → Insert 模式，按 ESC 回到 Normal 模式.x → 删当前光标所在的一个字符。:wq → 存盘 + 退出 (:w 存盘, :q 退出) （陈皓注：:w 后可以跟文件名）dd → 删除当前行，并把删除的行存到剪贴板里p → 粘贴剪贴板推荐:hjkl (强例推荐使用其移动光标，但不必需) →你也可以使用光标键 (←↓↑→). 注: j 就像下箭头。:help &lt;command&gt; → 显示相关命令的帮助。你也可以就输入 :help 而不跟命令。（陈皓注：退出帮助需要输入:q） 第二阶段 各种插入模式 1234a → 在光标后插入o → 在当前行后插入一个新行O → 在当前行前插入一个新行cw → 替换从光标所在位置后到一个单词结尾的字符 简单的移动光标 123450 → 数字零，到行头^ → 到本行第一个不是blank字符的位置（所谓blank字符就是空格，tab，换行，回车等）$ → 到本行行尾g_ → 到本行最后一个不是blank字符的位置。/pattern → 搜索 pattern 的字符串（陈皓注：如果搜索出多个匹配，可按n键到下一个） 拷贝/粘贴 12P → 粘贴yy → 拷贝当前行当行于 ddP Undo/Redo 12u → undo&lt;C-r&gt; → redo 打开/保存/退出/改变文件(Buffer) 123456:e &lt;path/to/file&gt; → 打开一个文件:w → 存盘:saveas &lt;path/to/file&gt; → 另存为 &lt;path/to/file&gt;:x， ZZ 或 :wq → 保存并退出 (:x 表示仅在需要时保存，ZZ不需要输入冒号并回车):q! → 退出不保存 :qa! 强行退出所有的正在编辑的文件，就算别的文件有更改。:bn 和 :bp → 你可以同时打开很多文件，使用这两个命令来切换下一个或上一个文件。（陈皓注：我喜欢使用:n到下一个文件） 第三阶段 使得 vim 重复任务 12. → 重复上一个命令—— 100 “desu “.3. → 重复 3 次 “desu” (注意：不是 300，你看，VIM多聪明啊). 光标的移动 1234567891011NG → 到第 N 行 （陈皓注：注意命令中的G是大写的，另我一般使用 : N 到第N行，如 :137 到第137行）gg → 到第一行。（陈皓注：相当于1G，或 :1）G → 到最后一行。按单词移动：w → 到下一个单词的开头。e → 到下一个单词的结尾。&gt; 如果你认为单词是由默认方式，那么就用小写的e和w。默认上来说，一个单词由字母，数字和下划线组成（陈皓注：程序变量）&gt; 如果你认为单词是由blank字符分隔符，那么你需要使用大写的E和W。（陈皓注：程序语句）% : 匹配括号移动，包括 (, &#123;, [. （陈皓注：你需要把光标先移到括号上）* 和 #: 匹配光标当前所在的单词，移动光标到下一个（或上一个）匹配单词（*是下一个，#是上一个） 第四阶段 12345678910在当前行上移动光标: 0 ^ $ f F t T , ;0 → 到行头^ → 到本行的第一个非blank字符$ → 到行尾g_ → 到本行最后一个不是blank字符的位置。fa → 到下一个为a的字符处，你也可以fs到下一个为s的字符。t, → 到逗号前的第一个字符。逗号可以变成其它字符。3fa → 在当前行查找第三个出现的a。F 和 T → 和 f 和 t 一样，只不过是相反方向。 区域选择 a 或 i(这个部分没有看懂，可以详细的看原文) 块操作: 12345块操作，典型的操作： 0 &lt;C-v&gt; &lt;C-d&gt; I-- [ESC]^ → 到行头&lt;C-v&gt; → 开始块操作&lt;C-d&gt; → 向下移动 (你也可以使用hjkl来移动光标，或是使用%，或是别的)I-- [ESC] → I是插入，插入“--”，按ESC键来为每一行生效。 可视化之后的操作123J → 把所有的行连接起来（变成一行）&lt; 或 &gt; → 左右缩进= → 自动给缩进 （陈皓注：这个功能相当强大，我太喜欢了） 分屏: :split 和 vsplit.（这个我个人使用的不多，更加的倾向于使用软件分屏，而不是命令分屏） 简明 VIM 练级攻略 使用技能： 使用 vi 命令搜索下一个字符串所在的位置 按esc键，输入/string回车，就能定位字符串string，按n定位下一个 linux 中进程和线程的关系 几种进程间的通信方式 管道（pipe）：管道是一种半双工的通信方式，数据只能单向流动，而且只能在具有血缘关系的进程间使用。进程的血缘关系通常指父子进程关系。 有名管道（named pipe）：有名管道也是半双工的通信方式，但是它允许无亲缘关系进程间通信。 信号量（semophore）：信号量是一个计数器，可以用来控制多个进程对共享资源的访问。它通常作为一种锁机制，防止某进程正在访问共享资源时，其他进程也访问该资源。因此，主要作为进程间以及同一进程内不同线程之间的同步手段。 消息队列（message queue）：消息队列是由消息组成的链表，存放在内核中 并由消息队列标识符标识。消息队列克服了信号传递信息少，管道只能承载无格式字节流以及缓冲区大小受限等缺点。 信号（signal）：信号是一种比较复杂的通信方式，用于通知接收进程某一事件已经发生。 共享内存（shared memory）：共享内存就是映射一段能被其他进程所访问的内存，这段共享内存由一个进程创建，但多个进程都可以访问，共享内存是最快的IPC方式，它是针对其他进程间的通信方式运行效率低而专门设计的。它往往与其他通信机制，如信号量配合使用，来实现进程间的同步和通信。 套接字（socket）：套接口也是一种进程间的通信机制，与其他通信机制不同的是它可以用于不同及其间的进程通信。 几种线程间的通信机制 锁机制 互斥锁：提供了以排它方式阻止数据结构被并发修改的方法。 读写锁：允许多个线程同时读共享数据，而对写操作互斥。 条件变量：可以以原子的方式阻塞进程，直到某个特定条件为真为止。对条件测试是在互斥锁的保护下进行的。条件变量始终与互斥锁一起使用。 信号量机制：包括无名线程信号量与有名线程信号量 信号机制：类似于进程间的信号处理。 线程间通信的主要目的是用于线程同步，所以线程没有象进程通信中用于数据交换的通信机制。 更多关于这方面的介绍可以参考这里]]></content>
      <categories>
        <category>CS基础</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>github</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode-List]]></title>
    <url>%2F2019%2F02%2F16%2Fleetcode-list%2F</url>
    <content type="text"><![CDATA[LeetCode 刷题总结（二）， 使用Python 实现。该篇题目类型主要是： list, linkedList 还有简单的 tree。 Add Two Numbers You are given two non-empty linked lists representing two non-negative integers. The digits are stored in reverse order and each of their nodes contain a single digit. Add the two numbers and return it as a linked list. Input: (2 -&gt; 4 -&gt; 3) + (5 -&gt; 6 -&gt; 4)Output: 7 -&gt; 0 -&gt; 8Explanation: 342 + 465 = 807. Tips: 加法的过程，使用 \% 和 整除进行求解，使用linkedList 进行存储。https://leetcode.com/problems/add-two-numbers/ 123456789101112131415161718192021222324252627# Definition for singly-linked list.# class ListNode(object):# def __init__(self, x):# self.val = x# self.next = Noneclass Solution(object): def addTwoNumbers(self, l1, l2): """ :type l1: ListNode :type l2: ListNode :rtype: ListNode """ dummy = cur = ListNode(0) carry = 0 while l1 or l2 or carry: if l1: carry += l1.val l1 = l1.next if l2: carry += l2.val l2 = l2.next cur.next = ListNode(carry%10) cur = cur.next carry //= 10 return dummy.next Remove Nth Node From End of List Given a linked list, remove the n-th node from the end of list and return its head. Tips: 可以使用”两指针“ 方法进行求解，前后两指针相差 N步数。 12345678910111213141516171819202122232425262728293031323334353637# Definition for singly-linked list.# class ListNode(object):# def __init__(self, x):# self.val = x# self.next = Noneclass Solution(object): def removeNthFromEnd(self, head, n): """ :type head: ListNode :type n: int :rtype: ListNode """ if not head or not head.next: return None new_head =ListNode(-1) new_head.next =head fast =new_head # 先走n 步 for i in range(n): # 这里很好的处理了 n &gt; 链表长度 情况 if fast.next: fast =fast.next else: return head slow =new_head # 一块走 # 因为是 fast 先走到none，所以这个判断条件 while fast.next: fast =fast.next slow =slow.next slow.next =slow.next.next # 这个指向是经验性，还是超级nice的 return new_head.next Merge Two Sorted Lists Merge two sorted linked lists and return it as a new list. The new list should be made by splicing together the nodes of the first two lists. Tips: 归并操作中的”并“ 操作。 https://leetcode.com/problems/merge-two-sorted-lists/ 123456789101112131415161718192021222324252627282930313233343536373839404142# Definition for singly-linked list.# class ListNode(object):# def __init__(self, x):# self.val = x# self.next = Noneclass Solution(object): def mergeTwoLists(self, l1, l2): """ :type l1: ListNode :type l2: ListNode :rtype: ListNode """ if not l1 and not l2: return None elif not l1 or not l2: return l1 or l2 # 这里新建了 node，作为最后返回list 中的head，因为使用哪个子list 都是不确定的 new_node =ListNode(-1) cur =new_node head1 =l1 head2 =l2 while head1 and head2: if head1.val &lt; head2.val: cur.next =head1 head1 =head1.next else: cur.next =head2 head2 =head2.next cur =cur.next # 对于这种 if else 需要是相当的清楚 if head1: cur.next =head1 elif head2: cur.next =head2 return new_node.next Swap Nodes in Pairs Given a linked list, swap every two adjacent nodes and return its head.You may not modify the values in the list’s nodes, only nodes itself may be changed. Given 1-&gt;2-&gt;3-&gt;4, you should return the list as 2-&gt;1-&gt;4-&gt;3. Tips: 常见的类型，使用三个指针修改指向。经常创建 dummy指针，如果head 可能被改变的话。 1234567891011121314151617181920212223242526272829303132# Definition for singly-linked list.# class ListNode(object):# def __init__(self, x):# self.val = x# self.next = Noneclass Solution(object): """ 三个指针的修改，应该是没有问题的 """ def swapPairs(self, head): """ :type head: ListNode :rtype: ListNode """ if not head: return head dummy=pre =ListNode(-1) dummy.next =head while True: cur =pre.next if not cur: break nex =cur.next if not nex: break pre.next, cur.next, nex.next, pre =nex, nex.next, cur, cur return dummy.next Count and Say The count-and-say sequence is the sequence of integers with the first five terms as following: 1. 1 2. 11 3. 21 4. 1211 5. 111221 1 is read off as &quot;one 1&quot; or 11. 11 is read off as &quot;two 1s&quot; or 21. 21 is read off as &quot;one 2, then one 1&quot; or 1211. Tips: 这个是属于循环，字符串处理。 12345678910111213141516171819202122232425262728class Solution(object): def doCountAndSay(self, string): char =string[0] num =0 result ="" for c in string: if char ==c: num +=1 else: result += (str(num)+ char) char =c num =1 result += (str(num) +char) return result def countAndSay(self, n): if 0 ==n: return "" elif 1== n: return "1" result ='1' for i in range(1, n): result =self.doCountAndSay(result) return result Trapping Rain Water Given n non-negative integers representing an elevation map where the width of each bar is 1, compute how much water it is able to trap after raining. Tips: 算法比较巧妙，左右两边进行遍历找出”累计“最高点，在O(N) 时间内完成。能装的水，取决于左右两边( neighbor) 的小值- height[i]。 https://leetcode.com/problems/trapping-rain-water/ 1234567891011121314151617181920212223242526class Solution(object): def trap(self, height): if not height: return 0 len_h =len(height) leftmax=[0]* len_h max_h=0 for i in range(len_h): if height[i] &gt;max_h: max_h =height[i] leftmax[i] =max_h rightmax =[0] *len_h max_h =0 for i in range(len_h-1, -1, -1): if height[i]&gt; max_h: max_h =height[i] rightmax[i] =max_h result =0 for i in range(len_h): result += (min(leftmax[i], rightmax[i])- height[i]) return result Maximum Subarray Given an integer array nums, find the contiguous subarray (containing at least one number) which has the largest sum and return its sum. Input: [-2,1,-3,4,-1,2,1,-5,4], Output: 6 Explanation: [4,-1,2,1] has the largest sum = 6. Tips: 一维数组返回最大的子数组，字符串处理。https://leetcode.com/problems/maximum-subarray/ 1234567891011121314151617181920212223class Solution(object): def maxSubArray(self, nums): """ :type nums: List[int] :rtype: int """ len_n =len(nums) if len_n ==1: return nums[0] max_n =nums[0] # 如果提前初始化，那么之后在更新max_n 时候就不用进行 None 的判断了 sum_n =0 for num in nums: sum_n += num if sum_n&gt; max_n: max_n =sum_n if sum_n &lt;0: sum_n =0 # continue return max_n Insert Interval Given a set of non-overlapping intervals, insert a new interval into the intervals (merge if necessary).You may assume that the intervals were initially sorted according to their start times. Input: intervals = [[1,3],[6,9]], newInterval = [2,5] Output: [[1,5],[6,9]] Tips: 使用( 0, 1) 去区分 start, end 这种点，类似一种数据结构的样子, 排序之后结果的start index 和end index 分别出现在首段和尾段，使用栈进行存储 start index 就行了。https://leetcode.com/problems/insert-interval/ 123456789101112131415161718192021222324252627282930313233343536class Solution(object): def insert(self, intervals, newinterval): """ 这个每个都是有 数据类型的，这个就是 LeetCode 的优点 :type intervals: List[List[int]] :type newInterval: List[int] :rtype: List[List[int]] """ if not intervals: return [newinterval] datas =[] if intervals: datas.append((newinterval[0], 0)) datas.append((newinterval[1], 1)) for interval in intervals: datas.append((interval[0], 0)) datas.append((interval[1], 1)) datas.sort() # sort() 是一个骚操作, 默认的排序显示根据 tuple[0] 进行排序，如果相同，那么根据 tuple[1] 进行排序 # 所以排在前面的一定是 start index。 merged =[] stack =[datas[0]] for i in range(1, len(datas)): data =datas[i] if data[1] ==0: stack.append(data) elif data[1] ==1: if stack: start =stack.pop() if len(stack) ==0: # 这个时候 data 是 end point merged.append((start[0], data[0])) return merged Rotate List Given a linked list, rotate the list to the right by k places, where k is non-negative. Tips： 细节在于k 可能大于 list 的长度。 123456789101112131415161718192021222324252627282930313233343536# Definition for singly-linked list.# class ListNode(object):# def __init__(self, x):# self.val = x# self.next = Noneclass Solution(object): def rotateRight(self, head, k): if k ==0: return head if not head: return head # 头指针 dummy =ListNode(-1) dummy.next =head p =dummy count =0 while p.next: p =p.next count +=1 # 指向了头指针，连成了一个环，下一步开始找头指针 p.next =dummy.next step =count -(k% count) for i in range(step): p =p.next # 找到了头指针，那么下一个就是尾指针 head =p.next p.next =None return head Unique Paths A robot is located at the top-left corner of a m x n grid (marked ‘Start’ in the diagram below).The robot can only move either down or right at any point in time. The robot is trying to reach the bottom-right corner of the grid (marked ‘Finish’ in the diagram below).How many possible unique paths are there? Tips： 最后求解的是unique paths 的数量，而不是具体的路径，所以可以不使用 dp，成了一道模拟排列组合的数学题。 12345678910111213141516171819202122class Solution(object): # 这个总的步数是一定的 m+n -1 ，然后下行和右行也是一定的， # 所以这个是模拟的“排列组合” 的思想 def uniquePaths(self, m, n): """ :type m: int :type n: int :rtype: int """ if m ==0 or n ==0: return 1 up =1 # 这个是分子 for i in range(m+n-2, n -1, -1): up *=i down =1 # 这个是分母 for j in range(1, m): down *= j return up/down Sort Colors Given an array with n objects colored red, white or blue, sort them in-place so that objects of the same color are adjacent, with the colors in the order red, white and blue.Here, we will use the integers 0, 1, and 2 to represent the color red, white, and blue respectively. Tips: 快排思想， 中间数字1 当做key index，左右两边分别是left，right index。 1234567891011121314151617class Solution(object): """ 0, 1, 2 (red, white, blue) """ def sortColors(self, nums): # zero and r record the position of "0" and "2" respectively index, two, zero = 0, len(nums) - 1, 0 while index &lt;= two: if nums[index] == 0: nums[index], nums[zero] = nums[zero], nums[index] index += 1; zero += 1 elif nums[index] == 2: nums[index], nums[two] = nums[two], nums[index] two -= 1 else: index += 1 Remove Duplicates from Sorted List II Given a sorted linked list, delete all nodes that have duplicate numbers, leaving only distinct numbers from the original list. Tips: 常规题，经常出现这样的逻辑， if… while ，如果发现有重复的，那么一直就找到不重复为止。 12345678910111213141516171819202122232425262728# Definition for singly-linked list.# class ListNode(object):# def __init__(self, x):# self.val = x# self.next = Noneclass Solution(object): """ 就是在删除节点的时候，如果head 节点也得删除，这个时候 常常创建一个 dummy 结点 求解的是distinct 的list """ def deleteDuplicates(self, head): dummy =pre =ListNode(0) dummy.next =head while head and head.next: if head.val ==head.next.val: while head and head.next and head.val ==head.next.val: head =head.next head =head.next pre.next =head else: # 这个更新很有意思， head =head.next, pre.next =head pre =pre.next head =head.next return dummy.next Partition List Given a linked list and a value x, partition it such that all nodes less than x come before nodes greater than or equal to x.You should preserve the original relative order of the nodes in each of the two partitions. Tips: 新建了两个结点，分别连接小于 x 和不小于 x 的结点，最后两个结点相连。 list 是直接进行交换位置，但是linkedList 不是这样的。 123456789101112131415161718192021222324252627282930# Definition for singly-linked list.# class ListNode(object):# def __init__(self, x):# self.val = x# self.next = Noneclass Solution(object): # 局部排序，不是全局排序 def partition(self, head, x): """ :type head: ListNode :type x: int :rtype: ListNode """ small =l1 =ListNode(0) great =l2 =ListNode(0) while head: if head.val &lt;x : l1.next =head l1 =l1.next else: l2.next =head l2 =l2.next head =head.next # 这个是一个细节， 最后l2 是需要一个none 进行结束标记 l2.next =None l1.next =great.next return small.next Reverse Linked List II Reverse a linked list from position m to n. Do it in one-pass.Note: 1 ≤ m ≤ n ≤ length of list. Tips: 局部进行reverse，找到该节点，然后迭代进行就可以了。 1234567891011121314151617181920212223242526# Definition for singly-linked list.# class ListNode(object):# def __init__(self, x):# self.val = x# self.next = Noneclass Solution(object): # The idea is simple and intuitive: find linkedlist [m, n], reverse it, then connect m with n+1, connect n with m-1 def reverseBetween(self, head, m, n): pre =dummy =ListNode(0) dummy.next =head #cur, pre =head, dummy for _ in range(m-1): #cur =cur.next pre =pre.next cur =pre.next for _ in range(n-m): tmp =cur.next cur.next =tmp.next tmp.next =pre.next pre.next =tmp return dummy.next Unique Binary Search Trees Given n, how many structurally unique BST’s (binary search trees) that store values 1 … n? Tips: 从处理子问题的角度来看，选取一个结点为根，就把结点切成左右子树，以这个结点为根的可行二叉树数量就是左右子树可行二叉树数量的乘积，所以总的数量是将以所有结点为根的可行结果累加起来。 123456789101112131415161718class Solution(object): # 二叉搜索树，当且仅当中序遍历的时候是单调非减的时候。 # 数学问题 def numTrees(self, n): """ :type n: int :rtype: int """ arr =[0]*(n+1) arr[0] =1 for i in range(1, n+1): for j in range(1, i+1): # 处理的是左右子树的乘积 arr[i] += arr[j-1] *arr[i-j] return arr[-1] Best Time to Buy and Sell Stock Say you have an array for which the ith element is the price of a given stock on day i.If you were only permitted to complete at most one transaction (i.e., buy one and sell one share of the stock), design an algorithm to find the maximum profit.Note that you cannot sell a stock before you buy one. Tips: 虽然这个说是最多一次买入卖出，但是这个价格变化是”连续“的，所以只要是下一个大于上一个就是可以 += profit 中的。 12345678910111213141516class Solution(object): # 从代码上来看，毫无算法可言 def maxProfit(self, prices): """ :type prices: List[int] :rtype: int """ if not prices or len(prices) ==1: return 0 profit =0 for i in range(1, len(prices)): if prices[i] &gt; prices[i-1]: profit += prices[i]-prices[i-1] return profit Best Time to Buy and Sell Stock II Say you have an array for which the ith element is the price of a given stock on day i.Design an algorithm to find the maximum profit. You may complete as many transactions as you like (i.e., buy one and sell one share of the stock multiple times).Note: You may not engage in multiple transactions at the same time (i.e., you must sell the stock before you buy again). Tips: 可以进行多次买卖。和上面的区别在于 下一个只要不小于上一个就是可以累加的。 123456789101112131415class Solution(object): # 可以多次买卖， 买一次然后卖一次。不能多次买入 # 这种就如同寻找的是 增序列。 def maxProfit(self, prices): """ :type prices: List[int] :rtype: int """ total =0 for i in range(1, len(prices)): if prices[i]&gt;= prices[i-1]: total += prices[i]-prices[i-1] return total Longest Consecutive Sequence Given an unsorted array of integers, find the length of the longest consecutive elements sequence.Your algorithm should run in O(n) complexity. Tips： 这种方法很巧妙，如果x-1 not in，那么去 try x+1，然后计数。 123456789101112131415161718class Solution(object): # 第一印象是 先进行排序， 然后选择的过程， 但是限制条件是 O(n) 的复杂度 def longestConsecutive(self, nums): """ :type nums: List[int] :rtype: int """ nums =set(nums) best =0 for x in nums: if x-1 not in nums: y =x+1 while y in nums: y +=1 best =max(best, y-x) return best Candy There are N children standing in a line. Each child is assigned a rating value.You are giving candies to these children subjected to the following requirements: Each child must have at least one candy. Children with a higher rating get more candies than their neighbors.What is the minimum candies you must give? Tips: 因为涉及到 neighbors，所以左右两边进行遍历，因为如果ratings 大的话，那么结果一定得大，所以返回的是较大者。 1234567891011121314151617181920class Solution(object): def candy(self, ratings): """ :type ratings: List[int] :rtype: int """ # 满足第一个条件，at least one candy res =len(ratings) *[1] # left to right, higher then more candies for i in range(1, len(ratings)): if ratings[i] &gt; ratings[i-1]: # 这个是严格的 &gt; res[i] = res[i-1] +1 # right to left, higher then more candy, neighbors for i in range(len(ratings)-1, 0, -1): if ratings[i-1] &gt; ratings[i]: res[i-1] =max(res[i-1], res[i] +1) return sum(res) Single Number Given a non-empty array of integers, every element appears twice except for one. Find that single one. Tips: 异或的性质 1234567891011121314class Solution(object): # 分分钟 异或就出来了 # integers, -2 -1 0 1 2 这样的数字 def singleNumber(self, nums): """ :type nums: List[int] :rtype: int """ standard =0 # 如果正好是其中的 0 出现一次，也是没有关系的， 因为初始化的 0 和 list 中的单数 0 正好匹配，异或操作之后相同为 0 # 结果上是没有什么问题的 for num in nums: standard = num^ standard return standard Single Number II Given a non-empty array of integers, every element appears three times except for one, which appears exactly once. Find that single one. Tips：这个 three times不能使用 异或，从二进制的角度进行考虑，以二进制的形式，将数字存储起来，如果是出现了 3次，那么 %3 结果就是0，最后只是剩下了 那个出现一次的数字 1234567891011121314151617181920212223242526class Solution(object): # 只是出现的一个的single one， 其他的出现三次 # var |= value is short for var = var | value def singleNumber(self, nums): """ :type nums: List[int] :rtype: int """ bit = [0] * 32 for num in nums: for i in range(32): bit[i] += num &gt;&gt; i &amp; 1 # 这个就是从左往右的顺序，先是进行 &gt;&gt; 运算，然后是 &amp; 运算 # 可以想象这个重复计算比较多，因为每次都需要 num &gt;&gt; i 进行位运算 res = 0 for i, val in enumerate(bit): # if the single numble is negative, # this case should be considered separately , 补码 和原码的转换关系 if i == 31 and val % 3: res = -((1 &lt;&lt; 31) - res) else: res |= (val % 3) * (1 &lt;&lt; i) # | 这个是位操作，更加类似不断的取 1 的过程， 然后和该位置的权重相乘 return res Copy List with Random Pointer A linked list is given such that each node contains an additional random pointer which could point to any node in the list or null.Return a deep copy of the list. Tips：在剑指offer 上是通过 指针操作进行做题，但是使用 defaultdict 基本上就不用出，使用dict 来处理这种关系，最后返回的是根节点、 123456789101112131415161718192021222324252627282930"""# Definition for a Node.class Node(object): def __init__(self, val, next, random): self.val = val self.next = next self.random = random"""# 使用dict 有没有感觉在作弊class Solution(object): # 做过这个 def copyRandomList(self, head): """ :type head: Node :rtype: Node """ import ipdb dic = collections.defaultdict(lambda: Node(0, None, None)) # 这个就是给定了一个默认的值, 直接初始化dict 中的value 为这个node # dict 的本身就是存储一种node 的关系，所以dict[n].val , next, random 可以这样进行操作 dic[None] = None n = head while n: dic[n].val = n.val dic[n].next = dic[n.next] dic[n].random = dic[n.random] n = n.next #ipdb.set_trace() return dic[head] Linked List Cycle Given a linked list, determine if it has a cycle in it.To represent a cycle in the given linked list, we use an integer pos which represents the position (0-indexed) in the linked list where tail connects to. If pos is -1, then there is no cycle in the linked list. Tips：快慢两个指针的问题，给了两种方法来实现。 1234567891011121314151617181920212223242526272829303132333435# Definition for singly-linked list.# class ListNode(object):# def __init__(self, x):# self.val = x# self.next = Noneclass Solution(object): # O(1) == constant memory, def hasCycle(self, head): """ :type head: ListNode :rtype: bool """ ''' # 这种不用进行判断 fast 是否可以访问的原因在于 try except 的使用 try: slow = head fast = head.next while slow is not fast: slow = slow.next fast = fast.next.next return True except: return False ''' # 这个是比较中规中矩的写法 slow = fast = head # 注意使用的是 fast进行判断，因为这个走的快 while fast and fast.next: fast = fast.next.next slow = slow.next if slow == fast: return True return False Reorder List Given a singly linked list L: L0→L1→…→Ln-1→Ln,reorder it to: L0→Ln→L1→Ln-1→L2→Ln-2→…You may not modify the values in the list’s nodes, only nodes itself may be changed. Tips： 从中间断开，后半部分翻转，然后和前半部分轮流连接 12345678910111213141516171819202122232425262728293031323334353637383940414243# Definition for singly-linked list.# class ListNode(object):# def __init__(self, x):# self.val = x# self.next = Noneclass Solution(object): def reorderList(self, head): """ :type head: ListNode :rtype: None Do not return anything, modify head in-place instead. """ if not head: return fast, slow = head.next, head # first part has the same or one more node while fast and fast.next: fast = fast.next.next slow = slow.next # reverse the send half p = slow.next slow.next = None node = None # 类似上一个结点， p 是cur 的结点 while p: nex = p.next p.next = node node = p p = nex # combine head part and node part p = head while node: tmp = node.next node.next = p.next # 两个 next 指向操作, 需要next 两次 p.next = node p =p.next.next node = tmp ** Majority Element Given an array of size n, find the majority element. The majority element is the element that appears more than ⌊ n/2 ⌋ times. Output: 2Input: K = 1, N = 2Explanation:Drop the egg from floor 1. If it breaks, we know with certainty that F = 0.Otherwise, drop the egg from floor 2. If it breaks, we know with certainty that F = 1.If it didn’t break, then we know with certainty F = 2.Hence, we needed 2 moves in the worst case to know what F is with certainty. Tips: 可以查看 solution 中的(讲解)[https://leetcode.com/problems/super-egg-drop/] 123456789101112131415161718class Solution(object): def majorityElement(self, nums): """ :type nums: List[int] :rtype: int """ # 初始化 counts =0 for num in nums: if counts ==0: majority =num counts =1 elif majority ==num: counts +=1 else: counts -=1 return majority Maximum Product Subarray Given an integer array nums, find the contiguous subarray within an array (containing at least one number) which has the largest product. Tips: 在于左右两遍遍历，分别得到 prefix和 suffix 的乘积。这个速度上比较快在于存储了之前的结果。实现的时候利用了 1 or prefix[i-1] 这种技巧。 12345678910111213class Solution(object): def maxProduct(self, nums): """ :type nums: List[int] :rtype: int """ prefix =nums suffix =prefix[::-1] for i in range(1, len(prefix)): prefix[i] *= 1 or prefix[i-1] suffix[i] *= 1 or suffix[i-1] return max(prefix+ suffix) Majority Element Given an array of size n, find the majority element. The majority element is the element that appears more than ⌊ n/2 ⌋ times. You may assume that the array is non-empty and the majority element always exist in the array. Tips: majority 的counts 的总数是大于其他所有counts相加之和的 123456789101112131415161718class Solution(object): def majorityElement(self, nums): """ :type nums: List[int] :rtype: int """ majority =None counts =0 for num in nums: if not majority or counts ==0: majority =num counts =1 elif num ==majority: counts +=1 else: counts -=1 return majority Rotate Array Given an array, rotate the array to the right by k steps, where k is non-negative. Tips： 对于python 而言，是不存在切分字符串算法的，一步操作。小的细节是 k %len(array) 更加合理 12345678910111213class Solution(object): def rotate(self, nums, k): """ :type nums: List[int] :type k: int :rtype: None Do not return anything, modify nums in-place instead. """ # 这个in-place 操作是不需要 return k =k%len(nums) # 这个是一个细节吧 #nums[:] =nums[-k:] +nums[:k+1] nums[:] =nums[-k:] +nums[:-k] # 左边有时候是nums 就行，有时候必须nums[:] 表示index的操作，因为环境的问题 # 这种 对称的切分真的是比较好看 Contains Duplicate Given an array of integers, find if the array contains any duplicates.Your function should return true if any value appears at least twice in the array, and it should return false if every element is distinct. Tips: 有很多种方法，比如 dictionary or set，这个简单之处最后返回的是 true or false，不是要找出来。 方法一：使用set，根据length判断。 1234567891011class Solution(object): def containsDuplicate(self, nums): """ :type nums: List[int] :rtype: bool """ # 想法一，排序之后判断， # 想法二：使用dictionary, 在建立的过程中就可以判断，没有必要建立完之后遍历，from collections import Counter # 想法三： 使用set，道理和dictionary 基本上是相同的 return len(nums) !=len(set(nums)) 方法二：使用dictionary，不需要建完之后再判断。 ···pythonclass Solution(object): def containsDuplicate(self, nums): “”” :type nums: List[int] :rtype: bool “”” dic ={} for num in nums: if num in dic: return True else: dic[num] =1 return False 12345678910111213141516171819202122232425** Move Zeroes **&gt; Given an array nums, write a function to move all 0&apos;s to the end of it while maintaining the relative order of the non-zero elements.&gt; Input: [0,1,0,3,12]Output: [1,3,12,0,0]Tips: 双指针问题，pre 指向的是0 ，index是遍历的发现如果不是0，那么进行操作```pythonclass Solution(object): def moveZeroes(self, nums): &quot;&quot;&quot; :type nums: List[int] :rtype: None Do not return anything, modify nums in-place instead. &quot;&quot;&quot; # in-place 操作 pre = 0 for index in range(0, len(nums)): if nums[index]: if not nums[pre]: nums[index], nums[pre] =nums[pre], nums[index] pre +=1 Shuffle an Array Shuffle a set of numbers without duplicates. Tips: 使用库函数randomint， 有 shuffle 和reset 两种操作，前者使用randomint 可以得到一个number，后者使用 list 备份。 12345678910111213141516171819202122232425class Solution(object): def __init__(self, nums): self.original =nums[:] self.nums =nums def reset(self): self.nums =self.original[:] # 这个应该 id() 是不同的 return self.nums def shuffle(self): tmp =self.nums[:] for i in range(len(self.nums)): rand =random.randint(0, len(tmp)-1) self.nums[i] =tmp[rand] del tmp[rand] return self.nums # Your Solution object will be instantiated and called as such:# obj = Solution(nums)# param_1 = obj.reset()# param_2 = obj.shuffle() Intersection of Two Arrays II Given two arrays, write a function to compute their intersection. https://leetcode.com/problems/intersection-of-two-arrays-ii/ Tips: 多看看题意，如果想要映射成dictionary，那么result 就是 min(dict1[i], dict1[j]), 两个dictionary 中values 的最小值。 123456789101112131415161718class Solution(object): def intersect(self, nums1, nums2): """ :type nums1: List[int] :type nums2: List[int] :rtype: List[int] """ # 存储dict 然后如果都存在，那么选择values 较小者 为好 # 说一下几种不同的思路 dic1 =collections.Counter(nums1) res =[] for num in nums2: if dic1[num] &gt;0: res += [num] dic1[num] -=1 return res Increasing Triplet Subsequence Given an unsorted array return whether an increasing subsequence of length 3 exists or not in the array. Input: [1,2,3,4,5]Output: true Tips: 首先学会在python 中表示最大数字(float(‘inf’)), 然后这个技巧相当于选择排序中一次遍历选择最小的那个。代码比较简洁哈 12345678910111213141516class Solution(object): def increasingTriplet(self, nums): """ :type nums: List[int] :rtype: bool """ first = second = float('inf') for n in nums: if n &lt;= first: first = n elif n &lt;= second: second = n else: return True return False Product of Array Except Self Given an array nums of n integers where n &gt; 1, return an array output such that output[i] is equal to the product of all the elements of nums except nums[i]. Tips:左右两遍，这个是一维的还是比较nice，换成 m*n 也是基本的思路吧。 12345678910111213141516171819202122class Solution(object): def productExceptSelf(self, nums): """ :type nums: List[int] :rtype: List[int] """ # 我记得使用一个数组存储起来中间的结果，然后进行操作的 len_n =len(nums) res =[1] *len_n # from left to right for i in range(1, len_n): res[i] =res[i-1] * nums[i-1] tmp =1 # from right to left for i in range(len_n-2, -1, -1): tmp *= nums[i+1] res[i] *= tmp return res Min Stack Design a stack that supports push, pop, top, and retrieving the minimum element in constant time. push(x) – Push element x onto stack. pop() – Removes the element on top of the stack. top() – Get the top element. getMin() – Retrieve the minimum element in the stack. Tips: 使用两个list，在push 和pop 的时候要维护栈，始终保持这stack[-1] 是最小值。getMin() 就直接调用结果就行。 https://leetcode.com/problems/min-stack/ 123456789101112131415161718192021222324252627class MinStack(object): def __init__(self): self.stack =[] self.min =[] def push(self, x): if not self.min: self.min.append(x) else: if x &lt;= self.min[-1]: self.min.append(x) self.stack.append(x) def pop(self): tmp =self.stack.pop() # 这个是合理的，因为只有pop 掉了最小值，然后 min list 才需要改变，之前min list 也只是存储的最小值序列 if tmp ==self.min[-1]: self.min.pop() def top(self): return self.stack[-1] def getMin(self): return self.min[-1] Kth Largest Element in an Array Find the kth largest element in an unsorted array. Note that it is the kth largest element in the sorted order, not the kth distinct element. Tips: 这个很巧妙，是快排的思想，每一次的pivot 是不是第 K大。 12345678910111213141516171819202122232425262728293031323334353637class Solution(object): def findKthLargest(self, nums, k): """ :type nums: List[int] :type k: int :rtype: int """ left, right =0, len(nums)-1 while True: index = self.partition(nums, left, right) if index ==k-1: return nums[index] elif index &lt; k-1: left =index+1 else: right =index -1 def partition(self, nums, left, right): pivot =nums[left] p1, p2 =left+1, right #找出 pivot 这个number 的位置 while p1 &lt;=p2: if nums[p1] &lt; pivot and nums[p2]&gt; pivot: nums[p1], nums[p2] =nums[p2], nums[p1] p1 +=1 p2 -=1 elif nums[p1] &gt;= pivot: p1 +=1 elif nums[p2] &lt;=pivot: p2 -=1 nums[left], nums[p2] =nums[p2] , nums[left] return p2 Min Stack Design a stack that supports push, pop, top, and retrieving the minimum element in constant time. push(x) – Push element x onto stack. pop() – Removes the element on top of the stack. top() – Get the top element. getMin() – Retrieve the minimum element in the stack. Tips: 需要维持两个stack， 一个是日常的，一个是min_stakc, 在push or pop 的过程中需要日常性维护 min_stack . 123456789101112131415161718192021222324252627class MinStack(object): def __init__(self): self.stack =[] self.min =[] def push(self, x): if not self.min: self.min.append(x) else: if x &lt;= self.min[-1]: self.min.append(x) self.stack.append(x) def pop(self): tmp =self.stack.pop() # 这个是合理的，因为只有pop 掉了最小值，然后 min list 才需要改变，之前min list 也只是存储的最小值序列 if tmp ==self.min[-1]: self.min.pop() def top(self): return self.stack[-1] def getMin(self): return self.min[-1] Kth Largest Element in an Array Find the kth largest element in an unsorted array. Note that it is the kth largest element in the sorted order, not the kth distinct element. Tips： 快排中的 pivot 如果恰好是 这个 kth，那么就成了。因为快排以 pivot 为分界点，左边是大于，右边是小于（假设是递减排序的话）；使用二分的方法，去寻找这个 pivot。属于一道比较好的题目。 ···pythonclass Solution(object): # 实际上是在试探 快排中pivot 这个点是否是第k 大值 def findKthLargest(self, nums, k): &quot;&quot;&quot; :type nums: List[int] :type k: int :rtype: int &quot;&quot;&quot; left, right =0, len(nums)-1 while True: index = self.partition(nums, left, right) if index ==k-1: return nums[index] elif index &lt; k-1: left =index+1 else: right =index -1 def partition(self, nums, left, right): pivot =nums[left] p1, p2 =left+1, right #找出 pivot 这个number 的位置 while p1 &lt;=p2: if nums[p1] &lt; pivot and nums[p2]&gt; pivot: nums[p1], nums[p2] =nums[p2], nums[p1] p1 +=1 p2 -=1 elif nums[p1] &gt;= pivot: p1 +=1 elif nums[p2] &lt;=pivot: p2 -=1 nums[left], nums[p2] =nums[p2] , nums[left] return p2 12345678910111213141516171819202122232425262728** Top K Frequent Elements**&gt; Given a non-empty array of integers, return the k most frequent elements. Input: nums = [1,1,1,2,2,3], k = 2 Output: [1,2]Tips: 如果是使用 dictionary 进行计数，那么直接调用 counter 是一个不错的选择； 下面是超级nice的代码。```pythonclass Solution(object): # 一种很简单的方法，就是放到dictionary 中，然后根据values从大到小排序，然后返回相应的keys # python 中的 sort() 函数默认是从小到大进行排序的 def topKFrequent(self, nums, k): &quot;&quot;&quot; :type nums: List[int] :type k: int :rtype: List[int] &quot;&quot;&quot; from collections import Counter # 本质上就是一个dictionary freq =Counter(nums) # 这个dictionary， 然后访问的时候freq 操作的是键，然后freq[x] 是值 #counters =sorted(counters, key =counters[1], reverse =True) uniques=sorted(freq,key=lambda x:freq[x],reverse=True) # 最后返回是一个list，只是按照value 进行排序，返回的是key的列表 return uniques[:k] 4Sum II Given four lists A, B, C, D of integer values, compute how many tuples (i, j, k, l) there are such that A[i] + B[j] + C[k] + D[l] is zero.To make problem a bit easier, all A, B, C, D have same length of N where 0 ≤ N ≤ 500. All integers are in the range of $-2^{28} $ to$ 2^{28} - 1 $ and the result is guaranteed to be at most $2^{31} - 1$. Tips: 思路和 2 sum 是一样，放到dictionary 中去。defaultdict 和dict 的唯一差别在于前者不用记性 key in dict 的判断。 12345678910111213141516171819202122232425class Solution(object): def fourSumCount(self, A, B, C, D): from collections import defaultdict length, dic, res =len(A), defaultdict(int), 0 for a in A: for b in B: dic[a+b] +=1 """ if a+b not in dic: dic[a+b] =1 else: dic[a+b] +=1 """ for c in C: for d in D: if -(c+d) in dic: res += dic[-(c+d)] return res Sliding Window Maximum Given an array nums, there is a sliding window of size k which is moving from the very left of the array to the very right. You can only see the k numbers in the window. Each time the sliding window moves right by one position. Return the max sliding window. Tips: 滑动窗口，然后窗口中的max(). 谁能想得到 python 是擅长处理 list，然后max() 函数就解决了呢 1234567891011121314class Solution(object): def maxSlidingWindow(self, nums, k): """ :type nums: List[int] :type k: int :rtype: List[int] """ if nums ==[]: return () res =[] for i in range(len(nums)-k +1): res.append(max(nums[i:i+k])) return res The Skyline Problem A city’s skyline is the outer contour of the silhouette formed by all the buildings in that city when viewed from a distance. Now suppose you are given the locations and height of all the buildings as shown on a cityscape photo (Figure A), write a program to output the skyline formed by these buildings collectively (Figure B). Tips: 关键点就是记录： 轮廓上升和轮廓下降的点，分别对应着 left 的上升和 right 的下降。评论区讲解，虽然比较难看懂 https://leetcode.com/problems/the-skyline-problem/ 12345678910111213141516171819202122232425262728293031323334from heapq import heappush, heappopclass Solution(object): def getSkyline(self, buildings): """ :type buildings: List[List[int]] :rtype: List[List[int]] """ events =[ (left, -height, right) for left, right, height in buildings] events += list((right, 0, 0) for _, right, _ in buildings) events.sort() # 先是按照left 升序排序，然后是 right 降序排序( 这个就是为什么时候 -right) res =[[0, 0]] # 最小堆，保存当前最高的轮廓 (-Height, right)， 使用-H 转换成最大堆，R 的作用是记录轮廓的有效长度 heap =[(0, float('inf'))] for left, height, right in events: # 如果轮廓上升 if height: heappush(heap, (height, right)) while heap[0][1] &lt;=left: heappop(heap) if res[-1][1] != -heap[0][0]: res += [[left, -heap[0][0]]] return res[1:] Wiggle Sort II Given an unsorted array nums, reorder it such that nums[0] &lt; nums[1] &gt; nums[2] &lt; nums[3]…. Tips: 算法题目被 python 中的lsit 操作给毁了, 可以学习以下 list[::-1], list[::2], 这种是模式化的操作，不是偶然。 12345678910111213141516class Solution(object): def wiggleSort(self, nums): """ :type nums: List[int] :rtype: None Do not return anything, modify nums in-place instead. """ #nums.sort() #half =len(nums)/2 #nums[::2], nums[1::2] =nums[:half][::-1], nums[half:][::-1] nums.sort() half = len(nums[::2]) # 注意这个是 half 必须是这样写的 # 这里面倒过来的原因， 前半个永远不大于后半个，所以这样能保证 波动 nums[::2], nums[1::2] = nums[:half][::-1], nums[half:][::-1] Find Peak Element A peak element is an element that is greater than its neighbors.Given an input array nums, where nums[i] ≠ nums[i+1], find a peak element and return its index. Input: nums = [1,2,3,1]Output: 2Explanation: 3 is a peak element and your function should return the index number 2. Tips: 二分查找， 如果是两个 if 那么就是两个步骤，如果 if else 那么就是一种选择。给定的条件中相邻的元素是不相同的。找到一个解进行了。。 https://leetcode.com/problems/find-peak-element/ 123456789101112131415161718192021class Solution(object): def findPeakElement(self, nums): """ :type nums: List[int] :rtype: int """ if not nums: return -1 if len(nums) ==1: return 0 left, right =0, len(nums)-1 while left &lt; right: mid =(left +right) /2 if nums[mid] &gt; nums[mid+1]: right =mid elif nums[mid] &lt; nums[mid+1]: left =mid +1 return left Find the Duplicate Number Given an array nums containing n + 1 integers where each integer is between 1 and n (inclusive), prove that at least one duplicate number must exist. Assume that there is only one duplicate number, find the duplicate one. Tips: 有两种思路。一种是二分法，一种是两个 pointer 的方法。后者类似linked list 中的操作。好好看看代码， fast =nums[nums[fast]] 这个操作就是 fast =fast.next.next 有木有很神奇的样子。 ···pythonclass Solution(object): # 感觉这个从时间和空间复杂度上限制的好多呀，如果满足这两个维度的，一般是先进行排序，O（nlgn） 时间，然后遍历找出重复的数字 # 基本上有两种思路，一种是 index(faster, slower point)， 一种是二分法 # 根据 indics 是有序的，然后使用二分查找 # The array is not sorted - but the indices of the array are sorted - #Insight &apos;&apos;&apos; def findDuplicate(self, nums): &quot;&quot;&quot; :type nums: List[int] :rtype: int &quot;&quot;&quot; if len(nums) == 0: return 0 low = 0 high = len(nums)-1 # 需要访问两个指针 while low &lt; high: mid = low + int((high-low)&gt;&gt;1) count = 0 for x in nums: if x &lt;= mid: count = count + 1 if count &gt; mid: high = mid else: low = mid+1 return low &apos;&apos;&apos; # 这两种方法的根本依据是 长度为n 包含n+1 个整数，并且只有一个 duplicate def findDuplicate(self, nums): slow = fast = finder = 0 while True: slow = nums[slow] fast = nums[nums[fast]] if slow == fast: while finder != slow: finder = nums[finder] slow = nums[slow] return finder 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960** Count of Smaller Numbers After Self**&gt; You are given an integer array nums and you have to return a new counts array. The counts array has the property where counts[i] is the number of smaller elements to the right of nums[i].&gt; Input: [5,2,6,1]Output: [2,1,1,0] Explanation:To the right of 5 there are 2 smaller elements (2 and 1).To the right of 2 there is only 1 smaller element (1).To the right of 6 there is 1 smaller element (1).To the right of 1 there is 0 smaller element.Tips: 这个本身的应用还是挺有意思的。python 中的库函数bisort (binary sort) 了解一下。逆序遍历，找到合适的位置，插进去，然后index 计数。```pythonclass Solution(object): # The problem is equal to find each number&apos;s inversion count. Actually there are three kinds of solutions: BST, mergeSort, and BITree. While the first two answer&apos;s time complexity is O(nlogn), and BITree time comlexity is O( nlog(maximumNum) ). # 太难了 # 两种解法 &apos;&apos;&apos; def merge(self,left,right,res): i,j=0,0 new_array=[] while i&lt;len(left) and j&lt;len(right): if left[i][1]&gt;right[j][1]: new_array+=[left[i]] res[left[i][0]]+=len(right)-j i+=1 else: new_array+=[right[j]] j+=1 new_array+=left[i:] new_array+=right[j:] return new_array def merge_sort(self,nums,res): if len(nums)&lt;2: return nums mid=len(nums)//2 left=self.merge_sort(nums[:mid],res) right=self.merge_sort(nums[mid:],res) return self.merge(left,right,res) def countSmaller(self, nums): res=[0]*len(nums) self.merge_sort([(i,num) for i,num in enumerate(nums)],res) return res &apos;&apos;&apos; def countSmaller(self, nums): count,sorted=[],[] for num in nums[::-1]: index=bisect.bisect_left(sorted,num) sorted.insert(index,num) count+=[index] return count[::-1] Longest Consecutive Sequence Given an unsorted array of integers, find the length of the longest consecutive elements sequence.Your algorithm should run in O(n) complexity. Tips: 这种解题的方式，是比较新颖的。 12345678910111213141516171819class Solution(object): # 第一印象是 先进行排序， 然后选择的过程， 但是限制条件是 O(n) 的复杂度 def longestConsecutive(self, nums): """ :type nums: List[int] :rtype: int """ nums =set(nums) best =0 # 这种方式真的很简洁 for x in nums: if x-1 not in nums: y =x+1 while y in nums: y +=1 best =max(best, y-x) return best House Robber Tips: dp 的思想运用到极致就是这个样子。使用两个变量句可以搞定。 12345678910111213141516class Solution(object): """ f(0) = nums[0] f(1) = max(num[0], num[1]) f(k) = max( f(k-2) + nums[k], f(k-1) ) """ def rob(self, nums): """ :type nums: List[int] :rtype: int """ # dp 有时候就能这样优化到使用两个变量 last, now =0, 0 for num in nums: last, now =now, max(last +num, now) return now Longest Increasing Subsequence Given an unsorted array of integers, find the length of longest increasing subsequence. Tips: f(i) 表示以 nums[i]为结尾的 longest encreasing subsequence( 第二个for 对比的对象是 f[:i] 使用j 进行遍历 ) 12345678910111213141516171819202122class Solution(object): # 求解最值 唯一解都是可以使用这样的方式的哦 # 不能使用 in 那种骚操作了， 只能踏踏实实的 dp # 这个版本的dp 没有优化好 def lengthOfLIS(self, nums): """ :type nums: List[int] :rtype: int """ if not nums: return 0 dp = [1] * len(nums) """ """ for i in range(len(nums)): for j in range(i): if nums[i] &gt; nums[j]: dp[i] = max(dp[i], dp[j] + 1) return max(dp) Coin Change You are given coins of different denominations and a total amount of money amount. Write a function to compute the fewest number of coins that you need to make up that amount. If that amount of money cannot be made up by any combination of the coins, return -1. Tips: dp问题 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647from sys import maxintclass Solution(object): """ :type coins: List[int] :type amount: int :rtype: int """ ''' # 这种方法不行 if len(coins) ==1: if amount % coins[0] ==0: return amount /coins[0] else: return -1 coins.sort(reverse =True) res =0 for i in range(len(coins)): res += amount /coins[i] amount %= coins[i] return res # table 作为dp， table[i] 表示前i 个数字使用数量最少的硬币能够 表示 # 多看几遍就能理解了 def coinChange(self, coins, amount): table = [0]*(amount + 1) for i in range(1, amount+1): minimum = maxint # 有好几种对于最小值和最大的初始化了 for j in coins: if i &gt;= j and table[i-j] != -1: minimum = min(minimum, table[i-j] + 1) table[i] = -1 if minimum == maxint else minimum return table[amount] ''' # python3 不能使用python2,python2 能使用 python3? def coinChange(self, coins, amount): table = [0 ] *(amount + 1) for i in range(1, amount +1): #minimum =maxint minimum =float('inf') for j in coins: if i &gt;= j and table[ i -j] != -1: minimum = min(minimum, table[ i -j] + 1) table[i] = -1 if minimum == float('inf') else minimum return table[amount]]]></content>
      <categories>
        <category>算法</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[LeetCode- Array]]></title>
    <url>%2F2019%2F02%2F16%2Fleetcode-array%2F</url>
    <content type="text"><![CDATA[LeetCode 刷题总结（一）， 使用Python 实现。该篇题目类型主要是： array 和string 的相关处理。 Two Sum Given an array of integers, find two numbers such that they add up to a specific target number. Tips： 返回的是 index，所以 dict 中存储的 (num, index) 这样的组合, 是两个不相同的数字的index。题目中确保有唯一解。 第一种解法 暴力法。时间复杂度是$O(n^2)$，空间复杂度是$O(1)$。 1234567891011121314151617class Solution &#123;public: vector&lt;int&gt; twoSum(vector&lt;int&gt;&amp; nums, int target) &#123; // 这个是唯一解 for(int i =1; i&lt; nums.size(); i++) &#123; for(int j =0; j&lt; i; j++) &#123; if(nums[i] +nums[j] == target) &#123; return vector&lt;int&gt;&#123;i, j&#125;; &#125; &#125; &#125; return vector&lt;int&gt;&#123;-1, -1&#125;; &#125;&#125;; 解法二：使用字典，其中的(key, val) -&gt; (数值，index)。使用字典的目的是为了记录值的index。 时间复杂度是$O(n)$空间复杂度是$O(n)$。 12345678910111213class Solution &#123;public: vector&lt;int&gt; twoSum(vector&lt;int&gt;&amp; nums, int target) &#123; unordered_map&lt;int, int&gt; cache; for(int i =0; i&lt; nums.size(); i++) &#123; if(cache.count(target -nums[i]) &gt;0) return vector&lt;int&gt;&#123;cache[target- nums[i]], i&#125;; cache[nums[i]] =i; &#125; return vector&lt;int&gt;&#123;-1, -1&#125;; &#125;&#125;; python 实现 12345678910111213class Solution(object): def twoSum(self, nums, target): """ :type nums: List[int] :type target: int :rtype: List[int] """ dic =&#123;&#125; for index,val in enumerate(nums): if target -val in dic: return (dic[target-val], index) dic[val] =index return (-1, -1) Two Sum II - Input array is sorted python解法，虽然是有序的，但是并不能达到 $O(logn)$的时间复杂度，只是是$O(n)$的时间复杂度。 1234567891011121314151617class Solution(object): def twoSum(self, numbers, target): """ :type numbers: List[int] :type target: int :rtype: List[int] """ l, r =0, len(numbers)-1 while l &lt;r : total = numbers[l] +numbers[r] if total ==target: return (l+1, r+1) elif total &lt; target: l +=1 else: r -=1 return (-1, -1) Two Sum IV - Input is a BST 将两数之和转移到了二叉搜索树，是否存在两个结点，使得其值相加为k。有两种解法，一种是遍历树结构，对于每一个结点，然后都需要遍历一次树，所以总的时间复杂度是$O(n^2)$，其中$n$ 是树的结点的个数。第二种解法，保存遍历树的结果，然后使用第二题中的思路，所以是$O(n)$的时间复杂度，其中$n$是结点的个数。 二叉搜索树按照中序遍历得到的是一个增序序列。 12345678910111213141516171819202122232425262728293031323334# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution(object): def traverse(self, root, res): if not root: return self.traverse(root.left, res) res.append(root.val) self.traverse(root.right, res) def findTarget(self, root, k): """ :type root: TreeNode :type k: int :rtype: bool """ path =[] self.traverse(root, path) # 这个是从小到大的顺序 l, r =0, len(path)-1 print(path) while(l &lt;r): total =path[l] +path[r] if total==k: return True elif total &lt; k: l +=1 else: r -=1 return False 在一个有序的整数数组中，给定一个目标数字target，要求返回比该target 小的第一个数字（index最大）。 这个是二分查找非常简单的变形，处理一下边界条件就行。 12345678910111213int find_target(vector&lt;int&gt; arr, int target)&#123; if (arr.size() ==0) return -1; int l =0, r =arr.size() -1; while(l &lt;r) &#123; int mid =l +r +1&gt;&gt;1; if(arr[mid] &lt;target) l =mid; else r =mid -1; &#125; return l;&#125; 请设计一个高效算法，找出数组中两数之和为指定值的所有整数对 双指针算法。首先排序，相同或者相近的数字在一块，然后使用双指针的思想求解。时间复杂度$O(nlogn)$，空间复杂度$O(1)$。寻找相同点的过程，和上面 TwoSum基本上是一样的。当发现相同的时候，可能存在两种情况，一种是全部相同，一种是有两个相同的部分组成。 样例，输入是：1[1,2,3,4,5],5,6 返回的值是12 12345678910111213141516171819202122232425262728293031class FindPair: def countPairs(self, arr, n, tsum): if n &lt;2: return 0 arr.sort() count =0 i , j =0, n -1 # 双指针问题 while i&lt;j: total =arr[i] +arr[j] if total &gt;tsum: j -=1 elif total &lt; tsum: i +=1 else: if arr[i] ==arr[j]: x = j -i+1 count += x*(x-1)/2 break else: ni, nj =1, 1 while i&lt; j and arr[i] ==arr[i+ni]: ni +=1 i =i +ni-1 while i &lt;j and arr[j] ==arr[j-nj]: nj +=1 j =j -nj+1 count += ni*nj i +=1 j -=1 return count Median of Two Sorted Arrays There are two sorted arrays nums1 and nums2 of size m and n respectively. Find the median of the two sorted arrays. The overall run time complexity should be O(log (m+n)). Tips: 先是 merge，然后选择 median，常规做法，时间0(mn)，不是最优的，还可以达到O(min(m, n) ) 这样的时间复杂度 123456789101112131415161718192021222324252627282930313233343536373839class Solution(object): # 使用 merge() 操作，然后根据，然后取得中位数 def median(self, nums): len_n =len(nums) if len_n &amp;1 ==1: return nums[len_n//2] else: return float(nums[len_n//2]+nums[len_n//2-1])/2 def findMedianSortedArrays(self, nums1, nums2): """ :type nums1: List[int] :type nums2: List[int] :rtype: float """ res =[] if not nums1 or not nums2: res =nums1 or nums2 return self.median(res) else: left, right =0, 0 len_1, len_2 =len(nums1)-1, len(nums2)-1 while left&lt;= len_1 and right &lt;=len_2: if nums1[left] &lt;nums2[right]: res.append(nums1[left]) left +=1 else: res.append(nums2[right]) right +=1 if left &lt;= len_1: res.extend(nums1[left:]) if right &lt;=len_2: res.extend(nums2[right:]) return self.median(res) ZigZag Conversion The string “PAYPALISHIRING” is written in a zigzag pattern on a given number of rows like this: (you may want to display this pattern in a fixed font for better legibility) P A H NA P L S I I GY I R And then read line by line: “PAHNAPLSIIGYIR” Tips: 字符串处理 12345678910111213141516171819202122class Solution(object): def convert(self, s, numRows): """ :type s: str :type numRows: int :rtype: str """ if numRows == 1 or numRows &gt;= len(s): # string of list return s L = [''] * numRows # string of list row, step = 0, 1 for x in s: L[row] += x if row == 0: step = 1 elif row == numRows -1: step = -1 row += step return ''.join(L) # array (list) 转成string 常用的方法 Container With Most Water Given n non-negative integers a1, a2, …, an , where each represents a point at coordinate (i, ai). n vertical lines are drawn such that the two endpoints of line i is at (i, ai) and (i, 0). Find two lines, which together with x-axis forms a container, such that the container contains the most water. The above vertical lines are represented by array [1,8,6,2,5,4,8,3,7]. In this case, the max area of water (blue section) the container can contain is 49. Tips: 左右双指针问题。首先移动高度较小的点，因为两者的距离是肯定变小，如果移动高度大的，那么最后的面积肯定变小；但是如果 移动高度较小的点，那么最后的面积是有可能变大的。所以这个是一个可能性的东西。 1234567891011121314151617181920212223class Solution(object): def maxArea(self, height): len_h = len(height) if len_h == 1: return 0 max_area = 0 left = 0 right = len_h - 1 # left, right =0, len_h -1 while left &lt; right: if height[left] &lt; height[right]: area = (right - left) * height[left] left += 1 else: area = (right - left) * height[right] right -= 1 if area &gt; max_area: max_area = area return max_area Longest Common Prefix Write a function to find the longest common prefix string amongst an array of strings.If there is no common prefix, return an empty string “”. Input: [&quot;flower&quot;,&quot;flow&quot;,&quot;flight&quot;] Output: &quot;fl&quot; Tips: 一个个寻找交集，最朴素的想法、 123456789101112131415161718192021222324252627282930313233class Solution(object): # 时间复杂度是 O(N^2), N=len(strs), 笼统的说 def longestCommonPrefix(self, strs): """ :type strs: List[str] :rtype: str """ if not strs: return "" n =len(strs) if n ==0: return "" elif n ==1: return strs[0] predix =strs[0] for s in strs[1:]: # 这种结构见过了，就是不断迭代，不断地的去寻找 ”交集“ predix =self.findPrefix(predix, s) if "" ==predix: break return predix def findPrefix(self, s1, s2): min_len =min(len(s1), len(s2)) # 这个if 和 return 写的都是很巧妙的 for i in range(0, min_len): if s1[i] != s2[i]: return s1[:i] return s1[:min_len] Remove Duplicates from Sorted Array Given a sorted array nums, remove the duplicates in-place such that each element appear only once and return the new length.Do not allocate extra space for another array, you must do this by modifying the input array in-place with O(1) extra memory. Tips: 前后两个指针覆盖的思想，最后返回的是index，如果发现了后者覆盖前者 12345678910111213141516171819202122class Solution(object): def removeDuplicates(self, nums): """ :type nums: List[int] :rtype: int """ if None == nums: return 0 len_n = len(nums) if len_n &lt;= 1: return len_n m = 0 n = 1 while n &lt; len_n: if nums[m] != nums[n]: m += 1 if m != n: nums[m] = nums[n] n += 1 return m + 1 Remove Element Given an array nums and a value val, remove all instances of that value in-place and return the new length.Do not allocate extra space for another array, you must do this by modifying the input array in-place with O(1) extra memory. Tips: in-place 表示不能创建数组，可以使用（临时）变量。通过双指针进行处理，想想为什么可以使用这么简洁的代码进行处理呢。m n 分别从左到右、从右到左进行遍历，将和 val 相同的元素都放到右边，不相同的放到左边 1234567891011121314151617181920212223242526class Solution(object): def removeElement(self, nums, val): """ :type nums: List[int] :type val: int :rtype: int """ if not nums: return 0 len_n =len(nums) m,n =0, len_n-1 # 注意跳出条件，遍历的方向和跳出条件是相关的, 这个 等号取于不取 一是比较难把握，可以具体带个值 while m &lt;=n: if val ==nums[m]: if val !=nums[n]: # 这个是不能使用 while 找，因为有比较多的case 需要考虑，所以使用 if 进行单步操作 nums[m], nums[n] =nums[n], nums[m] m +=1 n -=1 else: n -=1 else: m +=1 return m # 因为 m 是从0开始的 Search in Rotated Sorted Array Suppose an array sorted in ascending order is rotated at some pivot unknown to you beforehand.(i.e., [0,1,2,4,5,6,7] might become [4,5,6,7,0,1,2]).You are given a target value to search. If found in the array return its index, otherwise return -1. Tips：一定是 二分思想，关键是判断 增序列 和 target 的关系，所以有两层 if 判断条件，一层是增序列， 一层是 target 是否在增序列下面这个观点是要有的： 整个数组由两个有序的子序列构成，且左子序列中的每个元素都&gt;,右子序列中的每个元素。 123456789101112131415161718192021222324class Solution(object): # @param nums, a list of integers # @param target, an integer to be searched # @return an integer def search(self, nums, target): left = 0; right = len(nums) - 1 # 这个 == 是不容易进行取舍的， while left &lt;= right: mid = (left + right) / 2 if target == nums[mid]: return mid # 我当时面试的时候就是这种思路，一定要有条理就是了 # 左边是增序列 if nums[mid] &gt;= nums[left]: if target &lt; nums[mid] and target &gt;= nums[left]: right = mid - 1 else: left = mid + 1 elif nums[mid] &lt; nums[right]: if target &gt; nums[mid] and target &lt;= nums[right]: left = mid + 1 else: right = mid - 1 return -1 Search Insert Position Given a sorted array and a target value, return the index if the target is found. If not, return the index where it would be if it were inserted in order.You may assume no duplicates in the array. Tips： 二分查找，之前是found or not found，现在如果没有找见返回的是 index，没有什么本质区别。 1234567891011121314151617181920212223242526272829class Solution(object): def searchInsert(self, nums, target): """ :type nums: List[int] :type target: int :rtype: int """ if not nums: return None len_n =len(nums) if nums[0] &gt; target: return 0 if nums[-1] &lt; target: return len_n left, right =0, len_n-1 while left&lt;=right: mid =(left +right)/2 if nums[mid] ==target: return mid elif nums[mid] &lt;target: left =mid +1 else: right =mid -1 return left Rotate Image You are given an n x n 2D matrix representing an image.Rotate the image by 90 degrees (clockwise). Given input matrix =[ [1,2,3], [4,5,6], [7,8,9]],rotate the input matrix in-place such that it becomes:[ [7,4,1], [8,5,2], [9,6,3]] Tips: 好好观察四个等式，就是行变列，然后其他的一个坐标是对称的，这个就是旋转 90度；然后有五个变量（有重复的） 1234567891011 class Solution(object): def rotate(self, matrix): n = len(matrix) if 1 == n: return round = int(n / 2) for x in range(0, round): for y in range(x, n - x - 1): matrix[n - y - 1][x], matrix[n - x - 1][n - y - 1], matrix[y][n - x - 1], matrix[x][y] = matrix[n - x - 1][n - y - 1], matrix[y][n - x - 1], matrix[x][y], matrix[n - y - 1][x] Spiral Matrix Given a matrix of m x n elements (m rows, n columns), return all elements of the matrix in spiral order.螺旋形 Input:[ [ 1, 2, 3 ], [ 4, 5, 6 ], [ 7, 8, 9 ]]Output: [1,2,3,6,9,8,7,4,5] Tips: 在处理”行“ 信息的时候，是可以数组切割的。在处理列信息的时候，需要一个个append() 123456789101112131415161718192021222324252627282930313233343536class Solution(object): # 这个不是跟那个 剑指offer 中的顺时针打印输出一样吗， # 这个版本是比较容易理解，所以选择这个版本 def spiralOrder(self, matrix): if matrix ==[]: return [] top, bottom =0, len(matrix)-1 left, right =0, len(matrix[0])-1 res =[] # 当有一个等于的时候就应该跳出来了 while left &lt;right and top&lt; bottom: # 对于行 处理python 是有比较简单的方式的 res += matrix[top][left: right+1] for x in range(top+1, bottom): res.append(matrix[x][right]) res += matrix[bottom][left:right+1][::-1] # 倒叙 for x in range(bottom-1, top, -1): res.append(matrix[x][left]) top, bottom, left, right =top+1, bottom-1, left+1, right-1 if top ==bottom: res += matrix[top][left:right] elif left ==right: for x in range(top, bottom+1): res.append(matrix[x][right]) return res Spiral Matrix II Given a positive integer n, generate a square matrix filled with elements from 1 to $n^2$ in spiral order. Tips : 注意边角的细节。初始化赋值的应该是常见的操作，这里的cur 是比较核心的东西。 123456789101112131415161718192021222324252627282930313233343536class Solution(object): # version 1 是遍历获取， version 2 是填充。这个真是有趣的东西 # 还是设置上下左右四个坐标进行遍历的处理 def generateMatrix(self, n): ans =[ [0] *n for _ in range(n)] top, bottom, left, right =0, n-1, 0, n-1 cur =1 while left &lt;= right and top &lt;= bottom: for i in range(left, right+1): ans[top][i] =cur cur +=1 top +=1 # 根据问题需求，是可以在题目中 设置这种break，不需要等到 while 的判断 if top &gt; bottom: break for i in range(top, bottom+1): ans[i][right] =cur cur +=1 right -=1 if left &gt; right: break # 好好体会这个连接点的处理，左边是能够访问到的，右边为了能够访问到 # 进行了 -1 的操作 for i in range(right, left-1, -1): ans[bottom][i] =cur cur +=1 bottom -=1 if bottom &lt;top: break for i in range(bottom, top-1, -1): ans[i][left] =cur cur +=1 left +=1 return ans Merge Intervals Given a collection of intervals, merge all overlapping intervals. Tips: 需要区分区间的start 和end 点，分别使用 (0 1) 进行区分，然后 sort() ，那么那么start 就出现了最前面，end 就出现了最后面。默认的sort 是先按照 第一个元素排序，然后按照第二个元素排序，所以标识 (0, 1) 这个是没有收到影响的。 12345678910111213141516171819202122232425class Solution(object): def merge(self, intervals): if not intervals: return [] data = [] for interval in intervals: data.append((interval[0], 0)) data.append((interval[1], 1)) data.sort() merged = [] stack = [data[0]] for i in range(1, len(data)): d = data[i] if d[1] == 0: # this is a lower bound, push this onto the stack stack.append(d) elif d[1] == 1: if stack: start = stack.pop() if len(stack) == 0: # we have found our merged interval merged.append( (start[0], d[0])) return merged Length of Last Word Given a string s consists of upper/lower-case alphabets and empty space characters ‘ ‘, return the length of last word in the string.If the last word does not exist, return 0. Tips: 不能使用 split() 因为太多的case需要单独的处理，所以应该使用字母为基本，一个个处理。等不等于 ‘ ‘进行的切分。 1234567891011121314151617class Solution(object): # 这个是需要从 字母角度考虑，而不是从单词角度考虑 def lengthOfLastWord(self, s): len_s =len(s) if 0==len_s: return 0 index =len_s -1 # 找到第一个不是 ' '的字母 while index&gt;=0 and ' ' ==s[index]: index -=1 len_last_word =0 while index &gt;=0 and ' ' != s[index]: index -=1 len_last_word +=1 return len_last_word Valid Number Validate if a given string can be interpreted as a decimal number. Tips :对于小数(decimal ) 各种 case 的熟知程度 123456789101112131415161718192021222324252627class Solution(object): def isNumber(self, s): """ :type s: str :rtype: bool """ s = s.strip() digits = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'] met_dot = met_e = met_digit = False for i, char in enumerate(s): if char in ['+', '-']: if i &gt; 0 and s[i-1] != 'e': return False elif char == '.': if met_dot or met_e: return False met_dot = True elif char == 'e': if met_e or not met_digit: return False met_e, met_digit = True, False #elif char.isdigit(): elif char in digits: met_digit = True else: return False return met_digit Plus One Given a non-empty array of digits representing a non-negative integer, plus one to the integer.The digits are stored such that the most significant digit is at the head of the list, and each element in the array contain a single digit.You may assume the integer does not contain any leading zero, except the number 0 itself. Input: [1,2,3] Output: [1,2,4] Explanation: The array represents the integer 123. Tips: 重点在于加法的处理，一般使用 求余得到digit，然后使用carry 得到进位。 1234567891011121314151617181920class Solution(object): def plusOne(self, digits): """ :type digits: List[int] :rtype: List[int] """ len_s =len(digits) carry =1 for i in range(len_s-1, -1, -1): total =digits[i] +carry digit =int(total %10) carry =int(total //10) digits[i] =digit # 这个是最后一个进位 if carry ==1: digits.insert(0, 1) return digits Simplify Path Given an absolute path for a file (Unix-style), simplify it. Or in other words, convert it to the canonical path. Tips： 12345678910111213141516171819class Solution(object): # 这个从考点上是 stack，但是使用python字符串处理更好 # 按照 '/' 进行split() def simplifyPath(self, path): """ :type path: str :rtype: str """ stack = [] for token in path.split('/'): if token in ('', '.'): pass # continue 这两个是一样的效果， pass 就类似一种占位符，在测试的时候常见 elif token == '..': if stack: stack.pop() else: stack.append(token) return '/' + '/'.join(stack) Edit Distance Given two words word1 and word2, find the minimum number of operations required to convert word1 to word2. Tips: 动态规划就通过存储子问题结果来加快运算，但一个好的动态规划算法会尽量减少空间复杂度。 然后是可以继续优化的，使用 O(n) 的空间的复杂度. 真正的写出来之后，发现代码是比想法更加简单的。 提供了两种解法，第一种比较常规 dp，比较容易理解。123456789101112131415161718192021222324class Solution(object): def minDistance(self, word1, word2): """ :type word1: str :type word2: str :rtype: int """ m = len(word1) n = len(word2) table = [[0] * (n + 1) for _ in range(m + 1)] for i in range(m + 1): table[i][0] = i for j in range(n + 1): table[0][j] = j for i in range(1, m + 1): for j in range(1, n + 1): if word1[i - 1] == word2[j - 1]: table[i][j] = table[i - 1][j - 1] else: table[i][j] = 1 + min(table[i - 1][j], table[i][j - 1], table[i - 1][j - 1]) return table[-1][-1] 第二种就是参考一下吧。 1234567891011121314151617class Solution(object): # 从实现的角度讲，这个是需要把握住有一个 word的index 是不变的 def minDistance(self, word1, word2): l1, l2 = len(word1)+1, len(word2)+1 pre = [0 for _ in range(l2)] for j in range(l2): pre[j] = j for i in range(1, l1): cur = [i]*l2 for j in range(1, l2): cur[j] = min(cur[j-1]+1, pre[j]+1, pre[j-1]+(word1[i-1]!=word2[j-1])) #pre = cur[:] pre =cur return pre[-1] Set Matrix Zeroes Given a m x n matrix, if an element is 0, set its entire row and column to 0. Do it in-place. Tips: 使用第一行和第一列作为标记，使用 0作为标记。 123456789101112131415161718192021class Solution(object): def setZeroes(self, matrix): """ :type matrix: List[List[int]] :rtype: None Do not return anything, modify matrix in-place instead. """ firstRowHasZero = not all(matrix[0]) # all() 只有所有的不为0 返回的才不为0，否则返回0 for i in range(1,len(matrix)): for j in range(len(matrix[0])): if matrix[i][j] == 0: # 这种遍历并标记的方法还是比较优秀的 matrix[0][j] = 0 matrix[i][0] = 0 for i in range(1,len(matrix)): for j in range(len(matrix[0])-1,-1,-1): # 注意是从后往前标记的 if matrix[0][j] == 0 or matrix[i][0] == 0: matrix[i][j] = 0 if firstRowHasZero: matrix[0] = [0]*len(matrix[0]) #最后处理第一行 Remove Duplicates from Sorted List Given a sorted linked list, delete all duplicates such that each element appear only once. Tips: 注意这道题和上面有道题是有差别的，这个是 delete all duplicates 1234567891011121314151617181920212223# Definition for singly-linked list.# class ListNode(object):# def __init__(self, x):# self.val = x# self.next = Noneclass Solution(object): """ 使用两个 while 是因为，逻辑上简单 """ def deleteDuplicates(self, head): """ :type head: ListNode :rtype: ListNode """ cur =head while cur: while cur.next and cur.val == cur.next.val: cur.next =cur.next.next # skip duplicates cur =cur.next return head Merge Sorted Array Given two sorted integer arrays nums1 and nums2, merge nums2 into nums1 as one sorted array. The number of elements initialized in nums1 and nums2 are m and n respectively. You may assume that nums1 has enough space (size that is greater or equal to m + n) to hold additional elements from nums2. Tips: 题目中说了 nums1 是不会出现 index 访问报错的。从后往前遍历，因为这个是要求 merge 2 into 1的。 1234567891011121314151617181920212223242526class Solution(object): def merge(self, nums1, m, nums2, n): """ :type nums1: List[int] :type m: int :type nums2: List[int] :type n: int :rtype: None Do not return anything, modify nums1 in-place instead. """ i, j, k =m-1, n-1, m+n-1 while i &gt;=0 and j&gt;=0: if nums1[i] &gt; nums2[j]: nums1[k] =nums1[i] i -=1 else: nums1[k] =nums2[j] j -=1 k -=1 #import ipdb #ipdb.set_trace() # 如果这是 if 那么使用的就是字符串的切割，如果是while 那么就是一个个操作 if j&gt;=0: nums1[:k+1] =nums2[:j+1] Restore IP Addresses Given a string containing only digits, restore it by returning all possible valid IP address combinations. Tips： 细节比较多，在进行 dfs 的时候 123456789101112131415161718192021222324252627282930class Solution(object): def restoreIpAddresses(self, s): """ :type s: str :rtype: List[str] """ ans = [] self.helper(ans, s, 4, []) # ans 中的item 之间使用 . 进行隔开，这种技术，是非常常见的 # 这个是 list of list， 然后转换成了 list of string return ['.'.join(x) for x in ans] def helper(self, ans, s, k, temp): if len(s) &gt; k * 3: return if k == 0: #ans.append(temp[:]) ans.append(temp) else: for i in range(min(3, len(s) - k + 1)): # s 是一个字符串，当只有一位时，0可以成某一段，如果有两位或三位时，像 00， 01， 001， 011， 000等都是不合法的， # 只能是 0.1.0.0 而不能是00.1.0.0 ，这个是ip语法 # 这个是有连个并列的判断条件 if i == 2 and int(s[:3]) &gt; 255 or i &gt;0 and s[0] =='0': continue self.helper(ans, s[i + 1:], k - 1, temp + [s[:i + 1]]) Interleaving String Given s1, s2, s3, find whether s3 is formed by the interleaving of s1 and s2. Tips： interleaving 插入；s3 是否能够用s1 和s2 组成, len(s1) +len(s2) == len(s3) 这个样子的。行列分别表示s1 和s2 中的字母，然后 (x, y) 值表示当前的能够”走“ 通的路径。 12345678910111213141516171819202122232425262728293031class Solution(object): """ interleaving, 判断s3是否由s1和s2交叉构成， """ def isInterleave(self, s1, s2, s3): """ :type s1: str :type s2: str :type s3: str :rtype: bool """ r, c, l =len(s1), len(s2), len(s3) if r+c !=l: return False # 0 行和 0列的初始化，使用 true or false 来进行表示结果 dp =[ [True] * (c+1) for _ in range(r+1)] for i in range(1, r+1): dp[i][0] =dp[i-1][0] and s1[i-1] == s3[i-1] for j in range(1, c+1): dp[0][j] =dp[0][j-1] and s2[j-1] ==s3[j-1] # 看到代码之后觉得很简单， for i in range(1, r+1): for j in range(1, c+1): dp[i][j] = dp[i-1][j] and s1[i-1] ==s3[i+j-1] or dp[i][j-1] and s2[j-1] ==s3[i+j-1] return dp[-1][-1] 方法二：12345678910111213141516# 从运行的结果来说，内存下降了0.1M， 但是这个时间却商城了def isInterleave(self, s1, s2, s3): r, c, l= len(s1), len(s2), len(s3) if r+c != l: return False dp = [True for _ in range(c+1)] for j in range(1, c+1): dp[j] = dp[j-1] and s2[j-1] == s3[j-1] for i in range(1, r+1): dp[0] = (dp[0] and s1[i-1] == s3[i-1]) for j in range(1, c+1): dp[j] = (dp[j] and s1[i-1] == s3[i-1+j]) or (dp[j-1] and s2[j-1] == s3[i-1+j]) return dp[-1] Pascal’s Triangle Given a non-negative integer numRows, generate the first numRows of Pascal’s triangle. Tips: 这个是小学数学题，变成了编程题、对应好index 进行了。最后 res 可能不是正三角形（直角三角形）但一定是可以这样做的。 123456789101112131415class Solution(object): def generate(self, numRows): """ :type numRows: int :rtype: List[List[int]] """ res = [[1 for _ in range(i+1)] for i in range(numRows)] for i in range(2, numRows): for j in range(1, i): # 这个就是一个数学问题 # 就是上一行(i-1) 的 j-1 和j 元素的相加 res[i][j] =res[i-1][j-1] + res[i-1][j] return res Given a non-negative index k where k ≤ 33, return the kth index row of the Pascal’s triangle.Note that the row index starts from 0. Tips： 相比于上一个，这个只是返回了最后一行。 123456789101112131415class Solution(object): # 相对比上一个，只是输出最后一行的信息， rowIndex def getRow(self, rowIndex): """ :type rowIndex: int :rtype: List[int] """ res =[ [1 for _ in range(i+1) ] for i in range(rowIndex+1)] for i in range(2, rowIndex+1): for j in range(1, i): res[i][j ] = res[i-1][j] + res[i-1][j-1] return res[-1] Valid Palindrome Given a string, determine if it is a palindrome, considering only alphanumeric characters and ignoring cases.Note: For the purpose of this problem, we define empty string as valid palindrome. Input: &quot;A man, a plan, a canal: Panama&quot; Output: true Tips： 建议使用 is.alnum() 这个python 中自带的函数，因为这种判断还是挺常见的。回文数。先是预处理，然后才是 lower() 判断。 1234567891011121314151617181920212223242526class Solution(object): # palindrome 回文数， alphanumeric ， 字母与数字并用的; # 预处理之后，然后比较前后两个字符的异同 # Python isalnum() 方法检测字符串是否由字母和数字组成，这种函数只有在 歪果仁的代码中常见 # s[i] &gt;= 'a' and s[i] &lt;= 'z' or s[i] &gt;= '0' and s[i] &lt;= '9' or s[i] &gt;= 'A' and s[i] &lt;= 'Z':, 这个是国人的写法 # a=''.join([x for x in s if x.isalpha() or x.isdigit()]).lower() 或者这样 # 喜欢写源码 def isPalindrome(self, s): """ :type s: str :rtype: bool """ left, right =0, len(s)-1 while left &lt; right: while left &lt; right and not s[left].isalnum(): left +=1 while left &lt; right and not s[right].isalnum(): right -=1 if s[left].lower() != s[right].lower(): return False left +=1 right -=1 return True Gas Station There are N gas stations along a circular route, where the amount of gas at station i is gas[i].You have a car with an unlimited gas tank and it costs cost[i] of gas to travel from station i to its next station (i+1). You begin the journey with an empty tank at one of the gas stations.Return the starting gas station’s index if you can travel around the circuit once in the clockwise direction, otherwise return -1. Input:gas = [1,2,3,4,5]cost = [3,4,5,1,2]Output: 3Explanation:Start at station 3 (index 3) and fill up with 4 unit of gas. Your tank = 0 + 4 = 4Travel to station 4. Your tank = 4 - 1 + 5 = 8Travel to station 0. Your tank = 8 - 2 + 1 = 7Travel to station 1. Your tank = 7 - 3 + 2 = 6Travel to station 2. Your tank = 6 - 4 + 3 = 5Travel to station 3. The cost is 5. Your gas is just enough to travel back to station 3.Therefore, return 3 as the starting index. Tips: 看着挺吓人的，但是落实到代码上，就是一个循环，当不能出发时， r (rest) 是为0，然后寻求下一个可以出发的点。 123456789101112class Solution(object): def canCompleteCircuit(self, gas, cost): if sum(gas) &lt; sum(cost): return -1 index, rest = 0, 0 for i in range(len(gas)): if gas[i] + rest &lt; cost[i]: #这个是需要遍历整个 gas的，因为有可能开始行但是后来不行，所以开始的index 还是无法完成整个遍历 index = i +1 rest = 0 else: rest += gas[i] - cost[i] return index Evaluate Reverse Polish Notation Evaluate the value of an arithmetic expression in Reverse Polish Notation.Valid operators are +, -, *, /. Each operand may be an integer or another expression. Division between two integers should truncate toward zero. The given RPN expression is always valid. That means the expression would always evaluate to a result and there won&apos;t be any divide by zero operation. Tips： 术语，逆波兰表达式（操作数在前，操作符在后）的一种形式。栈是存储操作数 和运算结果的。对于负数不能整除的，向着 0 靠拢 1234567891011121314151617181920212223242526272829class Solution(object): # 计算逆波兰表达式：把操作数放前面，把操作符后置的一种写法 # 这个明显就是 栈的使用呀，两个栈， def evalRPN(self, tokens): """ :type tokens: List[str] :rtype: int """ stack =[] for t in tokens: if t not in "+-*/": stack.append(int(t)) else: right, left =stack.pop(), stack.pop() if t =="+": stack.append(left +right) elif t =="-": stack.append(left-right) elif t =="*": stack.append(left*right) else: # case like 1/(-2) 负数且不能整除 if left*right &lt;0 and left % right!=0: stack.append(left/right +1) else: stack.append(left/right) return stack.pop() Reverse Words in a String Given an input string, reverse the string word by word. Tips： 正常的思路是第一次全翻转，第二次按照 word 进行翻转。但是python 十分擅长 字符串的处理。 123456789class Solution(object): # 两次翻转。第一次是全翻转，然后第二次是word 翻转 # 字符串类型的算法题目，使用python 是无法get 到算法层面的 hahaha def reverseWords(self, s): """ :type s: str :rtype: str """ return " ".join(s.split()[::-1]) Search a 2D Matrix Write an efficient algorithm that searches for a value in an m x n matrix. This matrix has the following properties: Integers in each row are sorted from left to right. The first integer of each row is greater than the last integer of the previous row. Tips: 注意第二个条件，下一行的开头是大于上一行的末尾，所以如果 dense 一下，是可以看成大的有序，所以思路就是二叉排序。 https://leetcode.com/problems/search-a-2d-matrix/ Input:matrix = [ [1, 3, 5, 7], [10, 11, 16, 20], [23, 30, 34, 50]]target = 3Output: true 123456789101112131415161718192021222324252627class Solution(object): def searchMatrix(self, matrix, target): """ :type matrix: List[List[int]] :type target: int :rtype: bool """ # 如果写成 not target 是有问题，当 target ==0 的时候，这个是不成立的，所以需要看一下数据的范围 # 注意区分 if not matrix or target ==None: return False rows, cols=len(matrix), len(matrix[0]) low, high =0, rows*cols-1 # 总的二叉 while low &lt;=high: mid =(low +high) /2 num =matrix[mid/cols][mid%cols] if num ==target: return True elif num&lt; target: low =mid +1 else: high =mid -1 return False Search a 2D Matrix II Write an efficient algorithm that searches for a value in an m x n matrix. This matrix has the following properties: Integers in each row are sorted in ascending from left to right. Integers in each column are sorted in ascending from top to bottom. [ [1, 4, 7, 11, 15], [2, 5, 8, 12, 19], [3, 6, 9, 16, 22], [10, 13, 14, 17, 24], [18, 21, 23, 26, 30]] Tips: 注意区分和上一道题目的第二点的区别。这个只能是一步步走，下面的程序是从右上方开始走，如果 target 大则向下直走，否则左走。可以有两种初始化方式，一种是右上角，一种是左下角。 https://leetcode.com/problems/search-a-2d-matrix-ii/ 1234567891011121314151617181920212223242526272829303132333435class Solution(object): def searchMatrix(self, matrix, target): """ :type matrix: List[List[int]] :type target: int :rtype: bool """ # 一开始的时候不知道使用什么遍历方式，因为 for 好像不太行，应该使用 while 基于条件遍历 if matrix ==[]: return False rows, cols =len(matrix)-1, len(matrix[0])-1 """ row, col = 0, cols # start points while row &lt;= rows and col &gt;= 0: if matrix[row][col] == target: return True elif matrix[row][col] &lt; target: row +=1 else: col -=1 """ # 还有一种初始化方式 row, col =rows, 0 while row &gt;=0 and col &lt;= cols: if matrix[row][col] ==target: return True elif matrix[row][col] &lt; target: col +=1 else: row -=1 return False Kth Smallest Element in a Sorted Matrix Given a n x n matrix where each of the rows and columns are sorted in ascending order, find the kth smallest element in the matrix. Tips: 主要是看到 example 中的数据，有两种类型，一种是可以把matrix dense 之后依然是有序，另一种不是。这个是属于前者。下面这种解法比较新颖，使用heapq 进行操作，遍历k th 就得到了kth 最小。 ···pythonclass Solution(object): # 有一种方法是初始化为右上角，然后小往左走，大往下走 # 这个默认求解的k smallest，所以python 中heapq 默认也是小根堆，所以 def kthSmallest(self, matrix, k): &quot;&quot;&quot; :type matrix: List[List[int]] :type k: int :rtype: int &quot;&quot;&quot; heap, res, n =[(matrix[0][0], 0, 0)], 0, len(matrix) for k in range(1, k+1): # 这个是次数 res, row, col =heapq.heappop(heap) # 问题在于这里并没有体现了 row col相应的变化 +1 这类东西 # 这个是通过 heapq 不断地push 和pop 来得到相应的 row col 然后进行res 的获取的 if not row and col&lt; n-1: heapq.heappush(heap, (matrix[row][col+1], row, col+1)) if row&lt; n-1: heapq.heappush(heap, (matrix[row+1][col], row+1, col)) return res 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748** Evaluate Reverse Polish Notation**&gt; Evaluate the value of an arithmetic expression in Reverse Polish Notation.Valid operators are +, -, *, /. Each operand may be an integer or another expression.Tips: 逆波兰又称之为后缀表达式，操作符置于操作数后面，这个前后是以“操作符” 进行定义的。解题思路，如果是操作数，那么就压栈，如果是操作符，那么弹出进行运算。```pythonclass Solution(object): def evalRPN(self, tokens): stack =[] operators =[&apos;+&apos;, &apos;-&apos;, &apos;*&apos;, &apos;/&apos;] for token in tokens: if token not in operators: #这种比较nice exact stack.append(int(token)) # 细节 string to int else: if len(stack) &lt;2: return False second =stack.pop() first =stack.pop() if token ==&quot;+&quot;: result =first +second elif token ==&quot;-&quot;: result =first -second elif token ==&quot;*&quot;: result =first *second else: # 除法向来处理就比较麻烦 if second ==0: return False # 这个是操作中的abs 没有改变原来的值，所以比较nice result =abs(first)/abs(second) if first *second &lt;0: result =-result stack.append(result) # 最后只有一个result 值，所以十分简洁 if len(stack) !=1: return False return stack[0] Excel Sheet Column Number Given a column title as appear in an Excel sheet, return its corresponding column number. Tips: 在于 char 和num 的对应关系。ord() 用于 char 转成int 这种库函数还是要有的。 可以看成 26 进制。 123456789101112class Solution(object): def titleToNumber(self, s): """ :type s: str :rtype: int """ res =0 for char in s: res =res*26 +(ord(char)- ord('A') +1) return res Largest Number Given a list of non negative integers, arrange them such that they form the largest number. Tips: string 类型组合成的数字是最大的。 在string 里面 ‘9’ &gt; ‘88888’ 这个是成立，所以这个特性可以处理这个题目，很巧妙。 12345678910class Solution(object): def largestNumber(self, nums): """ :type nums: List[int] :rtype: str """ nums =map(str, nums) nums.sort(cmp =lambda a, b :cmp(a+b, b+a), reverse =True) # 降序 # 可能出现 00 这样的字符串，所以是先 int 然后再string，感觉这个不是算法的味道 return str(int(''.join(nums))) Longest Substring with At Least K Repeating Characters Find the length of the longest substring T of a given string (consists of lowercase letters only) such that every character in T appears no less than k times. Tips: 这个codes 中的else 还是相当的牛逼，第一次见这种写法的。如果 for 循环中的条件不成立，else。 12345678910111213141516171819class Solution: def longestSubstring(self, s, k): """ :type s: str :type k: int :rtype: int """ stack = [] stack.append(s) ans = 0 while stack: s = stack.pop() for c in set(s): if s.count(c) &lt; k: stack.extend([z for z in s.split(c)]) break else: ans = max(ans, len(s)) return ans Longest Increasing Path in a Matrix Given an integer matrix, find the length of the longest increasing path.From each cell, you can either move to four directions: left, right, up or down. You may NOT move diagonally or move outside of the boundary (i.e. wrap-around is not allowed). Tips: dfs, 判断条件是 val &gt; matrix[i][j] 12345678910111213141516171819202122232425262728class Solution(object): # 一看这个就是深度优先搜索 # 这种做法更加普世 def longestIncreasingPath(self, matrix): """ :type matrix: List[List[int]] :rtype: int """ # 表示以这点为终点的 路径是有多长 # 这个逻辑上是比较简单的， 就是dfs()，然哦吼如果从任意一点出发 range() range()， # 使用 dfs() ，如果是value &gt; matrix[][]，就直接返回了 dp[i][j] def dfs(i, j): if not dp[i][j]: val = matrix[i][j] # i-1 的时候要大于0 i+1的时候要i &lt; M 这样的操作 dp[i][j] = 1 + max( dfs(i - 1, j) if i and val &gt; matrix[i - 1][j] else 0, dfs(i + 1, j) if i &lt; M - 1 and val &gt; matrix[i + 1][j] else 0, dfs(i, j - 1) if j and val &gt; matrix[i][j - 1] else 0, dfs(i, j + 1) if j &lt; N - 1 and val &gt; matrix[i][j + 1] else 0) return dp[i][j] if not matrix or not matrix[0]: return 0 M, N = len(matrix), len(matrix[0]) dp = [[0] * N for i in range(M)] # 以该点为终点的 increasing path 有多少个 return max(dfs(x, y) for x in range(M) for y in range(N)) Word Ladder Given two words (beginWord and endWord), and a dictionary’s word list, find the length of shortest transformation sequence from beginWord to endWord, such that:Only one letter can be changed at a time.Each transformed word must exist in the word list. Note that beginWord is not a transformed word. Tips: 讲解-tm) https://leetcode.com/problems/word-ladder/ 123456789101112131415161718192021222324252627class Solution(object): # https://leetcode.com/problems/word-ladder/discuss/157376/Python-(BFS)-tm # 写出来之后就比较好理解，可以好好想想 def ladderLength(self, beginWord, endWord, wordList): """ :type beginWord: str :type endWord: str :type wordList: List[str] :rtype: int """ wordList =set(wordList) queue =collections.deque([(beginWord, 1)]) visited =set() alpha =string.ascii_lowercase # 'abcd..z' while queue: word, length =queue.popleft() if word == endWord: return length for i in range(len(word)): for ch in alpha: new_word =word[:i] +ch+word[i+1:] if new_word in wordList and new_word not in visited: queue.append((new_word, length+1)) visited.add(new_word) return 0 Fraction to Recurring Decimal Given two integers representing the numerator and denominator of a fraction, return the fraction in string format.If the fractional part is repeating, enclose the repeating part in parentheses. Tips: 分数变成小数 123456789101112131415161718192021222324252627282930313233343536373839class Solution(object): # 主要是考察分情况讨论，这样是比较多的 # 就是在拼接呀 def fractionToDecimal(self, numerator, denominator): """ :type numerator: int :type denominator: int :rtype: str """ res ='' if numerator % denominator ==0: return str(numerator/denominator) if numerator* denominator &lt;0: res += '-' numerator, denominator =abs(numerator), abs(denominator) res += str(numerator/denominator) res +='.' numerator %= denominator i =len(res) table =&#123;&#125; # 下面描述的就是辗转相除的过程， 使用 &#123;&#125; 进行存储 while numerator !=0: if numerator not in table: table[numerator] =i else: i =table[numerator] res =res[:i] +'('+res[i:]+')' return res numerator =numerator*10 res += str(numerator/denominator) numerator %= denominator i +=1 return res Reverse Bits Reverse bits of a given 32 bits unsigned integer. Tips: 与操作和左移操作 ( &amp; and &lt;&lt;) 是常见的 bit operation中用到的 ···pythonclass Solution: # @param n, an integer # @return an integer # 没有什么说的， 二级制操作，注意输入和输出都是 integer # One small thing is the plus operator can be replaced by &quot;bitwise or&quot;, aka &quot;|&quot;. # Just generate the answer bit by bit, do not use things like &quot;% 2&quot; or &quot;2 ** k&quot; or &quot;bin&quot;. Bit manipulation is a lot faster. def reverseBits(self, n): ans =0 # 从后往前处理，所以这就reverse 了 for i in range(32): # n&amp;1 是取最后一位 # ans &lt;&lt;1 左移一位，类似乘2 ans += n &amp;1 if i ==31: return ans n &gt;&gt;= 1 ans &lt;&lt;= 1 return ans 12345678910111213141516171819202122232425262728293031323334** Word Break**&gt; Given a non-empty string s and a dictionary wordDict containing a list of non-empty words, determine if s can be segmented into a space-separated sequence of one or more dictionary words.Tips: dp 问题```pythonclass Solution(object): # 字符串的处理，感觉有点难呀 # dp的思路， dp[i] 表示 s[:i] 是否可分 def wordBreak(self, s, wordDict): &quot;&quot;&quot; :type s: str :type wordDict: List[str] :rtype: bool &quot;&quot;&quot; dict =&#123;&#125; for w in wordDict: dict[w] =True dp =[False for x in range(len(s)+1)] dp[0] =True for i in range(1, len(s)+1): # 如果出现了 range(i) 这种是常用的处理字符串的手段，看前i 是否符合某种要求 # 前面的可分，后面的看一下是否可分 for j in range(i): if dp[j] and s[j:i] in dict: dp[i] =True break return dp[-1] Word Break II Given a non-empty string s and a dictionary wordDict containing a list of non-empty words, add spaces in s to construct a sentence where each word is a valid dictionary word. Return all such possible sentences. Tips : dfs 1234567891011121314151617181920212223242526272829303132333435class Solution(object): # 上一题是返回 true or false，这个是要求是路径，那么最直接的就是dfs(), 不能使用dp了 def wordBreak(self, s, wordDict): """ :type s: str :type wordDict: List[str] :rtype: List[str] """ return self.dfs(s, wordDict, &#123;&#125;) def dfs(self, s, wordDict, memo): # 这个memo 就是某长度的字符串，在之前的dfs 中是否存在过 # 'penapple': ['pen apple'], 'applepenapple': ['apple pen apple' if s in memo: return memo[s] if not s: return [] res =[] for word in wordDict: # 这种直接从 dictionary 中寻找要比 从string 中拼凑快一些 if not s.startswith(word): # 这个就是最贪婪的找开头的python 句子 continue if len(word) ==len(s): # 包含且长度相同，那么 res.append() 就是这个操作了 res.append(word) else: rest =self.dfs(s[len(word):], wordDict, memo) # 如果原来的 s 比较长 那么就切分了 # 这种不收 递归影响的思维还是挺牛逼的 哈哈 for item in rest: item =word +' '+item res.append(item) memo[s] =res return res Palindrome Partitioning Given a string s, partition s such that every substring of the partition is a palindrome.Return all possible palindrome partitioning of s. Tips: dfs, 其中 ispa 写的是比较简洁的 123456789101112131415161718192021222324252627class Solution(object): # 感觉这个有点难度呀 # 所有值的一般是 dfs() 这个还是得不断的加强认识的， def partition(self, s): """ :type s: str :rtype: List[List[str]] """ res =[] self.dfs(s, [], res) return res def dfs(self, s, path, res): if not s: res.append(path) return # 关键是这里的理解， path 是不断的增加的，并且 s[I:] 这个是不断的介绍的， # 先是要求 s[:i] 是 palindrome 然后递归 s[i:] 是palindrome ，整体上是比较nice的 for i in range(1, len(s)+1): if self.isPal(s[:i]): self.dfs(s[i:], path+[s[:i]], res) def isPal(self, s): return s ==s[::-1] Word Search II Given a 2D board and a list of words from the dictionary, find all words in the board.Each word must be constructed from letters of sequentially adjacent cell, where “adjacent” cells are those horizontally or vertically neighboring. The same letter cell may not be used more than once in a word. Tips: 使用字典树 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354class TrieNode(): def __init__(self): self.children = collections.defaultdict(TrieNode) self.isWord = Falseclass Trie(): def __init__(self): self.root = TrieNode() def insert(self, word): node = self.root for w in word: node = node.children[w] node.isWord = True def search(self, word): node = self.root for w in word: node = node.children.get(w) if not node: return False return node.isWord# 上面在上一道题目中就应该记住，这个是一道经典的题目class Solution(object): def findWords(self, board, words): res = [] trie = Trie() node = trie.root for w in words: trie.insert(w) # 先是insert，然后在每一个点进行查找，最后看res for i in range(len(board)): for j in range(len(board[0])): self.dfs(board, node, i, j, "", res) return res def dfs(self, board, node, i, j, path, res): if node.isWord: res.append(path) node.isWord = False if i &lt; 0 or i &gt;= len(board) or j &lt; 0 or j &gt;= len(board[0]): return tmp = board[i][j] node = node.children.get(tmp) if not node: return board[i][j] = "#" self.dfs(board, node, i + 1, j, path + tmp, res) self.dfs(board, node, i - 1, j, path + tmp, res) self.dfs(board, node, i, j - 1, path + tmp, res) self.dfs(board, node, i, j + 1, path + tmp, res) board[i][j] = tmp Valid Anagram Given two strings s and t , write a function to determine if t is an anagram of s. Tips: dictionary 的应用 12345678910111213141516171819202122232425262728293031323334353637class Solution(object): # 这个和旋转数组 感觉上是差不多的呀 # 解答的时候，应该从 dictionary 的角度考虑 def isAnagram(self, s, t): """ :type s: str :type t: str :rtype: bool """ dic =&#123;&#125; # dic =collections.defaultdic(int) 和上面的唯一差别就是，直接使用 dic[char] +=1 这样的操作 # 不用判断是否存在 这样的操作 for n in s: if n not in dic: dic[n] =1 else: dic[n] +=1 for n in t: if n not in dic: return False else: dic[n] -=1 """ for n in dic: if dic[n]!=0: return False return True """ for value in dic.values(): if value !=0: return False return True First Unique Character in a String Given a string, find the first non-repeating character in it and return it’s index. If it doesn’t exist, return -1. Tips: 注意是第一个 non-repeating的 1234567891011121314151617181920class Solution(object): # 这个不重复 是整体之后的不重复，而不是左右的不重复，是全局的 # 我的想法使用dict def firstUniqChar(self, s): """ :type s: str :rtype: int """ dic =&#123;&#125; seen =set() for index, ch in enumerate(s): if ch not in seen: dic[ch] =index seen.add(ch) elif ch in dic: del dic[ch] # 这个是通过更新index 达到的 # 因为题目中提到的是 第一个 non-repeating Reverse String Write a function that reverses a string. The input string is given as an array of characters char[].Do not allocate extra space for another array, you must do this by modifying the input array in-place with O(1) extra memory. Tips : in-place 操作 pointer 123456789101112131415class Solution(object): # 我反手一个reverse() 过去，有问题吗 # sting is immutable, cannot reverse in-place def reverseString(self, s): """ :type s: List[str] :rtype: None Do not return anything, modify s in-place instead. """ left, right =0, len(s)-1 while left &lt; right: s[left], s[right] =s[right], s[left] left +=1 right -=1]]></content>
      <categories>
        <category>算法</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[算法模板-基础算法]]></title>
    <url>%2F2019%2F02%2F06%2Falgorithm-demo1%2F</url>
    <content type="text"><![CDATA[介绍基础性算法 双指针问题 最长连续不重复子序列 时间负责度是 $O(n)$ 12345678910111213141516171819202122232425#include&lt;iostream&gt;using namespace std;const int N =100000 +11;int n;int arr[N];int tmp[N];int main()&#123; cin &gt;&gt;n; for(int i =0; i&lt; n ;i++) scanf("%d",&amp;arr[i]); int res =0; for(int i =0,j =0; i&lt; n; i++) &#123; tmp[arr[i]] ++; while(j&lt; i &amp;&amp; tmp[arr[i]] &gt;1) &#123; tmp[arr[j++]] --; &#125; //cout &lt;&lt; i&lt;&lt; " "&lt;&lt; j&lt;&lt;endl; res =max(res, i -j +1); &#125; cout &lt;&lt; res&lt;&lt;endl; return 0;&#125; 数组元素的目标和 要使用双指针算法，就要找到其中的单调性。当i，j 分别指向 A数组的开头和B 数组的结尾。当 i 向右移动，那么与之对应的j 必然在原先的左边。所以这就符合单调性的要求，那么就可以使用双指针来做。 1234567891011121314151617181920212223#include&lt;iostream&gt;#include&lt;vector&gt;using namespace std;const int N = 100000+11;int n, m, x;int A[N];int B[N];int main()&#123; cin &gt;&gt;n &gt;&gt;m &gt;&gt;x; for(int i =0; i&lt; n; i++) scanf("%d", &amp;A[i]); for(int i =0; i&lt; m; i++ ) scanf("%d", &amp;B[i]); // 使用双指针算法，必然需要找到其中的单调性， for(int i =0, j =m -1; i&lt; n ; i++) &#123; while(j &gt;=0 &amp;&amp; A[i] +B[j] &gt; x) j --; if(A[i] +B[j] ==x) printf("%d %d", i, j); &#125; return 0;&#125; 最长不含重复字符的子字符串 这道题目和上一道题目是很相似的。。第一种解法是是双指针解法。 123456789101112131415161718192021class Solution &#123;public: int longestSubstringWithoutDuplication(string s) &#123; unordered_map&lt;char, int&gt; has; // 双指针问题 int n =s.size() ; int res =0; for(int i =0, j =0; i&lt; n; i++ ) &#123; has[s[i]] ++; while( j&lt; i &amp;&amp; has[s[i]] &gt;1) &#123; has[s[j]] --; // 求解的长度，可以先减一个嘴说 j ++; &#125; res =max(res, i -j +1); &#125; return res; &#125;&#125;; 第二种解法是dp 解法。 字符流中第一个只出现一次的字符 对于这种重复不重复的题目，经常使用的就是 dictionary 进行判断。 12345678910111213141516171819202122class Solution&#123;public: // 队列模拟读入和弹出的过程， // 到底 什么是dictionary，就是一个快速访问的数据结构，从key 到value的那种 //Insert one char from stringstream unordered_map&lt;char, int&gt; dic; queue&lt;char&gt; q; void insert(char ch)&#123; if( ++dic[ch] &gt;1) &#123; while(q.size() &amp;&amp; dic[q.front()] &gt;1) q.pop(); &#125; else q.push(ch); &#125; //return the first appearence once char in current stringstream char firstAppearingOnce()&#123; if(q.size() &gt;=1) return q.front(); else return '#'; &#125;&#125;; 调整数组顺序使奇数位于偶数前面 两个指针相遇，走过的总路程是$n$, 所以时间复杂度是$O(n)$ 12345678910111213class Solution &#123;public: void reOrderArray(vector&lt;int&gt; &amp;array) &#123; int l =0, r =array.size() -1; while(l&lt; r) &#123; while(l &lt; r &amp;&amp; array[l] %2 ==1) l ++; //int t=array[l]; while(l&lt; r &amp;&amp; array[r] %2 ==0) r --; if(l&lt; r) swap(array[l], array[r]); &#125; &#125;&#125;; 和为S的连续正数序列 1234567891011121314151617181920212223class Solution &#123;public: // 使用i j 两个指针操作 vector&lt;vector&lt;int&gt; &gt; findContinuousSequence(int sum) &#123; int i=1, j =2; vector&lt;vector&lt;int&gt;&gt; res; while(i &lt; j &amp;&amp; j&lt; sum) &#123; int total = (j -i+1) *(i +j); if(total == 2*sum) &#123; vector&lt;int&gt; level; for(int k =i; k&lt;=j ; k++) level.push_back(k); res.push_back(level); level.clear(); i ++, j ++; &#125; else if(total &gt; 2*sum) i ++; else j ++; &#125; return res; &#125;&#125;; 翻转单词顺序 12345678910111213141516171819202122232425class Solution &#123;public: // 需要翻转两次，一次以字母为单位全翻转，一次以单词为单位翻转 // 可以处理前后有空格，中间有多余的空格的情况 string reverseWords(string s) &#123; int k =0;// k存储的是有效的长度 for(int i =0; i&lt; s.size() ; i++) &#123; // 找到字母开始 int j =i; while(j &lt; s.size() &amp;&amp; s[j] ==' ') j ++; // j指向的是非空格 if (j ==s.size()) break; // 找到字母的结束 i =j; while(j &lt; s.size() &amp;&amp; s[j] !=' ') j++;// j指向的是空格 //细节， if(k) s[k++] =' '; reverse(s.begin() +i, s.begin() +j);// 左闭右开 while(j -i) s[k++] =s[i++]; &#125; s.erase(s.begin() +k, s.end()); reverse(s.begin(), s.end()); return s; &#125;&#125;; 纪念品分组 贪心算法 + 双指针（题目还是有一定的规律，比如，操作 j– 那么必须 j&gt;=0 ），对于贪心，常见的套路就是 先排序，然后选择“价值” 最大的。对于双指针常见的写法 for 循环然后是while 循环。本题的时间复杂度是 $O(nlogn)$. 上限是sort() 函数的排序算法。 123456789101112131415161718192021222324#include&lt;iostream&gt;#include&lt;algorithm&gt;using namespace std;const int N = 30000 +11;int a[N];bool st[N];int main()&#123; int w,n; cin &gt;&gt;w&gt;&gt; n; for(int i =0; i&lt; n; i++) cin &gt;&gt; a[i]; sort(a, a+n); int res =0; // 尽可能贪心的选择，选择两个，然后选择价值最大的 for(int i =0, j =n -1; i&lt; n;i++) &#123; if(st[i]) continue; while( j &gt;= 0&amp;&amp;( st[j] || a[i] +a[j] &gt;w )) j --; st[i] =st[j] =true; res +=1; &#125; cout&lt;&lt; res &lt;&lt;endl; return 0;&#125; 所以模板可以处理两类问题。 123456789for (int i = 0, j = 0; i &lt; n; i ++ )&#123; while (j &lt; i &amp;&amp; check(i, j)) j ++ ; // 具体问题的逻辑&#125;常见问题分类： (1) 对于一个序列，用两个指针维护一段区间 (2) 对于两个序列，维护某种次序，比如归并排序中合并两个有序序列的操作 75. Sort Colors 时间复杂度是 $O(n)$ ，空间复杂度是$O(1)$123456789101112131415class Solution &#123;public: // l 和r 分别表示 01 边界问题， 12 边界问题 void sortColors(vector&lt;int&gt;&amp; nums) &#123; int l =0, r =nums.size() -1; int index =0; // 注意这个边界条件 while( index&lt;=r ) &#123; if(nums[index] ==1) index ++; else if(nums[index] ==0) swap(nums[index++], nums[l++]); else swap(nums[index], nums[r--]); &#125; &#125;&#125;; 合并子序列时间复杂度是 $NlogN$ 123456789101112131415161718192021222324252627282930313233343536373839#include&lt;algorithm&gt;#include&lt;vector&gt;#include&lt;iostream&gt;using namespace std;const int N =1e5+11;typedef pair&lt;int, int&gt; PAIR;int n;vector&lt;PAIR&gt; arr;void merge(vector&lt;PAIR&gt; &amp; segs)&#123; vector&lt;PAIR&gt; res; sort(segs.begin(), segs.end()); int l =-2e9, r =-2e9; for(auto seg : segs) &#123; // 好好理解为什么之类是 first if(seg.first &gt; r) &#123; if( l != -2e9) res.push_back(&#123;l, r&#125;); l =seg.first, r =seg.second; &#125; else r =max(r, seg.second); &#125; if(l != -2e9) res.push_back(&#123;l, r&#125;); segs =res;&#125;int main()&#123; cin &gt;&gt; n; for(int i =0; i&lt; n; i ++) &#123; int a, b; scanf("%d %d", &amp;a, &amp;b); arr.push_back(&#123;a, b&#125;); &#125; merge(arr); cout &lt;&lt; arr.size() &lt;&lt; endl; return 0;&#125; Insert Interval 时间复杂度是 $O(n)$，空间复杂度是$O(n)$ 1234567891011121314151617181920212223class Solution &#123;public: // 这个题目相对于 合并区间是简单的，因为一开始有有序的，然后加入一个无序的，总的是 O(n) vector&lt;vector&lt;int&gt;&gt; insert(vector&lt;vector&lt;int&gt;&gt;&amp; intervals, vector&lt;int&gt;&amp; newInterval) &#123; vector&lt;vector&lt;int&gt;&gt; res; int i=0, n =intervals.size(); // 如果intervals 中的一部分是小于 newInterval的左边 while( i &lt; n &amp;&amp; intervals[i][1] &lt; newInterval[0]) &#123; res.push_back(intervals[i++]); &#125; // 如果intervals 中的某一个和 newInterval 有交集 while(i &lt;n &amp;&amp; intervals[i][0] &lt;= newInterval[1]) &#123; newInterval[0] =min(intervals[i][0], newInterval[0]); newInterval[1] =max(intervals[i][1], newInterval[1]); i++;// 这里非常容易拉掉 i ++ &#125; res.push_back(newInterval); while(i&lt; n ) res.push_back(intervals[i++]); return res; &#125;&#125;; 高精度计算在32位或64位机器中，int占4个字节，即32位。C/C++规定该值为-2^31=-2147483648（大概是 21 亿的样子）。longlong 是64位。在字符串处理的时候，加法和乘法需要加上t(如果最后t &gt;0)， 对于除法和减法，需要去掉最后多余的0. 高精度加法 基本上是正序处理加法，然后是逆序输出和输入。 12345678910111213141516171819202122232425262728293031323334#include&lt;iostream&gt;#include&lt;vector&gt;#include&lt;string&gt;using namespace std;vector&lt;int&gt; add(vector&lt;int&gt; &amp; a, vector&lt;int&gt; &amp; b)&#123; vector&lt;int&gt; res; int t =0; // 这个 i相当于是每次处理的数字 // 这里的 || 是一个细节，如果是，是需要这样写的 for(int i =0 ; i&lt; a.size() || i&lt; b.size() ; i++ ) &#123; if(i &lt; a.size() ) t += a[i]; if( i&lt; b.size() ) t += b[i]; res.push_back(t %10); t =t/10; &#125; if(t &gt;0) res.push_back(t); return res;&#125;int main()&#123; string a, b; cin&gt;&gt; a&gt;&gt; b; vector&lt;int&gt; A, B; // 从低位往高位计算的 for(int i =a.size() -1; i&gt;=0 ; i--) A.push_back(a[i] -'0'); for(int i =b.size() -1; i&gt;=0. ; i--) B.push_back(b[i] -'0'); // 输出也很对称，也是逆序输出，因为先处理的低位，然后低位是push_back() 到后面了，应该是最后输出 vector&lt;int&gt; res =add(A, B); for(int i =res.size()-1 ; i&gt;=0 ; i-- ) cout&lt;&lt; res[i]; cout &lt;&lt;endl; return 0;&#125; 高精度减法 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152#include&lt;vector&gt;#include&lt;string&gt;#include&lt;iostream&gt;using namespace std;bool cmp(vector&lt;int&gt;&amp; a, vector&lt;int&gt;&amp; b)&#123; if(a.size() != b.size() ) return a.size() &gt; b.size() ; for(int i =0; i&lt; a.size() ; i++) &#123; if(a[i] != b[i] ) return a[i] &gt;b[i]; &#125; return true;&#125;// a&gt;=bvector&lt;int&gt; sub(vector&lt;int&gt;&amp; a, vector&lt;int&gt;&amp; b)&#123; vector&lt;int&gt; res; int t =0; for(int i =0; i&lt; a.size() ; i++) &#123; t = a[i] -t; if(i &lt; b.size()) t -= b[i]; res.push_back( (t+10) %10); // 加法中补位操作 if(t &lt;0) t =1; else t =0; &#125; // 去0 的操作 while(res.size() &gt;0 &amp;&amp; res.back() == 0) res.pop_back(); return res;&#125;int main()&#123; string a, b; cin &gt;&gt; a&gt;&gt; b; vector&lt;int&gt; A, B; for(int i =a.size() -1; i&gt;=0 ; i--) A.push_back(a[i]); for(int i =b.size() -1; i&gt;=0; i--) B.push_back(b[i]); // a &gt;= b if(cmp(A, B)) &#123; vector&lt;int&gt; c =sub(A, B); for(int i =c.size() -1; i&gt;=0; i--) cout &lt;&lt; c[i]; &#125; else &#123; vector&lt;int&gt; c =sub(B, A); cout &lt;&lt; '`'; for(int i =c.size() -1; i&gt;=0; i--) cout &lt;&lt; c[i]; &#125; return 0;&#125; 高精度乘法 两个数字是不一样的，一个是需要使用string 表示，一个是使用int 表示就行。 121≤A的长度≤1000001≤B≤10000 实现代码 1234567891011121314151617181920212223242526272829#include&lt;iostream&gt;#include&lt;vector&gt;#include&lt;string&gt;using namespace std;vector&lt;int&gt; multiply(vector&lt;int&gt; a, int b)&#123; vector&lt;int&gt; res; // 这个判断条件也是比较精巧的 int t=0; for(int i =0; i&lt; a.size() || !b ; i++) &#123; if(i &lt; a.size()) t +=a[i] *b; res.push_back(t %10); t /= 10; &#125; return res;&#125;// 对于大数转成字符串，逆序，然后处理，逆序输出，这个就是非常common 的操作了int main()&#123; string a; int b ; cin &gt;&gt; a&gt;&gt; b; vector&lt;int&gt; A; for(int i =a.size() -1; i&gt;=0; i--) A.push_back(a[i] -'0'); vector&lt;int&gt; res =multiply(A, b); for(int i =res.size() -1; i&gt;=0 ; i--) cout &lt;&lt; res[i]; return 0;&#125; 乘法还是比较nice 的。 12345678910111213141516171819202122232425262728#include&lt;string&gt;#include&lt;vector&gt;#include&lt;iostream&gt;using namespace std;vector&lt;int&gt; a, c;int b;void multiply()&#123; int t =0; for(int i =0; i&lt; a.size(); i++) &#123; t += a[i] *b; c.push_back(t %10); t /=10; &#125; if(t &gt;0) c.push_back(t);&#125;int main()&#123; string A; cin&gt;&gt; A ; cin &gt;&gt; b; for(int i =A.size() -1; i&gt;=0; i--) a.push_back(A[i] -'0'); multiply(); for(int i =c.size() -1; i&gt;=0 ; i--) cout &lt;&lt;c[i]; cout &lt;&lt; endl; return 0;&#125; 高精度除法 （个人感觉这个叫做大数除法更加合适）给定两个正整数A，B，请你计算 A / B的商和余数。 高精度除法 123456789101112131415161718192021222324252627282930313233#include&lt;string&gt;#include&lt;iostream&gt;#include&lt;vector&gt;#include&lt;algorithm&gt;using namespace std;vector&lt;int&gt; res;void divide(vector&lt;int&gt; &amp; a, int b , int &amp; r)&#123; for(int i =0 ; i&lt; a.size() ; i++) &#123; r =10*r +a[i]; res.push_back(r /b);// 这个是结果求商的 r %= b; &#125; reverse(res.begin(), res.end()); while(res.size() &gt;0 &amp;&amp; res.back() ==0) res.pop_back();&#125;int main()&#123; string A; vector&lt;int&gt; a; int b; cin &gt;&gt; A; cin &gt;&gt; b; for(int i =0; i&lt; A.size() ; i++) a.push_back(A[i] -'0'); int r =0; divide(a, b, r); // 有时候segment 错误真的是很烦，因为很有可能是下标的问题，不能访问导致的错误。 for(int i =res.size() -1; i&gt;=0 ; i--) cout &lt;&lt; res[i]; cout &lt;&lt; endl; cout&lt;&lt; r&lt;&lt; endl; return 0;&#125; 一维前缀和思想很简单，使用一个 $O(n)$ 的预处理，然后再 $O(1)$ 的时间内就可以基三每个查询。所以总的时间复杂度是 $O(n)$ 空间是 $O(N)$. 关键在于背会模板，注意边界条件。 1234567891011121314151617181920212223242526272829#include&lt;iostream&gt;#include&lt;vector&gt;using namespace std;const int N =1e5;int n,m;int arr[N];int pre[N];// 记录原来的序列，空间 O(n) 时间是 O(n)int main()&#123; int l, r; cin&gt;&gt; n&gt;&gt;m; for(int i =1; i&lt;=n; i++) &#123; int tmp; cin &gt;&gt;tmp; arr[i] =tmp; &#125; for(int i =1; i&lt;=n; i++) &#123; pre[i] =pre[i-1] +arr[i]; &#125; while(m --) &#123; cin &gt;&gt; l &gt;&gt;r; cout &lt;&lt; pre[r] -pre[l-1] &lt;&lt;endl; &#125; return 0;&#125; 还有一个写法是，不用保存原来的数组。123456789101112131415161718192021222324#include&lt;iostream&gt;#include&lt;vector&gt;using namespace std;const int N =1e5+11;int n, m;int pre[N];int main()&#123; cin &gt;&gt;n&gt;&gt;m; int l, r; for(int i=1; i&lt;=n ; i++) &#123; int tmp; cin &gt;&gt; tmp; // pre[i-1] 表示之前的累加和 pre[i] = pre[i-1] +tmp; &#125; while(m --) &#123; cin &gt;&gt; l &gt;&gt;r; cout &lt;&lt; pre[r] -pre[l-1]&lt;&lt; endl; &#125; return 0;&#125; 二维前缀和二维前缀和的定义还是很重要的，看图 $S[i,j]S[i,j] $即为图中红框中所有数的的和为： $$S[i,j]=S[i,j−1]+S[i−1,j]−S[i−1,j−1]+a[i,j]S[i,j]=S[i,j−1]+S[i−1,j]−S[i−1,j−1]+a[i,j]$$ $(x1,y1),(x2,y2)(x1,y1),(x2,y2) $这一子矩阵中的所有数之和为： $$S[x2,y2]−S[x1−1,y2]−S[x2,y1−1]+S[x1−1,y1−1]$$ 这道题目的关键是搞清下标，最后再求解某一个区间的面积的时候， $s[x1-1][y2] $和 $s[x2][y1-1] $注意这点。 1234567891011121314151617181920212223242526272829#include&lt;iostream&gt;#include&lt;cstdio&gt;using namespace std;const int N =1011;int s[N][N];//int a[N][N];int n,m, q;int main()&#123; cin &gt;&gt;n&gt;&gt;m &gt;&gt;q; for(int i=1; i&lt;=n; i++) &#123; for(int j =1; j&lt;=m ; j++) &#123; //scanf("%d", &amp;a[i][j]); int tmp; scanf("%d", &amp;tmp); s[i][j] = s[i-1][j] +s[i][j -1] -s[i-1][j-1] + tmp; &#125; &#125; while(q--) &#123; int x1, y1, x2, y2; scanf("%d%d%d%d", &amp;x1, &amp;y1, &amp;x2, &amp;y2); // 关键是下标呀 printf("%d\n", s[x2][y2] - s[x1-1][y2] -s[x2][y1-1] + s[x1-1][y1-1]); &#125; return 0;&#125; 差分前缀和差分是2个互逆的运算。 这种策略是，令$b_i = a_i - a_{i-1} $，即相邻两数的差。易得对这个序列做一遍前缀和就得到了原来的 $a$序列。 它可以维护多次对序列的一个区间加上一个数，并在最后询问某一位的数或是多次询问某一位的数。（总之修改操作一定要在查询操作之前） 具体怎么搞？譬如使 $[l, r]$ 每个数加上一个 $k$ ，就是 $b_l \leftarrow b_l+k $, $b_{r+1} \leftarrow b_{r+1} - k$ 。最后做一遍前缀和就好了。 注意特殊处：这道题是先进行整体区间修改，最后才统一查询。 所以，我们只要维护一个差分数组就行了。总的来说差分数组适用于离线的区间修改问题，如果是在线的话应该用线段树或其他数据结构。为什么要存差值呢？————因为数列中的数满 $A[i]=sum{D[1]…D[i]} $,便于用递推求得最后的值。 12345678910111213141516171819202122232425262728#include&lt;iostream&gt;using namespace std;const int N =1e5+11;int n, m;int arr[N];int s[N];int main()&#123; cin &gt;&gt; n&gt;&gt;m; for(int i =1; i&lt;= n; i++) scanf("%d", &amp;arr[i]); for(int i =1; i&lt;=n; i++) s[i] =arr[i] -arr[i-1]; while(m --) &#123; int l, r,c; cin &gt;&gt; l&gt;&gt; r&gt;&gt;c; s[l] +=c; s[r+1] -=c; &#125; // 求解一个前缀和 for(int i =1; i&lt;=n ; i++) &#123; s[i] += s[i-1]; cout &lt;&lt; s[i]&lt;&lt;' '; &#125; return 0;&#125; 差分矩阵 讲解 123456789101112131415161718192021222324252627282930313233343536373839404142434445#include&lt;iostream&gt;using namespace std;const int N =1011;// 构造差分矩阵// 维护差分矩阵// 求解前项和，然后输出int a[N][N];int b[N][N];// 在 (x1, y1) 和 (x2, y2)这个范围内 +cvoid insert(int x1, int y1, int x2, int y2, int c)&#123; b[x1][y1] +=c; b[x2+1][y1] -= c; b[x1][y2+1] -=c; b[x2+1][y2+1] += c;&#125;int main()&#123; int n, m, q; cin &gt;&gt;n&gt;&gt;m&gt;&gt;q; for(int i =1; i&lt;= n; i++) &#123; for(int j =1; j&lt;= m; j++) &#123; scanf("%d",&amp;a[i][j]); insert(i, j, i, j, a[i][j]); &#125; &#125; while(q--) &#123; int x1, y1, x2, y2, c; cin&gt;&gt; x1&gt;&gt; y1 &gt;&gt;x2&gt;&gt; y2&gt;&gt;c; insert(x1, y1, x2, y2, c); &#125; for(int i =1; i&lt;=n ; i++) &#123; for(int j =1; j&lt;=m ; j++) &#123; b[i][j] +=b[i-1][j] +b[i][j-1] -b[i-1][j-1]; printf("%d ", b[i][j]); &#125; puts(""); &#125; return 0;&#125; 区间专题题解：绝大部分涉及到区间的题目第一步要做的都是按照左端点或者右端点进行排序，排好序后找到其中的递推关系。具体到这一题，我们可以先把问题转化成最多能找到多少个互不重叠的区间。 56. Merge Intervals Given a collection of intervals, merge all overlapping intervals. 合并所有的重合的区间。思路：双关键字排序，先按照 begin 排序，然后按照 end 排序。然后在添加的时候，如果当前遍历的区间的begin 是大于cur 的end，那么把 cur 的区间放到结果中，更新cur 为当前遍历的区间。否则如果遍历的区间的 end 是大于cur 的end，那么更新cur 的end。时间复杂度是$nlogn$， 排序算法是瓶颈。 123456789101112131415161718192021222324252627282930class Solution &#123;public: typedef pair&lt;int, int&gt; PAIR; // 整体的思路是，sort(&#123;begin, end&#125;) 先是按照第一关键字排序，然后按照第二关键字排序 vector&lt;vector&lt;int&gt;&gt; merge(vector&lt;vector&lt;int&gt;&gt;&amp; intervals) &#123; int n =intervals.size() ; vector&lt;vector&lt;int&gt;&gt; res; vector&lt;PAIR&gt; pre; if(n ==0) return res; for (auto item : intervals) &#123; PAIR t =&#123;item[0], item[1]&#125;; pre.push_back(t); &#125; sort(pre.begin(), pre.end()); vector&lt;int&gt; cur =vector&lt;int&gt;&#123;pre[0].first, pre[0].second&#125;; for(int i =1; i&lt; pre.size(); i++) &#123; if(pre[i].first &gt; cur[1]) &#123; res.push_back(cur); cur =vector&lt;int&gt;&#123;pre[i].first, pre[i].second&#125;; &#125; else if(pre[i].second&gt; cur[1]) cur[1] =pre[i].second; &#125; res.push_back(cur); return res; &#125;&#125;; 57. Insert Interval 12345678910111213141516171819202122232425262728293031323334353637class Solution &#123;public: // 这个相对于前一个的思路可能是比较难的，该如何做呢？ // 需要设置一个特殊的标志位，表示是否已经把 newInterval 处理掉了 vector&lt;vector&lt;int&gt;&gt; insert(vector&lt;vector&lt;int&gt;&gt;&amp; intervals, vector&lt;int&gt;&amp; newinter) &#123; bool flag =false; vector&lt;vector&lt;int&gt;&gt; res; int n =intervals.size(); for(int i =0; i&lt; n; i++) &#123; if(newinter[1] &lt; intervals[i][0]) &#123; if(!flag)&#123; res.push_back(newinter); flag =true; &#125; // 如果已经处理过 newinter ，这个时候就可以不断的加入区间到最后的结果中去 res.push_back(intervals[i]); &#125; else if(intervals[i][1] &lt; newinter[0]) &#123; res.push_back(intervals[i]); &#125; else &#123; newinter[0] =min(newinter[0], intervals[i][0]); newinter[1] =max(newinter[1], intervals[i][1]); &#125; &#125; //cout &lt;&lt; flag&lt;&lt; endl; if(!flag) &#123; res.push_back(newinter); &#125; return res; &#125;&#125;; 数学知识 判断质数 一个数的因数都是成对出现的：例如12 的因数有3 和4，2和6.所以我们是可以枚举较小的那个，即根号n，假设较小的是d，较大的是n /d prime，素数和质数是一个意思，合数是其相反的方面。 1234567891011121314151617181920212223242526// 0和1 既非质数也非合数，2 是最小的质数，最大的质数不存在// 对于质数的定义，如果a 只能被1 和本身整除，那么a 就是质数(a &gt;2)#include&lt;bits/stdc++.h&gt;using namespace std;bool is_prime(int x)&#123; if(x &lt; 2) return false; for(int i =2; i&lt;= x/i; i++) &#123; if(x %i ==0) return false; &#125; return true;&#125;int main()&#123; int n; cin &gt;&gt;n; for(int i =0; i&lt;n; i++) &#123; int tmp; cin &gt;&gt;tmp; if(is_prime(tmp)) cout &lt;&lt; "Yes"&lt;&lt;endl; else cout &lt;&lt; "No" &lt;&lt; endl; &#125; return 0;&#125; AcWing 867. 分解质因数 有了上面的质数，那么再求解一个数字的质因数分解，应该不是很难的事情。 123456789101112131415161718192021222324252627282930#include&lt;bits/stdc++.h&gt;using namespace std;// 质因数分解， 这个是能够保证过程中 i 是质数的void divide(int n)&#123; for(int i =2; i&lt;=n ; i++) &#123; // 住一个这个过程中n 是不断减少的，所以这个复杂度不是n^2 if( n%i ==0) &#123; int s =0; // 对于 i 这个质因数，couts 是多少? while(n %i ==0) n /=i, s++; cout &lt;&lt; i &lt;&lt;" "&lt;&lt; s&lt;&lt; endl; &#125; &#125; if(n &gt;1) cout &lt;&lt; n &lt;&lt; " "&lt;&lt; 1&lt;&lt; endl; cout &lt;&lt; endl;&#125;int main()&#123; int n; cin &gt;&gt;n; for(int i =0; i&lt; n; i++) &#123; int a; cin &gt;&gt;a; divide(a); &#125; return 0;&#125; 筛质数 讲解 123456789101112131415161718192021222324252627282930313233343536373839404142// 定理， 任意整数x 的倍数 2x 3x 等都不是质数#include&lt;bits/stdc++.h&gt;using namespace std;const int N =1e6+11;int primes[N], cnt;int st[N];// 朴素筛 O(nlogn) 这样的复杂度void get_primes(int n)&#123; for(int i =2; i&lt;=n ; i++) &#123; if(!st[i]) primes[cnt ++] =i; // 是第一个质数倍数的，那么全部都置为 true，那么久无法访问了 for(int j =i +i; j&lt;=n ; j+= i) st[j] =true; &#125;&#125;// 这种方法说实话，没有看懂//线性筛选， 时间复杂度是 O(n)void get_prime(int x) &#123; for(int i = 2; i &lt;= x; i++) &#123; if(!st[i]) primes[cnt++] = i; for(int j = 0; primes[j] &lt;= x / i; j++) &#123; //对于任意一个合数x，假设pj为x最小质因子，当i&lt;x/pj时，一定会被筛掉 st[primes[j]*i] = true; if(i % primes[j] == 0) break; /* 1.i%pj == 0, pj定为i最小质因子，pj也定为pj*i最小质因子 2.i%pj != 0, pj定小于i的所有质因子，所以pj也为pj*i最小质因子 */ &#125; &#125;&#125; int main()&#123; int n ; cin &gt;&gt;n; get_prime(n); //get_primes(n); cout &lt;&lt; cnt&lt;&lt; endl; return 0;&#125; AcWing 869. 试除法求约数 可以从实现的角度比较 合数和质数之间的差距。 首先在计算时候的初始化值是不同的，然后对于如何条件的 (x%i) 的处理也是不同的。 123456789101112131415161718192021222324252627282930#include&lt;bits/stdc++.h&gt;using namespace std;vector&lt;int&gt; get_divisors(int x)&#123; vector&lt;int&gt; res; for(int i =1; i&lt;= x/i; i++) &#123; if(x %i ==0) &#123; res.push_back(i); // 这里是一个细节，如果两个数字相同的话，那么只需要放进去一个，保证不重复 if(i != x/i) res.push_back(x /i); &#125; &#125; sort(res.begin(), res.end()); return res;&#125;int main()&#123; int n ; cin &gt;&gt;n; for(int i =0; i&lt; n; i++) &#123; int tmp; cin &gt;&gt;tmp; vector&lt;int&gt; res =get_divisors(tmp); for(auto u : res) cout &lt;&lt; u&lt;&lt;" "; cout &lt;&lt; endl; &#125; return 0;&#125; 求解约数个数和约数之和，就是理解下面的公式。有了上面求解约数的经验，那么下面这个应该不是很难。 123如果 N = p1^c1 * p2^c2 * ... *pk^ck约数个数： (c1 + 1) * (c2 + 1) * ... * (ck + 1)约数之和： (p1^0 + p1^1 + ... + p1^c1) * ... * (pk^0 + pk^1 + ... + pk^ck) 12345678910111213141516171819202122232425262728293031#include&lt;bits/stdc++.h&gt;using namespace std;typedef long long ll;const int MOD =1e9+7;// 因为 hash 是可以使用 first second 进行访问的，所以这个本质上也是一种键值对unordered_map&lt;int ,int&gt; primes;int main()&#123; int n; cin &gt;&gt;n; while(n --) &#123; int x ; cin &gt;&gt;x; for(int i =2; i&lt;= x/i; i++) &#123; while(x %i ==0) &#123; x /=i; primes[i] ++; &#125; &#125; if(x &gt;1) primes[x] ++; &#125; // 对于这种可能有 int 溢出，所以使用long long 来进行表示 // 尤其是出现了 10^9 +7 取模，这种，说明数字很大，这个时候是需要使用long long 来表示结果的 ll ans =1; for(auto item : primes) ans = ans*(item.second +1) %MOD; cout &lt;&lt; ans&lt;&lt; endl; return 0;&#125; 求解最大公约数 123456789101112131415161718#include&lt;bits/stdc++.h&gt;using namespace std;int gcb(int a, int b)&#123; return b ? gcb( b,a% b) :a;&#125;int main()&#123; int n ; cin &gt;&gt;n; while(n --) &#123; int a, b; cin &gt;&gt; a&gt;&gt;b; cout &lt;&lt; gcb(a, b)&lt;&lt; endl; &#125; return 0;&#125; 快速幂 快速幂顾名思义，就是快速算某个数的多少次幂。其时间复杂度为 O(log₂N)， 与朴素的O(N)相比效率有了极大的提高。 时间复杂度是 $O(n)$ 优化成了 $O(logn)$. 所谓的快速幂，就是使用平方的方式，而不是一次次乘法进行处理。 123456789101112131415161718192021222324252627#include&lt;bits/stdc++.h&gt;using namespace std;int fast_power(int a, int b, int p)&#123; int res=1; while(b) &#123; if(b &amp;1)// 这个表示奇数 res = 1ll* a*res %p; // 这里体现的是 logn 的思想 a = 1ll* a* a %p; b &gt;&gt;= 1; &#125; return res;&#125;int main()&#123; int n; cin &gt;&gt;n; while(n--) &#123; int a, b, p; cin &gt;&gt;a&gt;&gt;b&gt;&gt;p; printf("%d\n", fast_power(a, b, p)); &#125; return 0;&#125;]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>双指针</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pytorch]]></title>
    <url>%2F2019%2F02%2F01%2Fpytorch%2F</url>
    <content type="text"><![CDATA[pytorch 学习笔记 不理解的地方，一个是关于 nn包搭建网络的部分。 pytorch 和torch 比较 编程语言：pytorch 采用python语言，实际上使用c语言和c++ 做接口torch 采用lua，使用c语言和lua 语言做接口（lua 语言相当于一个小型加强版的c语言，支持类和面向对象）依赖库：pytorch 和 torch 框架的区别和联系：pytorch 可调用python强大的第三方库，比如 opencvtorch 可调用 lua 库函数，目前 lua库函数没有python多pytorch 依赖库多于 torch效率：python 的debug 功能比lua 强大，所以pytorch 效率高于torch模型和中间变量的关系：pytorch 中中间变量都存在计算图中，所以model 共享中间变量torch 的中间变量在每一个模块中，所以想要调用其他模块的参数就必须复制这个模块然后再调用 总结pytorch可以说是torch 的python版本，并增加了很多新功能 常用的框架比较 tensorflow 背后是google， mxnet 是Amazon，pytorch背后是Facebook每个框架都有各自的有点，比如tensorflow的工程能力很强，Theano特别适合科研等等keras是一个很高层的结构，它的后端支持theano和tensorflow，它本质上并不是一个框架，只是对框架的操作做了一个封装，你在写keras的时候其实是对其后端进行调用，相当于你还是在tensorflow或者theano上跑程序，只不过你把你的语言交给keras处理了一下变成tensorflow听得懂的语言，然后再交给tensorflow处理，这样的后果当然方便你构建网络，方便定义模型做训练，极快的构建你的想法，工程实现很强，但是这样也有一个后果，那就是细节你没有办法把控，训练过程高度封装，导致你没有办法知道里面的具体细节，以及每个参数的具体细节，使得调试和研究变得很困难。 pytorch的思想 PyTorch 的构建者表明，PyTorch 的哲学是解决当务之急，也就是说即时构建和运行我们的计算图。这恰好适合 Python 的编程方法，因为我们不需等待整个代码都被写入才能知道是否起作用。我们很容易运行部分代码，并实时检查它。 PyTorch 是一个基于 Python 的库，旨在为深度学习提供一个灵活的开发平台。PyTorch 的工作流程非常接近于 Python 的科学计算库 NumPy。那么为什么我们需要使用 PyTorch 构建深度学习模型？以下作者根据实际经验提供了三个理由： 便于使用的 API：它的使用如同 Python 那样简单。 支持 Python：正如上文所述，PyTorch 可以平滑地与 Python 数据科学栈相结合。它与 NumPy 一样简单，甚至我们都感觉不出它们的区别。 动态计算图：PyTorch 不再采用特定的函数预定义计算图，而是提供构建动态计算图的框架，甚至我们可以在运行时修正它们。这种动态框架在我们不知道所构建的神经网络需要多少内存时非常有用。其它一些使用 PyTorch 的优点还有多 GPU 支持、自定义数据加载器和极简的预处理过程等。 在讨论 PyTorch 的各个组件前，我们需要了解它的工作流。PyTorch 使用一种称之为 imperative / eager 的范式，即每一行代码都要求构建一个图以定义完整计算图的一个部分。即使完整的计算图还没有完成构建，我们也可以独立地执行这些作为组件的小计算图，这种动态计算图被称为「define-by-run」方法。 PyTorch 提供了 CPU 张量和 GPU 张量，并且极大地加速了计算的速度。从张量的构建与运行就能体会到 PyTorch 相比 TensorFLow 需要声明张量、初始化张量要简洁地多。以下语句将随机初始化一个 5×3 的二维张量，因为 PyTorch 是一种动态图，所以它声明和真实赋值是同时进行的。1torch.Tensor(5, 3) 若我们希望随机初始化的张量服从某些分布，那么我们可以直接对张量对象使用一些方法。如下初始化的张量将服从均匀分布：1torch.Tensor(5, 3).uniform_(-1, 1) PyTorch 同样支持广播（Broadcasting）操作，一般它会隐式地把一个数组的异常维度调整到与另一个算子相匹配的维度以实现维度兼容。 如下，我们定义了两个 GPU 张量，并对这两个张量执行矩阵乘法。当然，我们也可以如下所示将 CPU 张量转换为 GPU 张量。1234567# 以下转化CPU张量为GPU张量x = torch.FloatTensor(5, 3).uniform_(-1, 1)print(x)x = x.cuda(device=0)print(x)x = x.cpu()print(x) AutoGrad 模块 TensorFlow、Caffe 和 CNTK 等大多数框架都是使用的静态计算图，开发者必须建立或定义一个神经网络，并重复使用相同的结构来执行模型训练。改变网络的模式就意味着我们必须从头开始设计并定义相关的模块。 PyTorch 使用的技术为自动微分（automatic differentiation），这个是用来自动求解微分的模块。在这种机制下，系统会有一个 Recorder 来记录我们执行的运算，然后再反向计算对应的梯度。这种技术在构建神经网络的过程中十分强大，因为我们可以通过计算前向传播过程中参数的微分来节省时间。 从概念上讲, Autograd 对数据记录记录了一个有向无环图（DAG）， 叫做计算图，用来表示它的计算过程。沿着计算图应用链式求导法则就可以求出其梯度。Autograd 包中有两个核心包： torch.Tensor 和torch.Function， 默认某个 tensor的属性是 .requires_grad 为true，当计算完成的时候可以调用 .backward() 来自动计算所有的梯度，针对这个tensor 可以在 .grad 属性中去查看。 设置一个张量不跟踪历史记录的方法： 调用 .detach() 将其从计算历史中分离出来 使用 torch.no_grad() 包裹代码块，那么在该代码块中的计算都不会计算梯度。使用的情况是， 在评估阶段（predict）阶段。 设置某个tensor 的属性为 Required_grad =False Function 类，每一个tensor都有一个.grad_fn 属性指向一个 Function，表示如何得到了当期的tensor。如果是用户自己创建的张量tensor，那么 grad_fn is None。 .requires_grad具有传递性，比如说 $x_1, x_2, \dots, x_n$ 中某一个满足 required_grad =True，那么这个时候由这些tensor表示tensor 的属性都是true。 最优化模块torch.optim 是实现神经网络中多种优化算法的模块，它目前已经支持大多数一般的方法，所以我们不需要从头构建优化算法。以下展示了使用 Adam 优化器的基本代码：1optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) 我们一般可以使用 torch.nn 包构建神经网络，下面提供了一些 API 的表达及意义： 线性层- nn.Linear、nn.Bilinear 卷积层 - nn.Conv1d、nn.Conv2d、nn.Conv3d、nn.ConvTranspose2d 非线性激活函数- nn.Sigmoid、nn.Tanh、nn.ReLU、nn.LeakyReLU 池化层 - nn.MaxPool1d、nn.AveragePool2d 循环网络 - nn.LSTM、nn.GRU 归一化 - nn.BatchNorm2dDropout - nn.Dropout、nn.Dropout2d 嵌入 - nn.Embedding 损失函数 - nn.MSELoss、nn.CrossEntropyLoss、nn.NLLLoss 张量12import torcha = torch.FloatTensor(5, 7) 相同点 / 不同点第一个区别是，所有的操作在张量操作需要有后缀。例如，add在此处无用，使用add是可用的。 123456a.fill_(3.5)# 将a填充值3.5。b = a.add(4.0)# a 依然是填充3.5# 新张量b的返回值为3.5 + 4＝7.5。print(a, b) 零索引123b = a[0, 3] # 选择a中的1行4列b = a[:, 3:5] # 从a中选择所以行的4到5列x.index_add_(1, torch.LongTensor([4, 0]), z) 下一个小的区别是所有的功能现在都不是驼峰命名了。例如indexAdd现在调用index_add_ CUDA传感器在pytorch中很好并且很容易，并将CUDA张量从CPU转移到GPU将保留其基础类型。123456789# 查看电脑是否支持CUDAif torch.cuda.is_available(): # 创建一个LongTensor并且把它全部转换为3 # to GPU as torch.cuda.LongTensor a = torch.LongTensor(10).fill_(3).cuda() print(type(a)) b = a.cpu() # transfers it to CPU, back to # being a torch.LongTensor 基本类型Tensor的基本数据类型有五种： 32位浮点型：torch.FloatTensor。 (默认) 64位整型：torch.LongTensor。 32位整型：torch.IntTensor。 16位整型：torch.ShortTensor。 64位浮点型：torch.DoubleTensor。 numpy 和 tensor 之间的相互转换 使用numpy 方法将tensor 转换成 ndarray1234a = torch.randn((3, 2))# tensor转化为numpynumpy_a = a.numpy()print(numpy_a) numpy转化为Tensor12torch_a = torch.from_numpy(numpy_a)torch_a 一般情况下可以使用.cuda方法将tensor移动到gpu，这步操作需要cuda设备支持 12cpu_a=torch.rand(4, 3)cpu_a.type() 12gpu_a=cpu_a.cuda()gpu_a.type() 使用.cpu 将tensor 转换成cpu12cpu_b=gpu_a.cpu()cpu_b.type() 如果我们有多GPU的情况，可以使用to方法来确定使用那个设备，这里只做个简单的实例：123456#使用torch.cuda.is_available()来确定是否有cuda设备device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)print(device)#将tensor传送到设备gpu_b=cpu_b.to(device)gpu_b.type() 下一章介绍PyTorch的自动求导机制 变量 梯度 从0.4起, Variable 正式合并入Tensor类, 通过Variable嵌套实现的自动微分功能已经整合进入了Tensor类中。虽然为了代码的兼容性还是可以使用Variable(tensor)这种方式进行嵌套, 但是这个操作其实什么都没做。所以，以后的代码建议直接使用Tensor类进行操作，因为官方文档中已经将Variable设置成过期模块。要想通过Tensor类本身就支持了使用autograd功能，只需要设置.requries_grad=True。 Variable类中的的grad和grad_fn属性已经整合进入了Tensor类中 每个变量都有两个标志：requires_grad和volatile。它们都允许从梯度计算中精细地排除子图，并可以提高效率。 requires_grad如果有一个单一的输入操作需要梯度，它的输出也需要梯度。相反，只有所有输入都不需要梯度，输出才不需要。如果其中所有的变量都不需要梯度进行，后向计算不会在子图中执行。 这个标志特别有用，当您想要冻结部分模型时，或者您事先知道不会使用某些参数的梯度。例如，如果要对预先训练的CNN进行优化，只要切换冻结模型中的requires_grad标志就足够了，直到计算到最后一层才会保存中间缓冲区，其中的仿射变换将使用需要梯度的权重并且网络的输出也将需要它们。 volatile 纯粹的inference模式下推荐使用volatile，当你确定你甚至不会调用.backward()时。它比任何其他自动求导的设置更有效——它将使用绝对最小的内存来评估模型。volatile也决定了require_grad is False。 volatile不同于require_grad的传递。如果一个操作甚至只有有一个volatile的输入，它的输出也将是volatile。Volatility比“不需要梯度”更容易传递——只需要一个volatile的输入即可得到一个volatile的输出，相对的，需要所有的输入“不需要梯度”才能得到不需要梯度的输出。使用volatile标志，您不需要更改模型参数的任何设置来用于inference。创建一个volatile的输入就够了，这将保证不会保存中间状态。 参考官方教程 PyTorch 基础 : 神经网络包nn和优化器optm pytorch 学习笔记 pytorch 的核心主要是提供了两个主要的功能： n维tensor，类似numpy，但可以运行在GPU 上Numpy是科学计算的通用框架;它对计算图形、深度学习或梯度一无所知。Tensor张量是pytorch 中最基本的概念。PyTorch张量可以利用GPU加速其数字计算。要在GPU上运行PyTorch Tensor，请在构造Tensor时使用device参数将Tensor放置在GPU上。 自动微分，用于构建和训练神经网络使用自动微分来自动计算神经网络中的反向通过。 在搭建网络的过程中，网络中的正向传播定义为一个 computational graph计算图： 图中的节点为张量，边为从输入张量产生输出张量的函数，然后通过改图进行反向传播，可以轻松计算梯度。默认张量中的参数 require_grad=True， 那么在反向传播的时候， x.grad将是另一个张量， 它保持了x 相对于某个标量值的梯度。如果在训练神经网络的时候，比如通常不想要更新步骤中向后传播（形成计算图），那么这个时候可以使用 torch.no_grad() 上下文管理器来防止构建计算图。 pytorch 中的计算图很想 tensorflow 中的计算图，但是两者不同在于前者是动态图，后者是静态图。在tensorflow 中，如果定义了一个计算图，然后一遍遍计算相同的图，可能将不同的输入数据提供给图。在pytorch 中，每一个前向传播都定义了一个新的计算图。静态图的优势，可以预先优化图，比如说融合某些图的操作，分布式之类的。而动态图，入门比较简单，方便debug。 pytorch中常见的包： nn 定义了一组模块，包括神经网络层和有用的损失函数 optim（优化器），优化算法的思想， 比如说 adagrad， rmsprop， adam 123456# 常见的操作调用模型前向传播 y_pred = model(x)调用损失函数 loss = loss_fn(y_pred, y)梯度归零 optimizer.zero_grad()后向传播 loss.backward()调用优化器更新参数 optimizer.step() 使用 custom nn module（这种是经常用到，搭建自己的nn 网络）通过子类nn.Module并定义一个forwad输入来定义自己的模块，该前向接收输入张量并使用其他模块或在张量上的其他自动转换操作产生输出张量。 control flow and weight sharing这个权值共享在RNN 中使用比较多，但是不是很多呀，多看例子把~ 参考文献 介绍PyTorch的简单示例LEARNING PYTORCH WITH EXAMPLES]]></content>
      <tags>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Regular Expression]]></title>
    <url>%2F2019%2F01%2F22%2Fregular-expression%2F</url>
    <content type="text"><![CDATA[介绍正则表达式 正则表达式使用单个字符串来描述、匹配一系列匹配某个句法规则的字符串。换句话说，正则表达式就是记录文本规则的代码。正则表达式(Regular Expression)是一种文本模式，包括普通字符（例如，a 到 z 之间的字母）和特殊字符（称为”元字符”）。 正则表达式的工作机制123456789101112131415161718 +--------+ | 编译 | +--------+ | ↓+----------------+| 设置开始位置 |←---------++----------------+ ↑ | | ↓ 其 |+----------------+ 他 || 匹配 &amp; 回溯 | 路 |+----------------+ 径 | | | ↓ |+----------------+ || 成功 or 失败 |---------→++----------------+ 常用的元字符 代码 说明 . 匹配除换行符以外的任意字符 \w 匹配字母或数字或下划线或汉字 \s 匹配任意的空白符 \d 匹配数字 \b 匹配单词的开始或结束 ^ 匹配字符串的开始 $ 匹配字符串的结束 元字符 描述 . 句号匹配任意单个字符除了换行符. [ ] 字符种类. 匹配方括号内的任意字符. [^ ] 否定的字符种类. 匹配除了方括号里的任意字符 * 匹配&gt;=0个重复的在*号之前的字符. + 匹配&gt;=1个重复的+号前的字符. ? 标记?之前的字符为可选. {n,m} 匹配num个中括号之前的字符 (n &lt;= num &lt;= m). (xyz) 字符集, 匹配与 xyz 完全相等的字符串. &#124; 或运算符,匹配符号前或后的字符. &#92; 转义字符,用于匹配一些保留的字符 [ ] ( ) { } . * + ? ^ $ \ &#124; ^ 从开始行开始匹配. $ 从末端开始匹配. 锚点 在正则表达式中, 想要匹配指定开头或结尾的字符串就要使用到锚点. ^ 指定开头, $ 指定结尾. ^ 用来检查匹配的字符串是否在所匹配字符串的开头. 例如, 在 abc 中使用表达式 ^a 会得到结果 a. 但如果使用 ^b 将匹配不到任何结果. 因为在字符串 abc 中并不是以 b 开头. 例如, ^(T|t)he 匹配以 The 或 the 开头的字符串. 同理于 ^ 号, $ 号用来匹配字符是否是最后一个. 例如, (at\.)$ 匹配以 at. 结尾的字符串. 重复 代码 说明 * 重复零次或更多次 + 重复一次或更多次 ? 重复零次或一次 {n} 重复n次 {n,} 重复n次或更多次 {n,m} 重复n到m次 反义有时需要查找不属于某个能简单定义的字符类的字符。比如想查找除了数字以外，其它任意字符都行的情况，这时需要用到反义： 代码 说明 \W 匹配任意不是字母，数字，下划线，汉字的字符 \S 匹配任意不是空白符的字符 \D 匹配任意非数字的字符 \B 匹配不是单词开头或结束的位置 [^x] 匹配除了x以外的任意字符 [^aeiou] 匹配除了aeiou这几个字母以外的任意字符 简写字符集 正则表达式提供一些常用的字符集简写. 如下: 简写 描述 . 除换行符外的所有字符 \w 匹配所有字母数字, 等同于 [a-zA-Z0-9_] \W 匹配所有非字母数字, 即符号, 等同于: [^\w] \d 匹配数字: [0-9] \D 匹配非数字: [^\d] \s 匹配所有空格字符, 等同于: [\t\n\f\r\p{Z}] \S 匹配所有非空格字符: [^\s] \f 匹配一个换页符 \n 匹配一个换行符 \r 匹配一个回车符 \t 匹配一个制表符 \v 匹配一个垂直制表符 \p 匹配 CR/LF (等同于 \r\n)，用来匹配 DOS 行终止符 回车”（carriage return）和”换行”（line feed） 的区别 回车每行后面加两个表示结束的字符。一个叫做”回车”，告诉打字机把打印头定位在左边界；另一个叫做”换行”，告诉打字机把纸向下移一行。从英文单词上也是能get 到意思的。 ‘\n’ 10 换行（newline）‘\r’ 13 回车（return） 回车 \r 本义是光标重新回到本行开头，r的英文return，控制字符可以写成CR，即Carriage Return换行 \n 本义是光标往下一行（不一定到下一行行首），n的英文newline，控制字符可以写成LF，即Line Feed 不同操作系统下的含义： \n: UNIX 系统行末结束符\r\n: window 系统行末结束符\r: MAC OS 系统行末结束符 软回车和硬回车 硬回车就是普通我们按回车产生的，它在换行的同时也起着段落分隔的作用。 软回车是用 Shift + Enter 产生的，它换行，但是并不换段，即前后两段文字在 Word 中属于同一“段”。在应用格式时你会体会到这一点。软回车能使前后两行的行间距大幅度缩小，因为它不是段落标记，要和法定的段落标记——硬回车区别出来。硬回车的html代码是 &lt;p&gt;..&lt;/p&gt;，段落的内容就夹在里面，而软回车的代码很精悍 &lt;br&gt;。网页的文字如果复制到word中，则硬回车变为弯曲的箭头，软回车变为向下的箭头。 标志 标志也叫修饰语, 因为它可以用来修改表达式的搜索结果. 这些标志可以任意的组合使用, 它也是整个正则表达式的一部分. 标志 描述 i 忽略大小写. g 全局搜索. m 多行的: 锚点元字符 ^ $ 工作范围在每行的起始. 修饰语 i 用于忽略大小写。例如, 表达式 /The/gi 表示在全局搜索 The, 在后面的 i 将其条件修改为忽略大小写, 则变成搜索 the 和 The, g 表示全局搜索. 修饰符 g 常用语执行一个全局搜索匹配, 即(不仅仅返回第一个匹配的, 而是返回全部). 例如, 表达式 /.(at)/g 表示搜索 任意字符(除了换行) + at, 并返回全部结果. 多行修饰符 m 常用语执行一个多行匹配. 像之前介绍的 (^,$) 用于检查格式是否是在待检测字符串的开头或结尾. 但我们如果想要它在每行的开头和结尾生效, 我们需要用到多行修饰符 m. 例如, 表达式 /at(.)?$/gm 表示在待检测字符串每行的末尾搜索 at后跟一个或多个 . 的字符串, 并返回全部结果. 分枝条件 （定义了几种不同的匹配规则）不幸的是，刚才那个表达式也能匹配(010)12345678或(022-87654321这样的“不正确”的格式。要解决这个问题，我们需要用到分枝条件。正则表达式里的分枝条件指的是有几种规则，如果满足其中任意一种规则都应该当成匹配，具体方法是用|把不同的规则分隔开。听不明白？没关系，看例子： 0\d{2}-\d{8}|0\d{3}-\d{7}这个表达式能匹配两种以连字号分隔的电话号码：一种是三位区号，8位本地号(如010-12345678)，一种是4位区号，7位本地号(0376-2233445)。 (?0\d{2})?[- ]?\d{8}|0\d{2}[- ]?\d{8}这个表达式匹配3位区号的电话号码，其中区号可以用小括号括起来，也可以不用，区号与本地号间可以用连字号或空格间隔，也可以没有间隔。你可以试试用分枝条件把这个表达式扩展成也支持4位区号的。 \d{5}-\d{4}|\d{5}这个表达式用于匹配美国的邮政编码。美国邮编的规则是5位数字，或者用连字号间隔的9位数字。之所以要给出这个例子是因为它能说明一个问题：使用分枝条件时，要注意各个条件的顺序。如果你把它改成\d{5}|\d{5}-\d{4}的话，那么就只会匹配5位的邮编(以及9位邮编的前5位)。原因是匹配分枝条件时，将会从左到右地测试每个条件，如果满足了某个分枝的话，就不会去再管其它的条件了。 分组（使用index 访问使用很广泛） 我们已经提到了怎么重复单个字符（直接在字符后面加上限定符就行了）；但如果想要重复多个字符又该怎么办？你可以用小括号来指定子表达式(也叫做分组)，然后你就可以指定这个子表达式的重复次数了，你也可以对子表达式进行其它一些操作(后面会有介绍)。 (\d{1,3}.){3}\d{1,3}是一个简单的IP地址匹配表达式。要理解这个表达式，请按下列顺序分析它：\d{1,3}匹配1到3位的数字，(\d{1,3}.){3}匹配三位数字加上一个英文句号(这个整体也就是这个分组)重复3次，最后再加上一个一到三位的数字(\d{1,3})。 不幸的是，它也将匹配256.300.888.999这种不可能存在的IP地址。如果能使用算术比较的话，或许能简单地解决这个问题，但是正则表达式中并不提供关于数学的任何功能，所以只能使用冗长的分组，选择，字符类来描述一个正确的IP地址：((2[0-4]\d|25[0-5]|[01]?\d\d?).){3}(2[0-4]\d|25[0-5]|[01]?\d\d?)。 常见的例子 除了使用[]表示或逻辑,()也是可以的。用法是(a|b)表示a或者b 匹配邮箱 123gaoyaqi411@126.com dyumc@google.net sam@sjtu.edu 步骤 任何一个以words开头的，一个或更多 \w+ 紧接着是一个@符号 \w+@ 接着有一个或者更多的words \w+@\w+ 接着一个.标点 \w+@\w+. 接着一个com net 或 edu \w+@\w+.(com|net|edu) 手机号正则1/^1[34578][0-9]&#123;9&#125;$/ 单词边界1/\bis\b/ URL分组替换1/http:(\/\/.+\.jpg)/ 正则表达式使用小括号用来分组，这个时候我们可以通过用 $1来获取 group#1的内容。 获取内容是为了 replace 正则表达式由两种基本字符组成： 原义字符 非打印字符 元字符 (* + ? $ ^ . | ( ) { } [ ]) 非打印字符包括换行符、回车符号， 分页符等等 字符类取反[^]范围类 [-]预定义类 （这个是为了方便使用，在上面也被称为 简写类） 边界类 ^ 表示开头， $ 表示结尾， \b 表示单词边界, \B 表示非单词边界 量词 正则表达式默认会匹配贪婪模式，什么是贪婪模式呢？如其名尽可能多的匹配。我们看个例子与贪婪对应就是懒惰模式，懒惰对应的就是匹配的尽可能少的情况。如何开启懒惰模式？ 在量词后面加?。继续上面的例子1/\d&#123;3,6&#125;?/ 邮箱正则表达式实例 分析邮件名称部分： 26个大小写英文字母表示为a-zA-Z 数字表示为0-9 下划线表示为_ 中划线表示为- 由于名称是由若干个字母、数字、下划线和中划线组成，所以需要用到+表示多次出现 根据以上条件得出邮件名称表达式：[a-zA-Z0-9_-]+ 分析域名部分： 常用正则表达式—邮箱（Email） 分析邮件名称部分： 汉字在正则表示为 [\u4e00-\u9fa5]字母和数字表示为 A-Za-z0-9 通过分析得出邮件名称部分表达式为 [A-Za-z0-9\u4e00-\u9fa5]+ C++ regex函数有3个：regex_match、 regex_search 、regex_replace match是全文匹配，即要求整个字符串符合匹配规则。 12cout &lt;&lt; regex_match("123", regex("\\d")) &lt;&lt; endl; //结果为0cout &lt;&lt; regex_match("123", regex("\\d+")) &lt;&lt; endl; //结果为1 search是搜索匹配，即搜索字符串中存在符合规则的子字符串。 12cout &lt;&lt; regex_match(&quot;123&quot;, regex(&quot;\\d&quot;)) &lt;&lt; endl; //结果为0cout &lt;&lt; regex_search(&quot;123&quot;, regex(&quot;\\d&quot;)) &lt;&lt; endl; //结果为1 regex_search和regex_match的主要区别是：regex_match是全词匹配，而regex_search是搜索其中匹配的字符串 regex_replace是替换正则表达式匹配内容的函数replace是替换匹配，即可以将符合匹配规则的子字符串替换为其他字符串。 1234string str = &quot;Hello_2018!&quot;;regex pattern(&quot;Hello&quot;); cout &lt;&lt; regex_replace(str, pattern, &quot;&quot;) &lt;&lt; endl; //输出：_2018，将Hello替换为&quot;&quot;cout &lt;&lt; regex_replace(str, pattern, &quot;Hi&quot;) &lt;&lt; endl; //输出：Hi_2018，将Hello替换为Hi 有时我们希望能够匹配的时候忽略大小写，这时候就要用到Regex的语法选项了。 12cout &lt;&lt; regex_match("aaaAAA", regex("a*", regex::icase)) &lt;&lt; endl; //结果为1cout &lt;&lt; regex_match("aaaAAA", regex("a*")) &lt;&lt; endl; 针对python 的正则表达式的练习题 12345671、search(pattern, string, flags=0) 在一个字符串中查找匹配2、findall(pattern, string ,flags=0) 找到匹配，返回所有匹配部分的列表3、sub(pattern, repl, string , count=0, flags=0) 将字符串中匹配正则表达式的部分替换为其他值4、split(pattern, string ,maxsplit=0, flags=0) 根据匹配分割字符串，返回分隔符串组成的列表 开始总结 python 版本的正则表达式 正则表达式 (Regular Expression) 又称 RegEx, 是用来匹配字符的一种工具. 在一大串字符中寻找你需要的内容. 它常被用在很多方面, 比如网页爬虫, 文稿整理, 数据筛选等等. 现在都是比较广泛学习，需要知道这里面都是有什么，等到用的时候，再好好琢磨，因为内容还是很多的呀。 要注意的是，正则表达式并不是一个程序，而是用于处理字符串的一种模式，如果你想用它来处理字符串，就必须使用支持正则表达式的工具，比如 Linux 中的 awk, sed, grep，或者编程语言 Perl, Python, Java 等等。 这个是一个引子, 关键字 in 表示字符串的匹配关系 123456# matching stringpattern1 = "cat"pattern2 = "bird"string = "dog runs to cat"print(pattern1 in string) # Trueprint(pattern2 in string) # False 如果想要使用更加强大的功能，那么使用 re 模块， 于是就是我们今天的主角- 正则匹配。 re 模块中提供了不少有用的函数， 比如 compile 函数 match 函数 search 函数 findall 函数 finditer 函数 split 函数 sub 函数 subn 函数 re 模块的一般使用的步骤 使用 compile 函数将正则表达式的字符串形式编译为一个 Pattern 对象 通过 Pattern 对象提供的一系列方法对文本进行匹配查找，获得匹配结果（一个 Match 对象） 最后使用 Match 对象提供的属性和方法获得信息，根据需要进行其他的操作 compile 函数compile 函数用于编译正则表达式，生成一个 Pattern 对象，它的一般使用形式如下： 1re.compile(pattern[, flag]) 其中，pattern 是一个字符串形式的正则表达式，flag 是一个可选参数，表示匹配模式，比如忽略大小写，多行模式等。 match 方法 match 方法用于查找字符串的头部（也可以指定起始位置），它是一次匹配，只要找到了一个匹配的结果就返回，而不是查找所有匹配的结果。 search 方法search 方法用于查找字符串的任何位置，它也是一次匹配，只要找到了一个匹配的结果就返回，而不是查找所有匹配的结果 findall 方法上面的 match 和 search 方法都是一次匹配，只要找到了一个匹配的结果就返回。然而，在大多数时候，我们需要搜索整个字符串，获得所有匹配的结果。 finditer 方法finditer 方法的行为跟 findall 的行为类似，也是搜索整个字符串，获得所有匹配的结果。但它返回一个顺序访问每一个匹配结果（Match 对象）的迭代器。 split 方法split 方法按照能够匹配的子串将字符串分割后返回列表 sub 方法sub 方法用于替换。 匹配中文 在某些情况下，我们想匹配文本中的汉字，有一点需要注意的是，中文的 unicode 编码范围 主要在 [\u4e00-\u9fa5]，这里说主要是因为这个范围并不完整，比如没有包括全角（中文）标点，不过，在大部分情况下，应该是够用的。 123456# -*- coding: utf-8 -*-import retitle = u&apos;你好，hello，世界&apos;pattern = re.compile(ur&apos;[\u4e00-\u9fa5]+&apos;)result = pattern.findall(title)print result 注意到，我们在正则表达式前面加上了两个前缀 ur，其中 r表示使用原始字符串，u 表示是 unicode 字符串。 执行结果: 1[u&apos;\u4f60\u597d&apos;, u&apos;\u4e16\u754c&apos;] 总结 re 模块的一般使用步骤如下： 使用 compile 函数将正则表达式的字符串形式编译为一个 Pattern 对象； 通过 Pattern 对象提供的一系列方法对文本进行匹配查找，获得匹配结果（一个 Match 对象）； 最后使用 Match 对象提供的属性和方法获得信息，根据需要进行其他的操作； Python 的正则匹配默认是贪婪匹配。 总结的小抄，出处 在爬虫中的实践 使用requests 得到内容之后，然后使用 re 进行解析。12345678910import requestsimport recontent = requests.get('https://movie.douban.com/chart').text# 豆瓣电影排行榜pattern = re.compile('class="pl2".*?&lt;.*?="(.*?)".*?&gt;(.*?)&lt;span.*?&gt;(.*?)&lt;/span&gt;.*?"rating_nums"&gt;(.*?)&lt;/span&gt;.*?"pl"&gt;(.*?)&lt;/span&gt;', re.S)# compile可以在多次使用中提高效率，这里影响不大results = re.findall(pattern, content)for result in results: url, name1, name2, nums, pl = result print(url, name1.replace("/","").strip(), name2.replace("/","").strip(), nums, pl) 正则匹配帮助文档 在线正则表达式测试 在线正则测试 细说python正则表达式 ，写的非常全Python 正则表达式 re 模块 也是比较好的博客 这个是实战，写的很容易理解 https://segmentfault.com/a/1190000013075245https://blog.csdn.net/make164492212/article/details/51656638]]></content>
      <tags>
        <tag>regular_exppression</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[剑指offer-其他(1)]]></title>
    <url>%2F2019%2F01%2F17%2F%E5%89%91%E6%8C%87offer-%E5%85%B6%E4%BB%96%2F</url>
    <content type="text"><![CDATA[这是剑指offer 系列四部曲中的最后一部，因为有些算法题目类别数量太少就汇总到了”其他“, 比如位运算、正则匹配等。第一部关于字符串和数组，第二部是栈、队列、链表和树， 第三部递归、回溯和动态规划。 二进制中1的个数 输入一个整数，输出该数二进制表示中1的个数。其中负数用补码表示。 Tips：首先处理正数, bitwise and operation， 很简单。对于负数，需要转换成 正数然后进行处理，math。 123456789101112131415161718192021class Solution: def NumberOf1(self, n): # write code here if n == 0: return 0 if n &gt; 0: counts = self.number_of_positive(n) else: n = abs(n) - 1 counts = 32 - self.number_of_positive(n) return counts def number_of_positive(self, n): if n == 0: return 0 counts = 0 while n: counts += (n &amp; 1) n = n &gt;&gt; 1 return counts 这是c++ 实现。 1234567891011121314151617181920212223242526272829#include&lt;iostream&gt;using namespace std;// positiveint number_of_1(int n)&#123; int res =0; while(n) &#123; res += (n&amp;1); n =n/2; &#125; return res;&#125;int main()&#123; int n; cin &gt;&gt;n; int res =0; if(n &gt;0) res =number_of_1(n); else &#123; n =abs(n) -1; res =32 -number_of_1(n); &#125; cout &lt;&lt; res&lt;&lt;endl; return 0;&#125; 补数和补码，2 和8 对于10 就是互为补数。那么补码这个概念也是类似的，两个机器数相加等于 1000000（在二级制下非常整齐的数字）这样的树，然后两个数字就互为补数。在计算机里面，对于负数就是使用其对应的补码进行表示的。正数使用本身表示，负数使用绝对值的补码进行表示。 首先转换成一个无符号整数，如果是无符号，当使用右移操作的时候补上的是0；但是当如果是有符号整数的时候，右移操作补充的是1。这个就是两者的区别。从本质上数字并没有发生变化，但是解读的方式发生了变化。 对于正数是比较好处理的，负数的补码表示形式。如果是8位，那么使用 1位表示符号位，使用7位进行技术，那么就有$2^7 =128 $种可能性。因为包含0，所以只能是正负127。负数中，原码补码，反码转换关系参考这里 这个是我见过的最为简洁的代码了。 1234567891011121314class Solution &#123;public: int NumberOf1(int _n) &#123; unsigned int n =_n; int res =0; while(n) &#123; res += (n&amp;1); n = n&gt;&gt;1; &#125; return res; &#125;&#125;; 数值的整数次方 给定一个double类型的浮点数base和int类型的整数exponent。求base的exponent次方。 Tips: 次方使用乘法来进行累乘 1234567891011121314151617181920212223242526272829303132333435class Solution: """ 这个就是边界条件比较多而已，需要分别判断 base 和 exponent 的正负 """ def Power(self, base, exponent): # write code here if base == 0 and exponent != 0: return 0 if base != 0 and exponent == 0: return 1 flag = 1 if base &lt;= 0 and (exponent % 2 == 1): flag = -1 base = abs(base) result = 1 if exponent &gt; 0: reverse = 0 else: reverse = 1 exponent = abs(exponent) if exponent % 2 == 0: result = base * base for i in range(exponent // 2 - 1): result = result * result else: result = base * base for i in range(exponent // 2 - 1): result = result * result result = result * base if reverse: result = 1.0 / result return result * flag 这个是需要看数据的范围，如果是在 $10^7$ 之内，那么可以直接使用 $O(n)$ 的做法。需要处理一个负号的情况。 谁说 c++ 中的代码是比较长的，关键是看谁写的。 123456789101112 class Solution &#123;public: double Power(double base, int exponent) &#123; double res =1; for(int i =0; i&lt;abs(exponent); i++) res *= base; if( exponent &lt;0) res =1/res; return res; &#125;&#125;; 最小的K个数 输入n个整数，找出其中最小的K个数。例如输入4,5,1,6,2,7,3,8这8个数字，则最小的4个数字是1,2,3,4,。 Tips： 这个有点投机取巧了，使用了 “heapq” 的库函数。这个题目跟 第 K个 smallest 是有差别的，快排中的 partition 是找到了 一个数字在最后排序结果中的位置。对于有”累加“前 K个数字还是要使用常规的排序。比较好的就是堆排序。 123456789class Solution: # 想说的是既然是使用这种开源的库函数 那么就记住这种函数名字 def GetLeastNumbers_Solution(self, tinput, k): # write code here if len(tinput) &lt; k: return [] import heapq res = heapq.nsmallest(k, tinput) return res The function partition puts the numbers smaller than nums[left] to its left and then returns the new index of nums[left]. The returned index is actually telling us how small nums[left] ranks in nums. 123456789101112131415161718192021222324252627282930313233343536class Solution(object): def findKthLargest(self, nums, k): """ :type nums: List[int] :type k: int :rtype: int """ left, right = 0, len(nums) - 1 while True: pos = self.partition(nums, left, right) # 这个在排序的时候，是把大的数字放到前面，而前面是pos 是从0 开始的， # 所以这里是 k-1 if pos == k - 1: return nums[pos] # 左边的并不足以构成k 个， 那么在右边 elif pos &lt; k - 1: left = pos + 1 else: right = pos - 1 def partition(self, nums, left, right): # choose nums[left] as pivot pivot = nums[left] # p1, p2就类似 working 中的left right p1, p2 = left + 1, right while p1 &lt;= p2: if nums[p1] &lt; pivot and nums[p2] &gt; pivot: nums[p1], nums[p2] = nums[p2], nums[p1] p1, p2 = p1 + 1, p2 - 1 elif nums[p1] &gt;= pivot: p1 += 1 else: #nums[p2] &lt;= pivot: p2 -=1 nums[left], nums[p2] = nums[p2], nums[left] return p2 有两种解法，第一种思路是快排的思路，先是找到第 $K$大，然后 $O(n)$ 的时间，遍历一遍，保留 $K $个最大的元素。 这个是快速查找（quick_search） 而不是quick_sort 所以是不会修改数组的有序与否。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455#include&lt;iostream&gt;#include&lt;vector&gt;using namespace std;int partition(vector&lt;int&gt; arr, int l, int r)&#123; int key =arr[r]; while(l&lt; r) &#123; while(l &lt;r &amp;&amp; arr[l] &gt;= key) l +=1; arr[r] =arr[l]; while(l &lt;r &amp;&amp; arr[r] &lt;= key) r -=1; arr[l] =arr[r]; &#125; arr[r] =key; return l;&#125;int find(vector&lt;int&gt; arr, int k)&#123; if(arr.empty() || arr.size() &lt; k) return -1; int l =0, r =arr.size() -1; while( true) &#123; int pos =partition(arr, l, r); if( pos == k-1) return arr[k]; else if (pos &gt; k-1) r= pos -1; else l =pos +1; &#125; return -1;&#125;int main()&#123; vector&lt;int&gt; arr; int n, k; cin &gt;&gt;n &gt;&gt;k; for(int i =0; i&lt;n; i++) &#123; int tmp ; cin &gt;&gt; tmp; arr.push_back(tmp); &#125; int key = find(arr, k); cout &lt;&lt; key&lt;&lt; endl; vector&lt;int&gt; res ; for(auto a: arr) &#123; if( a&gt; key) res.push_back(a); &#125; for(auto u: res) cout &lt;&lt; u&lt;&lt; " "; cout&lt;&lt; endl; &#125; 第二种思路使用堆的思想。使用大根堆存储最小的k 个数，堆顶元素就是所有元素中最大的，那么如果弹出，就一定弹出堆顶的元素。 使用大根堆实现，时间复杂度是 $nlog(k)$， k是个数，n 是数组的长度。大根堆保存是k 个元素，其中堆顶是这k 个元素中最大的那个。(卧槽，当时京东面试的时候，确实是可以使用大根堆进行实现的，现在有点想要哭，哈哈哈哈哈哈哈，) stack只需要标记top下标，队列通过front和rear来标记队首和队尾。 .queue也是线性表，可以用数组(由于假溢出的原因使用循环数组)和链表来实现 使用c++ 中的大根堆的思想（求解最小的k 个数字使用 大根堆） 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849#include&lt;iostream&gt;#include&lt;queue&gt;#include&lt;string&gt;using namespace std;// 寻找的是 第k 小vector&lt;int&gt; find_kth_largest(vector&lt;int&gt; &amp;arr, int k)&#123; if(arr.empty() || arr.size() &lt; k) return vector&lt;int&gt;(-1); priority_queue&lt;int&gt; q; for(auto u: arr) &#123; q.push(u); if(q.size() &gt;k) q.pop(); &#125; vector&lt;int&gt; res; while( k--) &#123; res.push_back(q.top()); q.pop(); &#125; return res;&#125;int main()&#123; vector&lt;int&gt; arr; int n, k; cin &gt;&gt;n &gt;&gt;k; for(int i =0; i&lt;n; i++) &#123; int tmp; cin &gt;&gt; tmp; arr.push_back(tmp); &#125; vector&lt;int&gt; res =find_kth_largest(arr, k); for(auto u : res) cout &lt;&lt; u&lt;&lt; " "; cout&lt;&lt; endl; cout&lt;&lt;"输出数字" &lt;&lt; endl; for(auto u: arr) &#123; cout &lt;&lt; u&lt;&lt; " "; &#125; return 0;&#125; 整数中1出现的次数（从1到n整数中1出现的次数）(过) 求出1~13的整数中1出现的次数,并算出100~1300的整数中1出现的次数？为此他特别数了一下1~13中包含1的数字有1、10、11、12、13因此共出现6次,但是对于后面问题他就没辙了。ACMer希望你们帮帮他,并把问题更加普遍化,可以很快的求出任意非负整数区间中1出现的次数（从1 到 n 中1出现的次数）。 Tips：math, 计数原理，按位统计该位为1时可能包含的数字总数.由低位向高位依次遍历数字n的每一位curn。记当前位数为c，curn左边（高位）的数字片段为highn，cur右边（低位）的数字片段为lown，lowc = 10 ^ c 若curn = 0，则高位范围为0 ~ highn - 1，低位0 ~ lowc - 1 若curn = 1，则高位范围为0 ~ highn - 1，低位0 ~ lowc - 1；或者 高位为highn， 低位0 ~ lown 若curn ＞ 1，则高位范围为0 ~ highn， 低位为0 ~ lowc - 1 一个小的例子 N= abcde,分别是各个位数上的数字。如果要统计 百位上出现 1的次数，那么他将受到三个因素的影响： 百位上的数字，百位以下的数字(低位) 和百位以上的数字( 高位)。 如果百位上的数字是 0 ， 高位数字 * 当前的数字 如果百位上的数字是1，高位数字 * 当前数字 + ( 低位数字+1) 如果百位数字大于1 (2-9), （高位数字+1 )* 当前的数字 初级版本: 通过不断的求余 和整除，计算每个位置上 1的个数。 12345678910111213141516171819int Count1InInteger(int n)&#123; int counts =0; while (n!=0) &#123; counts += (n%10) ==1? 1:0; n/=10; &#125; return counts;&#125;int main()&#123; int total =0; for (int i=0; i&lt;N; i++) &#123; total += Count1InInteger(i) &#125; return total;&#125; 进阶版本：python 版本，思路见上面分析。 1234567891011121314151617181920class Solution: # 数字的基本结构分成 weights +working_num + n%base 这三个部分 # 然后一个while 循环是处理一个数字 def NumberOf1Between1AndN_Solution(self, n): # write code here if n &lt; 1: return 0 num = n counts = 0 base = 1 while num: cur = num % 10 num = num // 10 #高位数字 counts += base * num if cur == 1: counts += (n % base) + 1 elif cur &gt; 1: counts += base base *= 10 return counts 最优的时间复杂度是 $O(log N)$。下面的代码的时间复杂度是 $ O(log^2(N))$. c++ 解法12345678910111213141516171819202122232425262728class Solution &#123;public: // 时间复杂度是 log^2(N) // 经过讲解之后，只要得到 left right 这样的数字那么后面就比较好说了 int NumberOf1Between1AndN_Solution(int n) &#123; if(!n) return 0; vector&lt;int&gt; number; // 把每一个数字单独的保存起来 // 很好的小的代码 // int 类型的数字，转换成 int 数组类型 while(n) number.push_back(n%10), n /=10; int res =0; for(int i =number.size() -1; i&gt;=0; i--) &#123; // t 是什么含义 auto left =0, right =0, t =1; for(int j =number.size() -1; j&gt;i; j--) left =left*10 +number[j]; for(int j =i-1; j&gt;=0; j--) right =right *10+number[j], t *=10; res += left *t; if(number[i] ==1) res += right +1; else if(number[i] &gt;1) res += t; &#125; return res; &#125;&#125;; 把数组排成最小的数 （复习到这里了） 输入一个正整数数组，把数组里所有数字拼接起来排成一个数，打印能拼接出的所有数字中最小的一个。例如输入数组{3，32，321}，则打印出这三个数字能排成的最小数字为321323。 Tips：使用sorted() 函数， string 类型的排序 和 int 类型的排序是一样的，在python 里面来说。 1234567class Solution: def PrintMinNumber(self, numbers): # write code here sorted_list = sorted(numbers, cmp=lambda a, b: cmp(str(a) + str(b), str(b) + str(a))) # 这个时候已经排好序，然后只要一个个连接起来就行了 return ''.join(map(str, sorted_list)) 这种方式是及其好用的。 12list =["abc", "ad"]print("".join(list)) 凡是能够推出来的，都是比较简单的；凡是需要猜出来的，都是比较难的。对于这道题目，需要先定义一种排序方式，然后再获得最小的数字。主要是比较函数。 12345678910111213141516171819202122232425262728293031#include&lt;iostream&gt;#include&lt;string&gt;#include&lt;vector&gt;using namespace std;// 拼接出所有数字中最小的一个bool cmp(int a, int b)&#123; string sa =to_string(a), sb =to_string(b); return sa +sb&lt; sb+sa;&#125;int main()&#123; vector&lt;int&gt; arr; int n ; cin &gt;&gt;n; for(int i =0; i&lt; n; i++) &#123; int tmp; cin &gt;&gt;tmp; arr.push_back(tmp); &#125; sort(arr.begin(), arr.end(), cmp); string res; for(auto u: arr) &#123; res += to_string(u); &#125; cout &lt;&lt; res&lt;&lt; endl; return 0;&#125; 丑数 把只包含质因子2、3和5的数称作丑数（Ugly Number）。例如6、8都是丑数，但14不是，因为它包含质因子7。 习惯上我们把1当做是第一个丑数。求按从小到大的顺序的第N个丑数。 Tips：之后的丑数肯定是2，3或5 的倍数，分别单独计数，然后选择最小的。 注意两个版本再实现的时候稍微有一点不同。 12345678910111213class Solution(object): def getUglyNumber(self, n): index =n if index &lt; 1: return 0 res = [1] t2,t3,t5 = 0,0,0 while len(res) &lt; index: minNum = (min(res[t2]*2, res[t3]*3, res[t5]*5)) if minNum &gt; res[-1]: res.append(minNum) if (minNum == res[t2]*2): t2 += 1 elif (minNum == res[t3]*3): t3 += 1 else: t5 += 1 return res[-1] 使用c++ 实现。是需要有 $O(N)$ 的空间的， 12345678910111213141516171819202122#include&lt;iostream&gt;#include&lt;string&gt;#include&lt;vector&gt;using namespace std;// 这个是比较有技巧的，使用 ijk 三个指针int main()&#123; int n; vector&lt;int&gt; res(1,1); cin &gt;&gt;n; int i =0, j =0, k=0; while (--n) &#123; int min_v =min(res[i] *2, min(res[j] *3, res[k] *5 )); res.push_back(min_v); if(min_v == res[i] *2) i +=1; if( min_v == res[j] *3 ) j+=1; if(min_v ==res[k] *5) k +=1; &#125; cout &lt;&lt; res.back()&lt;&lt; endl; return 0;&#125; 正则表达式匹配 请实现一个函数用来匹配包括’.’和’‘的正则表达式。模式中的字符’.’表示任意一个字符，而’‘表示它前面的字符可以出现任意次（包含0次）。 在本题中，匹配是指字符串的所有字符匹配整个模式。例如，字符串”aaa”与模式”a.a”和”abaca”匹配，但是与”aa.a”和”ab*a”均不匹配 Tips: dp 问题。转换方程 dp[i][j] i 表示 string 的index j表示 pattern 的index， dp[i][j] ==dp[i-1][j-1] or dp[i][j] =dp[i][j-2] or dp[i][j-1] 。 123456789101112131415161718192021222324class Solution: # s, pattern都是字符串 # https://www.youtube.com/watch?v=l3hda49XcDE 心中一定要有这个表格, a[i][j] 这个更像是一种指针 def match(self, s, pattern): if len(s) == 0 and len(pattern) == 0: return True dp = [[False for _ in range(len(pattern) + 1)] for _ in range(len(s) + 1)] dp[0][0] = True for j in range(1, len(pattern) + 1): if pattern[j - 1] == "*": dp[0][j] = dp[0][j - 2] for i in range(1, len(s) + 1): for j in range(1, len(pattern) + 1): if pattern[j - 1] == s[i - 1] or pattern[j - 1] == ".": dp[i][j] = dp[i - 1][j - 1] elif pattern[j - 1] == "*": dp[i][j] = dp[i][j - 2] if s[i - 1] == pattern[j - 2] or pattern[j - 2] == ".": dp[i][j] = dp[i][j] or dp[i - 1][j] else: dp[i][j] = False return dp[len(s)][len(pattern)] 视频中讲解dp 使用的dfs 的思想，但是我更加喜欢使用方格的思想。所以这个是可以暂时的过去。 数据流中的中位数 如何得到一个数据流中的中位数？如果从数据流中读出奇数个数值，那么中位数就是所有数值排序之后位于中间的数值。如果从数据流中读出偶数个数值，那么中位数就是所有数值排序之后中间两个数的平均值。我们使用Insert()方法读取数据流，使用GetMedian()方法获取当前读取数据的中位数。 Tips：主要是体现了数据流，要求能够 insert 元素，然后基于当前的状态去 getmedian() ，是动态的，而不是静态的。 1234567891011121314151617181920class Solution: """ 对于数据流 这个应该是第二次接触了，需要使用一个全局变量 """ # 虽然知道这个使用 堆的思想是更优的，搜索时间可以O（1）， 堆的调整是 O(log n) # 但是没有什么很好的教程，所以我也没有学会啊 def __init__(self): self.list1 = [] def Insert(self, num): self.list1.append(num) def GetMedian(self, ch): length = len(self.list1) # 我记得有一个更加快一些 self.list1 = sorted(self.list1) if length % 2 == 0: return (self.list1[length // 2] + self.list1[length // 2 - 1]) / 2.0 else: return self.list1[length // 2] 使用大根堆和小根堆实现的功能是非常的精妙呀，好好敲代码，好好学习、 123456789101112131415161718192021222324252627282930313233343536class Solution &#123;public: // 大根堆在下面，小根堆在上面 // 小根堆存储大的数字，堆顶是小的元素，大根堆存储小的数字，堆顶是大的元素。 // 插入的时候优先往小根堆插入，如果发现两堆相差大于1，那么调整两个堆 // 总数是奇数，那么返回小根堆的堆顶；总数是偶数，返回两个堆的堆顶的均值 // 两个堆 如何进行维护呢？ 交换 if 逆序； 转移 if 小根堆比较多 priority_queue&lt;int&gt; max_heap; // priority_queue&lt;int, vector&lt;int&gt;, greater&lt;int&gt;&gt; min_heap; void Insert(int num) &#123; max_heap.push(num); // 如果是逆序 if(min_heap.size() &amp;&amp; max_heap.top() &gt; min_heap.top() ) &#123; auto maxv =max_heap.top(), minv= min_heap.top(); max_heap.pop(), min_heap.pop(); max_heap.push(minv),min_heap.push(maxv); &#125; // 如果下面多 if(max_heap.size() &gt; min_heap.size() +1) &#123; min_heap.push(max_heap.top()); max_heap.pop(); &#125; &#125; double GetMedian() &#123; if(max_heap.size() + min_heap.size() &amp;1) return max_heap.top(); return (max_heap.top() + min_heap.top())/2.0; &#125; &#125;; 这个是单机版本的。 1234567891011121314151617181920212223242526272829303132#include&lt;iostream&gt;#include&lt;string&gt;#include&lt;queue&gt;using namespace std;priority_queue&lt;int&gt; max_h; // heap 堆priority_queue&lt;int, vector&lt;int&gt;, greater&lt;int&gt;&gt; min_h;void insert(int val)&#123; max_h.push(val); // 处理逆序 if(min_h.size() &amp;&amp; min_h.top() &lt; max_h.top()) &#123; int minv =min_h.top(), maxv =max_h.top(); min_h.pop(), max_h.pop(); min_h.push(maxv), max_h.push(minv); &#125; // 处理 diff &gt; 1 的情况 if(max_h.size() &gt; min_h.size() +1) &#123; min_h.push(max_h.top()); max_h.pop(); &#125;&#125;double get_median()&#123; if(max_h.size() + min_h.size() &amp;1) return max_h.top(); return (max_h.top() +min_h.top())/2.0;&#125;int main()&#123; return 0;&#125; 滑动窗口的最大值 给定一个数组和滑动窗口的大小，找出所有滑动窗口里数值的最大值。例如，如果输入数组{2,3,4,2,6,2,5,1}及滑动窗口的大小3，那么一共存在6个滑动窗口，他们的最大值分别为{4,4,6,6,6,5}； 针对数组{2,3,4,2,6,2,5,1}的滑动窗口有以下6个： {[2,3,4],2,6,2,5,1}， {2,[3,4,2],6,2,5,1}， {2,3,[4,2,6],2,5,1}， {2,3,4,[2,6,2],5,1}， {2,3,4,2,[6,2,5],1}， {2,3,4,2,6,[2,5,1]}。 Tips：使用max(list1) 这样的操作是可行的。 123456789class Solution: # 最简单的模拟滑动窗口 的过程 def maxInWindows(self, num, size): slip = [] if not num or len(num) &lt; size or size == 0: return [] for i in range(len(num) - size + 1): slip.append(max(num[i:i + size])) return slip c++ 中的deque 是双向队列。 这个在牛客网上通过的样例是 12.5%，估计是爆了内存之类吧。 1234567891011121314151617class Solution &#123;public: // 单调队列问题，单调在于队首到队尾是递减的。使用双向队列在于要支持队尾删除元素 // 使用队列是维持一个滑动窗口的概念， vector&lt;int&gt; maxInWindows(vector&lt;int&gt;&amp; nums, int k) &#123; deque&lt;int &gt; q; vector&lt;int&gt; res; for(int i =0; i&lt; nums.size() ; i++) &#123; while( q.size() &amp;&amp; q.front() &lt;= i -k) q.pop_front();// i 表示当前的遍历的index，q.front() 不可能比这个大 while(q.size() &amp;&amp; nums[q.back()] &lt;= nums[i]) q.pop_back(); q.push_back(i); if(i &gt;=k -1) res.push_back(nums[q.front()]); &#125; return res; &#125;&#125;; 求解连续子数组的最大和 c++ 代码 1234567891011121314151617181920212223242526272829303132 #include&lt;iostream&gt;#include&lt;vector&gt;using namespace std;// 整数数组中元素有正有负，找出一个连续的子数组，要求连续子数组中各元素和和最大。int maxSub(vector&lt;int&gt; arr, int n)&#123; int now =0; int max_v =INT_MIN; for(auto u: arr) &#123; now += u; if (now &lt;0) now =0; // 注意这里是求解最大的和 max_v =max(now, max_v); &#125; return max_v;&#125;int main()&#123; vector&lt;int&gt; arr; int n ; cin &gt;&gt;n; while(n --) &#123; int tmp; cin &gt;&gt;tmp; arr.push_back(tmp); &#125; int res =maxSub(arr, n); cout &lt;&lt; res&lt;&lt; endl; return 0;&#125; python 代码 12345678910111213141516 # 时间复杂度是 O(n), 空间是 O(1)def largestSub(arr): if not arr or len(arr) ==0: return now =0 max_v =arr[0] for i in range(len(arr)): now += arr[i] if(now &lt;0) : now =0 max_v =max(now, max_v) return max_v arr =[2, 4, -7, 5, 2, -1, 2, -4, 3]print(largestSub(arr))]]></content>
      <categories>
        <category>算法</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Python from Beginner to Master]]></title>
    <url>%2F2019%2F01%2F09%2Fpython-for-beginners%2F</url>
    <content type="text"><![CDATA[Basic Skillsmodulepython 文件可以当做主文件进行运行或者当做函数的集合进行调用。如果是前者一般是需要包含”__name__ ==”__main__”。对于后者就是在其他的python文件中进行调用。 12import my_module # python文件from my_module import my_object packagesfrom packageroot.packagefolder.mod import my_object Note: Ensure each directory within your package import contains a file __init__.py python-pathpython2 和python3 使用不同的解释器，导致在一些函数命名和计算上有一些差别，最好在文件的开头标明使用的解释器。 while or forwhile : provide a condition and run the loop until the condition is not met. for: loop for a number of specific times; loop over items or characters of a string. examples: 1234[Variable] AggregateFunction([Value] for [item] in [collection])x =[1, 2,3, 4, 5]y =[ 2*a for a in x if a%2 ==0]y &gt;&gt; [4, 8] 或者可以使用这样更加简洁的语句： 123lambda arguments : expressionfun1 = lambda a,b,c : a+b+cprint(fun1(5,6,2)) 来个比较复杂的例子: 12345nums =[1,2,3,4,5]letters =['a', 'b', 'c','d','e']# 两个for 循环也是要熟练nums_letters =[[n, l] for n in nums for l in letters ]nums_letters break, continue, or passThe break, continue, and pass statements in Python will allow you to use for loops and while loops more effectively in your code.12345678910number = 0for number in range(10): number = number + 1 if number == 5: pass # pass here print('Number is ' + str(number))print('Out of loop') pass 关键词的作用： 空语句 do nothing 保证格式完整 保证语义完整 简单来说就是占位，没有什么实质性的内容。 The pass statement occurring after the if conditional statement is telling the program to continue to run the loop and ignore the fact that the variable number evaluates as equivalent to 5 during one of its iterations. yield 可以用用作新的 if的测试, return results without termination The pass statement can create minimal classes, or act as a placeholder when working on new code and thinking on an algorithmic level before hammering out details.pass 的存在就是占坑，否则这个地方就是报错（IndentationError）。用于想要扩展的地方，但是现在还没有扩展。比如在某个method 下面或者某个 if 条件下。 yield or returnyield 经常被用来作为生成器，返回但是却没有解决函数，下一回从上一回的位置开始。 when you call a normal function with a return statement the function is terminated whenever it encounters a return statement. In a function with a yield statement the state of the function is ‘saved’ from the last call and can be picked up the next time you call a generator function. for examples12345678910111213141516171819gen_exp =(x **2 for x in range(10) if x %2 ==0)for x in gen_exp: print(x)def my_gen(): for x in range(5): yield xgen1 =my_gen()next(gen1)def my_generator1(): yield 1 yield 2 yield 3 my_gen =my_generator1()# 使用 next() 进行调用下一个next(my_gen) recursionA function calling itself is known as recursion.（ recursion 是递归， iteration 是循环） list, tuples, or dictionary在python 中是使用频繁的data structure，这个是属于 collection 类别，里面放的是element. list: to add/update/ delete an item of a collection 123456my_list.append('C') #adds at the endmy_list[1] = 'D' #updatemy_list.pop(1) # removesmylist.pop() # 默认就是类似 栈的结构，就是pop 出来最后一个mylist.pop(0) # 当然也可以根据index 指定特定的 pop(delete) 的element 对于index 的访问，一定要保证有相应的size() 长度，然后再进行index 的访问 12del mylist[1:2] # 通过指定 index range 然后进行delmylist.sort() # 支持 sorting 然后是从小到大, 这个sort是一种操作，inplace 的操作 tuples: tuples store a sequence of objects, the object can be of any type. Tuples are faster than lists. dictionary: It stores key/value pair objects. 123456789my_dict =dict()my_dict['key'] ='value'or my_dict =&#123;'key': 'value', ...&#125;for key in my_dict:# do somethingif 'some key' in my_dict:# do something Iterators1234567891011121314151617class yrange: def __init__(self, n): self.i = 0 self.n = n # 这个表明是一个 iterator，make an object iterable def __iter__(self): return self # 这个next 函数就被当做是 class的属性，可以被外部调用的， def next(self): if self.i &lt; self.n: i = self.i self.i += 1 return i else: raise StopIteration() shallow vs deep copypython3 中：对于简单的数据类型，像int ，string，这种 copy() 和copy.deepcopy() 这两者都是相同的，copy 都是一种映射，都是相当于”值“ 上的引用； 12345aa =2bb =aaprint(id(aa), id(bb)) # 相同bb =3print(id(aa), id(bb)) # 不同，因为把3 这个值重新复制给了变量bb 对于复杂的数据类型，使用deepcopy() 的时候，本来就是会重新拷贝一份到内存中。在python3 中copy() 和deepcopy() 这个是没有什么区别的。12345list1 =['a', 'b']list2 =list1 # 这个是引用，所以和list1 是相同的list3 =copy.copy(list1) # 这个id 和list1 不同list4 =copy.deepcopy(list1)# 这个id 和list1 不同 print(id(list1), id(list2), id(list3), id(list4)) 在python 中如何返回值有多个，可以使用元组。虽然是可以使用全局变量，但是在真正的工作中是不推荐这样做的，因为在多线程环境下，这种方式并不是一种 线程安全的方法。 关于 传值还是传引用？ 结论：python不允许程序员选择采用传值还是传引用。Python参数传递采用的肯定是“传对象引用”的方式。这种方式相当于传值和传引用的一种综合。如果函数收到的是一个可变对象（比如字典或者列表）的引用，就能修改对象的原始值－－相当于通过“传引用”来传递对象。如果函数收到的是一个不可变对象（比如数字、字符或者元组）的引用，就不能直接修改原始对象－－相当于通过“传值’来传递对象。 细说： 对于不可变的对象，原始的对象因为不能改变，于是被“抛弃”，实际上是新new 了一个数值，所以操作的不是原来主函数中的值。 python 中的基本数据类型有六种：数字，字符串，列表，字典，元组和集合，(Number, String, List, Dict, Tuple, Set) 数字包括整形，长整型，浮点型和复数，(Int, Long, Float, Complex)布尔值 True 和 False集合包含集合和不可变集合 (set, frozenset)字符串里包括 字符串，字节串和万国码 (str, bytes, unicode)列表包括 列表和字节数组 (list, bytearray)有的时候分四种，就是数字，序列，集合和字典。序列里包括字符串和列表和元组。有的时候分五种，就是数字，字符串，列表，元组和字典。列表里包括元组。其中不可变的类型有数字，字符串，布尔值，元组，和 frozenset， 可变类型有列表，字典，和集合。 可变和不可变的区分是该对象是否存在 add 或者 append 等方法来增加对象元素，使得对象在不改变自身 ID 的情况改变内容。 object oriented design1234567class ParentClass: def my_function(self): print 'I am here'class SubClass1(ParentClass): class SubClass2(ParentClass): 对于多继承的支持 （接口） 1class A(B,C): #A implments B and C 如果想要call parent class function then you can dp: 1super(A, self).funcion_name() garbage collectionall the objects in python are stored in a heap (堆) space. Python has an built-in garbage collection mechanism. Memory management in Python involves a private heap containing all Python objects and data structures. The management of this private heap is ensured internally by the Python memory manager. In computer science, a heap is a specialized tree-based data structure which is essentially an almost complete[1] tree that satisfies the heap property: if P is a parent node of C, then the key(the value) of P is either greater than or equal to (in a max heap) or less than or equal to (in a min heap) the key of C. try…catch123456789101112131415161718# raise exceptionstry: raise TyeErrorexcept: print('exception')# catching exceptionstry: do_something()except: print('exception')# try/ catch /finallytry: do_something()except TypeError: print('exception')finally: close_connections() 内置函数中的 all() 和 any()这两个函数的参数都是iterable，也就是为list或者tuple.all() 函数: “全‘真’为True，有‘假’为False” ; 当iterable为空的时候，函数返回值为Trueany() “全‘假’为False，有‘真’为True”. 当iterable为空的时候，函数返回值为False 给出all () 函数的一个简单的例子 123456789101112131415161718&gt;&gt;&gt; all(['a', 'b', 'c', 'd']) # 列表list，元素都不为空或0True&gt;&gt;&gt; all(['a', 'b', '', 'd']) # 列表list，存在一个为空的元素False&gt;&gt;&gt; all([0, 1，2, 3]) # 列表list，存在一个为0的元素False &gt;&gt;&gt; all(('a', 'b', 'c', 'd')) # 元组tuple，元素都不为空或0True&gt;&gt;&gt; all(('a', 'b', '', 'd')) # 元组tuple，存在一个为空的元素False&gt;&gt;&gt; all((0, 1, 2, 3)) # 元组tuple，存在一个为0的元素False &gt;&gt;&gt; all([]) # 空列表True&gt;&gt;&gt; all(()) # 空元组True 总结：all()：”有‘假’为False，全‘真’为True，iterable为空是True”any()：”有‘真’为True，全‘假’为False，iterable为空是False” sort() sorted() 函数在python3 中 sorted() 取消了对cmp 的支持, 所以只能使用 sort() 函数， 比如下面语句，表示先是按照第一个元素进行升序，然后在第一个元素相同的条件下，按照第二个元素进行降序。灰常nice的一种写法。12# type envelopes: List[List[int]]envelopes.sort(key =lambda x:(x[0], -x[1])) list.sort( ) 是in-place 操作， 在python2 中 sorted() 是一种有返回排序好的数组的操作。 python operatorsPython Arithmetic Operator Addition(+) Subtraction(-) Multiplication(*) Division(/) Exponentiation(**) Floor Division(//) 向下取整 Modulus(%) Python Relational Operator Less than(&lt;) Greater than(&gt;) Less than or equal to(&lt;=) Greater than or equal to(&gt;=) Equal to(= =) Not equal to(!=) Python Assignment Operator（python 中没有 ++ 这个符号，这个是c++ 中的符号） Assign(=) Add and Assign(+=) Subtract and Assign(-=) Divide and Assign(/=) Divide and Assign(/=) Modulus and Assign(%=) Exponent and Assign(**=) Floor-Divide and Assign(//=) Python Logical Operator(会有某种机制简化运算，比如 condition1 or condition2 ，如果condition1 是正确的，那么最后的结果就是正确的。)用于逻辑判断 and or not Python Membership Operator in ,not in Python Identity Operator is, is not , Python Bitwise Operator (这其中的 | 表现的是一种二级制’ 和’的 关系，如果在二进制下，0 | 1 那么就是1 ) 属于集合操作。 Binary AND(&amp;) Binary OR(|) Binary XOR(^) Binary XOR(^) Binary Left-Shift(&lt;&lt;) Binary Right-Shift(&gt;&gt;) Working with files Working with CSV, Json and XML Over the years, the list of possible formats that you can store your data in has grown significantly. But, there are 3 that dominate in their everyday usage: CSV, JSON, and XML. In this article, I’m going to share with you the easiest ways to work with these 3 popular data formats in Python! 有两种方式去读写 csv file：一种是 pd.read_csv() ，一种是built-in 的library 中的库函数之前一直使用的pd.read_csv(), 现在才发现python 有built-in 的library。We can do both read and write of a CSV using the built-in Python csv library. Usually, we’ll read the data into a list of lists. python built-in function.12345678910111213141516171819import csv filename = "my_data.csv"fields = [] rows = [] with open(filename, 'r') as csvfile: csvreader = csv.reader(csvfile) # 如果单单是这个for，那么内存是消耗比较大的 # fields = csvreader.next() for row in csvreader: rows.append(row)for row in rows[:5]: print(row)# Writing to csv file with open(filename, 'w+') as csvfile: csvwriter = csv.writer(csvfile) csvwriter.writerow(fields) csvwriter.writerows(rows) 12345678910111213141516171819202122import pandas as pdfrom dicttoxml import dicttoxmlimport json# Building our dataframedata = &#123;'Name': ['Emily', 'Katie', 'John', 'Mike'], 'Goals': [12, 8, 16, 3], 'Assists': [18, 24, 9, 14], 'Shots': [112, 96, 101, 82] &#125;df = pd.DataFrame(data, columns=data.keys())# Converting the dataframe to a dictionary# Then save it to filedata_dict = df.to_dict(orient="records")with open('output.json', "w+") as f: json.dump(data_dict, f, indent=4)# Converting the dataframe to XML# Then save it to filexml_data = dicttoxml(data_dict).decode()with open("output.xml", "w+") as f: f.write(xml_data) 12345678910111213141516171819import jsonimport pandas as pd# Read the data from file# We now have a Python dictionarywith open('data.json') as f: data_listofdict = json.load(f) # We can do the same thing with pandasdata_df = pd.read_json('data.json', orient='records')# We can write a dictionary to JSON like so# Use 'indent' and 'sort_keys' to make the JSON# file look nicewith open('new_data.json', 'w+') as json_file: json.dump(data_listofdict, json_file, indent=4, sort_keys=True)# And again the same thing with pandasexport = data_df.to_json('new_data.json', orient='records') 处理windows 和linux 中文件分割符不兼容的情况 python 中 open( mode =’rt’) 的选项：w,r,wt,rt都是python里面文件操作的模式。w是写模式，r是读模式。t是windows平台特有的所谓text mode(文本模式）,区别在于会自动识别windows平台的换行符。类Unix平台的换行符是\n，而windows平台用的是\r\n两个ASCII字符来表示换行，python内部采用的是\n来表示换行符。rt模式下，python在读取文本时会自动把\r\n转换成\n.wt模式下，Python写文件时会用\r\n来表示换行。 参考资料：https://towardsdatascience.com/the-easy-way-to-work-with-csv-json-and-xml-in-python-5056f9325ca9 输入和输出python2 有 raw_input() 和 input() 函数，前者把所有的接收值当做string，如果想要用int，那么需要自己进行转换。input() 如果得到int，那么就是int，string 类型就是string 类型。python3 中只有input() 函数，所有的接收都是 string，需要自己进行转换。 python3 中的input() 函数就是python2 中的raw_input() 函数。下面的代码使用 python3 实现。 1234567891011121314151617181920212223242526272829if __name__ =="__main__": # 输入输出 n*n 的arrar """ n =int(input()) arr =[[0]*n] *n # 在python3 中需要 exactly 的注意这种分割，输入的时候要及其的小心 for i in range(n): arr[i] = input().split(" ") arr[i] =[int(a) for a in arr[i]] # 转成 int 类型 print(arr) """ # 输入和输出 n*m 的数组 n =int(input()) m =int(input()) arr =[[0]*m]*n for i in range(n): arr[i] =input().split(" ") arr[i] =[int(a) for a in arr[i]] print(arr) 处理输入和输出问题，就是使用 python3 处理，然后注释使用 list(map(str, [])) 和 list(map(int, [])) 这种处理手段。涨涨记性吧 12345678910111213141516171819202122#python 中传递的是值 还是引用def quick_sort(nums, l, r): if l&gt;= r: return left, right =l, r key =nums[l] while(l&lt;r): while(l &lt;r and nums[r] &gt;= key): r -=1 nums[l] =nums[r] while(l &lt;r and nums[l] &lt;= key): l +=1 nums[r] =nums[l] nums[l] =key quick_sort(nums, left, l -1) quick_sort(nums, l +1, right)if __name__ =="__main__": n =int(input()) nums =list(map(int, input().split())) quick_sort(nums, 0, len(nums)-1) #print(nums) print(" ".join(list(map(str, nums)))) 面向对象 (for python)访问控制有三种级别：私有、受保护、公有。私有。（Private）：只有类自身可以访问 受保护。（Protected）：只有类自身和子类可以访问 公有。（Public）：任何类都可以访问。 公有（Public） 在Python的类中，默认情况下定义的属性都是公有的。 12345678910class Foo(object): bar = 123 def __init__(self, bob): self.bob = bobprint(Foo.bar) # 123foo = Foo(456)print(foo.bob) # 456 上面类Foo中的bar属性就是类属性，init方法中定义的bob是实例属性，bar和bob都是公有的属性，外部可以访问，分别print类中的bar和实例中的bob，输出了对应的值。 受保护（Protected） 在Python中定义一个受保护的属性，只需要在其名字前加一个下划线_，我们将Foo方法中的bob和bar改为_bob和_bar，他们就变成了受保护的属性了，代码如下： 123456789101112131415161718192021class Foo(object): _bar = 123 def __init__(self, bob): self._bob = bobclass Son(Foo): def print_bob(self): print(self._bob) @classmethod def print_bar(cls): print(cls._bar)Son.print_bar() # 123son = Son(456)son.print_bob() # 456 定义一个类Son继承自Foo，由于受保护的对象只能在类的内部和子类中被访问，不能直接调用print(Son._bar)或print(son._bob)来输出这两个属性的值，所以定义了print_bar和print_bob方法，实现在子类中输出，这段代码也正常的输出了_bar和_bob的值。 1234print(Son._bar) # 123son = Son(456)print(son._bob) # 456 （假装）惊讶的发现，竟然没有报错，也输出了正确的值。 Python中用加下划线来定义受保护变量，是一种约定的规范，而不是语言层面真的实现了访问控制，所以，我们定义的保护变量，依然可以在外部被访问到（这是个feature，不是bug）。 私有（private） Python定义私有属性，需要在属性名前加两个下划线__，把上面的代码修改一下，运行一下会发现下面的代码中的任何一个print都会报错的。 123456789101112131415161718192021class Foo(object): __bar = 123 def __init__(self, bob): self.__bob = bobclass Son(Foo): def print_bob(self): print(self.__bob) # Error @classmethod def print_bar(cls): print(cls.__bar) # Errorprint(Son.__bar) # Errorson = Son(456)print(son._bob) # Error 深入一下——私有属性真的就访问不到了吗？ 要了解私有属性是否真的访问不到，需要从Python是如何实现私有属性入手。CPython中，会把双下划线的属性变为_ClassName__PropertyName的形式，用代码演示一下： 1234class Foo(object): __bar = 123print(Foo._Foo__bar) # 123 再比如 12345class F: __name = "xurui"f = F()result = f._F__name ##通过这个方式可以从外部访问...print(result) 运行一下可以知道，正常输出了__bar的值，但是不推荐这样去访问私有属性，因为不同的Python解释器对于私有属性的处理不一样。 特例 使用双下划线定义私有属性，有一种特殊情况，当属性后也有两个下划线的时候，这个属性会被Python解释器当做魔术方法，从而不做私有处理。 12345class Foo(object): __bar__ = 123print(Foo.__bar__) # 123 上面代码输出了123，证明Python解释器并没有把bar当做私有属性。当定义私有属性时，需要注意名字最后最多只能有一个下划线。 假如定义的属性名就叫__呢？不妨直接试一下： 123class Foo(object): __ = 123print(Foo.__) # 123 可以发现名字叫的属性也不会被认为是私有属性，名字是多个下划线的属性也不是私有属性（比如_）。 12345678910class ProtectMe(object): def __init__(self): self.me = "wxx" self.__name = "zixin" @property def name(self): return self.__namep = ProtectMe()print p.name 与面试官谈笑风生 | Python面向对象之访问控制 类class的访问控制 （这个实际上体现的是封装的思想） Python中没有访问控制的关键字，例如private、protected等等。但是，在Python编码中，有一些约定来进行访问控制。Python中没有真正的私有属性或方法,可以在你想声明为私有的方法和属性前加上单下划线,以提示该属性和方法不应在外部调用.如果真的调用了也不会出错,但不符合规范. xx： 共有变量 单下划线 “_” 前置单下划线，私有化属性或方法，一般来讲，变量名_xx被看作是“私有 的”，在模块或类外不可以使用。当变量是私有的时候，用_xx 来表示变量是很好的习惯。类对象和子类可以访问,这并不能完全做到真正的私有，只是约定俗成的而已，这样写表示不希望这个变量在外部被直接调用。 双下划线”__” 前置双下划线，私有化属性或方法，无法在外部直接访问（名字重整所以访问不到,只能是允许这个类本身进行访问了。连子类也不可以） xx_：后置单下划线，用于避免与Python关键词的冲突（这种约定俗称的东西很好，一定要按照这种规范去做） 单下划线、双下划线、头尾双下划线说明： 12345__foo__: 定义的是特殊方法，一般是系统定义名字 ，类似 __init__() 之类的。_foo: 以单下划线开头的表示的是 protected 类型的变量，即保护类型只能允许其本身与子类进行访问，不能用于 from module import *__foo: 双下划线的表示的是私有类型(private)的变量, 只能是允许这个类本身进行访问了。 这种命名方式同样是适用方法的命名。 python 中也是一样的，重载是类本身就有的方法，重写是子类针对父类函数的重写。 12345678910111213141516class Site: def __init__(self, name, url): self.name = name # public self.__url = url # private # 这个是系统的方法 def who(self): print('name : ', self.name) print('url : ', self.__url) def __foo(self): # 私有方法 print('这是私有方法') def foo(self): # 公共方法 print('这是公共方法') self.__foo() 继承 Python3的继承机制不同于Python2。其核心原则是下面两条，请谨记！ 子类在调用某个方法或变量的时候，首先在自己内部查找，如果没有找到，则开始根据继承机制在父类里查找。 根据父类定义中的顺序，以深度优先的方式逐一查找父类！ 继承参数的书写有先后顺序，写在前面的被优先继承。 super() 函数 我们都知道，在子类中如果有与父类同名的成员，那就会覆盖掉父类里的成员。那如果你想强制调用父类的成员呢？使用super()函数！这是一个非常重要的函数，最常见的就是通过super调用父类的实例化方法init！ 1234567891011121314151617class A: def __init__(self, name): self.name = name print("父类的__init__方法被执行了！") def show(self): print("父类的show方法被执行了！")class B(A): def __init__(self, name, age): super(B, self).__init__(name=name) self.age = age def show(self): super(B, self).show()obj = B("jack", 18)obj.show() 继承可以把父类的所有功能都直接拿过来，这样就不必重零做起，子类只需要新增自己特有的方法，也可以把父类不适合的方法覆盖重写。 多态 1234567891011121314151617181920212223242526272829303132333435class Animal: def kind(self): print("i am animal") class Dog(Animal): def kind(self): print("i am a dog") class Cat(Animal): def kind(self): print("i am a cat")class Pig(Animal): def kind(self): print("i am a pig")# 这个函数接收一个animal参数，并调用它的kind方法def show_kind(animal): animal.kind()d = Dog()c = Cat()p = Pig()show_kind(d)show_kind(c)show_kind(p)------------------打印结果：i am a dogi am a cati am a pig 多态的思想，使用一个指针（变量）可以根据位置指向不同的子类。 python 中的三类方法一般使用实例方法最多，需要有 self 作为一个传入参数，在子类中可以使用该方法。 类方法使用修饰器 @classmethod ；静态方法使用修饰器 @staticmethod，对于静态方法，调用时并不需要传递类或者实例。其实，静态方法很像我们在类外定义的函数，只不过静态方法可以通过类或者实例来调用而已。 实例方法只能被实例对象调用，静态方法(由@staticmethod装饰的方法)、类方法(由@classmethod装饰的方法)，可以被类或类的实例对象调用。 实例方法，第一个参数必须要默认传实例对象，一般习惯用self。静态方法，参数没有要求。类方法，第一个参数必须要默认传类，一般习惯用cls。 1234567891011121314151617181920212223242526class Kls(object): def foo(self, x): print('executing foo(%s,%s)' % (self, x)) @classmethod def class_foo(cls,x): print('executing class_foo(%s,%s)' % (cls,x)) @staticmethod def static_foo(x): print('executing static_foo(%s)' % x)ik = Kls()# 实例方法ik.foo(1)print(ik.foo)print('==========================================')# 类方法ik.class_foo(1)Kls.class_foo(1)print(ik.class_foo)print('==========================================')# 静态方法ik.static_foo(1)Kls.static_foo('hi')print(ik.static_foo) 类方法Python 提供了 classmethod 装饰器让我们实现上述功能，看下面的例子： 12345678910class A(object): bar = 1 @classmethod def class_foo(cls): print 'Hello, ', cls print cls.bar&gt;&gt;&gt; A.class_foo() # 直接通过类来调用方法Hello, &lt;class '__main__.A'&gt;1 在上面，我们使用了 classmethod 装饰方法 class_foo，它就变成了一个类方法，class_foo 的参数是 cls，代表类本身，当我们使用 A.class_foo() 时，cls 就会接收 A 作为参数。另外，被 classmethod 装饰的方法由于持有 cls 参数，因此我们可以在方法里面调用类的属性、方法，比如 cls.bar。 静态方法 在类中往往有一些方法跟类有关系，但是又不会改变类和实例状态的方法，这种方法是静态方法，我们使用 staticmethod 来装饰，比如下面的例子： 1234567891011class A(object): bar = 1 @staticmethod def static_foo(): print &apos;Hello, &apos;, A.bar&gt;&gt;&gt; a = A()&gt;&gt;&gt; a.static_foo()Hello, 1&gt;&gt;&gt; A.static_foo()Hello, 1 可以看到，静态方法没有 self 和 cls 参数，可以把它看成是一个普通的函数，我们当然可以把它写到类外面，但这是不推荐的，因为这不利于代码的组织和命名空间的整洁。 python 中的单例/ 多例模式 定义 单例模式(Singleton Pattern)：单例模式确保某一个类只有一个实例，而且自行实例化并向整个系统提供这个实例，这个类称为单例类，它提供全局访问的方法。单例模式的要点有三个：一是某个类只能有一个实例；二是它必须自行创建这个实例；三是它必须自行向整个系统提供这个实例。单例模式是一种对象创建型模式。单例模式又名单件模式或单态模式。 单例模式的分类 懒汉版（Lazy Singleton）：单例实例在第一次被使用时才进行初始化，这叫做延迟初始化。 Lazy Singleton存在内存泄露的问题，有两种解决方法： 使用智能指针 使用静态的嵌套类对象 饿汉版（Eager Singleton）：指单例实例在程序运行时被立即执行初始化。由于在main函数之前初始化，所以没有线程安全的问题。 两种分类的总结：Eager Singleton 虽然是线程安全的，但存在潜在问题；Lazy Singleton通常需要加锁来保证线程安全，但局部静态变量版本在C++11后是线程安全的；局部静态变量版本（Meyers Singleton）最优雅。 什么情况下使用单例模式 系统只需要一个实例对象，如系统要求提供一个唯一的序列号生成器，或者需要考虑资源消耗太大而只允许创建一个对象。 客户调用类的单个实例只允许使用一个公共访问点，除了该公共访问点，不能通过其他途径访问该实例。 小的例子：某个服务器程序的配置信息存放在一个文件中，客户端通过一个 AppConfig 的类来读取配置文件的信息。如果在程序运行期间，有很多地方都需要使用配置文件的内容，也就是说，很多地方都需要创建 AppConfig 对象的实例，这就导致系统中存在多个 AppConfig 的实例对象，而这样会严重浪费内存资源，尤其是在配置文件内容很多的情况下。事实上，类似 AppConfig 这样的类，我们希望在程序运行期间只存在一个实例对象。 python 中的实现 有以下四种方式实现：1234使用模块使用 __new__使用装饰器（decorator）使用元类（metaclass） 使用模块 其实，Python 的模块就是天然的单例模式，因为模块在第一次导入时，会生成 .pyc 文件，当第二次导入时，就会直接加载 .pyc 文件，而不会再次执行模块代码。因此，我们只需把相关的函数和数据定义在一个模块中，就可以获得一个单例对象了。如果我们真的想要一个单例类，可以考虑这样做： 12345# mysingleton.pyclass My_Singleton(object): def foo(self): passmy_singleton = My_Singleton() 使用new 关键字, 如果已经有有了该实例，那么就不用生成了 12345678class Singleton(object): _instance = None def __new__(cls, *args, **kw): if not cls._instance: cls._instance = super(Singleton, cls).__new__(cls, *args, **kw) return cls._instance class MyClass(Singleton): a = 1 使用修饰器123456789101112from functools import wrapsdef singleton(cls): instances = &#123;&#125; @wraps(cls) def getinstance(*args, **kw): if cls not in instances: instances[cls] = cls(*args, **kw) return instances[cls] return getinstance@singletonclass MyClass(object): a = 1 python中的单例模式 数据处理函数读取 矩阵文件 12345678910111213141516171819202122232425262728293031#第一种方法 python 读取矩阵文件# 单精度是32 位，双精度是64位def read_file(file1): f =open(file1) matrix=[] while True: line =f.readline() if not line: break line=line.strip() line =line.split(",") line =map(float, line) # 这里转化成float 就是ok的 matrix.append(line) f.close() # matrix =numpy.array(matrix) return matrix# 第二种方法 基于numpy 的实现path ="text.txt"matrix =numpy.loadtxt(path)b =numpy.reshape(matrix,(100, 2) )b =numpy.reshape(matrix, (-1, 3, 2)) shuffle 函数 只要是打散，都是会用到 shuffle() 这样的一个函数。 123456789101112# 纯python 实现index= [i for i in range(len(train_x))]random.shuffle(index)train_x =train_x[index]train_y =train_y[index]# 基于 numpy 实现def shuffle_data(data): np.random.shuffle(data) return data 按照文件名进行排序 这个是按照字典序排序的。 1234567891011import numpy as npimport os img_path='./img/' img_list=sorted(os.listdir(img_path)) #文件名按字母排序img_nums=len(img_list)for i in range(img_nums): img_name=img_path+img_list[i] print(img_name) 注意下面的是按照数字进行排序的，也就是文件名。这种方法更加使用，注意在实现的时候，将 string 转换成了int 类型。 1234567891011import numpy as npimport osimg_path='./img/' img_list=os.listdir(img_path)img_list.sort()img_list.sort(key = lambda x: int(x[:-4])) ##文件名按数字排序img_nums=len(img_list)for i in range(img_nums): img_name=img_path+img_list[i] print(img_name) 库函数 heapq在python 中默认的是一个小根堆（在c++ 中默认的是一个大根堆） 1234567heapq.heappush(heap, item) 把item添加到heap中（heap是一个列表）heapq.heappop(heap) 把堆顶元素弹出，返回的就是堆顶heapq.heappushpop(heap, item) 先把item加入到堆中，然后再pop，比heappush()再heappop()要快得多# 这比较好用，不用自己维护 pushpop 操作heapq.nlargest(n, iterable, key=None) 返回最大的n个元素（Top-K问题）heapq.nsmallest(n, iterable, key=None) 返回最小的n个元素（Top-K问题）# 如果写成了这样，那么寻找最大K 个元素，瞬间就没有了意义，有没有感觉到 dictionary 1234import collections# 就类似一个计数器了hash =collections.defaultdict(int); 还有一种方式，直接使用 {} 进行表示 queue（队列） 队列是一种基于先进先出（FIFO）策略的数据结构。在python 中常见的有三种队列： 先进先出队列： 尾部进去，头部出来 优先队列：实现大根堆和小根堆，在python 中默认的是大根堆（貌似），按照级别进行弹出 双向队列： 头部和尾部都是可以弹出和弹入，可以在两段进行编辑，所以既可以用来实现stack 也可以用来实现队列上述三种队列都是有maxlen 属性，那么就可以设置队列的长度。 队列中常常使用的方法： 入队 q.put(item) 出队 item =q.get() ，从队头删除并且返回 返回队列的大小 q.qsize() 判断是否为空 q.empty() stack python 中没有专门的处理栈库函数，但是栈的操作可以使用list 全部实现。所以当你在使用stack 的时候，基本上就是在使用list 12345# python 中的入栈stack =[1, 2, 3]stack.append(4)item =stack.pop() # 注意这个是直接返回，stack 中不再存在 heap python 中默认实现的是小根堆。使用库函数 heapq 库函数，其中 heappush() 和 heappop() 其他python 中 in 操作 在不同的数据集合中的时间复杂度这个是取决于操作的数据结构： list (tuple) -average: O(N) set/dict- average: O(1), worst: O(N) (如果 dictionary 所有的值都相同的话，那么时间复杂度就是 O(n)) The O(n) worst case for sets and dicts is very uncommon, but it can happen if hash is implemented poorly. This only happens if everything in your set has the same hash value. 操作 平均情况 最坏情况 说明 列表 list O(n) O(n) list 是由数组实现的 字典 dict O(1) O(n) 集合 set O(1) O(n) 内部实现和 dict 很像 python 中表示最大和最小的数字123import syssys.maxsize #如果是整型float(‘inf’) # 如果是浮点型 python 中 string的切片的时间复杂度，因为python 中string是不可变的，所以切片操作基本上是通过复制完成的，所以时间复杂度一般是 $O(n)$，复制的时候需要申请一个临时的空间，所以空间复杂度基本上也稳定在 $O(n)$。常见的切片操作有三种： str[:] 完全复制，返回的是exact same 的 copy，前后两个变量没有什么关系了 str[1:1]， mystr[1:1] is mystr[2:2] is ‘’ 是一个空的对象 str[a: b] 其中 $a&lt; b$，最常见的切片复制, make a copy [::-1] reverse的操作]]></content>
      <categories>
        <category>CS基础</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[C++ 编程语言]]></title>
    <url>%2F2019%2F01%2F05%2Fcpp%2F</url>
    <content type="text"><![CDATA[主要介绍C++ 编程语言，面向对象中的封装、继承和多态； STL(Standard Template Library 标准模板库, string, list, map) 等函数；内存管理；指针。（持续更新中…） 多态（一个接口，多种形态）： 父类的指针或引用有多种表现形态，用父类的指针或者引用统一操作各种子类对象。 面向对象c语言是面向过程的语言，c++ 是面向对象的语言。c 语言结构体成员默认的访问权限是共有，c++ 的成员的默认访问权限是私有。面向对象主要体现在其三大特征：封装、继承和多态。 封装 把变量和函数写到一个类中 不想暴露在外界的成员私有化 可以使得代码模块化。 继承 可以使代码得到良好的复用，扩展已有的代码 使整个程序设计更加符合人们的逻辑（在水果的基础上可以生成热带水果，温带水果） 多态 多态性是允许你将父对象设置成为和一个或更多的他的子对象相等的技术，赋值之后，父对象就可以根据当前赋值给它的子对象的特性以不同的方式运作。简单的说：允许将子类类型的指针赋值给父类类型的指针（一个接口，多种方法）。 C++ 支持两种多态性 编译时多态性（静态多态）：通过重载函数实现 运行时多态性（动态多态）：通过虚函数实现。 虚函数： 就是允许被其子类重新定义的成员函数，子类重新定义父类虚函数的做法，可实现成员函数的动态覆盖（Override）。调用的时候声明基类的指针，利用该指针指向任意一个子类对象，调用相应的虚函数，可以根据指向的子类的不同而实现不同的方法。 12345678910111213141516171819202122#include&lt;iostream&gt;using namespace std;class basic&#123;private: static int a; char b[10]; int c;public: static void fun1()&#123;cout&lt;&lt;"fun1"&lt;&lt;endl;&#125; void fun2()&#123;cout&lt;&lt;"fun2"&lt;&lt;endl;&#125; virtual void fun3()&#123;cout&lt;&lt;"fun3"&lt;&lt;endl;&#125; //virtual void fun4()&#123;cout&lt;&lt;"fun4"&lt;&lt;endl;&#125; //virtual void fun5()&#123;cout&lt;&lt;"fun4"&lt;&lt;endl;&#125;&#125;;int basic::a=1;int main ()&#123; basic test; cout&lt;&lt;"size:"&lt;&lt;sizeof(test)&lt;&lt;endl; # 24 在64位机器上，不管有几个虚函数，只是在最开始的地方有一个虚函数表，地址字节(64/8)的大小 return 0;&#125; 纯虚函数： 是在基类中声明的虚函数，它在基类中没有定义，但要求任何派生类都要定义自己的实现方法。在基类中实现纯虚函数的方法是在函数原型后加“=0” 123//纯虚函数：虚函数只有声明，没有实现，函数体=0virtual void draw() =0;virtual void rotate(double ) =0; // 没有实现 抽象类： 包含纯虚函数的类称为抽象类。由于抽象类包含了没有定义的纯虚函数，所以不能进行实例化。 纯虚函数的作用：1). 很多情况下，基类本身生成对象是不合理的。例如动物作为一个父类可以派生出老虎，但是动物本身生成对象是不合常理的。2). 编译器要求在派生类中必须予以重写以实现多态性。同时含有纯虚拟函数的类称为抽象类，它不能生成对象。这样就很好地解决了上述两个问题。 （1）重载 vs. 重写 vs. 重定义（隐藏） 隐藏是指派生类的函数屏蔽了与其同名的基类函数，规则如下： 1). 如果派生类的函数与基类的函数同名，但是参数不同。此时，不论有无virtual关键字，基类的函数将被隐藏（注意别与重载混淆，重载是在同一个类中，而隐藏涉及派生类与基类）。2). 如果派生类的函数与基类的函数同名，并且参数也相同，但是基类函数没有virtual关键字。此时，基类的函数被隐藏（注意别与覆盖混淆，覆盖有virtual关键字）。 重载（overload） 重载是一个类内部函数之间的关系，调用时根据参数的等不同选择不同的函数。指函数名相同，但是它的参数表列个数或顺序，类型不同。但是不能靠返回类型来判断。 （1）相同的范围（在同一个作用域中） ； （2）函数名字相同； （3）参数不同； （4）virtual 关键字可有可无。 （5）返回值可以不同； 123void a(int x)&#123;&#125;void a(int x, int y)&#123;&#125;void a(int x, int y, int z)&#123;&#125; 重写（override） 重写是父类和子类之间的关系，要求父类函数中有关键字virtual，并且覆盖的时候子类和父类函数名和参数需要相同。派生类重新定义基类的虚函数，特征是： （1）不在同一个作用域（分别位于派生类与基类） ； （2）函数名字相同； （3）参数相同； （4）基类函数必须有 virtual 关键字，不能有 static 。 1234567891011class Base&#123;public: virtual void a(int x)&#123;&#125;&#125;class Derived: public Base&#123;public: void a(int x)&#123;&#125;&#125; 重定义（隐藏） 子类重定义父类中的非虚函数，屏蔽了父类的同名函数(相当于创建了一个新的函数，跟父类无关) 两种情况 1). 子类和父类函数的名称相同，参数也相同，父类中的函数不是virtual，父类的函数将被隐藏2). 子类和父类的函数名称相同，但参数不同，此时不管父类函数是不是virtual函数，都将被隐藏。 （2）构造函数 构造函数是一种比较特殊的成员函数，用于创建并初始化对象。声明对象时构造函数会被编译器自动调用。 构造函数的四个特点：1). 构造函数的访问权限必须为公有（public）2). 构造函数名和类名相同3). 构造函数没有返回值4). 构造函数可以带参数，用于初始化成员变量 123Circle();// 默认构造函数Circle(float a =0, float b =0, float c =0); // 默认构造函数Circle(float a, float b, float c); （3）析构函数 类的析构函数是类的一种特殊的成员函数，它会在每次删除所创建的对象时执行。 析构函数的名称与类的名称是完全相同的，只是在前面加了个波浪号（~）作为前缀，它不会返回任何值，也不能带有任何参数。析构函数有助于在跳出程序（比如关闭文件、释放内存等）前释放资源。 12345678910111213141516171819202122#include &lt;cstdio&gt;using namespace std;class A &#123;public: A() &#123; printf("A()\n"); &#125; virtual ~A() &#123; printf("~A()\n"); &#125;&#125;;class B : public A &#123;public: B() &#123; printf("B()\n"); &#125; ~B() &#123; printf("~B()\n"); &#125;&#125;;int main() &#123; A *p = nullptr; p = new B; delete p; return 0;&#125; 三种访问权限private: 只能由该类中的函数、其友元函数访问,不能被任何其他访问，该类的对象也不能访问.protected: 可以被该类中的函数、子类的函数、以及其友元函数访问,但不能被该类的对象访问public: 可以被该类中的函数、子类的函数、其友元函数访问,也可以由该类的对象访问注：友元函数包括两种：设为友元的全局函数，设为友元类中的成员函数 public: 可以被任意实体访问protected: 只允许子类和本类中的成员函数访问private: 只允许本类中的成员函数访问 当发生继承时访问权限的变化： public继承不改变基类成员的访问权限 private继承使得基类所有成员在子类中的访问权限变为private protected继承将基类中public成员变为子类的protected成员，其它成员的访问 权限不变。 基类中的private成员不受继承方式的影响，子类永远无权访问。 类的友元函数是定义在类外部，但有权访问类的所有私有（private）成员和保护（protected）成员。尽管友元函数的原型有在类的定义中出现过，但是友元函数并不是成员函数。友元可以是一个函数，该函数被称为友元函数；友元也可以是一个类，该类被称为友元类，在这种情况下，整个类及其所有成员都是友元。如果要声明函数为一个类的友元，需要在类定义中该函数原型前使用关键字 friend，如下所示： 123456789class Box&#123; double width;public: double length; friend void printWidth( Box box ); // 友元函数 void setWidth( double wid ); friend class ClassTwo; // 友元类&#125;; 常见的关键字const 关键字： 总的原则是只读的，不能变。 (1 ) const 定义常量 const float pi =3.14 (2 ) const 与指针 指针常量和常量指针 (谁在前谁不能变，常量指针中常量不能变，指针常量中指针不能变) 常量指针： 123456789101112#include&lt;iostream&gt;using namespace std;int main()&#123; int a =5; const int *p =&amp;a; // 不能使用 *p =20 这样的修改常量的操作 cout &lt;&lt; *p &lt;&lt; endl; return 0;&#125; 这三种形式都是正确的。12345int const *x;const int *y;int a =2;int *const p =&amp;a; 指针常量（指向常量的指针）： 1234567891011121314151617int a =5;int *const p =&amp;a;*p =20; // 指针不能变，但是可以修改内容#include&lt;iostream&gt;using namespace std;int main()&#123; int a =5; int *const p =&amp;a; // 不能使用 *p =20 这样的修改常量的操作 cout &lt;&lt; *p&lt;&lt;endl; int b =6; *p =20; cout &lt;&lt; *p &lt;&lt;endl; return 0;&#125; (3 ) const 与函数 const int func(const int &amp; a) const; 修饰形参，形参不能变；修饰成员函数时，函数体内不能修改成员变量的值 （注意一共可以有三个位置，常用的是位置二和位置三） 123456class Screen &#123;public: int ok() const &#123;return _cursor; &#125; int error(intival) const &#123; _cursor = ival; &#125;&#125; (4 ) const 对象 const Point p; 常量对象 const 对象 只能调用const 成员函数，不能调用普通成员函数 普通对象既可以调用const 成员函数 也可以调用普通成员函数 static 关键字 静态局部变量：函数结束后，静态局部变量的内存空间不会被系统回收，下一次调用函数时使用上一次退出时的值。 12345678910#include&lt;iostream&gt;using namespace std;void show_average(double x)&#123;static double num =0;&#125;int main()&#123;return 0;&#125; 原理： 1). 静态局部变量存储在静态存储区2). 静态局部变量在函数结束后不会被回收，下次使用的时候可以保留上一次的结果3). 静态局部变量如果未进行初始化，会被编译器初始化成0 或者NULL 静态局部变量或全局变量的区别：作用域不同。局部变量只是在一个函数中，全局变量是整个class 文件。 静态全局变量和全局变量的区别：静态全局变量只能被本文使用，而全局变量可以被别的文件使用 类的静态成员的访问有两种方式，推荐使用方式一来访问，因为静态变量属于整个类，而不是某个特定的对象。 12345//方式一 类名::静态成员名Student:: teacherName;// 方式二： 对象名.静态成员名Student s1; s1.teacherName&apos; 视频讲解很好 c++ 中队列操作的访问： front() 返回第一个元素pop() 删除第一个元素push() 在末尾加入一个元素back() 返回最后一个元素empty() 如果队列空则返回真size() 返回队列中元素的个数 c++ 栈stack 的操作访问 top() 返回栈顶元素，不删除（获取）pop() 移除栈顶元素 （删除）push() 在栈顶增加元素 （增加）empty() 堆栈为空则返回真size() 返回栈中元素数目 auto 关键词 在C++ 11 中，已经删除了该用法，取而代之的作用是：自动推断变量的类型。 define 和 typedef 的区别 #define 是 C 中定义的语法, typedef 是 C++ 中定义的语法, 二者在 C++ 中可以通用, 但 #define 成了预编译指令, typedef 当成语句处理. typedef 和 define 都可以用来给对象取一个别名, 但是俩者却有很大的不同, 有以下几点 执行时间 关键字 typedef 在编译阶段有效, 由于是在编译阶段, 因此 typededf 有类型检查的功能.define 是宏定义, 发生在预处理阶段, 也就是编译之前, 它只是进行简单而机械的字符串替换, 而不进行任何检查. 功能不同 typedef 用来定义类型的别名#define不止可以为类型取别名, 还可以定义常量, 变量, 编译开关等. 作用域不同 #define 没有作用域的限制, 只要是之前预定义过的宏, 在以后的程序中都可以使用. 而 typedef 有自己的作用域. 1234567void fun() &#123; #define A int&#125;void gun() &#123;//在这里也可以使用A，因为宏替换没有作用域，//但如果上面用的是typedef，那这里就不能用A ，不过一般不在函数内使用typedef&#125; 最指针操作作用不同 123456Typedef int * pint； #define PINT int * Const pint p；//p不可更改，p指向的内容可以更改，相当于 int * const p; Const PINT p；//p可以更改，p指向的内容不能更改，相当于 const int *p；或 int const *p； pint s1, s2; //s1和s2都是int型指针 PINT s3, s4; //相当于int * s3，s4；只有一个是指针。 123#define MAX(a,b) (a&gt;b)?a:bmax = MAX(x,y); define 和 typedef 区别C/C++ 中的 define 和 typedefC/C++ 中的宏/Macro decltype类型指示符 从表达式的类型推断出要定义的变量的类型，但是不想用该表达式的值初始化变量。 c++ 中的值传递、指针传递和引用传递 对于 传值、传递引用、传递指针的理解。对于前者是不会修改原来的值，相当于一种copy 之后的操作；但是对于后两者，直接操作的是原来的值，类似一种全局变量的感觉。所以如果不想要修改原来的值，那么传递值，如果想要修改原来的值，那么就传递指针。后者就类似维护一种全局的变量。 传值，就是copy 数值。 传引用，就是传递地址， 使用 ‘&amp;’ 这个符号 传指针， 使用 ‘*’ 符号。 在改变原始的数据上面，2， 3两种方式是没有什么区别的。一般来说能用引用就用引用，指针强大，但是容易修改地址，比较危险。引用并不能改变其目标变量，而指针可以任意改变其指向的地址。 1234567891011121314151617181920212223242526272829#include&lt;iostream&gt;using namespace std;//值传递 void change1(int n)&#123; cout&lt;&lt;"值传递--函数操作地址"&lt;&lt;&amp;n&lt;&lt;endl; //显示的是拷贝的地址而不是源地址 n++;&#125;//引用传递void change2(int &amp; n)&#123; cout&lt;&lt;"引用传递--函数操作地址"&lt;&lt;&amp;n&lt;&lt;endl; n++;&#125; //指针传递void change3(int *n)&#123; cout&lt;&lt;"指针传递--函数操作地址 "&lt;&lt;n&lt;&lt;endl; *n=*n+1; &#125; int main()&#123; int n=10; cout&lt;&lt;"实参的地址"&lt;&lt;&amp;n&lt;&lt;endl; change1(n); cout&lt;&lt;"after change1() n="&lt;&lt;n&lt;&lt;endl; change2(n); cout&lt;&lt;"after change2() n="&lt;&lt;n&lt;&lt;endl; change3(&amp;n); cout&lt;&lt;"after change3() n="&lt;&lt;n&lt;&lt;endl; return true;&#125; STLSTL的一个重要特点是数据结构和算法的分离。尽管这是个简单的概念，但这种分离确实使得STL变得非常通用。例如，由于STL的sort()函数是完全通用的，你可以用它来操作几乎任何数据集合，包括链表，容器和数组。 STL另一个重要特性是它不是面向对象的。为了具有足够通用性，STL主要依赖于模板而不是封装，继承和虚函数（多态性）——OOP的三个要素。你在STL中找不到任何明显的类继承关系。这好像是一种倒退，但这正好是使得STL的组件具有广泛通用性的底层特征。另外，由于STL是基于模板，内联函数的使用使得生成的代码短小高效。 STL提供了大量的模板类和函数，可以在OOP和常规编程中使用。所有的STL的大约50个算法都是完全通用的，而且不依赖于任何特定的数据类型。下面的小节说明了三个基本的STL组件：（1）迭代器提供了访问容器中对象的方法。（2） 容器是一种数据结构，如list，vector，和deques ，以模板类的方法提供。（3）算法是用来操作容器中的数据的模板函数。 字典序：按字母顺序排列的序列 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798vector, 变长数组，倍增的思想 size() 返回元素个数 empty() 返回是否为空 clear() 清空 front()/back() push_back()/pop_back() begin()/end() [] 支持比较运算，按字典序pair&lt;int, int&gt; first, 第一个元素 second, 第二个元素 支持比较运算，以first为第一关键字，以second为第二关键字（字典序）string，字符串 size()/length() 返回字符串长度 empty() clear() substr(起始下标，(子串长度)) 返回子串 c_str() 返回字符串所在字符数组的起始地址queue, 队列 size() empty() push() 向队尾插入一个元素 front() 返回队头元素 back() 返回队尾元素 pop() 弹出队头元素priority_queue, 优先队列，默认是大根堆 push() 插入一个元素 top() 返回堆顶元素 pop() 弹出堆顶元素 定义成小根堆的方式：priority_queue&lt;int, vector&lt;int&gt;, greater&lt;int&gt;&gt; q;stack, 栈 size() empty() push() 向栈顶插入一个元素 top() 返回栈顶元素 pop() 弹出栈顶元素deque, 双端队列 size() empty() clear() front()/back() push_back()/pop_back() push_front()/pop_front() begin()/end() []set, map, multiset, multimap, 基于平衡二叉树（红黑树），动态维护有序序列 size() empty() clear() begin()/end() ++, -- 返回前驱和后继，时间复杂度 O(logn) set/multiset insert() 插入一个数 find() 查找一个数 count() 返回某一个数的个数 erase() (1) 输入是一个数x，删除所有x O(k + logn) (2) 输入一个迭代器，删除这个迭代器 lower_bound()/upper_bound() lower_bound(x) 返回大于等于x的最小的数的迭代器 upper_bound(x) 返回大于x的最小的数的迭代器 map/multimap insert() 插入的数是一个pair erase() 输入的参数是pair或者迭代器 find() [] 注意multimap不支持此操作。 时间复杂度是 O(logn) lower_bound()/upper_bound()unordered_set, unordered_map, unordered_multiset, unordered_multimap, 哈希表 和上面类似，增删改查的时间复杂度是 O(1) 不支持 lower_bound()/upper_bound()， 迭代器的++，--bitset, 圧位 bitset&lt;10000&gt; s; ~, &amp;, |, ^ &gt;&gt;, &lt;&lt; ==, != [] count() 返回有多少个1 any() 判断是否至少有一个1 none() 判断是否全为0 set() 把所有位置成1 set(k, v) 将第k位变成v reset() 把所有位变成0 flip() 等价于~ flip(k) 把第k位取反 注意 1234567 #include&lt;queue&gt;queue&lt;Node *&gt; q;q.push(nullptr);q.push(nullptr);// 这个时候 q.empty() 是0， 但是 q.size() 是 2，所以当有空指针的时候，注意一下 empty() 和size() 的区别 C++ STL中最基本以及最常用的类或容器无非就是以下几个： vector C++ STL中的vector好比是C语言中的数组，但是vector又具有数组没有的一些高级功能。与数组相比，vector就是一个可以不用再初始化就必须制定大小的边长数组，当然了，它还有许多高级功能。 需要包含头文件。下面是常见的初始化的操作 1234567891011#include &lt;vector&gt;vector&lt;int&gt; v1;vector&lt;father&gt; v2;vector&lt;string&gt; v3;vector&lt;vector&lt;int&gt; &gt;; //注意空格。这里相当于二维数组int a[n][n];vector&lt;int&gt; v5 = &#123; 1,2,3,4,5 &#125;; //列表初始化,注意使用的是花括号vector&lt;string&gt; v6 = &#123; "hi","my","name","is","lee" &#125;;vector&lt;int&gt; v7(5, -1); //初始化为-1,-1,-1,-1,-1。第一个参数是数目，第二个参数是要初始化的值vector&lt;string&gt; v8(3, "hi");vector&lt;int&gt; v9(10); //默认初始化为0vector&lt;int&gt; v10(4); //默认初始化为空字符串 比如下面的代码是错误的，但是编译器不会报错，就像是数组越界。 12vector&lt;int&gt; vec;vec[0] = 1; //错误！ STL中vector的实现原理：vector的数据安排以及操作方式与array非常类似。两者的唯一差别在于空间的运用的灵活性。array是静态空间，一旦配置好了就不能再改变了。如果程序需要一个更大空间的array，只能自己再申请一个更大的array，然后将以前array中的内容全部拷贝到新的array中。vector是动态空间，随着元素的加入，它的内部机制会自动扩充空间以容纳新的元素。vector的关键技术在于其对大小的控制以及重新配置时的数据移动效率。 为了降低空间配置时候的速度，vector实际配置的大小可能比客户端需求量更大一些，以备将来可能的扩充。扩充空间需要经过的步骤：重新配置空间，元素移动，释放旧内存空间。 vector 中的查询操作 vector最常用的增删操作 string 在c 语言中这样使用字符串 12345678910 char* s1 = &quot;Hello SYSU!&quot;; //创建指针指向字符串常量，这段字符串我们是不能修改的//想要创建 可以修改的字符串，我们可以使用数组分配空间char s2[20] = &quot;Hello SYSU!&quot;;//或者这样char s3[] = &quot;Hello SYSU!&quot;;//当然我们也可以动态分配内存char* s4 = (char*)malloc(20）;gets(s4); 在 C++ 中string 类型是变长的，在头文件 string 中。用string初始化字符串分两类：用“=”号就是拷贝初始化，否则就是直接初始化。 12345678 #include &lt;string&gt; string s1;//初始化字符串，空字符串string s2 = s1; //拷贝初始化，深拷贝字符串string s3 = &quot;I am Yasuo&quot;; //直接初始化，s3存了字符串string s4(10, &apos;a&apos;); //s4存的字符串是aaaaaaaaaastring s5(s4); //拷贝初始化，深拷贝字符串string s6(&quot;I am Ali&quot;); //直接初始化string s7 = string(6, &apos;c&apos;); //拷贝初始化，cccccc string 中的IO 操作 使用cin读入字符串时，遇到空格或者分隔符（\n）就停止读取。比如程序输入的是 1&quot; Hello World&quot; 这个时候可以使用如下的语句进行读入1cin&gt;&gt;s1&gt;&gt;s2; hello存在s1里，world存在s2里了。如果想要存储一行内容而不是单个的，那么使用以下的语句123string str;getline(cin, str);cout &lt;&lt; str &lt;&lt; endl; 当把string对象和字符面值及字符串面值混在一条语句中使用时，必须确保+的两侧的运算对象至少有一个是string 1234string s1 = s2 + &quot;, &quot;; //正确string s3 = &quot;s &quot; + &quot;, &quot;; //错误string s4 = &quot;hello&quot; + &quot;, &quot; + s1; //错误string s5 = s1 + &quot;hello &quot; + &quot;, &quot;; //改一下顺序，s1放前头，正确了，注意理解=号右边的运算顺序 处理string 类型的字符 npos 表示 non-position 在cpp 中表示不可达的地址，如果找不见position，那么就返回这个值。不一定是 -1，在mac 中至少不是。 12345 for (int i = 0; i &lt; s3.size(); i++)&#123; cout &lt;&lt; s3[i] &lt;&lt; endl; s3[i] = 's';&#125; string还有一些很好用的函数，比如找子串 1234567891011121314151617 #include&lt;iostream&gt;#include&lt;string&gt;using namespace std;int main()&#123; string str ="abcddd"; char ch ='e'; auto position =str.find(ch, 0); if(position != str.npos) cout &lt;&lt; position&lt;&lt; endl; else cout &lt;&lt; "could not find "&lt;&lt; endl; return 0;&#125; string 中的查询操作 set set跟vector差不多，它跟vector的唯一区别就是，set里面的元素是有序的且唯一的，只要你往set里添加元素，它就会自动排序，而且，如果你添加的元素set里面本来就存在，那么这次添加操作就不执行。要想用set先加个头文件set。 常用的操作 set 和 map 都是有序且没有重复元素的。如果不强调有序，那么是可以使用 unordered_map 和 unordered_set 这两个进行定义的。 1234567891011121314151617181920212223# 访问容量empty() //判断set容器是否为空size() //返回当前set容器中的元素个数max_size() //返回set容器可能包含的元素最大个数# 元素的访问begin() //返回set容器的第一个元素end() //返回set容器的最后一个元素# 修改erase(iterator) //删除定位器iterator指向的值erase(first,second) //删除定位器first和second之间的值erase(key_value) //删除键值key_value的值insert(key_value) //将key_value插入到set中（比较常见）insert(first,second) //将定位器first到second之间的元素插入到set中# 使用迭代器进行遍历set&lt;int&gt;::iterator it; //声明迭代器for(it = s.begin(); it!=s.end(); it++) //迭代输出 &#123; cout&lt;&lt;*it&lt;&lt;endl; &#125; list map 查看是否存在某个key，可以使用 cout 判断。 12345678if (m1.count("Lee"))&#123; cout &lt;&lt; "Lee is in m1!" &lt;&lt; endl;&#125;else&#123; cout &lt;&lt; "Lee do not exist!" &lt;&lt; endl;&#125; 对于map 或者是 unordered_map的遍历。在C++ 11中使用 iterator 的思想进行遍历，其中这里使用到了关键词auto。 1234567891011121314#include&lt;iostream&gt;#include&lt;map&gt;using namespace std;int main()&#123; map&lt;int, string&gt; map_; for(auto it =map_.begin(); it!= map_.end(); it ++) &#123; int key =it-&gt;first; # map 就是一个结构体 string &amp; val =it-&gt;second; cout &lt;&lt; key &lt;&lt; " "&lt;&lt; val&lt;&lt; endl; &#125; return 0;&#125; C++ STL快速入门 介绍 C++ 中标准模板库 (standard template libary) STL C++ 中的大根堆和小根堆 使用 C++ 中的优先队列实现 大根堆和小根堆.与前面FIFO结构的队列不同，优先队列中元素出队列的顺序由元素的优先级决定。从优先队列中删除元素是根据优先权高或低的次序，而不是元素进入队列的次序。对于最大优先队列（max priority queue），查找操作用来搜索优先权最大的元素，删除操作用来删除该元素。优先权队列中的元素可以有相同的优先权，查找与删除操作可根据任意优先权进行。(为什么使用队列来实现大根堆和小根堆呢？ 因为队列的出队和入队顺序和大根堆弹出和加入的顺序类似) 123priority_queue&lt;Type, Container, Functional&gt;Type为数据类型， Container为保存数据的容器，Functional为元素比较方式。如果不写后两个参数，那么容器默认用的是vector，比较方式默认用operator&lt;，也就是优先队列是大顶堆，队头元素最大。 大根堆 12345//构造一个空的优先队列（此优先队列默认为大顶堆）priority_queue&lt;int&gt; big_heap; //另一种构建大顶堆的方法priority_queue&lt;int,vector&lt;int&gt;,less&lt;int&gt; &gt; big_heap2; 小根堆 12//构造一个空的优先队列,此优先队列是一个小顶堆priority_queue&lt;int,vector&lt;int&gt;,greater&lt;int&gt; &gt; small_heap; 意思是&lt;类型,&lt;存储方式&gt;,&lt;比较函数&gt; &gt; 注意如果使用 &gt; less 和 &gt; greater ，那么是需要包含头问题： 1234567891011#include &lt;functional&gt;/** addition equal_to 相等 not_equal_to 不相等 less 小于 greater 大于 less_equal 小于等于 greater_equal 大于等 这些在所有的排序算法中同样适用 */ c++中1234char ch =&apos;a&apos;;string str =&quot;a&quot;;// 注意单引号和双引号的区别 C++ queue 和 deque的区别 queue函数 一种先进先出(FIFO)的数据结构。 常见的操作 123456789101112// 访问函数front() 返回第一个元素back() 返回最后一个元素// 修改函数pop() 删除第一个元素push() 在末尾加入一个元素// 判断函数empty() 如果队列空则返回真size() 返回队列中元素的个数 deque是双向队列Double ended queue, 可以在两段扩展和收缩，有着和 vector 相似的性质，但并不是严格连续存储的。常见的操作如下: 1234567891011121314151617181920// 这里面存储的是index 信息，而不是数值信息// 增加函数void push_front(const T&amp; x):双端队列头部增加一个元素Xvoid push_back(const T&amp; x):双端队列尾部增加一个元素x// 删除函数void pop_front():删除双端队列中最前一个元素void pop_back():删除双端队列中最后一个元素Iterator erase(iterator it):删除双端队列中的某一个元素void clear():清空双端队列中的元素// 判断函数bool empty() const:向量是否为空，若true,则向量中无元素// 大小函数Int size() const:返回向量中元素的个数 简单的实现模板。 123456789101112131415161718192021222324#include&lt;iostream&gt;#include&lt;stack&gt;#include&lt;queue&gt;using namespace std;int main()&#123; stack&lt;double&gt; s; queue&lt;double&gt; q; for(int i =0; i&lt;10 ; i++) s.push(i); while ( !s.empty()) &#123; cout &lt;&lt; s.top()&lt;&lt;endl; s.pop(); &#125; for(int i =0; i&lt;10; i++) q.push(i); while ( !q.empty()) &#123; cout&lt;&lt; q.front()&lt;&lt; endl; q.pop(); &#125; return 0;&#125; queue 是单向队列，只能在尾部插入，在头部删除， 一般使用 vector 这类数组进行实现，连续空间。 deque 是双向队列，可以在头部和尾部进行删除和插入操作，一般存储地址不是连续的。 迭代器(iterator)是一中检查容器内元素并遍历元素的数据类型。 123456789101112131415#include &lt;vector&gt;#include &lt;iostream&gt;using namespace std;int main()&#123; vector&lt;int&gt; ivec; ivec.push_back(1); ivec.push_back(2); ivec.push_back(3); ivec.push_back(4); for(vector&lt;int&gt;::iterator iter = ivec.begin();1. iter != ivec.end(); ++iter) cout &lt;&lt; *iter &lt;&lt; endl; return 0;&#125; set, map, multiset, multimap, 基于平衡二叉树（红黑树），动态维护有序序列set/multisetbitset （对这个讲解没有很大的感觉） map和set容器中，一个键只对应一个实例。而在multimap和multiset中，一个键可以对应多个实例，例如每个人都有一个电话联系人列表，列表中肯定不止一个人。 有序关联容器 (按 key 值排序) map: key-value pairs set: 只有 key 值 multimap: 允许 key 重复的 map，定义在中 multiset: 允许 key 重复的 set，定义在中 无序关联容器 unordered_map: 通过 hash 组织的 map unordered_set: 通过 hash 组织的 set unordered_multimap: 通过 hash 组织的 multimap，定义在中 unordered_multiset: 通过 hash 组织的 multiset，定义在中 cpp 中栈和队列 需要在头文件中 1#include&lt;stack&gt; 基本操作：123456stack的基本操作有：对于stack&lt;int&gt; s 入 栈： s.push(x); 出 栈： s.pop(); //出栈只是删除栈顶元素，并不返回该元素访问栈顶元素： s.top();判 断 栈 空： s.empty();//栈空时，返回true栈中的元素个数：s.size(); 使用队列 1#include&lt;queue&gt; 常见的操作123456 入 队：q.push(x); // 将x 接到队列的末端。 出 队：q.pop(); //弹出队列的第一个元素，注意，并不会返回被弹出元素的值。访问队首元素：q.front(); //即最早被压入队列的元素。访问队尾元素：q.back(); //即最后被压入队列的元素。 判断队列空 q.empty(); //当队列空时，返回true。队列中的元素个数：q.size(); pair 的实现： pair的实现是一个结构体，主要的两个成员变量是first second 因为是使用struct不是class，所以可以直接使用pair的成员变量。 初始化和赋值：使用 {} 和 make_pair创建的是一个pair对象。 访问方式：pair是单个数据对的操作，pair是一struct类型，有两个成员变量，通过first,second来访问，用的是“.”访问。 pair的使用范围： STL中的map就是将key和value放在一起来保存。 另一个应用是，当一个函数需要返回2个数据的时候，可以选择pair。 内存管理 内存分配方式 在C++中，内存分成5个区，他们分别是堆、栈、自由存储区、全局/静态存储区和常量存储区。 栈：在执行函数时，函数内局部变量的存储单元都可以在栈上创建，函数执行结束时这些存储单元自动被释放。栈内存分配运算内置于处理器的指令集中，效率很高，但是分配的内存容量有限。 堆：就是那些由 new分配的内存块，他们的释放编译器不去管，由我们的应用程序去控制，一般一个new就要对应一个 delete。如果程序员没有释放掉，那么在程序结束后，操作系统会自动回收。 自由存储区：就是那些由malloc等分配的内存块，他和堆是十分相似的，不过它是用free来结束自己的生命的。 全局/静态存储区：全局变量和静态变量被分配到同一块内存中，在以前的C语言中，全局变量又分为初始化的和未初始化的，在C++里面没有这个区分了，他们共同占用同一块内存区。 常量存储区：这是一块比较特殊的存储区，他们里面存放的是常量，不允许修改。 样例分析： 12345678910111213141516#include&lt;iostream&gt;using namespace std;int a = 0; //全局初始化区char *p1; //全局未初始化区int main()&#123; int b; //栈 char s[] = "abc"; //栈 char *p2; //栈 char *p3 = "123456"; // 123456\0在常量区，p3在栈上。 static int c =0; //全局(静态)初始化区 p1 = (char *)malloc(10); p2 = (char *)malloc(20); 12: //分配得来得10和20字节的区域就在堆区。 strcpy(p1, "123456"); //123456\0放在常量区，编译器可能会将它与p3所指向的"123456"优化成一个地方。 return 0;&#125; a、代码区(code area) 存放函数体(类成员函数、全局函数)的二进制代码。b、全局区(data area) 静态存储区，存放全局变量、静态变量，初始化变量的在一块区域(低地址区域)，未初始化变量在另一块区域(高地址区域BSS)。文字常量、字符串常量，程序结束后由系统释放。c、堆区(heap area) 由低地址向高地址增长。一般new、malloc分配，由程序员分配和释放，分配方式类似于链表。e、栈区(stack area) 由高地址向低地址增长。存放函数形参、局部变量、返回值等，由编译器自动分配、释放。 （堆和自由存储区其实不过是同一块区域，new底层实现代码中调用了malloc，new可以看成是malloc智能化的高级版本） 堆和栈的区别？ （这里和数据结构中的堆栈是不一样的） 空间大小：一般来讲在32位系统下，堆内存可以达到4G的空间，从这个角度来看堆内存几乎是没有什么限制的。但是对于栈来讲，一般都是有一定的空间大小的，例如，在VC6下面，默认的栈空间大小是1M（可以修改）。 生长方向：对于堆来讲，生长方向是向上的，也就是向着内存地址增加的方向；对于栈来讲，它的生长方向是向下的，是向着内存地址减小的方向增长。 碎片问题：对于堆来讲，频繁的new/delete势必会造成内存空间的不连续，从而造成大量的碎片，使程序效率降低。对于栈来讲，则不会存在这个问题，因为栈是先进后出的队列 管理方式不同 对于栈来讲，是由编译器自动管理，无需我们手工控制;对于堆来说，释放工作由程序员控制，容易产生memory leak。 malloc / free 和 new/ delete 的关系 malloc与free是C++/C语言的标准库函数，new/delete是C++的运算符。它们都可用于申请动态内存和释放内存。对于非内部数据类型的对象而言，光用maloc/free无法满足动态对象的要求。 对象的动态内存分配 123456789101112131415161718192021#include &lt;iostream&gt;using namespace std; class Box&#123; public: Box() &#123; cout &lt;&lt; "调用构造函数！" &lt;&lt;endl; &#125; ~Box() &#123; cout &lt;&lt; "调用析构函数！" &lt;&lt;endl; &#125;&#125;; int main( )&#123; Box* myBoxArray = new Box[4]; delete [] myBoxArray; // 删除数组 return 0;&#125; 如果要为一个包含四个 Box 对象的数组分配内存，构造函数将被调用 4 次，同样地，当删除这些对象时，析构函数也将被调用相同的次数（4次）。 C/C++内存管理详解 两者的区别： malloc的全称是memory allocation,中文叫做：动态内存分配。注意事项：1). 申请内存空间后，必须检查是否分配成功；2). 当不需要再使用申请的内存时，记得释放；释放后应该把指向这块内存的指针指向NULL，以防后面的程序不小心使用了野指针3). malloc和free应该配对使用；释放只能释放一次，若释放两次或更多会出现错误，（释放空指针例外，释放空指针其实等于啥都没做，释放空指针多少次都没有问题）4). malloc从堆里面获得内存；函数返回的指针指向堆里面的一块内存。操作系统中有一个记录空闲内存地址的链表，当操作系统收到程序的申请时，就会遍历链表，然后寻找第一个空间大于所申请空间的堆结点，然后将该结点从空闲结点链表中删除，并将该结点分配给程序。 new运算符 c++中，用new和delete动态创建和释放数组或者单个对象。动态创建对象时，只需要指定其数据类型，而不必为该对象命名。 c++中，用new和delete动态创建和释放数组或者单个对象。 动态创建对象时，只需要指定其数据类型，而不必为该对象命名。 1） new运算符返回指向该创建对象的指针。 我们可以通过指针来访问此对象。int *pi = new int ;此new表达式在堆区中分配创建了一个整型对象，并返回此对象的地址，并用该地址初始化指针pi。 2） 动态创建对象的初始化 2.1） 动态创建的对象可以用初始化变量的方式初始化。 int * pi = new int (100);//指针pi所指向的对象初始化为100 string ps = new string(10,’9’)//ps为“9999999999” 如果不提供显示初始化，对于类类型，用该类的默认构造函数初始化；而内置类型的对象则无初始化。 2.2） 可以对动态创建的对象做值初始化： int *pi = new int ();//初始化为0； int *pi = new int ;//pi指向一个没有初始化的int string *ps = new string();//初始化为空字符串（对于提供了默认构造函数的类型，没有必要对其对象进行值初始化） 3）delete撤销动态创建的对象 delete表达式释放指针指向的地址空间； delete pi；//释放单个对象 delete [ ] pi;//释放数组 如果指针指向的不是new分配的内存地址，则使用delete是不合法的。 4） delete之后，重设指针的值 delete p；执行完该语句后，p变成了不确定的指针，尽管p值没有明确定义，但仍然存放了它之前所指对象的地址，然后p所指的内存已经被释放了，所以p不再有效。此时，该指针变成了悬垂指针（悬垂指针指向曾经存放对象的内存，但该对象已经不存在了）。悬垂指针往往导致程序错误，而且很难检测出来。 一旦删除了指针所指的对象，立即将指针置为0，这样就可以非常清楚的指明指针不再指向任何对象。（零值指针：int * ip =0;） 3 malloc和new的区别（1）new 返回指定类型指针，并且可以自动计算所需要的大小；malloc需要手动计算字节数，并且在返回后强制类型转换为实际类型的指针。 （2）malloc只管分配内存，并不能对所得到的内存进行初始化，所以得到的一片新内存中，其值将是随机的；new不仅分配内存，还对内存中的对象进行初始化;free只管释放内存；delete不仅释放内存，还会调用对象的析构函数，销毁对象。 （3） malloc/free是c++/c的标准库函数，头文件为stdlib.h;而new/delete是c++的运算符。他们都可用于申请动态内存和释放内存。 （4） 对于非内部数据结构的对象而言，光用malloc/free无法满足动态对象的要求，对象在创建的同时，还要自动执行构造函数，对象在消亡之前要自动执行析构函数，由于malloc/free是库函数而不是运算符，不在编译器控制权限之内，不能够把执行构造函数和析构函数的任务强加于malloc/free;因此，c++语言需要一个能够完成动态内存分配和初始化的运算符new，以及一个能完成清理与释放内存工作的运算符delete；我们不要企图用malloc和free来完成动态对象的内存管理，应该用new/delete。由于内部数据类型没有构造和析构过程，对他们而言malloc/free和new/delete是等价的。 new/delete的功能完全覆盖了malloc/free,为什么还要malloc和free了？这是因为c++程序经常要调用c函数，而c程序只能用malloc和free来管理动态内存。 如果用free释放new创建的动态对象，那么该对象因无法执行析构函数而可能导致程序出错。如果用delete释放掉malloc申请的动态内存，结果也会导致程序出错，结果理论上应该可以，但是程序的可读性很差。所以new/delete必须配对使用，malloc和free也一样。 malloc和new的区别 对于c++ 中malloc 的理解 在C++ STL中，vector是用内建（build-in）的动态数组（dynamic array）实现的。动态数组是相对于静态数组（数组大小不变）而言的，在C++中普通的数组的大小是固定的，在初始化的时候就确定了，使用的时候不能超过其范围。 静态数组是在栈上分配的空间 1int a[100] = &#123;0&#125;; 动态数组是在堆上分配的空间1auto *p = new int(100); memset(p, 0, sizeof(int)); 一般来说，如果重新分配，那么是之前的二倍。1234567function insertEnd(dynarray a, element e) if (a.size = a.capacity) // resize a to twice its current capacity: a.capacity ← a.capacity * 2 // (copy the contents to the new memory location here) a[a.size] ← e a.size ← a.size + 1 当不够的时候，需要考虑分配，释放，复制和析构等步骤。这些过程会大大影响系统的性能。那么如何避免vector在使用中频繁的分配？ 主要有两种方式。 使用reserve()保留适当容量: 12std::vector&lt;int&gt; v; //size 0,capacity 0v.reserve(80); //size 0,capacity 80 初始化期间就向构造函数传递附加参数，构造出足够的空间: 1std::vector&lt;T&gt; v(5); //size 0,capacity 80 对象数据和函数的存储方式#我们知道，用类去定义对象，系统会为每一个对象分配存储空间 每个对象占用存储空间的只是该对象的数据部分（虚函数指针和虚基类指针也属于数据部分），函数代码属于公用部分 你可能会问：静态成员函数和非静态成员函数都是在类的定义时放在内存的代码区的，因而可以说它们都是属于类的，但是类为什么只能直接调用静态类成员函数，而非静态类成员函数(即使函数没有参数)只有类对象才能调用呢 原因是：类的非静态类成员函数其实都内含了一个指向类对象的指针型参数(即this指针)，因此只有类对象才能调用(此时this指针有实值) 虚函数表原理简述C++实现虚函数的方法是：为每个类对象添加一个隐藏成员，隐藏成员保存了一个指针，这个指针叫虚表指针（vptr），它指向一个虚函数表（virtual function table, vtbl） 虚函数表就像一个数组，表中有许多的槽（slot），每个槽中存放的是一个虚函数的地址（可以理解为数组里存放着指向每个虚函数的指针） 即：每个类使用一个虚函数表，每个类对象用一个虚表指针 C++的编译器会保证虚函数表的指针存在于对象实例中最前面的位置（为了保证取虚函数表有最高的性能，在有多层继承或是多重继承的情况下），这意味着我们通过对象实例的地址得到这张虚函数表的地址，然后就可以遍历其中函数指针，并调用相应的函数 我们建立一个新类 1234567class Base &#123;public: virtual void f() &#123; cout &lt;&lt; "Base::f" &lt;&lt; endl; &#125; virtual void g() &#123; cout &lt;&lt; "Base::g" &lt;&lt; endl; &#125; virtual void h() &#123; cout &lt;&lt; "Base::h" &lt;&lt; endl; &#125;&#125;; 按照上面的说法，我们可以通过Base的实例来得到虚函数表，这个表（数组）存了指向f，g，h这三个函数的指针123456789101112131415typedef void(*Fun)(void);int main()&#123; Base bObj; Fun pFun = NULL; //指向void* pf(void)类的函数的指针pFun cout &lt;&lt; "虚函数表的地址：" &lt;&lt; (int*)(&amp;bObj) &lt;&lt; endl; cout &lt;&lt; "虚函数表的第一个函数地址：" &lt;&lt; (int*) * (int*)(&amp;bObj) &lt;&lt; endl; //再次取址得到第一个虚函数的地址 //第一个虚函数 pFun = (Fun) * ((int*) * (int*)(&amp;bObj)); pFun();&#125; （1）计算内存大小 c++中最重要的就是类，那么一个类的对象，它在内存中如何存储的？它占内存中多少个字节？首先确定类的构成：1) 数据成员：可以是内置类型，类类型；2) 函数成员：虚函数，非虚函数1）数据成员内置类型对齐原则 （对齐原则有两个维度，一个是在计算过程中，如果出现了前小后大，那么前面需要和后面对齐；最后就算完内存使用量之后，也是需要和类中最长的进行对齐。主要是为了方便存储。可以通过看下面链接中的实例 好好理解.） 内置类型就是常用的：char,short,long,int,float,double.这些内置类型在类的对象中对齐方式，字节为单位（在c 中结构体也是一样的）123456char 1short 2long 4int 4float 4fouble 8 类类型对齐原则（c 中就是结构体对齐原则）取类中最长的数据成员作为对齐原则。例如，类中最长为 double,那么就是8 个字节。2）函数成员函数成员是不占用内存中类的对象的字节。为什么呢，你可以这样理解，c++中为了兼容c也允许struct 作为类的声明。在c 中struct 是用来声明结构体类型的，只不过c 中的结构体没有函数成员。同样 c++中允许的函数成员，只不过是类给函数提供了一个作用域 上面所讲的其实都是关于sizeof()对于字符相关变量的操作，字符操作还有另一个常用的函数strlen()。前者是包括 \0 ，后者不包括。 12string str ="abc";sizeof(str) 本身大小是固定的，由编译器决定，不随着后面真实值的变化而变化。是一个常量。 派生类需要加上父类中的变量，因为前者是可以访问上述变量的。 （2）虚表和多重继承 首先说明一下多继承和多重继承是不一样的。多继承：A1，A2 是两个base 类，然后B 继承了A1 和A2。多重继承：A是base 类，A1，A2继承了A，然后B 继承了 A1，AA2，所以对于B 来说就是多重继承。 123456789101112131415161718192021222324252627282930313233343536373839404142434445class A&#123;public: virtual void fun1() &#123; cout&lt;&lt;"A::fun1"&lt;&lt;endl; &#125; virtual void fun2() &#123; cout&lt;&lt;"A::fun2"&lt;&lt;endl; &#125; virtual void fun3() &#123; cout&lt;&lt;"A::fun3"&lt;&lt;endl; &#125;&#125;;class A1:public A&#123;public: virtual void fun1() &#123; cout&lt;&lt;"A1::fun1"&lt;&lt;endl; &#125;&#125;;class A2:public A&#123;public: virtual void fun2() &#123; cout&lt;&lt;"A2::fun2"&lt;&lt;endl; &#125; virtual void fun3() &#123; cout&lt;&lt;"A2::fun3"&lt;&lt;endl; &#125;&#125;;class B :public A1,public A2&#123;public: virtual void fun4() &#123; cout&lt;&lt;"B::fun4"&lt;&lt;endl; &#125;&#125;; 将上述中的虚函数表总结如下： 注意事项： 如果子类没有重写父类的虚函数，那么子类虚函数 置于虚函数表的后面，并按声明顺序存放。如果子类函数有覆盖的情况，那么虚函数表也有相应的覆盖情况，详情见参考文献3上图中的最后的B类（多继承了 A1和A2），两个子表表示多继承，一个子表中可能发生多重继承。 第一个图表示单继承，第二个图表示多继承。都是从当前类的虚函数表的一个槽中引出来的。 参考文献： 1). C++类的存储及类对象内存结构2). C++ 虚函数表研究 （二） 多重继承3). C++进阶之虚函数表 指针和引用在C++中, 如果要对某个存储单元进行访问(读取/写入), 有两种方式, 一是通过变量名找到存储地址再进行访问, 二是直接通过存储地址进行访问。 通过变量名进行访问通过变量名进行访问就是我们通过编译器起一个名字, 程序在运行时, 系统为该程序建立一个变量名与内存单元地址之间的映射表, 通过名字即可找到该存储单元的地址并进行操作。 通过地址直接访问显然, 通过地址直接进行访问要比经过一次 “中转” 速度更快, 既然要通过地址进行直接访问那么就必须要知道这个地址的值(编号)是多少, 并且要把这个地址值(编号)给保存起来, 这样才能方便后来随时直接访问这个地址。 什么是指针？ 指针是一个变量，其值为另一个变量的地址，即，内存位置的直接地址。指针变量声明的一般形式为： 1type *var-name; 定义时, 其号的位置可以靠左( int pa; ), 居中( int pa; )或靠右( int pa; ), 具体使用哪种形式可根据个人习惯。 指针即为地址，指针几个字节跟语言无关，而是跟系统的寻址能力有关，比如在16位机器上，指针是2字节；32位上，指针是4字节，现在在 64位上，就是 8字节。 123456789101112131415#include &lt;stdio.h&gt; int main(void)&#123; int a=1; char b='a'; float c=1.0; void *p; p=&amp;a; printf("a的地址为：0x%x，其字节数为：%d\n",p,sizeof(p)); p=&amp;b; printf("b的地址为：0x%x，其字节数为：%d\n",p,sizeof(p)); p=&amp;c; printf("c的地址为：0x%x，其字节数为：%d\n",p,sizeof(p)); return 0;&#125; 指针的初始化与赋值 不一定声明的时候马上初始化，但是没有初始化的时候一定是不能访问的，否则就是野指针。因为在声明之后，系统会随机分配一个地址，如果这个时候访问其值，重新赋值，那么结果是不可预测的。 我们就来看两个基本的运算符:&amp;(取址运算符)和(间接访问运算符/解引用指针)在C++中, 用 ‘&amp;’ 符号来获取变量的地址; 这是因为 a 与 pa 实际上都是指向同一个存储地址, *号为间接访问符, 表示要访问指针变量中所存放的内存地址。 123456789#include &lt;iostream&gt;int main()&#123; int a = 10 ; int *pa = &amp;a ; //定义指针变量pa并初始化地址为变量a在内存中所在的地址 std::cout&lt;&lt; a &lt;&lt; "\n" ; //通过变量名访问a变量所在的地址 std::cout&lt;&lt; *pa &lt;&lt; "\n" ; //运用*号访问指针变量pa中所保存的地址 return 0 ;&#125; 种类 常量指针 不能把常量指针赋值给非常量指针。原因如下：定义常量指针的初衷是不能通过常量指针修改其指向的内容。如果把常量指针赋值给非常量指针，就可以通过非常量指针修改常量指针指向的内容。123456void MyPrintf(const char *p)&#123; //strcpy函数原形第一个参数为char*，由于不能将常量指针赋值给非常量指针，因此编译报错 strcpy(p,"this");//编译出错 printf("%s",p);&#125; 指针数组 1234 int var[MAX]=&#123;10,100,200&#125;; int *ptr[MAX]; //把 ptr 声明为一个数组，由 MAX 个整数指针组成。因此，ptr 中的每个元素，都是一个指向 int 值的指针。 ptr[i] = &amp;var[i]; // 赋值为整数的地址 cout&lt;&lt;*ptr[i]&lt;&lt;endl; // 10 100 200 函数指针 程序运行期间，每个函数都会占用一段连续的内存空间。而函数名就是该函数所占内存区域的起始地址（入口地址）。我们可以将函数的入口地址赋给一个指针变量，使该变量指向该函数。 12345678910111213141516171819#include &lt;iostream&gt;#include &lt;vector&gt;#include &lt;sys/stat.h&gt;#include &lt;unistd.h&gt;#include &lt;string&gt;#include &lt;fstream&gt;#include &lt;cstddef&gt;void print_smaller(int a,int b)&#123; std::cout &lt;&lt; ((a &lt; b) ? a:b )&lt;&lt; std::endl;&#125;int main()&#123; /*等价于 void (*fn)(int ,int ) = print_smaller;*/ void (*fn)(int ,int ); fn = print_smaller; fn(3,4); return 0;&#125; 函数指针 使用typedef定义函数指针1typedef int64_t int64; 传递指针给函数 声明函数参数为指针类型即可。 1double getAverage(int *arr, int size); //arr表示一个数组 NULL指针和void* NULL指针是一种非常特殊的指针(废话,不然单独说它干嘛?),不指向任何东西.表示不指向任何东西的指针。有一个非常重要的是,因为NULL不指向任何地方,所以,也就肯定不能够解引用了.在C++11中,新引入了一种特殊类型的字面值nullptr来初始化指针为空指针.他能够被转换成任何类型的指针. Void*是一种特殊类型的指针,能够用来存放任何类型对象的地址.通俗来说,就是我不知道这个指针指向的是什么类型的对象. 1234567891011121314151617#include &lt;iostream&gt;int fun(int num)&#123; return num+10;&#125;int main()&#123; double x=25.5; //普通指针的话类型要严格保证 double *p=&amp;x; //void* 类型可以接受任意类型对象地址 void *p_v=&amp;x; void *p_v2=p; std::cout&lt;&lt;"p_v:"&lt;&lt;p_v&lt;&lt;std::endl; std::cout&lt;&lt;"p_v2:"&lt;&lt;p_v2&lt;&lt;std::endl;&#125; 指针的指针 12345678910#include &lt;iostream&gt;int main()&#123; int a=10; int *p_a=&amp;a; int **pp_a=&amp;p_a; std::cout&lt;&lt;"p_a:"&lt;&lt;p_a&lt;&lt;std::endl&lt;&lt;"*p_a:"&lt;&lt;*p_a&lt;&lt;std::endl; std::cout&lt;&lt;std::endl; std::cout&lt;&lt;"PP_a:"&lt;&lt;pp_a&lt;&lt;std::endl&lt;&lt;"*pp_a:"&lt;&lt;*pp_a&lt;&lt;std::endl&lt;&lt;"**pp_a"&lt;&lt;**pp_a&lt;&lt;std::endl;&#125; 解引用的时候,带上两个星号,就回到的最开始的那个变量.这么说有点模糊.详细来说,*pp_a因为的从右向左的结合性,这个表达式可以写成(pp_a),那么我们知道pp_a存放的是p_a的地址,pp_a就是表示p_a这个地址中存放的内容即a的地址(不能晕!!!).那么(pp_a)就相当于p_a或者a. 指针数组和数组指针 指针数组：表示的是一个数组，数组中每一个变量都是指针型变量； 数组指针：表示的是一个指针类型的变量，这个指针变量指向的是一个数组。 12int *p1[10];int (*p2)[10]; 占用内存分析 12int *(p1[5]); //指针数组，可以去掉括号直接写作 int *p1[5];int (*p2)[5]; //二维数组指针，不能去掉括号 指针数组是数组，占用空间为 单个元素字节数 × 元素个数，p1在32位环境下占4×5 = 20个字节；数组指针是指针，占用空间为指针占用空间，p2在32位环境下占用4个字节。（所以如何使用 数组指针存储最后的结果，那么占用的大小就是常量级别的。） 1“[]”的优先级比“”要高。p1 先与“[]”结合，构成一个数组的定义，数组名为p1，int *修饰的是数组的内容，即数组的每个元素。那现在我们清楚，这是一个数组，其包含10 个指向int 类型数据的指针，即指针数组。至于p2 就更好理解了，在这里“（）”的优先级比“[]”高，“”号和p2 构成一个指针的定义，指针变量名为p2，int 修饰的是数组的内容，即数组的每个元素。数组在这里并没有名字，是个匿名数组。那现在我们清楚p2 是一个指针，它指向一个包含10 个int 类型数据的数组，即数组指针。我们可以借助下面的图加深理解： 给一个例子 int *a[4]; //指针数组 指针数组就相当于一次声明了多个指针。数组的每一个元素都是一个指针。[ ]的优先级高于，表示a是一个数组，元素为int ，个数为4。 int (*p)[4]; //数组指针 数组指针就相当于一次声明了一个指针。只不过这个指针指向很特别，是一个数组。()的优先级高于[]，表示p是一个数组，类型是为int [4] ，步长为4。下标区别：指针数组的下标表示数组的元素个数，数组指针的表示指针的步长 很多时候，使用指针数组来控制程序可以节约内存空间，也可以节约时间。 好好体会一下数组指针 指针数组 用法 区别 中的两个例子。 通过行指针访问二维数组的元素 123符号*：取内容，对指针取内容，就是把指针指向的东西取出来符号+：向前递进，如p+1就是向前递进一个元素的位置符号[]：向前递进并取内容。例如p[2]，即把原来的p先递进2个元素，再把它读出来，等价*(p+2)。 我们先给出指向第二行第三个元素的表达式： 12345p[1][2];*(p+1)[2];*(*(p+1)+2); strlen与sizeof的区别 sizeof：1、计算所有变量类型的占用字节大小2、在计算字符串的时候，会将字符串后面的’\0’的大小也计算上来。3、计算的是字节内存的大小4、计算数组名的时候特殊，会计算整个数组的长度。其他的均计算单个变量 strlen：1、计算的是字符串的长度大小3、计算字符串长度时，将会忽略’\0’结束符，遇到’\0’字符就会结束。 12345678910111213141516#include&lt;iostream&gt;#include&lt;string&gt;#include&lt;vector&gt;using namespace std;int main()&#123; char str[] ="world"; cout &lt;&lt; sizeof(str)&lt;&lt;":"; cout &lt;&lt; strlen(str)&lt;&lt;":"; char *p =str; cout &lt;&lt; sizeof(p) &lt;&lt;":"; char i =10; cout &lt;&lt; sizeof(i) &lt;&lt;":"; // 输出的结果 6:5:8:1: return 0;&#125; sizeof 会包含 \0 的长度， 但是strlen() 不会。但是当申请固定长度的 char 数组，那么sizeof 就不包括了\0，可以查看下面的例子。但是strlen 一直是有效的长度。12345678910111213141516#include&lt;iostream&gt;#include&lt;string&gt;#include&lt;vector&gt;using namespace std;int main()&#123; char str[10] ="world"; cout &lt;&lt; sizeof(str)&lt;&lt;":"; cout &lt;&lt; strlen(str)&lt;&lt;":"; char *p =str; cout &lt;&lt; sizeof(p) &lt;&lt;":"; char i =10; cout &lt;&lt; sizeof(i) &lt;&lt;":"; // 输出的结果 10:5:8:1: return 0;&#125; 利用malloc分配内存 如果想将某一个字符串复制到某一个控件，一定要记得分配足够的空间大小，不要忘记”\0”结束符。 比如12345678char * strSrc = "Hello Boy."; char * strDest; //错误，strlen并未计算"\0"结束符,赋值后的指针末尾指向未知空间。 strDest = (char *) malloc(strlen(strSrc)); //正确，为"\0"留出空间。 strDest = (char *) malloc(strlen(strSrc)+1); strcpy(strDest,strSrc); C++引用 引用变量是一个别名，也就是说，它是某个已存在变量的另一个名字。一旦把引用初始化为某个变量，就可以使用该引用名称或变量名称来指向变量。 引用很容易与指针混淆，它们之间有三个主要的不同：不存在空引用。引用必须连接到一块合法的内存。一旦引用被初始化为一个对象，就不能被指向到另一个对象。指针可以在任何时候指向到另一个对象。引用必须在创建时被初始化。指针可以在任何时间被初始化。 int i=17;int&amp; r=i; //&amp;读作引用，r 是一个初始化为 i 的整型引用cout&lt;&lt;i&lt;&lt;endl; //5cout&lt;&lt;r&lt;&lt;endl; //5 引用通常用于函数参数列表和函数返回值。 把引用作为参数。 C++ 支持把引用作为参数传给函数，这比传一般的参数更安全 12345678910111213141516171819202122232425#include &lt;iostream&gt;using namespace std;// 函数声明void swap(int&amp; x, int&amp; y);int main ()&#123; // 局部变量声明 int a = 100; int b = 200; cout &lt;&lt; "交换前，a 的值：" &lt;&lt; a &lt;&lt; endl; cout &lt;&lt; "交换前，b 的值：" &lt;&lt; b &lt;&lt; endl; /* 调用函数来交换值 */ swap(a,b); cout &lt;&lt; "交换后，a 的值：" &lt;&lt; a &lt;&lt; endl; //200 cout &lt;&lt; "交换后，b 的值：" &lt;&lt; b &lt;&lt; endl; //100 return 0;&#125;void swap(int&amp; x, int&amp; y)&#123; int temp; temp = x; /* 保存地址 x 的值 */ x = y; /* 把 y 赋值给 x */ y = temp; /* 把 x 赋值给 y */ return;&#125; 单独整理，需要有代码加持 内联函数函数调用是有开销的。如果函数本身只有几条语句，执行非常快，而且函数被反复执行很多次，相比之下调用函数所产生的这个开销就会显得比较大。为了减少函数调用的开销，引入了内联函数机制。编译器处理对内联函数的调用语句时，是将整个函数的代码插入到调用语句处，而不会产生调用函数的语句。在函数定义前面加inline关键字，即可定义内联函数。 123456789class B&#123; inline void func1(); void func2() &#123; &#125;&#125;;void B::func2()&#123;&#125; 友元函数 This approach may be used in friendly function when a function needs to access private data in objects from two different class. This may be accomplished in two similar ways:a function of global or namespace scope may be declared as friend of both classesa member function of one class may be declared as friend of another one. 12345678910111213141516171819202122232425262728293031323334353637383940414243#include &lt;iostream&gt;using namespace std; class Foo; // Forward declaration of class Foo in order for example to compile.class Bar &#123; private: int a; public: Bar(): a(0) &#123;&#125; void show(Bar&amp; x, Foo&amp; y); friend void show(Bar&amp; x, Foo&amp; y); // declaration of global friend&#125;; class Foo &#123; private: int b; public: Foo(): b(6) &#123;&#125; friend void show(Bar&amp; x, Foo&amp; y); // declaration of global friend friend void Bar::show(Bar&amp; x, Foo&amp; y); // declaration of friend from other class &#125;; // Definition of a member function of Bar; this member is a friend of Foovoid Bar::show(Bar&amp; x, Foo&amp; y) &#123; cout &lt;&lt; "Show via function member of Bar" &lt;&lt; endl; cout &lt;&lt; "Bar::a = " &lt;&lt; x.a &lt;&lt; endl; cout &lt;&lt; "Foo::b = " &lt;&lt; y.b &lt;&lt; endl;&#125; // Friend for Bar and Foo, definition of global functionvoid show(Bar&amp; x, Foo&amp; y) &#123; cout &lt;&lt; "Show via global function" &lt;&lt; endl; cout &lt;&lt; "Bar::a = " &lt;&lt; x.a &lt;&lt; endl; cout &lt;&lt; "Foo::b = " &lt;&lt; y.b &lt;&lt; endl;&#125; int main() &#123; Bar a; Foo b; show(a,b); a.show(a,b);&#125; 结果输出： 1234567Show via global functionBar::a = 0Foo::b = 6Show via function member of BarBar::a = 0Foo::b = 6Program ended with exit code: 0 使用一个友元函数去访问两个类别中的priate 变量。 为什么要使用友元函数？ 在C++中，我们使用类对数据进行了隐藏和封装，类的数据成员一般都定义为私有成员，成员函数一般都定义为公有的，以此提供类与外界的通讯接口。但是，有时需要定义一些函数，这些函数不是类的一部分，但又需要频繁地访问类的数据成员，这时可以将这些函数定义为该函数的友元函数。（如果封装是一堵墙，那么友元函数就是一道门） （所以建议慎重使用）友元的作用是提高了程序的运行效率（即减少了类型检查和安全性检查等都需要时间开销），但它破坏了类的封装性和隐藏性，使得非成员函数可以访问类的私有成员。 使用方法：需要再类内进行定义申明，但是在类外进行具体的实现。 对象的大小=所有成员变量的大小之和 c++ 中输入字符串的几种方式 对于字符数组方法一：getline()读入整行数据，它使用回车键输入的换行符来确定输入结尾。调用方法: cin.getline(str, len);第一个参数str是用来存储输入行的数组名称，第二个参数len是要读取的字符数。 1234567891011#include&lt;iostream&gt;using namespace std;int main()&#123; char str[30]; cin.getline(str, 30); cout&lt;&lt; str&lt;&lt;endl; return 0;&#125; 方法二：get()调用方法：cin.get(str, len); 可以接受空格 123456789101112131415#include&lt;iostream&gt;using namespace std;int main()&#123; char str1[30], str2[30]; cin.get(str1, 30); cin.get(); // 使用get 是保留了 '\n' 换行符，所以这里需要清除一下 cin.get(str2, 30); cout&lt;&lt; str1&lt;&lt;" "&lt;&lt;endl; cout&lt;&lt; str2 &lt;&lt; " "&lt;&lt; endl; return 0;&#125; cin.getline()cin.getline()实际上有三个参数，cin.getline(接受字符串到m,接受个数5,结束字符). 当第三个参数省略时，系统默认为’\0’ 是‘/n’吧。 cin 的用法接受一个字符串，遇“空格”、“TAB”、“回车”都结束 1234567891011#include&lt;iostream&gt;using namespace std;int main()&#123; char a[20]; cin &gt;&gt;a; cout&lt;&lt; a&lt;&lt;endl; return 0;&#125; 对于 char 的支持 123456789101112#include&lt;iostream&gt;using namespace std;int main()&#123; char ch; ch =cin.get(); cout&lt;&lt;ch&lt;&lt;endl; return 0;&#125; 对于string 类 上面的cin.getline() 是类函数，在string 中还有一个函数.getline() // 接受一个字符串，可以接收空格并输出，需包含“#include” 12345678910#include&lt;iostream&gt;#include&lt;string&gt;using namespace std;int main()&#123; string str; getline(cin ,str); return 0;&#125; gets() // 接受一个字符串，可以接收空格并输出，需包含“#include” 123456789101112#include&lt;iostream&gt;#include&lt;string&gt;using namespace std;int main()&#123; char str[20]; gets(str); cout&lt;&lt; str&lt;&lt;endl; return 0;&#125; getchar() //接受一个字符，需包含“#include” 123456789101112#include&lt;iostream&gt;#include&lt;string&gt;using namespace std;int main()&#123; char ch; ch =getline(); cout&lt;&lt;ch&lt;&lt;endl; return 0;&#125; 所以总的来说，是有两套输入string 的方案的。 使用 stringstream 实现字符串的split 功能。 stringstream为字符串输入输出流，继承自iostream，灵活地使用stringstream流可以完成很多字符串处理功能，例如字符串和其他类型的转换，字符串分割等。在这里，我们使用其实现字符串分割功能。注意stingstream的使用需要包含sstream头文件。 123456789101112131415161718192021222324252627#include&lt;iostream&gt;#include&lt;string&gt;#include&lt;sstream&gt;#include&lt;vector&gt;using namespace std;vector&lt;string&gt; split(const string &amp; str,const char ch)&#123; vector&lt;string&gt; res; stringstream input(str); string tmp; while(getline(input, tmp, ch)) res.push_back(tmp); return res;&#125;int main()&#123; string str("a,b,c#,aaa"); char ch=','; vector&lt;string&gt; res =split(str, ch); for(auto u : res) &#123; cout &lt;&lt; u; cout &lt;&lt; endl; &#125; return 0;&#125; 对于 cpp 中 getline() 的详解，下面是三个参数 123input - the stream to get data fromstr - the string to put the data intodelim - the delimiter character getline 是默认以 “\n” 作为结束符号的。 12345678910#include&lt;iostream&gt;#include&lt;string&gt;using namespace std;int main()&#123; string buff; getline(cin, buff); cout &lt;&lt; buff&lt;&lt; endl; return 0;&#125; 注意这个和上面的区别，在于最后的 delim 是 ‘,’。 那么只有当出现 , 才会停止12345678910#include&lt;iostream&gt;#include&lt;string&gt;using namespace std;int main()&#123; string buff; getline(cin, buff, ','); cout &lt;&lt; buff&lt;&lt; endl; return 0;&#125; 比较好的在 cpp 中实现 12345678910111213141516171819202122232425262728#include&lt;iostream&gt;#include&lt;string&gt;#include&lt;vector&gt;#include&lt;sstream&gt;using namespace std;vector&lt;string&gt; split(string &amp; str, char ch)&#123; vector&lt;string&gt; res; stringstream input(str); string tmp; // 第一个参数需要是一个输入流，然后第二个参数是存储结果是按照 delim 分开的一个结果 //res 是经过 split 之后的结果 while(getline(input, tmp,ch)) &#123; res.push_back(tmp); &#125; return res;&#125;int main()&#123; string str; cin &gt;&gt; str; char ch =','; vector&lt;string&gt; res =split(str, ch); for(auto u : res) cout&lt;&lt; u &lt;&lt; endl; return 0;&#125; 接口和抽象类的区别接口是一个概念。它在C++中用抽象类来实现，在C#和Java中用interface来实现。 接口是专门被继承的。接口存在的意义也是被继承。和C++里的抽象类里的纯虚函数是相同的。不能被实例化。定义接口的关键字是interface，例如： 1234public interface MyInterface&#123; public void add(int x,int y); public void volume(int x,int y,int z); &#125; 继承接口的关键字是implements，相当于继承类的extends。需要注意的是，当继承一个接口时，接口里的所有函数必须全部被覆盖。当想继承多个类时，开发程序不允许，报错。这样就要用到接口。因为接口允许多重继承，而类不允许（C++中可以多重继承）。所以就要用到接口。 C++中的结构体c++98 和c++ 11 是目前使用比较广泛的两个版本。 12345678910111213141516171819#include&lt;iostream&gt;#include&lt;string&gt;#include&lt;vector&gt;using namespace std;// 声明一个结构体类型 stustruct Stu&#123; int num; // 声明结构体中的整型变量 char name[20]; //声明字符串数组 float score; // 声明一个单精度变量 int age; // 声明一个整型age&#125;student1, student2;// 声明变量 student1, student2int main()&#123; cout &lt;&lt; sizeof(Stu)&lt;&lt;endl; cout &lt;&lt; sizeof(student1) &lt;&lt; endl; cout &lt;&lt; sizeof(student2) &lt;&lt; endl; return 0;&#125; c++11 中结构体的声明和使用。 声明的时候，可以加上构造器constructor，完成了初始化。 使用的时候，如果变量本身表示一个结构体，那么需要使用 . 访问变量，比如 node.data和 node.left，但是如果变量本身是一个指针，那么需要使用 -&gt; 访问，比如 Proot-&gt;left。 12345678910111213141516#include &lt;iostream&gt;using namespace std;struct TreeNode&#123; int data; TreeNode* left; TreeNode* right; TreeNode(int val): data(val), left(nullptr), right(nullptr) &#123;&#125;&#125;;int main() &#123; TreeNode foo(5); cout &lt;&lt; "data: " &lt;&lt; foo.data &lt;&lt; ", left: " &lt;&lt; foo.left &lt;&lt; ", right: " &lt;&lt; foo.right &lt;&lt;endl; return 0;&#125; C++ 中的typedeftypedef 用来设置别名 1typedef char* PCHAR; // 一般用大写 c++ 中各种数据类型的大小 字节 (byte) 是由 8位( bit) 组成。字：“字”由若干个字节构成，字的位数叫做字长，不同档次的机器有不同的字长。例如一台8位机，它的1个字就等于1个字节，字长为8位。如果是一台16位机，那么，它的1个字就由2个字节构成，字长为16位。字是计算机进行数据处理和运算的单位。 常见的转换关系：1 byte = 8 bit1 KB = 1024 bytes = $2^{10} $ bytes1 MB = 1024 KB = $2^{20} $bytes1 GB = 1024 MB = $2^{30} $ bytesterabyte(TB) = $2^{40}$ bytes 如果使用10 进制表示，那么上面分别可以约等于 $10^3$, $10^6$ 和 $10^9$ 。 sizeof的输出单位是字节，即B，1B=8b（位），测试工具为VS2013旗舰版。 bitsetbitset是STL提供的用于记录01串的容器，也就是bitset的每个元素只能为0/1。 类似于 vector，bitset也是一种类模板。bitset 类型之间的区别在于其长度而不是类型，在定义 bitset 类型时，要明确其含有多少位，&lt;&gt; 内指定它的长度： 1234#include&lt;bitset&gt;bitset&lt;16&gt; a; //第0~15位都是0bitset&lt;6&gt; b(string("010010")); //用字符串初始化bbitset&lt;32&gt; c(0x80000000); //第0位是1，其他都是0 注意bitset声明后长度不可改变。和数组一样，bitset从0开始编号 bitset的操作 123456789a.any() //a中是否含1a.none() //a是否全为0a.count() //a中有几个1a.[pos] //访问第pos位a.test(pos) //第pos位是否为1a.set() //全部设为1a.reset() //全部清零a.flip() //全部取反a.to_ulong() //转成32位无符号整数 bitset 也是支持位操作的123456a|ba&amp;ba^b~aa&lt;&lt;1a&gt;&gt;1 bitset的原理 将一个很长的01串按64位一组划分。每组01串用一个64位无符号整数记录。bitset所有的操作都是基于对整数的位操作实现的。所以bitset的效率非常高，可以看作$O(N/64) $。 bitset 的原理 bitset主要用于状态压缩，bitset可以用每一位0或者1来表示可达性，例如图上点与点的可达性，dp时可达性 bitset主要用于暴力枚举，如果bitset的长度是n,一次与运算的复杂度就是n/32 bitset主要用于合并集合，交并补运算显示不同的逻辑意义 C/C++编译过程主要分为4个过程 1) 编译预处理 2) 编译、优化阶段 在《编译原理》中我们可以了解到一个编译器对程序代码的编译主要分为下面几个过程：a) 词法分析b) 语法分析c) 语义分析d) 中间代码生成e) 代码优化f) 代码生成g) 符号表管理h) 将多个步骤组合成趟 3) 汇编过程 汇编过程实际上指把汇编语言代码翻译成目标机器指令的过程。对于被翻译系统处理的每一个C语言源程序，都将最终经过这一处理而得到相应的目标文件。目标文件中所存放的也就是与源程序等效的目标的机器语言代码。目标文件由段组成。通常一个目标文件中至少有两个段： 代码段：该段中所包含的主要是程序的指令。该段一般是可读和可执行的，但一般却不可写。 数据段：主要存放程序中要用到的各种全局变量或静态的数据。一般数据段都是可读，可写，可执行的。 汇编语言和二级制代码是不一样的。汇编语言是最接近机器语言的编程语言。常用的命令比如：123mov a,#10000000Bmov a,#80Hmov a,#128 4) 链接程序 链接程序的主要工作就是将有关的目标文件彼此相连接，也即将在一个文件中引用的符号同该符号在另外一个文件中的定义连接起来，使得所有的这些目标文件成为一个能够诶操作系统装入执行的统一整体。根据开发人员指定的同库函数的链接方式的不同，链接处理可分为两种： （1）静态链接 在这种链接方式下，函数的代码将从其所在地静态链接库中被拷贝到最终的可执行程序中。 这样该程序在被执行时这些代码将被装入到该进程的虚拟地址空间中。静态链接库实际上是一个目标文件的集合， 其中的每个文件含有库中的一个或者一组相关函数的代码。 （2） 动态链接在此种方式下，函数的代码被放到称作是动态链接库或共享对象的某个目标文件中。链接程序此时所作的只是在最终的可执行程序中记录下共享对象的名字以及其它少量的登记信息。在此可执行文件被执行时，动态链接库的全部内容将被映射到运行时相应进程的虚地址空间。动态链接程序将根据可执行程序中记录的信息找到相应的函数代码。C/C++编译过程对于可执行文件中的函数调用，可分别采用动态链接或静态链接的方法。使用动态链接能够使最终的可执行文件比较短小，并且当共享对象被多个进程使用时能节约一些内存，因为在内存中只需要保存一份此共享对象的代码。但并不是使用动态链接就一定比使用静态链接要优越。在某些情况下动态链接可能带来一些性能上损害。 Java程序编译过程 用gcc编译成执行程序。1gcc hello.c 该命令将hello.c直接生成最终二进制可执行程序a.out 1gcc hello.c -o hello.exe 如何要指定最终二进制可执行程序名，那么用-o选项来指定名称。比如需要生成执行程序hello.exe GCC的命令剖析–四步走GCC编译C源码有四个步骤： 预处理—–&gt; 编译 —-&gt; 汇编 —-&gt; 链接 预处理在该阶段，编译器将C源代码中的包含的头文件如stdio.h编译进来编译第二步进行的是编译阶段，在这个阶段中，Gcc首先要检查代码的规范性、是否有语法错误等，以确定代码的实际要做的工作，在检查无误后，Gcc把代码翻译成汇编语言。 汇编汇编阶段是把编译阶段生成的”.s”文件转成二进制目标代码.链接阶段(Link)在成功编译之后，就进入了链接阶段。 解释型语言和编译型语言 计算机是不能够识别高级语言的，所以当我们运行一个高级语言程序的时候，就需要一个“翻译机”来从事把高级语言转变成计算机能读懂的机器语言的过程。这个过程分成两类，第一种是编译，第二种是解释。 编译型语言在程序执行之前，先会通过编译器对程序执行一个编译的过程，把程序转变成机器语言。运行时就不需要翻译，而直接执行就可以了。最典型的例子就是C语言。 解释型语言就没有这个编译的过程，而是在程序运行的时候，通过解释器对程序逐行作出解释，然后直接运行，最典型的例子是Ruby。 通过以上的例子，我们可以来总结一下解释型语言和编译型语言的优缺点，因为编译型语言在程序运行之前就已经对程序做出了“翻译”，所以在运行时就少掉了“翻译”的过程，所以效率比较高。但是我们也不能一概而论，一些解释型语言也可以通过解释器的优化来在对程序做出翻译时对整个程序做出优化，从而在效率上超过编译型语言。 此外，随着Java等基于虚拟机的语言的兴起，我们又不能把语言纯粹地分成解释型和编译型这两种。 用Java来举例，Java首先是通过编译器编译成字节码文件，然后在运行时通过解释器给解释成机器文件。所以我们说Java是一种先编译后解释的语言。 再换成C#，C#首先是通过编译器将C#文件编译成IL文件，然后在通过CLR将IL文件编译成机器文件。所以我们说C#是一门纯编译语言，但是C#是一门需要二次编译的语言。同理也可等效运用到基于.NET平台上的其他语言。 简述Python的运行过程在说这个问题之前，我们先来说两个概念，PyCodeObject和pyc文件。 我们在硬盘上看到的pyc自然不必多说，而其实PyCodeObject则是Python编译器真正编译成的结果。我们先简单知道就可以了，继续向下看。 当python程序运行时，编译的结果则是保存在位于内存中的PyCodeObject中，当Python程序运行结束时，Python解释器则将PyCodeObject写回到pyc文件中。 当python程序第二次运行时，首先程序会在硬盘中寻找pyc文件，如果找到，则直接载入，否则就重复上面的过程。 所以我们应该这样来定位PyCodeObject和pyc文件，我们说pyc文件其实是PyCodeObject的一种持久化保存方式。 所以python 和java 是一样的，都是先编译然后再执行。 pyc的目的是重用 有时候能够find pyc文件，有时候不能find pyc 文件，那么问题是什么时候能够 find，什么时候不能find 呢？所以，我们需要编译成pyc文件的应该是那些可以重用的模块，这于我们在设计软件类时是一样的目的。我们可以这样理解Python解释器的意图，Python解释器只把我们可能重用到的模块持久化成pyc文件。 pyc的过期时间 其实了解Python程序的执行过程对于大部分程序员，包括Python程序员来说意义都是不大的，那么真正有意义的是，我们可以从Python的解释器的做法上学到什么，我认为有这样的几点： 其实Python是否保存成pyc文件和我们在设计缓存系统时是一样的，我们可以仔细想想，到底什么是值得扔在缓存里的，什么是不值得扔在缓存里的。 在跑一个耗时的Python脚本时，我们如何能够稍微压榨一些程序的运行时间，就是将模块从主模块分开。（虽然往往这都不是瓶颈） 在设计一个软件系统时，重用和非重用的东西是不是也应该分开来对待，这是软件设计原则的重要部分。 在设计缓存系统（或者其他系统）时，我们如何来避免程序的过期，其实Python的解释器也为我们提供了一个特别常见而且有效的解决方案。 常见的编译性语言： c/ c++, pascal常见的解释性语言： java python javascript 机器翻译的方式有两种，一个是编译，一个是解释。两种方式只是翻译的时间不同。编译型语言写的程序执行之前，需要一个专门的编译过程，把程序编译成为机器语言的文件，比如exe文件，以后要运行的话就不用重新翻译了，直接使用编译的结果就行了（exe文件），因为翻译只做了一次，运行时不需要翻译，所以编译型语言的程序执行效率高。 解释性语言的程序不需要编译，省了道工序，解释性语言在运行程序的时候才翻译，比如解释性java语言，专门有一个解释器能够直接执行java程序，每个语句都是执行的时候才翻译。这样解释性语言每执行一次就要翻译一次，效率比较低。 脚本语言是解释性语言。脚本语言一般都有相应的脚本引擎来解释执行。它们一般需要解释器才能运行。所以只要系统上有相应语言的解释程序就可以做到跨平台。脚本语言是一种解释性的语言，例如vbscript,javascript,install shield script等等,它不象c\c++等可以编译成二进制代码，以可执行文件的形式存在。脚本语言不需要编译，可以直接用，由解释器来负责解释。 编译语言： 整个程序经过编译之后再执行，执行效率高。解释性语言，一行一行执行，执行效率低，解释性语言在运行程序的时候才翻译。（所以在debug 的时候，python 的出错的地方能够定位到某一行，但是c++ 就比较困难。） 其实java 不能完全划分为第二类，java 执行的过程，先是编译成 字节码，然后再执行。脚本语言和解释性语言是一个意思。]]></content>
      <categories>
        <category>CS基础</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[leeetcode data structure]]></title>
    <url>%2F2018%2F10%2F04%2Fleeetcode-data-structure%2F</url>
    <content type="text"><![CDATA[正式进入代码模板2-数据结构专题。 单调队列（滑动窗口）滑动窗口题目总结： 首先考虑用一个普通队列怎么做？（实际上就是滑动窗口使用队列来维护），那么O(n) 是得到了队列，然后队列长度是k，那么总共的时间复杂度是O(nk) 普通队列该怎么做 将队列中的没有用的元素删掉-&gt; 具有了单调性 可以用 O(1) 时间从队头/ 尾取出最值 用处一般只有两处： 滑动窗口中的极值 找出离他最近的比他小的元素或者最大元素 12345678 常见模型：找出滑动窗口中的最大值/最小值int hh = 0, tt = -1;for (int i = 0; i &lt; n; i ++ )&#123; while (hh &lt;= tt &amp;&amp; check_out(q[hh])) hh ++ ; // 判断队头是否滑出窗口 while (hh &lt;= tt &amp;&amp; check(q[tt], i)) tt -- ; q[ ++ tt] = i;&#125; 例题 79. 滑动窗口的最大值 1234567891011121314151617181920class Solution &#123;public: // 单调队列问题， 队列中是递增的，那么队首就是最大值 // 需要维护窗口size 的大小和 保持单调性 // 需要使用双向队列 vector&lt;int&gt; maxInWindows(vector&lt;int&gt;&amp; nums, int k) &#123; deque&lt;int&gt; q; vector&lt;int&gt; res; for(int i =0; i&lt; nums.size(); i++ ) &#123; // 维持窗口的大小 while(q.size() &amp;&amp; q.front() &lt;= k-i) q.pop_front(); // 维持单调性 while(q.size() &amp;&amp; nums[i] &gt;= nums[q.back()]) q.pop_back(); q.push_back(i); if(q.front() +1&gt;= k) res.push_back(nums[q.front()]); &#125; return res; &#125;&#125;; 这个是单机版本 时间复杂度是$O(n)$ 1234567891011121314151617181920212223242526272829303132#include&lt;iostream&gt;#include&lt;queue&gt;#include&lt;vector&gt;using namespace std;int main()&#123; int n, k; vector&lt;int&gt; arr; cin&gt;&gt;n&gt;&gt;k; for(int i =0; i&lt; n; i++ ) &#123; int tmp; cin &gt;&gt; tmp; arr.push_back(tmp); &#125; deque&lt;int&gt; q; vector&lt;int&gt; res; for(int i =0; i&lt; n; i++) &#123; while(q.size() &amp;&amp; i-q.front() +1&gt; k ) q.pop_front(); while(q.size() &amp;&amp; arr[i] &gt;= arr[q.back()]) q.pop_back(); q.push_back(i); if(i-k +1&gt;=0) res.push_back(arr[q.front()]); &#125; for(auto u : res) &#123; cout &lt;&lt; u&lt;&lt;" "; &#125; cout &lt;&lt; endl; return 0;&#125; 154. 滑动窗口 123456789101112131415161718192021222324252627282930313233343536373839404142434445#include&lt;iostream&gt;#include&lt;queue&gt;#include&lt;vector&gt;using namespace std;int main()&#123; int n,k; cin &gt;&gt;n&gt;&gt;k; vector&lt;int&gt; arr; for(int i =0; i&lt;n; i++) &#123; int tmp; cin &gt;&gt;tmp; arr.push_back(tmp); &#125; deque&lt;int&gt; min_q, max_q; vector&lt;int&gt; min_r, max_r; for(int i =0; i&lt; n; i++) &#123; // 这个一定要分清谁大谁小 while( max_q.size() &amp;&amp;i- max_q.front() +1 &gt;k) max_q.pop_front(); while(min_q.size() &amp;&amp; i- min_q.front() +1&gt; k) min_q.pop_front(); // max_q 是递增的, min_q 是递减的 while(max_q.size() &amp;&amp; arr[i] &gt;= arr[max_q.back()]) max_q.pop_back(); while(min_q.size() &amp;&amp; arr[i] &lt;= arr[min_q.back()]) min_q.pop_back(); max_q.push_back(i); min_q.push_back(i); if(i -k +1 &gt;=0) max_r.push_back(arr[max_q.front()]); if(i -k +1 &gt;=0) min_r.push_back(arr[min_q.front()]); &#125; for(auto u : min_r) &#123; cout &lt;&lt; u&lt;&lt;" "; &#125; cout &lt;&lt;endl; for(auto u : max_r) &#123; cout &lt;&lt; u&lt;&lt;" "; &#125; cout &lt;&lt;endl;&#125; Trie字符串统计为什么使用trie 树，因为trie 树可以快速的（时间复杂度可以 O(n)）判断下面两个事情 是否存在一个串， 是当前串的前缀：遍历路径中是否存在字符串结尾标志。 当前串，是否是某个串的前缀：遍历路径中， 是否创建过新的节点。 142. 前缀统计 输入的数据大于 100万的话，那么使用 scanf() ，否则使用 cin 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647#include&lt;iostream&gt;using namespace std;const int N =1e6+11, M =5e5;int n, m;// 使用数组的方式存储trie,也是可以使用结构体的方式int son[M][26], cnt[N], idx;//cnt 表示以当前节点为终点的单词的个数, idx 是遍历的指针char str[N];void insert()&#123; int p =0; // 一般使用 0号节点表示根节点 for(int i=0; str[i]; i++ ) &#123; // 首先搞到当前节点的儿子 int &amp;s =son[p][str[i] -'a']; if(!s) s = ++ idx;// 如果不存在的话，那就重新分配一个新的指针 p =s; &#125; // 最后p就是终点， cnt[p] ++; // 这个是统计的时候，以当前的点为终点的个数&#125;int query()&#123; int p =0, res =0; for(int i =0; str[i] ; i++) &#123; int &amp;s =son[p][str[i] -'a']; if(!s) break; p =s; res += cnt[p]; &#125; return res;&#125;int main()&#123; scanf("%d%d", &amp;n, &amp;m); // 如果大于 100万 while(n --) &#123; scanf("%s", str); insert(); &#125; while(m --) &#123; scanf("%s", str); printf("%d\n", query()); &#125; return 0;&#125; 161. 电话列表 这个比较难， 可以接着看 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152#include&lt;cstring&gt;#include&lt;iostream&gt;#include&lt;algorithm&gt;using namespace std;const int N =100010;int n ;int son[N][10], idx;char str[10010];bool f[N];// 需要判断两个内容bool insert(char * str)&#123; int p =0; bool is_match =false; bool has_new_node =false; for(int i =0; str[i]; i++) &#123; int u =str[i] -'0'; if( !son[p][u]) &#123; son[p][u] = ++idx; has_new_node =true; &#125; p =son[p][u]; if(f[p]) is_match =true; &#125; f[p] =true; return !is_match &amp;&amp; has_new_node; &#125;int main()&#123; int T; cin &gt;&gt; T; // vector&lt;int&gt; 其实也是可以exactly 的初始化 // 对于 数组这样进行初始化, while(T--) &#123; cin &gt;&gt;n; memset(son, 0, sizeof son); memset(f, false, sizeof f); idx =0; bool res =true; for(int i =0; i&lt; n; i++) &#123; cin &gt;&gt; str; if(!insert(str)) res =false; &#125; if(res ) puts("YES"); else puts("NO"); &#125; return 0;&#125; 搜索和图论决定我们采用邻接矩阵还是采用邻接表来表示图，需要判断一个图是稀疏图还是稠密图。稠密图和稀疏图的判断是边数和顶点树的关系，如果点数少，边数多，那么就是稠密图，反之就是稀疏图。具体关系，邻接矩阵适用于稠密图（边数接近于顶点数的平方）这种f，邻接表适用于稀疏图（边数远小于顶点数的平方）。据说：邻接表是表示图的标准方法，原因是稠密图在实际应用中并不多见。因为稠密图用邻接表表示的话会占用很多空间，而邻接矩阵的空间是固定的都是n²，所以用矩阵表示稠密图。 问题解释：从图中的某个顶点出发到达另外一个顶点的所经过的边的权重和最小的一条路径，称为最短路径 算法特点： 迪科斯彻算法使用了广度优先搜索解决赋权有向图或者无向图的单源最短路径问题，算法最终得到一个最短路径树。该算法常用于路由算法或者作为其他图算法的一个子模块。 从顶点v1到其他各个顶点的最短路径 百度过后仿佛打开了新世界的大门，头文件居然还可以这样用！！1#include&lt;bits/stdc++.h&gt; // 万能头文件， 如果不背会 那才是傻逼 Dijkstra求最短路 I 比较好的讲解 朴素dijkstra算法 时间复杂是 $O(n^2+m)$， $n$表示点数，$m $表示边数 12345678910111213141516171819202122232425262728293031323334353637383940414243#include &lt;iostream&gt;#include &lt;cstring&gt;#include &lt;algorithm&gt;using namespace std;const int N = 510;int n, m;int g[N][N];int dist[N];bool st[N]; // true表示已经确定最短路int dijkstra()&#123; memset(dist, 0x3f, sizeof dist); dist[1] = 0; // 迭代n次，每次可以确定一个点到起点的最短路 for (int i = 0; i &lt; n; i++) &#123; // 找到当前没有确定最短路的点中距离最小的那个 int t = -1; for (int j = 1; j &lt;= n; j++) if (!st[j] &amp;&amp; (t == -1 || dist[j] &lt; dist[t])) t = j; st[t] = true; // 用t更新其他未确定最短路的点的到起点的距离 for (int j = 1; j &lt;= n; j++) if (!st[j]) // if可加可不加 dist[j] = min(dist[j], dist[t] + g[t][j]); &#125; if (dist[n] == 0x3f3f3f3f) return -1; // 1与n不连通 return dist[n];&#125;int main()&#123; cin &gt;&gt; n &gt;&gt; m; memset(g, 0x3f, sizeof g); while (m--) &#123; int a, b, c; scanf("%d%d%d", &amp;a, &amp;b, &amp;c); g[a][b] = min(g[a][b], c); // 有重边保留短的一条 &#125; cout &lt;&lt; dijkstra() &lt;&lt; endl; return 0;&#125; memset详解 设置无穷大INF 通常情况下在 图计算中的初始化都是使用 0x3f 而不是 0x7fffff（int 所能表示的最大值） 12345678910111213141516171819Memset中无穷大常量的设定技巧如果问题中各数据的范围明确，那么无穷大的设定不是问题，在不明确的情况下，很多程序员都取0x7fffffff作为无穷大，因为这是32-bit int的最大值。如果这个无穷大只用于一般的比较（比如求最小值时min变量的初值），那么0x7fffffff确实是一个完美的选择，但是在更多的情况下，0x7fffffff并不是一个好的选择。很多时候我们并不只是单纯拿无穷大来作比较，而是会运算后再做比较，例如在大部分最短路径算法中都会使用的松弛操作：if (d[u]+w[u][v]&lt;d[v]) d[v]=d[u]+w[u][v];我们知道如果u,v之间没有边，那么w[u][v]=INF，如果我们的INF取0x7fffffff，那么d[u]+w[u][v]会溢出而变成负数，我们的松弛操作便出错了，更一般的说，0x7fffffff不能满足“无穷大加一个有穷的数依然是无穷大”，它变成了一个很小的负数。除了要满足加上一个常数依然是无穷大之外，我们的常量还应该满足“无穷大加无穷大依然是无穷大”，至少两个无穷大相加不应该出现灾难性的错误，这一点上0x7fffffff依然不能满足我们。所以我们需要一个更好的家伙来顶替0x7fffffff，最严谨的办法当然是对无穷大进行特别处理而不是找一个很大很大的常量来代替它（或者说模拟它），但是这样会让我们的编程过程变得很麻烦。在我读过的代码中，最精巧的无穷大常量取值是0x3f3f3f3f，我不知道是谁最先开始使用这个精妙的常量来做无穷大，不过我的确是从一位不认识的ACMer(ID:Staginner)的博客上学到的，他/她的很多代码中都使用了这个常量，于是我自己也尝试了一下，发现非常好用，而当我对这个常量做更深入的分析时，就发现它真的是非常精巧了。0x3f3f3f3f的十进制是1061109567，也就是10^9级别的（和0x7fffffff一个数量级），而一般场合下的数据都是小于10^9的，所以它可以作为无穷大使用而不致出现数据大于无穷大的情形。另一方面，由于一般的数据都不会大于10^9，所以当我们把无穷大加上一个数据时，它并不会溢出（这就满足了“无穷大加一个有穷的数依然是无穷大”），事实上0x3f3f3f3f+0x3f3f3f3f=2122219134，这非常大但却没有超过32-bit int的表示范围，所以0x3f3f3f3f还满足了我们“无穷大加无穷大还是无穷大”的需求。最后，0x3f3f3f3f还能给我们带来一个意想不到的额外好处：如果我们想要将某个数组清零，我们通常会使用memset(a,0,sizeof(a))这样的代码来实现（方便而高效），但是当我们想将某个数组全部赋值为无穷大时（例如解决图论问题时邻接矩阵的初始化），就不能使用memset函数而得自己写循环了（写这些不重要的代码真的很痛苦），我们知道这是因为memset是按字节操作的，它能够对数组清零是因为0的每个字节都是0，现在好了，如果我们将无穷大设为0x3f3f3f3f，那么奇迹就发生了，0x3f3f3f3f的每个字节都是0x3f！所以要把一段内存全部置为无穷大，我们只需要memset(a,0x3f,sizeof(a))。所以在通常的场合下，0x3f3f3f3f真的是一个非常棒的选择。 堆优化版dijkstra —— 模板题 AcWing 850. Dijkstra求最短路 II 时间复杂度 $O(mlogn) $, $n $表示点数，$m$表示边数， 从时间复杂度角度分析为什么这种是比较优秀的算法，因为 $n$ （点数）远远大于 $m$ ，所以经过 log 处理之后，最后时间复杂度是比较令人满意的。 这个时间复杂度不知道是怎么分析出来的？ 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556#include&lt;bits/stdc++.h&gt;using namespace std;int m, n;struct Node&#123; int vec; int w;&#125;;const int N =1e5+11;bool vis[N];int dis[N];vector&lt;Node&gt; g[N];typedef pair&lt;int, int&gt; P;void dijkstra()&#123; // 初始化 memset(vis, 0, sizeof vis); for(int i =1; i&lt;=m ; i++) dis[i] =INT_MAX; dis[1] =0; priority_queue&lt;P, vector&lt;P&gt;, greater&lt;P&gt;&gt; heap; heap.push(&#123;0, 1&#125;); // 第一个维度是 distance, 第二个维度是 vec while(heap.size()) &#123; auto t =heap.top(); heap.pop(); int vec =t.second; int distance =t.first; if(vis[vec] ==1) continue; vis[vec] =1; for(int i =0; i&lt; g[vec].size() ; i++) &#123; int next_node =g[vec][i].vec; if(distance+ g[vec][i].w &lt; dis[next_node]) &#123; dis[next_node] =distance + g[vec][i].w; heap.push(&#123;dis[next_node], next_node&#125;); &#125; &#125; &#125;&#125;int main()&#123; cin &gt;&gt;m &gt;&gt;n; //m 个点， n 个边 Node tmp; int a, b, c; for(int i =0; i&lt; n; i++) &#123; cin &gt;&gt; a&gt;&gt; b&gt;&gt; c; tmp.vec =b; tmp.w =c; g[a].emplace_back(tmp); &#125; dijkstra(); if(dis[m] ==INT_MAX) cout &lt;&lt; -1 &lt;&lt; endl; else cout &lt;&lt; dis[m] &lt;&lt; endl; return 0;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879#include&lt;bits/stdc++.h&gt;using namespace std;int m, n; // 表示m 个边， n个点// 定义一个结点，struct node&#123; int index; int w;&#125;;const int N =100010;bool vis[N];int dis[N];vector&lt;node&gt; g[N];typedef pair&lt;int, int&gt; P;// 迪杰斯特拉算法void dijkstra()&#123; // 初始化 memset(dis, INT_MAX, sizeof dis); dis[1] =0; memset(vis, 0, sizeof vis); // 朴素dijstra算法 时间复杂度 O(n^2+m) n表示点数， m 表示边数 // 使用小根堆优化时间复杂度为O(mlogn) m表示边数 n 表示点数 // 使用小根堆 priority_queue&lt;P, vector&lt;P&gt;, greater&lt;P&gt;&gt; heap; heap.push(&#123;0, 1&#125;); while(heap.size()) &#123; auto t =heap.top(); heap.pop(); int ver =t.second; int d =t.first; // 这个优化一下 if(vis[ver] ==1) continue; vis[ver] =1; for(int i =0; i&lt; g[ver].size(); i++) &#123; int nex_node =g[ver][i].index; if(d +g[ver][i].w &lt; dis[nex_node] &amp;&amp; vis[nex_node] ==0) &#123; dis[nex_node] = d + g[ver][i].w; heap.push(&#123;dis[nex_node], nex_node&#125;); &#125; &#125; &#125; &#125;int main()&#123; cin&gt;&gt; n &gt;&gt;m; int x, y, z; node tmp; for(int i =0; i&lt;m ; i++) &#123; cin &gt;&gt;x&gt;&gt;y&gt;&gt;z; tmp.index =y; tmp.w =z; g[x].push_back(tmp); &#125; dijkstra(); for(int i =0; i&lt;=n ; i++) cout &lt;&lt; dis[i] &lt;&lt; " "; if(dis[n] !=INT_MAX) cout &lt;&lt; dis[n]&lt;&lt; endl; else cout &lt;&lt; -1&lt;&lt; endl; return 0;&#125; cpp 中默认是大根堆，可以通过设置( greater 参数和 small_heap 正好是相反的) 12//构造一个空的优先队列,此优先队列是一个小顶堆priority_queue&lt;int,vector&lt;int&gt;,greater&lt;int&gt; &gt; small_heap; //emplace函数的参数根据元素类型而变化，参数必须与元素类型的构造函数相匹配。emplace函数在容器中直接构造元素。传递给emplace函数的参数必须与元素类型的构造函数相匹配。// emplace相关函数可以减少内存拷贝和移动。当插入rvalue，它节约了一次move构造，当插入lvalue，它节约了一次copy构造。 787. Cheapest Flights Within K Stops 这个是双关键字的 dijkstra 算法， 讲解 12345678910111213141516171819202122232425262728293031323334353637383940414243444546struct Node&#123; int no, stops; // 可以理解为双关键字 int distance; Node(int no_, int stops_, int distance_): no(no_), stops(stops_), distance(distance_) &#123;&#125; // 在结构体中，就是这样使用的 bool operator &gt;(const Node &amp;other) const &#123; return distance &gt; other.distance; &#125; &#125;;class Solution &#123;public: int findCheapestPrice(int n, vector&lt;vector&lt;int&gt;&gt;&amp; flights, int src, int dst, int K) &#123; //const int INF =1e9; vector&lt;vector&lt;pair&lt;int, int&gt;&gt;&gt; graph(n); // 这个是邻接矩阵 for(auto &amp;v :flights) graph[v[0]].emplace_back(v[1], v[2]); K ++; vector&lt;vector&lt;int&gt;&gt; dis(n, vector&lt;int&gt;(K +1,INT_MAX )); vector&lt;vector&lt;bool&gt;&gt; vis(n, vector&lt;bool&gt;(K +1, false)); priority_queue&lt;Node, vector&lt;Node&gt;, greater&lt;Node&gt;&gt; q; q.push(Node(src, 0, 0)); dis[src][0] =0; while(q.size()) &#123; auto u =q.top(); q.pop(); // 这里体现了双关键字 if(vis[u.no][u.stops] ) continue; vis[u.no][u.stops] =true; for(const auto &amp;v : graph[u.no]) if(u.stops +1 &lt;=K &amp;&amp; dis[u.no][u.stops] + v.second &lt; dis[v.first][u.stops +1])&#123; dis[v.first][u.stops +1] =dis[u.no][u.stops] +v.second; q.push(Node(v.first, u.stops+1, dis[v.first][u.stops +1]));// 这种双关键字就不太好容易理解了 &#125; &#125; int ans =INT_MAX; for(int i =1; i&lt;=K ; i++) &#123; ans =min(ans, dis[dst][i]); &#125; if(ans ==INT_MAX) ans =-1; return ans; &#125;&#125;; floyd算法 —— 模板题 AcWing 854. Floyd求最短路时间复杂度是 $O(n^3)$, $n$表示点数 简单的说就是解决任意两点间的最短路径的一种算法，可以正确处理有向图或负权的最短路径问题，同时也被用于计算有向图的传递闭包。Floyd-Warshall算法的时间复杂度为O(N3)，空间复杂度为O(N2)。 1.dijkstra算法,最经典的单源最短路径算法 上篇文章已经讲到 2.bellman-ford算法,允许负权边的单源最短路径算法 3.spfa,其实是bellman-ford+队列优化,其实和bfs的关系更密一点 4.floyd算法,经典的多源最短路径算法 单源就是从一个点到所有其他点的最短路径，得到的结果是一个数组，表示某个点到其他点的最短距离。常用的算法有Dijkstra算法和Bellmanford算法。多源最短路径计算所有点到其他点的最短距离，得到的是一个矩阵。常用的算法有Floyd算法。 1234567891011121314初始化： for (int i = 1; i &lt;= n; i ++ ) for (int j = 1; j &lt;= n; j ++ ) if (i == j) d[i][j] = 0; else d[i][j] = INF;// 算法结束后，d[a][b]表示a到b的最短距离void floyd()&#123; for (int k = 1; k &lt;= n; k ++ ) for (int i = 1; i &lt;= n; i ++ ) for (int j = 1; j &lt;= n; j ++ ) d[i][j] = min(d[i][j], d[i][k] + d[k][j]);&#125; 123456789101112131415161718192021222324252627282930313233343536373839#include&lt;bits/stdc++.h&gt;using namespace std;const int N =210, INF =0x3f3f3f3f; // 这个在竞赛中是非常常见的int d[N][N];int n, m , Q;void floyd()&#123; for(int k =1; k&lt;=n ; k++) for(int i =1; i&lt;=n ; i++) for(int j =1; j&lt;=n ; j++) d[i][j] =min(d[i][j], d[i][k] +d[k][j]); &#125;int main()&#123; cin &gt;&gt; n&gt;&gt;m&gt;&gt; Q; // 初始化 for(int i =1; i&lt;= n; i++) &#123; for(int j =1; j&lt;=n ; j++) if(i ==j) d[i][j] =0; else d[i][j] =INF; &#125; while(m --) &#123; int a, b,c; cin &gt;&gt; a&gt;&gt;b&gt;&gt;c; d[a][b] =min(d[a][b], c); &#125; floyd(); while(Q --) &#123; int a, b; cin &gt;&gt; a&gt;&gt;b; if(d[a][b] &gt; INF/2) puts("impossible"); else cout &lt;&lt; d[a][b] &lt;&lt; endl; &#125; return 0;&#125; 朴素版prim算法 —— 模板题 AcWing 858. Prim算法求最小生成树 时间复杂度是 $O(n^2+m)$, $n$ 表示点数，$m$ 表示边数 1234567891011121314151617181920212223242526272829int n; // n表示点数int g[N][N]; // 邻接矩阵，存储所有边int dist[N]; // 存储其他点到当前最小生成树的距离bool st[N]; // 存储每个点是否已经在生成树中// 如果图不连通，则返回INF(值是0x3f3f3f3f), 否则返回最小生成树的树边权重之和int prim()&#123; memset(dist, 0x3f, sizeof dist); int res = 0; for (int i = 0; i &lt; n; i ++ ) &#123; int t = -1; for (int j = 1; j &lt;= n; j ++ ) if (!st[j] &amp;&amp; (t == -1 || dist[t] &gt; dist[j])) t = j; if (i &amp;&amp; dist[t] == INF) return INF; if (i) res += dist[t]; st[t] = true; for (int j = 1; j &lt;= n; j ++ ) dist[j] = min(dist[j], g[t][j]); &#125; return res;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051#include&lt;bits/stdc++.h&gt;using namespace std;const int N =211;const int M =2e4+11;int arr[N][N];const int INF =0x3f3f3f3f;int n, m, q;void floyd()&#123; for(int k =1; k&lt;=n ; k++) &#123; for(int i =1; i&lt;=n ; i++) &#123; for(int j =1; j&lt;=n ; j++) if(arr[i][k] + arr[k][j] &lt; arr[i][j]) arr[i][j] = arr[i][k] + arr[k][j]; &#125; &#125;&#125;int main()&#123; cin &gt;&gt;n &gt;&gt;m &gt;&gt;q; // init for(int i= 1; i&lt;=n ; i++) for(int j =1; j&lt;=n ; j++) if(i ==j) arr[i][j] =0; else arr[i][j] =INF; while(m --) &#123; int a, b, c; cin &gt;&gt; a&gt;&gt; b&gt;&gt;c; arr[a][b] =min(arr[a][b], c); &#125; floyd(); while(q--) &#123; int a, b; cin &gt;&gt; a&gt;&gt;b; // 这个判断条件是非常巧妙的 if(INF &gt; INF/2) , 这个是yes 的操作， 因为本身的 inf ++ -- 是不会影响最后的结果 if(arr[a][b] &gt; INF/2) puts("impossible"); else cout &lt;&lt; arr[a][b]&lt;&lt; endl; &#125; return 0;&#125; Kruskal算法 —— 模板题 AcWing 859. Kruskal算法求最小生成树 时间复杂度是 $O(mlogm)$, $n $表示点数，$m$表示边数 123456789101112131415161718192021222324252627282930313233343536373839404142int n, m; // n是点数，m是边数int p[N]; // 并查集的父节点数组struct Edge // 存储边&#123; int a, b, w; bool operator&lt; (const Edge &amp;W)const &#123; return w &lt; W.w; &#125;&#125;edges[M];int find(int x) // 并查集核心操作&#123; if (p[x] != x) p[x] = find(p[x]); return p[x];&#125;int kruskal()&#123; sort(edges, edges + m); for (int i = 1; i &lt;= n; i ++ ) p[i] = i; // 初始化并查集 int res = 0, cnt = 0; for (int i = 0; i &lt; m; i ++ ) &#123; int a = edges[i].a, b = edges[i].b, w = edges[i].w; a = find(a), b = find(b); if (a != b) // 如果两个连通块不连通，则将这两个连通块合并 &#123; p[a] = b; res += w; cnt ++ ; &#125; &#125; if (cnt &lt; n - 1) return INF; return res;&#125;]]></content>
      <categories>
        <category>算法</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[基于simhash的大文本相似度比较]]></title>
    <url>%2F2018%2F08%2F23%2F%E5%9F%BA%E4%BA%8Esimhash%E7%9A%84%E6%96%87%E6%9C%AC%E7%9B%B8%E4%BC%BC%E5%BA%A6%E6%AF%94%E8%BE%83%2F</url>
    <content type="text"><![CDATA[本文主要记录使用simhash比较中文大文本的相似度问题。先说一下文本特征，数据属于中文文本，每篇文章的字数大于500,小于2000,基本上属于大文本。步骤如下： 基于tf-idf提取文本的关键词。如果这些关键词在之后的比较中是相同的，那么认为对应的文章也是相同。简而言之，这些提取的关键词可以看做原文章的”代表”. 根据关键字计算simhash编码，然后使用hamming distance进行比较两者的不同。如果对于上述概念比较模糊，建议首先阅读该篇博客。 实战 顺滑过渡到代码实现：123456789101112131415# 常规导包import sys,codecsimport pandas as pdimport numpy as npimport jieba.possegimport jieba.analysefrom sklearn import feature_extractionfrom sklearn.feature_extraction.text import TfidfTransformerfrom sklearn.feature_extraction.text import CountVectorizer# 数据集的路径path =&quot;../tianmao2.csv&quot;names =[&apos;where&apos;, &apos;time&apos;, &apos;title&apos;, &apos;url&apos;, &apos;contents&apos;]data =pd.read_csv(path, delimiter=&apos;\t&apos;, names= names, nrows=200)data[&apos;id&apos;] =data.index+1data.head() 我们使用title和contents 组合作为原始处理的数据，我们认为该数据能够就是文章的内容。1stopkey = [w.strip() for w in codecs.open(&apos;../keyword_extraction/data/stopWord.txt&apos;, &apos;r&apos;).readlines()] 该stop words是中文停用词，就是常见的”的 了”。常见的有百度停用词表、哈尔滨工业大学停用词表以及中科院的停用词表。这里使用的是中科院的停用词。对于停用词的存储，可以使用set ，因为set 要比 list的检索要快。12345678def dataPrepos(text, stopkey): l = [] pos = [&apos;n&apos;, &apos;nz&apos;, &apos;v&apos;, &apos;vd&apos;, &apos;vn&apos;, &apos;l&apos;, &apos;a&apos;, &apos;d&apos;] # 定义选取的词性 seg = jieba.posseg.cut(text) # 分词 for i in seg: if i.word not in stopkey and i.flag in pos: # 去停用词 + 词性筛选 l.append(i.word) return l 我们选择名词作为主要的分析对象。12345678idList, titleList, abstractList = data[&apos;id&apos;], data[&apos;title&apos;], data[&apos;contents&apos;]corpus = [] # 将所有文档输出到一个list中，一行就是一个文档# 这个 虽然使用 &quot; &quot; 进行分割 但是实际上还是一个打的listfor index in range(len(idList)): text = &apos;%s。%s&apos; % (titleList[index], abstractList[index]) # 拼接标题和摘要 text = dataPrepos(text, stopkey) # 文本预处理 text = &quot; &quot;.join(text) # 连接成字符串，空格分隔 corpus.append(text) 这里的corus 是将所有的经过预处理文档作为当前计算 idf 的语料库。123456789vectorizer = CountVectorizer()X = vectorizer.fit_transform(corpus) # 词频矩阵,a[i][j]:表示j词在第i个文本中的词频# 2、统计每个词的tf-idf权值transformer = TfidfTransformer()tfidf = transformer.fit_transform(X)# 3、获取词袋模型中的关键词word = vectorizer.get_feature_names()# 4、获取tf-idf矩阵，a[i][j]表示j词在i篇文本中的tf-idf权重weight = tfidf.toarray() 使用sklearn 内置的函数计算tf-idf。1234567891011121314151617181920212223242526272829topK = 10ids, titles, keys, weights = [], [], [], []for i in range(len(weight)): print(&quot;-------这里输出第&quot;, i + 1, &quot;篇文本的词语tf-idf------&quot;) ids.append(idList[i]) titles.append(titleList[i]) df_word, df_weight = [], [] # 当前文章的所有词汇列表、词汇对应权重列表 for j in range(len(word)): # print(word[j],weight[i][j]) df_word.append(word[j]) df_weight.append(weight[i][j]) df_word = pd.DataFrame(df_word, columns=[&apos;word&apos;]) df_weight = pd.DataFrame(df_weight, columns=[&apos;weight&apos;]) word_weight = pd.concat([df_word, df_weight], axis=1) # 拼接词汇列表和权重列表 word_weight = word_weight.sort_values(by=&quot;weight&quot;, ascending=False) # 按照权重值降序排列 # 在这里可以查看 k的选取的数值应该是多大， # from ipdb import set_trace # set_trace() keyword = np.array(word_weight[&apos;word&apos;]) # 选择词汇列并转成数组格式 word_split = [keyword[x] for x in range(0, topK)] # 抽取前topK个词汇作为关键词 word_split = &quot; &quot;.join(word_split) keys.append(word_split) wei = np.array(word_weight[&apos;weight&apos;]) wei_split = [str(wei[x]) for x in range(0, topK)] wei_split = &quot; &quot;.join(wei_split) weights.append(wei_split) # 这里的命名 容易混淆result = pd.DataFrame(&#123;&quot;id&quot;: ids, &quot;title&quot;: titles, &quot;key&quot;: keys, &apos;weight&apos;: weights&#125;, columns=[&apos;id&apos;, &apos;title&apos;, &apos;key&apos;, &apos;weight&apos;]) 选择前10个频率最高的词语作为该篇文章的代表，当然这个参数是可以调整，需要根据具体的问题和结果进行调整。1result.head() 最后的效果如上。至此我们第一步的提取文章的关键词就已经做完。下面进行相似度的比较。 1234import jiebaimport jieba.analyseimport pandas as pd#日常导包 数据和上述的一样，所以就不截图了。123datasets =pd.read_csv(&quot;../tianmao2-tf-idf.csv&quot;)tokens =datasets[&apos;key&apos;]weights =datasets[&apos;weight&apos;] 提取关键词和对应的权重。123456789101112print(tokens[0], len(tokens[0]))print(weights[0], len(weights[0]))tokens0 =tokens[0].split()weights0 =weights[0].split()len(tokens0)len(weights0)tokens1 =tokens[1].split()weights1 =weights[1].split()import astweights0 =[ ast.literal_eval(i) for i in weights0]weights1 =[ ast.literal_eval(i) for i in weights1] 构造测试用例。因为权重是字符串，所以简单处理转成整数。 12dict0 =dict(zip(tokens0, weights0))dict1 =dict(zip(tokens1, weights1)) 定义一个Simhash，提供对文档的数值映射和文档间相似度计算的功能.123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081class Simhash(object): # 初始化函数 def __init__(self, weights_dict, tokens=&apos;&apos;, hashbits=64): self.hashbits = hashbits self.hash = self.simhash_function(tokens, weights_dict) # toString函数 # 不懂这个 self.hash ，凡是带有self 的函数都是可以类变量，所以这个就是返回的 self.hash这个变量 #凡是使用__str__ 这种类型的函数 都是重写 原来的函数 def __str__(self): return str(self.hash) &quot;&quot;&quot; ord() 函数是 chr() 函数（对于8位的ASCII字符串）或 unichr() 函数（对于Unicode对象）的配对函数，它以一个字符（长度为1的字符串）作为参数，返回对应的 ASCII 数值，或者 Unicode 数值 &quot;&quot;&quot; # 给每一个单词生成对应的hash值 # 这个操作搞懂之后一定很简洁， 但是现在很难理解，因为不是字符串，而是位操作 def _string_hash(self, source): if source == &apos;&apos;: return 0 else: x = ord(source[0]) &lt;&lt; 7 # &lt;&lt; 表示 乘以2^7 ; &gt;&gt; 表示除以 ; ** 表示次方的意思 # ^ : 按位异或 (二进制进行异或)； &amp; 按位进行与 操作 # 左移位操作也是可以理解为 2^x 的操作，因为存储是二进制，这样左移一位 表示×2 一次 m = 1000003 mask = 2 ** self.hashbits - 1 for c in source: x = ((x * m) ^ ord(c)) &amp; mask x ^= len(source) if x == -1: x = -2 return x # 生成simhash值 def simhash_function(self, tokens, weights_dict): v = [0] * self.hashbits # 这种使用 &#123;&#125; dictionary 然后强行得到item 再进行遍历也是牛逼 for key, t in &#123;x: self._string_hash(x) for x in tokens&#125;.items(): for i in range(self.hashbits): bitmask = 1 &lt;&lt; i if t &amp; bitmask: v[i] += weights_dict[key] else: v[i] -= weights_dict[key] fingerprint = 0 for i in range(self.hashbits): if v[i] &gt;= 0: fingerprint += 1 &lt;&lt; i return fingerprint # 求文档间的海明距离 def hamming_distance(self, other): x = (self.hash ^ other.hash) &amp; ((1 &lt;&lt; self.hashbits) - 1 ) tot = 0 while x : tot += 1 x &amp;= x - 1 return tot #求相似度 # 这个相似度的计算，十分简单，如果两个数接近，那么就是认为相似。越是接近1 越是相似， # 不是原先那种以某一个参数整数 如3 为距离的相似度 def similarity(self, other): a = float(self.hash) b = float(other.hash) if a &gt; b: return b / a else: return a / b if __name__ == &apos;__main__&apos;: hash0 = Simhash(weights_dict=dict0, tokens=tokens0) print(hash0) hash1 = Simhash(weights_dict=dict1, tokens=tokens1) print(hash1) print(hash0.hamming_distance(hash1)) print(hash0.similarity(hash1)) 结果如上。可以看出该例子中使用的两两比较的方式，对于大数据来说，一般可能会用到倒排索引和cpu并行技术。]]></content>
      <categories>
        <category>NLP</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[文本相似度比较基本知识(1)]]></title>
    <url>%2F2018%2F08%2F23%2F%E6%96%87%E6%9C%AC%E7%9B%B8%E4%BC%BC%E5%BA%A6%E6%AF%94%E8%BE%83%E5%9F%BA%E6%9C%AC%E7%9F%A5%E8%AF%86%2F</url>
    <content type="text"><![CDATA[互联网网页存在大量的重复内容网页，无论对于搜索引擎的网页去重和过滤、新闻小说等内容网站的内容反盗版和追踪，还是社交媒体等文本去重和聚类，都需要对网页或者文本进行去重和过滤。本文介绍的 Locality Sensitive Hashing 是常见的一类hash 函数用于去重。 SimHash是一种局部敏感hash，它也是Google公司进行海量网页去重使用的主要算法。 Locality Sensitive HashingHash 函数能够保证最后的映射空间唯一性和均匀分布，但是不能保证原来相似向量，映射之后也是相似的。但是局部敏感hash（比如 simhash or minhash ）是能够保证这一点的。也可以从降维的角度进行理解，降维之前和降维之后，相似的文档（这里就具体化一个东西）hash 之后也是相似的。 局部敏感哈希的基本思想：在高维数据空间中的两个相邻的数据被映射到低维数据空间中后，将会有很大的概率任然相邻；而原本不相邻的两个数据，在低维空间中也将有很大的概率不相邻。通过这样一映射，我们可以在低维数据空间来寻找相邻的数据点，避免在高维数据空间中寻找，因为在高维空间中会很耗时。有这样性质的哈希映射称为是局部敏感的。simhash 或者minhash是局部敏感hash的一种具体实现。局部敏感哈希是一种思想。 应用：对于高维数据的海量数据近邻查找，局部敏感哈希是一个很好的解决方法。在很多问题中，从海量数据库中寻找到与查询数据相似的数据是一个很关键的问题。可以具体应用到文本相似度检测、网页搜索等领域。 Simhash我们现在处理的是大数据维度上的文本去重，这就对算法的效率有着很高的要求。但是在小的样本上这个是不一定有保证有效的，小文本使用 NLP 相关知识可能得到更好的精度。SimHash算法是Google公司进行海量网页去重的高效算法，它通过将原始的文本映射为64位的二进制数字串，然后通过比较二进制数字串的差异进而来表示原始文本内容的差异。本文服务于该篇博客,主要进行名词解释。 基本概念simhash 也是一种hash，一般的hash 函数映射规则只需要满足以下两个条件： 对很多不同的特征来说，它们对所对应的向量是均匀随机分布的 相同的特征来说对应的向量是唯一 simhash 和传统的 hash 不同点在于前者的01 串是可以表征文本之间的相似度，而后者是不可以的。 简单来说普通的hash映射需要满足随机分布和唯一性两个条件。simhash想要实现的是，如果原来的文本的特征是相似，那么映射之后的编码也是相似。这里使用 hamming distance 进行比较simhash映射之后的距离。根据经验值，对64位的 SimHash值，海明距离在3以内的可认为相似度比较高。编码之后的表示在英文中是 fingerprint(指纹)。 simhash最初被google 用于网页去重，当时使用的fingerprint 是64,所以这里沿用了这个传统。64位的签名可以表示多达$2^64$个象限，因此只保存所在象限的信息也足够表征一个文档了。更进一步，表示的文档的数字最多是多少？这个应该可以准确计算特征的个数应为如果用三位(01) 表示，那么有8种，那么2^64 这么多种特征，所以16*10^18 这么多。 算法步骤Simhash 分为5个步骤：分词、hash（md5 要求均匀映射到某空间就行，不要求反应原始样本的相关性）、加权、合并（列项相加）、降维（正数为1 负数为0），得到每篇文章的simhash 之后，计算两个文章的海明距离（两个字符串对应位置的不同字符的个数）。对于64 位的simhash 值，在3以内就可以认为是比较相似的。 第一步：文本预处理得到分词（去重，去除的了stop words）,然后weight权重可以使用分词的frequency 或者tfidf 进行得到第二步：进行hash 映射（可以使用md5这种传统的映射方式，基本的要求就是均匀映射到一个空间，这种映射并不能反映原始样本的相关性）第三步：hash 映射值和weight 进行相乘，如果原来是1 则乘以1，如果是0 则乘以-1，第四步： 列向相加，得到summing weights,进行降维如果是正数那么为1，如果是负数那么为-1第五步： 计算不同文本之间的 hamming distance。 然后这个simhash就出来了.有图有真相 simhash的局限性：只考虑到文章存在哪些词，没有考虑到词的顺序。不过相应的优点是，可以实现海量文章相似度计算。文章相似度计算忽略词的顺序之后效果更好。所以在处理大文本时候，simhash是有效的，但是在处理小文本，这种效果往往不能被保证。直观上理解，在一片段文章或者段落中，词语出现的顺序还是比较重要的。更加准确的说，这个是用来进行判重的算法，而不是计算相似度的算法。 开源实现：simhash 的实现，调用一个库 https://www.twblogs.net/a/5c178f2cbd9eee5e40bbc9e9/zh-cn simhash 的一个github https://github.com/yanyiwu/simhash 百度去重： top k 最长的语句，作为源数据 minhash总的操作步骤如下： 对一个文档转化为关键词的集合，用这个集合来表示这个文档，叫Shingling。 用MinHashing函数来构造哈希表。 使用LSH来寻找相似的文档。 对于第一点中的Shingling，这个k 是一个关键参数，可以体现上下文的那种。例如：k=2，doc=adcab，这个集合的2-shingles={ab,ba,ca} 。我们对这个字符串进行划分，得到的是ad dc ca ab，由于集合是唯一性的所以不可能有重复的元素。k=2 其实是一个比较糟糕的选择，我们一般选择K在实际情况中一般会选择9或者10，我们要求这个k一般要大于我们文章中出现的单词的长度。这样的选择会比较合理一些。 使用一个具体的例子讲解 minhash 的操作步骤： 第一步： 文档的Shingling：对于中文首先进行分词，得到每篇文章的词语的集合（集合是去重之后的结果），这里是可以做n-gram 的思想的，这个n 的取值越大，越能找到真正相似的文档，代价是dictionary 很大，存储上的 1234&gt; s1 = &quot;我 减肥&quot;&gt; s2= &quot;要&quot;&gt; s3 = &quot;他 减肥 成功&quot;&gt; s4 = &quot;我 要 减肥&quot; 第二步： 文档的矩阵表示（如果keyword 在相应的文章中出现，标记为1 否则标记为0） 元素 S1 S2 S3 S4 我 1 0 0 1 他 0 0 1 0 要 0 1 0 1 减肥 1 0 1 1 成功 0 0 1 0 真正实践中的矩阵应该是十分稀疏的。 第三步： 最小hash定义为：特征矩阵按行进行一个随机的排列后，第一个列值为1的行的行号。举例说明如下，假设之前的特征矩阵按行进行的一个随机排列如下： 元素 S1 S2 S3 S4 我 1 0 0 1 减肥 1 0 1 1 成功 0 0 1 0 他 0 0 1 0 要 0 1 0 1 最小哈希值：h(S1)=1，h(S2)=5，h(S3)=2，h(S4)=1. 从图中可以知道，应该从 input matrix（原始的matrix ）得到新的 signature matrix。 签名的相似性 1/3 ，以此类推我们可以得到我们所求的图中要求的相似性，我们看图中第2列和第3列 Jaccard similarity 就是 1/5，我们签名的相似性就是1/3 。 使用 signature matrix 去近似的表示 Jaccard similarity 这个是第一二列的 input matrix 0 10 01 00 10 01 10 0 这个是signature matrix 3 12 21 5 为什么使用上述方法是有效？ 事实上，两列的最小hash值就是这两列的Jaccard相似度的一个估计，换句话说，两列最小hash值同等的概率与其相似度相等，即P(h(Si)=h(Sj)) = sim(Si,Sj)。为什么会相等？我们考虑Si和Sj这两列，它们所在的行的所有可能结果可以分成如下三类： （1）A类：两列的值都为1； （2）B类：其中一列的值为0，另一列的值为1； （3）C类：两列的值都为0. 特征矩阵相当稀疏，导致大部分的行都属于C类，但只有A、B类行的决定sim(Si,Sj)，假定A类行有a个，B类行有b个，那么sim(si,sj)=a/(a+b)。现在我们只需要证明对矩阵行进行随机排列，两个的最小hash值相等的概率P(h(Si)=h(Sj))=a/(a+b)，如果我们把C类行都删掉，那么第一行不是A类行就是B类行，如果第一行是A类行那么h(Si)=h(Sj)，因此P(h(Si)=h(Sj))=P(删掉C类行后，第一行为A类)=A类行的数目/所有行的数目=a/(a+b)，这就是最小hash的神奇之处。 第四步： 这个只是一次随机采样，根据中心极限定理，只有多次随机重复采样，才能得到比较稳定的结果。那么现在出现另一个问题，将随机排列去排序，这耗费很长的时间。于是这里使用了另一种方式，选择n 个hash 函数 $h_1$, $h_2$, $h_3$ .. $h_n$ 得到不同的签名矩阵，而不是将矩阵进行重新排序。 对于两个document，在Min-Hashing方法中，它们hash值相等的概率等于它们降维前的Jaccard相似度。 就是说，对于两个document，在Min-Hashing方法中，它们hash值相等的概率等于它们降维前的Jaccard相似度。 minhash 的缺点 在工程中，不容易找到一系列的hash 函数，不同的hash 函数之间可能相关 局部敏感哈希是相对的，而且我们所说的保持数据的相似度不是说保持100%的相似度，而是保持最大可能的相似度。对于局部敏感哈希“保持最大可能的相似度”的这一点，我们也可以从数据降维的角度去考虑。数据对应的维度越高，信息量也就越大，相反，如果数据进行了降维，那么毫无疑问数据所反映的信息必然会有损失。哈希函数从本质上来看就是一直在扮演数据降维的角色。 simhash 有两个比较典型的应用：一个是网页抓取的排重，一个是检索时相似doc 的排重 simhash与Minhash的联系和区别： 相同点：simhash和minhash可以做到两个文档Hash之后仍然相似，但是simhash计算相似的方法是海明距离；而minhash计算距离的方式是Jaccard距离。不同点：理论上讲，simhash 的准确率低于minhash。原因有二： simhash 对文本进行分词并统计词频，可以认为是一个词袋模型，没有统计词汇的先后顺序。而minhash 使用滑动窗口的方式，加入了词汇的词序信息。 simhash 对词汇特征向量按列求和符号映射，丢失了文本特征信息。 参考资料： 讲解minhas https://www.cnblogs.com/maybe2030/p/4953039.html 这个也是比较好的：https://www.cnblogs.com/fengfenggirl/p/lsh.html 距离函数这里的距离函数都是用来文本相似度。 Jaccard相似度简单来说交集除以并集。这个集合中存放的是文章或者段落的关键词。 1234567891011def JaccardSim(str_a, str_b): &apos;&apos;&apos; Jaccard相似性系数 计算sa和sb的相似度 len（sa &amp; sb）/ len（sa | sb） &apos;&apos;&apos; seta = splitWords(str_a)[1] setb = splitWords(str_b)[1] sa_sb = 1.0 * len(seta &amp; setb) / len(seta | setb) return sa_sb 可以看到核心代码很简单，经过分词之后，就是seta 和setb 进行的操作。 Jaccard 系数Jaccard相似指数用来度量两个集合之间的相似性,它被定义为两个集合交集元素个数除以两个集合并集元素个数。 $$\mathrm { J } ( \mathrm { A } , \mathrm { B } ) = \frac { | A \cap B | } { | A \cup B | }$$Jaccard距离用来度量两个集合之间的差异性m它是jaccard的相似系数的补集: $$d _ { J } ( A , B ) = 1 - J ( A , B ) = \frac { | A \cup B | - | A \cap B | } { | A \cup B | }$$利用jaccard相似度来衡量文档之间的相似性,使用LSH来实现文档相似度计算。 cosine12345def cos_sim(a, b): a = np.array(a) b = np.array(b) # return &#123;&quot;文本的余弦相似度:&quot;:np.sum(a*b) / (np.sqrt(np.sum(a ** 2)) * np.sqrt(np.sum(b ** 2)))&#125; return np.sum(a * b) / (np.sqrt(np.sum(a ** 2)) * np.sqrt(np.sum(b ** 2))) 将文本的关键词映射成某种高维函数，然后在高维空间中计算两者的相似度。 tf-idf在simhash 中使用 tf-idf作为我们的比较函数。TF-IDF的主要思想就是：如果某个词在一篇文档中出现的频率高，也即TF高；并且在语料库中其他文档中很少出现，即DF的低，也即IDF高，则认为这个词具有很好的类别区分能力。$$ TF-IDF = 词频(TF) x 逆文档频率(IDF) $$ 算法步骤： 计算词频$$ 词频(TF) = 某个词在文章中出现的次数( 频数) $$或者可以进一步进行“标准化”$$ 词频( TF) = \frac{某次在文中出现的次数}{文章的总词语数} $$ 逆文档频率(这对这个术语的还是好好记忆)这个时候需要一个语料库 (corpus)，模拟语言环境$$ 逆文档频率 (IDF) = log(\frac{语料中的文档总数}{ 包含该词的文档数 +1}) $$ TF-IDF 优点是简单快速，比较符合实际。缺点，无法体现词的位置信息，所有的位置都是被认为重要性相同，但是开头结尾，段落的开头和段落的结尾，therefore，so，but这些词语都是没有体现的；还有一个缺点是，是基于统计的，没有表达出词语的语意信息 or context 上下文的信息。 Hamming distancehamming distance就是比较01串的不同，按照位进行比较。算法：异或时，只有在两个比较的位不同时其结果是1 ，否则结果为0，两个二进制“异或”后得到1的个数即为海明距离的大小。 123456789101112131415161718hashbits =64 # 使用64位进行编码def simhash_function(tokens, weights_dict): v = [0] * hashbits # 这种 &#123;key: value&#125;.item() 的操作也是没有了谁了 for key, t in &#123;x: _string_hash(x) for x in tokens&#125;.items(): for i in range(hashbits): bitmask = 1 &lt;&lt; i if t &amp; bitmask: v[i] += weights_dict[key] else: v[i] -= weights_dict[key] fingerprint = 0 for i in range(hashbits): if v[i] &gt;= 0: fingerprint += 1 &lt;&lt; i return fingerprintfingerprint = simhash_function(tokens, weights) 分词在英文中存在天然的空格可以进行分词操作，但是中文的分词就比较复杂了。常用的中文分词开源工具有 jieba和HanLP前者简单易行，容易上手；后者在自然语言处理作为汉语言处理包，可以用于词性标注，命名实体识别等一系列功能。常用的英文分词 corenlp 倒排索引倒排索引使用python在实现上就是一个dictionary 嵌套一个 set(). 一般的索引都是数字或者英文字母映射内容，具体在放到simhash的情景下就是使用文章的序列号对应提取出来的关键词。但是倒排索引就是关键词对应文章的序列号，类似与原来的”值”对应这”键”，所以称之为倒排索引。一般使用在召回的场景下，使用关键词然后出现了该关键词下的index 的集合。可以参考这篇文章。 一般的情况是key 是索引，value 对应的是关键词之列的内容； 但是倒排索引正好相反，关键字作为key，然后索引作为value，所以称之为倒排索引。]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>simhash</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Differences Between l1 and l2 as Loss Function and Regularization]]></title>
    <url>%2F2018%2F07%2F21%2Fdifferences-between-l1-and-l2-as-loss-function-and-regularization%2F</url>
    <content type="text"><![CDATA[L1 和L2 既可以作为 loss function 也可以作为 reguarization，分别介绍了一下两者，最后介绍 overfitting 的概念。 As loss functionloss function or error function 是用来衡量真实$y$ 和生成的$ f(x)$ 之间差距的函数。在模型训练中我们一般情况下不断训练模型使得loss function不断下降（如果task要求loss function是增大，这时候一般加上符号或者转换成 1- loss fucntion，最后实现的还是loss function下降）。好的回到L1 loss function和L2 loss function. 概括：从鲁棒性（对待异常值）的角度看，L1 是比L2 具有更好的性质，因为L2 是把误差进行了平方处理，误差回放大；从稳定性角度（数据集的一个小的移动），L2 是比L1 具有更好的性质，这个是可以从实验的角度进行总结。 L1-norm loss function is also known as least absolute deviations(LAD), least absolute errors. It is basically minimizing the sum of the absolute differences between the target value Y and the estimated values (f(x)). $$S = \sum _ { i = 1 } ^ { n } \left| y _ { i } - f \left( x _ { i } \right) \right|$$ L2-norm loss function is also known as least squares error(LSE). It is basically minimizing the sum of the square of the differences between the target value Y and the estimated values (f(x)). $$S = \sum _ { i = 1 } ^ { n } \left( y _ { i } - f \left( x _ { i } \right) \right) ^ { 2 }$$ 比较L1-norm 和L2-norm在前两个评价指标中的表现： 对于是否具有唯一解，可以从这个角度分析：L1是可以有多种解，而L2只有一种解。(曼哈顿距离) 使用范围： L1 适合在稀疏数据上使用，具有特征选择功能，得到的稀疏解；L2 适合在稠密数据上使用，得到的是唯一解，没有特征选择的功能。 As regularization从XGBoost调参指南中我们知道objective function = loss funcion + regularization. 而我们大多数情况下提及的都是loss function,常常忽略了regularization 的作用。 The regularization term controls the complexity of the model, which helps us to avoid overfitting.对于模型训练，一开始的想法是尽量的overfitting, 因为就现在不成熟的经验而言，对于overfitting这个问题有很多处理方法，比如卷积深度神经网络中的dropout, LightGBM中的early stop 和随机采样的思想。 这些方法都是可以缓解overfitting，所以可以出现overfitting。相反，如果你的模型是underfitting，那么你就微显尴尬了。好，收回到L1 and L2。 总结： L1 在系数weights 中使用，L2 在非稀疏的情况下使用更好。可以从计算效率、是否稀疏输出和特征选择进行分析。 计算效率 稀疏结果 特征选择 L1 在稀疏解 上效率比较高，在非稀疏解上效率比较低 产生稀疏解 具有特征选择的功能 L2 在非稀疏解上效率高 不产生稀疏解 没有特征选择的功能 首先理解为什么要正则化？ 如果网络足够的强大，数据量不是那么充足，那么网络完全是可以通过“记住” 这些样本，然后得到很高的 training acc，但是这个 test error 会比较大，这个时候就出现了过拟合。正则化就类似减轻了数据的复杂程度。关于过拟合是可以从两个维度进行解决：网络和数据。 常常使用的 dropout 就是减弱网络复杂性的一种手段，随机减少了某些网络中的节点，那么网络就没有那么强的“记忆”功能，但是最后的loss（要求）还是不变的，所以只能去寻找数据更加“简单普遍”的规律；正则化是对数据和weights 两种类型的，前者是对于数据的操作，然后weights 更加倾向于对于网络结构的操作，使得系数更加“光滑”。 为什么L1 相比于L2 产生了更加稀疏的解？ （这里从数据分布的角度进行解读，L2 的数据来源是高斯分布；L1 可以看成是来自laplace 分布） 这里从数学角度和空间角度进行了解释。 但是从更加 tuition 的角度去理解，L1 没有一个连续的导数，能产生权值为0，所以类似剔除了某些特征，产生了稀疏的权值，而L2 是具有比较连续的导数，产生了比较平滑的权值。 更加数学化的理解： $$f ( x ) = \frac { 1 } { \sqrt { 2 \pi } \sigma } \exp \left( - \frac { ( x - \mu ) ^ { 2 } } { 2 \sigma ^ { 2 } } \right)$$如果把高斯密度函数取对数，那么就只会剩下一个平方项，这个就是L2正则项的来源，所以数据是比较稠密的。 同样，如果是数据比较稀疏，不妨假设来自 laplace 分布，下面是其中的 $u$ 表示位置参数， $ b $ 表示尺度参数, 当 $b$ 越大的时候，图像越低矮，数据越不集中，越类似均匀分布，所以有一种稀疏的感觉。这两个都是可以对标正太分布的。可以看到图像的尾部都是比较平滑，然后大多数是接近于0的。 公式：$$f ( x | \mu , b ) = \frac { 1 } { 2 b } \exp \left( - \frac { | x - \mu | } { b } \right)$$同样取对数，那么得到是L1 正则项。 所以L2 得到的是一个比较平滑的weights 适合处理稠密的向量，而L1 得到是一个稀疏的结果，因为有很大程度的被置为0. 先上公式L1 regularization on least squares:$$\mathbf { w } ^ { * } = \underset { \mathbf { w } } { \arg \min } \sum _ { j } \left( t \left( \mathbf { x } _ { j } \right) - \sum _ { i } w _ { i } h _ { i } \left( \mathbf { x } _ { j } \right) \right) ^ { 2 }$$ L2 regularization on least squares: The difference between their properties can be promptly summarized as follows: 对于第一点computational efficient的理解：平方比绝对值更容易计算，平方可以求导直接求最值，但是绝对值就无法求导。并且L1 regularization在 non-sparse cases中是 computational inefficient，但是在 sparse(0比较多) cases中是有相应的稀疏算法来进行优化的，所以是computational efficient.对于第二点是否具有sparse solution可以从几何意义的角度解读：The green line(L2-norm) is the unique shortest path, while the red, blue yellow(L1-norm) are all same length for the same route. Built-in feature selection is frequently mentioned as a useful property of the L1-norm, which the L2-norm does not. This is actually a result of the L1-norm, which tends to produces sparse coefficients (explained below). Suppose the model have 100 coefficients but only 10 of them have non-zero coefficients, this is effectively saying that “the other 90 predictors are useless in predicting the target values”. L2-norm produces non-sparse coefficients, so does not have this property. 所以表格中第三点也是顺理成章的了。至此，我们区分了L1-norm vs L2-norm loss function 和L1-regularization vs L2-regularization，下面说一下 overfitting的东西。 overfitting现在，我们的训练优化算法是一个由两项内容组成的函数：一个是损失项，用于衡量模型与数据的拟合度，另一个是正则化项，用于衡量模型复杂度。 对于过拟合有两种解读方式，一种是模型是复杂，然后是去拟合了所有的数据，没有了泛化性能； 一种是从数据角度，模型去拟合了noise ，这些random的数据。然后从loss function的角度去优化的话，就是降低模型的复杂度，正则项就是降低模型复杂度的一种手段。所以从这个角度，L2 是降低模型复杂度的一种手段，也是一种减轻overfit的手段，这两者是相辅相成的。 L0 范数对于 范数的一般定义, p-norrm 如下： $$|x|{p} :=\left(\sum{i=1}^{n}\left|x_{i}\right|^{p}\right)^{\frac{1}{p}}$$虽然L0严格说不属于范数，我们可以采用上述等式给出l0-norm得定义： $$|x|{0} :=\sqrt[0]{\sum{i=0}^{n} x_{i}^{0}}$$ 0的指数和平方根严格意义上是受限条件下才成立的。因此在实际应用中，多数人给出下面的替代定义：$$|x|{0}=#(i) \text { with } x{i} \neq 0$$其表示向量中所有非零元素的个数。正是L0范数的这个属性，使得其非常适合机器学习中稀疏编码，特征选择的应用。通过最小化L0范数，来寻找最少最优的稀疏特征项。但不幸的是，L0范数的最小化问题在实际应用中是NP难问题。因此很多情况下，L0优化问题就会被relaxe为更高维度的范数问题，如L1范数，L2范数最小化问题。 从最优化问题解的平滑性来看，L1范数的最优解相对于L2范数要少，但其往往是最优解，而L2的解很多，但更多的倾向于某种局部最优解。 L0,L1,L2范数及其应用 复习总结 作为loss function L1和L2 的区别： (1) 鲁棒性（对待异常值）角度，L1 是比L2 具有更好的性质，因为后者把误差进行了平方放大。(2 ) L2 是具有唯一解L1 是有多种解 （3） L1适合在稀疏矩阵上使用，具有特征选择的功能（因为是有0 存在） 关于正则项L1 是如何求导？ 判断导数是否为0 有两种方式，一种是令导数为0；另一种是判断一阶导数在该点左右两个导数的符号，如果符号相反，那么该点导数为0. 对于L1，就是使用这种方式进行求导。这个链接 解释从导数这个角度解释了为什么L1 是容易产生稀疏解，是因为在0 点产生了极值点。对于L1 和L2 的一种解读方式，L1是来自拉普拉斯分布，L2 是来自正太分布。 过拟合问题可以从数据和模型两方面进行考虑。(1 )数据角度，使用更多的数据集 (2 ) 模型分成节点和连线(weights) 两部分。dropout 随机减少了网络中的节点，减少网络复杂程度； 正则化可有作用于weights，可以作用于训练数据集，减少了noise，都是使得系数更加”光滑“。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>l1</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LightGBM和XGBoost及其调参]]></title>
    <url>%2F2018%2F07%2F21%2FLightGBM%E5%92%8CXGBoost%E5%8F%8A%E5%85%B6%E8%B0%83%E5%8F%82%2F</url>
    <content type="text"><![CDATA[先主要介绍树的基本知识，然后介绍LightGBM和XGBoost及其调参. 进入正文之前简单的说一下决策树。一棵树很容易过拟合或者欠拟合（根据树的深度），然后需要使用多棵树进行组合预测，而GBDT是实现这个手段的方式之一。sklearn中也是实现了 GBDT 这种思想，但是比较难用，训练速度跟不上。但是 lightGBM 和XGBoost 实现的效果就比较好。 lightGBM调参(常用参数)Since lightGBM is based on decision tree algorithms, it splits the tree with the best fit whereas boosting algorithms split the tree depth wise or level wise rather than leaf-wise. So when growing on the same leaf in Light GBM, the leaf-wise algorithm can reduce more loss than the level-wise algorithm and hence results in much better accuracy which can rarely be achieved by any of the existing boosting algorithms. Also, it is surprisingly very fast, hence the word ‘Light’. Leaf wise splits lead to increase in complexity and may lead to overfitting and it can be overcome by specifying another parameter max_depth parameter. Advantages of LightGBM faster training speed and higher efficiencyLight GBM use histogram based algorithm i.e it buckets continuous feature values into discrete bins which fasten the training procedure. lower memory usageReplaces continuous values to discrete bins which result in lower memory usage. better accuracy than any other boosting algorithmIt produces much more complex trees by following leaf wise split approach rather than a level-wise approach which is the main factor in achieving higher accuracy. compatibility with large datasetsIt is capable of performing equally good with large datasets with a significant reduction in training time as compared to XGBOOST. parallel learning supported lightGBM调参(常用参数) taskdefault= train, option: train, prediction applicationdefault= regression, option: regression, binary, multiclass, lambdarank(lambdarank application) datatraining data, 这个比较诡异，你需要创建一个lightGBM类型的data num_iterationsdefault =100, 可以设置为的大一些，然后使用early_stopping进行调节。 early_stopping_rounddefault =0, will stop training if one metric of one validation data doesn’t improve in last early_stopping_round rounds. num_leavesdefault =31, number of leaves in a tree devicedefault =cpu, options: gpu, cpu, choose gpu for faster training. max_depthspecify the max depth to which tree will grow, which is very important. feature_fractiondefault =1, specifies the fraction of features to be taken for each iteration. bagging_fractiondefault =1, spefifies the fraction of data to be used for each iteration and it generally used to speed up the training and avoid overfitting. max_binmax number of bins to bucket the feature values.因为模型是基于bin训练的，如果bin 数量越多，得到better accuracy,同时更加容易 overfitting. num_threads labelspecify the label columns. categorical_featurespecify the categorical features num_classdefault =1, used only for multi-class classification referrencewhich-algorithm-takes-the-crown-light-gbm-vs-xgboostLightGBM 如何调参官方文档param_tuning官方文档parameter XGBoost调参Advantage of XGBoost regularizationstandard GBM implementation has no regularization, in fact, XGBoost is also known as ‘regularized boosting’ technique. parallel processingwe know that boosting is sequential process so how can it be parallelized? this link to explore further. high flexibilityXGBoost allow users to define custom optimization objectives and evaluation criteria handling missing valuesvery useful property. XGBoost has an in-built routine to handle missing values. User is required to supply a different value than other observations and pass that as a parameter. XGBoost tries different things as it encounter a missing value on each node and learns which path to take for missing values in future. Tree pruningA GBM would stop splitting a node when it encounters a negative loss in the split. Thus it is more of a greedy algorithm. XGBoost on the other hand make splits upon the max_depth specified and then start pruning the tree backwards and remove splits beyond which there is no positive gain. built-in cross-validationThis is unlike GBM where we have to run a grid-search and only a limited values can be tested. continue on existing model XGBoost Parametersgeneral parametersGeneral Parameters: Guide the overall functioning booster:default =gbtree, can be gbtree, gblinear or dart. 一般使用gbtree. silent:default =0, silent mode is activated if set to 1(no running messages will be printed) nthread:default to maximum of threads. booster parametersBooster Parameters: Guide the individual booster (tree/regression) at each step eta(learning rate):default=0.3, typical final values to be used: 0.01-0.2, using CV to tune min_child_weight:minimum sum of instance weight (hessian) needed in a child. If the tree partition step results in a leaf node with the sum of instance weight less than min_child_weight, then the building process will give up further partitioning. In linear regression mode, this simply corresponds to minimum number of instances needed to be in each node. The larger, the more conservative the algorithm will be.default =1,too high values can lead to under-fitting, it should be tuned using CV. 数值越小越容易过拟合，越大越容易 under-fitting. max_depth:default =6, typical values: 3-10, should be tuned using CV. gamma:default =0, Gamma specifies the minimum loss reduction required to make a split.如果在分裂过程中小于该值，那么就不会继续分裂。 subsample:default =1, typical values: 0.5-1. Denotes the fraction of observations to be randomly samples for each tree. colsample_bytree:default =1, typical values: 0.5-1. colsample_bytree和subsample不同点：colsample_by是特征的随机fraction, subsample是rows的随机fraction。 lambda:default =1, L2 regularization term on weights(analogous to Ridge regression). Though many data scientists don’t use it often, it should be explored to reduce overfitting. alpha:default =0, L1 regularization term on weight (analogous to Lasso regression) scale_pos_weight:default =1, a value greater than 0 should be used in case of high class imbalance as it helps in faster convergence. learning task parametersLearning Task Parameters: Guide the optimization performed objectivebinary: logistic- returns predicated probability(not class)multi: softmax- returns predicated class(not probabilities)multi: softprob- returns predicated probability of each data point belonging to each class. eval_metircdefault according to objective(rmse for regression and error for classification), used for validation data.typical values: rmse(root mean square error), mse(mean absolute error), logloss(negative log-likelihood), error(binary classification error rate, 0.5 threshold), auc(area under the curve) seeddefault =0, used for reproducible results and also for parameter tuning. Control OverfittingThere are in general two ways that you can control overfitting in xgboost. The first way is to directly control model complexity. The second way is to add regularization parameters Referrencecomplete guide parameter tuning xgboost with codes python官方文档 param_tuning官方文档 parameter 补充一些理论知识 LightGBM 和 XGBoost 的一些区别 树的增长方式 这个原先是lightGBM所特有的，然后xgboost 在最新的版本上也实现该中方式，这估计就是开源，并不是一成不变的。两者都是基于叶子进行增长的，但是增长的方式是不同的。一种是 level-wise 一种是 leaf-wiseboth xgboost and lightGBM use the leaf-wise growth strategy when growing the decision tree.there are two strategies that can be employed: level-wise and leaf-wise. level-wise maintains a balanced tree（平衡树，左右子树的高度差不超过1）;但是 leaf-wise 这个就比较随意了。这个是这两者的区别：当叶子总数相同的时候，leaf-wise 这种生长方式得到的树的深度是大于 level-wise 的深度的。Compared to the case of level-wise growth, a tree grown with leaf-wise growth will be deeper when the number of leaves is the same. find the best split The key challenge in training a GBDT is the process of finding the best split for each leaf. The computational complexity is thus $O \left( n _ { \text {data} } n _ { \text {features} } \right)$.这两者采用的方式都是：现阶段的中的的数据 数据量和特征量都是很大的，所以这种方式是不可取的。然后这两种方法都是采用了 Histogram-based methods，这样最后的时间复杂度降低到：reducing the computational complexity to $O \left( n _ { d a t a } n _ { b i n s } \right)$. 这个复杂度是取决于number of bins。这个是引入了一个超参数，number of bins ，trade off between precision and time, 当更多的 bins 的时候，这个precision 会提高，但是 time 也会增大。 Ignoring sparse inputs 这个是处理缺省值（或者 0）的手段：两者在split 分裂点的时候，都是先不处理数值 0；然后找到分裂点之后，把0 放到哪边造成的loss 下降的比较大，然后就放到哪边。 Subsampling the data: Gradient-based One-Side Sampling (lightGBM)biased sampling(抽样的基本原则是随机性，但是在抽样过程中由于一系列因素造成偏差抽样，造成样本是不符合真实样本的分布)这个就是缓解，也不是为了彻底让其均衡，lightgbm increases the weigths of the samples.This means that it is more efficient to concentrate on data points with larger gradients.In order to mitigate this problem, lightGBM also randomly samples from data with small gradients.lightGBM increases the weight of the samples with small gradients when computing their contribution to the change in loss (this is a form of importance sampling, a technique for efficient sampling from an arbitrary distribution). xgboost + lr如果清楚GBDT原理的话（机器学习-一文理解GBDT的原理-20171001），最后模型预测结果也是将叶子结点进行线性加总，权值就是训练得到的各叶子结点的权重。如果将GBDT得到的叶子节点特征喂入LR的话，其实相当于用逻辑回归对叶子结点的权重进行了重新训练计算，在这里GBDT只是将数据中的非线性关系进行了一层转换，从而可以利用lr进行线性组合。 除了原理和实现方式之外，其实还有一些小的细节需要注意的，比如： 1、进行LR训练时，原始特征是否要加进去；原始特征的加入在xgb欠拟合的时候起的作用更大，当xgb树的颗数上升到一定数量时，原始特征的加入没什么提升。2、gbdt训练的模型效果对最终LR的训练出来的模型有多大的影响； 当xgb训练充分时，lr直接利用xgb叶子结点的编码特征在合适的惩罚系数下可以训练得到和xgb一样的甚至更好的效果（不显著）； 3、树的棵数，叶子结点数，学习率，树的深度，采样会如何影响最终的模型效果等等； lr利用xgb叶子结点的编码特征进行训练，分别在不同的C下训练得到train和test的auc值 简单地说，就是把gbdt的输出，作为logistic regression的输入，最后得到一个logistic regression模型。例如，gbdt里有3棵树T1,T2,T3，每棵树的叶节点个数为4，第i个树的第j个叶节点是Li,j。当gdbt训练完成之后，样本X1在第一棵树中被分到了第3个叶节点上，也就是L1,3，那么这个样本在T1上的向量表达为(0,0,1,0)。 样本X1在T2被分到了L2,1，那么X1在T2上的向量表达为(1,0,0,0)样本X1在T3被分到了L3,4，那么X1在T3上的向量表达为(0,0,0,1)那么X1在整个gbdt上的向量表达为 (0,0,1,0,1,0,0,0,0,0,0,1)所以每个样本都会被表示为一个长度为12的0-1向量，其中有3个数值是1。 然后这类向量就是LR模型的输入数据。 LR 和 XGOOST 是 CTR 中常用的两种模型，二者各有优缺点，在 facebook 中使用 XGBOOST（提取特征） + LR（预测） 的方式。GBDT 模型擅长处理连续特征值，而 LR 则擅长处理离散特征值。 在工业界，很少直接将连续值作为特征喂给逻辑回归模型，而是将连续特征离散化为一系列0、1特征交给逻辑回归模型，这样做的优势有以下几点： 123451.稀疏向量内积乘法运算速度快，计算结果方便存储，容易scalable（扩展）。2.离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄&gt;30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰。3.逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合。4.离散化后可以进行特征交叉，由M+N个变量变为M*N个变量，进一步引入非线性，提升表达能力。5.特征离散化后，模型会更稳定，比如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。当然处于区间相邻处的样本会刚好相反，所以怎么划分区间是门学问。 先用已有特征训练Xgboost模型，然后利用Xgboost模型学习到的树来构造新特征，最后把这些新特征加入原有特征一起训练模型。构造的新特征向量是取值0/1的，向量的每个元素对应于Xgboost模型中树的叶子结点。当一个样本点通过某棵树最终落在这棵树的一个叶子结点上，那么在新特征向量中这个叶子结点对应的元素值为1，而这棵树的其他叶子结点对应的元素值为0。新特征向量的长度等于XGBoost模型里所有树包含的叶子结点数之和。最后将新的特征扔到LR模型进行训练。 xgboost+lr模型融合方法用于分类或者回归的思想最早由facebook在广告ctr预测中提出，其论文Practical Lessons from Predicting Clicks on Ads at Facebook有对其进行阐述。在这篇论文中他们提出了一种将xgboost作为feature transform的方法。大概的思想可以描述为如下：先用已有特征训练XGBoost模型，然后利用XGBoost模型学习到的树来构造新特征，最后把这些新特征加入原有特征一起训练模型。构造的新特征向量是取值0/1的，向量的每个元素对应于XGBoost模型中树的叶子结点。当一个样本点通过某棵树最终落在这棵树的一个叶子结点上，那么在新特征向量中这个叶子结点对应的元素值为1，而这棵树的其他叶子结点对应的元素值为0。新特征向量的长度等于XGBoost模型里所有树包含的叶子结点数之和。最后将新的特征扔到LR模型进行训练。实验结果表明xgboost+lr能取得比单独使用两个模型都好的效果。 实现角度 在实践中的关键点是如何获得每个样本落在训练后的每棵树的哪个叶子结点上。 A、对于Xgboost来说，因为其有sklearn接口和自带接口，因此有两种方法可以获得： ①、sklearn接口。可以设置pre_leaf=True获得每个样本在每颗树上的leaf_Index。XGBoost官方文档 ②、自带接口。利用apply()方法可以获得leaf indices。SKlearn GBDT API ！！！此过程需注意： 无论是设置pre_leaf=True还是利用apply()方法，获得的都是叶子节点的 index，也就是说落在了具体哪颗树的哪个叶子节点上，并非是0/1变量，因此需要自己动手去做 onehot 编码。onehot 可以在 sklearn 的预处理包中调用即可。 onehot的知识点可以参看这篇博客。 B、对于其它的树模型，如随机森林和GBDT，我们只能使用apply()方法获得leaf indices。 XGBoost+LR融合的原理和简单实现xgboost和LR模型级联 对于原始数据的编码方式 连续特征离散化 离散的特征one-hot 或者特征组合 使用 boosting 方式将连续特征离散化（xgboost 特征提取）XGBoost + LR 是一种有效的特征工程手段 利用 GBDT+LR 融合的方案有很多好处，利用 GDBT 主要是发掘有区分度的 特征和特征组合： LR 模型无法实现特征组合，但是模型中特征组合很关键，依靠人工经验非常耗时而且不一定能有好的效果。利用 GBDT 可以自动发现有效的特征、特征组合。GBDT 每次迭代都是在减少残差的梯度方向上面新建一棵决策树，GBDT 的每个叶子结点对应一个路径，也就是一种组合方式。 为进一步理解GBDT-LR的运行过程，同时排查编程实现的错误。这里对整个过程中的数据形态变化进行检视。数据的处理流程为： 原始特征数据 -&gt; 标签编码 -&gt; GBDT -&gt; 独热编码 -&gt; LR -&gt; 输出。 xgboost和LR模型级联 12345更新：：：经过实践检验后，我们发现xgb+lr相比xgb几乎没有提升。xgb不像DNN，其本身的特征交叉能力是有限的，用xgb作为lr的特征交叉部分是远远不够的，所以还是需要在LR这边做大量的人工特征交叉设计，我们当时并没有一个实践检验过很好的单LR模型，因为我们没有时间和人力做精细的人工特征交叉设计。其实当人工的特征设计足够优秀的时候，特征维度很丰满的单LR已经很强了，xgb那点叶子结点的影响力其实不大。一个好的单LR模型其实根本不太需要xgb这点交叉，一个单xgb模型本身也足够优秀，级联一个垃圾LR不如不级联。 Facebook在2014年的思路是没错的，xgb能够很好的处理连续性型特征，LR来补齐xgb对离散类特征信息的盲区。但是工程上如果想有提升，不可避免还是要做特征工程，要踩得坑太多而且收益甚微，有这个精力不如搞深度学习。想要有质变的话建议直接上深度学习，Wide&amp;Deep用Deep部分代替xgb，DNN的特征交叉能力和信息量要比xgb大得多。 xgboost 确实能够处理连续特征，但是在特征交叉方面，还是有不足的，所以要么是特征工程，增强对离散特征LR。要么是深度网络， DNN（或者transformer）的特征交叉能力和信息量要比xgb大得多。xgboost也就是做了特征筛选和特征交叉。人工特征工程做的够好，LR是可以俯瞰众生，但是考虑到花费的人力物力，还是选模型级联吧！特征工程慢慢做= = LR是广义线性模型，能够并行化处理大量亿万级特征的训练样本。LR模型是只使用离散型特征的，连续型特征不是不能用，而是用了以后效果不好，对模型有负影响。LR快速高效，完全可以处理高纬度稀疏的离散化特征。 复习笔记 xgboost 的优势 (1) 能够 handle missing values (2) 数的增长方式，一种是基于level 增长，一种是基于叶子节点增长（如果是相同的结点数量，那么基于叶子增长的方式是更加的深） 调参是能够调整的参数（max_depth, 随机样本的比例，随机特征的比例），不是常见的参数（比如说 ntthread 线程数量， silent 与否）。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>LightGBM</tag>
        <tag>XGBoost</tag>
        <tag>xgboost+lr</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[那些年的算法题目（二）]]></title>
    <url>%2F2018%2F07%2F21%2F%E9%82%A3%E4%BA%9B%E5%B9%B4%E7%9A%84%E7%AE%97%E6%B3%95%E9%A2%98%E7%9B%AE%EF%BC%88%E4%BA%8C%EF%BC%89%2F</url>
    <content type="text"><![CDATA[刷题笔记（二） 最长公共子串 给定两个字符串A和B，长度分别为m和n，要求找出它们最长的公共子串，并返回其长度。例如： 12A = &quot;HelloWorld&quot;B = &quot;loop&quot; 第一种做法是暴力枚举， 时间复杂度是 $O(n^3)$； 第二种做法时间复杂度 $O(n^2)$，使用dp 的做法。那么状态转移方程是 $$ dp[i][j] = \begin{cases}0 &amp; str1[i-1] !=str2[j-1] \\dp[i-1][j-1] +1&amp; str1[i-1] ==str2[j-1]\end{cases}$$ 注意公共子串，要求的是需要连续的，所以当发现不连续的时候， $dp[i][j] =0$ 应该是这样的考虑。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697#include&lt;iostream&gt;#include&lt;string&gt;#include&lt;vector&gt;using namespace std;// 这个是最基础的算法， 使用暴力枚举，时间复杂度是 n^3string common_str(string &amp;str1, string &amp; str2)&#123; string res; int n =str1.size(), m =str2.size(); //保证n&gt;= m if(n&lt; m) &#123; res =common_str(str2, str1); return res; &#125; int max_l =0; for(int i=0; i&lt;n; i++) &#123; for(int j =i+1; j&lt;n; j++) &#123; // start point , len string substr= str1.substr(i,j-i+1); if( str2.find(substr) != str2.npos) &#123; if(j -i+1 &gt; max_l) &#123; res =substr; max_l =j-i+1; &#125; &#125; &#125; &#125; return res;&#125;// 使用二维dp 进行优化，时间复杂度是O(n^2) 空间是O(n^2)int common_str2(string &amp;str1, string &amp;str2)&#123; int n =str1.size(), m =str2.size() ; vector&lt;vector&lt;int&gt;&gt; dp(n+1, vector&lt;int&gt;(m+1, 0)); if(n &lt;m) return common_str2(str2, str1); int max_l =0; for(int i =1; i&lt;=n ; i++) &#123; for(int j =1; j&lt;=m ; j++) &#123; if(str1[i-1] ==str2[j-1]) &#123; dp[i][j] =dp[i-1][j-1] +1; max_l =max(max_l, dp[i][j]); &#125; else dp[i][j] =0; &#125; &#125; // 这个是dp 思想求解，但是最长的长度不是dp[n][m] return max_l;&#125;// 使用一维dp 进行优化int findLongest(string A, int n, string B, int m) &#123; if(n == 0 || m == 0) return 0; int rs = 0; int dp[n + 1][m + 1]; for(int i = 0 ; i &lt;= n; i++)//初始状态 dp[i][0] = 0; for(int i = 0; i &lt;= m; i++) dp[0][i] = 0; for(int i = 1; i &lt;= n; i++) for(int j = 1; j&lt;= m; j++) &#123; if(A[i - 1] == B[j - 1]) &#123; dp[i][j] = dp[i -1][j - 1] + 1; rs = max(rs,dp[i][j]);//每次更新记录最大值 &#125; else//不相等的情况 dp[i][j] = 0; &#125; return rs;//返回的结果为rs&#125;int main()&#123; string str1, str2; //cin &gt;&gt;str1 &gt;&gt;str2; str1 ="abc", str2 ="bcd"; //string res =common_str(str1, str2); //for(auto u : res) cout &lt;&lt; u; int max_l =common_str2(str1, str2); cout &lt;&lt; max_l&lt;&lt; endl; //cout &lt;&lt; findLongest(str1, 3, str2, 3)&lt;&lt; endl; cout &lt;&lt;endl; return 0;&#125; python 解法 12345678910111213141516171819def longest_substring(str1, str2): n, m =len(str1), len(str2) dp=[[0] *m]*n max_ =0 res ="" for i in range(n): for j in range(m): if str1[i] ==str2[j]: if i and j: dp[i][j] =dp[i-1][j-1] +1 else: dp[i][j] =1 if dp[i][j] &gt;max_: max_ =dp[i][j] res += str1[i] return resstr1 ="abc"str2 ="bcd"print(longest_substring(str1, str2)) 求解最长公共子序列 dp[i][j] 表示string1中第 i 位置之前和string2 中第j 位置之前，最长的公共子序列。(求解的是max 操作，对于集合的划分可以重复，但是不能缺少) 123456789101112131415161718192021222324252627282930#include&lt;iostream&gt;#include&lt;string&gt;#include&lt;vector&gt;using namespace std;int max_sub_len(string &amp; str1, string &amp;str2)&#123; int res =0; int n =str1.size(), m =str2.size(); vector&lt;vector&lt;int&gt;&gt; dp(n+1, vector&lt;int&gt;(m+1, 0)); // 坐标的转换 // 如果是string，这个坐标是可以从 for(int i =0; i&lt;n; i++) &#123; for(int j =0; j&lt;m; j++) &#123; dp[i+1][j+1] =max(dp[i][j+1], dp[i+1][j]); if(str1[i] ==str2[j]) dp[i+1][j+1] =max(dp[i+1][j+1], dp[i][j] +1); &#125; &#125; return dp[n][m];&#125;int main()&#123; string str1, str2; cin &gt;&gt; str1 &gt;&gt; str2; int res =max_sub_len(str1, str2); cout &lt;&lt; res&lt;&lt; endl; return 0;&#125; python 解法 123456789101112131415161718# 最长公共子序列，这个可以是离散的def longest_common_string(str1, str2): n, m =len(str1), len(str2) dp =[[0]* (m+1)]*(n+1) res ="" for i in range(1, n+1): for j in range(1, m+1): dp[i][j] =max(dp[i-1][j], dp[i][j-1]) if str1[i-1] ==str2[j-1]: if dp[i][j]&lt; dp[i-1][j-1] +1: res +=str1[i-1] dp[i][j] = dp[i-1][j-1] +1 # dp[m][n] 是最大值 return resstr1 ="abc"str2 ="adc"print(longest_common_string(str1, str2)) 最长上升子序列 最长公共上升子序列 暴力的解法是 $O(n^2)$，就是枚举当前位置之前的数字是否合法，如果合法，那么就需要 +1 操作。对于dp 动态规划，含义，初始化和动态转移方程。dp[i] 表示位置为i 的所有的前面的数字有多少个上升序列数字。 123456789101112131415161718192021222324class Solution &#123;public: // 最长递增子序列，动规的角度去考虑， 枚举法， 时间复杂度是O(n^2) int lengthOfLIS(vector&lt;int&gt;&amp; nums) &#123; int n =nums.size(); if(n ==0) return 0; // 遍历当前位置之前的所有的位置，统计出现的小于当前数值的数字，就可以得到最后的结果 int res =1; vector&lt;int&gt; dp(n, 1); for( int i =1; i&lt;n; i++) &#123; // 这个是保证之前的所有位置的数字 for(int j =0; j&lt;i; j++) &#123; if(nums[i]&gt; nums[j]) dp[i] =max(dp[i], dp[j]+1); &#125; res =max(res, dp[i]); &#125; for(auto u : dp) cout &lt;&lt; u&lt;&lt;" "; cout &lt;&lt; endl; return res; &#125;&#125;; 使用二分进行优化，空间上的复杂度不变，但是时间上的复杂度由原先的 $O(n^2)$ 优化成 $O(nlogn)$，这个样子。使用数组存储的是有序的数字，那么最后数组的长度就是最长上升序列的长度。 一旦有序之后，那么就可以使用二分等思想进行 123456789101112131415161718192021222324class Solution &#123;public: int lengthOfLIS(vector&lt;int&gt;&amp; nums) &#123; // 使用二分查找进行了优化 时间复杂度从O(n^2) 优化成了 O(nlogn) int n =nums.size(); if(n ==0) return 0; vector&lt;int&gt; h(n+1, 0); h[1] =nums[0]; int max_ =1; for(int i =1; i&lt; n; i++) &#123; int l =1, r =max_; while(l&lt;=r) &#123; int mid =l +r &gt;&gt;1; if(h[mid] &lt; nums[i]) l =mid +1; else r =mid -1; &#125; h[l] =nums[i]; if(l &gt; max_) max_ =l; &#125; return max_; &#125;&#125;; python 解法 12345678910111213141516171819202122class Solution(object): def lengthOfLIS(self, nums): """ :type nums: List[int] :rtype: int """ # 最长上升子序列，暴力解法， n^2 的时间复杂度， 然后怎么优化呢？ 因为这个是有序的，所以可以考虑使用 二分进行优化 n =len(nums) if(n ==0): return 0; h =[0] *(n+1); h[1] =nums[0] max_ =1 for i in range(1, n): l, r = 1, max_ while(l &lt;=r ): mid =l +r &gt;&gt;1; if(h[mid] &lt; nums[i]): l =mid +1 else: r =mid-1 h[l] =nums[i] if(l &gt; max_): max_ =l return max_ 将嵌套列表转换成一维列表 123456789101112res =[]def flatten(items): for item in items: if isinstance(item, int) or isinstance(item, str): #flatten(item) res.append(item) else: flatten(item) #res.append(item) return resitems =[1,2 , 'a', [3, 4, ], (5, 6)]print(flatten(items)) 其实还可以写成一个小的生成器的形式 12345678910111213141516def generator(a): for each in a: if not isinstance(each, list): yield each else: yield from generator(each)a =[1,2, [3, 4], 5]print(generator(a))def generator1(a): for each in a: if isinstance(each, collections.Iterable) : yield from generator1(each) else: yield eachprint(generator1(a)) remove linked list ellements c++ 版本，主要想要个强调，如果是删除一个元素，那么需要处理f 删除头结点的问题，所以使用一个 dummy 这样的虚拟头结点，可以处理边界条件。对于有可能删除头结点的情况下，一般使用 dummy 这种虚拟头结点 123456789101112131415161718192021222324252627/** * Definition for singly-linked list. * struct ListNode &#123; * int val; * ListNode *next; * ListNode(int x) : val(x), next(NULL) &#123;&#125; // 这个属于显式构造函数 * &#125;; */class Solution &#123;public: ListNode* removeElements(ListNode* head, int val) &#123; ListNode * dummy = new ListNode(-1); ListNode * p =dummy; p -&gt;next =head; while(p-&gt;next) &#123; auto nex =p-&gt;next; if(nex-&gt;val == val) &#123; p-&gt;next = nex-&gt;next; &#125; else p =p-&gt;next; &#125; return dummy-&gt;next; &#125;&#125;; python 版本代码123456789101112131415161718# definition of listnodeclass ListNode: def __init__(self, x): self.val =x self.next =None class Solution: def removeElements(self, head: ListNode, val: int) -&gt; ListNode: dummy = ListNode(-1) p =dummy p.next =head while p.next: nex =p.next if(val == nex.val): p.next =nex.next else: p= p.next return dummy.next 74. Search a 2D Matrix python 解法123456789101112class Solution: def searchMatrix(self, matrix: List[List[int]], target: int) -&gt; bool: if not matrix: return False n =len(matrix) if n ==0: return false m =len(matrix[0]) i, j =0, m -1 while i&lt; n and j &gt;=0: if(matrix[i][j] == target): return True elif matrix[i][j] &gt; target: j -=1 else : i +=1 return False c++ 解法是类似的，就不写了 统计中英文字符的个数 和 统计英文单词并按照倒序排列 关于统计，提供了常规做法和正则匹配两种方法 123456789101112131415161718192021222324252627282930313233343536373839import stringstr1 ="找出字符串中的中英文、i only regret空格、数字、标点符号 1 99 0个数"def count_(str): cout_num =cout_zh =cout_en =cout_space =cout_pu =0 for ch in str: if ch.isspace(): cout_space +=1 elif ch.isdigit(): cout_num +=1 elif ch in string.ascii_letters: cout_en +=1 elif ch.isalpha(): # 这里使用的是减法的思路 cout_zh +=1 else: cout_pu +=1 print("英文字符数量：", cout_en) print("中文字符数量：", cout_zh) print("数字个数：", cout_num) print("空格个数: ", cout_space) print("特殊字符个数:",cout_pu)#count_(str1)# 方法二，使用正则表达式import redef cout2(str): regex_digit = re.compile(r'[0-9]' ) regex_en =re.compile(r'[a-zA-Z]') regex_zh =re.compile(r'[\u4E00-\u9FA5]') digit_list =regex_digit.findall(str) en_list =regex_en.findall(str) zh_list =regex_zh.findall(str) print("数字序列", digit_list) print("英文序列", en_list) print("中文序列", zh_list)cout2(str1) 统计的单词的个数（注意不是字母 char类型的）, 有不同的方法 a. 在linux 环境下文件格式：文件中每一行为一个单词1234sort filename | uniq -c| sort -nr-c 输出重复次数 -n 按照数值比较排序-r 逆序输出结果 b. 全程python 操作123456789101112131415161718192021222324252627282930import sysword2count =&#123;&#125;def word_count(): f1 =open("", 'r') f2 =open("", 'w') while True: line =f1.readline() if not line: break # python 内置的split 只能使用单个 分割符 line =line.replace(',', ' ') line =line.replace('.', ' ') line =line.replace('!', ' ') line =line.strip().split(" ") # 可以使用 re模块中的 split() ，使用多个分割符对句子进行分割 import re line =re.split(',.?', line) for item in line: if item not in word2count: word2count[item] =1 else: word2count[item] +=1 # 按照逆序进行排序 res =sorted(word2count.item(), key =lambda x : x[1], reverse =True) for item in res: f2.write(item[0]) f1.close() f2.close() python 中常见的大小写问题 1234print(str.upper()) # 把所有字符中的小写字母转换成大写字母print(str.lower()) # 把所有字符中的大写字母转换成小写字母print(str.capitalize()) # 把第一个字母转化为大写字母，其余小写print(str.title()) # 把每个单词的第一个字母转化为大写，其余小写 最长01相同子串已知一个长度为N的字符串，只由0和1组成， 求一个最长的子串，要求该子串出现0和1的次数相等。思路：最简单的方式是先生成字串，然后判断每个字串是否满足0的个数和1的个数相同。这种暴力求解时间复杂度O(n^3),明显是不合理的。下面说一下简单的做法：定义一个数组B[N]，B[i]表示从A[0…i]中 num_of_0 - num_of_1，0的个数与1的个数的差 。那么如果A[i] ~ A[j]是符合条件的子串，一定有 B[i] == B[j]，因为中间的部分0、1个数相等，相减等于0。 时间复杂度：O(n)，空间复杂度：O(n) 注意这个是求解的长度，是一个最值问题，而不是最长的01 子串本身。所以使用临时数组记录目前为止的01 的差值，然后选择最大的就行了。时间复杂度是O(N)，空间复杂度是O(N)。 算法的优化的关键在于，可以基于前一步的计算结果继续往下计算。前一步算出了 (i-1) 的01 的差值，那么下一步可以计算前 i 的01 的差值。 12345678910111213141516171819202122232425262728def lengest01SubStr(s): ''' 最长0,1 相等的子串长度 ''' count =[0, 0] B =[0]*len(s) dic =&#123;&#125; # 保存 0 1 的差值 lengest =0 for i in range(len(s)): count[int(s[i])] +=1 B[i] =count[0] - count[1] # start from 0th index if B[i] ==0: lengest +=1 continue if B[i] in dic: # i -dic[B[i]] , not from 0th index lengest =max(lengest, i- dic[B[i]]) else: dic[B[i]] =i return lengesta ='1011010'b ='10110100'print(lengest01SubStr(a)) # 6 # '011010'print(lengest01SubStr(b)) # 8 # '10110100' 顺时针打印矩阵 输入一个矩阵，按照从外向里以顺时针的顺序依次扫印出每一个数字。 使用四个循环，就可以解决，这个应该属于找规律的题目。 思路：找到左上角的，一个start_point， 然后根据这个点进行上下左右的循环。 python 中的 range() 是一种左闭右开的区间，并且在逆序遍历的时候，区间的值是不变的，最后使用 -1 就OK了。还有遍历时候的 i, j 都只是一种index，如果一个够用的话，那么就没有必要使用两个。 123456789101112131415161718192021222324252627282930313233343536373839404142def printMatrix(matrix): if not matrix or matrix ==[[]]: return # 第一次见这样判断空的matrix row =len(matrix) column =len(matrix[0]) # 这里的left, right, up, down 都是真实能够access到数据的 left =0 right =column -1 up =0 down =row -1 res =[] while left &lt;right and up &lt;down: # from left to right for i in range(left, right+1): res.append(matrix[up][i]) # from up to down for i in range(up+1, down+1): res.append(matrix[i][right]) # from right to left for i in range(right-1, left-1, -1): res.append(matrix[down][i]) for i in range(down-1, up, -1): res.append(matrix[i][left]) left +=1 right -=1 up +=1 down -=1 # 最后对于这种特殊情况的处理是容易忘记的 # left one row 这种情况很特殊，只是从左往右遍历 if up ==down and left &lt;right: for i in range(left, right+1): res.append(matrix[up][i]) # left one column 只有可能是从上往下遍历 if left ==right and up &lt;down: for i in range(up, down+1): res.append(matrix[i][left]) if up ==down and left ==right: res.append(matrix[left][up]) return resprint(printMatrix(matrix)) 最小调整有序 有一个整数数组，请编写一个函数，找出索引m和n，只要将m和n之间的元素排好序，整个数组就是有序的。注意：n-m应该越小越好，也就是说，找出符合条件的最短序列。给定一个int数组A和数组的大小n，请返回一个二元组，代表所求序列的起点和终点。(原序列位置从0开始标号,若原序列有序，返回[0,0])。保证A中元素均为正整数。 样例12[1,4,6,5,9,10],6返回：[2,3] 第一种比较暴力，时间复杂度 $O(nlogn)$ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748/* 最小调整有序 找出索引m 和n，只要是m 和n 之间的元素排好序，整个数组就是有序的。注意n-m 应该是越小越好，找出符合条件的最短序列 */#include&lt;iostream&gt;#include&lt;vector&gt;using namespace std;vector&lt;int&gt; find_segment(vector&lt;int&gt;&amp; arr)&#123; vector&lt;int&gt; res(2, 0); vector&lt;int&gt; tmp(arr.begin(), arr.end()); sort(arr.begin(), arr.end()); for(int i =0; i&lt; arr.size(); i++) &#123; if(tmp[i] != arr[i]) &#123; res[0] =i; break; &#125; &#125; for(int i =arr.size() -1; i&gt;=0; i--) &#123; if(tmp[i] != arr[i]) &#123; res[1] =i; return res; &#125; &#125; return res;&#125;int main()&#123; int n; vector&lt;int&gt; arr; cin &gt;&gt;n; for(int i =0; i&lt; n; i++) &#123; int tmp; cin &gt;&gt; tmp; arr.push_back(tmp); &#125; vector&lt;int&gt; res =find_segment(arr); for(auto u: res) cout &lt;&lt; u &lt;&lt; " "; cout &lt;&lt; endl; return 0;&#125; 第二种是$O(n)$ 的时间复杂度。分成两个步骤 从左往右找，如果当前元素小于之前的最大元素则说明当前元素应处于A[first,last]无序序列中而且当前元素是当前最大下标的无序元素。 从右往左找，如果当前元素大于之前的最小元素则说明当前元素应处于A[first,last]无序序列中而且当前元素是当前最小下标的无序元素。 关键在于找到之后，继续遍历直到数组的最后； 增加新的一个样例 1对于一个整数数组[1,3,5,8,4,10,12,9,15,17]，则最短无序数组为[5,8,4,10,12,9]。 好好思考一下，当时为什么想不到，这种思路是什么，我当时是想到了 $O(n)$ 遍历，但是没有相当即使第一回发现了不符合要求的数字，那么也是要遍历完，然后不断地的更新结果的。 开始的想法是正确的，左右两次遍历。但是需要不断的更新最值，从右往左是最小值，从左往右是最大值。不能只是和周围的数字进行比较。 简而言之就是找出，逆序的， 因为是有序的，所以如果发现一个比在之前数字小，那么说明找到了右端点；从右向左，如果发现比左边大，说明找到了左端点；需要转换一下思路 12345678910111213141516171819202122class Rearrange &#123;public: vector&lt;int&gt; findSegment(vector&lt;int&gt; nums, int n) &#123; // write code here int maxv =nums[0], minv =nums[n-1]; vector&lt;int&gt; res(2, 0); for(int i =0; i&lt; n; i++) &#123; if(nums[i] &gt;= maxv) maxv =nums[i]; else res[1] =i; &#125; for(int i =n -1; i&gt;=0; i--) &#123; if(nums[i] &lt;= minv )minv =nums[i]; else res[0] =i; &#125; return res; &#125;&#125;; python 实现的代码123456789101112131415# -*- coding:utf-8 -*-class Rearrange: def findSegment(self, nums, n): # write code here if n==1: return [0, 0] minv, maxv =nums[-1], nums[0] res =[0, 0] for i in range(n): if nums[i] &gt;= maxv: maxv =nums[i] else: res[1] =i for i in range(n-1, -1, -1): if nums[i] &lt;=minv: minv =nums[i] else: res[0] =i return res 915. Partition Array into Disjoint Intervals 考察想法的题目真的是太难了，如果想不出来，那么真的是不会做。分成左右两个部分，左边的全部比右边的小。那么只要保证左边最大的是小于等于右边最小的就行。这个思路是，数学题。下面就需要使用一个 $O(n)$ 空间复杂度记录从 $i$ 到$n$ 的最小值。不用使用数组保存前 $i$ 的最大值，直接在遍历的时候进行判断进行了。 1234567891011121314151617181920212223242526class Solution &#123;public: // tong // 因为要求分成两部分，左边的都比右边的小， // 使用一个数组 tmp[i] 表示从 i 到 n-1 最大值 // 然后经过一次遍历之后，如果某个arr[i] &lt; tmp[i] 那么就得到了正确的解 int partitionDisjoint(vector&lt;int&gt;&amp; nums) &#123; int n =nums.size(); vector&lt;int&gt; minv(n, INT_MAX); minv[n-1] =nums[n-1]; for(int i =n-2; i&gt;=0; i--) &#123; minv[i] =min(minv[i+1], nums[i] ); //cout &lt;&lt; maxv[i] &lt;&lt; endl; &#125; // 这里还需要记录到到当前位置的最大值 int maxv =0; for(int i =0 ; i&lt;n-1; i++) &#123; maxv= max(maxv, nums[i]); if (maxv &lt;=minv[i+1]) return i +1; &#125; return -1; &#125;&#125;; python 实现 12345678910111213141516171819import sysclass Solution(object): def partitionDisjoint(self, A): """ :type A: List[int] :rtype: int """ n =len(A) minv=[sys.maxsize]*n minv[n-1] =A[n-1] for i in range(n-2, -1, -1): minv[i] =min(minv[i+1], A[i]) #print(minv) maxv =0 for i in range(n-1): maxv=max(maxv, A[i]) if(maxv &lt;= minv[i+1]): return i +1 return -1 149. Max Points on a Line 这个题目需要处理竖直直线，重复的点等特殊情况。剩下的一般情况是需要使用 顶点+ 斜率进行表示，使用dictionary 进行表示。 注意在python 中使用的是 gcb 先求解了 最大公因数，然后再进行表示。使用c++ 实现，那么时间复杂度是$O(n^2)$ 在 c++ 中使用 double long 处理精度的问题，但是可以使用gcb(最大公约数) 更加合理。123456789101112131415161718192021222324252627282930313233343536class Solution &#123;public: // 首先这个是一个 $n^2$ 的算法，因为需要计算每一个点到其他的所有点的斜率（如果可以的话） int maxPoints(vector&lt;vector&lt;int&gt;&gt;&amp; points) &#123; int n =points.size(); int res =0; // 以这个点为基准点 进行发射 for(int i =0; i&lt; n; i++) &#123; unordered_map&lt;double long, int&gt; hash; int veticles =1, duplicates =0; for(int j =i +1; j&lt;n; j++) &#123; if(points[i][0] ==points[j][0]) &#123; veticles +=1; if(points[i][1] ==points[j][1]) duplicates +=1; &#125; &#125; for(int j =i +1; j&lt;n; j++) &#123; if(points[i][0] != points[j][0]) &#123; double long slope = (double long)(points[i][1] -points[j][1])/(points[i][0] -points[j][0]); if(hash[slope] ==0) hash[slope] =2; else hash[slope ] ++; res =max(res, hash[slope] + duplicates); &#125; &#125; res =max(res, veticles); &#125; return res; &#125;&#125;; 使用python 实现的 123456789101112131415161718192021222324252627282930313233343536class Solution(object): # 在python 中 使用 gcb 处理精度问题 def maxPoints(self, points): """ :type points: List[List[int]] :rtype: int """ res =0 n =len(points) for i in range(n): veticles =1 duplicates =0 dic =&#123;&#125; for j in range(i+1, n): # 对应的是不能计算 slope 斜率的情况 if(points[i][0] ==points[j][0]): veticles +=1; if(points[i][1] ==points[j][1]): duplicates +=1 for j in range(i +1, n): if points[i][0] != points[j][0]: dx =points[i][0] -points[j][0] dy =points[i][1] -points[j][1] common =self.gcb(dx, dy) if (dx//common, dy//common) in dic: dic[(dx//common, dy //common)] +=1 else: dic[(dx//common, dy//common)] =2 res =max(res, dic[(dx//common, dy//common)] +duplicates) res =max(res, veticles) return res; def gcb(self, a, b): if(b ==0): return a return self.gcb(b, a%b) 279. Perfect Squares 时间复杂度 $O(n\sqrt(n))$，复杂度的计算是有个技巧，当 $i =n$ 时候，那么 $j$ 的遍历次数最多是 $\sqrt(n)$ 所以两者相乘就可以了。 1234567891011121314151617class Solution &#123;public: // 动态规划求解 f[i] 表示n 所需要的最少的数量 // f[i] =min(f[i -j*j]) 其中 1&lt;= j&lt;= sqrt(n) // 最后的f[n] 表示最后的解 int numSquares(int n) &#123; // 按照道理求解最小值，可以初始化为 INT_MAX 这样的数字 vector&lt;int&gt; f(n+1, n); f[0] =0; for(int i =1; i&lt;=n; i++) &#123; for(int j =1; j*j &lt;=i; j++) f[i] =min(f[i], f[i-j*j] +1); &#125; return f[n]; &#125;&#125;; python 中使用这样的方式进行实现。关键是 sqrt(i) 是可以使用 i**0.5 进行实现的。 123456789101112class Solution(object): def numSquares(self, n): """ :type n: int :rtype: int """ nums =[n] *(n+1) nums[0] =0 for i in range(1, n+1): for j in range(1, int(i**0.5)+1): nums[i] =min(nums[i], nums[i -j*j] +1) return nums[n] python 有两种写法，一种是for 小暖使用 **0.5 表示 $sqrt(x) $ 的计算，一种是使用 while 循环方式。 123456789101112131415class Solution(object): # 能够想到的就是枚举， 从 1 到 sqrt(n) ，然后最后的时间复杂度是 nsqrt(n) def numSquares(self, n): """ :type n: int :rtype: int """ f=[n]*(n+1) f[0] =0 for i in range(1, n+1): j =1 while j*j &lt;=i: f[i] =min(f[i], f[i -j*j] +1) j +=1 return f[n] LeetCode 647. Palindromic Substrings 每次固定回文子串的中间位置，然后向左右开始扩展；每次固定后，分为奇数长度和偶数长度两种情况，然后暴力统计答案即可。时间复杂度 共有 $O(n)$ 个中间位置，固定后，最坏情况下需要$ O(n)$ 的时间扩展回文串，故总时间复杂度为 $O(n^2)$。 使用 while 更加能够体现条件循环的精髓. 123456789101112131415161718192021222324class Solution &#123;public: int countSubstrings(string s) &#123; int n =s.length(); int res =0; // 这个是遍历中间结点的， for(int i =0; i&lt;n; i++) &#123; int j =0; while( i-j &gt;=0 &amp;&amp; i +j &lt;n &amp;&amp; s[i-j] ==s[i+j]) &#123; res +=1; j +=1; &#125; j =1; while(i-j +1 &gt;=0 &amp;&amp; i+j &lt;n &amp;&amp; s[i-j +1] == s[i+j]) &#123; res +=1; j +=1; &#125; &#125; return res; &#125;&#125;; 使用python 实现 123456789101112131415class Solution: def countSubstrings(self, s: str) -&gt; int: n =len(s) res =0 for i in range(n): j =0 while i-j&gt;=0 and i+j &lt;n and s[i-j] ==s[i+j]: res +=1 j +=1 j =1 while i-j+1 &gt;=0 and i +j &lt;n and s[i-j +1] ==s[i+j]: res +=1 j +=1 return res; cpp 中小技巧，循环输入直到遇到回车。 12345678910111213141516171819202122232425#include&lt;stdio.h&gt;#include&lt;iostream&gt;#include&lt;string&gt;using namespace std;int main()&#123; // 时间复杂度是 O(n^2) string str; while(getline(cin, str)) &#123; //if(s[0] =="\n") break; int ans =0; int n =str.length(); for (int i = 0; i &lt; n; i++) &#123; for (int j = 0; i - j &gt;= 0 &amp;&amp; i + j &lt; n &amp;&amp; str[i - j] == str[i + j]; j++) ans++; for (int j = 1; i - j + 1 &gt;= 0 &amp;&amp; i + j &lt; n &amp;&amp; str[i - j + 1] == str[i + j]; j++) ans++; &#125; cout &lt;&lt; ans &lt;&lt; endl; // 如果不是回车，那么就一直循环，如果是回车，那么就跳出 if(getchar() =='\n') break; &#125; return 0;&#125; 394. Decode String 使用python 实现， 时间复杂度是$O(n)$ 空间复杂度是 $O(n)$；在python 中常常使用的 isdigit(), isalpha()。也是可以使用c++ 进行实现，但是因为c++ 实现的时间复杂度是$k^n$， 所以不写了， 可以参考这里 这里假设嵌套的层数是 $n$， 数字数值大小是 $k$。 12345678910111213141516171819202122class Solution(object): def decodeString(self, s): """ :type s: str :rtype: str """ stack =[['', 1,'']] a =n ='' for ch in s: if ch.isdigit(): n +=ch elif ch.isalpha(): a +=ch elif ch =='[': # 把之前的东西放到里面 stack.append([a, int(n),'']) a =n ='' else: a1, a2, a3 =stack.pop() stack[-1][-1] += a1+ a2*(a3+a) # 如果在后面就是在后面，如果再前面就是在前面 a ='' return stack[-1][-1]+a 一般这种题目都是可以基于 stack进行解答的。这道题目推荐使用 python 作答， 容易出错的点是 break 函数、1234567891011121314151617181920212223242526272829class Solution(object): def decodeString(self, s): """ :type s: str :rtype: str """ # 这个算是比较简单的方式 stack =[] for ch in s: # 类似一种结束符号 if ch!=']': stack.append(ch) else: # do so # 判断是否是 [ 或者 ] 这两个标志符号，因为有两个不同的操作 str_ ='' # 在pop() 执勤啊确实应该是检查一下的 while stack: x =stack.pop() if x =='[': # 处理的是数字 num ="" while stack and stack[-1].isdigit(): num =stack.pop() + num stack.append(str_* int(num)) break else: str_ = x + str_ return "".join(stack) String Compression 和上面一道题目相反的是，压缩字符串，关键是双指针的算法，不难。12345678910111213141516171819202122class Solution(object): def compress(self, chars): """ :type chars: List[str] :rtype: int """ n =len(chars) res ="" i =0 # 如果这里使用 range() 那么是无视下面对于i 的修改的，所以 while i&lt;n: #for i in range(n): j =i while j&lt; n and chars[j] == chars[i] : j +=1 # 这个是常见的字符串计数的方式 if j-i &gt;1: res += chars[i] +str(j -i) else: # 如果只是有一个的话，是不需要计数的 res += chars[i] i =j chars[:] =list(res) # 这个是作弊的操作，因为本文要求的是 in-place 的操作，所以按照道理说是不应该申请新的内存的 return len(chars) 48. Rotate Image 顺时针选择数组，可以分成两个步骤。 按照 y =x 轴对称旋转数组 按照 y的中点 的轴对称 旋转数组 时间复杂度是 $O(n^2)$ 12345678910111213141516171819class Solution &#123;public: // 关键是思路的问题， rotate image ，首先是按照 y =x 进行对称； 然后是按照 竖坐标轴中间进行对称 void rotate(vector&lt;vector&lt;int&gt;&gt;&amp; matrix) &#123; if(matrix.empty()) return; int n =matrix.size(); for (int i =0; i&lt;n; i++) &#123; for(int j =i+1; j&lt;n; j ++) &#123; swap(matrix[i][j] ,matrix[j][i]); &#125; // 第二个转换 int m =matrix[0].size(); for(int j=0, k =m -1; j&lt;k ; j ++, k--) swap(matrix[i][j], matrix[i][k]); &#125; &#125;&#125;; python 实现123456789101112131415class Solution: def rotate(self, matrix: List[List[int]]) -&gt; None: """ Do not return anything, modify matrix in-place instead. """ n, m =len(matrix), len(matrix[0]) for i in range(n): # 这个在python中实现是比较nice的 for j in range(i+1, n): matrix[i][j], matrix[j][i] =matrix[j][i], matrix[i][j] j, k =0, m-1 while(j &lt;k): matrix[i][k], matrix[i][j] =matrix[i][j], matrix[i][k] j +=1 k -=1 扩展， 如果是逆时针旋转数组，那么这个时候，可以先按照 y =-x 进行旋转，然后按照 y的中点旋转数组。 54. Spiral Matrix 这个比较简单，是四个for 循环遍历解决的问题，边界问题是需要注意的。还没有找到除此之外其他的方法。下面的代码是错误的，边界问题太难处理了。123456789101112131415161718192021222324252627282930313233343536class Solution &#123;public: vector&lt;int&gt; spiralOrder(vector&lt;vector&lt;int&gt;&gt;&amp; matrix) &#123; vector&lt;int&gt; res; if(matrix.empty()) return res; int n= matrix.size(), m = matrix[0].size(); int top=0, down=n-1, l=0, r= m-1; while(top &lt;= down &amp;&amp; l &lt;= r) &#123; for(int i =l ; i&lt;=r; i++) res.push_back(matrix[top][i]); for(int i =top+1; i&lt;=down; i++) res.push_back(matrix[i][r]); for(int i =r-1; i&gt;=l ; i--) res.push_back(matrix[down][i]); for(int i =down-1; i&gt;top; i--) res.push_back(matrix[i][l]); top +=1, down -=1, l +=1, r -=1; &#125; // 处理 or 的情况 if(top &lt; down) for(int i =top ; i&lt; down; i++) &#123; cout &lt;&lt; matrix[top][i]&lt;&lt;" "; res.push_back(matrix[top][i]); &#125; if(l&lt; r) for(int i =l; i&lt;r; i++) &#123; res.push_back(matrix[top][i]); cout &lt;&lt; matrix[top][i]&lt;&lt;" "; &#125; //if(top ==down and l ==r) res.push_back(matrix[top][l]); return res; &#125;&#125;; 使用类似的版本的代码， 边界问题通过多个 break语句进行处理， exactly 的方式进行处理。123456789101112131415161718192021222324252627282930class Solution &#123;public: // 边界问题如何处理 vector&lt;int&gt; spiralOrder(vector&lt;vector&lt;int&gt;&gt;&amp; matrix) &#123; vector&lt;int&gt; res; int n = matrix.size(); if(!n) return res; int m =matrix[0].size(); int l=0, r=m-1, u=0, d=n-1; while (true)&#123; if(l&lt;=r) for(int i =l ; i&lt;=r; i++) res.push_back(matrix[u][i]); else break; u +=1; if(u &lt;= d) for(int i =u ; i&lt;=d; i++) res.push_back(matrix[i][r]); else break; r -=1; if(l &lt;=r) for(int i=r; i&gt;=l; i--) res.push_back(matrix[d][i]); else break; d -=1; if(u&lt;=d) for(int i=d; i&gt;=u; i--) res.push_back(matrix[i][l]); else break; l +=1; &#125; return res; &#125;&#125;; https://leetcode.com/problems/remove-duplicates-from-sorted-list-ii/ 1234567891011121314151617181920212223242526/** * Definition for singly-linked list. * struct ListNode &#123; * int val; * ListNode *next; * ListNode(int x) : val(x), next(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: // 这个是常常考察的笔试题 ListNode* deleteDuplicates(ListNode* head) &#123; ListNode * dummy = new ListNode(-1); dummy -&gt;next =head; ListNode * p =dummy; while(p -&gt;next) &#123; auto q =p-&gt;next; while(q &amp;&amp; q-&gt;val ==p-&gt;next-&gt;val) q = q-&gt;next; // if 中表示的没有重复的元素， else中表示有重复的元素 if(p-&gt;next -&gt;next ==q) p =p-&gt;next; else p-&gt;next =q; &#125; return dummy-&gt;next; &#125;&#125;; python 实现。 注意在新建对象中 python 是直接 ListNode 但是 c++ 中需要new 关键字。然后指针的使用 c++ 中 -&gt; 然后python 中使用 . 关键字. 123456789101112131415161718192021222324# Definition for singly-linked list.# class ListNode(object):# def __init__(self, x):# self.val = x# self.next = Noneclass Solution(object): def deleteDuplicates(self, head): """ :type head: ListNode :rtype: ListNode """ # 有一些细节需要注意 dummy = ListNode(-1) dummy.next =head p = dummy while( p.next): q =p.next while(q and p.next.val == q.val): q =q.next if(p.next.next ==q): p=p.next; else: p.next =q return dummy.next Remove Duplicates from Sorted List 其中的 if 表示并没有舍弃当前的结点，每次都是前进一步。和上面那道题中，使用while 找到最后的结点不同。一个个进行操作，因为其中使用的 if 语句。上面的全部去掉使用的是 while 语句。 python 版本 1234567891011121314151617181920# Definition for singly-linked list.# class ListNode(object):# def __init__(self, x):# self.val = x# self.next = None# 这道题目相对上一道题目就比较简单，class Solution(object): def deleteDuplicates(self, head): """ :type head: ListNode :rtype: ListNode """ if not head: return head; p =head while p.next: if(p.val == p.next.val): p.next =p.next.next else: p =p.next return head c++ 版本12345678910111213141516171819202122/** * Definition for singly-linked list. * struct ListNode &#123; * int val; * ListNode *next; * ListNode(int x) : val(x), next(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: ListNode* deleteDuplicates(ListNode* head) &#123; if(! head) return head; ListNode* p =head; while(p-&gt;next) &#123; // 这句话并没有舍弃当前的结点，每次都是移动一步， if(p-&gt;next-&gt;val == p-&gt;val) p-&gt;next= p-&gt;next-&gt;next; else p =p-&gt;next; &#125; return head; &#125;&#125;;]]></content>
      <categories>
        <category>算法</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Activation Function and Loss Function]]></title>
    <url>%2F2018%2F07%2F07%2FLoss-Activation-and-Optimisation-Function%2F</url>
    <content type="text"><![CDATA[介绍激活函数（activation function）、损失函数（loss function）和在pytorch框架下的使用。 Activation FunctionWhat? It’s just a thing (node) that you add to the output end of any neural network. It is also known as Transfer Function. It can also be attached in between two Neural Networks. $$Output = activation function \left( x _ { 1 } w _ { 1 } + x _ { 2 } w _ { 2 } + \cdots + x _ { n } w _ { n } + b i a s \right)$$ A weighted sum is computed as:$$x _ { 1 } w _ { 1 } + x _ { 2 } w _ { 2 } + \cdots + x _ { n } w _ { n }$$Then, the computed value is fed into the activation function, which then prepares an output.$$activation function \left( x _ { 1 } w _ { 1 } + x _ { 2 } w _ { 2 } + \cdots + x _ { n } w _ { n } + b i a s \right)$$ Think of the activation function as a mathematical operation that normalises the input and produces an output. The output is then passed forward onto the neurons on the subsequent layer. 激活函数的本质是增加网络的非线性。 The thresholds are pre-defined numerical values in the function. This very nature of the activation functions can add non-linearity to the output. Activation Function Types Linear Activation Function: $$ output = k * x$$where $k$ is a scalar value, as an instance 2, and $x$ is the input. Sigmoid or Logistic Activation Function The sigmoid activation function is “S” shaped. It can add non-linearity to the output and returns a binary value of 0 or 1. $$Output = \frac { 1 } { 1 + e ^ { - x } }$$ 这个函数有一个很好的导数形式，在反向传播的时候，效果比较明显。 1import torch.nn.functional as F # pytorch 大多数激活函数都是这里 Tanh Activation Function Tanh is an extension of the sigmoid activation function. Hence Tanh can be used to add non-linearity to the output. The output is within the range of -1 to 1. Tanh function shifts the result of the sigmoid activation function: $$\text { Output } = \frac { 2 } { 1 + e ^ { - 2 x } } - 1$$ Rectified Linear Unit Activation Function (RELU) RELU is one of the most used activation functions. It is preferred to use RELU in the hidden layer. The concept is very straight forward. It also adds non-linearity to the output. However the result can range from 0 to infinity. $$ Output = \max ( 0 , x )$$这个是很高的评价了。If you are unsure of which activation function you want to use then use RELU. 在pytorch12from torch import nnrelu = nn.Relu() 但是问题在于负值容易引起神经死亡，每次遇到负值部分，激活函数都是为0. Leaky Relu 为了处理负值问题，Relu 有了变种，表达式为$$ Output = \max ( 0.01 *x , x )$$ 解决了当 $x&lt;0$时候，网络死亡的问题。 12from torch import nnrelu = nn.leaky_relu() Pre Relu 为了处理负值问题，Relu 有了变种，表达式为$$ Output = \max ( a *x , x )$$ 解决了当 $x&lt;0$时候，网络死亡的问题。其中的超参数$a$ 是可以调整的。 12from torch import nnrelu = nn.PRelu() Elu Exponential Line Unit $$\text{ELU}(x) = \max(0,x) + \min(0, \alpha * (\exp(x) - 1))`.$$继承了Relu的所有优点，but贵一点，均值为0的输出、而且处处一阶可导。 Softmax Activation Function Softmax is an extension of the Sigmoid activation function. Softmax function adds non-linearity to the output, however it is mainly used for classification examples where multiple classes of results can be computed. $$Output = \frac { e ^ { x } } { \operatorname { sum } \left( e ^ { x } \right) }$$ 这个一般使用在最后，作为多分类的结束。 Loss Function(Error Function)机器学习中所有的算法都需要最大化或最小化一个函数，这个函数被称为“目标函数”。其中，我们一般把最小化的一类函数，称为“损失函数”。它能根据预测结果，衡量出模型预测能力的好坏。 损失函数 (Loss function) 是用来衡量模型的预测值 $f(x)$ 和真实值 $Y$ 的不一样的程度，通常使用 $L (Y, f(x))$ 来进行表示，损失函数越小，模型的鲁棒性越强。 选择loss 的时候需要考虑两点：分类or 回归问题 和结果的输出情况。 the choice of loss function must match the framing of the specific predictive modeling problem, such as classification or regression. Further, the configuration of the output layer must also be appropriate for the chosen loss function. 总的来说是可以分成三类：回归模型，二分类模型和多分类模型 Regression Loss Functionsa. Mean Squared Error Lossb. Mean Squared Logarithmic Error Lossc. Mean Absolute Error Loss Binary Classification Loss Functionsa. Binary Cross-Entropyb. Hinge Lossc. Squared Hinge Loss Multi-Class Classification Loss Functionsa. Multi-Class Cross-Entropy Lossb. Sparse Multiclass Cross-Entropy Lossc. Kullback Leibler Divergence Loss Regression Loss Function (回归) 如何进行选择？ 对于回归问题，一个baseline 的loss function 是可以选择平方损失函数。从数据的角度进行分析，如果数据服从正太分布，那么平方损失函数没有问题，如果数据有一些 outlier，可以使用 mean squared logarithmic error loss， 先进行 $\hat { y } $然后再计算平方和。如果 outlier 比较多的话，那么使用 mean absolute error loss，计算差值的时候换成绝对值函数。 平方损失函数 定义： Mean Squared Error (MSE), or quadratic, loss function is widely used in linear regression as the performance measure, and the method of minimizing MSE is called Ordinary Least Squares (OSL)。 To calculate MSE, you take the difference between your predictions and the ground truth, square it, and average it out across the whole dataset. $$Loss = \frac { 1 } { n } \sum _ { i = 1 } ^ { n } \left( y ^ { ( i ) } - \hat { y } ^ { ( i ) } \right) ^ { 2 }$$ 在线性回归中，它假设样本和噪声都服从高斯分布（为什么假设成高斯分布呢？其实这里隐藏了一个小知识点，就是中心极限定理），最后通过极大似然估计MLE可以推导出最小二乘式子，即平方损失函数可以通过线性回归在假设样本是高斯分布的条件下推导得到。 $$S E = \sum _ { i = 1 } ^ { n } \left( y _ { i } - y _ { i } ^ { p } \right) ^ { 2 }$$ 为什么选择欧式距离作为误差的度量？ 简单，计算方便 欧式距离是一种很好的相似度衡量标准 在不同的表示域变换之后，特征的性质能够保持不变。 在实际应用中，通常会使用 均方差作为一种衡量指标，就是在上面的公式中除以 N. 使用说明： 如果 target 是服从高斯分布，那么使用 mean squared error 是没有问题；并且没有很好的理由进行替换的话，那么就是他了。 Mathematically, it is the preferred loss function under the inference framework of maximum likelihood if the distribution of the target variable is Gaussian. It is the loss function to be evaluated first and only changed if you have a good reason. Mean Squared Logarithmic Error Loss 和上面的的mse 有一点差别。这个是先记性log 求结果，然后再计算 mse. you can first calculate the natural logarithm of each of the predicted values, then calculate the mean squared error. This is called the Mean Squared Logarithmic Error loss, or MSLE for short. 好处:It has the effect of relaxing the punishing effect of large differences in large predicted values. 使用说明：如果最后的结果的数值有大值，那么可以尝试一下。不是那么符合高斯分布，就可以尝试一下。 均方差误差数学表达为：\begin{equation}\frac{1}{n} \sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)^{2}\end{equation}或者：\begin{equation} \frac{1}{n-1} \sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)^{2}\end{equation} 其中 $\bar{X} = \frac{X_1 + \dots + X_n}{n}$。 在pytorch 中使用nn.MSELoss 实现。首先定义了二维数组用于计算： 1234567891011import torchfrom torch.autograd import Variableimport torch.nn as nnimport torch.nn.functional as Fsample = Variable(torch.ones(2,2))a=torch.Tensor(2,2)a[0,0]=0a[0,1]=1a[1,0]=2a[1,1]=3target = Variable (a) 123criterion = nn.MSELoss()loss = criterion(sample, target)print(loss) Mean Absolute Error Loss 定义：Mean Absolute Error (MAE) is a quantity used to measure how close forecasts or predictions are to the eventual outcomes, which is computed by $$Loss = \frac { 1 } { n } \sum _ { i = 1 } ^ { n } \left| y ^ { ( i ) } - \hat { y } ^ { ( i ) } \right|$$ where $| .|$ denotes the absolute value. Albeit, both MSE and MAE are used in predictive modeling, there are several differences between them. MSE has nice mathematical properties which makes it easier to compute the gradient. However, MAE requires more complicated tools such as linear programming to compute the gradient. Because of the square, large errors have relatively greater influence on MSE than do the smaller error. Therefore, MAE is more robust to outliers since it does not make use of square. On the other hand, MSE is more useful if concerning about large errors whose consequences are much bigger than equivalent smaller ones. MSE also corresponds to maximizing the likelihood of Gaussian random variables. 使用说明： 当有很多的点偏离 mean and variance 的时候，可以尝试使用 mae loss function，这个显著的特点在于 对outlier 是有抵抗作用的。 The Mean Absolute Error, or MAE, loss is an appropriate loss function in this case as it is more robust to outliers. It is calculated as the average of the absolute difference between the actual and predicted values. 在pytorch 中使用 nn.L1Loss 使用（因为绝对值损失又叫做L1 损失）123criterion = nn.L1Loss()loss = criterion(sample, target)print(loss) 基于绝对值损失改进的是SmoothL1Loss （也叫做 Huber Loss）， 其在(-1,1) 上是平方损失，其他情况是 L1 损失。123criterion = nn.SmoothL1Loss()loss = criterion(sample, target)print(loss) Binary Classification Loss Function (二分)Binary Cross-Entropy Loss Cross-entropy loss is often simply referred to as “cross-entropy,” “logarithmic loss,” “logistic loss,” or “log loss” for short. Cross-entropy is the default loss function to use for binary classification problems. 如果没有更好的二分类的选择（理由），那么这个就是首选。 数学定义： Cross-entropy will calculate a score that summarizes the average difference between the actual and predicted probability distributions for predicting class 1. The score is minimized and a perfect cross-entropy value is 0. 使用说明： Mathematically, it is the preferred loss function under the inference framework of maximum likelihood. It is the loss function to be evaluated first and only changed if you have a good reason. It is intended for use with binary classification where the target values are in the set {0, 1}. 在pytorch 中 nn.CrossEntropyLoss()是nn.logSoftmax()和 nn.NLLLoss()的整合。损失函数计算如下： \begin{equation}\operatorname{loss}(x, \text { class })=-\log \left(\frac{\exp (x[\text { class }])}{\sum_{j} \exp (x[j])}\right)=-x[\text { class }]+\log \left(\sum_{j} \exp (x[j])\right)\end{equation} 其中 $C$ 表示类别数。损失函数中也有权重weight参数设置，针对样本不均衡的情况下，是有优化效果的。 \begin{equation}\operatorname{loss}(x, \text { class })= \text{weight}[\text{class}] (-x[\text { class }]+\log \left(\sum_{j} \exp (x[j])\right))\end{equation} 需要注意的是，target输入必须是 tensor long 类型（int64位) 12345678mport torch import torch.nn as nnloss = nn.CrossEntropyLoss()# input, NxC=2x3input = torch.randn(2, 3, requires_grad=True)# target, Ntarget = torch.empty(2, dtype=torch.long).random_(3)output = loss(input, target) 其中这里的 input 在实际的模型中是 $y_{pred}$， 而 target 是是标签类比的index。上面的代码等价于12345m = nn.LogSoftmax()loss = nn.NLLLoss()input=m(input)output = loss(input, target)print('output:',output) 在pytorch 中使用该损失函数，前面是不需要加 softmax 层。 负对数似然损失函数（Negative Log Likelihood） 同样适用于多类分类器，使用 nn.NLLLoss 实现，如果最后一层使用了log softmax 处理，那么就直接使用 nn.NLLLoss。经过了logSoftmax 得到的结果和 12345678m = nn.LogSoftmax(dim=1)loss = nn.NLLLoss()# input is of size N x C = 3 x 5input = torch.randn(3, 5, requires_grad=True)# each element in target has to have 0 &lt;= value &lt; Ctarget = torch.tensor([1, 0, 4])output = loss(m(input), target)output.backward() 和下面是等价的1234loss_fn = torch.nn.CrossEntropyLoss(reduce=False, size_average=False)input=torch.autograd.Variable(torch.randn(3,4))target=torch.autograd.Variable(torch.LongTensor(3).random_(4))loss = loss_fn(input, target) Hinge Loss An alternative to cross-entropy for binary classification problems is the hinge loss function, primarily developed for use with Support Vector Machine (SVM) models. Hinge Loss，又称合页损失，其表达式如下： $$Loss = \max ( 0,1 - y s )$$其中 y 和 s 的取值范围都是 [-1, 1]. 想想 SVM 中最大化间隔的内容就理解了。图像如下： 如同合起来的书，所以称之为 合页损失。显然，只有当 ys &lt; 1 时，Loss 才大于零；对于 ys &gt; 1 的情况，Loss 始终为零。Hinge Loss 一般多用于支持向量机（SVM）中，体现了 SVM 距离最大化的思想。 而且，当 Loss 大于零时，是线性函数，便于梯度下降算法求导。Hinge Loss 的另一个优点是使得 ys &gt; 0 的样本损失皆为 0，由此带来了稀疏解，使得 SVM 仅通过少量的支持向量就能确定最终超平面。 使用说明：要求 target 转换成 {-1, 1} ，效果有时候比 cross-binary 要好。 It is intended for use with binary classification where the target values are in the set {-1, 1}.Reports of performance with the hinge loss are mixed, sometimes resulting in better performance than cross-entropy on binary classification problems. ( 这句话比较迷离呀) The hinge loss function encourages examples to have the correct sign, assigning more error when there is a difference in the sign between the actual and predicted class values. Squared Hinge Loss (通常上讲，最后的结果更加光滑是没有什么劣势的)A popular extension is called the squared hinge loss that simply calculates the square of the score hinge loss. It has the effect of smoothing the surface of the error function and making it numerically easier to work with. 需要从 {0, 1} -&gt; {-1, 1} 这样target 的转换, 非常容易实现。 123# change y from &#123;0,1&#125; to &#123;-1,1&#125;y[where(y == 0)] = -1` 使用说明： （很强的相关性了）If using a hinge loss does result in better performance on a given binary classification problem, is likely that a squared hinge loss may be appropriate. Multi-Class Classification Loss Functions对于多类问题的定义： The problem is often framed as predicting an integer value, where each class is assigned a unique integer value from 0 to (num_classes – 1). The problem is often implemented as predicting the probability of the example belonging to each known class. Multi-Class Cross-Entropy Loss Cross-entropy is the default loss function to use for multi-class classification problems. （可见 交叉熵对于分类问题的重要性） 同理，如果是最大似然，概率模型，donot hesitate.( 数学基础就是在这里)Mathematically, it is the preferred loss function under the inference framework of maximum likelihood. It is the loss function to be evaluated first and only changed if you have a good reason. Cross-entropy will calculate a score that summarizes the average difference between the actual and predicted probability distributions for all classes in the problem. The score is minimized and a perfect cross-entropy value is 0. 基于 keras实现的时候需要先把 target (label) 转成 one-hot 类型的，当然这个可能造成 loss 曲线的波动（后话）。12# one hot encode output variabley = to_categorical(y) Sparse Multiclass Cross-Entropy Loss 和上面的区别主要在于 label 是不需要转成 one-hot 类型的，保持这原来的 number 形式。Sparse cross-entropy addresses this by performing the same cross-entropy calculation of error, without requiring that the target variable be one hot encoded prior to training. Kullback Leibler Divergence Loss Kullback Leibler Divergence, or KL Divergence for short, is a measure of how one probability distribution differs from a baseline distribution. 数学原理：(以 bit 为单位的 信息熵) A KL divergence loss of 0 suggests the distributions are identical. In practice, the behavior of KL Divergence is very similar to cross-entropy. It calculates how much information is lost (in terms of bits) if the predicted probability distribution is used to approximate the desired target probability distribution. 使用说明：更常使用 在复杂的模型上，比如 dense representation 之列。当然也是可以使用在多分类的情况下，这个时候如同 multi-class cross-entropy. As such, the KL divergence loss function is more commonly used when using models that learn to approximate a more complex function than simply multi-class classification, such as in the case of an autoencoder used for learning a dense feature representation under a model that must reconstruct the original input. In this case, KL divergence loss would be preferred. Nevertheless, it can be used for multi-class classification, in which case it is functionally equivalent to multi-class cross-entropy. 参考文献： How to Choose Loss Functions When Training Deep Learning Neural Networks log 对数损失函数 在逻辑回归的推导中，它假设样本服从伯努利分布（0-1分布），然后求得满足该分布的似然函数，接着取对数求极值等等。 指数损失函数 公式如下：$$loss = e ^ { - y s }$$ 曲线如下： Exponential Loss 与交叉熵 Loss 类似，但它是指数下降的，因此梯度较其它 Loss 来说，更大一些。 Exponential Loss 一般多用于AdaBoost 中。因为使用 Exponential Loss 能比较方便地利用加法模型推导出 AdaBoost算法。 $$L = - \log \frac { e ^ { s } } { \sum _ { j = 1 } ^ { C } e ^ { s _ { j } } } = - s + \log \sum _ { j = 1 } ^ { C } e ^ { s _ { j } }$$ softmax loss 的曲线如下图所示： 上图中，当 s &lt;&lt; 0 时，Softmax 近似线性；当 s&gt;&gt;0 时，Softmax 趋向于零。Softmax 同样受异常点的干扰较小，多用于神经网络多分类问题中。 若我们把 ys 的坐标范围取得更大一些，上面 5 种 Loss 的差别会更大一些，如图： 显然，这时候 Exponential Loss 会远远大于其它 Loss。从训练的角度来看，模型会更加偏向于惩罚较大的点，赋予其更大的权重。如果样本中存在离群点，Exponential Loss 会给离群点赋予更高的权重，但却可能是以牺牲其他正常数据点的预测效果为代价，可能会降低模型的整体性能，使得模型不够健壮（robust）。 相比 Exponential Loss，其它四个 Loss，包括 Softmax Loss，都对离群点有较好的“容忍性”，受异常点的干扰较小，模型较为健壮。 Softmax loss 对于多分类问题，可以使用 softmax loss。 其中，C 为类别个数，小写字母 s 是正确类别对应的 Softmax 输入，大写字母 S 是正确类别对应的 Softmax 输出。 由于 log 运算符不会影响函数的单调性，我们对 S 进行 log 操作。另外，我们希望 log(S) 越大越好，即正确类别对应的相对概率越大越好，那么就可以对 log(S) 前面加个负号，来表示损失函数： 如何选择损失函数？ 对于异常点的处理是一个维度，比如L1 损失函数处理异常点更加稳定，相对L2 损失函数。 what? 衡量模型好坏的 function，如果模型表现好，那么loss 应该是小；如果模型表现不好，那么loss 应该是大的。 At its core, a loss function is incredibly simple: it’s a method of evaluating how well your algorithm models your dataset. If your predictions are totally off, your loss function will output a higher number. If they’re pretty good, it’ll output a lower number. As you change pieces of your algorithm to try and improve your model, your loss function will tell you if you’re getting anywhere. Log Loss (Cross Entropy Loss) Cross-entropy loss, or log loss, measures the performance of a classification model whose output is a probability value between 0 and 1. Cross-entropy loss increases as the predicted probability diverges from the actual label. So predicting a probability of .012 when the actual observation label is 1 would be bad and result in a high loss value. A perfect model would have a log loss of 0. The graph above shows the range of possible loss values given a true observation (isDog = 1). As the predicted probability approaches 1, log loss slowly decreases. As the predicted probability decreases, however, the log loss increases rapidly. Log loss penalizes both types of errors, but especially those predictions that are confident and wrong! Cross-entropy and log loss are slightly different depending on context, but in machine learning when calculating error rates between 0 and 1 they resolve to the same thing. In binary classification, where the number of classes M equals 2, cross-entropy can be calculated as: $$- ( y \log ( y ) + ( 1 - y ) \log ( 1 - y ) )$$ If $ M&gt;$2 (i.e. multiclass classification), we calculate a separate loss for each class label per observation and sum the result. $$- \sum _ { c = 1 } ^ { M } y _ { o , c } \log \left( y _ { o , c } \right)$$ M - number of classes (dog, cat, fish)log - the natural logy - binary indicator (0 or 1) if class label c is the correct classification for observation o 想要表达的是 log loss 是从 Likelihood Loss，改进过来的，有没有发现最大似然的痕迹。log loss 表达式如下：$$\begin{split}P(Y | X) &amp;= P(X_1 | Y) \times P(X_2 | Y) \times \dots \times P(X_n | Y) \times P(Y) = P(Y) \prod_{i}^{n} P(X_i | Y) \\&amp;\Rightarrow log(P(Y | X)) = log(\prod_{i}^{n} P(X_i | Y) \Rightarrow \sum_{i}^{n} log(P(X_i | Y))\end{split}$$交叉熵表达式：$$CE(\hat{y}, y) = - \sum_{i=1}^{n} y_i log(\hat{y}) + (1 - y_i) log(1 - \hat{y})$$ L2 这两个loss function 在这里介绍过，所以本博客中简单说一下。 L2 loss function is the square of the L2 norm of the difference between actual value and predicted value. It is mathematically similar to MSE, only do not have division byn, it is computed by $$ Loss = \sum _ { i = 1 } ^ { n } \left( y ^ { ( i ) } - \hat { y } ^ { ( i ) } \right) ^ { 2 }$$ Kullback Leibler (KL) Divergence（计算的是两个分布的问题）KL Divergence, also known as relative entropy, information divergence/gain, is a measure of how one probability distribution diverges from a second expected probability distribution. KL divergence loss function is computed by$$D _ { K L } ( p | q ) = \sum _ { x } p ( x ) \log \frac { p ( x ) } { q ( x ) }$$ 交叉熵的定义：$$H ( p , q ) = - \sum _ { x } p ( x ) \log q ( x )$$两者的关系推导，$$\begin{split}D _ { K L } ( p | q ) &amp;= \sum _ { x } p ( x ) \log \frac { p ( x ) } { q ( x ) } \\&amp;= \sum _ { x } ( p ( x ) \log p ( x ) - p ( x ) \log q ( x ) ) \\&amp;= - H ( p ) - \sum _ { x } p ( x ) \log q ( x ) \\&amp;= - H ( p ) + H ( p , q )\end{split}$$所以说， cross entropy 也是可以写成这样：$$H ( p , q ) = D _ { K L } ( p | q ) + H ( p )$$ logistic loss 和 cross entropy的关系 当 $ p \in { y , 1 - y }$, $q \in { \hat { y } , 1 - \hat { y } }$ ，cross entropy 可以写成 logistic loss: $$H ( p , q ) = - \sum _ { x } p ( x ) \log q ( x ) = - y \log \hat { y } - ( 1 - y ) \log ( 1 - \hat { y } )$$ 交叉熵函数是怎么来的？从上面可以清楚的了解到，交叉熵函数在分类问题上是 default 的选择，那么我们有没有思考过 这个loss function 的数学基础 ？是怎么来的呢？ 真实的样本标签为 [0, 1], 分别表示负类和正类，模型最终会经过一个 sigmoid 函数，输出一个概率值。sigmoid 函数的表达式如下：$$g ( s ) = \frac { 1 } { 1 + e ^ { - s } }$$ 所以sigmoid 的输出值表示 1的概率：$$\hat { y } = P ( y = 1 | x )$$表示 0的概率：$$1 - \hat { y } = P ( y = 0 | x )$$ 进而，从最大似然的角度出发，将上面的两种情况整合起来： $$P ( y | x ) = \hat { y } ^ { y } \cdot ( 1 - \hat { y } ) ^ { 1 - y }$$ 从另一个角度也可以理解这个公式，分别令 y =0 和y =1， 发现两种情况正好对应着 $P ( y = 0 | x ) = 1 - \hat { y }$ 和 $P ( y = 1 | x ) = \hat { y }$。 我们做的就是把上面两种情况给整合起来了。 接下来引入 log 函数$$\log P ( y | x ) = \log \left( \hat { y } ^ { y } \cdot ( 1 - \hat { y } ) ^ { 1 - y } \right) = y \log \hat { y } + ( 1 - y ) \log ( 1 - \hat { y } )$$我们希望公式越大越好，反过来，希望 $\log \mathrm { P } ( \mathrm { y } | \mathrm { x } )$ 越小越好，于是得到了最终的 损失函数：$$L = - [ y \log \hat { y } + ( 1 - y ) \log ( 1 - \hat { y } ) ]$$ 上面是 单个样本的损失函数，计算N 个样本，只需要将上面的公式叠加起来。$$L = - \sum _ { i = 1 } ^ { N } y ^ { ( i ) } \log \hat { y } ^ { ( i ) } + \left( 1 - y ^ { ( i ) } \right) \log \left( 1 - \hat { y } ^ { ( i ) } \right)$$ ok，这个就是交叉熵损失函数完整的推导过程。 自定义损失函数（1） 继承nn.Module 定义：123456class My_loss(nn.Module): def __init__(self): super().__init__() def forward(self, x, y): return torch.mean(torch.pow((x - y), 2)) 使用:12criterion = My_loss()loss = criterion(outputs, targets) （2）自定义函数 注意要所有的数学操作要使用 tensor完成。不需要维护参数，梯度等信息。12def my_mse_loss(x, y): return torch.mean(torch.pow((x - y), 2)) 二分类、多分类和多标签问题的区别二分类、多分类与多标签分类问题使用不同的激活函数和损失函数。 基本概念二分类：判别这个水果是苹果还是香蕉。多分类：对于一堆水果，辨别是苹果、梨还是橘子。一个样本只能有一个标签。多标签分类： 给每一个样本一系列的目标标签。比如一个文档有不同的相关话题，需要加上不同的tag 如宗教、政治和教育。 多分类问题常常是可以转换成二分类问题进行处理的，常见有两种策略。 一对一的策略给定数据集D这里有N个类别，这种情况下就是将这些类别两两配对，从而产生 $\frac{N*(N-1)}{2}$个二分类任务，在测试的时候把样本交给这些分类器，然后进行投票。 一对其余策略将每一次的一个类作为正例，其余作为反例，总共训练N个分类器。测试的时候若仅有一个分类器预测为正的类别则对应的类别标记作为最终分类结果，若有多个分类器预测为正类，则选择置信度最大的类别作为最终分类结果。 同样，多标签分类和二分类问题也是有关系的。 面临的问题： 图片的标签数目不是固定的，有的有一个标签，有的有两个标签，但标签的种类总数是固定的，比如为5类。 解决该问题： 采用了标签补齐的方法，即缺失的标签全部使用0标记，这意味着，不再使用one-hot编码。例如：标签为：-1,1,1,-1,1 ;-1表示该类标签没有，1表示该类标签存在。 如何衡量损失？ 计算出一张图片各个标签的损失，然后取平均值。 如何计算精度 计算出一张图片各个标签的精度，然后取平均值。 该处理方法的本质：把一个多标签问题，转化为了在每个标签上的二分类问题。 损失函数的选择 基于逻辑回归的二分类问题：使用逻辑回归二分类loss function的推导，上面的一小节是有详细的介绍的。 基于 Softmax 的多分类问题 softmax层中的softmax 函数是logistic函数在多分类问题上的推广，它将一个N维的实数向量压缩成一个满足特定条件的N维实数向。压缩后的向量满足两个条件： 向量中的每个元素的大小都在[0,1] 向量所有元素的和为 1 因此，softmax适用于多分类问题中对每一个类别的概率判断，softmax的函数公式如下：$$a _ { j } ^ { L } = \frac { e ^ { z _ { j } ^ { L } } } { \sum _ { k } e ^ { z _ { k } ^ { L } } }$$ 基于 Softmax 的多分类问题采用的是 log似然代价函数（log-likelihood cost function）来解决。单个样本的 log似然代价函数的公式为：$$C = - \sum _ { i } \left( y _ { i } \log a _ { i } \right)$$其中， $y_i $表示标签向量的第 i 个分量。因为往往只有一个分量为 1 其余的分量都为 0，所以可以去掉损失函数中的求和符号，化简为， $$C = - \ln a _ { j }$$其中，$ a_j $是向量 y 中取值为 1 对应的第 j 个分量的值。 $$\begin{split}\operatorname { cost } \left( h _ { \theta } ( x ) , y \right) &amp;= - y _ { i } \log \left( h _ { \theta } ( x ) \right) - \left( 1 - y _ { i } \right) \log \left( 1 - h _ { \theta } ( x ) \right)$ \\C &amp;= - \sum _ { i } \left( y _ { i } \log a _ { i } \right)$\\\end{split}$$ 理论上都是使用多类交叉熵函数，但是在实现的时候，深度学习工具keras 是支持两种形式，针对于标签y 的形式，一种是 sparse 一种是 dense分别对应的是 one-hot 形式和 label 的形式。 因为这两个交叉熵损失函数对应不同的最后一层的输出。第一个对应的最后一层是 sigmoid，用于二分类问题，第二个对应的最后一层是 softmax，用于多分类问题。但是它们的本质是一样的，请看下面的分析。 可以看一下交叉熵函数的定义： $$-\int p ( x ) \log g ( x ) d x$$ 交叉熵是用来描述两个分布的距离的，神经网络训练的目的就是使 $g(x)$ 逼近$ p(x)$。 总结 分类问题名称 输出层使用激活函数 对应的损失函数 二分类 sigmoid 二分类交叉熵损失函数（binary_crossentropy） 多分类 softmax 多类别交叉熵损失函数（categorical_crossentropy） 多标签分类 sigmoid函数 二分类交叉熵损失函数（binary_crossentropy）]]></content>
      <categories>
        <category>Deep learning</category>
      </categories>
      <tags>
        <tag>loss function</tag>
        <tag>Activation Function</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[降维（Dimensionality Reduction）]]></title>
    <url>%2F2018%2F06%2F29%2Fdimensionality_reduction%2F</url>
    <content type="text"><![CDATA[降维分为以PCA为代表的线性降维和T-SNE为代表非线性降维。本文主要介绍这两种降维方式和代码实现，当然也会提及其他的降维算法比如LDA。 PCA Principal component analysis (PCA) is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables (entities each of which takes on various numerical values) into a set of values of linearly uncorrelated variables called principal components.PCA（principal component analysis）主成分分析是一种通过正交变换，将原来可能相关的变量转换成线性不相关的变量。 PCA的优化目标：将一组$N$维向量降为$K$维度（$0&lt; K &lt; N $），选择$K$个单位正交基，使得原始数据变换到这组正交基上，各个特征之间协方差为0，单个特征的方差尽可能大。 （1）数学概念 均值表达式：\begin{equation}\bar{X}=\frac{\sum_{i=1}^{n} X_{i}}{n}\end{equation} 标准差（SD）是衡量单个数据离散程度的指标。 \begin{equation}s=\sqrt{\frac{\sum_{i=1}^{n}(X-\bar{X})^{2}}{(n-1)}}\end{equation}在统计学中，使用采样来预估总体的性质。分母是$n-1$而不是$n$，其中有比较复杂的数学证明，简单的结论就是，使用$n-1$计算出来的样本标准差和真实整体数据的标准差更加接近。 方差（Variance）：在一维空间中数据偏离均值的程。\begin{equation}\sigma^{2}=\frac{\sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)^{2}}{(n-1)}\end{equation} 协方差（Covariance）：二维空间中，单个特征和其他的特征之间的关系。 \begin{equation}\operatorname{cov}(X, Y) = s^2 =\frac{\sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)\left(Y_{i}-\bar{Y}\right)}{(n-1)}\end{equation} 解读协方差值： Exact value is not as important as its sign. 数值的符号的意义大于具体数值意义 如果是正值，那么说明两个维度特征是同增同减 如果是负值，那么说明两个维度特征是异步，一增一减 如果是0，两个维度是线性独立 线性无关数学表达为： \begin{equation}c_{1} x_{1}+c_{2} x_{2}+\ldots+c_{k} x_{k}=0\end{equation}当且仅当 $c_1 =c_2= \dots =c_k =0$图形表达为：可以发现，PCA消除的真的只是线性的相关性。 协方差矩阵（Covariance Matrix）将协方差矩阵话。该矩阵具有对称性。其中主对角线是方差。 \begin{equation}C=\left[\begin{array}{ccc}{\operatorname{cov}(X, X)} &amp; {\operatorname{cov}(X, Y)} &amp; {\operatorname{cov}(X, Z)} \\ {\operatorname{cov}(Y, X)} &amp; {\operatorname{cov}(Y, Y)} &amp; {\operatorname{cov}(Y, Z)} \\ {\operatorname{cov}(Z, X)} &amp; {\operatorname{cov}(Z, Y)} &amp; {\operatorname{cov}(Z, Z)}\end{array}\right]\end{equation} 特征值只有方阵才有特征值和特征向量。其中特征值和特征向量是成对出现的。其次并不是所有的方阵都有特征向量。 特征向量 A vector consists of both length and direction.一个向量由长度和方向组成。 \begin{equation}\mathbf{A} \cdot \mathbf{v}=\lambda \cdot \mathbf{v}\end{equation} 其中 $\mathbf{A} $是$m * m$矩阵， $\mathbf{v}$是特征向量， $\lambda$是特征值。所有的特征向量都是正交（相互垂直）的。 矩阵相乘的意义，不加解释的给出：两个矩阵相乘的意义是将右边矩阵中的每一列列向量变换到左边矩阵中每一行行向量为基所表示的空间去。数学表达为： $$\begin{pmatrix} p_1 \\ p_2 \\ \vdots \\ p_R \end{pmatrix} \begin{pmatrix} a_1 &amp; a_2 &amp; \cdots &amp; a_M \end{pmatrix} = \begin{pmatrix} p_1a_1 &amp; p_1a_2 &amp; \cdots &amp; p_1a_M \\ p_2a_1 &amp; p_2a_2 &amp; \cdots &amp; p_2a_M \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ p_Ra_1 &amp; p_Ra_2 &amp; \cdots &amp; p_Ra_M \end{pmatrix}$$ 在PCA中中$R$表示降维之后的维度，如果$R$比原来的维数小，那么就达到了降维效果。 （2）PCA 既然是降维算法，那么需要去除一些维度，留下一些维度。留下的维度需要满足以下的要求 不依赖别的特征，具有独立性（低协方差） 具有较大的变化（和噪声不一样，是较高的方差） 算法步骤 1). 数据标准化（减均值，不是必须要除以标准差）2). 计算协方差矩阵3). 计算协方差矩阵的特征值和特征向量4). 计算主成分5). 降维 不是必须要除以标准差，如果数据特征都是在一个量纲，那么没有必要，否则话，需要。 （3）PCA的特点 简单的，无参数化的降维方法 对于离群点很敏感，因为PCA涉及到计算方差无法处理非线性的依赖关系 （4）代码实现 1). 基于numpy实现PCA 1234567891011121314151617181920212223from numpy import arrayfrom numpy import meanfrom numpy import covfrom numpy.linalg import eig# define a matrixA = array([[1, 2], [3, 4], [5, 6]])print(A)# calculate the mean of each columnM = mean(A.T, axis=1)print(M)# center columns by subtracting column meansC = A - Mprint(C)# calculate covariance matrix of centered matrixV = cov(C.T)print(V)# eigendecomposition of covariance matrixvalues, vectors = eig(V)print(vectors)print(values)# project dataP = vectors.T.dot(C.T)print(P.T) 2). Sklearn实现 12345678910111213141516# Principal Component Analysisfrom numpy import arrayfrom sklearn.decomposition import PCA# define a matrixA = array([[1, 2], [3, 4], [5, 6]])print(A)# create the PCA instancepca = PCA(2)# fit on datapca.fit(A)# access values and vectorsprint(pca.components_)print(pca.explained_variance_)# transform dataB = pca.transform(A)print(B) t-SNE（1）PCA和 t-SNE的background： 我们有一堆数据并且想要知道这些数据 “looks like”， 然后建立了一个”map”，使这些数据投影到了二维或者三维平面上。但是PCA 是线性 mapping， PCA learns a linear mapping, which is very restrictive. PCA focuses on preserving large pairwise distances.对于 t-SNE 的定义compute pairwise similarities between data with normalized Gaussian kernel （2）计算步骤 1). 在高纬计算两点 with normalized Gaussian kernel(高斯分布)$$p_{i,j}=\frac{\exp (-|x_{i}-x_{j}|^{2} / 2 \sigma^{2})}{\sum_{k} \sum_{l \neq k} \exp (-|x_{k}-x_{l}|^{2} / 2 \sigma^{2})}$$2). 在低维计算两点（使用 normalized Student-t similarities in the t-SNE map）$$q_{i,j}=\frac{(1+\overline{|} y_{i}-y_{j} |^{2})^{-1}}{\sum_{k} \sum_{l \neq k}(1+|y_{k}-y_{l}|^{2})^{-1}}$$3). 计算KL 散度（尽可能的最小化两者之间的差异， kl尽可能的小）$$K L(P | Q)=\sum_{i} \sum_{j \neq i} p_{i j} \log \frac{p_{i j}}{q_{i j}}$$ （3）数学补充 1). 为什么选择 KL divergence? The KL divergence preserves local data structure： 如果在高维度 p中相差大，但是在低维q 中相差小，那么kl 的值非常大。如果再高纬度p 相差小，但是在低纬度相差大，那么kl 的值就非常小。所以说尽可能保存了 local information。 降维必然带来信息损失，这里更加保存的是局部信息，那么必然损失的全局信息。而 t 分布能够放大这种密度，因为t 分布更加长尾。 2). t分布和卡方分布 关于 $t$ 分布， 假设 $X \sim N(0, 1) $ ， $Y \sim \chi^{2}(n)$ ，那么 $ Z=\frac{X}{\sqrt{Y / n}} $ 的分布就被称为自由度为 $n$ 的 $t$ 分布， 记做 $Z \sim t(n)$。 但自由度$n$ 越大，那么越是接近正太分布。一般来说 $t$ 分布比 正太分布更加长尾。如果是正太分布，那么就可以使用中心极限定理，但是 $t$ 分布就不可以了。$t$ 分布也被称为 学生t 分布。 若 $k$个随机变量 $Z_1, Z_2, … Z_n$是相互独立，符合标准正态分布的随机变量（数学期望为0、方差为1），则随机变量 $Z$的平方和$$X=\sum_{i=1}^{k} Z_{i}^{2}$$被称为自由度为 $k$ 的卡方分布， 记做 $X \sim \chi^{2}(k)$。 （3）T-SNE特点 1). 可以被用来做Do Use t-SNE to get some qualitative hypotheses on what your features capture 这个是定性分析 在输入和输出形式上可以 more creative 2). 不可以被用来 Don’t 不是证明你的理论的工具 assign meaning to distance across empty space（注意这个是局部的特征，not 全局的信息） think that t-sne will help you find outlier, or assign meaning to point densities in cluster forget the scale (perplexity) matters（ you can think of perplexity as the “effective” number of nearest neighbors， 所以说当这个 perplexity 越是接近原始一个簇的neighbors的个数，那么这个分类的效果是） forget that t-SNE 是解决一个 non-convex objective: there are local mimima local minima generally split a natural cluster into multiple parts （就是如何有这个效果，那么不要惊讶，有可能出现多个不同的parts） forget that low-dimentional metric spaces cannot caputure non-metric similarities （4）How input similarities in t-SNE are actually computed 1). compute conditional similarities$$p_{j |i}=\frac{\exp (-|x_{i}-x_{j}|^{2} / 2 \sigma_{i}^{2})}{\sum_{j^{\prime} \neq i} \exp (-|x_{i}-x_{j^{\prime}}|^{2} / 2 \sigma_{i}^{2})}$$perform a binary search over $\sigma_{i}$ to obtain a target perplexity ( 从这里得到启发，如果某个指标是不对称的，那么 symmetrize 就是可以得到一个对称的指标， 想想从kl divergence 到JS divergence这个过程)2). Symmetrize the conditions$$p_{i |j}=\frac{p_{j | i}+p_{i | j}}{2 N}$$ t-sne 时间复杂度 $n^2$ naive implementations are quadratic in the number of data points （5）Conclusion t-SNE is a valuable tool in generating hypotheses and understanding, but does not produce conclusive evidence. 来自do’s and Don’ts of using t-SNE to Understand Vision Models的阅读笔记。理解TSNE算法 feature selection定义： feature selection 的过程就是dimension reduction的过程。就是说由较多的数据集 映射到 较少的数据集，这种方式就叫做降维。 Using dimensionality reduction techniques, of course. You can use this concept to reduce the number of features in your dataset without having to lose much information and keep (or improve) the model’s performance. 为什么? （必要性分析） 时间角度，空间（内存）角度。减少冗余信息，就减少了模型去拟合”噪声“数据的可能性。 常见的有以下几种方式： 可以归结成几类：特征本身（数据缺省值比较大，数据的波动性比较小），特征和特征之间（特征具有较高的相关性，使用PCA 进行降维），特征和最后的target的关系（机器学习模型的 feature importance，卡方分布检测特征和target 的重要性，Pearson 相关系数：-1 表示负相关，0 表示不相关，1表示正相关）。 可以划分成三类： 一、独立于模型的特征选择（没有在入模时候进行的特征选择）或者叫做 filter： 移除低方差的特征 (Removing features with low variance) 当特征值都是离散型变量的时候这种方法才能用，如果是连续型变量，就需要将连续变量离散化之后才能用。 单变量特征选择 (Univariate feature selection) 对于分类问题(y离散)，可采用：卡方检验，f_classif, mutual_info_classif，互信息对于回归问题(y连续)，可采用：皮尔森相关系数，f_regression, mutual_info_regression，最大信息系数 这里说的是特征选择，但是上面说的都是针对“特征重要性” 这点展开的，但是一个特征入模是一个复杂的过程，需要考虑的因素很多。比如：变量的预测能力，变量之间的相关性，变量的简单性（容易生成和使用），变量的强壮性（不容易被绕过），变量在业务上的可解释性（被挑战时可以解释的通）等等。当然，其中最主要和最直接的衡量标准是变量的预测能力。 尤其是当你使用LR 这类简单的模型的时候，是需要重点的在特征上下功夫的，因为模型是线性的，比较简单，引入特征，加入非线性，然后才能更好的表达实际问题。 二、基于模型选择的特征排序 有些机器学习方法本身就具有对特征进行打分的机制，或者很容易将其运用到特征选择任务中，例如回归模型，SVM，决策树，随机森林等等。这种方法的思路是直接使用你要用的机器学习算法，针对 每个单独的特征 和 响应变量建立预测模型。假如 特征 和 响应变量 之间的关系是非线性的，可以用基于树的方法(决策树、随机森林)、或者 扩展的线性模型 等。 三、无监督的模型选择 聚类，可以从降维的角度理解。可以在机器学习算法中的importance 不是很大，容易被忽略的特征。 SVDSVD decomposes the original variables into three constituent matrices. It is essentially used to remove redundant features from the dataset. It uses the concept of Eigenvalues and Eigenvectors to determine those three matrices. We will not go into the mathematics of it due to the scope of this article, but let’s stick to our plan, i.e. reducing the dimensions in our dataset. $$\operatorname { Data } _ { m \times n } = U _ { m \times m } \Sigma _ { m \times n } V _ { n \times n } ^ { T }$$SVD将原始的数据集矩阵Data分解成三个矩阵：U、Sigma、VT，如果原始矩阵是m行n列，那么U、Sigma和VT分别就是m行m列、m行n列、n行n列。比较值得一提的是矩阵Sigma，该矩阵只有对角元素，其他元素均为0，有一个惯例是：Sigma的对角元素是从大到小排列的。这些对角元素就称为奇异值. PCA 是方阵是 $ m^2 $ 操作，那么SVD 是 $mn$ 就是更加广泛的矩阵操作。 特征分解只能分解方阵，奇异值分解可以分解任意矩阵，pca中的特征分解通常会使用svd。（方阵是一种特殊的矩阵，当行数和列数相同的时候就叫做方阵） 投影也是一种降维手段这种思想真的是服气，虽然我也不是很懂，但是思想是很好的By projecting one vector onto the other, dimensionality can be reduced. 当投影到另一个平面的时候，原来的平面维度就消失了，所以只剩下了投影面的维度。 T-sne就是指出 t-sne 这个可以非线性降维。有local approaches 和 global approaches 两种方式，我大概的理解就是：类之间还能尽可能的远离，类内保持差异性。（不知道对不对）So far we have learned that PCA is a good choice for dimensionality reduction and visualization for datasets with a large number of variables. But what if we could use something more advanced? What if we can easily search for patterns in a non-linear way? t-SNE is one such technique. There are mainly two types of approaches we can use to map the data points:Local approaches : They maps nearby points on the manifold to nearby points in the low dimensional representation.Global approaches : They attempt to preserve geometry at all scales, i.e. mapping nearby points on manifold to nearby points in low dimensional representation as well as far away points to far away points. LDA（有监督）和上面最大的区别在于 LDA 是有监督的，LDA试图让不同类别样本之间的距离最大，同时让相同类别样本之间的距离最小。简单来说LDA是为了使降维后的数据点尽可能的可分。 上图中国提供了两种投影方式，哪一种能更好的满足我们的标准呢？从直观上可以看出，右图要比左图的投影效果好，因为右图的黑色数据和蓝色数据各个较为集中，且类别之间的距离明显。左图则在边界处数据混杂。以上就是LDA的主要思想了，当然在实际应用中，我们的数据是多个类别的，我们的原始数据一般也是超过二维的，投影后的也一般不是直线，而是一个低维的超平面 这个是关于该算法的一个讲解， 如果对于步骤感兴趣的话。 PCA vs. LDALDA用于降维，和PCA有很多相同，也有很多不同的地方，因此值得好好的比较一下两者的降维异同点。 首先我们看看相同点： 1）两者均可以对数据进行降维。 2）两者在降维时均使用了矩阵特征分解的思想。 3）两者都假设数据符合高斯分布。我们接着看看不同点： 1）LDA是有监督的降维方法，而PCA是无监督的降维方法 2）LDA降维最多降到类别数k-1的维数，而PCA没有这个限制。 3）LDA除了可以用于降维，还可以用于分类。 4）LDA选择分类性能最好的投影方向，而PCA选择样本点投影具有最大方差的方向。 当类别特别多的时候，每个类中的样本就越少，此时更加适合使用PCA而不是LDA。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>pca</tag>
        <tag>svd</tag>
        <tag>t-sne</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[特征工程]]></title>
    <url>%2F2018%2F06%2F29%2F%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[关于特征工程的概念，特征离散化、特征生成、组合特征、特征选取的方法。 特征生成机器学习的一大任务在于手工 create 特征。特征主要来源有两个，一个统计学知识，一个该场景下的特征。前者属于常规操作，后者需要有对业务领域比较熟悉的了解。后者举个例子，比如在衡量个人信用的时候， days_employed / days_birth 就是一个属于金融（保险）领域的较为熟悉，才能理解的一个特征, 这种百分比，占比的思想还是非常常见的。前者同样举个栗子，对于子表（传统机器学习还是有很多来自数据库的信息的）常用的操作是 aggregation 操作（min max sum variance mean）等聚合操作，然后和主表进行连接。一般来说子表是可以广泛的使用 aggregation 操作，但是对于主表来说，这个就取决于信息是什么，特征是什么内容，聚合函数的本质在于”总结“，就是你操作的变量是否有必要这样做，看一下数据是不是流水账。 特征离散化连续化特征就是一些不可枚举的有理数。那么什么是离散化特征呢？ 离散化特征就是可枚举的特征。离散化的作用是把数据变成可计算状态。而特征工程就是从原始字段中根据业务提取出对模型有效的特征出来。 在线性模型下(w.x)，w已经确定的情况下，x的某个特征的值是20，或者30，w.x的值相差是很大的，但是往往20岁的人跟30岁的人对同一个广告的兴趣差距不会那么大。连续特征离散化的基本假设，是默认连续特征不同区间的取值对结果的贡献是不一样的。离散化和连续化最大的区别是，对一个字段做连续化后的结果就还只是一个特征，而离散化后的这一列有多少个key(字段可能的值)就会抽取出多少个特征。当经过离散化之后，特征各有各的权重，彼此之间就没有关系了。 模型是使用离散特征还是连续特征, 其实是一个“海量离散特征+简单模型” 同 “少量连续特征+复杂模型”的权衡。既可以离散化用线性模型，也可以用连续特征加深度学习。 常用的选取离散点的方法：等距离离散，等样本离散、画图观察趋势和决策树模型(天生就可以对连续特征分段)。 离散化：变量分箱（在风控模型中是这样进行阐述的），是对于连续变量的离散化的一种称呼。分箱的方式，一般有等距离分段，等深分段（先确定分段数量，然后令每个分段中的数据数量大致相等）和最优分段。 在工业界，很少直接将连续值作为特征喂给逻辑回归模型，而是将连续特征离散化为一系列0、1特征交给逻辑回归模型，这样做的优势有以下几点： 单变量离散化为N个后，每个变量有单独的权重，在激活函数的作用下相当于为模型增加了非线性，能够提升模型表达能力，加大拟合。 离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄&gt;30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰，因为特征值的异常会导致权重也就是w的值也会异常。 一定有同学担心特征过多会导致运算缓慢，但是LR是线性模型，我们在内部计算的时候是向量化计算，而不是循环迭代。稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展。 所以不用担心像GBDT算法那样，特征多了就跑不动了(我们都说GBDT不能用离散特征不是因为它处理不了离散特征，而是因为离散化特征后会产生特别多的特征，决策树的叶子节点过多，遍历的时候太慢了)。 所以海量离散特征＋LR是业内常见的一个做法。而少量连续特征+复杂模型是另外一种做法，例如GBDT。 当然也可以通过深度学习来进行特征选择：目前这种手段正在随着深度学习的流行而成为一种手段，尤其是在计算机视觉领域，原因是深度学习具有自动学习特征的能力，这也是深度学习又叫unsupervised feature learning的原因。从深度学习模型中选择某一神经层的特征后就可以用来进行最终目标模型的训练了。 参考文献:https://blog.csdn.net/lujiandong1/article/details/52412123 组合特征先是离散化，然后是特征组合。交叉从理论上而言是为了引入特征之间的交互，也即为了引入非线性。LR(逻辑回归）分类算法:因为线性函数的表达能力有限，所以我们引入激活函数就是给LR增加非线性关系。能让一条直线变成曲线。这样可以拟合出更好的效果。（也由此才后了后来说的过拟合问题而引入了正则化超参数） LR模型之所以很受欢迎，主要是因为LR模型本质是对数线性模型，实现简单，易于并行，大规模扩展方便，迭代速度快，同时使用的特征比较好解释，预测输出在0与1之间契合概率模型。（模型的可解释性举例，比如A-B的权重比较大，A代表用户，B代表物品，那么可以认为A是对B比较感兴趣的）但是，线性模型对于非线性关系缺乏准确刻画，特征组合正好可以加入非线性表达，增强模型的表达能力。另外，广告LR中，基本特征可以认为是用于全局建模，组合特征更加精细，是个性化建模，因为在这种大规模离散LR中，单对全局建模会对部分用户有偏，对每一用户建模又数据不足易过拟合同时带来模型数量爆炸，所以基本特征+组合特征兼顾了全局和个性化。 从统计的角度解释，基本特征仅仅是真实特征分布在低维的映射，不足以描述真实的分布，加入特征在高维空间拟合真实分布，使得预测更加准确。 寻找高级特征最常用的方法有：若干项特征加和： 我们假设你希望根据每日销售额得到一周销售额的特征。你可以将最近的7天的销售额相加得到。若干项特征之差： 假设你已经拥有每周销售额以及每月销售额两项特征，可以求一周前一月内的销售额。若干项特征乘积： 假设你有商品价格和商品销量的特征，那么就可以得到销售额的特征。若干项特征除商： 假设你有每个用户的销售额和购买的商品件数，那么就是得到该用户平均每件商品的销售额。 正则化真正测试一个模型的不是简单与否，更重要在于它在预测新的情况时表现如何。小权重意味着网络的行为不会因为我们随意更改了一些输入而改变太多。 这里是需要使用latex 补充一下公式的。公式是可以参考着上面进行书写。 https://testerhome.com/topics/10811 这是我们加了正则化之后的成本函数，可以看我们后面加入了正则化 λ 的表达式来完善成本函数。为什么加入λ能够减轻过拟合呢？直观一点的解释是设置的λ值越大，那么参数w的值就会被压缩的越小(在梯度下降中, 每次迭代的步长，也就是这个公式w=w - 学习率*成本函数对w的导数， 现在由于成本函数增加了正则项，使得J和w变得数值相关了)。 假设λ设置的足够大，那么w会无限的趋近于0. 把多隐藏层的单元的权重设置为0以后，那么基本上就是消除掉了这些单元的作用，而使得网络模型得到简化，就像下面的图一样。由于正则化的设置，消除了一些隐藏单元的作用。而使得整个模型越来越接近于线性化，也就是从下图中的过拟合往欠拟合偏转。当然我们有一个适合的λ的值，能让我们的拟合状态达到最佳。所以我们在训练模型的时候，往往都会有一个Ｌ２正则项的超参数需要我们设置。 feature selection定义： feature selection 的过程就是dimension reduction的过程。就是说由较多的数据集 映射到 较少的数据集，这种方式就叫做降维。 Using dimensionality reduction techniques, of course. You can use this concept to reduce the number of features in your dataset without having to lose much information and keep (or improve) the model’s performance. 为什么? （必要性分析） 时间角度，空间（内存）角度。减少冗余信息，就减少了模型去拟合”噪声“数据的可能性。 常见的有以下几种方式： 可以归结成几类：特征本身（数据缺省值比较大，数据的波动性比较小），特征和特征之间（特征具有较高的相关性，使用PCA 进行降维），特征和最后的target的关系（机器学习模型的 feature importance，卡方分布检测特征和target 的重要性，Pearson 相关系数：-1 表示负相关，0 表示不相关，1表示正相关）。 可以划分成三类： 一、独立于模型的特征选择（没有在入模时候进行的特征选择）或者叫做 filter： 移除低方差的特征 (Removing features with low variance) 当特征值都是离散型变量的时候这种方法才能用，如果是连续型变量，就需要将连续变量离散化之后才能用。 单变量特征选择 (Univariate feature selection) 对于分类问题(y离散)，可采用：卡方检验，f_classif, mutual_info_classif，互信息对于回归问题(y连续)，可采用：皮尔森相关系数，f_regression, mutual_info_regression，最大信息系数 这里说的是特征选择，但是上面说的都是针对“特征重要性” 这点展开的，但是一个特征入模是一个复杂的过程，需要考虑的因素很多。比如：变量的预测能力，变量之间的相关性，变量的简单性（容易生成和使用），变量的强壮性（不容易被绕过），变量在业务上的可解释性（被挑战时可以解释的通）等等。当然，其中最主要和最直接的衡量标准是变量的预测能力。 尤其是当你使用LR 这类简单的模型的时候，是需要重点的在特征上下功夫的，因为模型是线性的，比较简单，引入特征，加入非线性，然后才能更好的表达实际问题。 二、基于模型选择的特征排序 有些机器学习方法本身就具有对特征进行打分的机制，或者很容易将其运用到特征选择任务中，例如回归模型，SVM，决策树，随机森林等等。这种方法的思路是直接使用你要用的机器学习算法，针对 每个单独的特征 和 响应变量建立预测模型。假如 特征 和 响应变量 之间的关系是非线性的，可以用基于树的方法(决策树、随机森林)、或者 扩展的线性模型 等。 三、无监督的模型选择 聚类，可以从降维的角度理解。可以在机器学习算法中的importance 不是很大，容易被忽略的特征。 为什么选择 KL divergence? The KL divergence preserves local data structure： 如果在高维度 p中相差大，但是在低维q 中相差小，那么kl 的值非常大。如果再高纬度p 相差小，但是在低纬度相差大，那么kl 的值就非常小。所以说尽可能保存了 local information。 降维必然带来信息损失，这里更加保存的是局部信息，那么必然损失的全局信息。而 t 分布能够放大这种密度，因为t 分布更加长尾。 补充知识点（t分布和卡方分布） 关于 $t$ 分布， 假设 $X \sim N(0, 1) $ ， $Y \sim \chi^{2}(n)$ ，那么 $ Z=\frac{X}{\sqrt{Y / n}} $ 的分布就被称为自由度为 $n$ 的 $t$ 分布， 记做 $Z \sim t(n)$。 但自由度$n$ 越大，那么越是接近正太分布。一般来说 $t$ 分布比 正太分布更加长尾。如果是正太分布，那么就可以使用中心极限定理，但是 $t$ 分布就不可以了。$t$ 分布也被称为 学生t 分布。 若 $k$个随机变量 $Z_1, Z_2, … Z_n$是相互独立，符合标准正态分布的随机变量（数学期望为0、方差为1），则随机变量 $Z$的平方和$$X=\sum_{i=1}^{k} Z_{i}^{2}$$被称为自由度为 $k$ 的卡方分布， 记做 $X \sim \chi^{2}(k)$。 t-sne 时间复杂度 $n^2$ naive implementations are quadratic in the number of data points 可以被用来做Do Use t-SNE to get some qualitative hypotheses on what your features capture 这个是定性分析 在输入和输出形式上可以 more creative 不可以被用来 Don’t 不是证明你的理论的工具 assign meaning to distance across empty space（注意这个是局部的特征，not 全局的信息） think that t-sne will help you find outlier, or assign meaning to point densities in cluster forget the scale (perplexity) matters（ you can think of perplexity as the “effective” number of nearest neighbors， 所以说当这个 perplexity 越是接近原始一个簇的neighbors的个数，那么这个分类的效果是） forget that t-SNE 是解决一个 non-convex objective: there are local mimima local minima generally split a natural cluster into multiple parts （就是如何有这个效果，那么不要惊讶，有可能出现多个不同的parts） forget that low-dimentional metric spaces cannot caputure non-metric similarities How input similarities in t-SNE are actually computed compute conditional similarities$$p_{j |i}=\frac{\exp (-|x_{i}-x_{j}|^{2} / 2 \sigma_{i}^{2})}{\sum_{j^{\prime} \neq i} \exp (-|x_{i}-x_{j^{\prime}}|^{2} / 2 \sigma_{i}^{2})}$$perform a binary search over $\sigma_{i}$ to obtain a target perplexity ( 从这里得到启发，如果某个指标是不对称的，那么 symmetrize 就是可以得到一个对称的指标， 想想从kl divergence 到JS divergence这个过程) Symmetrize the conditions$$p_{i |j}=\frac{p_{j | i}+p_{i | j}}{2 N}$$ Conclusion t-SNE is a valuable tool in generating hypotheses and understanding, but does not produce conclusive evidence. 来自do’s and Don’ts of using t-SNE to Understand Vision Models的阅读笔记。理解TSNE算法 杂货铺 对于encodding 的理解，从字符转成数字，并且尽可能的保留原来的信息。接触的有三种，一种 label encoding，适合类别信息只有两类。如果大于两类那么就使用one-hot。第三种就是万物可以embedding，使用神经网络的思想。 特征工程分为特征构造 (feature creation or construction)和 特征选择（feature selection） 。 归一化（去中心，方差归一）是属于特征(预)处理:把特征值压缩成0~1的区间。 One-hot（也叫One-of-k）的方法把每个无序特征转化为一个数值向量。比如一个无序特征color有三种取值：red，green，blue。那么可以用一个长度为3的向量来表示它，向量中的各个值分别对应于red，green，blue。如：这种方法在NLP里用的很多，就是所谓的词向量模型。变换后的向量长度对于词典长度，每个词对应于向量中的一个元素。 过拟合和欠拟合问题： 可以从数据和模型两方面考虑。 过拟合 欠拟合 数据 收集更多的数据 或者 数据增强 模型 模型简单化 (dropout, normalizaion )，目标函数加上L1 or L2 ，不同模型取平均 模型变得复杂（如果树的结构，那么树个数增加），loss function 训练 early stop, 随机采样 (加入了随机性) 多训练一下看看效果 复习总结 特征来源一部分是业务场景，一部分是常规操作。常规操作包括aggregation聚合操作（主要针对子表），特征离散化，组合特征。业务特征，就是根据不同的场景，构造在该场景下重要字段。当然还有一些骚操作，使用机器模型进行特征的构造和选择，优点是work，缺点是可解释性差，比如使用xgboost +LR 模型，前者就是一种特征提取的功能，最后叶子结点的输出，知道其是重要的，但是不知道其含义是什么。 连续特征离散化的好处： (1) 增加了模型的非线性，提升了模型表达能力 (2) 离散化特征对异常数据具有很强的鲁棒性。常用的选取离散点的方法：(1) 等距离离散 (2) 等样本离散 (3) 画图 (4) 根据实际场景，比如对于年龄的划分 特征组合( 1)基本特征的非线性组合 (2) 特征之间的差和乘积商，mean,variance，std 统计学特征 特征选择（降维）的方法: (1) 特征本身 （如果缺省值比较大或者数据的波动比较小） (2) 特征之间的关系（特征之间有较强的相关性，可以使用PCA进行降维） (3 ) 特征和最后target 的关系 (feature importance, 卡房分布， pearson 相关系数) (4) 很多常见的机器学习模型都是一种特征选择的方式，比如xgboost LR 是广义的线性模型，内部是矩阵运算，速度相当的快。随着工业界计算能力的提升，使用xgboost 的范围是越来越大，不再局限于LR 模型。 正则化是使得模型不再变得那么复杂，具体是通过减少目标函数中loss 中weights 的相对大小。 embedding 的方式 (1) label embedding (2) one-hot (3) 神经网络，万物embedding]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[常见的排序算法总结]]></title>
    <url>%2F2018%2F06%2F29%2F%E5%B8%B8%E8%A7%81%E7%9A%84%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[分类和总结 根据待排序的数据大小不同，使得排序过程中所涉及的存储器不同，可分为内部排序和外部排序。 排序关键字可能出现重复，根据重复关键字的排序情况可分为稳定排序和不稳定排序。冒泡、插入和归并是稳定，在这里提到的其他的几个都是不稳定的。 对于内部排序，依据不同的排序原则，可分为插入排序、交换(快速)排序、选择排序、归并排序和计数排序。 针对内部排序所需的工作量划分，可分为:简单排序 O(n^2)、先进排序 O(nlogn)和基数排序 O(d*n)。常见算法的性质总结： 排序算法实现默认都是升序… 插入排序(Insert Sort)思想： 有序数组+insert one every one time 插入排序的工作方式非常像人们排序一手扑克牌一样。开始时，我们的左手为空并且桌子上的牌面朝下。然后，我们每次从桌子上拿走一张牌并将它插入左手中正确的位置。为了找到一张牌的正确位置，我们从右到左将它与已在手中的每张牌进行比较，如下图所示：步骤： 从第一个元素开始，该元素可以认为已经被排序 取出下一个元素，在已经排序的元素序列中从后向前扫描 如果该元素（已排序）大于新元素，将该元素移到下一位置 重复步骤3，直到找到已排序的元素小于或者等于新元素的位置 将新元素插入到该位置后 重复步骤2~5 1234567891011121314151617181920212223242526272829#include&lt;iostream&gt;#include&lt;string&gt;#include&lt;vector&gt;using namespace std;void insert_sort(vector&lt;int&gt; &amp; nums)&#123; int n = nums.size(); for(int i =1; i&lt;n; i++) &#123; int tmp =nums[i]; int j =i -1; // 如果当前的元素比之前的元素小，那么一直是为当前的元素往后挪地方 // 这样写法也是有规律的，好好体会 while(j &gt;=0 &amp;&amp; tmp &lt; nums[j]) &#123; nums[j +1] =nums[j]; j --; &#125; nums[j+1] =tmp; &#125;&#125;int main()&#123; vector&lt;int&gt; nums =&#123;12, 15, 9, 20, 6, 31, 24&#125;; insert_sort(nums); for(auto u: nums) cout &lt;&lt; u&lt;&lt;" "; cout &lt;&lt; endl; return 0;&#125; c++实现 1234567891011121314151617181920212223242526272829#include&lt;iostream&gt;#include&lt;string&gt;#include&lt;vector&gt;using namespace std;void insert_sort(vector&lt;int&gt; &amp; nums)&#123; int n = nums.size(); for(int i =1; i&lt;n; i++) &#123; int tmp =nums[i]; int j =i -1; // 如果当前的元素比之前的元素小，那么一直是为当前的元素往后挪地方 // 这样写法也是有规律的，好好体会 while(j &gt;=0 &amp;&amp; tmp &lt; nums[j]) &#123; nums[j +1] =nums[j]; j --; &#125; nums[j+1] =tmp; &#125;&#125;int main()&#123; vector&lt;int&gt; nums =&#123;12, 15, 9, 20, 6, 31, 24&#125;; insert_sort(nums); for(auto u: nums) cout &lt;&lt; u&lt;&lt;" "; cout &lt;&lt; endl; return 0;&#125; 选择排序(Select Sort)思想和步骤： select one every time 简单选择排序是最简单直观的一种算法，基本思想为每一趟从待排序的数据元素中选择最小（或最大）的一个元素作为首元素，直到所有元素排完为止，简单选择排序是不稳定排序。 分析： 无论什么数据进去都是 O(n²) 的时间复杂度。所以用到它的时候，数据规模越小越好。唯一的好处可能就是不占用额外的内存空间了吧。 代码实现: 12345678910111213def select_sort(lists): count =len(lists) for i in range(0, count): min =i for j in range(i+1, count): if lists[min] &gt;lists[j]: min =j lists[min], lists[i] =lists[i], lists[min] return lists# testlists =[1, 23,45, 0,-1]print(select_sort(lists)) 冒泡排序(Bubble Sort)思想： move smallest one time (假设不减) 一遍排序之后，右边是最大，也就是排好序的。这个规则是以此类推的。 有两种说法 从右往左： 最小值被移到了最左边。 【冒泡法】的本意 从左往右： 最大值被移到了最右边。 此时其实应该叫 【沉石法】 从左向右 沉石法 图解： 分析：平均时间复杂度：O(n^2)最坏空间复杂度： 总共 O(n)，需要辅助空间 O(1) 1234567891011def bubble_sort(lists): count =len(lists) for i in range(0, count): for j in range(i+1, count): if lists[i]&gt; lists[j]: lists[i], lists[j] =lists[j], lists[i] return lists# testlists =[1,34,45,0,89]print(bubble_sort(lists)) 归并排序(Merge Sort)思想： divide-and-conquer +递归 归并排序（MERGE-SORT）是利用归并的思想实现的排序方法，该算法采用经典的分治（divide-and-conquer）策略（分治法将问题分(divide)成一些小的问题然后递归求解，而治(conquer)的阶段则将分的阶段得到的各答案”修补”在一起，即分而治之)。 分析：时间复杂度：O(nlogn)空间复杂度：O(N)，归并排序需要一个与原数组相同长度的数组做辅助来排序 12345678910111213141516171819202122232425262728293031323334353637def merge_sort(nums, l, r, tmp): if(l &gt;= r): return mid =l +r &gt;&gt;1 merge_sort(nums, l , mid,tmp) merge_sort(nums, mid +1, r, tmp) i, j, k =l, mid +1, 0 while i &lt;=mid and j &lt;= r : if(nums[i] &lt;= nums[j]): tmp[k] =nums[i] i +=1 k +=1 else: tmp[k] =nums[j] j +=1 k +=1 while i&lt;= mid: tmp[k ] =nums[i] k +=1 i +=1 while j&lt;=r : tmp[k] =nums[j] k +=1 j +=1 i =l for j in range(k): nums[i] = tmp[j] i +=1 if __name__ =="__main__": n =int(input()) nums =list(map(int, input().split())) tmp =[0] *n merge_sort(nums, 0, n-1, tmp) nums =list(map(str, nums)) print(" ".join(nums)) c++ 的实现 1234567891011121314151617181920212223242526272829303132333435// 归并排序，分而治之，时间复杂度是 O(nlogn)#include&lt;bits/stdc++.h&gt;using namespace std;const int N =1e5+11;int nums[N], tmp[N];int n;void merge_sort(int nums[], int l, int r)&#123; if(l &gt;=r ) return ; int mid = l +r &gt;&gt;1; merge_sort(nums, l, mid); merge_sort(nums, mid +1, r); int k =0, i =l, j =mid +1; while(i &lt;=mid &amp;&amp; j &lt;= r) &#123; if(nums[i]&lt;= nums[j]) tmp[k ++] =nums[i++]; else tmp[k++] =nums[j ++]; &#125; while(i &lt;= mid) tmp[k ++] =nums[i ++]; while(j &lt;= r) tmp[k++]= nums[j ++]; // push back for(int i =l,j =0; j &lt;k;) nums[i++]= tmp[j++];&#125;int main()&#123; //注意c 语言中的读入和输出 都是有f 的 scanf("%d", &amp;n); for(int i =0; i&lt;n; i++) scanf("%d", &amp;nums[i]); merge_sort(nums, 0, n -1); for(int i =0; i&lt;n; i++) printf("%d ", nums[i]); printf("\n"); return 0;&#125; 快速排序(Quick Sort)思想：任意选择一个key(通常选择a[0])，将比他小的数据放在它的前面，比他大的数字放在它的后面。递归进行。 步骤： 从数列中挑出一个基准值。 将所有比基准值小的摆放在基准前面，所有比基准值大的摆在基准的后面(相同的数可以到任一边)；在这个分区退出之后，该基准就处于数列的中间位置。 递归地把”基准值前面的子数列”和”基准值后面的子数列”进行排序。 分析：快速排序的时间复杂度在最坏情况下是O($N^2$)，平均的时间复杂度是O(N*logN)，采用的是分治的思想，二叉树的结构。 下面以数列 a={30,40,60,10,20,50} 为例，演示它的快速排序过程(如下图)。 这个是经过一个迭代的结果，每一个迭代，都排好了基准数字的位置。按照同样的方法，对子数列进行递归遍历。最后得到有序数组！ 在实现的过程中，key 值的选择 while 的遍历顺序是相关的。如果key =arr[left] 那么第一个while 循环是从right 开始的（从后往前）；如果key =arr[right] 那么是从left 进行遍历的。 基于python3 的实现。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849# 从小到大排def quickSort(arr, l, r): if l&gt; r: return left, right =l , r key = arr[l] while(l &lt;r): while(l &lt;r and arr[r] &gt;= key): r -=1 arr[l] =arr[r] while l&lt;r and arr[l] &lt;= key : l +=1 arr[r] =arr[l] arr[l] =key quickSort(arr, left, l -1) quickSort(arr, l +1, right)def partition(arr, l , r): key =arr[l] while(l &lt;r): while(l &lt;r and arr[r] &gt;= key): r -=1 arr[l] =arr[r] while l&lt; r and arr[l] &lt;= key : l +=1 arr[r] =arr[l] arr[l] =key return ldef quickSort2(arr, l, r): if( l&gt; r): return pos =partition(arr, l, r) quickSort2(arr, l, pos -1) quickSort2(arr, pos +1, r)if __name__ ==&quot;__main__&quot;: n =int(input()) arr =input().split(&quot; &quot;) arr =[int(a) for a in arr] print(arr) #quickSort(arr, 0, len(arr)-1) quickSort2(arr, 0, len(arr) -1) print(arr) python 版本 12345678910111213141516171819def quick_sort(arr, l, r): left, right =l, r key =arr[l] while(l &lt;r): while(l &lt;r and arr[r] &gt;=key): r --; arr[l] =arr[r] while(l&lt; r and arr[l] &lt;= key): l ++; arr[r] =arr[l] arr[l] =key quick_sort(arr, left, l-1) quick_sort(arr, l +1, right)arr =[12, 0,3, 34]quick_sort(arr, 0, len(arr)-1)print(arr) 给出两种 C ++ 的代码，发现有两点。 分开写之后，可以发现和 下面那个算法题目的关系： 这个基准点就是那个 Kth 的一个参考 关于while 条件的分析， 如果是i &lt;=j 那么返回的是j， 如果是 i&lt;j 那么返回的i 作为基准点 关于快速排序的三种写法。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899#include&lt;iostream&gt;#include&lt;vector&gt;using namespace std;// 从大到小void quickSort(vector&lt;int&gt;&amp; arr, int l, int r)&#123; if(l &gt;r) return ; int left =l, right =r; int key =arr[r]; while(l &lt;r) &#123; while(l &lt;r &amp;&amp; arr[l] &gt;= key) l ++; arr[r] =arr[l]; while(l &lt;r &amp;&amp; arr[r] &lt;= key) r --; arr[l] =arr[r]; &#125; arr[r] =key; quickSort(arr, left, l -1); quickSort(arr, l +1, right); &#125;// 从小到大void quickSort2(vector&lt;int&gt; &amp;arr, int l, int r)&#123; if (l &gt;r) return ; int left =l, right =r; int key =arr[l]; while(l &lt; r) &#123; while(l &lt;r &amp;&amp; arr[r] &gt;= key) r --; arr[l] =arr[r]; while(l&lt;r &amp;&amp; arr[l] &lt;= key) l ++; arr[r] =arr[l]; &#125; arr[l] =key; quickSort2(arr, left, l-1); quickSort2(arr,l +1, right); &#125;/// 从小到大进行排序int partition(vector&lt;int&gt; &amp;arr, int l, int r)&#123; int key =arr[l]; while(l &lt;r) &#123; while(l &lt;r &amp;&amp; arr[r] &gt;= key) r --; arr[l] =arr[r]; while(l &lt;r &amp;&amp; arr[l] &lt;= key) l++; arr[r] =arr[l]; &#125; arr[l] =key; return l;&#125;void quickSort3(vector&lt;int&gt; &amp; arr, int l, int r)&#123; if(l &gt;r) return ; int pos =partition(arr, l, r); quickSort3(arr, l, pos -1); quickSort3(arr, pos+1, r);&#125;int main()&#123; vector&lt;int&gt; arr; int n ; cin&gt;&gt;n; arr =vector&lt;int&gt;(n); for(int i =0; i&lt;n; i++) cin &gt;&gt;arr[i]; //for(auto u :arr) cout&lt;&lt; u&lt;&lt;" "; //cout&lt;&lt;endl; quickSort3(arr, 0, n-1); for(auto u :arr) cout&lt;&lt; u&lt;&lt;" "; cout&lt;&lt;endl; return 0;&#125; 1234567891011121314151617181920212223242526272829303132333435363738#include&lt;iostream&gt;#include&lt;vector&gt;using namespace std;// 快排 一般时间复杂度 nlogn ，最坏的时间复杂 n^2，一般很难达到void quick_sort(vector&lt;int&gt; &amp; arr, int l, int r)&#123; if (l &gt;r) return ; int left =l, right =r; int key =arr[l]; while(l &lt;r) &#123; while(l &lt;r &amp;&amp; arr[r] &gt;= key) r -=1; arr[l] =arr[r]; while(l&lt; r &amp;&amp; arr[l] &lt;= key) l +=1; arr[r] =arr[l]; &#125; arr[l] =key; quick_sort(arr, left, l-1); quick_sort(arr, l+1, right);&#125;int main()&#123; vector&lt;int&gt; arr; int n; cin &gt;&gt;n; for(int i =0; i&lt;n; i++) &#123; int tmp; cin&gt;&gt; tmp; arr.push_back(tmp); &#125; quick_sort(arr, 0, arr.size() -1); for(auto u: arr) cout &lt;&lt; u&lt;&lt;" "; cout &lt;&lt;endl; return 0;&#125; Kth Largest Element in an Array Find the kth largest element in an unsorted array. Note that it is the kth largest element in the sorted order, not the kth distinct element. Tips: 使用的是 快排的思想，最后的平均时间复杂度是O(N) ，所以是一个不错的算法。 kth 和 ”快排“ 实现的时候稍微有一些区别，前者只有发现 一组数据（一个比pivot 大，一个比 pivot 小）才进行交换，后者是是 两个while， 只要发现一个就进行交换。这个是细微的差别。求解K 最大，那么数组适合的是降序，求解K 最小，那么数组适合升序。关键这个求解的是 K 最大或者最小，那么是不需要整个数组是有序的，而是找出某个数字是，即可。 123456789101112131415161718192021222324252627282930def partition(arr, left, right): key =arr[right] low, high =left, right while left&lt; right: while left &lt;right and arr[left]&gt;=key: left +=1 arr[right] =arr[left] while left&lt;right and arr[right] &lt;=key: right -=1 arr[left] =arr[right] arr[right] =key return leftdef findKthLargest(arr, k): left, right =0, len(arr)-1 while True: pos =partition(arr, left, right) if pos ==k-1: return arr[pos] elif pos &gt; k-1: right =pos -1 else: left = pos+1 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283#include&lt;iostream&gt;#include&lt;vector&gt;using namespace std;// 时间复杂度是 nlog k// 注意这里一定要传递的是引用int partition(vector&lt;int&gt;&amp; arr, int l, int r)&#123; int key =arr[r]; while(l &lt;r) &#123; while(l &lt;r &amp;&amp; arr[l] &gt;= key) l ++; arr[r] =arr[l]; while(l &lt;r &amp;&amp; arr[r] &lt;= key) r --; arr[l] =arr[r]; &#125; arr[r] =key; return l;&#125;int find_kth_largest(vector&lt;int&gt;&amp; arr,int n, int k)&#123; if(k &gt; n) return -1; int l =0, r =n -1; while( true) &#123; int pos =partition(arr, l, r); if( pos ==k -1) return arr[pos]; else if(pos &gt; k -1) r =pos -1; // k-1在小部分，那么r =pos -1 else l =pos +1; &#125;&#125;int partition1(vector&lt;int&gt; &amp; arr, int l, int r)&#123; int key =arr[l]; while(l &lt;r) &#123; while(l &lt;r &amp;&amp; arr[r] &gt;= key) r --; arr[l] =arr[r]; while(l &lt;r &amp;&amp; arr[l] &lt;= key) l ++; arr[r] =arr[l]; &#125; arr[l] =key; return l;&#125;int find_kth_smallest(vector&lt;int&gt;&amp; arr, int n, int k)&#123; if(k &gt;n ) return -1; int l =0, r =n -1; while(true) &#123; int pos =partition1(arr, l, r); if(pos ==k -1) return arr[pos]; else if(pos &lt; k-1) l =pos +1; // 二分的真谛在于一定包含结果区间 else r =pos -1; &#125;&#125;int main()&#123; /* int n, k ; cin &gt;&gt;n&gt;&gt;k; vector&lt;int&gt; arr; for(int i =0; i&lt;n; i++) &#123; int tmp; cin &gt;&gt;tmp; arr.push_back(tmp); &#125; */ vector&lt;int&gt; arr =&#123;1, 5, 7, 0, 2&#125;; int k =2; int n =5; cout &lt;&lt;"result: "&lt;&lt;endl; cout &lt;&lt; find_kth_largest(arr, n, k)&lt;&lt; endl; cout &lt;&lt; find_kth_smallest(arr, n, k)&lt;&lt; endl; return 0;&#125; 使用内置的函数先sort () 然后选择，那么时间复杂度是 $nlogn$。 如果使用partition() 函数， 就能优化成 $O(n)$. 讲解如下 That’s not quick-sort, that’s quick-select. After partitioning, the the former continues to sort both parts while the latter continues to search only one part. So your costs are like n+n/2+n/4+n/8+… = 2n = O(n) instead of n+2·n/2+4·n/4+… = O(n log n). 123456789101112131415161718192021222324252627282930313233343536373839404142434445#include&lt;iostream&gt;#include&lt;vector&gt;using namespace std;int partition(vector&lt;int&gt; &amp; arr, int l, int r)&#123; int key =arr[r]; while(l &lt;r) &#123; while(l &lt;r &amp;&amp; arr[l] &gt;= key) l ++; arr[r] =arr[l]; while (l &lt;r &amp;&amp; arr[r]&lt;= key) r --; arr[l] = arr[r]; &#125; return l;&#125;int kth_largest(vector&lt;int&gt; arr, int k)&#123; if(arr.empty()) return -1; int l =0,r= arr.size()-1; if (k &gt;arr.size()) return -1; while (true) &#123; int pos =partition(arr, l, r); if(pos ==k -1) return arr[k]; else if(pos &gt; k-1) r =pos -1; else l = pos +1; &#125; return -1;&#125;int main()&#123; int n, k; vector&lt;int&gt; arr; cin&gt;&gt; n&gt;&gt;k; arr =vector&lt;int&gt;(n); for(int i =0; i&lt;n; i++) cin&gt;&gt; arr[i]; cout&lt;&lt; kth_largest(arr, k)&lt;&lt;endl; return 0;&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041# Hello World program in Python# python 中只是支持 +=1 不支持 ++ 操作def partition(arr, l, r): key = arr[r] while l &lt; r: while l &lt; r and arr[l] &gt;= key: l +=1 arr[r] = arr[l] while l &lt; r and arr[r] &lt;= key: r -=1 arr[l] = arr[r] arr[r] = key return ldef findKthLargest(nums, k): """ :type nums: List[int] :type k: int :rtype: int """ if not nums or k &gt; len(nums): return False; l, r = 0, len(nums) - 1 while True: pos = partition(nums, l, r) if (pos == k - 1): return nums[pos] elif (pos &gt; k - 1): r =pos -1 else: l =pos +1 return -1nums = [1, 2, 3, 4, 5]k = 2print(findKthLargest(nums, k)) 堆排序堆排序(Heap Sort)的时间复杂度是$O(nlogn) $, 最坏情况下也是如此.而快速排序(Quick Sort), 若初始记录序列有序, 快速排序将退化为起泡排序(Bubble Sort), 时间复杂度是$O(n^2) $.这是堆排序比快速排序的优点. 堆排序可以使用数组实现，假设”第一个元素”在数组中的索引为 1 的话，则父节点和子节点的位置关系如下： 索引为i的左孩子的索引是 (2*i); 索引为i的左孩子的索引是 (2*i+1); 索引为i的父结点的索引是 floor(i/2); 当然也是可以从 0开始计数。在C++ 中使用 标准库函数中的 priority_queue 就可以使用。优先队列。 参看另外一篇博客 使用堆排序的一个问题，求解最小K 个元素。时间复杂度是 $O(nlog k) $.push_up操作时间复杂度O(logk)，push_down也是O(logk)，总计遍历数据一遍。所以最终复杂度是O(Nlogk)。详细的解释可以看这里 。实现的时候，直接使用库函数了。 堆是一棵顺序存储的完全二叉树，堆排序是一种树形选择排序，其时间复杂度为O(nlogn)，空间复杂度:对于记录较少的文件不推荐使用，对于较大的文件还是有效的.堆分为大根堆和小根堆。大根堆的要求是每个节点的值都不大于其父节点的值，即A[PARENT[i]] &gt;= A[i]。小根堆的要求是每个节点的值都不小于其父节点的值，即A[PARENT[i]] &lt;= A[i]。 堆的每次调整交换堆顶和最后一个元素，然后只是调整堆顶和堆顶的左右孩子树的关系。有建立堆，调整堆和堆排序三个步骤。 这个是讲解视频 123456789101112131415161718192021222324n,m =tuple(map(int, input().split()))nums =list(map(int, input().split()))def heapify( x): if x&gt;=n : return cl =2*x+1 cr =cl +1 min_v =x if(cl &lt;n and nums[min_v] &gt; nums[cl]): min_v =cl if(cr &lt;n and nums[min_v] &gt; nums[cr]): min_v =cr if(min_v != x): nums[min_v], nums[x] =nums[x], nums[min_v] heapify(min_v) if __name__ =="__main__": # build heap for i in range((n-1)//2, -1, -1): heapify(i) # heap sort while m: print(str(nums[0])+" ") nums[0] =nums[n-1] heapify(0) n -=1 m -=1 使用c++ 实现的是小根堆， heapify , build heap 和heap sort 的操作. 使用数组作为存储的结构，从 1到n 存储，这样第 i 个结点的父节点是 i/2， 左右子节点是 2*i 和2*i+1 123456789101112131415161718192021222324252627282930313233343536// 堆排序的核心思想，保证堆顶元素和左右两个孩子的大小关系； 如果是大根堆，那么就是堆顶元素大于两个孩子的元素#include&lt;bits/stdc++.h&gt;using namespace std;const int N =1e5+11;int n,m;int nums[N];// 这个是小根堆void heapify(int x)&#123; int lc = 2*x, rc =lc+1; int min_v =x; if(lc &lt;=n &amp;&amp; nums[min_v] &gt; nums[lc] ) min_v =lc; if(rc &lt;=n &amp;&amp; nums[min_v] &gt; nums[rc]) min_v =rc; if(min_v != x) &#123; swap(nums[min_v], nums[x]); heapify(min_v); &#125;&#125;int main()&#123; cin &gt;&gt;n &gt;&gt;m; for(int i =1; i&lt;=n;i++) scanf("%d", &amp;nums[i]); // build heap for(int i =n/2; i; i--) heapify(i); // heap sort while(m --) &#123; printf("%d ", nums[1]); // 堆顶元素 nums[1] =nums[n]; heapify(1); n --; &#125; return 0;&#125; 补充： stack 栈， FILO，先进后出，栈溢出都是它 heap，大根堆、小根堆，常用在大量的数据进行排序，是一种树形结构 queue 队列，先进先出 c++ 和python中都是如何调用的呢？（补充） 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138```### 桶排序排序算法分两大类，基于比较的排序和非基于比较的排序。没啥好解释的，基于比较的，比如冒泡排序、插入排序、选择排序、希尔排序、堆排序、归并排序、快速排序，基于比较的排序复杂度下限为 $O(nlogn) $。非基于比较的排序如，桶排序、计数排序、基数排序。非基于比较的排序不受 $O(nlogn) $这个下限的约束，能够达到 $O(n)$的复杂度。但是非基于比较的算法一般对数据有比较严格的要求，所以一般用来处理一些有特征性的数据的排序。适用范围：主要适用于小范围整数数据，且独立均匀分布，可以计算的数据量很大，而且符合线性期望时间。算法步骤：1. 设置固定数量的空桶。2. 把数据放到对应的桶中。3. 对每个不为空的桶中数据进行排序。4. 拼接从不为空的桶中数据，得到结果。![](https://ftp.bmp.ovh/imgs/2019/09/baa4ff53505e9cb3.png)1. 设置桶的数量为5个空桶，找到最大值110，最小值7，每个桶的范围20.8=(110-7+1)/5 。2. 遍历原始数据，以链表结构，放到对应的桶中。数字7，桶索引值为0，计算公式为floor((7 – 7) / 20.8)， 数字36，桶索引值为1，计算公式floor((36 – 7) / 20.8)。3. 当向同一个索引的桶，第二次插入数据时，判断桶中已存在的数字与新插入数字的大小，按照左到右，从小到大的顺序插入。如：索引为2的桶，在插入63时，桶中已存在4个数字56，59，60，65，则数字63，插入到65的左边。4. 合并非空的桶，按从左到右的顺序合并0，1，2，3，4桶。5. 得到桶排序的结构实际的应用场景：一年的全国高考考生人数为500 万，分数使用标准分，最低100 ，最高900 ，没有小数，你把这500 万元素的数组排个序。### 拓扑排序拓扑序列： 设在 $G =(V, E)$ 是一个具有 $n$ 个顶点的有向图，集合 $V$ 的顶点序列 $v_1$, $v_2$... $v_n$ 称为一个拓扑序列，需满足下列条件： 若从顶点 $v_i$ 到$v_j$ 有一条路径，那么在顶点序列中顶点 $v_i$ 必须在顶点 $v_j$ 之前。在图论中，拓扑排序（Topological Sorting）是一个有向无环图（DAG, Directed Acyclic Graph）的所有顶点的线性序列。且该序列必须满足下面两个条件：- 每个顶点出现且只出现一次。- 若存在一条从顶点 A 到顶点 B 的路径，那么在序列中顶点 A 出现在顶点 B 的前面。拓扑排序基本思想：1. 从AOV 网中选择一个没有前驱顶点并且输出2. 从AOV 网中删除该顶点，并且删去所有以该顶点为尾的弧3. 重复上述两部，直到全部顶点都被输出，或者AOV 网中不存在没有前驱的顶点。拓扑序列不是唯一的哦。AOV 网： activity on vexes（在顶点上的活动）** 拓扑排序的应用**1. 判断是否存在环，如果输出能够得到所有的顶点，那么没有存在环；否则就存在环。2. 拓扑排序通常用来“排序” 具有依赖关系的任务。这里要补充两个概念，偏序和全序？偏序：有向图中两个顶点之间不存在环路，至于连通与否，是无所谓的。全序：就是在偏序的基础之上，有向无环图中的任意一对顶点还需要有明确的关系(反映在图中，就是单向连通的关系，注意不能双向连通，那就成环了)。 意思就是讲，一个不确定的偏序关系经全序后就有一种确定的先后顺序了。既然有先后，那么在实际生活中的选课问题，比如大一时一定要修完这门课，大二才学第二门课，这种排课问题就是拓扑排序问题。图的存储结构- 邻接矩阵和邻接表简单的说，图由表示数据元素的集合V和表示数据之间关系的集合E组成，记为G=&lt;V,E&gt;。图又分为有向图与无向图。下面是图的一些基本元素：- 边(edge)：顶点的序偶。- 顶点(vertex)：数据元素。- 权重(weight)：用来表示一个顶点到另一个顶点的距离、代价、耗费等。- 度(degree)：与该顶点相关联的边的数目，入度、出度等等。![Screen Shot 2019-10-11 at 7.32.09 PM.png](https://i.loli.net/2019/10/11/ORUaEy354eIdri1.png)图的相邻矩阵储存类型图的相邻矩阵或邻接矩阵表示定点之间的邻接关系，即表示顶点之间有边或没有边的情况。如下图则是无向图G1和有向图G2的邻接矩阵。![1.png](https://i.loli.net/2019/10/11/OwATCVBh3dova45.png)无论是有向图还是无向图，邻接矩阵比较好表示。有向图的邻接表是比较好表示：![img](https://upload.cc/i1/2019/10/11/tfJLEh.png)下一个结点是有向图中结点的下一个结点。![img](https://upload.cc/i1/2019/10/11/XQnRjA.png)使用邻接表表示无向图[](http://tva1.sinaimg.cn/large/007X8olVly1g7ui24l3eqj30n409hq54.jpg)[Course Schedule](https://leetcode.com/problems/course-schedule/description/)使用邻接表和队列 （bfs）实现拓扑排序。 时间复杂度是 $ O(n+m)$```c++class Solution &#123;public: bool canFinish(int num, vector&lt;vector&lt;int&gt;&gt;&amp; pre) &#123; vector&lt;vector&lt;int&gt;&gt; adj(num); vector&lt;int&gt; in_degree(num, 0); for(int i =0; i&lt; pre.size() ; i++) &#123; in_degree[pre[i][0]] ++; adj[pre[i][1]].push_back(pre[i][0]); &#125; for(auto u: in_degree) cout &lt;&lt; u&lt;&lt;&quot; &quot;; queue&lt;int&gt; q; int couts =0; for(int i =0; i&lt; num; i++) &#123; if(in_degree[i] ==0) q.push(i); &#125; while(!q.empty()) &#123; int t =q.front(); q.pop(); couts +=1; for(int i =0; i&lt; adj[t].size(); i++) &#123; in_degree[adj[t][i]] --; if(in_degree[adj[t][i]] ==0) q.push(adj[t][i]); &#125; &#125; return couts ==num; &#125;&#125;; 207. Course Schedule 拓扑排序 ($O(m +n)$) 将先修关系构成一张图，由每个数对的第二个数字向第一个数字连边。 首先将所有入度为0的点进队，准备拓扑排序。 宽搜过程中，将当前结点所关联的结点的入度减1；若发现新的入度为0的结点，则将其进队。 最后如果遍历了所有结点，则说明可以满足要求；否则，先修关系存在环。 12345678910111213141516171819202122232425262728293031class Solution &#123;public: // 是一个判别题目， 如果说拓扑排序能够排下来，那么就可以认为是达到了要求 bool canFinish(int n, vector&lt;vector&lt;int&gt;&gt;&amp; pre) &#123; vector&lt;vector&lt;int&gt;&gt; adj(n); // 这个表明可以 adj[i] 这样进行遍历 vector&lt;int&gt; in_degree(n, 0); queue&lt;int&gt; q; int counts =0; // 处理原来的数据变成邻接表和入度表 for(int i =0; i&lt; pre.size() ; i++) &#123; adj[pre[i][1]].push_back(pre[i][0]); in_degree[pre[i][0]] ++; &#125; // 这个n 表示的就是课程的数量，判别的是课表的合理与否，所以直接push的i for(int i =0; i&lt; n ; i++) if(in_degree[i] ==0) q.push(i); while(q.size()) &#123; auto t =q.front(); q.pop(); counts ++; for(int i =0; i&lt; adj[t].size(); i++) &#123; in_degree[adj[t][i]] --; if(in_degree[adj[t][i]] ==0) q.push(adj[t][i]); &#125; &#125; return counts ==n; &#125;&#125;; python 实现 12345678910111213141516171819202122232425262728293031from queue import Queueclass Solution: def canFinish(self, n: int, pre: List[List[int]]) -&gt; bool: adj =&#123;&#125; in_degrees =&#123;&#125; # 初始化的操作, 类似实现了一种键值对的关系 for i in range(n): adj[i] =[] in_degrees[i] =0 # 赋值语句 ''' for i in range(len(pre)): adj[pre[i][1]].append(pre[i][0]) in_degrees[pre[i][0] ] +=1 ''' for c, p in pre: adj[p].append(c) in_degrees[c] +=1 q = Queue() # 将所有入度为0 的点，放入到 for i in range(n): if(in_degrees[i] ==0): q.put(i) count =0 print(adj) while q.qsize(): t =q.get() count +=1 for val in adj[t]: in_degrees[val] -=1 if(in_degrees[val] ==0): q.put(val) return n ==count; python3 中使用queue 的常见的操作 12345python 中使用queue 的常用操作from queue import *q = Queue()q.put(i)t =q.get() 210. Course Schedule II 是第一类 1234567891011121314151617181920212223242526272829303132class Solution &#123;public: // 使用c++ 实现拓扑排序 vector&lt;int&gt; findOrder(int n, vector&lt;vector&lt;int&gt;&gt;&amp; pre) &#123; vector&lt;vector&lt;int&gt;&gt; adj(n); vector&lt;int&gt; in_degree(n, 0); queue&lt;int&gt; q; // 初始化 for(int i =0; i&lt; pre.size(); i++) &#123; adj[pre[i][1]].push_back(pre[i][0]); in_degree[pre[i][0]] +=1; &#125; // 维护一个队列 for(int i =0; i&lt; n; i++) if(in_degree[i] ==0) q.push(i); vector&lt;int&gt; res; while(q.size()) &#123; auto t =q.front(); q.pop(); res.push_back(t); for(int i =0; i&lt; adj[t].size() ; i++) &#123; in_degree[adj[t][i]] -=1; if(in_degree[adj[t][i]] ==0) q.push(adj[t][i]); &#125; &#125; if(res.size() == n) return res; else return vector&lt;int&gt;&#123;&#125;; &#125;&#125;; python 版本 1234567891011121314151617181920212223242526class Solution: # 上一道题目返回的是一个true or false 的问题，这道题目返回的是拓扑排序之后的结果， 使用一个list 存储就好了 def findOrder(self, n: int, pre: List[List[int]]) -&gt; List[int]: adj =&#123;&#125; in_degree=&#123;&#125; # 初始化 for i in range(n): adj[i] =[] in_degree[i] =0 # 处理pre 的关系 for c, p in pre: adj[p].append(c) in_degree[c] +=1 # 初始化queue from queue import Queue q =Queue() for i in range(n): if(in_degree[i] ==0): q.put(i) res =[] while q.qsize(): t =q.get() res.append(t) for val in adj[t]: in_degree[val] -=1 if(in_degree[val] ==0): q.put(val) return res if len(res) ==n else [] 时间复杂度排序时间复杂度为 $O(nlogn)$，枚举课程需$ O(n)$，每次访问单调队列需要 $O(logn)$（因为基于 大根堆实现的），故总时间复杂度为 $O(nlogn)$。 空间复杂度$O(n)$ 630. Course Schedule III 这种算法更加的精妙，先是无脑的加入，如果有不符合条件的，那么就pop 出来。需要单独记录一个tot 的时间， tot[i]+ lasting &gt; d[i] 的时间，那么就需要弹出。 1234567891011121314151617181920212223242526class Solution &#123;public: // 排序算法+使用大根堆 int static cmp(vector&lt;int&gt; &amp; a, vector&lt;int&gt; &amp;b) &#123; return a[1] &lt; b[1]; &#125; int scheduleCourse(vector&lt;vector&lt;int&gt;&gt;&amp; courses) &#123; // 首先是按照结束时间排序， sort(courses.begin(), courses.end(), cmp); // 使用大根堆，保证当每次弹出的时候，弹出的都是持续时间最长的那个 priority_queue&lt;int&gt; max_heap; int ton =0; for(int i =0; i&lt; courses.size(); i++) &#123; max_heap.push(courses[i][0]); ton += courses[i][0]; if(ton &gt; courses[i][1]) &#123; ton -= max_heap.top(); // 这个还是比较有意思的 max_heap.pop(); &#125; &#125; return max_heap.size(); &#125;&#125;; python 实现python 内置的函数实现的是小根堆，那么push 进去的是 -t,这个操作中更加能体现 先是无脑加，如果不符合要求，那么再pop 出来。 多思考使用大根堆和小根堆区别。是要保证pop 出来的是时长最长的那个，因为如果可以的话，尽可能选择时长短的课程呢。 12345678910111213class Solution: # python 中使用heapq 实现的小根堆 # 如果从逻辑上应该使用大根堆，那么在添加值的时候，应该添加负值 def scheduleCourse(self, courses: List[List[int]]) -&gt; int: courses.sort(key=lambda x: x[1]) tot =0 min_heap =[] for l, d in courses: heapq.heappush(min_heap, -l) tot += l if(tot &gt; d): tot += heapq.heappop(min_heap) return len(min_heap) 小范围（基本有序）排序已知一个几乎有序的数组，几乎有序是指，如果把数组排好顺序的话，每个元素移动的距离可以不超过 $k$，并且k相对于数组来说比较小。请选择一个合适的排序算法针对这个数据进行排序。 给定一个int数组A，同时给定A的大小 $n$和题意中的 $k$，请返回排序后的数组。 测试样例： [2,1,4,3,6,5,8,7,10,9],10,2返回：[1,2,3,4,5,6,7,8,9,10] 算法思路：对前$k$ 个数字简历小根堆，然后对剩下的元素和对进行维护，每次从堆顶弹出就是最小元素，记录出堆顶的序列，最后从小到大的序列。时间复杂度是$klogk + (n-k)*logk $，相对于$n$来说$k$是比较小的，所以最后的时间复杂度可以表示为$nlogk$。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455#include&lt;iostream&gt;#include&lt;vector&gt;using namespace std;// 这里 c++ 和c 进行了混用void heapify(int * heap, int p, int k)&#123; int lc =2*p+1, rc =lc +1; // 如果heap 的存储是从0 开始，那么是要从这样写的 int min_v =p; if(lc &lt; k &amp;&amp; heap[min_v] &gt; heap[lc]) min_v =lc; if(rc &lt;k &amp;&amp; heap[min_v] &gt; heap[rc]) min_v =rc; if(min_v != p) &#123; swap(heap[min_v], heap[p]); heapify(heap, min_v, k); &#125;&#125;vector&lt;int&gt; sortElement(vector&lt;int&gt; &amp; nums, int n, int k)&#123; int heap[k]; for(int i =0; i&lt; k; i++) heap[i] =nums[i]; for( int i =k /2 -1; i&gt;=0; i--) heapify(heap, i, k); for(int i =k; i&lt; n; i++) &#123; nums[i-k] =heap[0]; heap[0] = nums[i]; heapify(heap, 0, k); &#125; // 对最后k 个元素进行排序 for(int i =n-k; i&lt;n; i ++) &#123; nums[i] =heap[0]; swap(heap[k-1], heap[0]); //最后是变小的 heapify(heap, 0, --k); &#125; return nums;&#125;int main()&#123; int n, k; //cin &gt;&gt;n&gt;&gt;k; n =10; k =2; vector&lt;int&gt; nums =&#123;2, 1, 4,3, 6, 5, 8, 7, 10, 9&#125;; vector&lt;int&gt; res =sortElement(nums, n, k); for(auto u: res) cout &lt;&lt; u&lt;&lt; " "; return 0;&#125; 参考文献 1). 算法动图效果2). 排序算法分类3). 桶排序算法详解]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>排序算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[那些年的算法题目（一）]]></title>
    <url>%2F2018%2F06%2F22%2F%E9%82%A3%E4%BA%9B%E5%B9%B4%E7%9A%84%E7%AE%97%E6%B3%95%E9%A2%98%E7%9B%AE%EF%BC%88%E4%B8%80%EF%BC%89%2F</url>
    <content type="text"><![CDATA[完全二叉树插入问题描述已知一个完全二叉树的结构，现在需要将一个节点插入到这颗完全二叉树的最后，使得它还是一个完全二叉树。第一种解法：如果该树为满二叉树或者左子树不为满二叉树，那么就进入左子树，否则进入右子树，递归进行。 二叉树(Binary Tree) 二叉树 任何一个节点的子节点数量不超过2 完全二叉树 所有叶子结点都在最后一层或倒数第二层。 最后一层的叶子结点在左边连续，倒数第二节的叶子结点在右侧连续。 满二叉树 所有叶子结点都在最后一层 结点的总数为 $2^n -1$ (n 为树的高度) 满二叉树是一种特殊的完全二叉树 平衡二叉树 也叫 AVL 树 它是一颗空树或左右两个子树的高度差的绝对值不超过1。 左右两个子树均为平衡二叉树。 二叉搜索树（Binary Search Tree） 也叫二叉查找树、二叉排序树 若子树不空，则子树上所有节点的值均小于或等于根节点的值。 若右子树不空，则右子树所有节点的值均大于或等于根节点的值。 左、右子树也分别为二叉排序树，或是一颗空树。 哈夫曼树 带权路径长度达到最小的二叉树，也叫做最优二叉树。 树的深度和高度：深度是从上往下数；高度是从下往上数 代码实现平滑过渡到本问题的代码实现。树的高度就是树的遍历。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253#include&lt;iostream&gt;using namespace std;typedef struct Node&#123; int value; struct Node *lchild, *rchild;&#125;Tree;int GetLeftDepth(Tree* root)&#123; Tree* pNode =root-&gt;lchild ; int depth =0; while(pNode != NULL) &#123; depth ++; pNode =pNode-&gt;lchild; &#125; return depth;&#125;int GetRightDepth(Tree* root)&#123; Tree* pNode =root-&gt;rchild; int depth =0 ; while(pNode != NULL) &#123; depth ++ ; pNode =pNode-&gt;rchild ; &#125; return depth;&#125;bool IsFullBinaryTree(Tree* root)&#123; return GetLeftDepth(root) == GetRightDepth(root) ;&#125;void insert(Tree* root, Tree * node)&#123; if (IsFullBinaryTree(root) || !IsFullBinaryTree(root-&gt;lchild))&#123; insert(root-&gt;lchild, node); return ; &#125; if (root-&gt;rchild ==NULL)&#123; root-&gt;rchild =node ; return ; &#125; insert(root-&gt;rchild, node) ;&#125;int main()&#123; Node* a = new Node(); a-&gt;value =1;&#125; inplace 去除连续的 0给定一个一维整数数组，不使用额外的空间，本地去掉数组中连续的0。 Tips： 前后两个指针，判断是否是连续的0。第三个指针标记新的数组，前者覆盖后者。 12345678910111213141516171819202122232425262728293031#include&lt;iostream&gt;using namespace std;int RemoveDuplicates(int* sortBuffer,int length)&#123; if(sortBuffer == NULL || length == 0) &#123; return false; &#125; int count = 0; for(int i = 1; i &lt; length; i++) &#123; if(sortBuffer[i] ==0 &amp;&amp; 0 == sortBuffer[i-1]) &#123; continue; &#125; else &#123; sortBuffer[count]=sortBuffer[i]; count++; &#125; &#125; return count; &#125;int main()&#123; int length =sizeof(array)/sizeof(int); &#125; 最大连续子数组和已知一个整数二维数组，求最大的子数组和(子数组的定义从左上角(x0,y0) 到右下角(x1,y1)的数组)先考虑一维整数数组的情况。 12345678910111213141516171819#include&lt;iostream&gt;using namespace std;int Max(int a, int b)&#123; return a&gt;b ?a:b;&#125;int FindGreatestSubarray(int *arr, int n)&#123; int sum =arr[0]; int max =arr[0]; for(int i =1; i&lt;n; i++)&#123; sum =Max(sum+arr[i], arr[i]); max =Max(sum, max) &#125; return max;&#125;int main()&#123; return 0;&#125; 本题目的要求是从二位的数组中求解最大的子矩阵。我们可以将其转化成一维数组的问题。如果是二维数组可以压缩为一维数组（我当时也是不懂这里）。如果最大子矩阵和原矩阵等高，就可以这样压缩。 不是很懂，感觉K 的值应该是具有某种限制，但是这个仍然是 0-k 这样的数字。 12345678910111213141516171819202122232425262728293031323334353637383940414243#include&lt;stdio.h&gt;#include&lt;iostream&gt;#include&lt;vector&gt;using namespace std;#define inf 0x3f3f3f3fint Max(int a, int b)&#123; return a&gt;b? a:b;&#125;// 求解一维数组的最大连续子数列int FindGreatestSubarray(int *arr, int n)&#123; int sum =arr[0]; int max =arr[0]; for(int i =1;i&lt;n;i++)&#123; sum =Max(sum+arr[i], arr[i]) if(sum &gt;=max)&#123; max =sum; &#125; &#125; return max;&#125;int GreatestMatrix(int[][] arr, int rows, int cols)&#123; int maxVal =- inf for(int i =0 ; i &lt;rows; i++)&#123; vector&lt;int&gt; temp(arr[i]); maxVal =Max(maxVal, FindGreatestSubarray(temp)); // 得到第一行的最大和 // 将行的n个元素加到上一行，然后计算最大和 for(int j =i+1; j&lt;rows; j++)&#123; for(int k =0;k&lt;cols ;k++)&#123; temp[k] =arr[j][k]; &#125; // 依次0~k行的最大和 maxVal =Max(maxVal, FindGreatestSubarray(temp)) &#125; &#125;&#125;int main()&#123;&#125; KMP（字符串高效查找)计算部分匹配表 前缀和后缀的定义：”前缀”指除了最后一个字符以外，一个字符串的全部头部组合；”后缀”指除了第一个字符以外，一个字符串的全部尾部组合。 如果给定的模式串是：“ABCDABD”，从左至右遍历整个模式串，其各个子串的前缀后缀分别如下表格所示： 123456789101112131415161718192021222324def kmp_match(s, p): m, n =len(s) ,len(p) cur =0 table = partial_table(p) while cur &lt;= m-n: for i in range(n): if s[i+cur] != p[i]: cur += max(i -table[i-1], 1) break else: return True return Falsedef partial_table(p): prefix =set() postfix =set() ret =[0] for i in range(1, len(p)): prefix.add(p[:i]) postfix =&#123; p[j:i+1] for j in range(1, i+1)&#125; ret.append(len((prefix &amp; postfix or &#123;''&#125;).pop())) # &amp;两个set求交集 return retprint(partial_table('ABCDABD'))print(kmp_match("BBC ABCDAB ABCDABCDABDE", "ABCDABD")) kmp 算法的时间复杂度是 $O(m +n)$ 其中 m, n 分别表示pattern 和string 的长度。 123456789101112131415161718192021222324252627282930313233#include&lt;iostream&gt;using namespace std;const int N =1e4+11;const int M =1e5+11;int n, m;char p[N], s[M];int nex[N];void get_next()&#123; for(int i =2, j =0; i&lt;=n ; i++) &#123; while(j &amp;&amp; p[i] != p[j +1]) j =nex[j]; if(p[i] ==p[j+1]) j ++; nex[i] =j; &#125;&#125;int main()&#123; cin &gt;&gt;n &gt;&gt; p +1&gt;&gt; m&gt;&gt; s +1; get_next(); // i 表示 s的遍历， j 表示p 的遍历 for(int i =1, j =0; i&lt;=m; i++) &#123; while(j &amp;&amp; s[i] != p[j+1]) j =nex[j]; if(s[i] ==p[j+1]) j ++; if(j ==n) &#123; printf("%d ", i-n); j =nex[j]; &#125; &#125; return 0;&#125; 二叉树的遍历关于树的遍历，可以有先序，中序和后序遍历方式。有递归和非递归两种方式。所以总共有 6种实现方式。 在python中二叉树的结构: 12345class BinNode(): def __init__(self, val): self.value =val self.lchild =None self.rchild =None 先序遍历(preOrder)第一种思路是递归实现，第二种思路借助栈的结构来实现。栈的大小空间为O(h)，h为二叉树高度；时间复杂度为O(n)，n是树的节点的个数。 递归的写法便于理解，循环的方式内存比较省。 循环的版本从变量命名和结构上都是可以优化的。 123456789101112131415161718192021222324# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution(object): def preorderTraversal(self, root): """ :type root: TreeNode :rtype: List[int] """ res, stack =[], [root] while stack: cur_node =stack.pop() # default pop(-1) 这个时间复杂度是 O(1)，如果是pop(0) 那么时间复杂度是 O(n) if cur_node: res.append(cur_node.val) stack.append(cur_node.right) # 注意这个顺序，这个是正确的姿态 stack.append(cur_node.left) return res c++ 递归版本 对于递归，应该先考虑是整个递归的过程主线，而不是跳出的条件（该问题比较细，然后很容易陷进去）1234567891011121314151617181920/** * Definition for a binary tree node. * struct TreeNode &#123; * int val; * TreeNode *left; * TreeNode *right; * TreeNode(int x) : val(x), left(NULL), right(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: vector&lt;int&gt; res; vector&lt;int&gt; preorderTraversal(TreeNode* root) &#123; if(!root) return res; res.push_back(root-&gt; val); if(root -&gt; left) preorderTraversal(root -&gt; left); if(root -&gt; right) preorderTraversal(root -&gt; right); return res; &#125;&#125;; c++非递归版本，注意使用的是栈这种结构，并且在push() 的时候是先push(root-&gt; right) 然后是 push(root -&gt; left) 这样能够保证先弹出来 root -&gt;left 结点。 1234567891011121314151617181920212223242526272829/** * Definition for a binary tree node. * struct TreeNode &#123; * int val; * TreeNode *left; * TreeNode *right; * TreeNode(int x) : val(x), left(NULL), right(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: // 如果是非递归版本，使用的是栈 这种结构 vector&lt;int&gt; preorderTraversal(TreeNode* root) &#123; vector&lt;int&gt; res; if(! root) return res; stack&lt;TreeNode *&gt; s; s.push(root); while(s.size()) &#123; auto t =s.top(); s.pop(); res.push_back(t-&gt;val); if(t -&gt;right) s.push(t-&gt;right); if(t-&gt;left) s.push(t-&gt;left); &#125; return res; &#125;&#125;; 中序遍历（inorder）递归和非递归两种实现思路。入栈的顺序是一样的，只是改变的遍历(print())的顺序. 中序遍历是先把所有的左子树遍历完之后，然后遍历根节点，然后遍历右子树。 12345678910111213141516171819202122# 递归def inOrder(self, root): if root ==None: return self.inOrder(root.lchild) print(root.val) self.inOrder(root.rchild)# 借助栈结构class Solution: def inorderTraversal(self, root): res, stack =[], [] while True: while root: stack.append(root) root =root.left if not stack: return res node =stack.pop() res.append(node.val) root =node.right c++ 中序遍历递归写法， 中序遍历是先一直遍历到子树的最左边，然后回溯遍历根节点，然后遍历右子树。 123456789101112131415161718192021/** * Definition for a binary tree node. * struct TreeNode &#123; * int val; * TreeNode *left; * TreeNode *right; * TreeNode(int x) : val(x), left(NULL), right(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: // 使用中序遍历，dfs vector&lt;int&gt; res; vector&lt;int&gt; inorderTraversal(TreeNode* root) &#123; if(! root) return res; if(root -&gt; left) inorderTraversal(root -&gt;left); res.push_back(root -&gt; val); if(root -&gt; right) inorderTraversal(root -&gt;right); return res; &#125;&#125;; c++ iteration 版本，使用一个栈来模拟过程 12345678910111213141516171819202122232425262728293031/** * Definition for a binary tree node. * struct TreeNode &#123; * int val; * TreeNode *left; * TreeNode *right; * TreeNode(int x) : val(x), left(NULL), right(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: vector&lt;int&gt; res; vector&lt;int&gt; inorderTraversal(TreeNode* root) &#123; stack&lt;TreeNode *&gt; s; TreeNode* p =root; while(p || s.size()) &#123; // bad case 如果最开始的时候， 根节点的左孩子是空，那么就没有放进去 while(p) &#123; s.push(p); p =p-&gt;left; &#125; auto t =s.top(); s.pop(); res.push_back(t-&gt;val); if(t-&gt; right) p=t-&gt;right; &#125; return res; &#125;&#125;; 后序遍历(post order)仍然是递归和非递归版本，非递归中使用两个stack,两个stack的后进先出等于一个先进先出。 12345678910111213141516171819202122232425262728293031# 递归def postOrder(self, root): if root == None: return self.postOrder(root.lchild) self.postOrder(root.rchild) print(root.val)# 借助栈结构# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution(object): def postorderTraversal(self, root): """ :type root: TreeNode :rtype: List[int] """ res, stack =[], [root] while stack: cur_node =stack.pop() if cur_node: res.append(cur_node.val) stack.append(cur_node.left) # 注意和先序遍历的顺序，还有最后的 reverse 操作 stack.append(cur_node.right) return res[::-1] c++ 后序遍历123456789101112131415161718192021/** * Definition for a binary tree node. * struct TreeNode &#123; * int val; * TreeNode *left; * TreeNode *right; * TreeNode(int x) : val(x), left(NULL), right(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: // 递归写法 vector&lt;int&gt; res; vector&lt;int&gt; postorderTraversal(TreeNode* root) &#123; if(! root) return res; if(root -&gt; left) postorderTraversal(root -&gt; left); if(root -&gt; right) postorderTraversal(root -&gt; right); res.push_back(root -&gt; val); return res; &#125;&#125;; 依赖于前序遍历而实现的后序遍历， cpp 迭代版本。最后使用 reverse 函数实现vector 的反转。 1234567891011121314151617181920212223242526272829303132/** * Definition for a binary tree node. * struct TreeNode &#123; * int val; * TreeNode *left; * TreeNode *right; * TreeNode(int x) : val(x), left(NULL), right(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: // 如果是前序可以使用 栈模拟，那么后序同样是可以使用栈来模拟的 vector&lt;int&gt; res; vector&lt;int&gt; postorderTraversal(TreeNode* root) &#123; if(!root ) return res; // 因为是后进先出，所以先要把跟结点放到stack 中 stack&lt;TreeNode *&gt; s; auto p =root; s.push(p); while(s.size()) &#123; auto t =s.top(); s.pop(); res.push_back(t-&gt;val); if(t-&gt; left) s.push(t-&gt;left); if(t-&gt; right) s.push(t-&gt;right); &#125; reverse(res.begin(), res.end()); return res; &#125;&#125;; 层序遍历使用到了队列的思想，先进先出。实际上，用的是Python中list.pop(0).注意默认是list.pop(-1),也就是默认弹出的是最后一个元素。 层序遍历使用 while 循环就比较好理解 1234567891011121314def levelOrder(self, root): if root ==None: return myQueue =[] node =root myQueue.append(node) while myQueue: # remove and return item at index (default last) node =myQueue.pop(0) print(node.val) if node.lchild != None: myQueue.append(node.lchild) if node.rchild != None: myQueue.append(node.rchild) c++实现的分层层次遍历 Binary Tree Level Order Traversal 12345678910111213141516171819202122232425262728293031323334353637/** * Definition for a binary tree node. * struct TreeNode &#123; * int val; * TreeNode *left; * TreeNode *right; * TreeNode(int x) : val(x), left(NULL), right(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: // 层序遍历，使用 nullptr 来标志一层的结束 vector&lt;vector&lt;int&gt;&gt; levelOrder(TreeNode* root) &#123; vector&lt;vector&lt;int&gt;&gt; res; queue&lt;TreeNode *&gt; q; q.push(root); q.push(nullptr); while(q.front()) &#123; auto t =q.front(); q.pop(); vector&lt;int&gt; level; // 有了 while 那么必然是一个循环，否则的话，使用if 就是ok的了 while(t) &#123; // 这个处理的是一个结点的, 如果想要处理一行的，那么确实需要使用循环的这样的方式 level.push_back(t-&gt;val); if(t-&gt;left) q.push(t-&gt;left); if(t -&gt;right) q.push(t-&gt;right); t =q.front(), q.pop(); &#125; q.push(nullptr); res.push_back(level); &#125; return res; &#125;&#125;; 从下往上层序打印二叉树 Binary Tree Level Order Traversal II 123456789101112131415161718192021222324252627282930313233343536/** * Definition for a binary tree node. * struct TreeNode &#123; * int val; * TreeNode *left; * TreeNode *right; * TreeNode(int x) : val(x), left(NULL), right(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: // 和上一道题目类似，就是在最后的时候，使用二维数组的 reverse，然后得到最后的结果 vector&lt;vector&lt;int&gt;&gt; levelOrderBottom(TreeNode* root) &#123; vector&lt;vector&lt;int&gt;&gt; res; queue&lt;TreeNode *&gt; q; q.push(root); q.push(nullptr); while(q.front()) &#123; auto t =q.front(); q.pop(); vector&lt;int&gt; level; while(t) &#123; level.push_back(t-&gt;val); if(t-&gt;left) q.push(t-&gt;left); if(t-&gt;right) q.push( t-&gt;right); t =q.front(), q.pop(); &#125; q.push(nullptr); res.push_back(level); &#125; reverse(res.begin(), res.end()); return res; &#125;&#125;; (Binary Tree Zigzag Level Order Traversal)[https://leetcode.com/problems/binary-tree-zigzag-level-order-traversal/submissions/] 1234567891011121314151617181920212223242526272829303132333435363738394041/** * Definition for a binary tree node. * struct TreeNode &#123; * int val; * TreeNode *left; * TreeNode *right; * TreeNode(int x) : val(x), left(NULL), right(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: // 同样的方式，只不过在偶数的时候 reverse一下 vector&lt;vector&lt;int&gt;&gt; zigzagLevelOrder(TreeNode* root) &#123; vector&lt;vector&lt;int&gt;&gt; res; queue&lt;TreeNode *&gt; q; q.push(root); q.push(nullptr); bool is_even =false; while(q.front()) &#123; auto t =q.front(); q.pop(); vector&lt;int&gt; level; while(t) &#123; level.push_back(t-&gt;val); if(t-&gt;left) q.push(t-&gt;left); if(t-&gt;right) q.push(t-&gt;right); t =q.front(), q.pop(); &#125; if(is_even) reverse(level.begin(), level.end()); is_even = !is_even; q.push(nullptr); res.push_back(level); &#125; return res; &#125;&#125;; 旋转数组找最小值把一个数组最开始的若干个元素搬到数组的末尾，我们称之为数组的旋转。 输入一个非递减排序的数组的一个旋转，输出旋转数组的最小元素。 例如数组{3,4,5,1,2}为{1,2,3,4,5}的一个旋转，该数组的最小值为1。 NOTE：给出的所有元素都大于0，若数组大小为0，请返回0。 直接是二分的思路，时间复杂度是 $O(logn)$1234567891011121314151617181920class Solution &#123;public: // 这里说 no duplicates，所以不用处理一个条件 // 比较 nums[0] 和 nums[mid] 关系， if nums[mid] &gt; nums[0] l = mid +1 int findMin(vector&lt;int&gt;&amp; nums) &#123; int n = nums.size(); if(n ==1) return nums[0]; if(nums[0] &lt; nums[n-1]) return nums[0]; int l =0, r =n -1; while(l &lt; r) &#123; int mid =l +r &gt;&gt;1; // 如果 mid 和0 表示一个位置，那么该解也不是最小值，虽然没有重复的元素，但这个等于号是应该在这里的 if(nums[mid] &gt;= nums[0]) l =mid +1; else r =mid ; &#125; return nums[l]; &#125;&#125;; 单链表反转c++ 版本 循环法 1234567891011121314151617181920212223/** * Definition for singly-linked list. * struct ListNode &#123; * int val; * ListNode *next; * ListNode(int x) : val(x), next(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: // 对于链表操作修改的list，需要使用到三个指针， pre, cur, next 所以空间复杂度是 O(1), 空间复杂度是 $O(n)$ ListNode* reverseList(ListNode* head) &#123; ListNode * pre =nullptr; ListNode * cur = head; while(cur) &#123; ListNode * next =cur -&gt;next; cur -&gt;next = pre; pre =cur, cur = next; &#125; return pre; &#125;&#125;; python 版本 ，迭代版本 1234567891011121314151617181920# Definition for singly-linked list.# class ListNode(object):# def __init__(self, x):# self.val = x# self.next = Noneclass Solution(object): def reverseList(self, head): """ :type head: ListNode :rtype: ListNode """ pre =None cur =head while cur: nex =cur.next cur.next = pre pre =cur cur =nex return pre 还可以使用递归的思路进行求解，具体参考这里， 遍历所有的结点，时间复杂度是 $O(n)$， 需要 $n$ 个栈空间，空间复杂度是$O(n)$。 Minimum Window Substring Given a string S and a string T, find the minimum window in S which will contain all the characters in T in complexity O(n). 12Input: S = &quot;ADOBECODEBANC&quot;, T = &quot;ABC&quot;Output: &quot;BANC&quot; c++ 版本 123456789101112131415161718192021222324252627class Solution &#123;public: // 首先对于 t 进行 hash， 然后再s 中使用双指针 i j ， i &gt;=j ，时间复杂度是 $O(n)$ // 空间是 $O(n)$ string minWindow(string s, string t) &#123; unordered_map&lt;char, int&gt; hash; int cnt =0; // unique character for(auto ch : t) &#123; if(!hash[ch]) cnt ++; hash[ch] ++; &#125; string res =""; for(int i=0, j =0, c =0; i&lt; s.size(); i ++) &#123; // 往里面加 if(hash[s[i]] == 1) c ++; hash[s[i]] --; while(c ==cnt &amp;&amp; hash[s[j]] &lt;0) hash[s[j++]] ++; if(c ==cnt) &#123; if(res.empty() || res.size() &gt; i-j +1) res =s.substr(j, i -j +1); &#125; &#125; return res; &#125;&#125;; python 实现，也是双指针 + dictionary， 和上面的c++ 代码中有的变量名表示的含义不同。 123456789101112131415161718192021222324252627class Solution(object): def minWindow(self, s, t): """ :type s: str :type t: str :rtype: str """ from collections import Counter dic , cnt =Counter(t), len(t) i , start, end =0, 0, 0 # muberate(string, start_index) 从1 开始计数 for j, ch in enumerate(s, 1): cnt -= dic[ch] &gt;0 dic[ch] -=1 if not cnt: while dic[s[i]] &lt;0: dic[s[i]] += 1 i +=1 if end ==0 or j -i &lt; end -start: start, end =i, j dic[s[i]] +=1 i +=1 cnt +=1 return s[start: end]]]></content>
      <categories>
        <category>算法</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[机器学习中的常见的评价指标]]></title>
    <url>%2F2018%2F06%2F13%2Froc_and_unbalanced_data%2F</url>
    <content type="text"><![CDATA[介绍机器学习中分类模型常用的评价指标：AUC、Precision-Recall, ACC和KS值。 评估指标和代价函数的关系 代价函数，又称Cost function，loss function objective function。一般用在训练过程中，用来定义预测值和真实值之间的距离（也就是衡量模型在训练集上的性能），作为模型调整参数的反馈。代价函数越小，模型性能越好。 评判指标，一般用于训练和测试过程中，用于评估模型好坏。评判指标越大（或越小），模型越好。 两者的相同点：本质上代价函数和评判指标都是一家人，只他们的应用场景不同，分工不同。代价函数是用来优化模型参数的，评价指标是用来评判模型好坏的。不同点： 作为代价函数所具备的条件：函数光滑且可导：可用梯度下降求解极值；函数为凸函数：可用梯度下降求解最优解 我们经常使用的分类器评判指标 AUC 就不能直接被优化，因此我们常采用交叉熵来代替 AUC 进行优化。 一般情况下，交叉熵越小，AUC 就会越大。 ROC曲线和AUC值ROC全称是“受试者工作特征”（Receiver Operating Characteristic）。ROC曲线的面积就是AUC（Area Under the Curve）。AUC用于衡量“二分类问题”机器学习算法性能（泛化能力）。说到这里不得不提及就是经常使用的符号，TP(True Positive), FP(False, Positive), TN(True Negative),FN(False Negative)。他们是根据真实数据类别和模型预测类别进行的排列组合。 （1）定义 ROC 曲线（接收者操作特征曲线）是一种显示分类模型在所有分类阈值下的效果的图表。该曲线绘制了以下两个参数：真正例率 和 假正例率。真正例率 (TPR) 是召回率的同义词，数学表达为：$$T P R = \frac { T P } { T P + F N }$$假正例率 (FPR) 的定义如下：$$F P R = \frac { F P } { F P + T N }$$ TPR 是所有真实样本中被预测为真实样本的比例；FPR是所有虚假样本中被预测为真实样本的概率。可以使用混淆矩阵表示上述的关系： 图中第一类错误和第二类措施是假设检验中的概念。第一类错误是原来假设是错误的，但是却被认为是正确的；第二类错误是原来的假设是正确的，但是被认为是错误的。 （2）ROC 曲线是如何绘制的 采用不同分类阈值时的 TPR 与 FPR。降低分类阈值会导致将更多样本归为正类别，从而增加假正例（FP）和真正例（TP）的个数，可以理解为降低了被认为正确的标准，数量自然就增多了。下图显示了一个典型的 ROC 曲线。注意观察图中TPR 和 FPR是呈正相关的，验证了上述的结论。 为了计算 ROC 曲线上的点，我们可以使用不同的分类阈值多次评估逻辑回归模型，但这样做效率非常低。幸运的是，有一种基于排序的高效算法可以为我们提供此类信息，这种算法称为曲线下面积。最理想的目标：tpr =1， fpr =0，即图中的 (0, 1) 点， 故 ROC 曲线越靠近(0, 1 ) 点，分类效果越好。关于该图像还有一点，如果你的曲线拟合对角线（图中虚线），那么相当于随机猜测。 阈值设定的两种方法： 1). 等距离阈值，range(0, 1, 100) 生成了100 个阈值，那么对应着p-r 中的100 个点2). 二分类结果是模型的概率值，对概率值进行排序，依次使用这些概率值作为阈值，也是可以得到不同的点的坐标。 （3）基于sklearn 绘制ROC曲线 12345678910111213gbc = GradientBoostingClassifier()gbc.fit(x_train, y_train)resu = gbc.predict(x_test) #进行预测y_pred_gbc = gbc.predict_proba(x_test)[:,1] ###这玩意就是预测概率的fpr, tpr, threshold = roc_curve(y_test, y_pred_gbc) ###画图的时候要用预测的概率，而不是你的预测的值plt.plot(fpr, tpr, 'b', label='AUC = %0.2f' % rocauc)#生成ROC曲线plt.legend(loc='lower right')plt.plot([0, 1], [0, 1], 'r--')plt.xlim([0, 1])plt.ylim([0, 1])plt.ylabel('真正率')plt.xlabel('假正率')plt.show() 一般来说，如果ROC曲线是光滑的，那么基本可以判断没有太大的overfitting（比如下图中0.2到0.4可能就有问题，但是样本太少了），这个时候调模型可以只看AUC，面积越大一般认为模型越好。 （4）AUC 数值 Alternatively, it can be shown that ROC AUC score is equivalent to calculating the rank correlation between predictions and targets. From an interpretation standpoint, it is more useful because it tells us that this metric shows how good at ranking predictions your model is. It tells you what is the probability that a randomly chosen positive instance is ranked higher than a randomly chosen negative instance. 对于AUC 有两类解释： 从ROC 线下面积理解：AUC量化了ROC曲线表达的分类能力； 从概率角度理解：AUC就是从所有正样本（1）中随机选取一个样本， 从所有负样本（0）样本中随机选取一个样本，然后根据你的分类器对两个随机样本进行预测，把1样本预测为1的概率为$p_1$，把0样本预测为1的概率为 $p_0$，$p1&gt;p0 $的概率就等于AUC。所以AUC 反应的是分类器的排序能力。例如0.7的AUC，其含义可以大概理解为：给定一个正样本和一个负样本，在70\%的情况下，模型对正样本的打分高于对负样本的打分。可以看出在这个解释下，我们关心的只有正负样本之间的分数高低，而具体的分值则无关紧要。 AUC 的优点：不关注具体得分，只关注排序结果，这使得它特别适用于排序问题的效果评估，例如推荐排序的评估，CTR的线下评估 AUC的缺点： AUC只关注正负样本之间的排序，并不关心正样本内部，或者负样本内部的排序。这也体现了AUC的本质：任意个正样本的概率都大于负样本的概率的能力。 （5）应用场景 You should use it when you ultimately care about ranking predictions and not necessarily about outputting well-calibrated probabilitiesYou should not use it when your data is heavily imbalanced. The intuition is the following: false positive rate for highly imbalanced datasets is pulled down due to a large number of true negatives.You should use it when you care equally about positive and negative classes. Precision-Recall在分类模型评价中还有一个曲线：PR曲线。同样使用混淆矩阵中的参数定义如下： \begin{equation}\text { Precision }=\frac{\mathrm{TP}}{\mathrm{TP}+\mathrm{FP}}\end{equation}准确率（precision）表示预测的样本中正样本的比例。\begin{equation}\text { Recall }=\frac{\mathrm{TP}}{\mathrm{TP}+\mathrm{FN}}\end{equation}召回（recall）表示真实为正样本被预测为正样本的比例。 AUC曲线中有 ROC值，类似的，P-R曲线中也有F 值，定义$F_{\beta}$分数为$$F_{\beta}=\left(1+\beta^{2}\right) \cdot \frac{\text {precision} \cdot \text {recall}}{\left(\beta^{2} \cdot \text {precision}\right)+\text {recall}}$$$F_β$的物理意义就是将准确率和召回率这两个分值合并为一个分值，在合并的过程中，召回率的权重是准确率的 $β $倍。F1分数认为召回率和准确率同等重要，F2分数认为召回率的重要程度是准确率的2倍，而 $F_{0.5}$分数认为召回率的重要程度是准确率的一半。 实际中使$F_1$更加常见，即当$ \beta =1$时候。$$F_{\beta}= \frac{ 2 \cdot \text{Precision} \cdot \text{Recall}}{ \text{Precision} + \text{Recall}}$$ 同样使用不同的阀值，统计出精确率和召回率。如下图： 准确率应用场景： 如果做疾病监测、反欺诈，这种情况下正负样本严重失衡，则是保准确率的条件下，提升召回。 召回率应用场景： 如果是做搜索，那么需要保证召回的条件下，提升准确率。因为即使召回的不是正样本，那么也没有很大的影响。 如果是精准营销领域的商品推荐模型，模型目的是尽量将商品推荐给感兴趣的用户，若用户对推荐的商品不感兴趣，也不会有很大损失，因此此时TPR相对FPR更重要。 使用场景 when you want to communicate precision/recall decision to other stakeholders when you want to choose the threshold that fits the business problem. when your data is heavily imbalanced. As mentioned before, it was discussed extensively in this article by Takaya Saito and Marc Rehmsmeier. The intuition is the following: since PR AUC focuses mainly on the positive class (PPV and TPR) it cares less about the frequent negative class. when you care more about positive than negative class. If you care more about the positive class and hence PPV and TPR you should go with Precision-Recall curve and PR AUC (average precision). PR和ROC联系和区别（1）联系1). 对于一个给定的的数据集，ROC空间和PR空间存在一一对应的关系，因为二者包含完全一致的混淆矩阵。我们可以将ROC曲线转化为PR曲线，反之亦然。2). 都有一个曲线和一个数值进行衡量：F1对于PRC就好象AUC对于ROC一样。（2）区别1). 最大的区别：在正负样本分布得极不均匀(highly skewed datasets)的情况下，PRC比ROC能更有效地反应分类器的好坏。2). 图像上区别 PR 曲线是以 Recall 为横轴，Precision 为纵轴；而 ROC曲线则是以 FPR 为横轴，TPR 为纵轴。 “曲线A优于曲线B” 是指曲线 B 的所有部分与曲线 A 重合或在曲线 A 之下。而在ROC空间，ROC曲线越凸向左上方向效果越好。与ROC曲线左上凸不同的是，PR曲线是右上凸效果越好。3). 代码实现上 123456import numpy as npimport matplotlib.pyplot as pltx = np.arange(0, 5, 0.1)y = np.sin(x)plt.plot(x, y) ROC 曲线123456fpr, tpr, thresholds = roc_curve(testy, probs)pyplot.plot([0, 1], [0, 1], linestyle='--')pyplot.plot(fpr, tpr, marker='.')pyplot.show()auc_score = roc_auc_score(testy, probs)print('AUC: %.3f' % auc_score) P-R曲线1234pyplot.plot([0, 1], [0.5, 0.5], linestyle='--')pyplot.plot(recall, precision, marker='.')pyplot.show()print('AUC: %.3f' % auc_score) 当正负样本的分布发生变化时，ROC曲线的形状能够基本保持不变，而P-R曲线的形状一般会发生较剧烈的变化。ROC能够尽量降低不同测试集带来的干扰，更加客观的衡量模型本身的性能。如果在实际应用中更加关注正样本的表现，那么P-R曲线更加适合。 总体来说ROC曲线的适用场景更多，被广泛用于排序、推荐、广告等领域。但需要注意的是，选择P-R曲线还是ROC曲线是因实际问题而异的，如果研究者希望更多地看到模型在特定数据集上的表现，P-R曲线则能够更直观地反映其性能。 其他指标（1） ACC ACC 是在分类中熟悉的，但是不是那么常用。缺点主要有以下两点： 正负样本失衡时候，比较难正确衡量模型的优劣。 很多机器学习模型分类的结果都是概率，如果要计算ACC，那么需要手动设置阈值，该超参数的设置很大程度上影响了 accuracy的计算。 （2）logloss logloss衡量的是预测概率分布和真实概率分布的差异性，取值越小越好。与AUC不同，logloss对预测概率敏感。 （3）KS曲线 KS值是在模型中用于区分预测正负样本分隔程度的评价指标，一般应用于金融风控领域。KS曲线以阈值作为横坐标，以FPR和TPR作为纵坐标，KS曲线则为TPR-FPR，KS曲线的最大值通常为KS值。与ROC曲线相似，通过改变不同阈值可以得到曲线。 如何求解KS值呢？我们知道，当阈值减小时，TPR和FPR会同时减小，当阈值增大时，TPR和FPR会同时增大。而在实际工程中，我们希望TPR更大一些，FPR更小一些，即TPR-FPR越大越好，即ks值越大越好。 KS值的取值范围是[0，1]。通常来说，值越大，模型区分正负样本的能力越强（一般0.3以上，说明模型的效果比较好）。 （4）马修斯相关系数 （Matthews Correlation Coefficient，MCC） 可以看出该指标也是根据混淆矩阵计算得来的。 \begin{equation}M C C=\frac{t p t n-f p f n}{(t p+f p)(t p+f n)(t n+f p)(t n+f n)}\end{equation} 如果接近+1 ，表示完美屁屁额；如果接近0，那么相当于随机猜测；如果接近于-1，那么说明预测和观察之间的完全不一样。 计算123from sklearn.metrics import matthews_corrcoefy_pred_class = y_pred_pos &gt; thresholdmatthews_corrcoef(y_true, y_pred_class) 使用场景： When working on imbalanced problems, When you want to have something easily interpretable. 结论If you have an imbalanced dataset accuracy can give you false assumptions regarding the classifier’s performance, it’s better to rely on precision and recall, in the same way a Precision-Recall curve is better to calibrate the probability threshold in an imbalanced class scenario as a ROC curve. ROC Curves: summarise the trade-off between the true positive rate and false positive rate for a predictive model using different probability thresholds. Precision-Recall curves: summarise the trade-off between the true positive rate and the positive predictive value for a predictive model using different probability thresholds. ROC curves are appropriate when the observations are balanced between each class, whereas precision-recall curves are appropriate for imbalanced datasets. In both cases the area under the curve (AUC) can be used as a summary of the model performance. Why a ROC curve cannot measure well? The Receiver Operating Characteristic (ROC) curves plot FPR vs. TPR as shown below. Because TPR only depends on positives, ROC curves do not measure the effects of negatives. The area under the ROC curve (AUC) assesses overall classification performance . AUC does not place more emphasis on one class over the other, so it does not reflect the minority class well. Davis and Goadrich in this paper propose that Precision-Recall (PR) curves will be more informative than ROC when dealing with highly skewed datasets. The PR curves plot precision vs. recall (FPR). Because Precision is directly influenced by class imbalance so the Precision-recall curves are better to highlight differences between models for highly imbalanced data sets. When you compare different models with imbalanced settings, the area under the Precision-Recall curve will be more sensitive than the area under the ROC curve. 参考文献 Evaluation Metrics, ROC-Curves and imbalanced datasets 24 Evaluation Metrics for Binary Classification (And When to Use Them)]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>roc</tag>
        <tag>precision-recall</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[对抗生成网络实验对比]]></title>
    <url>%2F2018%2F06%2F05%2F%E5%AF%B9%E6%8A%97%E6%80%A7%E7%94%9F%E6%88%90%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[GAN模型是典型的隐式无监督生成模型，建模过程中没有利用到数据的语义标签。但在实际应用中生成模型的可控性至关重要，根据标签生成对可控有样本更具实际应用价值。图像生成模型被广泛应用于数据增强、风格转换和数据补全领域，需要可控且语义完备的生成模型。条件GAN模型在GAN模型建模思路的基础上，将语义标签加入了建模过程，将无监督生成模型转变为有监督的条件生成模型。条件GAN模型具体包括Conditional GAN模型、Semi-GAN模型和AC-GAN模型。 实验模型介绍（1）Conditional GAN模型GAN模型中判别器D对输入的数据样本的来源进行判别，是典型的判别模型流程。如果在GAN框架中加入有监督信息来辅助训练，如图像的类别信息来辅助判别器D进行判别，则会帮助生成更加真实的样本。其中最初的尝试方式是Conditional GAN模型，结构如图所示。Conditional GAN模型在生成器G和判别器D输入中都加入了标签信息，试图让生成器G学习到从数据标签y到样本x的映射，让判别器D学习对样本x和标签y的组合进行判别。与GAN模型相比，Conditional GAN模型增加了标签信息的输入将模型转变为条件生成模型，在一定程度上提高了模型的稳定性。Conditional GAN模型训练过程中，判别器D对样本x和标签y的类别组合进行训练，并没有输入样本x和标签y的类别错误组合进行训练，因此模型并没有学习样本x和标签y的联合分布。（2）Semi-GAN模型Conditional GAN模型利用了标签信息进行建模，但没有对标签语义的信息进行表征，导致模型能够学到的信息有限。在生成模型过程中，如果判别器D能够明确指出生成样本的类别错误，则可为生成器G提供更加精确的梯度信息，最终能生成更加真实的样本。Semi-GAN基于此思路进行改进，具体结构如图9所示。Semi-GAN模型在Conditional GAN模型的基础上，对判别器D的分类输出进行细化加入了半监督学习过程。（3）AC-GAN模型与Conditional GAN模型相比，Semi-GAN模型中判别器D能够判别真实样本的来源，增强了判别器D的判别能力。但研究表明过强的判别信息会影响生产样本的质量，具体原因为Semi-GAN模型的建模过程为半监督分类过程，目标优化函数为无监督分类和有监督分类目标函数之和。若判别器D的监督分类信息过强，则会削弱判别器D对样本来源的判别能力。Conditional GAN模型能够生成指定类别的样本，Semi-GAN模型能够判别样本的类别信息。AC-GAN模型将以上两个模型思路进行整合，得到够进行条件生成的生成器G，和能够判别样本类别和来源信息的判别器D。AC-GAN模型的结构如图10所示。AC-GAN模型在Conditional GAN的基础上，让判别器D在判别样本来源的同时，让样本进行分类。此时的判别器D的输出分为样本来源信息LS和样本分类LC信息。 不同模型比较下面用表格的方式对比在实验中使用的模型的目标函数(ps,图画比较丑，之后再修改) Name Paper Link Value Function GAN Arxiv DCGAN Arxiv Semi-GAN Arxiv 和GAN 模型相同 CGAN Arxiv ACGAN Arxiv our model Arxiv 数据集介绍在常用于图像生成的图像数据集中，大部分数据的标签类型为离散类型。其中MNIST和Fashion-MNIST为常用的灰度图像数据，每类样本分布较为独立，常用于进行图像分类和样本生成的实验；SVHN和CIFAR10为彩色数据集图像像素分布较为复杂，其中CIFAR10常用来检验分类网络性能的评价数据集；CelebA为大规模的人脸识别和属性分类数据集，每幅人脸图像包括40个属性标签；ImageNet为图像分类和识别数据集，数据集类别分布比较复杂具体包括自然图像和人为图像。UnityEyes为人眼视觉合成数据集，数据集标签包括瞳孔标签和视觉方向标签，其中视觉方向标签为连续的语言标签。常见的离散标签图像数据集的样例: 我们的模型在原始GAN模型中，目标函数定义为生成器G和判别器D的博弈过程，定义V(G;D)为模型的目标函数，由生成器G和判别器D组成。语义匹配目标函数FMloss。基于语义匹配的条件生成网络模型的生成器G和判别器D的目标函数分别为：如上式，LS为样本来源，LC为分类结果。判别器D目标为最大化LC+ LS + FMloss，其试图对输入样本进行分类，并通过来源和语义匹配区分生成样本和原始样本。生成器G的目的是最大化LC − LS −FMloss，其试图通过样本分类结果、样本来源和语义匹配结果来欺骗判别器D。LS来源损失与原始GAN模型相同，LC为语义标签分类损失，在类别分类中使用交叉信息熵，在数值回归中则使用均方差回归。 生成结果对比MNIST数据集生成结果Fashion-MNIST数据集生成结果SVHN数据集生成结果CIFAR10数据集生成结果CelebA数据集生成结果UnityEyes数据集生成结果]]></content>
      <categories>
        <category>Deep learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[XGBoost]]></title>
    <url>%2F2018%2F06%2F05%2FXGBoost%2F</url>
    <content type="text"><![CDATA[介绍 xgboost 原理, xgboost+ lr 模型和 决策树。 提升树简介监督学习的要素 model and parameters The parameters are the undetermined part that we need to learn from data. In linear regression problems, the parameters are the coefficients $\theta$. 对于监督学习中，学习的是模型的参数 $\theta$。不论是回归还是分类模型，基本假设是参数表示数据的分布。 目标函数= 训练loss function + 正则项$$obj(\theta) =L(\theta) + \Omega(\theta)$$ training loss measures how well model fit on training data, while regularization, measures complexity of model其中 $L(\theta)$ 是loss function，表示模型的预测性； $\Omega(\theta) $表示模型的复杂度。 这个的选择就是bias-variance tradeoff 的问题。 为什么要simple models? simple models tends to have smaller variance in future predictions, making prediction stable。一般来说简单的参数，得到的结果是more stable。 决策树集合（Decision Tree Ensembles） To begin with, let us first learn about the model choice of XGBoost: decision tree ensembles. The tree ensemble model consists of a set of classification and regression trees (CART). xgboost 就是一系列决策树的集合。这些决策树是由一系列的 CART 树组成的。 A CART is a bit different from decision trees, in which the leaf only contains decision values. In CART, a real score is associated with each of the leaves, which gives us richer interpretations that go beyond classification. This also allows for a principled, unified approach to optimization, as we will see in a later part of this tutorial. 通常情况下，一棵树是不足以进行预测，所以使用的是决策树的集合。对于某一个样本的结果是多个树的总和。 决策树就是分段函数，所以一个决策树是可以使用 $f(x)$ 进行表示的。$$\hat{y} _{i}=\sum _{k=1} ^{K} f _{k}(x _{i}), f _{k} \in \mathcal{F}$$其中 $K$ 是树的个数（函数的个数）, $f $ 表示某个函数, $ \mathcal{F}$所有的CARTs 可能的总的空间。 于是新的目标函数可以： $$obj(\theta) =\sum _{i} ^{n} l(y _{i}, \hat{y} _{i})+\sum _{k=1} ^{K} \Omega(f _{k})$$ Now here comes a trick question: what is the model used in random forests? Tree ensembles! So random forests and boosted trees are really the same models; the difference arises from how we train them. 但是一个问题出现了，boosted trees 和 random forest 都是相同的模型，其区别在于如何训练。 提升树这个小结讲的是如何进行训练的问题。第一小节中陈述的是监督学习中通用模型是通过梯度下降进行训练的，在这个过程中学习到了参数。树模型不能使用梯度下降进行训练，需要把树看做整体。 加法学习 It is intractable to learn all the trees at once. Instead, we use an additive strategy: fix what we have learned, and add one new tree at a time. 它是一种启发式学习，优化的目标不是全部的树。而是每次在上一步的基础上进行优化，每次增加一棵树。 但是还有一个问题，那么如何去选择哪个树呢？当然是使得最后loss 下降的那颗树。 $$\begin{split}obj ^{(t)} &amp;= \sum _{i=1} ^{n} l(y _{i}, \hat{y} _{i} ^{(t)})+\sum _{i=1} ^{t} \Omega(f _{i}) \\&amp; =\sum _{i=1} ^{n} l(y _{i}, \hat{y} _{i} ^{(t-1)}+f _{t}(x _{i}))+\Omega(f _{t})+\text { constant }\end{split}$$ 这个时候，使用均方误差作为损失函数，可以得到： \begin{split}obj _{(t)} &amp; =\sum _{i=1} ^{n} (y _{i}, \hat{y} _{i} ^{(t-1)}+f _{t}(x _{i})) ^2+ \sum _{i=1} ^t \Omega(f _{i}) \\&amp;=\sum _{i=1} ^{n}[2(\hat{y} _{i} ^{(t-1)}-y _{i}) f _{t}(x _{i})+f _{t}(x _{i}) ^{2}]+\Omega(f _{t})+\text { constant }\end{split} 当使用均方误差的时候，很nice 得到了比较简洁的形式（残差形式），但是如果是其他的损失函数，就没有这种组合成 $f(x), f^2(x)$ 这种形式，所以这个时候使用到的是talor 展开式。所以更加通用的形式如下：$$\mathrm{obj} ^{(t)}=\sum _{i=1} ^{n}[l(y _{i}, \hat{y} _{i} ^{(t-1)})+g _{i} f _{t}(x _{i})+\frac{1}{2} h _{i} f _{t} ^{2}(x _{i})]+\Omega(f _{t})+\mathrm{constant}$$ 其中 $g_i$和 $h_i$ 分别定义为如下形式： $$\begin{split}g _{i} &amp;=\partial _{\hat{y} _{i} ^{(t-1)}} l(y _{i}, \hat{y} _{i} ^{(t-1)}) \\h _{i} &amp;=\partial _{\hat{y} _{i} ^{(t-1)}} ^{2} l(y _{i}, \hat{y} _{i} ^{(t-1)})\end{split}$$ 最后去掉所有的常量，在 $t$时刻的目标函数是如下形式：$$\sum _{i=1} ^{n}[g _{i} f _{t}(x _{i})+\frac{1}{2} h _{i} f _{t} ^{2}(x _{i})]+\Omega(f _{t})$$ 有了上述通用的形式，那么就可以定义任意的损失函数，只要该损失函数有一阶导数和二阶导数即可， so convinent。 模型的复杂度 this is not the only possible definition对于模型的复杂度有多种定义方式，下面的这种是一种常见的并且在实际应用中效果很好的。 $$\Omega(f)=\gamma T+\frac{1}{2} \lambda \sum_{j=1}^{T} w_{j} ^{2}$$ 其中$T$ 表示number of leaves， $w$表示L2 norm of leaf scores Simpler model means less overfitting and lower variance. 减少方差和过拟合 The Structure Score $$\begin{split}obj ^{(t)} \approx \sum _{i=1} ^{n}[g _{i} w _{q(x _{i})}+\frac{1}{2} h _{i} w _{q(x _{i})} ^{2}]+\gamma T+\frac{1}{2} \lambda \sum _{j=1} ^{T} w _{j} ^{2}=\sum _{j=1} ^{T}[(\sum _{i \in I _{j}} g _{i}) w _{j}+\frac{1}{2}(\sum _{i \in I _{j}} h _{i}+\lambda) w _{j} ^{2}]+\gamma T\end{split}$$ 公式出现 $\approx$， 这个是因为使用一阶导数和二阶导数去近似地代替原始的函数，这个在计算的时候比较方便~。这个时候可以看成是一个二次方程，那么就可以求解最值了。 学习树的结构 （additive training） We can not use methods such as SGD, to find f (since they are trees, instead of just numerical vectors)需要使用 Additive Training (boosting)， 以树为单位进行加法和减法。 $$\text {Gain}=\frac{1}{2}[\frac{G _{L} ^{2}}{H _{L}+\lambda}+\frac{G _{R} ^{2}}{H _{R}+\lambda}-\frac{(G _{L}+G _{R}) ^{2}}{H _{L}+H _{R}+\lambda}]-\gamma$$ 这个时候令 $G _{j}=\sum _{i \in I _{j}} g _{i}$ ， $H _{j}=\sum _{i \in I _{i}} h _{i}$， 所以可以化简成以下的形式：$$\mathrm{obj} ^{(t)}=\sum _{j=1} ^{T}[G _{j} w _{j}+\frac{1}{2}(H _{j}+\lambda) w _{j} ^{2}]+\gamma T$$ 二次方程是可以求解到最值的，那么可以得到： $$\begin{split}w _{j} ^{} &amp;=-\frac{G _{j}}{H _{j}+\lambda} \\obj ^{} &amp;=-\frac{1}{2} \sum _{j=1} ^{T} \frac{G _{j} ^{2}}{H _{j}+\lambda}+\gamma T\end{split}$$进行分裂时的策略， 如果分裂的左右子树是能够有信息的增加，那么就进行分裂，否则的话不进行分裂，如果式子前半部小于 $\gamma$的话，那么就进行剪枝。 $$obj=-\sum _{j} \frac{G _{j} ^{2}}{H _{j}+\lambda}+3 \gamma$$ This score is like the impurity measure in a decision tree, except that it also takes the model complexity into account. 其中，最后的数值越小，那么结构也是越好。（从这个角度出发是不是和基尼系数相似） 分裂点的寻找 For each node, enumerate over all features for each feature, sorted the instances by feature value use a linear scan to decide the best split along that feature take the best split solution along all the features 对于每一个结点，都遍历所有的特征 对于每一个特征，样本按照特征的值进行排序 使用线性扫描的方式，决定最好的分裂点 在所有的特征中，按照最优的分裂点进行分裂 时间复杂度分析 Time Complexity growing a tree of depth$ K$ It is $O(n d K log n) $: or each level, need $O(n log n) $time to sort, there are $d$ features, and we need to do it for $K$ level This can be further optimized (e.g. use approximation or caching the sorted features) Can scale to very large dataset （1） Basic Exact Greedy Algorithm 暴力解法就是exact greedy algorithm。做法：在所有可能的分裂点上遍历所有的features 的值。效果虽然好，但是时间效率不高。（2）approximate algorithmexact greedy algorithm的缺点除了时间成本，还有内存上方面的考虑。当数据无法一次性读入内存，那么就不能使用。approximate algorithm： - 根据feature 的分布选择候选分裂点 - 根据分裂点划分连续数值的feature，分成不同的桶 - 根据桶find 最好的结果（遍历成本就小了） sparsity-aware split finding 有三种来源： missing values in the data frequent zero entries in the statistics artifacts of feature engineering such as one-hot encoding 类别变量是第三种情况： Actually it is not necessary to handle categorical separately We can encode the categorical variables into numerical vector using one-hot encoding. Allocate a #categorical length vector The vector will be sparse if there are lots of categories, the learning algorithm is preferred to handle sparse data 把类别变量进行one-hot 转换成数值类型，那么最后就可以使用树的模型进行计算；算法比较擅长处理稀疏数据 Shrinkage and Column Subsampling Shrinkage scales newly added weights by a factor η after each step of tree boosting. Similar to a learning rate in stochastic optimization, shrinkage reduces the influence of each individual tree and leaves space for future trees to improve the model. Shrinkage 的思想和 stochastic optimizer中类似，减弱了当前的结点的影响力，leave space for future or before trees According to user feedback, using column sub-sampling prevents over-fitting even more so than the traditional row sub-sampling (which is also supported). The usage of column sub-samples also speeds up computations of the parallel algorithm described later. 对于行（数据的数量）采样的思想早就有了，xgboost 中实现了列采样，不仅有利于加快计算速度（并行），也可以缓解过拟合（the idea from random forest） 学习的过程 在监督学习中，不断减去 $\frac{\partial f(x)}{\partial x}$， 可以得到 $min_xf(x)$。同理不断减去 $\frac{\partial l}{\partial F}$，这样就得到了 $min_FL(F)$。只不过在决策树中使用的是一阶梯度，但是在xgboost 中使用的二阶梯度。 优势和不足Advantages Invariant to scale of feature, need less data preprocessing, But it never means we don’t need to clean and transform features 不需要进行减均值除方差的操作，但是基本的数据清洗还是要有的 Naturally support categorical feature自动处理类别特征，不需要手动one-hot Disadvantages Overfitting过拟合 疑问？图中叶子节点上的数值是如何得到，有什么计算方法吗？ 参考文献Introduction to Boosted TreesXGBoost: A Scalable Tree Boosting System xgboost +lr模型简单地说，就是把gbdt的输出，作为logistic regression的输入，最后得到一个logistic regression模型。 例如，gbdt里有3棵树T1,T2,T3，每棵树的叶节点个数为4，第i个树的第j个叶节点是Li,j。 当gdbt训练完成之后，样本X1在第一棵树中被分到了第3个叶节点上，也就是L1,3，那么这个样本在T1上的向量表达为(0,0,1,0)。 样本X1在T2被分到了L2,1，那么X1在T2上的向量表达为(1,0,0,0)样本X1在T3被分到了L3,4，那么X1在T3上的向量表达为(0,0,0,1)那么X1在整个gbdt上的向量表达为 (0,0,1,0,1,0,0,0,0,0,0,1)所以每个样本都会被表示为一个长度为12的0-1向量，其中有3个数值是1。 然后这类向量就是LR模型的输入数据。 GBDT + LR 是什么本质上GBDT+LR是一种具有stacking思想的二分类器模型，所以可以用来解决二分类问题。这个方法出自于Facebook 2014年的论文 Practical Lessons from Predicting Clicks on Ads at Facebook 。 GBDT+LR 使用最广泛的场景是CTR点击率预估，即预测当给用户推送的广告会不会被用户点击。 离线部分 数据收集：主要收集和业务相关的数据，通常会有专门的同事在app位置进行埋点，拿到业务数据 预处理：对埋点拿到的业务数据进行去脏去重； 构造数据集：经过预处理的业务数据，构造数据集，在切分训练、测试、验证集时应该合理根据业务逻辑来进行切分； 特征工程：对原始数据进行基本的特征处理，包括去除相关性大的特征，离散变量one-hot，连续特征离散化等等; 模型选择：选择合理的机器学习模型来完成相应工作，原则是先从简入深，先找到baseline，然后逐步优化； 超参选择：利用gridsearch、randomsearch或者hyperopt来进行超参选择，选择在离线数据集中性能最好的超参组合； 在线A/B Test：选择优化过后的模型和原先模型（如baseline）进行A/B Test，若性能有提升则替换原先模型； 在线部分 Cache &amp; Logic：设定简单过滤规则，过滤异常数据； 模型更新：当Cache &amp; Logic 收集到合适大小数据时，对模型进行pretrain+finetuning，若在测试集上比原始模型性能高，则更新model server的模型参数； Model Server：接受数据请求，返回预测结果； 这个可以参考 GBDT+LR算法解析及Python实现 过程 用已有特征训练 GBDT 模型，然后利用 GBDT 模型学习到的树来构造新特征，最后把这些新特征加入原有特征一起训练模型。构造的新特征向量是取值 0/1 的，向量的每个元素对应于 GBDT 模型中树的叶子结点。当一个样本点通过某棵树最终落在这棵树的一个叶子结点上，那么在新特征向量中这个叶子结点对应的元素值为 1，而这棵树的其他叶子结点对应的元素值为 0。新特征向量的长度等于 GBDT 模型里所有树包含的叶子结点数之和。 XGBoost + LR 融合方式原理很简单。先用数据训练一个 XGBoost 模型，然后将训练数据中的实例给 XGBoost 模型得到实例的叶子节点，然后将叶子节点当做特征训练一个 LR 模型。得到的是一种 transformer 之后的特征，不能完全取代之前的特征工程。 此过程需注意： sklearn 或者 xgboost 输出的结果都是叶子节点的 index，所以需要自己动手去做 onehot 编码，然后交给 lr 训练，onehot 可以在 sklearn 的预处理包中调用即可。下面进入正题。 论文中 GBDT 的参数，树的数量最多 500 颗（500 以上就没有提升了），每棵树的节点不多于 12。 当树的颗数增大时，叶子结点数也相应成倍数的增加，造成lr模型训练时间拉长，同时也需要更大的惩罚项来适应； 结论：当xgb训练充分时，lr直接利用xgb叶子结点的编码特征在合适的惩罚系数下可以训练得到和xgb一样的甚至更好的效果（不显著）； 顺便来讲，RF也是多棵树，但从效果上有实践证明不如GBDT。且GBDT前面的树，特征分裂主要体现对多数样本有区分度的特征；后面的树，主要体现的是经过前N颗树，残差仍然较大的少数样本。优先选用在整体上有区分度的特征，再选用针对少数样本有区分度的特征，思路更加合理，这应该也是用GBDT的原因。 现在，我们思考这样一个问题，Logistic Regression是一个线性分类器，也就是说会忽略掉特征与特征之间的关联信息，那么是否可以采用构建新的交叉特征这一特征组合方式从而提高模型的效果？ 其次，我们已经在2.3小节中了解到GBDT很有可能构造出的新训练数据是高维的稀疏矩阵，而Logistic Regression使用高维稀疏矩阵进行训练，会直接导致计算量过大，特征权值更新缓慢的问题。 针对上面可能出现的问题，可以翻看我之前的文章：FM算法解析及Python实现 ，使用FM算法代替LR，这样就解决了Logistic Regression的模型表达效果及高维稀疏矩阵的训练开销较大的问题。然而，这样就意味着可以高枕无忧了吗？当然不是，因为采用FM对本来已经是高维稀疏矩阵做完特征交叉后，新的特征维度会更加多，并且由于元素非0即1，新的特征数据可能也会更加稀疏，那么怎么办？ 所以，我们需要再次回到GBDT构造新训练数据这里。当GBDT构造完新的训练样本后，我们要做的是对每一个特征做与输出之间的特征重要度评估并筛选出重要程度较高的部分特征，这样，GBDT构造的高维的稀疏矩阵就会减少一部分特征，也就是说得到的稀疏矩阵不再那么高维了。之后，对这些筛选后得到的重要度较高的特征再做FM算法构造交叉项，进而引入非线性特征，继而完成最终分类器的训练数据的构造及模型的训练。 其他 介绍GBDT 中防止过拟合的方法： 限制树的高度 采样（训练每一棵树的时候，只使用一部分样本） 列采样（训练每一个树的时候，只使用一部分特征。这是xgboost 中的创新，将随机森林的思想引入了GBDT 中） shrinkage（衰减稀疏，惩罚系数） early stop（在深度学习中，early stop 的指标是根据某个指标，但是在树的模型中，early stop是根据要不要加上某个树，注意效果的类比） 启发式学习 启发式算法（heuristic algorithm)是相对于最优化算法提出的。启发式算法可以这样定义：一个基于直观或经验构造的算法，在可接受的花费（指计算时间和空间）下给出待解决组合优化问题每一个实例的一个可行解，该可行解与最优解的偏离程度一般不能被预计。现阶段，启发式算法以仿自然体算法为主，主要有蚁群算法、模拟退火法、神经网络等。 图解常见的集成学习 bagging 的特点： Weak learners are independent. Bagging can be easily parallelized. adaboost的特点： Guarantee: sum of unnormalized weights is monotonously decreasing. Sum of unnormalized weights measures the error. 通过重采样，重新分配权重来处理bad 样本。（这点不是很理解哈？） Gradient Boosting 的特点： [img]https://upload.cc/i1/2019/11/21/EVuaCp.png[/img] Guarantee: sum of residuals is monotonously decreasing. Residuals are highly related to loss. 通过减少残差的方式减少误差。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>xgboost原理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[等概率生成器]]></title>
    <url>%2F2018%2F05%2F28%2F%E7%AD%89%E6%A6%82%E7%8E%87%E7%94%9F%E6%88%90%E5%99%A8%2F</url>
    <content type="text"><![CDATA[rand3() -&gt; rand7() 给定一个可以等概率生成1-3的rand3()函数生成器，求解可以随机等概率生成1-7的rand7()函数生成器。 使用多个 rand3()，保证产生的值域是大于后者的，然后再产生的值域上进行裁剪取尾。 3 6 9 1 4 7 10 2 5 8 11 3 6 9 12 转成这种样子： 3 6 9 1 1 4 7 2 2 5 8 3 3 6 9 从以上的两个例子中，这种随机数的套路应该是掌握了吧。如果是 randx() 那么这个生成式子 就是 randx() + randx() *x 这个乘数必须是 x ，要不然就重复生成了一个数字，那么最后就没有办法达到等概率，至于最后的减去的数字则是根据实际情况选择。一定要生成连续不重复的数字，这样才能是等概率的。 减去的数字保证了取余能够整除的。 123456789def rand3_to_rand7(): value =0 for i in range(1,4): for j in range(1, 4): value =i + j*3 -3 if value &gt;7: return return value% 7 rand2() -&gt; rand5()Given rand2(), you should get rand5() 如果只是使用两个 rand2() 那么只能得到四种可能性，不足以生成 rand5() ，所以要使用3 个rand2() ，这个时候得到了 8种可能性，减去3 种，那么就OK 了。( 从这个例题中可以得到是使用多个 randx() 的倍数是 x) 12345int rand7() &#123; int x = rand2() * 4 + rand2() * 2 + rand2(); if (x == 7) return rand7(); // restart else return x;&#125;]]></content>
      <categories>
        <category>算法</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[机器学习概念]]></title>
    <url>%2F2018%2F05%2F28%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[解释一些机器学习中小的基本的概念。梯度下降、牛顿法、凸函数 梯度下降批量梯度下降法是最原始的形式，它是指在每一次迭代时使用所有样本来进行梯度的更新。 优点： （1）一次迭代是对所有样本进行计算，此时利用矩阵进行操作，实现了并行。 （2）由全数据集确定的方向能够更好地代表样本总体，从而更准确地朝向极值所在的方向。当目标函数为凸函数时，BGD一定能够得到全局最优。缺点： （1）当样本数目 m 很大时，每迭代一步都需要对所有样本计算，训练过程会很慢。 随机梯度下降（Stochastic Gradient Descent，SGD） 随机梯度下降法不同于批量梯度下降，随机梯度下降是每次迭代使用一个样本来对参数进行更新。使得训练速度加快。 解释一下为什么SGD收敛速度比BGD要快？答：这里我们假设有30W个样本，对于BGD而言，每次迭代需要计算30W个样本才能对参数进行一次更新，需要求得最小值可能需要多次迭代（假设这里是10）；而对于SGD，每次更新参数只需要一个样本，因此若使用这30W个样本进行参数更新，则参数会被更新（迭代）30W次，而这期间，SGD就能保证能够收敛到一个合适的最小值上了。也就是说，在收敛时，BGD计算了 10×30W 次，而SGD只计算了 1×30W 次。 小批量梯度下降，是对批量梯度下降以及随机梯度下降的一个折中办法。其思想是：每次迭代 使用 batch_size 个样本来对参数进行更新。 上面的图可以看出 SGD 的搜索空间是有点盲目的，但是快。 随机梯度下降和批量梯度下降是两种迭代求解思路 是在梯度下降的基础之上增加了某一些限制来使得其快速运算的目的。引入随机梯度下降法与mini-batch梯度下降法是为了应对大数据量的计算而实现一种快速的求解。 牛顿法 梯度下降 vs 牛顿法 梯度下降：$$x _ { n + 1 } = x _ { n } - \mu f ^ { \prime } \left( x _ { n } \right)$$ 牛顿法递推式： $$x _ { n + 1 } = x _ { n } - \frac { f ^ { \prime } \left( x _ { n } \right) } { f ^ { \prime \prime } \left( x _ { n } \right) }$$ $$f ( x + \Delta x ) = f ( x ) + f ^ { \prime } ( x ) \Delta x + \frac { 1 } { 2 } f ^ { \prime \prime } ( x ) \Delta x ^ { 2 }$$ 对于 两种优化方法的几种解读方式： wiki 上的解释： 牛顿下降法是用二次曲面去拟合当前的局部曲面，而梯度下降法是用平面去拟合当前的局部曲面，一般用二次曲面拟合的更好，所以一般牛顿算法收敛快。 红色为牛顿下降法，绿色为梯度下降法 牛顿法是基于当前位置的切线来确定下一次的位置，所以牛顿法又被很形象地称为是”切线法”。牛顿法的搜索路径（二维情况）如下图所示： 牛顿法搜索动态示例图 数学角度 使用平面去拟合使用泰勒展开式进行解释：$$f ( x + \Delta x ) = f ( x ) + f ^ { \prime } ( x ) * \Delta x$$ 使用曲面去拟合$$f ( x + \Delta x ) = f ( x ) + f ^ { \prime } ( x ) \Delta x + \frac { 1 } { 2 } f ^ { \prime \prime } ( x ) \Delta x ^ { 2 }$$对两边 $\Delta x$进行求导 $$\Delta x = - \frac { f ^ { \prime } ( x ) } { f ^ { \prime \prime } ( x ) }$$进而： $$x _ { n + 1 } = x _ { n } - \frac { f ^ { \prime } \left( x _ { n } \right) } { f ^ { \prime \prime } \left( x _ { n } \right) }$$ 优缺点： 牛顿法起始点不能离局部极小点太远，否则很可能不会收敛。(考虑到二阶拟合应该很容易想象)，所以实际操作中会先使用别的方法，比如梯度下降法，使更新的点离最优点比较近，再开始用牛顿法。 牛顿法每次需要更新一个二阶矩阵，当维数增加的时候是非常耗内存的，所以实际使用是会用拟牛顿法。 梯度下降法在非常靠近最优点时会有震荡，就是说明明离的很近了，却很难到达，因为线性的逼近非常容易一个方向过去就过了最优点(因为只能是负梯度方向)。但牛顿法因为是二次收敛就很容易到达了。 牛顿法最明显快的特点是对于二阶函数(考虑多元函数的话要在凸函数的情况下)，牛顿法能够一步到达，非常有效。 所以这个综合上面两种方法，一般在开始的时候使用 梯度下降，然后快要接近最值点（这个时候使用 梯度下降容易发生震荡），使用牛顿法，牛顿法是需要计算二阶梯度，一般计算量比较大，所以可以考虑使用拟牛顿法。 雅克比矩阵和海森矩阵雅可比矩阵类似于多元函数的导数. $$H(f){i j}(x)=D{i} D_{j} f(x)$$ 凸函数上任意一点对应的Hessian矩阵都是半正定的。证明可以看这里 凸函数对于损失函数无非是凸函数和非凸函数。其中对凸函数一般是可以求得全局最小值的。对于非凸函数来说，其函数是抖动的，可能存在很多局部解，而在对这类问题求解时可能不能有效地找出其全局最小值， 凸优化:对凸优化的问题我们在基础数学上面已经有了很多解决方法，例如可以将凸优化问题Lagerange做对偶化，然后用Newton、梯度下降算法求解。 凸函数的来源是和凸集相对应的。凸集的“凸”是可以从几何上直观理解的，连接点集中的任意两点，线段上的每一点都在该点集内，则为凸集。这样就可以理解为甚 $y =x^2$ 是凸函数。 凸函数：Jacobian矩阵和Hessian矩阵：在向量分析中, 雅可比矩阵是一阶偏导数以一定方式排列成的矩阵, 其行列式称为雅可比行列式。 bias and variance个人感觉 variance 更像是一种稳定性的评价，预测结果的波动性，如果variance 小，那么就是稳定性好。 bias variance tradeoff 实际上是有三个名词 bias (偏差)，variance (方差) and noise (噪声)。偏差是预测值和实际值的差值，是准不准的问题；而方差是预测值的属性，如果最后的结果比较分散，那么方差就比较大。所以是两个不同的维度。最好的模型当然是低方差并且低偏差了。一图胜千言 常见的几种说法： 当网络结构比价简单的时候，容易出现高偏差，一般来说这个时候是欠拟合 当网络比较复杂的时候，容易出现高方差，一般来说这个时候是过拟合 如果出现了过拟合，一般说拟合了过多的noise (噪声)，没有抓住问题的本质。 问题 解决方案 高方差 采集更多的样本， 降低特征的维度，降低参数 高偏差 采集更多的特征，增大参数 在训练方法上也是可以改进的， 比如使用 K-Fold 交叉验证。简单的说，就是将训练样本分成k份，每次取其中一份作为验证集，另外 k-1 份作训练集。这样进行 k 次训练得到 k 个模型。这 k 个模型对各自的验证集进行预测，得到 k 个评估值（可以是误差、准确率，或按某种规则计算的得分等等）。注意到每个样本参与了 k-1 个模型的训练（导致模型之间存在关联），每个样本有一次被用作测试（没有用另外的从未见过的测试集数据），所以这与标准的计算过程是不一样的。 当 K值比较大的时候，模型容易过拟合数据集(K-1 数据集占比是接近于1，所以记住了训练数据集)，bias 比较小，但是当 测试集上的时候，variance 比较大； 同理，当K 值标胶小的时候，bias 比较大，没有过拟合数据，因此在 test 数据集上 bias 是比较小的。 偏差和方差又与「欠拟合」及「过拟合」紧紧联系在一起。由于随机误差是不可消除的，所以此篇我们讨论在偏差和方差之间的权衡（Bias-Variance Tradeoff）。 当模型处于欠拟合状态时，训练集和验证集上的误差都很高； 当模型处于过拟合状态时，训练集上的误差低，而验证集上的误差会非常高。 这两个概念和模型的复杂度又有着很深的联系： bias-variance 和 bagging-boosting的关系 数据的质量决定了学习的上限。而假设在数据已经给定的情况下，此时上限已定，我们要做的就是尽可能的接近这个上限。而模型的期望泛化误差由偏差和方差组成。一般来说，简单模型偏差高，方差低；复杂模型方差高，偏差低。 偏差描述的是算法的预测的平均值和真实值的关系（可以想象成算法的拟合能力如何），而方差描述的是同一个算法在不同数据集上的预测值和所有数据集上的平均预测值之间的关系（可以想象成算法的稳定性如何）。 （ps：个人认为可以把偏差认为是单个模型的学习能力，而方差则描述的是同一个学习算法在不同数据集的不稳定性） Bagging和Boosting是集成学习当中比较常用的两种方法，刚好分别对应了降低模型方差和偏差。 decision + bagging其实做了两个事情减少了variance。 bootstrap；就是从training data里面sample with replacement，然后生成training data x N。（例如sample 是 [1,2,3,4,5,6], bootstrap之后有了三份[2,2,3,4,1,6], [1,3,5,6,1,2], [1,2,3,1,2,3]。）bootstrap是个很重要的idea 数学上有很多证明这样做能有效减低variance。 averaging from all the output。这个很好理解，做个思考实验 - 从方差固定的population里取10 个sample，对比与从同一个population取2个sample-取平均-重复10次；两组10个数字的variance的期望肯定是前者高。补充一点，random forest其实是bagging的延伸。加的idea就是每次build tree的时候只选一部分的attribute - 因为decision tree一般很受重要的attribute的影响（例如有个attribute和response/label 相关性很高，基本每次bootstrap第一个split都是这个attribute），导致bootstrap完之后variance没有减少（因为每次的树都是很相似的）- 所以大家想remove这个attribute in some bootstrap sample Bagging是通过重采样的方法来得到不同的模型，假设模型独立则有：$$E\left(\frac{\sum_{i} \vec{E}\left(X_{i}\right)}{n}\right)=E\left(X_{i}\right)$$$$\operatorname{Var}\left(\frac{\sum_{i} \operatorname{Var}\left(X_{i}\right)}{n}\right)=\frac{1}{n} \operatorname{Var}\left(X_{i}\right)$$所以从这里我们可以看出Bagging主要可以降低的是方差。 bagging是对许多强（甚至过强）的分类器求平均。在这里，每个单独的分类器的bias都是低的，平均之后bias依然低；而每个单独的分类器都强到可能产生overfitting的程度，也就是variance高，求平均的操作起到的作用就是降低这个variance。 而Boosting每一次都关注使整体的loss减少，很显然可以降低bias。基于boosting框架的Gradient Tree Boosting模型中基模型也为树模型，同Random Forrest，我们也可以对特征进行随机抽样来使基模型间的相关性降低，从而达到减少方差的效果。（比如说xgboost 中的sample 参数，既可以对样本数量进行sample 也可以对特征进行sample）boosting 可以降低偏差应该是很好理解的，其原理就是将多个弱学习器组合成一个强学习器。boosting是把许多弱的分类器组合成一个强的分类器。弱的分类器bias高，而强的分类器bias低，所以说boosting起到了降低bias的作用。variance不是boosting的主要考虑因素。为什么随机森林的树的深度往往大于GBDT的树的深度？ 总之要记住： bias跟模型有关，variance跟数据有关。 生成式模型和判别式比较判别式模型对 $ P(Y|X) $进行建模,目的是找到一个决策边界,根据这个边界来确定新样本的类别.生成式模型对 $P(X,Y) $进行建模,目的是找到每个类别的分布,根据类别的分布情况确定新样本的类别. 如上图所示,当一个新样本进来后,如果是判别式模型,会判断它在决策边界的左边还是右边,即计算P(Y_1|X)与P(Y_2|X)来判断新样本是蓝色还是黄色.如果是生成式模型会根据两个类别分布分别计算属于两个类别的概率,即P(Y_1,X)与P(Y_2,X)来判断新样本是蓝色还是黄色. 举例子说明常见的算法： 线性回归、逻辑回归、神经网络、支持向量机、决策树、 K近邻 都是属于判别模型；最常见的生成模型的例子就是 朴素贝叶斯了。 朴素贝叶斯通过贝叶斯法则,对 $ P(y | x) $进行变换: $$P ( y | x ) = \frac { P ( x , y ) } { P ( x ) }$$ 分母是常数,分子根据条件独立性可以转化为:$$P ( x , y ) = p \left( y , f _ { 1 } , \ldots , f _ { n } \right) = \prod _ { i = 1 } ^ { n } P \left( f _ { i } | y \right)$$ 也就是说朴素贝叶斯相当于为每一个特征建立了一个高斯模型.所以朴素贝叶斯是生成式模型. 对于判别式模型来说求得 $P(Y|X) $，对未见示例 $X $，根据P(Y|X)可以求得标记Y，即可以直接判别出来，如上图的左边所示，实际是就是直接得到了判别边界，所以传统的、耳熟能详的机器学习算法如线性回归模型、支持向量机SVM等都是判别式模型，这些模型的特点都是输入属性X可以直接得到Y。 而生成式模型求得 $P(Y,X) $，对于未见示例X，你要求出X与不同标记之间的联合概率分布，然后大的获胜，如上图右边所示，并没有什么边界存在，对于未见示例（红三角），求两个联合概率分布（有两个类），比较一下，取那个大的。比如朴素贝叶斯算法和隐式马尔科夫模型。 判别式模型举例：要确定一个羊是山羊还是绵羊，用判别模型的方法是从历史数据中学习到模型，然后通过提取这只羊的特征来预测出这只羊是山羊的概率，是绵羊的概率。生成式模型举例：利用生成模型是根据山羊的特征首先学习出一个山羊的模型，然后根据绵羊的特征学习出一个绵羊的模型，然后从这只羊中提取特征，放到山羊模型中看概率是多少，在放到绵羊模型中看概率是多少，哪个大就是哪个。 one-hotOne-Hot编码，又称为一位有效编码，主要是采用N位状态寄存器来对N个状态进行编码，每个状态都由他独立的寄存器位，并且在任意时候只有一位有效。One-Hot编码是分类变量作为二进制向量的表示。这首先要求将分类值映射到整数值。然后，每个整数值被表示为二进制向量，除了整数的索引之外，它都是零值，它被标记为1。 在很多机器学习任务中，特征并不总是连续值，而有可能是分类值。离散特征的编码分为两种情况： 离散特征的取值之间没有大小的意义，比如color：[red,blue],那么就使用one-hot编码 离散特征的取值有大小的意义，比如size:[X,XL,XXL],那么就使用数值的映射{X:1,XL:2,XXL:3} 优缺点优点： 简单，将类别数据解析成分类器可以处理的数据类型。 要是one hot encoding的类别数目不太多，建议优先考虑。缺点： 当类别的数量很多时，特征空间会变得非常大。在这种情况下，一般可以用PCA来减少维度。而且one hot encoding+PCA这种组合在实际中也非常有用。 降维前可以交叉的降维后可能变得不能交叉 实现 手动实现 1234567891011121314151617import numpy as npdef one_hot(list1): list1_u =np.unique(list1) # save (val, index) dict1 =dict() for (ind, val) in enumerate(list1_u): dict1[str(val)] =ind res =np.zeros((len(list1), len(list1_u))) # modify (ind, val) for (ind, val) in enumerate(list1): res[ind, dict1[str(val)]] =1 return reslist1 =[1, 2, 2, 4]print(one_hot(list1)) 使用pandas 实现 123456import pandas as pds = pd.Series(list("abcdd"))arr =pd.get_dummies(s)arr1 =pd.get_dummies(s, sparse =True)print(type(arr), arr)print(type(arr1), arr1) 使用sklearn 实现 12345678import numpy as npfrom sklearn.preprocessing import OneHotEncoderlabels =[0, 1, 0, 2]labels =np.array(labels).reshape(len(labels), -1)enc =OneHotEncoder()enc.fit(labels)targets =enc.transform(labels).toarray() 数据矩阵是4*3，即4个数据，3个特征维度。（所以最后的结果不一定是只是出现了一个 1） 其他 什么情况下(不)需要归一化？需要： 基于参数的模型或基于距离的模型，都是要进行特征的归一化。不需要：基于树的方法是不需要进行特征的归一化，例如随机森林，bagging 和 boosting等。 Tree Model不太需要one-hot编码 常见的几种分类算法的总结机器学习常见分类算法机器学习中常用的分类算法常用机器学习算法汇总(中）机器学习常见算法个人总结（面试用） 复习笔记 优化器： 梯度下降得到是全局最优解，但是当数据量大的时候，训练速度变慢。随机梯度下降得到局部解，训练速度快；batch_size 得到了 batch梯度下降，是两者方法的折中。 牛顿法相比于梯度下降是二阶求导，基于当前位置切线来确定下一次的位置，更加快。和一阶在最优点会震荡，但是二阶不会。 凸函数与否在于是否可以求得全局最优（小）解。 bias（偏差） 和variance（方差） 更加适合评价回归模型的效果，分别对应准和确。当模型比较简单的，容易出现高偏差，这个是欠拟合；当模型复杂时候，容易出现高方差，这个是过拟合。 判别模型是 $p(y|x)$ ，给定条件，目的需要给出一个决策边界。 $p(x, y)$ 是生成模型，根据类别的分布情况确定新样本的类别。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>bias-variance</tag>
        <tag>one-hot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[面试算法题]]></title>
    <url>%2F2018%2F02%2F22%2F%E9%9D%A2%E8%AF%95%E7%AE%97%E6%B3%95%E9%A2%98%2F</url>
    <content type="text"><![CDATA[Consecutive Numbers SumTips：如果是连续数，那么是可以使用等差数列求和的。并且这个求解的是符合条件的解的个数。时间复杂度是O ($\sqrt{n}$) 。对于一个正整数N，如果能写成K个连续正整数相加的形式，则有， $$\begin{split}N &amp;= ( x + 1 ) + ( x + 2 ) + \dots + ( x + K ) \\N &amp;= K \times x + \frac { ( 1 + K ) \times K } { 2 } \\\end{split}$$ 所以， N 能够被 K个连续正整数相加的条件是， $\left( N - \frac { K * ( K + 1 ) } { 2 } \right)$ 能够被 K 整除。 1234567891011121314151617181920class Solution(object): def consecutiveNumbersSum(self, N): """ :type N: int :rtype: int """ res =0 n =0 while n*(n+1) &lt;= 2*N: top =N -(n *n +n)/2 if top &lt;=0: break elif top %(n +1) ==0: res +=1 n +=1 return res C ++的代码 12345678910111213141516171819class Solution &#123;public: // 数学就是要求 excactly 这样的准确 // 如果最后多了，那么限制条件一定是不够严格；反之也是成立的 int consecutiveNumbersSum(int N) &#123; // 这个i 表示连续数字的长度 int i =0; int counts =0; while(i*(i +1) &lt;= 2*N) &#123; // 边界条件必要难把握 int top = N - (1+i) *i/2; if(top &lt;=0) break; if(top % (i +1) ==0) counts ++; i ++; &#125; return counts; &#125;&#125;; 如何近似的求解 piTips: 随机抽样，注意是均匀分布而不是正太分布，对应python 中的实现是 randrange() 而不是 random() 函数。注意有精度的问题 1234567891011from random import randrangetotal = 100000res =0i =0while i&lt; total: x = randrange(0, total+1)/100000.0 y = randrange(0, total+1)/ 100000.0 if(x*x + y*y &lt;= 1): res +=1; i += 1print( res*1.0/ total*4) 在一个圆内随机采样得到结果的概率？ 4/pai求解的过程？方法一： 使用 range(0，1) 平均函数方法二： 使用 range(0, 1) 随机得到r， 使用 sigta 得到[0, 2pai]，然后两者相乘。但是对于 r 是需要 进行 根号r 处理。否则的话，最后的结果是不均匀的 对于第二种方式，使用极坐标表示的方式不会 1234567891011import mathfrom random import randrangen = 1000000r = randrange(0,n+1)/nthetha = r * 2*math.piarea =thetha * r* 4print(r) print(thetha)print(area) 中文转换成阿拉伯数字 中文转成阿拉伯数字（含有点） 如果有“点”，那么就截断处理，分成小数点之前和之后。 二进制数中的1 在二进制中，经常使用到的就是 &amp;和 &gt;&gt; 两个操作。 &gt;&gt; 移位操作只是针对整数，对于浮点数和 unsigned int 这个是不能用的，在后面两种类型中，需要使用到 ·/=· 这样的操作，该操作是通用的。时间复杂度是 $log(n)$。 12345678910111213class Solution &#123;public: int hammingWeight(uint32_t n) &#123; int cout =0; while(n) &#123; if(n &amp;1 ==1) cout ++; n /=2; &#125; return cout; &#125;&#125;; 如果1出现的次数比较少， 时间复杂度变成了$O(k)$， K 表示字符串中 1 的个数， 所以 n &amp;(n-1) 是一个很好的操作。 1234567891011121314int count3(Byte v)&#123; int count =0; while (v) &#123; v &amp;= (v-1);// 使得最大位的1变成0 ，在二进制的角度上 count +=1 ; &#125;&#125;// 使用空间代替时间, 这个是可以进行穷举的int count4(Byte v)&#123; int countTable [256] =&#123;0, 1,1,2...8&#125; int count =countTable[v]; return count;&#125; 给定一个数字N N的阶乘有多少个0? Tips: 数学问题，N 是可以通过质因数(2, 3, 5)进行分析 $N =(2^x +3^y+ 5^z) $， 然后0的个数是 $min(x, z) -&gt; z $, 最后就相当于每个数字的5的个数。到最后是在统计 5 的个数，数字N 分解因式之后 5 的个数。 时间复杂度是$O(log_5N)$ 还是log 的时间复杂度 1234567891011int count(Byte， v)&#123; int count; for(int i =0; i&lt;=v; i++) &#123; while (i) &#123; count +=1; i /=5; &#125; &#125; return count;&#125; 123456789int count(Byte, v)&#123; int count =0; while (v &gt;4) &#123; count += v/5; v =v /5; &#125; return count;&#125; 求解 N! 中的二级制表示中最低位1的位置 在二进制中，经常使用到的就是 &amp;和 &gt;&gt; 两个操作。 &gt;&gt; 移位操作只是针对整数，对于浮点数和 unsigned int 这个是不能用的，在后面两种类型中，需要使用到 ·/=· 这样的操作，该操作是通用的。 还是从二进制的角度出发。 12345678910111213class Solution &#123;public: int hammingWeight(uint32_t n) &#123; int cout =0; while(n) &#123; if(n &amp;1 ==1) cout ++; n /=2; &#125; return cout +1; &#125;&#125;; 寻找每个 ID，该ID 出现的频率是大于 0.5, 出现的次数大于总数的一半 Tips: 如果每次删除两个不同的ID，那么 最多的ID 出现的次数还是大于总数的一般，不断的重复这个过程。时间复杂度是O(n)，想法是上面那种，但是在操作的时候，并不是删除了某个ID。看代码是一清二楚的。 1234567891011121314151617181920212223242526Type Find(Type *ID, int N)&#123; Type candidate; int nTimes =0, i; for(i =0; i&lt; N; i++)&#123; if (nTimes ==0) &#123; candidate =ID[i]; nTimes =1 &#125; else &#123; if(ID[i] ==candidate)&#123; nTimes +=1; &#125; else&#123; nTimes -=1; &#125; &#125; &#125; // 这个应该还是需要最后判断一下 candidate 是不是出现了 [2/N] 次数，因为有一种情况是不存在的&#125; 尾递归尾调用定义: 在某个函数的最后一步调用另一个函数.进入下一个递归之后，该层次的递归已经结束，不需要使用栈空间进行保留信息。 123456function f(x) &#123; if (x &gt; 0) &#123; return m(x) &#125; return n(x); &#125; 上面代码中，函数m和n都属于尾调用，因为它们都是函数f的最后一步操作。不一定是最后一行。 同理是可以推广到尾递归。 12345678910111213141516function f() &#123; let m = 1; let n = 2; return g(m + n);&#125;f();// 等同于function f() &#123; return g(3);&#125;f();// 等同于g(3); 尾递归的意义在于防止栈溢出。因为递归的调用是非常消耗内存的，需要再栈中保存N 个调调用记录，很容易发生栈溢出 (stack overflow) 。但是对于为递归来说，只有一个调用记录，因为上一个已经结束，所以是不会发生 “栈溢出”错误的。 Fibonacci 数列最原始的解法，递归，其中有很多重复的计算子单元。 程序的开始就像百度的面试官那样说的，都是应该考虑一下特殊情况，有可能考虑不全，但是一定要有这个意识。到了递归中，不再是特殊情况这种事情，而是跳出条件。 123456789101112131415int Fibonacci(int n)&#123; if (n &lt;0) &#123; return 0; &#125; else if(n ==1) &#123; return 1; &#125; else &#123; return Fibonacci(n -1) + Fibonacci(n -2); &#125;&#125; 解法二： 时空都是 O(N)，重点是从递归转成了循环。 1234567891011def Fibonacci(n ): if n ==0: return 0 elif n ==1: return 1 arr =[0] *(n+1) arr[0] =0 arr[1] =1 for i in range(2, n+1): arr[i] =arr[i-1] +arr[i-2] return arr[-1] 在上面的基础上进行优化，空间复杂度变成 O(1) a, b =b, a+b # 在python，连续赋值是从左往右的。也就是说， 先保存 b, 和 a+b，然后执行 a =b, b = a+b所以在python 中交换两个数字可以写成这样： a, b = b, a (python 中是从左往右进行赋值的) 时间复杂度是$O(n)$, 空间复杂度 $O(1)$, 因为空间上只是依赖前两个，并不需要保存整个数组。 1234567891011121314class Solution(object): def fib(self, N): """ :type N: int :rtype: int """ if N &lt;=0: return 0 elif N ==1: return 1 a, b =0, 1 for i in range(N ): a, b =b, a+b # 在python，连续赋值是从左往右的。 return a 寻找数组中的最大值和最小值时间复杂度是 O(N)，使用了两个值进行min_n 和max_n 的判断。 如果数组本身没有什么有序性的话 ，那么时间复杂度就是 $O(n)$ 12345678910111213141516171819202122232425262728293031323334#include&lt;iostream&gt;using namespace std;#define INF 10^9void FindMinMax(int a[], int size, int &amp;min, int &amp;max)&#123; max =-INF; min =INF; for (int i =0; i&lt;size -1; i++) &#123; // 这种判断方式 还是值得学习的，细节就是 index 的边界 if (a[i] &lt; a[i+1]) &#123; if(a[i+1]&gt; max ) max =a[i+1]; if(a[i] &lt; min) min =a[i]; &#125; else &#123; if(a[i] &gt; max) max =a[i] ; if(a[i+1] &lt; min) min =a[i+1]; &#125; &#125;&#125;int main()&#123; int arr[10]; int length =sizeof(arr)/ sizeof(arr[0]); int min, max; FindMinMax(arr, length, min, max); &#125; 寻找最近点对在二维平面上的n个点中，找出最接近的一对点第一次碰到这个题目的时候，还在想图的结构，现在看来只要是有array 的存在，那么这个是一定按照 array 的情况进行遍历求解。 解法一： 暴力法，时间复杂度是$O(n^2)$ 12345678910111213141516171819202122232425262728293031323334353637383940414243#include&lt;iostream&gt;#include&lt;cmath&gt;using namespace std;const int N =101;struct Point&#123; float x, y;&#125;point[N];// 因为是枚举的，所以还是得存储一下原来输入int n;double cal_dis(Point &amp; a, Point &amp; b)&#123; return sqrt((a.x-b.x) *(a.x - b.x) + (a.y - b.y) * (a.y - b.y));&#125;int main()&#123; cin &gt;&gt;n; int i =0; while(i &lt;n) &#123; int x, y; cin &gt;&gt;x &gt;&gt;y; point[i].x =x; point[i].y =y; &#125; double min_v= cal_dis(point[0],point[1]), tmp; int ind1, ind2; // 枚举的思想就是，就是两两进行比较 for(int i =0; i&lt;n; i++) &#123; for(int j =i+1; j&lt;n; j++) &#123; tmp =cal_dis(point[i], point[j]); if(tmp &lt; min_v) &#123; min_v =tmp; ind1 =i; ind2 =j; &#125; &#125; &#125; cout &lt;&lt; min_v&lt;&lt; endl; return 0;&#125; 解法二：分治法 参考这里, 最后的时间复杂度可以降低到 $O(nlogn)$，没有看懂。 使用分治法（divide and conquer）解题步骤： divide 将要解决的问题分成若干小规模的同类问题 conquer- 当子问题划分的足够小，能够使用较简单的方式解决 combine 将子问题的解逐层合并构成原问题的解 采用分而治之的思想，分成左右两个子集，$S_l$ 和$S_r$ ，然后分别计算两个子集之内的最小的距离，然后需要计算两个子集边界附近的点的距离。从算法步骤上讲，先存储点 point 这样的一个结构体，然后按照point 中的x 进行排序。 $\delta$ 是从左右两个子集中取得的点与点之间的最小距离，那么意味着两点之间的距离 最小是 $\delta $，所以在正方形 ($2\delta$, $2\delta$ ) 的区间内，左半部分的点到右半部分的点的距离最少是 $\delta$，根据割舍原理，那么最多只需要比较6个点。这里的是每个点，只需要个其他的6个点进行比较，而不是总共只有6个点。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137#include &lt;ctime&gt;#include &lt;cmath&gt;#include &lt;iostream&gt;#include &lt;algorithm&gt;using namespace std;#define INFINITE_DISTANCE 65535 // 无限大距离#define COORDINATE_RANGE 100.0 // 横纵坐标范围为[-100,100]#ifndef Closest_pairtypedef struct Point&#123;// 二维坐标上的点Point double x; double y;&#125;Point;double Distance(Point a, Point b)&#123;//平面上任意两点对之间的距离公式计算 return sqrt((a.x - b.x)*(a.x - b.x) + (a.y - b.y)*(a.y - b.y));&#125;bool compareX(Point a, Point b)&#123;//自定义排序规则：依照结构体中的x成员变量升序排序 return a.x &lt; b.x;&#125;bool compareY(Point a, Point b)&#123;//自定义排序规则：依照结构体中的x成员变量升序排序 return a.y &lt; b.y;&#125;float ClosestPair(Point points[], int length, Point &amp;a, Point &amp;b)&#123;// 求出最近点对记录，并将两点记录再a、b中 double distance; //记录集合points中最近两点距离 double d1, d2; //记录分割后两个子集中各自最小点对距离 int i = 0, j = 0, k = 0, x = 0; //用于控制for循环的循环变量 Point a1, b1, a2, b2; //保存分割后两个子集中最小点对 if (length &lt; 2) return INFINITE_DISTANCE; //若子集长度小于2，定义为最大距离，表示不可达 else if (length == 2) &#123;//若子集长度等于2，直接返回该两点的距离 a = points[0]; b = points[1]; distance = Distance(points[0], points[1]); &#125; else &#123;//子集长度大于3，进行分治求解 Point *pts1 = new Point[length]; //开辟两个子集 Point *pts2 = new Point[length]; sort(points, points + length, compareX); //调用algorithm库中的sort函数对points进行排序，compareX为自定义的排序规则 double mid = points[(length - 1) / 2].x; //排完序后的中间下标值，即中位数 for (i = 0; i &lt; length / 2; i++) pts1[i] = points[i]; for (int j = 0, i = length / 2; i &lt; length; i++) pts2[j++] = points[i]; d1 = ClosestPair(pts1, length / 2, a1, b1); //分治求解左半部分子集的最近点 d2 = ClosestPair(pts2, length - length / 2, a2, b2); //分治求解右半部分子集的最近点 if (d1 &lt; d2) &#123; distance = d1; a = a1; b = b1; &#125; //记录最近点，最近距离 else &#123; distance = d2; a = a2; b = b2; &#125; //merge - 进行子集合解合并 //求解跨分割线并在δ×2δ区间内的最近点对 Point *pts3 = new Point[length]; for (i = 0, k = 0; i &lt; length; i++) //取得中线2δ宽度的所有点对共k个 if (abs(points[i].x - mid) &lt;= distance) pts3[k++] = points[i]; sort(pts3, pts3 + k, compareY); // 以y排序矩形阵内的点集合 for (i = 0; i &lt; k; i++) &#123; if (pts3[j].x - mid &gt;= 0) // 只判断左侧部分的点 continue; x = 0; for (j = i + 1; j &lt;= i + 6 + x &amp;&amp; j &lt; k; j++) //只需与有序的领接的的6个点进行比较 &#123; if (pts3[j].x - mid &lt; 0) &#123;// 假如i点是位于mid左边则只需判断在mid右边的j点即可 x++; continue; &#125; if (Distance(pts3[i], pts3[j]) &lt; distance) &#123;//如果跨分割线的两点距离小于已知最小距离，则记录该距离和两点 distance = Distance(pts3[i], pts3[j]); a = pts3[i]; b = pts3[j]; &#125; &#125; &#125; &#125; return distance;&#125;void SetPoints(Point *points, int length)&#123;//随机函数对点数组points中的二维点进行初始化 srand(unsigned(time(NULL))); for (int i = 0; i &lt; length; i++) &#123; points[i].x = (rand() % int(COORDINATE_RANGE * 200)) / COORDINATE_RANGE - COORDINATE_RANGE; points[i].y = (rand() % int(COORDINATE_RANGE * 200)) / COORDINATE_RANGE - COORDINATE_RANGE; &#125;&#125;int main()&#123; int num; //随机生成的点对个数 Point a, b; //最近点对 double diatance; //点对距离 cout &lt;&lt; "请输入二维点对个数:"; cin &gt;&gt; num; if (num &lt; 2) cout &lt;&lt; "请输入大于等于2的点个数！！" &lt;&lt; endl; else &#123; cout &lt;&lt; endl &lt;&lt; "随机生成的" &lt;&lt; num &lt;&lt; "个二维点对如下：" &lt;&lt; endl; Point *points = new Point[num]; SetPoints(points, num); for (int i = 0; i &lt; num; i++) cout &lt;&lt; "(" &lt;&lt; points[i].x &lt;&lt; "," &lt;&lt; points[i].y &lt;&lt; ")" &lt;&lt; endl; diatance = ClosestPair(points, num, a, b); cout &lt;&lt; endl &lt;&lt; endl &lt;&lt; "按横坐标排序后的点对:" &lt;&lt; endl; for (int i = 0; i &lt; num; i++) cout &lt;&lt; "(" &lt;&lt; points[i].x &lt;&lt; "," &lt;&lt; points[i].y &lt;&lt; ")" &lt;&lt; endl; cout &lt;&lt; endl &lt;&lt; "最近点对为：" &lt;&lt; "(" &lt;&lt; a.x &lt;&lt; "," &lt;&lt; a.y &lt;&lt; ")和" &lt;&lt; "(" &lt;&lt; b.x &lt;&lt; "," &lt;&lt; b.y &lt;&lt; ")" &lt;&lt; endl &lt;&lt; "最近点对距离为：" &lt;&lt; diatance &lt;&lt; endl; &#125; system("pause");&#125;#endif // !Closest_pair 对比一下一维的情况。123456789101112131415161718double MinDifference(double arr[], int n)&#123; if (n &lt;2) return 0; sort(arr, arr+n); // sort(arr, arr+n myfunction) 降序 double minDiff =arr[1] - arr[0]; for (int i =2; i&lt;n;i++) &#123; double tmp =arr[i] -arr[i-1]; if (tmp &lt;minDiff) &#123; minDiff =tmp; &#125; &#125; return minDiff; &#125; 快速寻找满足条件的两个数 第一种解法： 排序之后 然后二分查找 O(nlogn) +O(logn) -&gt; O(nlogn) 在有序的数组中，必然使用二分进行查找，这样查找的效率从O(n) -&gt; O(log n) 123456789101112131415161718192021222324252627282930313233343536373839404142434445#include&lt;iostream&gt;#include&lt;algorithm&gt;using namespace std;#define MAXN 1001int arr[MAXN];bool findSum(int arr[], int n, int sum)&#123; int i =0; int j =n-1; while (i&lt;j) &#123; int tmp =arr[i] +arr[j]; if (tmp ==sum) &#123; printf("(%d, %d)", arr[i], arr[j]); return true; &#125; else if(tmp &lt;sum) i ++; else j --; &#125; return false;&#125;int main()&#123; int n, sum, i,j; cin &gt;&gt; n &gt;&gt;sum; for(i =0; i&lt;n;i++) cin &gt;&gt;arr[i]; sort(arr, arr+n); bool res =findSum(arr, n, sum); return 0;&#125; 第二种解法：使用辅助的空间 dictionary 存储，空间复杂度O(N) + 时间复杂度 O(N) 使用python 中的 dictionary， in 这种操作是非常具有可读性的。并且这种操作的时间复杂度是O(1) ,这个是不容置疑的，这个是字典的特性。 123456789101112131415def twoSum(arr, total): if not arr: return arr from collections import defaultdict dic =defaultdict(int) for num in arr: dic[num] = dic[num] +1 for key in dic: if total -key in dic: return (key, total -key) return (-1, -1) 子数组的最大乘积 给定一个长度为 N 的整数数组，只允许用乘法，不能用除法， 计算任意 ( N-1) 个数的组合中乘积最大的一组。 限制条件： 必须要选出 （N-1）个数字，只能使用乘法。 第一种解法：暴力求解，遍历出n 个，然后需要 n-1 次相乘，所以时间复杂度是 $O(N^2)$ 1234567891011121314151617181920212223242526#include&lt;iostream&gt;using namespace std;class Solution &#123;public:int maxProduct(int *nums) &#123; int res; int max; int i, j; int n= sizeof(nums) /sizeof(nums[0]); for(i =0; i&lt;n; i++) &#123; for(j =0; j&lt;n; j++) // 使用 for 循环的形式将子集相乘 &#123; res =1; res *= ( (i==j) ? 1: nums[j]); if (res ==0) break; &#125; max =( i==0 ? res: max); // 实际上是对于 max 的出释怀 max =( max &lt; res? res: max); // 这种语法是比较简洁的 &#125; return max;&#125;&#125;​ 上述方式进行了很多重复的运算，可以保存下来子问题，减少时间复杂度。实现的时候，两个list 分别从左右两边进行累乘运算。所以第二种解法：时间复杂度从 $O(n^2) $ 降低为 $ O(n)$ 。从做题的结果上看，如果最后求解的是一个值，那么很有可能使用 $O(1) $ 的空间复杂度得到。​1234567891011121314151617class Solution &#123;public: // 简单粗暴的解法，在遍历的过程中需要维护 minv 和maxv int maxProduct(vector&lt;int&gt;&amp; nums) &#123; if(nums.empty()) return 0; int n =nums.size(); int minv =nums[0], maxv =nums[0], res =nums[0]; for(int i =1; i&lt; n; i++) &#123; int nv =minv, xv =maxv; maxv =max(nums[i], max(nums[i]* nv, nums[i] * xv)); minv =min(nums[i], min(nums[i] * nv, nums[i] * xv)); res =max(res, maxv); &#125; return res; &#125;&#125;; 最长递增子序列 第一种方式，时间复杂度是 O($n^2$),思想是dp. dp[i] 表示 到目前为止前i 个数字中的 最长递增子序列。 12345678910111213141516171819202122232425class Solution &#123;public: // O(m^2) 的时间复杂度，dp[i] 表示前i 个数字最长的递增子序列的长度 // 递推方程是 dp[i] =max(dp[j] +1) 其中 j&lt; i， 并且 nums[i] &gt; nums[j] // 初始化 dp =&#123;1&#125; 因为如果只有一个元素，那么dp[0] =1 int lengthOfLIS(vector&lt;int&gt;&amp; nums) &#123; if(nums.empty()) return 0; int n =nums.size(); vector&lt;int&gt;dp(n, 1); int res =0; for(int i =0; i&lt; n; i++) &#123; // 遍历前面的所有dp[j] for(int j =0; j&lt;i; j++) &#123; if(nums[i] &gt; nums[j]) &#123; dp[i] =max(dp[i], dp[j] +1); &#125; &#125; res =max(res, dp[i]); &#125; return res; &#125;&#125;; python 实现 12345678910111213141516class Solution(object): def lengthOfLIS(self, nums): """ :type nums: List[int] :rtype: int """ if(not nums) : return 0; n =len(nums) dp =[1] *n res =0 for i in range(n): for j in range(0, i): if(nums[i] &gt; nums[j]): dp[i] =max(dp[i], dp[j] +1) res =max(res, dp[i]) return res 上面的时间复杂度是O($ n^2$), 在于每次都是遍历了之前所有的情况 (dp) ，还有一种方式创建一个辅助数组，用于存储之前已经排好序的元素，遍历的时候使用二分查找进行遍历。于是第二种解法就出来了，时间复杂度是 O(n logn). 这个辅助函数不是从原来的数组不是最长增数组本身，这个函数只能用于计算最后的长度，得不到数组。 123456789101112131415161718192021222324252627282930class Solution &#123;public: int binary_search(vector&lt;int&gt; &amp;h, int l, int r, int target) &#123; while(l&lt;r) &#123; int mid = l +r &gt;&gt;1; if(h[mid] &lt; target) l =mid +1; else r =mid; &#125; return l; &#125; int lengthOfLIS(vector&lt;int&gt;&amp; nums) &#123; if(nums.empty()) return 0; int n =nums.size(); vector&lt;int&gt; h(n, 0); h[0] =nums[0]; int index =0; for(int i =1; i&lt;n; i++) &#123; if(nums[i] &gt; h[index]) h[++index] =nums[i]; else &#123; int pos =binary_search(h, 0, index, nums[i]); h[pos] =nums[i]; &#125; &#125; return index+1; &#125;&#125;; 数组循环移位 暴力求解 时间复杂度是O($KN$) K表示移位的次数 N 表示数组的长度。在c++中，数组如果是一个个移位，那么时间复杂度是$O(kn)$， 但是如果是两两交换位置，那么时间复杂度是$O(n)$，所以这个是有优化的空间的。 C++ 解法 12345678910111213141516171819202122232425#include&lt;iostream&gt;#include&lt;vector&gt;using namespace std;void rightShift(vector&lt;int&gt; arr, int n , int k)&#123; if (arr.empty() or arr.size() ==0) return ; if (k&gt;n) k =k%n; while(k --) &#123; int temp =arr[n-1]; for(int i =n-1; i&gt;0; i--) arr[i] =arr[i-1]; arr[0] =temp; &#125; &#125;int main()&#123; return 0;&#125; C语言解法 123456789101112131415161718void rightShift(int *arr, int n, int k)&#123; if (arr ==NULL or n ==0) return ; if (k&gt;n) k =k%n; while(k --) &#123; int temp =arr[n-1]; for (int i =n-1; i&gt;0; i--) &#123; arr[i] =arr[i-1]; &#125; arr[0] =temp; &#125;&#125; 解法二： 原来序列 abcd1234 转换成 1234abcd ，分成两部分， 1234 和abcd 看成两个整体，右移就是把数组的两个部分交换一下通过以下的方法进行实现， 逆序1234, 逆序abcd , 逆序整个数组, 整个arr 的 reverse 时间复杂度：这个不是递归，只是同一个函数调用了三次，时间复杂度分别是 O($ \frac{k}{2}$), O( $\frac{N-k}{2}$) 和 O($\frac{N}{2}$)，所以总的时间复杂度是 O(N) 123456789101112131415161718192021222324252627282930#include&lt;stdio.h&gt;const int N =101;int n, k;void reverse(char *arr, int b, int e)&#123; while( b&lt;e) &#123; int t =arr[e]; arr[e] =arr[b]; arr[b] =t; b ++; e --; &#125;&#125;void right_shift(char *arr, int n, int k)&#123; k %=n; reverse(arr, 0, n-k -1); reverse(arr, n-k, n -1); reverse(arr, 0, n -1);&#125;int main()&#123; char arr[N]; scanf("%d %d", &amp;n, &amp;k); gets(arr); right_shift(arr, n, k); puts(arr); return 0;&#125; 数组分割?题目概述：有一个没有排序，元素个数为2N的正整数数组。要求把它分割为元素个数为N的两个数组，并使两个子数组的和最接近。 这里的dp[k][s]表示从前k个数中取k个数，且k不超过n，且这些数之和为s的取法是否存在。 算法时间复杂度是 $O(N^2*sum)$ 讲解可以参考这个，反正我是没有很看动画https://blog.csdn.net/linyunzju/article/details/7729774 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354#include&lt;iostream&gt;#include&lt;algorithm&gt;using namespace std;#define MAXN 101#define MAXSUM 100000int A[MAXN];bool dp[MAXN][MAXSUM];// 使用全局变量 解决定义二维的数组的问题void find(int *a, int n, int sum)&#123; int k1, k2, s; for (k1 =1; k1&lt;2*n; k1++) &#123; for(k2 =min(k1, n) ;k2&gt;=1; k2--) &#123; for(s =1; s&lt;=sum/2; s++) &#123; if (s &gt;=A[k1] &amp;&amp; dp[k2-1][s-A[k1]]) dp[k2][s] =true; &#125; &#125; &#125; for (s =sum/2; s&gt;=1 &amp;&amp; !dp[n][s] ; s--) &#123; cout &lt;&lt; sum-2*s&lt;&lt; endl; &#125;&#125;int main()&#123; int n, i; cin &gt;&gt; n; for(i=1; i&lt;=2*n; i++) &#123; cin &gt;&gt; A[i]; &#125; int sum =0; for (i =1; i&lt;=2*n; i++) &#123; sum += A[i]; &#125; memset(A, 0, sizeof(dp)); dp[0][0] =true; find(A, n, sum); return 0;&#125; 只考加法的面试题 写一个程序，对于一个 64位正整数，输出所有可能的连续（两个以上）自然数之和的算式 因为这里涉及到了连续自然数，那么一定要使用等差数列，然后进行枚举。在时间复杂度 O(n) 内解决问题。 数学问题 设存在连续自然数，首项为$A_1$，项数为m，即$$ A_1 + A_2 + \ldots + A_m = { M }$$ 使用求和公式 $$A_1 \times m + \frac{m \times ( m - 1 )}{2} = M$$可以整理得到关于 $m$ 的一元二次方程方程 $$ m^2 +2\times( A_1 -1) \times m -2 \times M =0$$ 解得 $$m = \frac{ (1- 2 \times A_1) \pm \sqrt{( 4*(A_1-1)^2 +8 \times M}}{2}$$ 所以要求 m 是整数，那么需要满足两个条件： $\sqrt{( 4*(A_1-1)^2 +8 \times M}$ 是整数 整个分子是偶数 1234567891011121314151617181920212223242526#include&lt;iostream&gt;#include&lt;cmath&gt;using namespace std;int main()&#123; int n; while (cin &gt;&gt; n) &#123; for (int i =0; i&lt;n/2; i ++) &#123; int a =i; int w =(2*a -1)*(2*a -1) +8*n; int k =(int)sqrt(w); if (k*k != w) continue; int m =k +1-2*a; if (m%2 ==1) continue; else cout &lt;&lt; i &lt;&lt; " "&lt;&lt;i+m/2-1 &lt;&lt; endl; &#125; &#125; return 0;&#125; 字符串移位包含的问题 这种移位并没有说明怎么移位，然后移动几位，所以应该是不能模拟出来的，下面是有一个既定的结论，如果在移位中包含，那么一定是在 strstr 这个中的。于是解法一就出来了。 补充cpp 的一些语法： 1 和true是等价的 字符串和字符数组的定义和初始化 直接进行初始化定义：12char *a ="string1";char b[] ="string2"; 局部变量 a b 都是在栈中，a 是指向了一个常亮， “string1” 而b 是指向了了一个数组。实际上前者的赋值是错误的，因为是其实不可改变的。使用 const char * a=”string1” 更加合适。char 类型转换成 string类型 cpp 中使用 string 第一种方式 12345678910#include&lt;iostream&gt;#include&lt;cstring&gt;using namespace std;int main()&#123; string str; str ="hello world"; cout &lt;&lt; str &lt;&lt;endl; return 0;&#125; 这种是 cpp 中使用string 的第二种方式 1234567891011#include&lt;iostream&gt;using namespace std;int main()&#123; std:: string str; str ="hello world"; cout &lt;&lt; str &lt;&lt;endl; return 0;&#125; 问题的解法： 1234567891011121314151617181920212223242526272829#include&lt;iostream&gt;using namespace std;bool isContain(char *s1, char *s2) &#123; int len = strlen(s1); //假设s1位较长的字符串 char s[len * 2]; for(int i = 0; i &lt; 2; i++) &#123; for(int j = 0; j &lt; len; j++) &#123; s[i * len + j] = s1[j]; &#125; &#125; if(strstr(s, s2) != NULL) &#123; return true; &#125; else &#123; return false; &#125;&#125;int main()&#123; char* s1 ="hello"; char* s2 ="hel"; bool result; result =isContain(s1, s2); cout &lt;&lt; isContain(s1, s2) &lt;&lt;endl; return 0;&#125; 解法二： python 实现，时间复杂度是O(n) 空间复杂度是(1) ，使用取模运算 123456789101112131415161718192021def match(src, des): if not src or not des: return False len1 =len(src) len2 =len(des) if len1 ==0 and len2 ==0: return True for i in range(len1): if des[0] == src[i]: for j in range(1, len2): if des[j] != src[ (i+j)%len1]: break else: return True return False Letter Combinations of a Phone Number 这个是做过的，深度优先算法 12345678910111213141516171819202122232425262728293031323334class Solution: def letterCombinations(self, digits): if not digits or digits == "": return [] # 命名很到位，任何的dictionary 都是可以使用 maps 进行命名的 # 注意从 list 到tuple 减少了内存的使用 maps =&#123; '1': (), '0': (), '2': ('a', 'b', 'c'), '3': ('d', 'e', 'f'), '4': ('g', 'h', 'i'), '5': ('j', 'k', 'l'), '6': ('m', 'n', 'o'), '7': ('p', 'q', 'r', 's'), '8': ('t', 'u', 'v'), '9': ('w', 'x', 'y', 'z') &#125; results = [""] for digit in digits: tuple1 = maps[digit] tmp =[] if len(tuple1) == 0: continue # 二重循环， results 是之前的结果，然后每次都是要在其基础上添加一些东西 for prefix in results: for suffix in tuple1: tmp.append(prefix + suffix) results = tmp return results 求一维子数组的最大和 题目：输入一个整形数组，数组里有正数也有负数。数组中连续的一个或多个整数组成一个子数组，每个子数组都有一个和。求所有子数组的和的最大值。要求时间复杂度为O(n)。 使用了 $O(n)$ 的空间，可以优化成 $O(1)$ 的空间 1234567891011121314151617class Solution &#123;public: // 注意转移方程是 dp[i] =max(dp[i-1]+ nums[i], nums[i]) int maxSubArray(vector&lt;int&gt;&amp; nums) &#123; int n =nums.size(); vector&lt;int&gt; dp(n, 0); dp[0] =nums[0]; int res =nums[0]; for(int i =1; i&lt; n; i++) &#123; // 因为可能出现负数的情况 dp[i] =max(dp[i-1] + nums[i], nums[i]); res =max(dp[i], res); &#125; return res; &#125;&#125;; 12345678910111213141516class Solution &#123;public: // 注意转移方程是 dp[i] =max(dp[i-1]+ nums[i], nums[i]) int maxSubArray(vector&lt;int&gt;&amp; nums) &#123; int n =nums.size(); int max_v =INT_MIN; int t =0; // 开始的时候要初始化好 for(auto num : nums) &#123; t +=num; max_v =max(max_v, t); t =max(0, t); &#125; return max_v; &#125;&#125;; 二维子数组最大和Tips: 通过枚举矩阵的上下界，然后再用一维情况的方法确定左右边界，就可以得到二维问题的解。二维数组是$ (m \times n) $， 那么该方法的时间复杂度是 $O( n^2 \times m) $.当然也可以枚举左右边界，然后使用一维情况去确定上下界，所以时间复杂度是 $ O( m \times n \times min(m ,n) )$. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162#include&lt;iostream&gt;#include&lt;cstdio&gt;#include&lt;ctime&gt;using namespace std;#define M 4#define N 4#include &lt;memory.h&gt;int maxSubArray(int *arr, int len) //最大子序列和&#123; int i, sum = arr[0], b = 0; for (i = 0; i&lt;len; ++i) &#123; if (b&gt;0) b += arr[i]; else b = arr[i]; if (b&gt;sum) sum = b; &#125; return sum;&#125;int maxSubMatrix(int n, int m, int array[M][N])&#123; int i, j, h, max, sum = -100000; int b[100]; for (i = 0; i &lt; n; i++) &#123; memset(b, 0, sizeof(b)); //初始化b[] for (j = i; j &lt; n; j++) //把第i行到第j行相加,对每一次相加求出最大值 &#123; for (h = 0; h&lt;m; h++) &#123; b[h] += array[j][h]; //二维数组压缩成一维数组，然后求最大子序列和 &#125; max = maxSubArray(b, h); if (max&gt;sum) sum = max; &#125; &#125; return sum;&#125;int main()&#123; int arr[M][N] = &#123; &#123; -15, -21,5, -12 &#125;, &#123; -7, 21, 20, 12 &#125;, &#123; 21, 0, -1, 13 &#125;, &#123; 10, 20, -10, -18 &#125; &#125;; cout &lt;&lt; "随机二维数组为：" &lt;&lt; endl; //srand(time(0)); //for (int i = 0; i &lt; M; i++) //&#123; // for (int j = 0; j &lt; N; j++) // &#123; // arr[i][j] = rand() % 50 - 25; // cout &lt;&lt; arr[i][j] &lt;&lt; " "; // &#125; // cout &lt;&lt; endl; //&#125; cout &lt;&lt; maxSubMatrix(M, N, arr) &lt;&lt; endl; system("pause"); return 0;&#125; 使用C++ 中的vector 进行改写，思路正确，但结果出错。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970#include&lt;iostream&gt;#include&lt;vector&gt;#include&lt;algorithm&gt;using namespace std;#define M 4#define N 4#define MAX 100000int maxSubarray2(vector&lt;int&gt; arr, int n)&#123; int max_num= -MAX; int cur_num =0; for (int i =0; i&lt;n; i++) &#123; cur_num += arr[i]; if (cur_num &lt; 0) cur_num =0; if( cur_num &gt; max_num) max_num =cur_num; &#125; // 这个步骤是处理整数中有正有负的情况 /* if (max_num ==0) &#123; max_num =arr[0]; for(int i =1; i&lt;n;i ++) if (arr[i] &gt;max_num) max_num =arr[i]; &#125; */ return max_num;&#125;int maxSubmatrix(int rows, int cols, vector&lt;vector&lt;int&gt;&gt; arr)&#123; int i, j, k, max=arr[0][0], sum =-10000; vector&lt;int&gt; b; for (i =0;i&lt; rows;i++) &#123; vector &lt;int&gt; b(cols, 0) ; for (j =i; j&lt; rows; j++) &#123; for(k =0;k &lt;cols;k++) b[k] += arr[j][k]; sum =maxSubarray2(b, k); if(sum &gt; max) max =sum; &#125; &#125; return max;&#125;int main()&#123; vector&lt;vector&lt;int&gt;&gt; arr = &#123; &#123; -15, -21,5, -12 &#125;, &#123; -7, 21, 20, 12 &#125;, &#123; 21, 0, -1, 13 &#125;, &#123; 10, 20, -10, -18 &#125; &#125;; //cout &lt;&lt; arr[1][1]&lt;&lt;endl; cout &lt;&lt; maxSubmatrix(M, N, arr) &lt;&lt; endl; vector&lt;int&gt; tmp =&#123;2, 0, -4, 9, -1,2&#125;; cout &lt;&lt; tmp.size()&lt;&lt;endl; cout&lt;&lt; maxSubarray2(tmp, tmp.size())&lt;&lt; endl; return 0;&#125; 把ip 地址转换成唯一的映射关系。 12345678910111213141516171819202122232425262728293031323334353637# 方法一# 这个实际上是转成了大整数,比如 0.0.0.0 转成 0; 255.255.255.255 转成 255255255255def fun2(str): res =0 for i in range(4): tmp =str[i*3:(i+1)*3] res = res*1000+ int(tmp) return resdef fun1(str): if len(str)&gt;3 or len(str) ==0: return -1; if len(str) ==3: return str if len(str) ==2: return "0"+str if(len(str) ==1): return "00"+strdef str2int(str): str1 =str.split(".") if len(str1) != 4: return -1 res ="" for i in range(4): res += fun1(str1[i]) print("to string", res) res2 =fun2(res) print("to int ", res2)if __name__ =="__main__": str =input() str2int(str)]]></content>
      <categories>
        <category>算法</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Titanic Challenge]]></title>
    <url>%2F2018%2F02%2F05%2FTitanic-Challenge%2F</url>
    <content type="text"><![CDATA[用此博客记录自己解决 Kaggle Titanic challenge过程中的个人总结。 问题描述说道Titanic(泰坦尼克号)，最熟悉莫过于Titanic(1997 film),这部由著名导演詹姆斯·卡梅隆执导，莱昂纳多·迪卡普里奥、凯特·温斯莱特领衔主演的电影，经久不衰…但是我们今天的画风不是这样的… 我们今天解决的问题是以该事件问背景，但是没有那么浪漫。关于这个案例的介绍网上有很多内容，为了避免累赘，在这里就不进行详述。总的要求：预测在这个事件中乘客是否死亡。附上对于train sets和 test sets中数据的介绍。更多详细的内容参看：https://www.kaggle.com/c/titanic 数据分析顺滑过渡到第二阶段，数据分析，对于竞赛而言，我感觉对于数据的认识的重要性完全不亚于模型的重要性。之后我们将再次提到这句话。我将结合代码进行数据分析。 pandas 原生数据分析函数1234import pandas as pdtrain =pd.read_csv(&apos;data/train.csv&apos;)test =pd.read_csv(&apos;data/test.csv&apos;)train.describe(include=&apos;all&apos;) 除了上面 train.describe()，下面这两个也是比较常用的12train.head()train.columns 因为总体的数据量比较少，所以我们选择把train set 和test set连接起来进行数据分析和处理。1combined2 = pd.concat([train_data, test_data], axis=0) 数据质量分析 缺省值对于缺省值，常用的手段就是填充，但是针对不同的数据有不同的填充手段，有的是均值填充，有的是默认值填充还有的是根据现有数据训练一个 regression进行拟合(这种情况出现在缺省的数据比较重要，对于结果的预测有比较强的相关性的时候)。1combined2.Embarked.fillna(&apos;S&apos;, inplace=True) Embardked(上船港口)不是那么能表现出和结果(survival)相关的变量，我么可以直接采用某个默认值进行填充。1combined2.Fare.fillna(np.median(combined2.Fare[combined2.Fare.notnull()]), inplace=True) Fare(船票)我们选择使用均值填充 12345678classers = [&apos;Fare&apos;,&apos;Parch&apos;,&apos;Pclass&apos;,&apos;SibSp&apos;,&apos;TitleCat&apos;,&apos;CabinCat&apos;,&apos;Sex_female&apos;,&apos;Sex_male&apos;, &apos;EmbarkedCat&apos;, &apos;FamilySize&apos;, &apos;NameLength&apos;, &apos;FamilyId&apos;]age_et = ExtraTreesRegressor(n_estimators=200)X_train = full_data.loc[full_data.Age.notnull(),classers]Y_train = full_data.loc[full_data.Age.notnull(),[&apos;Age&apos;]]X_test = full_data.loc[full_data.Age.isnull(),classers]age_et.fit(X_train,np.ravel(Y_train))age_preds = age_et.predict(X_test)full_data.loc[full_data.Age.isnull(),[&apos;Age&apos;]] = age_preds 因为在特征提取看来 age 是一个比较重要的属性（下文中使用age来进一步计算性别特征，而性别特征对于survival 是重要的因素），所以需要通过 fit来进行填充 null 值。 异常值异常值的检测12combined2.boxplot()plt.ylim(0, 1000) 异常值处理大多数情况下我们都采取忽视，但是有时候异常值中却跟结果有比较强的相关性，比如说该题目分数在0.9的一位大神在博客中使用的特征包含名字长度。这个在我一开始的特征提取中确实没有太在意名字长度也可以当作一种和结果(servival or dead)相关的特征。 重复值重复值的检测1train[train.duplicated()==True] 如果运行结果为空，那么就是没有重复值,如果有重复值，一般使用下面类似的代码都是可以去除掉的。1df.drop_duplicates() 分布特征分析1234567#分布分析fig ,ax =plt.subplots(2,2, figsize=(8,6))sns.countplot(&apos;Embarked&apos;, data =train, ax =ax[0,0])sns.countplot(&apos;Pclass&apos;, data =train, ax =ax[0,1])sns.violinplot(&apos;Survived&apos;, &apos;Age&apos;, data= train, ax =ax[1,0]).set(ylim =(-10, 80))sns.countplot(x =&apos;Survived&apos;, data= train, ax =ax[1,1])plt.tight_layout() 运行需要导入 seaborn1import seaborn as sns 这里安利一个数据可视化工具-seaborn。回正题 counplot()可以直观的看出单个数据的特征，但是我们更加关心的是数据和数据之间的关系，更准确的是数据和预测数据(survival )之间的关系。所以,我们进行相关性分析。123456plt.subplots(figsize=(10,8)) corrmat = train[train.columns[1:]].corr()sns.set(font_scale=1.5) hm = sns.heatmap(corrmat, cbar=True, annot=True, square=True, fmt=&apos;.2f&apos;, annot_kws=&#123;&apos;size&apos;: 10&#125;) plt.title(&apos;Pearson Correlation of Features&apos;, y=1.05, size=15) plt.show() 从图中可以看出，在原始数据集特征中(为什么这么说，嗯，这意味着我们下文还要进行 generate new features),Fare特征是和 survival最相关的。这从数据角度这你船票价钱越高，你生存的几率就越大(三观尽毁)。嗯，这是符合经济社会的运行规律的。我们在分析 Pearson Correlation的时候，关注的是数值的绝对值，如果是正值，表示正相关；如果是负值，表示负相关。 如果细心的小伙伴发现，这个并没有把所有的变量的相关性表示出来，是的，下文我将给出一个加强版的。 特征工程当我们对数据有了一个初步的认识，这时候就可以进行特征工程了。网上流传很广的一句话”数据特征决定了机器学习的上限，而算法优化只是尽可能逼近这个上限”，我深有体会。因为之前进行特征提取，然后在kaggle的submission score是0.73205,经过模型融合然后达到了0.78947，提高了5个百分点。当自己在思考数据特征重新进行特征提取的时候，最后的score 是0.82296.这都是以10个百分点的提高啊。所以这句话很有道理，我试图找到这句话的出处，以表示我对于版权的尊重，但是科学上网能力有限，没有找见，也许这句话来自群众的智慧吧。图：我在kaggle 的submission 和相应的score 但是我想强调的是特征提取是个很难有模板化的东西，这得看个人对于这个问题的理解和对于数据的理解，对于数据异常值的处理。并且还想说的是这个一个迭代的过程，不是一步到位的。当初步构造好自己特征之后可以使用图形化工具进行简单的分析一下。(下图是我第一次构造的特征工程的图形化)12345678910111213141516171819def correlation_heatmap(df): _ , ax = plt.subplots(figsize =(14, 12)) colormap = sns.diverging_palette(220, 10, as_cmap = True) _ = sns.heatmap( df.corr(), cmap = colormap, square=True, cbar_kws=&#123;&apos;shrink&apos;:.9 &#125;, ax=ax, annot=True, linewidths=0.1,vmax=1.0, linecolor=&apos;white&apos;, annot_kws=&#123;&apos;fontsize&apos;:12 &#125; ) plt.title(&apos;Pearson Correlation of Features&apos;, y=1.05, size=15) train2 =train1.drop([&apos;NullCabin&apos;], axis =1)correlation_heatmap(train2) 上图是第一次特征分析的结果(kaggle score: 0.73205),survival和Pclass和Fare是有较强的正相关性。 该图是第二次特征分析分析结果（kaggle score:0.82296,survival与male_adult(-0.56)、sex_male(-0.54)负相关,和female_adult(0.54)、sex_female(0.52)正相关。你的survival的概率和你的性别和年龄有关，如果你是成年女子，那么你很大的概率不会死亡(像Rose那样)；如果你是成年男子，那么你有很大概率体现英伦的绅士风度，主动(Jack那样)或者被选择死亡。瞬间想起了Titanic电影中Jack和Rose 的场景，好感人啊!!! 模型训练发现写了这么久，还没有开始训练模型。加快脚步…下面的内容以第一次训练模型为例。在建立基本模型之前我们需要先引入评价函数，以评价不同模型性能的好坏。 通过均值和方差来评价模型性能的优劣1234from sklearn import cross_validation def rmsl(clf): s = cross_validation.cross_val_score(clf, X_train, y_train, cv=5) return (s.mean(),s.std()) 建立基本模型1234567891011121314151617181920212223242526272829303132333435363738394041424344from sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis,gaussian_processNLA =[ #ensemble methods ensemble.AdaBoostClassifier(), ensemble.BaggingRegressor(), ensemble.GradientBoostingClassifier(), ensemble.RandomForestClassifier(n_estimators=60), # Gaussian process gaussian_process.GaussianProcessClassifier(), # LM linear_model.LogisticRegression(C=1.0, penalty=&apos;l1&apos;), #Navies Bayes naive_bayes.GaussianNB(), # Nearest Neighbor neighbors.KNeighborsClassifier(n_neighbors=3), # Svm svm.SVC(probability=True), svm.LinearSVC(), #Tree tree.DecisionTreeClassifier(), tree.ExtraTreeClassifier() ]#create table to compare MLAMLA_columns = [&apos;MLA Name&apos;, &apos;MLA Parameters&apos;,&apos;MLA Train Accuracy Mean&apos;, &apos;MLA Test Accuracy Mean&apos;, &apos;MLA Test Accuracy Min&apos; ,&apos;MLA Time&apos;] MLA_compare = pd.DataFrame(columns = MLA_columns) row_index = 0 for alg in MLA: #set name and parameters MLA_compare.loc[row_index, &apos;MLA Name&apos;] = alg.__class__.__name__ MLA_compare.loc[row_index, &apos;MLA Parameters&apos;] = str(alg.get_params()) #score model with cross validation: cv_results = model_selection.cross_validate(alg, X_train, y_train, cv =5,return_train_score=True) MLA_compare.loc[row_index, &apos;MLA Time&apos;] = cv_results[&apos;fit_time&apos;].mean() MLA_compare.loc[row_index, &apos;MLA Train Accuracy Mean&apos;] = cv_results[&apos;train_score&apos;].mean() MLA_compare.loc[row_index, &apos;MLA Test Accuracy Mean&apos;] = cv_results[&apos;test_score&apos;].mean() MLA_compare.loc[row_index, &apos;MLA Test Accuracy Min&apos;] = cv_results[&apos;test_score&apos;].min() #let&apos;s know the worst that can happen! row_index+=1MLA_compare.sort_values(by = [&apos;MLA Test Accuracy Mean&apos;], ascending = False, inplace = True) 当我们发现某个模型效果比较好的时候，我们仍然可以进一步调参。但是这种调参并不是每次会得到better result,有时候只是一个decent result。调参是个技术活。以上图中的 DecisionTreeClassifier为例进行调参。12345678param_grid = &#123;&apos;criterion&apos;: [&apos;gini&apos;, &apos;entropy&apos;], &apos;splitter&apos;: [&apos;best&apos;, &apos;random&apos;], &apos;max_depth&apos;: [None, 2,4,6,8,10], &apos;min_samples_split&apos;: [5,10,15,20,25], &apos;max_features&apos;: [None, &apos;auto&apos;, &apos;sqrt&apos;, &apos;log2&apos;] &#125;tune_model = model_selection.GridSearchCV(tree.DecisionTreeClassifier(), param_grid=param_grid, scoring = &apos;accuracy&apos;, cv = 5) cv_results = model_selection.cross_validate(tune_model, X_train, y_train, cv = 5) 使用的是sklearn 中的model_selection 模块，进行GridSearch，其实只是把调参过程自动化程序化。得到的结果是1230.863068608315 #train mean0.79803322024 # test mean0.77094972067 #test min 我们通过比对发现这个结果和上图的结果是稍微变差的。可视化显示各个算法的效率：1234sns.barplot(x=&apos;MLA Test Accuracy Mean&apos;, y = &apos;MLA Name&apos;, data = MLA_compare, color = &apos;m&apos;) plt.title(&apos;Machine Learning Algorithm Accuracy Score \n&apos;) plt.xlabel(&apos;Accuracy Score (%)&apos;) plt.ylabel(&apos;Algorithm&apos;) 对比之后我们选取几个效果比较“好”的模型，然后进行下一步的模型融合。12345678910111213141516171819MLA_best = [ #Ensemble Methods ensemble.AdaBoostClassifier(), # 0.76076 ensemble.BaggingClassifier(), # 0.72248 ensemble.GradientBoostingClassifier(), # 0.73684 ensemble.RandomForestClassifier(n_estimators = 60), # 0.72727 #GLM linear_model.LogisticRegression(C=1.0, penalty=&apos;l1&apos;, tol=1e-6), # 0.77990 linear_model.RidgeClassifierCV(), # 0.77033 linear_model.LogisticRegressionCV() #0.77033 ] row_index = 0 for alg in MLA_best: algname = alg.__class__.__name__ alg.fit(X_train, y_train) predictions = alg.predict(X_test) result = pd.DataFrame(&#123;&apos;PassengerId&apos;:test[&apos;PassengerId&apos;].as_matrix(), &apos;Survived&apos;:predictions.astype(np.int32)&#125;) result.to_csv(algname+&quot;.csv&quot;, index=False) # save the results row_index+=1 模型融合简单的说模型融合就是通过多个decent模型的结果通过某种方式的结合，产生了比原来单个模型better的结果。关于模型融合的详细内容，请移步另一篇文章模型融合(Ensemble learning)我们这里以stacking(二层)为例说明模型融合。1234567891011121314151617181920212223ntrain = train.shape[0] #891 ntest = test.shape[0] #418 SEED = 0 # for reproducibility NFOLDS = 5 # set folds for out-of-fold prediction kf =model_selection.KFold(n_splits=NFOLDS, random_state=SEED)# 封装算法基本操作 class SklearnHelper(object): def __init__(self, clf, seed=0, params=None): params[&apos;random_state&apos;] = seed self.clf = clf(**params) def train(self, x_train, y_train): self.clf.fit(x_train, y_train) def predict(self, x): return self.clf.predict(x) def fit(self,x,y): return self.clf.fit(x,y) def feature_importances(self,x,y): print(self.clf.fit(x,y).feature_importances_) return self.clf.fit(x,y).feature_importances_ 下面是定义五折交叉验证的方法，默认是三折。123456789101112131415161718def get_oof(clf, x_train, y_train, x_test): oof_train = np.zeros((ntrain,)) oof_test = np.zeros((ntest,)) oof_test_skf = np.empty((NFOLDS, ntest)) for i, (train_index, test_index) in enumerate(kf.split(x_train)): x_tr = x_train[train_index] y_tr = y_train[train_index] x_te = x_train[test_index] clf.train(x_tr, y_tr) oof_train[test_index] = clf.predict(x_te) oof_test_skf[i, :] = clf.predict(x_test) oof_test[:] = oof_test_skf.mean(axis=0) return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1) # 想让z变成只有一列，行数不知道多少 1234567891011121314151617181920212223242526272829303132from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier,AdaBoostClassifier, GradientBoostingClassifier# 定义四个不同的弱分类器的参数值 # Random Forest parameters rf_params = &#123; &apos;n_jobs&apos;: -1,&apos;n_estimators&apos;: 500,&apos;warm_start&apos;: True, &apos;max_depth&apos;: 6,&apos;min_samples_leaf&apos;: 2, &apos;max_features&apos; : &apos;sqrt&apos;,&apos;verbose&apos;: 0#&apos;max_features&apos;: 0.2, &#125; # Extra Trees Parameters et_params = &#123; &apos;n_jobs&apos;: -1,&apos;n_estimators&apos;:500,&apos;max_depth&apos;: 8,&apos;min_samples_leaf&apos;: 2,&apos;verbose&apos;: 0 #&apos;max_features&apos;: 0.5, &#125; # AdaBoost parameters ada_params = &#123; &apos;n_estimators&apos;: 500,&apos;learning_rate&apos; : 0.75 &#125; # Gradient Boosting parameters gb_params = &#123; &apos;n_estimators&apos;: 500,&apos;max_depth&apos;: 5,&apos;min_samples_leaf&apos;: 2, &apos;verbose&apos;: 0 #&apos;max_features&apos;: 0.2, &#125; # Support Vector Classifier parameters # svc_params = &#123; # &apos;kernel&apos; : &apos;linear&apos;,&apos;C&apos; : 0.025 # &#125; # 创建四个若分类器模型 rf = SklearnHelper(clf=RandomForestClassifier, seed=SEED, params=rf_params) et = SklearnHelper(clf=ExtraTreesClassifier, seed=SEED, params=et_params) ada = SklearnHelper(clf=AdaBoostClassifier, seed=SEED, params=ada_params) gb = SklearnHelper(clf=GradientBoostingClassifier, seed=SEED, params=gb_params) 1234567891011121314151617181920#X_train =X_train.values#X_test =X_test.values# 使用五折交叉方法分别计算出使用不同算法的预测结果，这些结果将用于Stacking的第二层预测 et_oof_train, et_oof_test = get_oof(et, X_train, y_train, X_test) # Extra Trees rf_oof_train, rf_oof_test = get_oof(rf,X_train, y_train, X_test) # Random Forest ada_oof_train, ada_oof_test = get_oof(ada, X_train, y_train, X_test) # AdaBoost gb_oof_train, gb_oof_test = get_oof(gb,X_train, y_train, X_test) # Gradient Boost rf_feature = rf.feature_importances(X_train,y_train) et_feature = et.feature_importances(X_train, y_train) ada_feature = ada.feature_importances(X_train, y_train) gb_feature = gb.feature_importances(X_train,y_train) feature_dataframe = pd.DataFrame( &#123;&apos;features&apos;: cols, &apos;Random Forest feature importances&apos;: rf_feature, &apos;Extra Trees feature importances&apos;: et_feature, &apos;AdaBoost feature importances&apos;: ada_feature, &apos;Gradient Boost feature importances&apos;: gb_feature &#125;) 接下来以第一层为为基础训练第二层12345678base_predictions_train = pd.DataFrame( &#123; &apos;RandomForest&apos;: rf_oof_train.ravel(),# # ravel函数在降维时默认是行序优先 &apos;ExtraTrees&apos;: et_oof_train.ravel(), &apos;AdaBoost&apos;: ada_oof_train.ravel(), &apos;GradientBoost&apos;: gb_oof_train.ravel() &#125;) X_train2 = np.concatenate(( et_oof_train, rf_oof_train, ada_oof_train, gb_oof_train), axis=1) X_test2 = np.concatenate(( et_oof_test, rf_oof_test, ada_oof_test, gb_oof_test), axis=1) 使用XGBoost训练第二层的数据。关于XGBoost为什么是有效的和相关的概念，请移步XGBoost123456789101112131415# XGboost import xgboost as xgbgbm = xgb.XGBClassifier( #learning_rate = 0.02, n_estimators= 2000, max_depth= 4, min_child_weight= 2, #gamma=1, gamma=0.9, subsample=0.8, colsample_bytree=0.8, objective= &apos;binary:logistic&apos;, nthread= -1, scale_pos_weight=1).fit(X_train2, y_train) predictions = gbm.predict(X_test2) 最后产生结果文件 StackingSubmission.csv12StackingSubmission = pd.DataFrame(&#123;&apos;PassengerId&apos;:test.PassengerId, &apos;Survived&apos;: predictions &#125;) StackingSubmission.to_csv(&quot;StackingSubmission.csv&quot;, index=False) # 0.78947 参考文献本文在效果可视化中借鉴该博客特征提取参看kaggle多位大神，在这里就谢过…]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[集成学习]]></title>
    <url>%2F2018%2F02%2F05%2Femsemble%2F</url>
    <content type="text"><![CDATA[将集成学习分为基本集成学习方法、高级集成学习方法和基于集成学习的算法。 Ensemble models in machine learning operate on a similar idea. They combine the decisions from multiple models to improve the overall performance. This can be achieved in various ways, which you will discover in this article. 基本集成学习方法（1）Max Voting 多人投票机制，使用所有的模型预测结果的多数，常用于分类问题。 123456789101112131415model1 = tree.DecisionTreeClassifier()model2 = KNeighborsClassifier()model3= LogisticRegression()model1.fit(x_train,y_train)model2.fit(x_train,y_train)model3.fit(x_train,y_train)pred1=model1.predict(x_test)pred2=model2.predict(x_test)pred3=model3.predict(x_test)final_pred = np.array([])for i in range(0,len(x_test)): final_pred = np.append(final_pred, mode([pred1[i], pred2[i], pred3[i]])) （2）Averaging Averaging can be used for making predictions in regression problems or while calculating probabilities for classification problems.如果是回归问题，那么求解均值；如果是分类问题，那么求解概率 12345678910111213model1 = tree.DecisionTreeClassifier()model2 = KNeighborsClassifier()model3= LogisticRegression()model1.fit(x_train,y_train)model2.fit(x_train,y_train)model3.fit(x_train,y_train)pred1=model1.predict_proba(x_test)pred2=model2.predict_proba(x_test)pred3=model3.predict_proba(x_test)finalpred=(pred1+pred2+pred3)/3 （3）Weighted Averaging 和上面的思想类似，是有权重的，权重的依据是模型的准确率等指标。如果模型效果越好，那么权重越高。 12345678910111213model1 = tree.DecisionTreeClassifier()model2 = KNeighborsClassifier()model3= LogisticRegression()model1.fit(x_train,y_train)model2.fit(x_train,y_train)model3.fit(x_train,y_train)pred1=model1.predict_proba(x_test)pred2=model2.predict_proba(x_test)pred3=model3.predict_proba(x_test)finalpred=(pred1*0.3+pred2*0.3+pred3*0.4) 高级集成学习方法这就引出了如何组合这些模型的问题。我们可以用三种主要的旨在组合弱学习器的「元算法」： bagging，该方法通常考虑的是同质弱学习器，相互独立地并行学习这些弱学习器，并按照某种确定性的平均过程将它们组合起来。 boosting，该方法通常考虑的也是同质弱学习器。它以一种高度自适应的方法顺序地学习这些弱学习器（每个基础模型都依赖于前面的模型），并按照某种确定性的策略将它们组合起来。 stacking，该方法通常考虑的是异质弱学习器，并行地学习它们，并通过训练一个「元模型」将它们组合起来，根据不同弱模型的预测结果输出一个最终的预测结果。 （1）Bagging 思想：使用同质模型和相同的数据集，大概率得到的是相同的结果，这个是后boosting 算法中使用了一种采样方式 Bootstrapping。 Bootstrapping is a sampling technique in which we create subsets of observations from the original dataset, with replacement. 算法步骤： 从训练集 $𝑆$中有放回的随机选取数据集 $𝑀(∣𝑀∣&lt;∣𝑆∣) $; 生成一个分类模型 $𝐶 $; 重复以上步骤 $m$次，得到$m$个分类模型 $𝐶_1$, $𝐶_2 $,…, $𝐶_m $; 对于分类问题，每一个模型投票决定，少数服从多数原则; 对于回归问题，取平均值。 （2）Boosting 思想：如果模型本身的准确率就不高，那么多个模型组合起来不见得好。所以Boosting 的思想是在针对上一个模型的错误来训练当下的模型。 （3）Stacking 以二层的stacking 作为讲解。 算法步骤： 1). The train set is split into 10 parts.2). A base model (suppose a decision tree) is fitted on 9 parts and predictions are made for the 10th part. This is done for each part of the train set.3). The base model (in this case, decision tree) is then fitted on the whole train dataset.4). Using this model, predictions are made on the test set.5). Steps 2 to 4 are repeated for another base model (say knn) resulting in another set of predictions for the train set and test set.6). The predictions from the train set are used as features to build a new model.7). This model is used to make final predictions on the test prediction set. 关键步骤是以第一层模型的输出结果作为第二次的输入。给出代码实例。 定义一个通用函数。123456789101112def Stacking(model,train,y,test,n_fold): folds=StratifiedKFold(n_splits=n_fold,random_state=1) test_pred=np.empty((test.shape[0],1),float) train_pred=np.empty((0,1),float) for train_indices,val_indices in folds.split(train,y.values): x_train,x_val=train.iloc[train_indices],train.iloc[val_indices] y_train,y_val=y.iloc[train_indices],y.iloc[val_indices] model.fit(X=x_train,y=y_train) train_pred=np.append(train_pred,model.predict(x_val)) test_pred=np.append(test_pred,model.predict(test)) return test_pred.reshape(-1,1),train_pred 第一层有两个基本的模型：决策树和K最近邻。 1234model1 = tree.DecisionTreeClassifier(random_state=1)test_pred1 ,train_pred1=Stacking(model=model1,n_fold=10, train=x_train,test=x_test,y=y_train)train_pred1=pd.DataFrame(train_pred1)test_pred1=pd.DataFrame(test_pred1) 1234model2 = KNeighborsClassifier()test_pred2 ,train_pred2=Stacking(model=model2,n_fold=10,train=x_train,test=x_test,y=y_train)train_pred2=pd.DataFrame(train_pred2)test_pred2=pd.DataFrame(test_pred2) 第一层的两个模型得到了predict的值，然后将结果拼接起来作为第二层模型的输入数据集，第二层模型的label 还是原始训练数据集的label。 123456df = pd.concat([train_pred1, train_pred2], axis=1) # 基本模型prediction 的结果当做训练的输入df_test = pd.concat([test_pred1, test_pred2], axis=1) # 为了保证同分布，这里对test 数据集也做相同的转换model = LogisticRegression(random_state=1)model.fit(df,y_train) # 数据的label 还是作为第一层模型的label model.score(df_test, y_test) # 最后是模型的输出 在 test 数据集上的预测，需要经过两层模型：首先输入到（决策树和k最近邻）的模型中，其结果再输入到第二层模型决策树中，决策树的预测值就是最后的结果。 每一轮根据上一轮的分类结果动态调整每个样本在分类器中的权重，训练得到k个弱分类器，他们都有各自的权重，通过加权组合的方式得到最终的分类结果(综合所有的基模型预测结果)。 （4）Blending Blending 相比于Stacking而言，Blending 是在训练集上train，在验证集和测试集上prediction，然后使用验证集和测试集的prediction作为 features 去学习下一个模型。 算法步骤： 1). The train set is split into training and validation sets.2). Model(s) are fitted on the training set.3). The predictions are made on the validation set and the test set.4). The validation set and its predictions are used as features to build a new model.5). This model is used to make final predictions on the test and meta-features. 同样给出了sample codes 12345678910111213model1 = tree.DecisionTreeClassifier()model1.fit(x_train, y_train)val_pred1=model1.predict(x_val)test_pred1=model1.predict(x_test)val_pred1=pd.DataFrame(val_pred1)test_pred1=pd.DataFrame(test_pred1)model2 = KNeighborsClassifier()model2.fit(x_train,y_train)val_pred2=model2.predict(x_val)test_pred2=model2.predict(x_test)val_pred2=pd.DataFrame(val_pred2)test_pred2=pd.DataFrame(test_pred2) 第二层使用了逻辑回归在test set 上进行预测。 123456df_val=pd.concat([x_val, val_pred1,val_pred2],axis=1)df_test=pd.concat([x_test, test_pred1,test_pred2],axis=1)model = LogisticRegression()model.fit(df_val,y_val)model.score(df_test,y_test) 基于集成学习的算法Boosting 和Bagging 是最常使用的两类集成算法。Bagging algorithms的代表是Random fores。Boosting algorithms有以下几种： AdaBoost GBM XGBM Light GBM CatBoost （1）Random Forest Random Forest 的算法步骤：1). Random subsets are created from the original dataset (bootstrapping).2). At each node in the decision tree, only a random set of features are considered to decide the best split.3). A decision tree model is fitted on each of the subsets.4). The final prediction is calculated by averaging the predictions from all decision trees. 总结来说，随机森林随机 选择数据点和特征，然后组成了多棵树的集合（森林） 常见的超参数1 ). n_estimators (子树的个数) It defines the number of decision trees to be created in a random forest. Generally, a higher number makes the predictions stronger and more stable, but a very large number can result in higher training time. 2 ). max_features (使用最多特征的数量进行建立树) It defines the maximum number of features allowed for the split in each decision tree. Increasing max features usually improve performance but a very high number can decrease the diversity of each tree. 3 ). max_depth ( 是一个树的最大深度，也是所有树的最大深度) The maximum depth of the tree. 4 ). min_samples_leaf This defines the minimum number of samples required to be at a leaf node. Smaller leaf size makes the model more prone to capturing noise in train data. （2）AdaBoost Adaptive boosting or AdaBoost is one of the simplest boosting algorithms. Usually, decision trees are used for modelling. Multiple sequential models are created, each correcting the errors from the last model. AdaBoost assigns weights to the observations which are incorrectly predicted and the subsequent model works to predict these values correctly.提升策略主要是从分错类的样本角度提升，通过给予更大的权重。下一个模型预测的时候，着重该样本。 （3）Gradient Boosting (GBM) Gradient Boosting or GBM is another ensemble machine learning algorithm that works for both regression and classification problems. GBM uses the boosting technique, combining a number of weak learners to form a strong learner. Regression trees used as a base learner, each subsequent tree in series is built on the errors calculated by the previous tree.提升策略是从梯度角度考虑，使用决策树模型减少loss，进而提高模型的预测能力。 优点 在分布稠密的数据集上，泛化能力和表达能力都很好，这使得GBDT在Kaggle的众多竞赛中，经常名列榜首。 缺点 GBDT在高维稀疏的数据集上，表现不如支持向量机或者神经网络。（所以说这种树的模型是有利于处理连续数值， 如果是 one-hot 就不建议使用gbdt） GBDT在处理文本分类特征问题上，相对其他模型的优势不如它在处理数值特征时明显。 （4）XGBoost XGBoost has high predictive power and is almost 10 times faster than the other gradient boosting techniques. It also includes a variety of regularization which reduces overfitting and improves overall performance. Hence it is also known as ‘regularized boosting‘ technique.提升速度和使用正则方式处理过拟合。 主要采用以下的技术：1). Regularization: Standard GBM implementation has no regularisation like XGBoost. Thus XGBoost also helps to reduce overfitting.2). Parallel Processing: XGBoost implements parallel processing and is faster than GBM . XGBoost also supports implementation on Hadoop.3). High Flexibility: XGBoost allows users to define custom optimization objectives and evaluation criteria adding a whole new dimension to the model.4). Handling Missing Values: XGBoost has an in-built routine to handle missing values.5). Tree Pruning: XGBoost makes splits up to the max_depth specified and then starts pruning the tree backwards and removes splits beyond which there is no positive gain.6). Built-in Cross-Validation: XGBoost allows a user to run a cross-validation at each iteration of the boosting process and thus it is easy to get the exact optimum number of boosting iterations in a single run. GBDT与XGboost联系与区别 (1) GBDT是机器学习算法，XGBoost是该算法的工程实现。(2) 在使用CART作为基分类器时，XGBoost显式地加入了正则项来控制模型的复杂度，有利于防止过拟合，从而提高模型的泛化能力。(3) GBDT在模型训练时只使用了代价函数的一阶导数信息，XGBoost对代价函数进行二阶泰勒展开，可以同时使用一阶和二阶导数。(4) 传统的GBDT采用CART作为基分类器，XGBoost支持多种类型的基分类器，比如线性分类器。(5) 传统的GBDT在每轮迭代时使用全部的数据，XGBoost则采用了与随机森林相似的策略，支持对数据进行采样。(6) 传统的GBDT没有设计对缺失值进行处理，XGBoost能够自动学习出缺失值的处理策略。 （6）Light GBM Light GBM beats all the other algorithms when the dataset is extremely large. Compared to the other algorithms, Light GBM takes lesser time to run on a huge dataset.LightGBM is a gradient boosting framework that uses tree-based algorithms and follows leaf-wise approach while other algorithms work in a level-wise approach pattern. The images below will help you understand the difference in a better way. 效果上使用基于叶子的树的生长方式，而非层次的生成方式。 （7）CatBoost Handling categorical variables is a tedious process, especially when you have a large number of such variables. When your categorical variables have too many labels (i.e. they are highly cardinal), performing one-hot-encoding on them exponentially increases the dimensionality and it becomes really difficult to work with the dataset.CatBoost can automatically deal with categorical variables and does not require extensive data preprocessing like other machine learning algorithms. Here is an article that explains CatBoost in detail. 从名字上就知道在处理类别信息很多的数据集中，不需要预处理，直接使用。 代码实现1). A Comprehensive Guide to Ensemble Learning (with Python codes)：关于集成学习全面的讲解。2). How to Develop a Stacking Ensemble for Deep Learning Neural Networks in Python With Keras：搭建的是CNN 的网络。3). How to Implement Stacked Generalization (Stacking) From Scratch With Python： 仅仅使用了python，没有使用其他的各种机器学习的库。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>ensemble</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Binary Search]]></title>
    <url>%2F2018%2F01%2F23%2Fbinary-search%2F</url>
    <content type="text"><![CDATA[介绍二分查找中的三个模板，以 LeetCode 中的习题为例。 首先介绍一下二分查找中使用的术语： 目标 Target —— 你要查找的值索引 Index —— 你要查找的当前位置左、右指示符 Left，Right —— 我们用来维持查找空间的指标中间指示符 Mid —— 我们用来应用条件来确定我们应该向左查找还是向右查找的索引 二分的思想：每次把区间缩小一半，在缩小的过程中一定要保证，答案是在区间里面的，最后答案的区间长度为1的时候，就得到了目标区间。70%的题目都具有某种单调性。（具有单调性，那么是可以使用二分解决；但是使用二分的题目不一定都具有单调性）95% 的题目是存在二段性。 模板1模板 1 是二分查找的最基础和最基本的形式。这是一个标准的二分查找模板，是非常基础简单的二分查找。模板 1 用于查找可以通过访问数组中的单个索引来确定的元素或条件。模版 1 不需要后处理，因为每一步中，你都在检查是否找到了元素。如果到达末尾，则知道未找到该元素。 语法关键： 初始条件：left = 0, right = length-1终止：left &gt; right向左查找：right = mid-1向右查找：left = mid+1 模版#1 对应的例题为：LeetCode69 x 的平方根LeetCode374 猜数字大小LeetCode33 搜索旋转排序数组 模板2模板 2 是二分查找的高级模板。它用于查找需要访问数组中当前索引及其直接右邻居索引的元素或条件。查找条件需要访问元素的直接右邻居。使用元素的右邻居来确定是否满足条件，并决定是向左还是向右。保证查找空间在每一步中至少有 2 个元素。需要进行后处理。 当你剩下 1 个元素时，循环 / 递归结束。 需要评估剩余元素是否符合条件。 语法关键： 初始条件：left = 0, right = length终止：left == right向左查找：right = mid向右查找：left = mid+1 模版#2 对应的例题为：LeetCode278 第一个错误的版本LeetCode75 寻找峰值LeetCode159 寻找旋转排序数组中的最小值 LeetCode69 x 的平方根 LeetCode69 x 的平方根 实现 int sqrt(int x) 函数。 需要处理一下 int 类型数字越界的问题 123456789101112131415class Solution &#123;public: typedef long long ll; int mySqrt(int x) &#123; ll l =0, r =x; while(l &lt;r) &#123; ll mid = l +1ll+r &gt;&gt;1; if(mid *mid &lt;= x) l =mid; else r =mid -1; &#125; return l; &#125;&#125;; 12345678910111213141516class Solution &#123;public: // 使用二分进行求解, 时间复杂度是logn // 二分有两种更新方式， 一种是 l =mid, r =mid -1; 一种是 l =mid +1, r =mid 这个记住也是不难的，r 只能是mid 或者是 mid -1； l 只能是mid 或者是mid +1 // 并且这种mid +-1 操作只能出现一次。当出现mid -1 那么就需要设置一下 mid = (l +r +1ll )/2 ，防止陷入死循环。通过观察可以知道，都是需要出席 +1 这种操作 int mySqrt(int x) &#123; int l =0, r =x; while(l &lt; r) &#123; int mid =(l +r +1ll) /2; if(mid &lt;= x/mid) l =mid; else r =mid -1; &#125; return l; &#125;&#125;; python解法 12345678910111213class Solution(object): def mySqrt(self, x): """ :type x: int :rtype: int """ l, r =0, x while l &lt; r: mid = l+r +1 &gt;&gt;1 if(mid *mid &lt;= x): l =mid else: r =mid -1 return l 374. Guess Number Higher or Lower 这个题目 有bug 吧，说反了。 12345678910111213141516171819// Forward declaration of guess API.// @param num, your guess// @return -1 if my number is lower, 1 if my number is higher, otherwise return 0int guess(int num);class Solution &#123;public: int guessNumber(int n) &#123; int l =1, r =n; while(l &lt;r) &#123; int mid = (l+r) /2; if(guess(mid) ==0) return mid; else if(guess(mid) == 1) l =mid +1; else r = mid -1; &#125; return l; &#125;&#125;; c++ 中使用 二分解法，很容易出现int 溢出的这种现象。所以注意一下使用long long进行解决12345678910111213141516171819202122// Forward declaration of guess API.// @param num, your guess// @return -1 if my number is lower, 1 if my number is higher, otherwise return 0int guess(int num);class Solution &#123;public: // 凡是能够分块处理的，那么都是可以使用二分的进行处理的 // 一部分满足某种条件，一部分满足另一种条件 typedef long long ll; int guessNumber(int n) &#123; ll l =1, r =n; while(l &lt;r) &#123; ll mid =l +r &gt;&gt;1; if(guess(mid) == 0) return mid; else if (guess(mid) ==-1) r =mid -1; else l =mid +1; //cout &lt;&lt; l &lt;&lt;" "&lt;&lt; r&lt;&lt; endl; &#125; return l; &#125;&#125;; python 解法 1234567891011121314151617181920# The guess API is already defined for you.# @param num, your guess# @return -1 if my number is lower, 1 if my number is higher, otherwise return 0# def guess(num):class Solution(object): def guessNumber(self, n): """ :type n: int :rtype: int """ l, r = 1, n while l&lt;r: mid = l+r &gt;&gt;1 if( guess(mid) ==0): return mid elif (guess(mid) ==1): l =mid +1 else: r =mid -1 return l Search in Rotated Sorted Array 这个题目非常的经典的，使用了一个二分，时间复杂度是 $logn$， 解释可以参考这里 1234567891011121314151617181920212223242526272829class Solution &#123;public: //数据特点， 分成了左右两个递增序列， nums[0] &gt; 后面的递增序列的 int search(vector&lt;int&gt;&amp; nums, int target) &#123; int n =nums.size(); if (n ==0) return -1; int l=0, r =nums.size() -1; while(l &lt;r) &#123; int mid = (l+r) &gt;&gt;1; if(nums[mid] &gt;= nums[0]) &#123; if( target &gt;= nums[0] &amp;&amp; target &lt;= nums[mid]) r =mid ; else l =mid +1; &#125; else &#123; if(target &lt;nums[0] &amp;&amp; target &gt;nums[mid]) l =mid +1; else r =mid; &#125; &#125; return target == nums[l] ? l : -1; &#125;&#125;; 这个是属于 python实现 123456789101112131415161718192021222324class Solution(object): # 这个是需要处理两个if 的判断，一个是 mid 和 0 的关系，一个是target 和mid 的关系， # 嵌套的if 判断 def search(self, nums, target): """ :type nums: List[int] :type target: int :rtype: int """ l, r =0, len(nums) while l &lt;r: mid = l +r &gt;&gt;1 if(nums[mid] &gt;= nums[0]): if target &gt;= nums[0] and target &lt;= nums[mid]: r =mid else: l =mid +1 else: if target &lt;nums[0] and target &gt; nums[mid]: l =mid+1 else: r =mid if(l &lt;len(nums) and nums[l] ==target): return l return -1 so easy 1234567891011121314151617181920212223242526272829class Solution &#123;public: int search(vector&lt;int&gt;&amp; nums, int target) &#123; if(nums.size() ==0) return -1; int n =nums.size() ; int l =0, r =n -1; while(l &lt;r) &#123; int mid = l +r &gt;&gt;1; if(nums[mid] &gt;= nums[0]) &#123; if(target &gt;= nums[0] &amp;&amp; target &lt;= nums[mid]) r =mid; else l =mid +1; &#125; else &#123; if(target &lt; nums[0] &amp;&amp; target &gt; nums[mid]) l =mid +1; else r =mid; &#125; &#125; if(l &lt;n &amp;&amp; nums[l] ==target) return l; return -1; &#125;&#125;; Search in Rotated Sorted Array II 这种做法是比较好的， LeetCode平台上 两个 90%，可以近似的看成 $logn$ 123456789101112131415161718192021222324252627282930313233343536class Solution &#123;public: // 这个是33 有点类似，只是多了可能存在重复的数字，所以要保证二分的条件，保证的是严格的单调性 bool search(vector&lt;int&gt;&amp; nums, int target) &#123; int n =nums.size(); if(n ==0) return false; if(target == nums[0]) return true; int k =n -1; while(k &gt;=0 &amp;&amp; nums[k] ==nums[0]) k --; n =k; // 然后现在就是不 int l =0, r =n; while(l &lt;r) &#123; int mid = (l+r) &gt;&gt;1; if(nums[mid] &gt;= nums[0]) &#123; if( target &gt;= nums[0] &amp;&amp; target &lt;= nums[mid]) r =mid ; else l =mid +1; &#125; else &#123; if(target &lt;nums[0] &amp;&amp; target &gt;nums[mid]) l =mid +1; else r =mid; &#125; &#125; return nums[l] ==target? true:false; &#125;&#125;; 这个和上一道题目是类似的。 在进行 – 操作的时候，一定要判断一下是否能够–， 比如代码中的 i&gt;=0 然后才能进行 –； 如果 i&lt;n ，那么才能进行 ++操作 1234567891011121314151617181920212223242526272829303132class Solution &#123;public: // 这个和上面的题目的区别，可能有重复的元素， 所以需要先处理一下。保证前一部分的元素是绝对大于后面的元素的 bool search(vector&lt;int&gt;&amp; nums, int target) &#123; if(nums.size() ==0) return false; int n =nums.size(); int l =0, r =n -1; while(r &gt;=0 &amp;&amp; nums[0] ==nums[r]) r --; // 那么现在之前的方式就可以使用了 while(l &lt;r) &#123; int mid = l +r &gt;&gt;1; if(nums[mid] &gt;= nums[0]) &#123; if(target &gt;= nums[0] &amp;&amp; target &lt;= nums[mid]) r =mid; else l =mid +1; &#125; else &#123; if(target &lt; nums[0] &amp;&amp; target &gt; nums[mid]) l =mid +1; else r =mid; &#125; &#125; if(l &lt;n &amp;&amp; nums[l] ==target) return true; return false; &#125;&#125;; Find Peak Element 12345678910111213141516class Solution &#123;public: // 这个关键是要 会使用二分进行求解 int findPeakElement(vector&lt;int&gt;&amp; nums) &#123; int n =nums.size() ; if(n ==0) return -1; int l =0, r =n -1; while(l &lt;r) &#123; int mid = l +r+1 &gt;&gt;1; if(nums[mid] &gt; nums[mid -1]) l =mid; else r =mid -1; &#125; return l; &#125;&#125;; 如果说知道这个考点是 二分，那么还是很简单的，在判断条件时候，需要注意下. 123456789101112131415class Solution &#123;public: int findPeakElement(vector&lt;int&gt;&amp; nums) &#123; int n =nums.size(); int l =0, r =n-1; while(l &lt;r) &#123; int mid =l +r &gt;&gt;1; if(mid +1 &lt;n &amp;&amp; nums[mid] &lt;= nums[mid +1]) l =mid +1; else r =mid; &#125; return l; &#125;&#125;; Find First and Last Position of Element in Sorted Array 划分结点的选择，一定是假定mid 就是那个能够划分成两个部分的结点。好好理解一下 12345678910111213141516171819202122232425262728293031323334class Solution &#123;public: // 自己写这种函数的的时候，一定要以 mid 作为分裂点看 vector&lt;int&gt; searchRange(vector&lt;int&gt;&amp; nums, int target) &#123; int n =nums.size(); if (n ==0) return vector&lt;int&gt;&#123;-1, -1&#125;; int l =0, r =n-1; while(l &lt;r) &#123; int mid = l +r &gt;&gt;1; if(nums[mid] &lt; target) l =mid+1 ; else r =mid ; &#125; //cout &lt;&lt; l &lt;&lt; endl; if(nums[l] != target) return vector&lt;int&gt;&#123;-1, -1&#125;; int left =l; l =0, r =n-1; while(l &lt;r) &#123; int mid =l +r +1&gt;&gt;1; if(nums[mid]&gt; target ) r =mid -1; else l =mid ; &#125; int right =l; if(left&gt;=0 &amp;&amp; l&lt; n) return vector&lt;int&gt;&#123;left, right&#125;; return vector&lt;int&gt;&#123;-1, -1&#125;; &#125;&#125;; 需要注意一些细节，其他的都还好.注意可以直接使用 vector res{-1,-1} 这样的形式进行初始化 1234567891011121314151617181920212223242526272829303132class Solution &#123;public: // 有重复的元素，找到重复元素的首尾，使用两次 二分 // 起点的左边是严格小于 element, 右边是大于等于; // 终点的右边是严格大于 element，左边是小于等于 vector&lt;int&gt; searchRange(vector&lt;int&gt;&amp; nums, int target) &#123; vector&lt;int&gt; res&#123;-1, -1&#125;; if(nums.size() ==0) return res; int n =nums.size(); int l =0, r =n -1; while(l &lt;r) &#123; int mid = l+r &gt;&gt;1; if(nums[mid] &lt; target) l =mid +1; else r =mid; &#125; if(l &gt;= n || nums[l] != target) return res; int left = l; l =0, r =n -1; while(l &lt;r) &#123; int mid = l+r+1 &gt;&gt;1; if(nums[mid] &gt; target) r =mid-1; else l =mid; &#125; if(l &gt;=n) return res; return vector&lt;int&gt;&#123;left, l&#125;; &#125;&#125;; LeetCode278 第一个错误的版本 由于每个版本都是基于之前的版本开发的，所以错误的版本之后的所有版本都是错的。你可以通过调用 bool isBadVersion(version) 接口来判断版本号 version 是否在单元测试中出错。实现一个函数来查找第一个错误的版本。你应该尽量减少对调用 API 的次数。 题目非常简单，直接套用模板#2即可。此题与模板#1的最大区别就在于，right不能修改为mid-1，而必须修改为mid。因为当如果mid是错误产品，无法判断第一个错误版本在mid之前，还是就是当前mid。这是模板#2与模板#1的最大不同。 12345678910111213class Solution &#123;public: int firstBadVersion(int n) &#123; long left = 0; long right = n; while(left&lt;right)&#123; long mid = (left+right)/2; //计算中点 if(isBadVersion(mid)==false) left = mid+1; //如果mid是正常产品，证明第一个错误产品在右侧 if(isBadVersion(mid)==true) right = mid; //如果mid是错误产品，证明第一个错误产品在是自己或者在左侧 &#125; return left; &#125;&#125;; Find Minimum in Rotated Sorted Array II 如果有重复的元素，那么是从右边开始进行比较。好好考虑这种合理性。 123456789101112131415161718class Solution &#123;public: // 对于这种 duplicates 是很容出来的 int findMin(vector&lt;int&gt;&amp; arr) &#123; int n =arr.size(); if(n ==1) return arr[0]; if(arr[0] &lt; arr.back()) return arr[0]; int l =0, r =n -1; while(l &lt;r) &#123; int mid = l +r &gt;&gt;1; if(arr[mid] &gt; arr[r]) l =mid +1; else if(arr[mid] &lt; arr[r]) r =mid; else r --; &#125; return arr[l]; &#125;&#125;; 解题的思路，先是处理主题的内容，使用二分的做法，然后处理一个case 的东西，比如 重复的元素或者是正序的情况。注意 r &gt;0 而不是 r&gt;=0 123456789101112131415161718192021class Solution &#123;public: // 在旋转数组中寻找最小的值 int findMin(vector&lt;int&gt;&amp; nums) &#123; int n =nums.size(); if(n ==1) return nums[0]; int l=0, r =n -1; // 处理重复的, r &gt; 0 这个很重要呀，不是 &gt;=0 while(r &gt;0 &amp;&amp;nums[r] ==nums[l]) r --; // 处理正序的情况 if(nums[l] &lt; nums[r]) return nums[l]; while(l &lt;r) &#123; int mid =l +r &gt;&gt;1; if(nums[mid] &gt;= nums[0]) l =mid +1; else r =mid; &#125; return nums[l]; &#125;&#125;; 对于二分的解法，时间复杂度是 $O(logn)$， 因为每次都是可以去掉一部分数字。 123456789101112131415161718// Forward declaration of isBadVersion API.bool isBadVersion(int version);class Solution &#123;public: // 如果第 i 个版本是错误的，那么 i+1 也一定是错误的，所以 i 构成了二分的条件 int firstBadVersion(int n) &#123; int l =1, r =n; while(l &lt;r) &#123; int mid = l +0ll +r &gt;&gt;1; if(isBadVersion(mid)) r =mid ; else l =mid +1; &#125; return l; &#125;&#125;; isBadVersion 时间复杂度是$O(logn)$ 123456789101112131415161718// Forward declaration of isBadVersion API.bool isBadVersion(int version);class Solution &#123;public: // 二分的本质是可以分块查找，找到一个分界点，左边是一种情况，右边是另一种情况。 //mid 是坏的版本，r =mid， 如果mid 是好的版本， l =mid +1 （这种加一 or 之间等于是根据实际问题作出的选择） // 然后根据这种情况，去选择第一个或者是第二个版本的模板 int firstBadVersion(int n) &#123; int l =1, r =n; while(l &lt;r) &#123; int mid = (0ll+l +r )/2; // 这个可能产生溢出的问题， 注意细节，如果 0ll 放到了最后，那么同样是会溢出，因为三个数相加本质上是两个数先相加然后加上第三个数 if(isBadVersion(mid)) r =mid; else l =mid +1; &#125; return l; &#125;&#125;; Find Minimum in Rotated Sorted Array 123456789101112131415161718192021222324252627282930class Solution &#123;public: // 最小值前面的数字都满足 nums[i] &gt;= nums[0]， 最小值后面的数字都不满足 // 这个就形成了分块（可以使用二分） // 如果 nums[mid] &gt;= nums[0] , l =mid +1, r =mid int findMin(vector&lt;int&gt;&amp; nums) &#123; // 注意这个是严格的大于号，因为没有重复元素，所以可以这么操作 if(nums.back() &gt; nums[0]) return nums[0]; // 这个是边界问题的一种处理方法， 这个是一种方式（关键是不影响时间复杂度的，常数级别的） /* if(nums.size() &lt;=5) &#123; int min_v =INT_MAX; for(auto u : nums) min_v =min(min_v, u); return min_v; &#125; */ int l =0, r =nums.size() -1; while(l &lt;r) &#123; int mid =(l +r )/2; if( nums[mid] &gt;=nums[0]) l =mid +1; else r =mid; &#125; return nums[l]; &#125;&#125;; 题目非常简单，直接套用模板#2即可。此题与模板#1的最大区别就在于，right不能修改为mid-1，而必须修改为mid。因为当如果mid是错误产品，无法判断第一个错误版本在mid之前，还是就是当前mid。这是模板#2与模板#1的最大不同。 模板3模板 3 是二分查找的另一种独特形式。它用于搜索需要访问当前索引及其在数组中的直接左右邻居索引的元素或条件。搜索条件需要访问元素的直接左右邻居。使用元素的邻居来确定它是向右还是向左。保证查找空间在每个步骤中至少有 3 个元素。需要进行后处理。 当剩下 2 个元素时，循环 / 递归结束。 需要评估其余元素是否符合条件。 对应的leetcode 例题LeetCode34 在排序数组中查找元素的第一个和最后一个位置LeetCode658 找到 K 个最接近的元素 语法关键： 初始条件：left = 0, right = length-1终止：left + 1 == right向左查找：right = mid向右查找：left = mid binarySearch 模板题 12345678910111213141516171819202122int binarySearch(vector&lt;int&gt;&amp; nums, int target)&#123; if (nums.size() == 0) return -1; int left = 0, right = nums.size() - 1; while (left + 1 &lt; right)&#123; // Prevent (left + right) overflow int mid = left + (right - left) / 2; if (nums[mid] == target) &#123; return mid; &#125; else if (nums[mid] &lt; target) &#123; left = mid; &#125; else &#123; right = mid; &#125; &#125; // Post-processing: // End Condition: left + 1 == right if(nums[left] == target) return left; if(nums[right] == target) return right; return -1;&#125; 来个模板, 所谓的访问文件保留几个元素，就是包括与否mid 的意思。如果 left 和 right 都可以为 mid，那么就是三个访问空间；如果只有 left or right 可以= mid，那么就是两个访问空间；如果 left =mid -1 且 right =mid +1 那么只有一个访问空间。同样三个条件依次为 left &lt;= right, left &lt; right 和 left +1 &lt; right 保证访问空间是可以 access 的。 总结如下： Find K Closest Elements 这个算法太精妙了，真的是难以理解了。总的时间复杂度 $O(log(n -k) +k)$123456789101112131415class Solution &#123;public: //时间复杂度 O(log (n-k) +k) vector&lt;int&gt; findClosestElements(vector&lt;int&gt;&amp; arr, int k, int x) &#123; int n = arr.size(); int l =0, r =n -k; while( l&lt; r) &#123; int mid = l +r &gt;&gt;1; if(x -arr[mid] &gt; arr[mid+k] -x) l =mid +1; else r =mid; &#125; return vector&lt;int&gt;(arr.begin() +l, arr.begin() +l+k); &#125;&#125;; 关键是是二分的条件比较难想，可以使用极端的思路， 如果 x - arr[mid] 之间的距离是大于 arr[mid+k] -x 那么这个是需要调整 l 的距离，然后靠近mid` ;反之，也是成立的。 12345678910111213141516class Solution &#123;public: // 使用二分的思想， 时间复杂度是logn 的 vector&lt;int&gt; findClosestElements(vector&lt;int&gt;&amp; arr, int k, int x) &#123; int n =arr.size() ; int l =0, r = n -k; while(l &lt;r) &#123; int mid =l +r &gt;&gt;1; if(x - arr[mid] &gt; arr[mid+k] -x) l =mid +1; else r =mid; &#125; // 这种写法也是需要多加学习的 return vector&lt;int&gt;&#123;arr.begin() +l, arr.begin() +l +k&#125;; &#125;&#125;; 浮点数二分算法 浮点二分不存在 +1 还是 -1 的操作，需要注意的是精度问题。 数的三次方根 double 和 float 的区别：float ：单精度浮点数在机内占4个字节，用32位二进制描述。double：双精度浮点数在机内占8个字节，用64位二进制描述。 1234567891011121314151617#include&lt;iostream&gt;using namespace std;double x;int main()&#123; cin &gt;&gt;x; double l = -10000, r = 100000; // 如果是保留6 位消失，那么使用 1e-8，如果是2位，那么是 1e-4 while(r -l &gt; 1e-8) &#123; double mid = (l+r) /2; if(mid *mid *mid &gt;x) r =mid; else l =mid; &#125; printf(&quot;%.6f&quot;, l);// 输出的格式 %.6f 表示double 类型 return 0;&#125; 1234567891011121314151617#include&lt;bits/stdc++.h&gt;using namespace std;int main()&#123; double n; cin &gt;&gt;n; double l =-10000, r = 10000; while(r -l &gt; 1e-8) &#123; double mid = (l+r) /2; if(mid *mid *mid &gt; n) r =mid; else l =mid; &#125; printf("%.6f\n", l); return 0;&#125; 参考文献： 数据结构也不难：二分查找模版与例题 python 对于 binary search 的支持包函数 bisect 1234567891011121314151617import bisect L = [1,3,3,6,8,12,15]x = 3 x_insert_point = bisect.bisect_left(L,x) #在L中查找x，x存在时返回x左侧的位置，x不存在返回应该插入的位置..这是3存在于列表中，返回左侧位置１print x_insert_point x_insert_point = bisect.bisect_right(L,x) #在L中查找x，x存在时返回x右侧的位置，x不存在返回应该插入的位置..这是3存在于列表中，返回右侧位置３ print x_insert_point x_insort_left = bisect.insort_left(L,x) #将x插入到列表L中，x存在时插入在左侧print L x_insort_rigth = bisect.insort_right(L,x) #将x插入到列表L中，x存在时插入在右侧 防止越界的tips：change “mid = (low + high) / 2 “ -&gt; “mid = low + (high - low) / 2 “或者使用 ”mid = (low + high) &gt;&gt; 1“，]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>binary-search</tag>
      </tags>
  </entry>
</search>
