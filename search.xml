<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Dynamic Programming]]></title>
    <url>%2F2019%2F08%2F29%2Fdynamic-programming%2F</url>
    <content type="text"><![CDATA[动态规划专题，题目来源于LeetCode。主要使用c++ 实现，有的会用python 实现。 动态规划的求解需要考虑三个问题： 状态怎么表示 怎么计算每个状态 怎么初始化 Unique Paths II 原题连接 对于动态规划，需要考虑三个问题： 状态怎么表示： f[i][j] 表示是(i, j) 这点的方案数 怎么计算下一步： f[i][j] 只有两个来源，一种是从上面走下来，一种是从左边走下来 怎么初始化： dp很重要的一点： 使用到的状态之前都已经计算过。针对这道题目，求解方案数，有可能最后的结果超过了int 的范围，所以使用long long应该是比较好的。 12345678910111213141516171819202122232425262728293031323334class Solution &#123;public: // f[i][j] 表示(i, j) 这个点路径的个数 // 转移 f[i][j] =f[i-1][j] or f[i][j-1] 这样的方式 // 初始化, 0 if nums[i][j] ==1 , else 1 int uniquePathsWithObstacles(vector&lt;vector&lt;int&gt;&gt;&amp; nums) &#123; if(!nums.size() || !nums[0].size()) return 0; int n =nums.size(), m =nums[0].size(); if (nums[0][0]) return 0; vector&lt;vector&lt;long long&gt;&gt; f(n, vector&lt;long long&gt;(m)); // 默认初始为0， 因为这个是在 public 中 f[0][0] =1;// 这个是从左上角开始的, 因为上文已经判断case，所以可以直接进行初始化 for(int i =0; i&lt; n; i++) &#123; for(int j =0; j&lt;m; j++) &#123; if(i || j) f[i][j] =0;// 只有在 非 if(!nums[i][j]) &#123; if(i) f[i][j] += f[i-1][j] ; if(j) f[i][j] += f[i][j-1]; &#125; &#125; &#125; return f[n-1][m-1]; &#125;&#125;; triangle LeetCode 版本 12345678910111213141516171819202122class Solution &#123;public: // 实现的时候，如何进行遍历是，我的弱点。 // 这种三角形在(i, j) 上是有特点的，因为类似一种直角三角形 int minimumTotal(vector&lt;vector&lt;int&gt;&gt;&amp; triangle) &#123; int n =triangle.size(); // 滚动数组 vector&lt;int&gt; f(triangle[n-1].begin(), triangle[n-1].end()), g(n);// f()是拷贝， g是初始化 // cpp 语法，这里是没有 triangle[-1] 表示的，不是python 中的 for(int i =n-2; i&gt;=0; i--) &#123; for(int j =0; j&lt;=i; j++) // 直角三角形的特点 &#123; g[j] =min(f[j], f[j+1]) +triangle[i][j]; &#125; f =g; &#125; return f[0]; &#125;&#125;; 单机版 1234567891011121314151617181920212223242526272829#include&lt;iostream&gt;#include&lt;vector&gt;using namespace std;int main()&#123; int n; cin &gt;&gt;n; vector&lt;vector&lt;int&gt;&gt; arr(n, vector&lt;int&gt;(n)); for(int i =0;i&lt;n; i++) for(int j =0; j&lt;=i; j++) cin&gt;&gt; arr[i][j]; vector&lt;int&gt; f(arr[n-1].begin(), arr[n-1].end()), g(n);// 如何去理解 g(n) 作为一种备胎，滚动数组 for(int i =n-2; i&gt;=0; i--) &#123; for(int j =0; j&lt;=i; j++) &#123; g[j] =min(f[j], f[j+1])+ arr[i][j]; // 这步骤很重要 &#125; f =g; &#125; cout&lt;&lt; f[0] &lt;&lt;endl; return 0;&#125; 354. Russian Doll Envelopes 原题链接 一共有两种解法。先进行排序，然后转换成最长连续递增子序列,。时间复杂度分析： 排序最快是 $nlogn$， 如果使用dp，那么总的是$n^2$; 如果使用二分总的是$nlogn$。下面是第一种解法，使用dp 的思想。 1234567891011121314151617181920212223242526class Solution &#123;public: int maxEnvelopes(vector&lt;vector&lt;int&gt;&gt;&amp; envelopes) &#123; int n =envelopes.size(); vector&lt;int&gt; dp(n,0); dp[0] =1; int result =1; sort(envelopes.begin(), envelopes.end()); for(int i =0; i&lt;n; i++) &#123; for(int j =0;j&lt;i ;j++)// 遍历前面所有的状态的 &#123; if(envelopes[i][0]&gt; envelopes[j][0] &amp;&amp; envelopes[i][1]&gt; envelopes[j][1] ) dp[i] =max(dp[i], dp[j]+1); &#125; result =max(result, dp[i]); &#125; return result; &#125;&#125;; 单机版 1234567891011121314151617181920212223242526272829303132333435363738394041424344#include&lt;iostream&gt;#include&lt;vector&gt;#include&lt;algorithm&gt;using namespace std;// sort 然后最长递增子序列吧// dp[i] 表示长度为i 的最长递增, 转移方程 dp[i] =max(dp[i], dp[j]+1) j&lt;i, 初始化 dp[0] =0int maxEnvelopes(vector&lt;vector&lt;int&gt;&gt;&amp; arr)&#123; if(arr.empty()) return 0; int result =1; int n =arr.size(); vector&lt;int&gt; dp(n); sort(arr.begin(), arr.end());// 默认按照第一个元素进行递增排序 for(int i =0; i&lt;n ;i++) &#123; for(int j=0; j&lt;i; j++) &#123; dp[i] =max(dp[i], dp[j] +1); &#125; result =max(result, dp[i]); &#125; return result;&#125;int main()&#123; int n ; cin &gt;&gt;n; vector&lt;vector&lt;int&gt;&gt; arr(n, vector&lt;int&gt;(2)); for(int i =0; i&lt;n;i++) &#123; for(int j =0; j&lt;2;j++) cin &gt;&gt;arr[i][j]; &#125; cout &lt;&lt; maxEnvelopes(arr) &lt;&lt;endl; return 0;&#125; 第二种解法，使用二分的思想进行检索，总的时间复杂度能够达到 $O(nlog n)$.使用python 实现。 leetcode 版本 12345678910111213141516171819202122232425262728293031323334class Solution(object): def maxEnvelopes(self, envelopes): """ :type envelopes: List[List[int]] :rtype: int """ if not envelopes: return 0; envelopes.sort(key =lambda x:(x[0], -x[1])) # 按照第一个元素升序，然后第二个元素降序 # 二分查找 第二个元素的位置，当第一个元素相同的时候 #print(envelopes) lst =[] for fir, sec in envelopes: y =sec if lst ==[] or y&gt; lst[-1]: lst.append(y) else: # 二分查找y 合适的位置 left, right =0, len(lst) while left&lt;right: mid =(left +right)//2 if y&lt;= lst[mid]: right =mid else: left =mid +1 lst[left] =y return len(lst) 单机版本(c++ 中在main 或者 普通function 中对vector 申请空间之后，都是有默认的初始化的。比如对于 vector ，那么都是初始化成0)python 中sort() 函数中的自定义sort 是非常nice 的一定写法。 123456789101112131415161718192021222324252627282930313233343536373839404142434445# 先排序，然后就是 最长递增子序列的问题, 而该问题使用二分查找进行解决def maxEnvelopes(arr): if not arr: return 0 arr.sort(key =lambda x: (x[0], -x[1])) lst =[] #print(arr) for _, b in arr: import ipdb #ipdb.set_trace() y =b if lst==[] or y&gt; lst[-1]: lst.append(y) else: # 二分查找 left, right =0, len(lst)-1 while left &lt;right: mid =(left+ right)//2 if( y&lt; lst[mid]): right =mid else: left =mid+1 lst[left] =y return len(lst)if __name__ =="__main__": n =int(input()) arr =[[0]*2] *n for i in range(n): arr[i] =input().split(" ") arr[i] =[int(a) for a in arr[i]] print(arr) print(maxEnvelopes(arr)) 338. Counting Bits 原题 思想： 二进制的思想。使用dp 的时候，往往初始化 f[0] 然后在遍历循环的时候，就从i =1 的时候进行循环。 12345678910111213141516class Solution &#123;public: vector&lt;int&gt; countBits(int num) &#123; vector&lt;int&gt; f(num+1); f[0] =0; for(int i =1; i&lt;=num; i++) &#123; f[i] =f[i/2] + i&amp;1; // 表示i 的个位是不是1 &#125; return f; &#125;&#125;; 单机版本 12345678910111213141516171819202122232425#include&lt;iostream&gt;#include&lt;vector&gt;using namespace std;int main()&#123; int n; cin&gt;&gt; n; vector&lt;int&gt; f(n+1); f[0] =0; for(int i =1; i&lt;=n; i++) &#123; f[i] =f[i &gt;&gt;1] + i&amp;1;// 可以写得更加简单一点 &#125; for(auto u : f) cout &lt;&lt; u&lt;&lt; " "; cout&lt;&lt;endl; return 0;&#125; 329. Longest Increasing Path in a Matrix 原题 leetcode 版本 本题的特点，没有一个非常明确的转移顺序（枚举顺序）。因为是可以有四个方向进行选择的，这个枚举顺序是不好写的。所以可以学习以下下面的代码。写的比较nice。 这个题目很容易使用f[i][j] 表示点 (i,j) 位置的最长的路径，但是怎么转移？ 这个是没有固定的说是按照行枚举还是按照列进行枚举的。所以这个转移方程是不太好些的，所以这里用到一种方法，叫做记忆化搜索. 这种使用 dx, dy 的方式，然后得到新的坐标(a, b)。这种方式是比较通用的。 12345678910111213141516171819202122232425262728293031class Solution &#123;public: int n, m; vector&lt;vector&lt;int&gt;&gt; f, g; int dx[4] = &#123;-1, 0, 1, 0&#125;, dy[4] = &#123;0, 1, 0, -1&#125;; int dp(int x, int y) &#123; if (f[x][y] != -1) return f[x][y]; f[x][y] = 1; for (int i = 0; i &lt; 4; i ++ ) &#123; int a = x + dx[i], b = y + dy[i]; if (a &gt;= 0 &amp;&amp; a &lt; n &amp;&amp; b &gt;= 0 &amp;&amp; b &lt; m &amp;&amp; g[a][b] &lt; g[x][y]) f[x][y] = max(f[x][y], dp(a, b) + 1); &#125; return f[x][y]; &#125; int longestIncreasingPath(vector&lt;vector&lt;int&gt;&gt;&amp; matrix) &#123; if (matrix.empty()) return 0; g = matrix; n = g.size(), m = g[0].size(); f = vector&lt;vector&lt;int&gt;&gt;(n, vector&lt;int&gt;(m, -1)); int res = 0; for (int i = 0; i &lt; n; i ++ ) for (int j = 0; j &lt; m; j ++ ) res = max(res, dp(i, j)); return res; &#125;&#125;; 单机版 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748#include&lt;iostream&gt;#include&lt;vector&gt;using namespace std;int n, m;vector&lt;vector&lt;int&gt;&gt; f, arr;int dx[4] =&#123;-1, 0, 1, 0&#125;, dy[4] =&#123;0, 1, 0, -1&#125;;int dp(int i, int j)&#123; if(f[i][j] != -1) return f[i][j]; f[i][j] =1; for(int k =0; k&lt;4 ;k++) &#123; int a = i+dx[k], b =j+dy[k]; if(a &gt;=0 &amp;&amp; a&lt; n &amp;&amp; b&gt;= 0 &amp;&amp; b&lt;m &amp;&amp; arr[a][b] &gt; arr[i][j]) f[i][j] =max(f[i][j], dp(a, b)+1); &#125; return f[i][j];&#125;int main()&#123; cin&gt;&gt; n&gt;&gt;m; arr =vector&lt;vector&lt;int&gt;&gt;(n, vector&lt;int&gt;(m)); for(int i =0; i&lt;n; i++) for(int j =0; j&lt;m ;j++) cin &gt;&gt; arr[i][j]; // 初始化 f =vector&lt;vector&lt;int&gt;&gt;(n, vector&lt;int&gt;(m, -1)); int res =0; for(int i =0; i&lt;n; i++) for(int j =0; j&lt;m; j++) &#123; res =max(res, dp(i, j)); &#125; cout&lt;&lt; res&lt;&lt;endl; return 0;&#125; Coin change 原题链接 使用硬币组成给定的钱数，硬币可以无限使用，钱的总数是一定的，要求尽可能使用少的硬币。 是一种完全背包问题，从小到大进行枚举。(多重背包问题是对于完全背包问题的一种约束，后者是物品可以无限的选，前者是对于物品的数量有限制。也就是说多重背包中，某件物品只能选择几个) f[i] 表示钱数是i 需要使用最少的硬币数量。 f[i] =min(f[i], f[i-c]+1) 分成选和不选两种方案。初始化 f[0] =0 （根据实际问题含义进行初始化）。背包问题是就是先枚举物品，然后再枚举体积（这里 是金钱）的做法。 LeetCode版本 123456789101112131415161718class Solution &#123;public: int coinChange(vector&lt;int&gt;&amp; coins, int amount) &#123; vector&lt;int&gt; f(amount+1, INT_MAX/2); f[0] =0; for(auto c : coins) for(int i =c ; i&lt;= amount; i++) &#123; f[i] =min(f[i], f[i-c]+1); &#125; if(f[amount] == INT_MAX/2) return -1; else return f[amount]; &#125;&#125;; 单机版 123456789101112131415161718192021222324252627282930#include&lt;iostream&gt;#include&lt;vector&gt;using namespace std;int main()&#123; int n; cin&gt;&gt;n; vector&lt;int&gt; coins(n); int amount ; cin &gt;&gt;amount; vector&lt;int&gt; f(amount+1, INT_MAX/2); // 这个在cpp 中的常数还是经常使用的哦 f[0] =0; for(int i =0; i&lt;n ; i++) &#123; int c; cin &gt;&gt;c; for(int j =c; j&lt;= amount; j++) f[j] =min(f[j], f[j-c]+1); &#125; if(f[amount] ==INT_MAX/2) cout &lt;&lt; -1 &lt;&lt;endl; else cout &lt;&lt; f[amount] &lt;&lt;endl; return 0;&#125; Maximal Square 题目： 最大的包含全部都是1 的正方形。f[i][j] 表示右下角位置是 (i, j) 的正方形的边长。这个转移和上面increasing path 是一样的，都是没有固定的枚举方式，所以使用 dx dy 得到下一个新的坐标。根据上方，左边和左上角进行求解最小的边长。 原题链接LeetCode版本 1234567891011121314151617181920212223242526272829303132333435363738394041class Solution &#123;public: // 如何表示，dp[i][j] 表示以位置(i,j)为右下角的正方形的全部都是1的面积。 // 如何计算 dp[i][j] =min(dp[i-1][j-1], dp[i-1][j], dp[i][j-1]) 这个是转移方程 // 初始化, dp =&#123;0&#125; int maximalSquare(vector&lt;vector&lt;char&gt;&gt;&amp; matrix) &#123; if(!matrix.size() || !matrix[0].size()) return 0; int n =matrix.size(), m =matrix[0].size(); vector&lt;vector&lt;int&gt;&gt; dp(n, vector&lt;int&gt;(m, 0)); int res =0; for(int i =0; i&lt;n; i++) &#123; for(int j =0; j&lt;m; j++) &#123; //if (!i || !j) dp[i][j] =0; if(matrix[i][j] =='0') dp[i][j] =0; else &#123; dp[i][j] =1; if(i &gt;=1 &amp;&amp; j&gt;=1) &#123; // 那么这个就十分的通顺，计算的就是最小的边长 dp[i][j] += min(dp[i-1][j-1], min(dp[i-1][j], dp[i][j-1])) ; &#125; res =max(res, dp[i][j]); &#125; &#125; &#125; return res*res;// dp 得到的是边长，不是面积 &#125;&#125;; 单机版中array 的输入是 int 类型，所以在进行 array[i][j] ==’1’ 判断就是错误的， 因为 int 类型的 1 和 char 类型的 ‘1’ 这个是不一样的。所以在 c++ 或者 c 中一定要注意这个小的区别。 12345678910111213141516171819202122232425262728293031323334353637383940#include&lt;iostream&gt;#include&lt;algorithm&gt;#include&lt;vector&gt;using namespace std;int n, m;vector&lt;vector&lt;int&gt;&gt; arr, dp;int main()&#123; cin&gt;&gt;n&gt;&gt;m; // 需要申请空间的，即使是没有初始化成特定的值 /* arr =vector&lt;vector&lt;int&gt;&gt;(n, vector&lt;int&gt;(m)); for(int i =0; i&lt;n; i++) for(int j =0; j&lt;m; j++) cin&gt;&gt;arr[i][j]; */ arr =&#123;&#123;1, 0 , 1, 0, 0&#125;, &#123;1, 0, 1, 1, 1&#125;, &#123;1, 1, 1, 1,1&#125;, &#123;1, 0, 0, 1, 0&#125;&#125;; // 这种先是定义，然后再申请空间，也是昌吉nice的e dp =vector&lt;vector&lt;int&gt;&gt;(n, vector&lt;int&gt;(m, 0)); int res =0; for(int i =0;i&lt;n; i++) for(int j =0; j&lt;m; j++) &#123; if(arr[i][j] =='0') dp[i][j] =0; else &#123; dp[i][j] =1; // 这个条件很重要，因为当 arr[i][j] ==1 的时候，该位置就组成了dp[][] ==1 if(i&gt;=1 &amp;&amp; j&gt;=1) dp[i][j] += min(dp[i-1][j-1], min(dp[i][j-1],dp[i-1][j]) ); res =max(res, dp[i][j]); &#125; &#125; cout&lt;&lt; res*res &lt;&lt;endl; return 0;&#125; Out of boundary paths(到这里了) leetcode 版本 原题 f[i][j][k] 表示位置在(i, j) 并且还剩下k 步，走出边界的方案数。同样使用到了记忆化搜索，当计算过的时候，直接返回结果。 1234567891011121314151617181920212223242526272829303132333435class Solution &#123;public: // 这个应该是最为复杂的dp 了吧。dp[i][j][k] 表示 (i,j) 同时还有k 步的时候，走出边界的方案数 vector&lt;vector&lt;vector&lt;int&gt;&gt;&gt; f; int dx[4] =&#123;-1, 0, 1, 0&#125;, dy[4] =&#123;0, 1, 0, -1&#125;; int mod =1000000007 int findPaths(int m, int n, int N, int i, int j) &#123; f =vector&lt;vector&lt;vector&lt;int&gt;&gt;&gt;(m, vector&lt;vector&lt;int&gt;&gt;(n, vector&lt;int&gt;(N+1, -1))); return dp(m, n, N, i, j); &#125; int dp(int m, int n, int k, int x, int y) &#123; int &amp;v =f[x][y][k]; if(v !=-1) return v; v =0; if(!k) return v; for(int i =0; i&lt;4; i++) &#123; int a =x +dx[i], b = y+dy[i]; if(a &lt;0 || a ==m || b&lt;0 || b ==n) v++; else v += dp(m, n, k -1, a, b); v %= mod; &#125; return v; &#125;&#125;; 单机版1234567891011121314151617181920212223242526272829303132333435363738394041424344454647#include&lt;iostream&gt;#include&lt;vector&gt;using namespace std;// dp[i][j][k] 表示(i, j) 位置同时还剩下k 步，走出边界的方案书// 转移 dp[i][j][k] +=1 if 走出了边界 else += dp[a][b][k-1]// dp[i][j][k] =0vector&lt;vector&lt;vector&lt;int&gt;&gt;&gt; f;int MOD =1000000007;int dx[4] =&#123;-1, 0, 1, 0&#125;, dy[4] =&#123;0, 1, 0, -1&#125;;int dp(int m, int n, int k, int x, int y)&#123; int &amp;v =f[x][y][k]; // 记忆化搜索 if (v !=-1) return v; v =0; if (!k ) return v; // 枚举四个方向 for(int i =0; i&lt;4; i++) &#123; int a =x+dx[i], b =y+dy[i]; if(a &lt;0 || a==m || b&lt;0 || b ==n ) v ++; else v +=dp(m,n, k-1, a, b); v %=MOD; &#125; return v;&#125;int main()&#123; int m, n, N, i,j; // 简单的先通过case 再说 m=1, n =3, N =3, i =0, j=1; // 这个最后为什么手机 N+1, 测试一下边界 f= vector&lt;vector&lt;vector&lt;int&gt;&gt;&gt;(m, vector&lt;vector&lt;int&gt;&gt;(n, vector&lt;int&gt;(N+1, -1))); cout&lt;&lt; dp(m, n, N, i, j)&lt;&lt; endl; return 0;&#125; Decode Ways 存在一个英文字母到数字的映射表，给定数字，问有几种 decode 方式。 f[i] 表示前 i个数字一共的解码方式。可以分为两种情况，一种方案是当前的数字可以映射到一个字母，另一种方案是当前数字和上一个数字映射到一个字母。f[i] += f[i-1] （这种关系表示一种累加，继承的意思）。 （为什么分成了两种情况，映射表中是1 到26，数字使用两位就可以直接表示） 网站链接 LeetCode 版本 123456789101112131415161718192021222324252627282930313233class Solution &#123;public: // dp 的思路， dp[i] 表示前i 个字符串的总的方案书 // 转移 dp[i] += dp[i-1] if s[i-1] !='0' , += dp[i-2] if 10&lt;= int(s[i-2: i-1]) &lt;=26 // dp 一定是要进行初始化的 // 卧槽关于数据类型，这个是第二遍了吧， 0 和 '0' 不是一个数据类型 int numDecodings(string s) &#123; int n =s.size(); vector&lt;int&gt; f(n+1); s =' '+s; f[0] =1; for(int i =1; i&lt;=n ;i++) &#123; f[i]=0; if(s[i] !='0') f[i] +=f[i-1]; if(i &gt;1) &#123; int t =(s[i-1] -'0')*10 +s[i] -'0'; if (t &gt;=10 &amp;&amp; t&lt;= 26) f[i] += f[i-2]; &#125; &#125; //for(auto u : f) cout&lt;&lt; u&lt;&lt; " "; return f[n]; &#125;&#125;; 单机版 123456789101112131415161718192021222324252627282930313233343536373839#include&lt;iostream&gt;#include&lt;vector&gt;#include&lt;string&gt;using namespace std;/** 首先是 f[i] 表示前 i 个可以解码的总数 转移方程 f[i] += f[i-1] if s[i-1] !='0' f[i] += f[i-2] if 10&lt;=int(s[i-2,i])&lt;=26 初始化 f[i] =0 &amp;&amp; f[0] =1 **/int main()&#123; string s; cin&gt;&gt; s; int n =s.size(); vector&lt;int&gt; f(n+1); // 边界问题 s = ' '+s; f[0] =1; for(int i =1 ; i&lt;=n; i++) &#123; f[i] =0; if(s[i] !='0') f[i] += f[i-1]; if(i&gt;1) &#123; int t =(s[i-1]-'0')*10 + s[i] -'0'; if( t&lt;=26 &amp;&amp; t&gt;= 10) f[i] += f[i-2]; &#125; &#125; cout&lt;&lt; f[n]&lt;&lt;endl; return 0;&#125; ugly number leetcode 版本原题链接 vector.back() 访问返回最后一个元素。 123456789101112131415161718192021222324class Solution &#123;public: // 使用三个指针，之前使用python 使用过类似的算法才对 int nthUglyNumber(int n) &#123; vector&lt;int&gt; q; q.push_back(1); int i =0, j=0,k =0; while( --n) &#123; int t =min(q[i]*2, min(q[j]*3, q[k]*5)); q.push_back(t); // 这个是并列的if if( t == q[i]*2) i++; if(t ==q[j] *3) j++; if(t == q[k] *5) k++; &#125; return q.back(); &#125;&#125;; 单机版 123456789101112131415161718192021222324252627282930313233343536#include&lt;iostream&gt;#include&lt;vector&gt;#include&lt;algorithm&gt;using namespace std;int main()&#123; int n; cin &gt;&gt;n; //vector&lt;int&gt; arr(n+1); vector&lt;int&gt; q; //for(int i =0; i&lt;n; i++) cin &gt;&gt;arr[i]; q.push_back(1); int i =0,j =0, k =0; while( --n) &#123; int t =min(q[i] *2, min(q[j]*3, q[k]*5)); q.push_back(t); if(q[i]*2 == t) i++; if(q[j] *3 ==t) j++; if(q[k] *5 ==t) k++; &#125; cout&lt;&lt; q.back()&lt;&lt;endl; // vector.back() 是访问最后一个数字 // vector.front() 是访问第一个数字 return 0;&#125; Distinct Subsequences LeetCode 版本链接 思路： 使用dp。含义：f[i][j] 表示source 为i ，dest 为j 的情况下，最大的方案数转换： f[i][ j] = f[i-1][j] 不管什么情况下，对于source 中的i 都可以不选f[i][j] 对于 s[i] ==d[j] 的情况下， f[i][j] += f[i-1][j-1] ， 当两个相同的时候，这个是可以选择的。（不选就没有累加的方案数，选择就是累加的方案数） 子序列和子串是不同的概念， 子序列要求的不连续的index，子串要求的是连续的index。最后求的是s 能够拼凑成 t的方案数。 1234567891011121314151617181920212223class Solution &#123;public: // f[i][j] = f[i-1][j] 或者 =f[i-1][j-1] if(s[i] ==t[j]) long long numDistinct(string s, string t) &#123; int m =s.size(), n =t.size(); vector&lt;vector&lt;long long&gt;&gt; f(m+1, vector&lt;long long&gt;(n+1)); // 初始化 for(int i =0; i&lt;=m; i++) f[i][0] =1; for(int i =1; i&lt;=m; i++) for(int j =1; j&lt;=n ; j++) &#123; f[i][j] = f[i-1][j]; if(s[i-1] ==t[j-1]) f[i][j]+= f[i-1][j-1];// 这个累加的是不选的方案，然后如果相同，那么就再累加一下。 &#125; return f[m][n]; &#125;&#125;; 单机版 12345678910111213141516171819202122232425262728293031#include&lt;iostream&gt;#include&lt;vector&gt;#include&lt;string&gt;using namespace std;int main()&#123; string S; string T; cin&gt;&gt; S; cin&gt;&gt; T; int n =S.size(), m =T.size(); vector&lt;vector&lt;long long&gt;&gt; f(n+1, vector&lt;long long&gt;(m+1)); for(int i =0;i&lt;n; i++) f[i][0] =1; for(int i =1; i&lt;=n; i++) for(int j =1; j&lt;=m ; j++) &#123; f[i][j] = f[i-1][j]; if(S[i-1] == T[j-1]) f[i][j] += f[i-1][j-1]; &#125; cout&lt;&lt; f[n][m] &lt;&lt;endl; return 0; &#125; Palindrome Partitioning II 原题链接 这道题使用两次 dp。 第一次： dp[i:j]方便的判断任意两个区间组成的字符串是否是回文数。第二次： dp[i] 前i 个字符串中最少有多少个回文数(题目要求是最少的 partition的数量) 两个for 循环，保证了每次枚举的是 i( 1-n) 的left and right，二部仅仅是 1-n 的left and right。 对于dp 的debug，一般先从逻辑上（看代码）进行找错；实在不行才是每行输出。因为dp 当数据量比较大的时候，这个是不容易输出的。 leetCode-132 95:17 讲解连接 https://www.bilibili.com/video/av35161871/?p=1 123456789101112131415161718192021222324252627282930313233343536373839class Solution &#123;public: // f[i] 表示前i 个字母是多少个回文串， 数量的回文串 -1 就是 需要多少 刀 // 分成多少个 palindrome partitioning， // 使用动规 进行初始化，然后 int minCut(string s) &#123; int n =s.size(); vector&lt;vector&lt;bool&gt;&gt; c(n, vector&lt;bool&gt;(n, false)); // 这个c 表示从 [j][i] 这个子串是不是回文串, 这个操作很神奇 for(int i =1;i&lt;=n ; i++) for(int j =0; j+i-1 &lt;n; j++) &#123; int l =j, r =j+i-1; c[l][r] = s[l] ==s[r] &amp;&amp; (l+1 &gt; r-1 || c[l+1][r-1] ); &#125; for(auto u : c) &#123; for(auto v : u) cout &lt;&lt; v&lt;&lt; " "; cout &lt;&lt;endl; &#125; vector&lt;int&gt; f(n+1); f[0] =0; for(int i =1; i&lt;=n ;i++) &#123; f[i] =INT_MAX; for(int j =1; j&lt;=i; j++) if(c[j-1][i-1]) // 为甚这个是回文串，之后，然后才能+1 f[i] =min(f[i], f[j-1]+1); &#125; return f[n]-1; &#125; &#125;; 这个是单机版本 12345678910111213141516171819202122232425262728293031323334353637383940#include&lt;iostream&gt;#include&lt;vector&gt;#include&lt;string&gt;using namespace std;int main()&#123; string s; cin &gt;&gt;s; int n =s.size(); vector&lt;vector&lt;bool&gt;&gt; c(n, vector&lt;bool&gt;(n, false)); for(int i =1; i&lt;=n; i++) &#123; for(int j =0; j+i-1&lt;n; j++) &#123; int l =j, r =j+i -1; c[l][r] = s[l] ==s[r] &amp;&amp;(l+1 &gt; r-1 || c[l+1][r-1]); &#125; &#125; vector&lt;int&gt; f(n+1); f[0]= 0; for(int i =1; i&lt;=n; i++) &#123; f[i] =INT_MAX; for(int j =1; j&lt;=i; j++) if(c[j-1][i-1]) f[i] =min(f[i], f[j-1] +1); &#125; cout&lt;&lt; f[n]-1&lt;&lt;endl; return 0;&#125;]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>dp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Depth-first Search]]></title>
    <url>%2F2019%2F08%2F29%2Fdepth-first-search%2F</url>
    <content type="text"><![CDATA[深度优先搜索专题，题目来源于LeetCode。大多数题目使用c++ 实现，少量使用python 实现。 使用场景：当状态数量非常庞大的时候，解的个数非常少的时候，适合使用深度优先搜索。但搜索不等于深度优先搜索。搜索是一种思想，深搜索是一种实现方式。 letter combinations of a phone number 原题链接其中的 state 就是最后的结果， now 是当前的结果，也是有点滚动数组的意思。 有点不断的更新前缀和后缀的意思。 c++ 语法，目前理解如果是常量string，那么使用 string dict[[] 这样形式，如果是可变的string 那么使用 vector 这种形式 1234567891011121314151617181920212223242526class Solution &#123;public: // 显示枚举数字， 然后枚举状态，再枚举字母 string dict[8] =&#123;"abc", "def", "ghi", "jkl", "mno", "pqrs", "tuv", "wxyz"&#125;; vector&lt;string&gt; letterCombinations(string digits) &#123; if (digits.empty()) return vector&lt;string&gt;(); vector&lt;string&gt; state(1, "");// 不必固定初始化，因为你也是不知道最后的size() for (auto digit: digits) &#123; vector&lt;string&gt; now; for(auto s : state) &#123; for(auto c: dict[digit-'2']) now.push_back(s+c); &#125; state=now; &#125; return state; &#125;&#125;; 单机版本123456789101112131415161718192021222324252627282930313233#include&lt;iostream&gt;#include&lt;vector&gt;#include&lt;string&gt;using namespace std;vector&lt;string&gt; dict =&#123;"abc", "def", "ghi", "jkl", "mno", "pqrs", "tuv", "wxyz"&#125;;// 深度优先搜索, 关键树顺序，按照digit 一个个进行枚举，每个digit 对应的字母都可以再次进行枚举int main()&#123; string digits; cin &gt;&gt; digits; vector&lt;string&gt; res(1, ""); //vector&lt;string&gt; now; for(auto digit: digits ) &#123; vector&lt;string&gt; now ; for(auto u: dict[digit-'2']) &#123; for(auto v : res) now.push_back(v+u); &#125; res =now; &#125; for(auto u: res) cout &lt;&lt;u&lt;&lt;" "; cout&lt;&lt;endl; return 0;&#125; Word Search 原题链接 dfs 的思路应用到该题目的做题思路： 枚举起点 从起点开始，依次搜索下一个点的位置 在枚举的过程中，需要实时判断（要保证和目标单词匹配） 时间复杂度分析$ n \times m \times 3^k$ 棋盘类的题目，一般都是需要用到搜索和动态规划。当数据比较小的时候使用搜索（暴搜），当数据比较大的时候，使用动态规划。 所谓回溯，就是需要恢复现场的。 LeetCode 版本 12345678910111213141516171819202122232425262728293031323334353637383940414243444546class Solution &#123;public: // 使用dfs 进行解决 int n_words, n, m; bool exist(vector&lt;vector&lt;char&gt;&gt;&amp; board, string word) &#123; n =board.size(); m =board[0].size(); n_words =word.size(); // 列举每个起点 bool flag =false; for(int i =0; i&lt;n ;i++) for(int j =0; j&lt;m ;j++) if( dfs(board, i, j, word, 0)) &#123; return true; &#125; return flag; &#125; // 表示 board[x][y] 和 str[u] 是否相同 // dfs 中的跳出的条件 int dx[4]=&#123;-1, 0, 1, 0&#125;, dy[4]=&#123;0, 1, 0, -1&#125;; bool dfs(vector&lt;vector&lt;char&gt;&gt;&amp; board, int x, int y, string&amp; str, int u) &#123; if(board[x][y] != str[u]) return false; if(u ==n_words-1) return true; board[x][y] ='.'; // 这个也是比较牛逼 for(int i =0; i&lt;4; i++) &#123; int a =x+dx[i], b =y +dy[i]; if(a &gt;=0 &amp;&amp; a&lt;n &amp;&amp; b&gt;= 0 &amp;&amp; b&lt;m) if(dfs(board, a, b, str, u+1)) return true; // 这个是如何去理解，如果下一个字节u 也能够走通，那么就可以返回 true &#125; board[x][y] =str[u]; return false; &#125;&#125;; 这个是单机版。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465#include&lt;iostream&gt;#include&lt;vector&gt;#include&lt;string&gt;using namespace std;#define LEN 10int n, m;int dx[4] =&#123;-1, 0, 1, 0&#125;, dy[4] =&#123;0, 1, 0, -1&#125;;// word 是一个string 类型，但是word.size() 依然是可以这样用的e// 注意这个指针的符号bool dfs(vector&lt;vector&lt;char&gt;&gt;&amp; board, int x, int y, string&amp; word, int u)&#123; //边界条件 if (board[x][y] != word[u]) return false; if(u ==word.size() -1) return true; board[x][y] ='.'; // 寻找下一步 for(int i =0; i&lt;4 ; i++) &#123; int a =x +dx[i], b =y+dy[i]; if(a &gt;=0 &amp;&amp; a&lt;n &amp;&amp; b&gt;=0 &amp;&amp; b&lt;m) if(dfs(board, a, b, word, u+1)) return true; &#125; board[x][y] =word[u]; return false; &#125;int main()&#123; cin &gt;&gt; n&gt;&gt;m; vector&lt;vector&lt;char&gt;&gt; board(n, vector&lt;char&gt;(m)); //vector&lt;string&gt; word(LEN); string word; for(int i =0; i&lt;n; i++) &#123; for(int j =0; j&lt;m; j++) cin &gt;&gt; board[i][j]; &#125; cin &gt;&gt; word; if(n ==0 || m ==0) return false; bool flag ; // 列举起点 for(int i=0; i&lt;n; i++) for(int j =0; j&lt;m; j++) &#123; if(dfs(board, i, j, word, 0 )) flag =true; &#125; flag =false; cout &lt;&lt; flag; return 0;&#125; leetcode 46: permutations 有两种思路： 枚举每个位置放哪个数字； 枚举每个数放哪些位置。 这道题目是枚举数字的思路求解，下一道题是枚举位置。 LeetCode版本： 123456789101112131415161718192021222324252627282930313233343536373839404142434445class Solution &#123;public: // 全排列的问题，有两种思路，一种是给定数字然后选择位置； 一种是给定位置选择数字。这里是给定数字然后选择位置 // 对于 dfs 在递归的过程中需要传递大量的变量，这个时候设置成全局变量更加合理 int n; vector&lt;int&gt; path; vector&lt;vector&lt;int&gt;&gt; ans; vector&lt;bool&gt; flag; vector&lt;vector&lt;int&gt;&gt; permute(vector&lt;int&gt;&amp; nums) &#123; if(nums.empty()) return vector&lt;vector&lt;int&gt;&gt;(); n =nums.size(); // 对于flag 的初始化 flag =vector&lt;bool&gt;(n); dfs(nums, 0); return ans; &#125; void dfs(vector&lt;int&gt;&amp; nums, int u) &#123; if(u ==n) &#123; ans.push_back(path); return; &#125; for(int i =0; i&lt;n; i++) &#123; if(!flag[i]) &#123; flag[i] =true; path.push_back(nums[i]); dfs(nums, u+1); // 是在原来的基础上，进行的 u+1 ， path.pop_back(); // 这个是回复现场的操作，所以path 中只会有一个解 flag[i] =false; &#125; &#125; &#125;&#125;; 个人单机版 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354#include&lt;iostream&gt;#include&lt;vector&gt;using namespace std;// permutations#define LEN 10int n;vector&lt;int&gt; path;vector&lt;vector&lt;int&gt;&gt; ans;vector&lt;bool&gt; flag;void dfs(vector&lt;int&gt;&amp; arr, int u)&#123; if( u==n) &#123; ans.push_back(path); return; &#125; for(int i =0; i&lt;n;i++) &#123; if(!flag[i]) &#123; flag[i] =true; path.push_back(arr[i]); dfs(arr, u+1); path.pop_back(); flag[i] =false; &#125; &#125;&#125;int main()&#123; vector&lt;int&gt; arr(LEN); cin &gt;&gt;n; for(int i =0; i&lt;n ;i++) cin&gt;&gt;arr[i]; flag =vector&lt;bool&gt;(n); dfs(arr, 0); for(auto u: ans) &#123; for(int i =0; i&lt; u.size(); i++) cout&lt;&lt; u[i] &lt;&lt;" "; cout &lt;&lt;endl; &#125; return 0;&#125; Permutations II LeetCode 版本 枚举位置。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647class Solution &#123;public: // 这个题目和上一个题目的不同点在于有了重复的数字， // 如果避免结果的重复呢？ 就是相同的数字的相对顺序保持不变，这样就不会发生结果的重复。 // 上一步基于一个简单的假设，相同的数字是在一块的，如何实现该假设，使用排序就可以轻松的实现 // 这个只是变量的定义，并没有开辟空间 // int n; vector&lt;vector&lt;int&gt;&gt; ans; vector&lt;int&gt; path; vector&lt;bool&gt; st; vector&lt;vector&lt;int&gt;&gt; permuteUnique(vector&lt;int&gt;&amp; nums) &#123; // 下面是开辟了空间 n =nums.size(); st =vector&lt;bool&gt;(n); path =vector&lt;int&gt;(n); sort(nums.begin(), nums.end()); // 这种是对于 nums 非常简单的实现了 dfs(nums, 0, 0); return ans; &#125; void dfs(vector&lt;int&gt; &amp;nums, int u, int start) &#123; if(u ==n) &#123; ans.push_back(path); return ; &#125; for(int i =start; i&lt;n; i++) &#123; if(!st[i]) &#123; st[i] =true; path[i] =nums[u]; // 枚举不同的位置 dfs(nums, u+1, u+1&lt; n &amp;&amp; nums[u+1] ==nums[u] ? i+1:0); st[i] =false; &#125; &#125; &#125; &#125;; 个人版123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960#include&lt;iostream&gt;#include&lt;algorithm&gt;#include&lt;cmath&gt;#include&lt;vector&gt;using namespace std;int n;vector&lt;vector&lt;int&gt;&gt; ans;vector&lt;int&gt; path;vector&lt;bool&gt; flag;void dfs(vector&lt;int&gt;&amp; arr, int u, int start )&#123; if(u ==n) &#123; ans.push_back(path); return; &#125; for(int i =start; i&lt;n; i++) &#123; if(! flag[i]) &#123; flag[i] =true; path[i] =arr[u]; dfs(arr, u+1, u+1&lt;n &amp;&amp; arr[u+1] ==arr[u] ? i+1: 0); flag[i] =false; &#125; &#125;&#125;int main()&#123; vector&lt;int&gt; arr; cin &gt;&gt;n; //初始化 path =vector&lt;int&gt;(n); flag =vector&lt;bool&gt;(n); arr =vector&lt;int&gt;(n); for(auto u: flag) cout&lt;&lt; u&lt;&lt;" "; for(int i =0;i&lt;n;i++) cin &gt;&gt; arr[i]; sort(arr.begin(), arr.end()); dfs(arr, 0, 0); for(auto u : ans) &#123; for(int i =0; i&lt;u.size(); i++) cout &lt;&lt;u[i] &lt;&lt;" "; cout &lt;&lt;endl; &#125; return 0;&#125; 上面是排列的问题，下面是组合的问题。 Subsets LeetCode 版本 123456789101112131415161718192021222324class Solution &#123;public: // 使用循环的方式解题：数字的二进制表示用来表示该位置是否出现过， // 数字i 的二进制表示中第 j位是否为1 : i&gt;&gt;j &amp;1 这样进行判断 vector&lt;vector&lt;int&gt;&gt; subsets(vector&lt;int&gt;&amp; nums) &#123; vector&lt;vector&lt;int&gt;&gt; res; //vector&lt;int&gt; now; for(int i =0; i&lt; 1&lt;&lt; nums.size(); i++) &#123; vector&lt;int&gt; now; for(int j =0; j&lt; nums.size(); j++) if(i &gt;&gt;j &amp;1) &#123; now.push_back(nums[j]); &#125; res.push_back(now); &#125; return res; &#125;&#125;; 单机版 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950#include&lt;iostream&gt;#include&lt;vector&gt;using namespace std;vector&lt;vector&lt;int&gt;&gt; fun(vector&lt;int&gt;&amp; arr)&#123; cout&lt;&lt; "he"&lt;&lt;endl; vector&lt;vector&lt;int&gt;&gt; res; int n =arr.size(); for(int i =0; i&lt; 1&lt;&lt;n; i++) &#123; vector&lt;int&gt; path; for(int j =0; j&lt;n; j++) &#123; if(i &gt;&gt;j &amp;1) path.push_back(arr[j]); &#125; res.push_back(path); &#125; return res; &#125;int main()&#123; int n; cin&gt;&gt;n; vector&lt;int&gt; arr(n); for(int i =0;i&lt;n;i++) cin&gt;&gt;arr[i]; for(int i =0;i&lt;n;i++) cout&lt;&lt; arr[i] &lt;&lt;" "; //fun(arr); vector&lt;vector&lt;int&gt;&gt; res =fun(arr); for(auto u : res) &#123; for(int i =0;i&lt; u.size(); i++) cout &lt;&lt; u[i] &lt;&lt;" "; cout &lt;&lt;endl; &#125; return 0;&#125; 90. Subsets II这个是上一道题目的扩展https://leetcode.com/problems/subsets-ii/ LeetCode版本12345678910111213141516171819202122232425262728293031323334353637class Solution &#123;public: vector&lt;vector&lt;int&gt;&gt; ans; vector&lt;int&gt; path; // 为了保证不产生重复的，先是进行排序，然后对于相同的数字，枚举的个数是和数字的次数相关 vector&lt;vector&lt;int&gt;&gt; subsetsWithDup(vector&lt;int&gt;&amp; nums) &#123; sort(nums.begin(), nums.end()); dfs(nums, 0); return ans; &#125; void dfs(vector&lt;int&gt;&amp; nums, int u) &#123; if( u ==nums.size()) &#123; ans.push_back(path); return; &#125; // u表示当前的数字，然后k 表示当前数字重复的个数 int k =0; while( u+k &lt;nums.size() &amp;&amp; nums[u+k] == nums[u]) k++; // 重复的数字可以选择 0~k 个这样的数字 for(int i =0; i&lt;=k ;i++) &#123; dfs(nums, u+k); path.push_back(nums[u]); &#125; for(int i =0; i&lt;=k ;i++) path.pop_back(); &#125; &#125;; 单机版123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566#include&lt;iostream&gt;#include&lt;algorithm&gt;#include&lt;vector&gt;using namespace std;#define LEN 10vector&lt;vector&lt;int&gt;&gt; res;vector&lt;int&gt; path;void dfs(vector&lt;int&gt;&amp; arr, int u)&#123; if(u == arr.size()) &#123; res.push_back(path); return; &#125; // 数字出现的k 次数 int k =0; while (k+u&lt; arr.size() &amp;&amp; arr[k+u] ==arr[u]) &#123; k ++; &#125; for(int i=0; i&lt;=k; i++) &#123; dfs(arr, u+k); path.push_back(arr[u]); &#125; // 回溯 for(int i =0; i&lt;=k ; i++) &#123; path.pop_back(); &#125;&#125;int main()&#123; int n; //vector&lt;int&gt; arr(LEN); cin&gt;&gt;n; vector&lt;int&gt; arr(n); for(int i =0; i&lt;n; i++) cin&gt;&gt;arr[i]; //for(int i =0; i&lt;n; i++) cout &lt;&lt; arr[i]&lt;&lt; " "; sort(arr.begin(), arr.end()); dfs(arr, 0); for(auto u: res) &#123; for(int i =0; i&lt;u.size(); i++) &#123; cout &lt;&lt;arr[i]&lt;&lt; " "; &#125; cout&lt;&lt;endl; &#125; return 0;&#125; 216. Combination Sum III 原题链接 Leetcode 版本12345678910111213141516171819202122232425262728class Solution &#123;public: //dfs(枚举数字的个数, 当前枚举的数字, 数字的总和) vector&lt;vector&lt;int&gt;&gt; ans; vector&lt;int&gt; path; vector&lt;vector&lt;int&gt;&gt; combinationSum3(int k, int n) &#123; dfs(k, 1, n); return ans; &#125; void dfs(int k, int start, int n) &#123; if(!k) &#123; if(!n) ans.push_back(path); return ; &#125; for(int i =start; i&lt;=9; i++) &#123; path.push_back(i);// 这个还是本程序的步骤 dfs(k -1, i +1, n-i);// 这个是状态的转移，是下一个步骤 path.pop_back(); &#125; &#125; &#125;; 单机版1234567891011121314151617181920212223242526272829303132333435363738394041424344#include&lt;iostream&gt;#include&lt;vector&gt;#include&lt;algorithm&gt;using namespace std;vector&lt;vector&lt;int&gt;&gt; ans;vector&lt;int&gt; path;void dfs(int k, int start, int n)&#123; if(!k) &#123; if(!n) ans.push_back(path); return; &#125; for(int i =start; i&lt;=9; i++) &#123; path.push_back(i); dfs(k -1, i+1, n-i); path.pop_back(); &#125;&#125;int main()&#123; int k,n; cin &gt;&gt;k&gt;&gt;n; dfs(k, 1, n); for(auto u: ans) &#123; for(int i=0;i&lt;u.size(); i++) cout&lt;&lt; u[i]&lt;&lt;" "; cout &lt;&lt;endl; &#125; return 0;&#125; 52. N-Queens II 视频讲解 关键是按照行进行dfs(), 然后使用 col 对列进行判断，使用 d,nd 对于对角线和斜对角线进行判断。 针对对角线和斜对角线， 是 x+y 和 x-y+n 这样的形式进行判断的。 12345678910111213141516171819202122232425262728293031323334353637383940class Solution &#123;public: int ans =0, n; vector&lt;bool&gt; col, d, nd; int totalNQueens(int _n) &#123; n =_n; // 初始化的时候，使用这样的语句 col =vector&lt;bool&gt;(n); d =nd =vector&lt;bool&gt;(n*2); dfs(0); return ans; &#125; void dfs(int row) &#123; if(row ==n) &#123; ans +=1; return ; &#125; for(int i =0; i&lt; n; i++) &#123; if(!col[i] &amp;&amp; !d[row+i] &amp;&amp; !nd[row-i +n]) &#123; col[i] =d[row+i] =nd[row-i +n] =true; dfs(row+1); col[i] =d[row+i] =nd[row-i +n] =false; &#125; &#125; &#125;&#125;; 单机版12345678910111213141516171819202122232425262728293031323334353637383940414243#include&lt;iostream&gt;#include&lt;vector&gt;using namespace std;int ans =0, n;vector&lt;bool&gt; col, d, nd;void dfs(int row)&#123; if(row ==n) &#123; ans ++; return; &#125; for(int i =0; i&lt; n; i++) &#123; if( !col[i] &amp;&amp; !d[row+i] &amp;&amp; !nd[row-i+n]) &#123; col[i] =d[row+i] =nd[row-i+n] =true; dfs(row+1); col[i] =d[row+i]=nd[row-i+n] =false; &#125; &#125; &#125;int main()&#123; cin &gt;&gt;n; col =vector&lt;bool&gt;(n); d =nd =vector&lt;bool&gt;(2*n); dfs(0); cout &lt;&lt; ans&lt;&lt;endl; return 0;&#125; leetcode 讲解版473. Matchsticks to Square 原题连接 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556class Solution &#123;public: /** (nums, 当前的第几条边，当期的边的长度，边的总长度) 剪枝： 1. 从大到小枚举（先是枚举分支少的，因为这样，如果发生了剪枝，那么效果更加明显） 2. 如果dfs() 失败，且当前边是第一条，那么剪枝 3. 如果dfs() 失败，且当期边是最后一天，那么剪枝 4. 如果当前的边失败，那么之后和其相同的边也是会失败的 */ // 剪枝是在内存和时间上的优化，即使没有剪枝，给了足够的时间和空间，那么最后也是能够得到结果、 vector&lt;bool&gt; st; bool makesquare(vector&lt;int&gt;&amp; nums) &#123; int length =0; for(auto u : nums) length += u; if( !length || length %4) return false; // 排序 这是剪枝的操作 ，时间的上的操作 sort(nums.begin(), nums.end()); reverse(nums.begin(), nums.end()); st= vector&lt;bool&gt;(nums.size()); return dfs(nums, 0, 0, length/4); &#125; bool dfs(vector&lt;int&gt;&amp; nums, int u, int cur, int sum) &#123; if(cur ==sum) cur =0, u+=1; if( u ==4) return true; for(int i =0; i&lt; nums.size(); i++) if(!st[i] &amp;&amp; cur+ nums[i] &lt;= sum) &#123; st[i] =true; // 这个判断条件是比较清奇的 if(dfs(nums, u, cur+nums[i], sum)) return true; st[i] =false; //开始进行剪枝 if(!cur) return false; if( cur+nums[i] ==sum ) return false; while(i+1 &lt;= nums.size() &amp;&amp; nums[i] ==nums[i+1]) i+=1; &#125; return false; &#125;&#125;; 单机版 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253#include&lt;iostream&gt;#include&lt;vector&gt;#include&lt;algorithm&gt;using namespace std;vector&lt;bool&gt; st;bool dfs(vector&lt;int&gt;&amp; arr, int u, int cur, int sum)&#123; if(cur ==sum ) cur =0, u +=1; if( u==4) return true; for(int i =0; i&lt; arr.size(); i++) if(!st[i] &amp;&amp; cur+arr[i] &lt;= sum) &#123; st[i] =true; if(dfs(arr, u, cur+ arr[i], sum )) return true; st[i] =false; //开始进行剪枝 if(!cur) return false; if(arr[i] +cur ==sum ) return false; // 如何去表示一条边的最后一个 while(i+1 &lt;= arr.size() &amp;&amp; arr[i+1] ==arr[i]) i++; &#125; return false;&#125;int main()&#123; int n; cin&gt;&gt;n; vector&lt;int&gt; arr(n); for(int i =0;i&lt;n; i++) cin&gt;&gt;arr[i]; int sum =0; for(auto u : arr) sum +=u; st =vector&lt;bool&gt;(n); if(!sum || sum %4) cout&lt;&lt; "false"&lt;&lt;endl; // 第一个剪枝操作，从大到小枚举 sort(arr.begin(), arr.end()); reverse(arr.begin(), arr.end()); bool flag =dfs(arr, 0, 0, sum/4); cout &lt;&lt; flag&lt;&lt;endl; return 0;&#125;]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>dfs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Model Selection]]></title>
    <url>%2F2019%2F08%2F12%2Fmodel-selection%2F</url>
    <content type="text"><![CDATA[介绍模型选择的两种方法：交叉检验和正则化。使用交叉检验（K-fold cross validation）进行选择模型并不难理解，但正则化和模型选择的关系，一开始还不是很理解，所以通过整理，希望能带来一些启发。 模型选择模型选择有两种方式，一种是交叉验证，一种是正则化。对于前者是常规的做法，对于后者是从模型的复杂度角度去理解。 K-fold cross validation交叉检验通常被用作评估一个机器学习模型的表现，可以用于进行模型的选择。而K折交叉验证(K-fold cross validation)指的是把训练数据D 分为 K份，用其中的(K-1)份训练模型，把剩余的1份数据用于评估模型的质量。将这个过程在K份数据上依次循环，并对得到的K个评估结果进行合并，如求平均或投票。 具体的步骤如下： 第一步，不重复抽样将原始数据随机分为 k 份。 第二步，每一次挑选其中 1 份作为测试集，剩余 k-1 份作为训练集用于模型训练。 第三步，重复第二步 k 次，这样每个子集都有一次机会作为测试集，其余机会作为训练集。 在每个训练集上训练后得到一个模型， 用这个模型在相应的测试集上测试，计算并保存模型的评估指标， 第四步，计算 k 组测试结果的平均值作为模型精度的估计，并作为当前 k 折交叉验证下模型的性能指标。 当 K =10 的时候 ，训练数据D被分为了10 份，每次取其中9份数据作为训练集，1份作为测试集，最终将循环后所有的评估结果取平均。 交叉验证的分类 当然有两种比较极端的 K 的取值，当k =1，全部的数据被用于训练，相当于只有训练数据集；当k =n 的时候，也被称为留一法。留一法交叉验证：假设有N个样本，将每一个样本作为测试样本，其它N-1个样本作为训练样本。这样得到N个分类器，N个测试结果。用这N个结果的平均值来衡量模型的性能。 如何去理解交叉验证呢？ 简单的说是可以从方差和偏差的角度进行分析。当k =1，全部的数据被用于训练，容易出现过拟合，容易出现低偏差、高方差的；当k =n，也被称为留一法，偏差升高了而方差是减少了。所以说取值范围的变化可以看做是偏差和方差相互妥协的过程。 如何选择K 的值 经验上的选择，一般k =10。当数据量比较大的时候，k 可以小点；当数据量比较小的时候，k 可以小点。基本的原则是保证 有足够的训练样本用于训练。 正则化角度模型训练过程中很容易出现过拟合，根本原因是模型的复杂度远小于数据量，模型的复杂度和训练误差于测试误差的关系如下图所示。 从图中可以看到， 随着模型复杂度的提高，训练误差是不断下降的，但测试误差是先下降后上升的。模型选择 的经典方法是正则化。正则化的作用是选择经验风险于模型复杂度同时较小的模型。也可以说，在所有可以选择的模型中，能够很好的解释数据并且十分简单的才是好模型。 学习方法的泛化能力是指由该方法学习到的模型对未知数据的预测能力，是学习方法本质上重要的性质。现实中采用最多的方法是通过测试误差来评价学习方法的泛化能力。 结构化风险要区分这三个概念，首先要引入一个损失函数的概念。损失函数是期望风险、经验风险和结构风险的基础。 损失函数是针对单个具体的样本而言的。表示的是模型预测的值与样本真实值之间的差距。比如对于某个样本$(x_i, y_i)$，其真实的值为Yi,而我们的模型选择决策函数为 $f $,那么通过模型预测的值为$ f(x_i)$;损失函数就是用来表示$y_i$与$ f(x_i)$之间的差距的，我们用函数 $L(f(x_),y_i) $来衡量。我们希望的是这个L函数最小化。理想的情况是我们的模型决策函数预测值$f（x_i）$刚好等于该样本的真值 $y_i$。常见的损失函数有： 平方损失函数、绝对值损失函数和对数损失函数。 通过损失函数我们只能知道模型决策函数 $f(x) $对于单个样本点的预测能力（借用损失函数 $L(y ,f(x)) $，损失函数越小，说明模型对于该样本预测越准确。），那么如果想知道模型 $f(x) $对训练样本中所有的样本的预测能力应该怎么办呢？显然只需所有的样本点都求一次损失函数然后进行累加就好了。如下式 $$ (R_{exp}(f)=\frac{1}{N} \sum_{i=1}^{N} L\left(y_{i}, f\left(x_{i}\right)\right))$$ 这就经验风险，所谓的经验风险最小化便是让这个式子最小化，注意这个式子中累加和的上标N表示的是训练样例集中样本的数目。 经验风险是对训练集中的所有样本点损失函数的平均最小化。经验风险越小说明模型f(X)对训练集的拟合程度越好，但是对于未知的样本效果怎么样呢？我们知道未知的样本数据$（x, y）$的数量是不容易确定的，所以就没有办法用所有样本损失函数的平均值的最小化这个方法，那么怎么来衡量这个模型对所有的样本（包含未知的样本和已知的训练样本）预测能力呢？熟悉概率论的很容易就想到了用期望。即假设 $x $和 $y $服从联合分布 $P(x ,y) $.那么期望风险就可以表示为： $$R_{exp}(f)=E_{P}[L(Y, f(X))]=\int_{\mathrm{x \times y}} L(y, f(x)) P(x, y) d x d y$$ 这就是期望风险，期望风险表示的是全局的概念，表示的是决策函数对所有的样本 $x,y $预测能力的大小，而经验风险则是局部的概念，仅仅表示决策函数对训练数据集里样本的预测能力。理想的模型（决策）函数应该是让所有的样本的损失函数最小的（也即期望风险最小化），但是期望风险函数往往是不可得到的，即上式中， $x $与 $ y $的联合分布函数不容易得到。现在我们已经清楚了期望风险是全局的，理想情况下应该是让期望风险最小化，但是呢，期望风险函数又不是那么容易得到的。怎么办呢？那就用局部最优的代替全局最优这个思想吧。这就是经验风险最小化的理论基础。 通过上面的分析可以知道，经验风险与期望风险之间的联系与区别。现在在总结一下： 经验风险是局部的，基于训练集所有样本点损失函数最小化的。 期望风险是全局的，是基于所有样本点的损失函数最小化的。 经验风险函数是现实的，可求的； 期望风险函数是理想化的，不可求的； 只考虑经验风险的话，会出现过拟合的现象，过拟合的极端情况便是模型f(x)对训练集中所有的样本点都有最好的预测能力，但是对于非训练集中的样本数据，模型的预测能力非常不好。怎么办呢？这个时候就引出了结构风险。结构风险是对经验风险和期望风险的折中。在经验风险函数后面加一个正则化项（惩罚项）便是结构风险了。如下式： $$R_{\mathrm{em}}(f)=\frac{1}{N} \sum_{i=1}^{N} L\left(y_{i}, f\left(x_{i}\right)\right)+\lambda J(f)$$ 相比于经验风险，结构风险多了一个惩罚项，其中是一个 $\lambda $是一个大于0的系数。$J(f) $表示的是是模型f的复杂度。结构风险可以这么理解： 经验风险越小，模型决策函数越复杂，其包含的参数越多，当经验风险函数小到一定程度就出现了过拟合现象。也可以理解为模型决策函数的复杂程度是过拟合的必要条件，那么我们要想防止过拟合现象的方式，就要破坏这个必要条件，即降低决策函数的复杂度。也即，让惩罚项 $J(f) $最小化，现在出现两个需要最小化的函数了。我们需要同时保证经验风险函数和模型决策函数的复杂度都达到最小化，一个简单的办法把两个式子融合成一个式子得到结构风险函数然后对这个结构风险函数进行最小化。 常见的说法： 极大似然估计是经验风险最小化的一个例子，当模型是条件概率分布，损失函数是对数损失函数时，经验风险最小化就等于极大似然估计。（但是，当样本量很小时，经验风险最小化学习的效果未必很好，会产生后面叙述的“过拟合”现象。） 结构风险最小化是为了防止过拟合而提出来的 策略。结构风险最小化等价于正则项或罚项。 贝叶斯估计中的最大后验概率估计（MAP,maximum posterior probability estimation）就是结构风险最小化的一个例子。当模型是条件概率分布、损失函数是对数损失函数、模型复杂度由模型的先验概率表示时，结构风险最小化就等价于最大后验概率估计。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>结构化风险</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Union-Find (并查集算法)]]></title>
    <url>%2F2019%2F08%2F05%2Funion-find%2F</url>
    <content type="text"><![CDATA[并查集是一种树型数据结构，用于处理不相交 (Disjoint Sets) 的合并及查询问题。 定义并查集，在一些有N个元素的集合应用问题中，我们通常是在开始时让每个元素构成一个单元素的集合，然后按一定顺序将属于同一组的元素所在的集合合并，其间要反复查找一个元素在哪个集合中。 对于查找操作，假设需要确定x所在的的集合，也就是确定集合的代表元。判断两个元素是否属于同一集合，只需要看他们的代表元是否相同即可。并查集为了避免时间和空间上的损耗，在每一轮的查找时，都要进行一次路径压缩优化。 路径压缩 ： 在递归找到根节点的时候，把当前节点到根节点间所有节点的父节点都设置为根节点。 我们来看下图，首先我们有个这样的一棵树，现在要找到元素9所在树的根节点，在找根节点的过程中使用路径压缩，也就是说9到根的路径上的节点9，6，3，1的父节点都设置成为根节点0，所以呢，在FIND-SET(9)之后，树的形态就变成了下面的样子 我们可以看到经过路径压缩的节点及其子树到根节点的深度减小了很多，所以在以后的操作中，查找根节点的速度会快很多 这样可以将查询一个结点的根节点的时间复杂度从 O(log N) 降到 O(1) 还有一个比较有趣的图解可以说明这个优化： 基于C语言的两种实现基于数组和结构体的两种实现（C 语言），一般来说简单问题使用数组，复杂问题使用结构体。 数组 初始化 123456789101112#define MAX 10000int set[max];//集合index的类别，或者用parent表示int rank[max];//集合index的层次，通常初始化为0int data[max];//集合index的数据类型//初始化集合void Make_Set(int i)&#123; set[i]=i;//初始化的时候，一个集合的parent都是这个集合自己的标号。没有跟它同类的集合，那么这个集合的源头只能是自己了。 rank[i]=0;&#125; 查找函数 123456789//查找集合i（一个元素是一个集合）的源头（递归实现）int Find_Set(int i)&#123; //如果集合i的父亲是自己，说明自己就是源头，返回自己的标号 if(set[i]==i) return set[i]; //否则查找集合i的父亲的源头 return Find_Set(set[i]); &#125; 合并函数 123456789101112void Union(int i,int j)&#123; i=Find_Set(i); j=Find_Set(j); if(i==j) return ; if(rank[i]&gt;rank[j]) set[j]=i; else &#123; if(rank[i]==rank[j]) rank[j]++; set[i]=j; &#125;&#125; 结构体实现 初始化 123456struct Node&#123; int data; int rank; int parent; &#125;node[MAX]; 查找函数 1234567891011/***查找集合i（一个元素是一个集合）的源头（递归实现）。 如果集合i的父亲是自己，说明自己就是源头，返回自己的标号； 否则查找集合i的父亲的源头。**/int get_parent(int x)&#123; if(node[x].parent==x) return x; return get_parent(node[x].parent);&#125; 合并函数 12345678910111213void Union(int a,int b)&#123; a=get_parent(a); b=get_parent(b); if(node[a].rank&gt;node[b].rank) node[b].parent=a; else &#123; node[a].parent=b; if(node[a].rank==node[b].rank) node[b].rank++; &#125;&#125; 应该是有三个版本的 union-find 算法的： find() union() 总的时间复杂度 quick-find O(1) O(N) O(M*N) quick-union O(logN ~ N) O(1) O(M*N) 极端 WeightedUF O(log N) O(N) O(M *log N) 这里有三个版本的实现 应用1、维护无向图的连通性。支持判断两个点是否在同一连通块内，和判断增加一条边是否会产生环。2、媒体社交（比如：向通一个社交圈的朋友推荐商品）3、数学集合（比如：判断元素p,q之后选择是否进行集合合并） 常见的一个算法题， 给出10W 条任何人之间的朋友关系，求这些朋友关系中有多少个朋友圈，并且给出算法的时间复杂度。样例：A-B, B -C, D-E, E-F 这 四对关系中存在着两个朋友圈 参考文献 https://www.cnblogs.com/SeaSky0606/p/4752941.html]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>union-find</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LSTM]]></title>
    <url>%2F2019%2F08%2F04%2Flstm%2F</url>
    <content type="text"><![CDATA[主要介绍 LSTM 的概念和网络结构。 循环神经网络(RNN)一个RNN可以看作是同一个网络的多份副本，每一份都将信息传递到下一个副本。如果我们将环展开的话： 这种链式结构展示了RNN与序列和列表的密切关系。RNN的这种结构能够非常自然地使用这类数据。 RNN 的主要应用如下： 文本相关。主要应用在自然语言处理方面（NLP）、对话系统、情感分析、机器翻译 时序相关。就是在做时序预测问题，比如预测天气、温度，包括有很多人使用其在做预测股票价格的问题 长期依赖(Long Term Dependencies)的问题有时候，我们只需要看最近的信息，就可以完成当前的任务。比如，考虑一个语言模型，通过前面的单词来预测接下来的单词。如果我们想预测句子“the clouds are in the sky”中的最后一个单词，我们不需要更多的上下文信息——很明显下一个单词应该是sky。 RNN 是可以被用来进行这样问题的训练学习。 然而，有时候我们需要更多的上下文信息。比如，我们想预测句子“I grew up in France… I speak fluent French”中的最后一个单词。不幸的是，随着距离的增大，RNN对于如何将这样的信息连接起来无能为力。 LSTM 中的基本概念LSTM 是用来解决RNN 中的梯度消失/ 梯度爆炸问题的，可以处理 long-term sequence了。 门（gate ）定义： gate 实际上就是一层全连接层，输入是一个向量，输出是一个 0到1 之间的实数向量。公式如下：$$g ( \mathbf { x } ) = \sigma ( W \mathbf { x } + \mathbf { b } )$$ 遗忘门（forget gate）它决定了上一时刻的单元状态 $c_{t-1} $有多少保留到当前时刻$ c_t$输入门（input gate）它决定了当前时刻网络的输入 $x_t$ 有多少保存到单元状态 $c_t$输出门（output gate）控制单元状态$ c_t $有多少输出到 LSTM 的当前输出值 $h_t$ LSTM网络在普通的RNN中，重复模块结构非常简单，例如只有一个tanh层。 LSTM也有这种链状结构，不过其重复模块的结构不同。LSTM的重复模块中有4个神经网络层，并且他们之间的交互非常特别。 LSTM分步详解LSTM的第一步是决定我们将要从元胞状态中扔掉哪些信息。遗忘门观察$h_{t−1}$和 $x_t$，对于元胞状态 $C_{t−1} $中的每一个元素，输出一个0-1之间的数。1表示“完全保留该信息”，0表示“完全丢弃该信息”。 下一步是决定我们将会把哪些新信息存储到元胞状态中。这步分为两部分。首先，有一个叫做“输入门(Input Gate)”的Sigmoid层决定我们要更新哪些信息。接下来，一个tanh层创造了一个新的候选值，$\tilde { C } _ { t }$，该值可能被加入到元胞状态中。在下一步中，我们将会把这两个值组合起来用于更新元胞状态。 现在我们该更新旧元胞状态 $C_{t−1} $到新状态 $C_t$了。上面的步骤中已经决定了该怎么做，这一步我们只需要实际执行即可。 最后，我们需要决定最终的输出。输出将会基于目前的元胞状态，并且会加入一些过滤。首先我们建立一个Sigmoid层的输出门(Output Gate)，来决定我们将输出元胞的哪些部分。然后我们将元胞状态通过tanh之后（使得输出值在-1到1之间），与输出门相乘，这样我们只会输出我们想输出的部分。 优点： 解决了RNN 中的梯度消失的问题，可以处理 长依赖 缺点： 计算复杂度高，运行时间长 GRUGRU (gated recurrent unit) 是对于 LSTM 速度上的提升，但是相应的表达能力也受到了限制 GRU 中一共有两个门。GRU 把LSTM 中遗忘门(forget gate) 和输入门(input gate) 使用 更新门(update gate) 进行代替。还有一个重置门(reset gate)， 重置门主要决定了多少过去的信息需要遗忘。GRU 不会保存内部记忆 context，而且没有输出门。 复习 RNN的应用场景： 文本相关和时序相关（比如 预测天气、温度） LSTM 主要是解决RNN 中的梯度消失/ 梯度爆炸问题，使用遗忘门、输入门 和输出门分别控制上一时刻信息到context的保留程度、当前时刻输入到context的程度和context信息输出程度。可以处理常依赖的问题。]]></content>
      <categories>
        <category>NLP</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Algorithm Practice]]></title>
    <url>%2F2019%2F07%2F30%2Falgorithm-practice%2F</url>
    <content type="text"><![CDATA[刷题整理笔记。 最小编辑距离 leetcode 版本 123456789101112131415161718192021222324252627282930313233343536373839404142#include&lt;iostream&gt;#include&lt;vector&gt;using namespace std;class Solution &#123;public: int minDistance(string word1, string word2) &#123; int rows = word1.length(); int cols = word2.length(); vector&lt;vector&lt;int&gt; &gt; dp(rows+1, vector&lt;int&gt;(cols+1, 0)); for(int i=1; i&lt;=rows; ++i) dp[i][0] = i; for(int j=1; j&lt;=cols; ++j) dp[0][j] = j; for(int i=1; i&lt;=rows; ++i)&#123; for(int j=1; j&lt;=cols; ++j)&#123; if(word1[i-1] == word2[j-1]) dp[i][j] = dp[i-1][j-1]; else dp[i][j] = min(dp[i-1][j-1], min(dp[i-1][j], dp[i][j-1])) + 1; &#125; &#125; return dp[rows][cols]; &#125;&#125;;int main()&#123; Solution solution; string str1 ="hello"; string str2 ="hello1"; cout &lt;&lt; solution.minDistance(str1, str2) &lt;&lt; endl; return 0;&#125; 单机版本12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849#include&lt;iostream&gt;#include&lt;vector&gt;#include&lt;string&gt;#include&lt;algorithm&gt;using namespace std;/**使用dp 的思想， f[i][j] 表示string1中第i 个位置和 string2 中第j 位置之间的最小编辑距离 转移: f[i][j] =f[i-1][j-1] if string1[i] ==string2[j] = min(f[i-1][j], f[i][j-1] ) 对应的是修改 初始化: f[0][0] =1, 然后f[i][0] 和 f[0][j] 都是0， 最后的f[m][n] 就是最后的结果 **/int main()&#123; string str1, str2; cin &gt;&gt; str1&gt;&gt; str2; int n,m; n =str1.size(), m =str2.size(); vector&lt;vector&lt;int&gt;&gt; f(n+1, vector&lt;int&gt;(m+1)); // 初始化需要根据实际意义进行 f[0][0] =0; for(int i =1; i&lt;=n; i++) f[i][0] =i; for(int j =1; j&lt;=m ;j++) f[0][j] =j; for(int i =1; i&lt;=n; i++) &#123; for(int j =1; j&lt;=m; j++) &#123; //if(!i ||!j) f[i][j] =0; if(str1[i] ==str2[j]) f[i][j] =f[i-1][j-1]; else &#123; f[i][j] =min(f[i-1][j-1], min(f[i-1][j], f[i][j-1]))+1; &#125; &#125; &#125; cout&lt;&lt;f[n][m]&lt;&lt;endl; return 0;&#125; python 语言实现 注意两种初始化的区别：12dp =[(cols+1)*[0]]*(rows+1) # 这种是不 work 的dp =[ [0] *(cols+1) for _ in range(rows+1)] # 这种是 work 的 123456789101112131415161718192021222324252627282930def minDistance(string1, string2): if not string1 or not string2 : return rows, cols =len(string1), len(string2) if rows ==0: return cols; elif cols ==0: return rows; #dp =[(cols+1)*[0]]*(rows+1) dp =[ [0] *(cols+1) for _ in range(rows+1)] for i in range(1, rows+1): dp[i][0] =i for j in range(1, cols+1): dp[0][j] =j #ipdb.set_trace() for i in range(1, rows+1): for j in range(1, cols+1): if string1[i-1] ==string2[j-1]: dp[i][j] =dp[i-1][j-1] else: dp[i][j] =1 + min(dp[i-1][j-1], dp[i-1][j], dp[i][j-1]) return dp[-1][-1]print(minDistance("string1", "string")) 从无头单链表中删除节点 删除节点的通常做法是找到该结点的前一个结点（头结点），然后 head.next =head.next.next这个题目说没有头结点，直接给出的就是应该删除的结点假设这个是头结点，那么下一个是待删除的结点，所以 current.next =current.next.next ，但是需要把current.next.value 赋值给 current.value 12345678910111213141516171819202122232425262728#include&lt;stdio.h&gt;#include&lt;stdlib.h&gt;#include&lt;assert.h&gt;typedef struct node&#123; int data; node *next;&#125;Node;void delete_node(Node *current)&#123; assert(current !=NULL); Node *next =current-&gt;next; if (next !=NULL) &#123; current-&gt;next =next-&gt;next current-&gt; data =next-&gt; data; &#125;&#125;int main()&#123; return 0;&#125; Best Time to Buy and Sell Stock Say you have an array for which the ith element is the price of a given stock on day i.If you were only permitted to complete at most one transaction (ie, buy one and sell one share of the stock), design an algorithm to find the maximum profit. Tips: 记录一个最小值 和最大的max differences 1234567891011121314151617181920class Solution: def maxProfit(self, prices): if not prices or len(prices) ==1: return 0 low, maxDiff =prices[0], 0 for i in range(1, len(prices)): if prices[i] &lt; low: low =prices[i] else: diff =prices[i] -low if diff&gt; maxDiff: maxDiff =diff return maxDiff Design an algorithm to find the maximum profit. You may complete as many transactions as you like (i.e., buy one and sell one share of the stock multiple times).Note: You may not engage in multiple transactions at the same time (i.e., you must sell the stock before you buy again). Tips： 这个是可以进行多次买卖，如同寻找增序列 1234567891011class Solution: def maxProfit(self, prices): total =0 for i in range(1, len(prices)): if prices[i] &gt;= prices[i-1]: total += prices[i]-prices[i-1] return total 最多进行两次交易。 这个返回的最后是最大值 maximum profit。好好理解一下吧 1234567891011121314151617class Solution(object): def maxProfit(self, prices): """ :type prices: List[int] :rtype: int """ one_buy = two_buy = sys.maxsize one_profit = two_profit = 0 for p in prices: one_buy = min(one_buy, p) one_profit = max(one_profit, p - one_buy) two_buy = min(two_buy, p - one_profit) # 为什么和 one_profit 进行比较呢 two_profit = max(two_profit, p - two_buy) return two_profit Design an algorithm to find the maximum profit. You may complete at most k transactions. 最多进行 k 次交易 https://leetcode.com/problems/best-time-to-buy-and-sell-stock-iv/discuss/273435/Python-1-D-DP 123456789def maxProfit(k, prices): if k &gt; len(prices) &gt;&gt; 1: return sum(prices[i+1]-prices[i] for i in range(len(prices)-1) if prices[i+1]&gt;prices[i]) cash, asset = [float('-inf')] * (k+1), [0] * (k+1) for price in prices: for i in range(1,k+1): cash[i] = max(cash[i], asset[i-1]-price) asset[i] = max(asset[i], cash[i]+price) return asset[k] 这个是比较好理解一些的https://blog.csdn.net/xx_123_1_rj/article/details/80857144 12345678910111213141516171819202122class Solution: def maxProfit(self, k, prices): n, res = len(prices), 0 if n &lt; 2: return 0 if k &gt; n //2: # 现在这个情况，就相当于题目2 for i in range(1, n): if prices[i] &gt; prices[i-1]: res += prices[i] - prices[i-1] return res hold, sold = [float('-inf')] * (k + 1), [0] * (k + 1) for price in prices: for j in range(1, k+1): hold[j] = max(hold[j], sold[j-1]-price) # hold-&gt;hold, sold-&gt;hold sold[j] = max(sold[j], hold[j]+price) # sold-&gt;sold, hold-&gt;sold return sold[k] if __name__ == '__main__': prices, k = [7, 1, 5, 3, 6, 4], 4 solu = Solution() print(solu.maxProfit(k, prices))]]></content>
      <categories>
        <category>算法</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Transfer Learning]]></title>
    <url>%2F2019%2F07%2F29%2Ftransfer-learning%2F</url>
    <content type="text"><![CDATA[介绍迁移学习 和多任务学习。 迁移学习定义： 涉及一个源领域source domain和一个目标领域（target domain），在source domain学习，并把学习到的知识迁移到target domain，提升target domain的学习效果（performance）。起源： 在一些新的领域，标注数据很难获得；目标：从一个环境中学习到的知识帮助新环境的学习任务。和传统机器学习的理论区别：传统的机器学习假设训练数据与测试数据服从相同的数据分布，迁移学习不会像传统机器学习那样作同分布假设。迁移学习是指一个学习算法可以利用不同学习任务之间的共性来共享统计的优点和在任务间迁移知识。 基本上，神经网络迁移学习主要有两个应用场景：特征提取（Feature Extraction）和微调（Fine Tuning）。第一种： 特征提取：在特征提取中，可以在预先训练好的网络结构后添加一个简单的分类器，将源任务上的预先训练好的网络作为另一个目标任务的特征提取器，只对最后增加的分类器参数进行重新学习，而预先训练好的网络参数不被修改。这使得新任务的特征提取时使用的是源任务中学习到的参数，而不用重新学习所有参数。适用场景：源任务 和目标任务保持同一分布最好，因为网络参数是更加适合的。第二种：微调固定底层的参数，调整一些顶层的参数。这样做的好处可以减少训练参数的数量，同时也有助于克服过拟合现象的发生，尤其是当目标任务的数据量不足够大的时候，该方法实践起来很有效果。实际上，微调要优于特征提取，因为它能够对迁移过来的预训练网络参数进行优化，使其更加适合新的任务。 多任务学习是迁移算法的一种。 多任务学习定义：基于共享表示，把多个相关的任务放到一起进行学习的一种机器学习方法。多任务学习是相对于单任务学习的一种机器学习的方法。 例子：我们希望有一个网络模型可以将输入的人脸图像分类为男性或女性，同时还能够预测其年龄。这个案例中有两个相关的任务：一个是二元分类任务，另一个是回归任务。显而易见，这两个任务是相关的，学习其中一个任务的同时应该增强对另一个任务的理解。 共享部分学习到的是多个任务的共享表示，共享表示具有较强的抽象能力，能够适应多个不同但相关的目标任务，通常使得多任务中的主任务获得更好的泛化能力。另一方面，针对每个不同的任务都会设计具体的顶层网络结构（头），顶层网络结构用来学习如何使用共享表示来完成每个特定的任务。 单任务学习 vs 多任务学习： 单任务学习：一次只学习一个任务（task），大部分的机器学习任务都属于单任务学习。多任务学习：把多个相关（related）的任务放在一起学习，同时学习多个任务。 从图中可以发现，单任务学习时，各个task任务的学习是相互独立的，多任务学习时，多个任务之间的浅层表示共享（shared representation） 分类： 一是不同任务之间共享相同的参数（common parameter），二是挖掘不同任务之间隐藏的共有数据特征（latent feature） 多任务学习有效性的原因：（1）多人相关任务放在一起学习，有相关的部分，但也有不相关的部分。当学习一个任务（Main task）时，与该任务不相关的部分，在学习过程中相当于是噪声，因此，引入噪声可以提高学习的泛化（generalization）效果。（2）多个任务在浅层共享表示，可能削弱了网络的能力，降低网络过拟合，提升了泛化效果。]]></content>
      <categories>
        <category>Deep learning</category>
      </categories>
      <tags>
        <tag>transfer_learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Binary Search]]></title>
    <url>%2F2019%2F07%2F23%2Fbinary-search%2F</url>
    <content type="text"><![CDATA[介绍二分查找中的三个模板，以 LeetCode 中的习题为例。 首先介绍一下二分查找中使用的术语： 目标 Target —— 你要查找的值索引 Index —— 你要查找的当前位置左、右指示符 Left，Right —— 我们用来维持查找空间的指标中间指示符 Mid —— 我们用来应用条件来确定我们应该向左查找还是向右查找的索引 模板1模板 1 是二分查找的最基础和最基本的形式。这是一个标准的二分查找模板，是非常基础简单的二分查找。模板 1 用于查找可以通过访问数组中的单个索引来确定的元素或条件。模版 1 不需要后处理，因为每一步中，你都在检查是否找到了元素。如果到达末尾，则知道未找到该元素。 语法关键： 初始条件：left = 0, right = length-1终止：left &gt; right向左查找：right = mid-1向右查找：left = mid+1 模版#1 对应的例题为：LeetCode69 x 的平方根LeetCode374 猜数字大小LeetCode33 搜索旋转排序数组 模板2模板 2 是二分查找的高级模板。它用于查找需要访问数组中当前索引及其直接右邻居索引的元素或条件。查找条件需要访问元素的直接右邻居。使用元素的右邻居来确定是否满足条件，并决定是向左还是向右。保证查找空间在每一步中至少有 2 个元素。需要进行后处理。 当你剩下 1 个元素时，循环 / 递归结束。 需要评估剩余元素是否符合条件。 语法关键： 初始条件：left = 0, right = length终止：left == right向左查找：right = mid向右查找：left = mid+1 模版#2 对应的例题为：LeetCode278 第一个错误的版本LeetCode75 寻找峰值LeetCode159 寻找旋转排序数组中的最小值 LeetCode69 x 的平方根 实现 int sqrt(int x) 函数。 123456789101112131415161718class Solution &#123;public: int mySqrt(int x)&#123; int left = 0; int right = x; if (x &lt;= 1) return x; int ans= 0; while(left&lt;=right)&#123; int mid = (right+left) /2; //计算中间值 if(x/mid &gt;= mid ) &#123; left = mid+1; //如果mid*mid&lt;=x 证明mid小了 left = mid+1 ans = mid; //当前的mid作为ans &#125; else right = mid-1; //否则right = mid-1 &#125; return ans; &#125;&#125;; LeetCode278 第一个错误的版本 由于每个版本都是基于之前的版本开发的，所以错误的版本之后的所有版本都是错的。你可以通过调用 bool isBadVersion(version) 接口来判断版本号 version 是否在单元测试中出错。实现一个函数来查找第一个错误的版本。你应该尽量减少对调用 API 的次数。 题目非常简单，直接套用模板#2即可。此题与模板#1的最大区别就在于，right不能修改为mid-1，而必须修改为mid。因为当如果mid是错误产品，无法判断第一个错误版本在mid之前，还是就是当前mid。这是模板#2与模板#1的最大不同。 12345678910111213class Solution &#123;public: int firstBadVersion(int n) &#123; long left = 0; long right = n; while(left&lt;right)&#123; long mid = (left+right)/2; //计算中点 if(isBadVersion(mid)==false) left = mid+1; //如果mid是正常产品，证明第一个错误产品在右侧 if(isBadVersion(mid)==true) right = mid; //如果mid是错误产品，证明第一个错误产品在是自己或者在左侧 &#125; return left; &#125;&#125;; 题目非常简单，直接套用模板#2即可。此题与模板#1的最大区别就在于，right不能修改为mid-1，而必须修改为mid。因为当如果mid是错误产品，无法判断第一个错误版本在mid之前，还是就是当前mid。这是模板#2与模板#1的最大不同。 模板3模板 3 是二分查找的另一种独特形式。它用于搜索需要访问当前索引及其在数组中的直接左右邻居索引的元素或条件。搜索条件需要访问元素的直接左右邻居。使用元素的邻居来确定它是向右还是向左。保证查找空间在每个步骤中至少有 3 个元素。需要进行后处理。 当剩下 2 个元素时，循环 / 递归结束。 需要评估其余元素是否符合条件。 对应的leetcode 例题LeetCode34 在排序数组中查找元素的第一个和最后一个位置LeetCode658 找到 K 个最接近的元素 语法关键： 初始条件：left = 0, right = length-1终止：left + 1 == right向左查找：right = mid向右查找：left = mid 12345678910111213141516171819202122int binarySearch(vector&lt;int&gt;&amp; nums, int target)&#123; if (nums.size() == 0) return -1; int left = 0, right = nums.size() - 1; while (left + 1 &lt; right)&#123; // Prevent (left + right) overflow int mid = left + (right - left) / 2; if (nums[mid] == target) &#123; return mid; &#125; else if (nums[mid] &lt; target) &#123; left = mid; &#125; else &#123; right = mid; &#125; &#125; // Post-processing: // End Condition: left + 1 == right if(nums[left] == target) return left; if(nums[right] == target) return right; return -1;&#125; 来个模板, 所谓的访问文件保留几个元素，就是包括与否mid 的意思。如果 left 和 right 都可以为 mid，那么就是三个访问空间；如果只有 left or right 可以= mid，那么就是两个访问空间；如果 left =mid -1 且 right =mid +1 那么只有一个访问空间。同样三个条件依次为 left &lt;= right, left &lt; right 和 left +1 &lt; right 保证访问空间是可以 access 的。 总结如下： 参考文献： 数据结构也不难：二分查找模版与例题 python 对于 binary search 的支持包函数 bisect 1234567891011121314151617import bisect L = [1,3,3,6,8,12,15]x = 3 x_insert_point = bisect.bisect_left(L,x) #在L中查找x，x存在时返回x左侧的位置，x不存在返回应该插入的位置..这是3存在于列表中，返回左侧位置１print x_insert_point x_insert_point = bisect.bisect_right(L,x) #在L中查找x，x存在时返回x右侧的位置，x不存在返回应该插入的位置..这是3存在于列表中，返回右侧位置３ print x_insert_point x_insort_left = bisect.insort_left(L,x) #将x插入到列表L中，x存在时插入在左侧print L x_insort_rigth = bisect.insort_right(L,x) #将x插入到列表L中，x存在时插入在右侧 防止越界的tips：change “mid = (low + high) / 2 “ -&gt; “mid = low + (high - low) / 2 “或者使用 ”mid = (low + high) &gt;&gt; 1“，]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>binary-search</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Naive Bayes Classifier]]></title>
    <url>%2F2019%2F07%2F22%2Fnaive-bayes%2F</url>
    <content type="text"><![CDATA[从贝叶斯定理到 naive bayes 分类器，最后说明一下其应用和优缺点。 贝叶斯定理贝叶斯学派的思想可以概括为先验概率+数据=后验概率。也就是说我们在实际问题中需要得到的后验概率，可以通过先验概率和数据一起综合得到。 我们经常遇到的情况是 已知 $P(A |B)$ 然后求解的是 $P(B |A)$。这里先说一下条件概率。 条件概率： $$P ( A | B ) = \frac { P ( A B ) } { P ( B ) }$$ 事件的独立性：当两个事件 A，B 满足 $P ( A B ) = P ( A ) P ( B )$, 则称A， B 相互独立。 因此bayes 定理打通了上述两个条件概率的关系。$$P ( B | A ) = \frac { P ( A | B ) P ( B ) } { P ( A ) }$$ 朴素贝叶斯分类的原理和流程总体的公式：$$ p(类别 | 特征) = \frac {p(类别) * p特征|类别）}{p(特征)}$$ 设特征 $x = { a _ { 1 } , a _ { 2 } , \ldots , a _ { m } }$， 其中 x 是一条数据， $a_i$ 是一个特征属性。 有类别信息 $C = { y _ { 1 } , y _ { 2 } , \ldots , y _ { n } }$ 计算 $P ( y _ { 1 } | x ) , P ( y _ { 2 } | x ) , \ldots , P ( y _ { n } | x )$ 如果 $P ( y _ { k } | x ) = \max { P ( y _ { 1 } | x ) , P ( y _ { 2 } | x ) , \ldots , P ( y _ { n } | x ) }$， 那么 $x \in y _ { k }$. 所以现在的关键步骤是如何计算第 3 步骤中的各个条件概率。可以这样做， 找到一个已知分类的待分类项集合，这个集合叫做训练样本集。 统计得到在各类别下各个特征属性的条件概率估计 $P \left( a _ { 1 } | y _ { 1 } \right) , P \left( a _ { 2 } | y _ { 1 } \right) , \ldots , P \left( a _ { m } | y _ { 1 } \right) ;P \left( a _ { 1 } | y _ { 2 } \right) , P \left( a _ { 2 } | y _ { 2 } \right) , \ldots , P \left( a _ { m } | y _ { 2 } \right) ;\ldots ; P \left( a _ { 1 } | y _ { n } \right) , P \left( a _ { 2 } | y _ { n } \right) , \ldots , P \left( a _ { m } | y _ { n } \right)$ 如果各个特征属性是条件独立的，则根据贝叶斯定理有如下推导 $$P \left( y _ { i } | x \right) = \frac { P \left( x | y _ { i } \right) P \left( y _ { i } \right) } { P ( x ) }$$ 因为分母对于所有类别为常数，因为我们只要将分子最大化皆可。又因为各特征属性是条件独立的，所以有：$$P \left( x | y _ { i } \right) P \left( y _ { i } \right) = P \left( a _ { 1 } | y _ { i } \right) P \left( a _ { 2 } | y _ { i } \right) \ldots P \left( a _ { m } | y _ { i } \right) P \left( y _ { i } \right) = P \left( y _ { i } \right) \prod _ { j = 1 } ^ { m } P \left( a _ { j } | y _ { i } \right)$$ 应用很多实际应用中，贝叶斯理论很好用，比如垃圾邮件分类，文本分类 优缺点优点： 算法比较简单，常用于文本分类 同时具备接收大数据量训练和查询时具备高速度的特点 具有支持增量式训练的能力（不借助于旧有训练数据，每一组新的训练数据都有可能引起概率值的变化，而如决策树和支持向量机，则需要我们一次性将整个数据集都传给它们。）对于一个如垃圾邮件过滤这样的应用程序而言，支持增量式训练的能力是非常重要的，因为过滤程序时常要对新到的邮件进行训练，然后必须即可进行相应的调整；更何况，过滤程序也未必有权限访问已经收到的所有邮件信息。 缺点： 对缺省数据不太敏感 无法处理基于特征值组合所产生的变化结果。例如：“在线”和“药店”分开出现时一般出现在正常邮件中，但当组合起来时“在线药店”却一般出现在垃圾邮件中，贝叶斯分类器无法理解这种特征组合。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>naive bayes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[overfit]]></title>
    <url>%2F2019%2F07%2F21%2Foverfit%2F</url>
    <content type="text"><![CDATA[分析机器学习 和深度学习中出现的过拟合现象，从不同的角度简述常用的处理手段。 什么是过拟合?训练数据集规模小是导致过拟合的原因，而网络足够的复杂（有能力）记住了所有的样本，然后在train sets 表现要远远好于 test sets。 还有一种说法是网络拟合了噪声数据。 如何处理过拟合处理该问题可以从数据和模型两个方面去考虑。 模型角度简化模型，通过不断降低模型的复杂度（比如随机森林中的估计量，神经网络中的参数），最终达到一个平衡状态：模型足够简单到不产生过拟合，又足够复杂到能从数据中学习。这样操作时一个比较方便的方法是根据模型的复杂程度查看模型在所有数据集上的误差。如 图 1 所示。 图 1 简化模型的另一个好处在于训练速度更加快. 数据角度 获取更多的数据 数据增强 获取更多的数据侧重获取得到的原始的训练数据集；而数据增强在图像处理中更加常见，主要是图像的变形，噪声方面进行考虑。 训练过程角度提前终止 (early stop) 如图 1 ，当 test error 增加的时候，那么模型就应该停止了。 正则化角度神经网络中有主要有两类实体：神经元和连接神经元的边。所以按照规范化的操作对象的不同可以分成两大类，一类是对于L 层的神经元的激活值或者说对于第 L+1 层网络神经元的输入值进行normalization 操作，比如说 batch normalization / Layer normalization 等方法都是属于这类；另一种是对于神经元之间相连的边上的权重进行规范化操作，比如说 weights normalization就属于这类。 广义上讲，一般机器学习中的损失函数中加入的 L1/ L2 等正则项 属于第二类。L1 正则项造成参数的稀疏性，使得大量的参数取得 0值， L2 正则项使得原始参数值有效的减小。通过这些规范化的手段改变参数值，已达到避免模型过拟合的目的。 （最初对于输入data 的normalization， 是属于神经元的 normalization） 虽然上述方法分别对神经元和weights 进行了规范化，但本质上都实现了对数据的规范化，只是 scale 的参数的来源是不同的。 使用L1 or L2 在 loss function (error function) 中添加正则项，对损失函数中的weights 进行限制其变大。 对于神经元的激活值来说，不管哪种方式，其目标都是一样的，将激活值规范到均值为0，方差为1 的正太分布。 BN 定义：BN针对一个minibatch的输入样本，计算均值和方差，基于计算的均值和方差来对某一层神经网络的输入X中每一个case进行归一化操作。 BN 的优点： 是一种正则化手段，加上BN 之后，学习率可以有很大的提高，可以加快模型的收敛速度 一般来说在激活函数之前比较好解释一些，效果好一些；输入激活函数之前进行了数据的归一化，防止进去到激活函数的饱和区。 所以可以得到BN 的适用场景：每个mini-batch 都比较大，数据分布比较接近。在训练之前，最好是做好了 充分的shuffle，否则效果可能不太好。 BN 的不足： 高度依赖 mini-batch 的大小，当batch size 比较小的时候，效果不好。因为数据样本少，得不到有效的统计量，也可以说噪声比较大。当然是可以通过调整 batch size 的大小规避这种问题，但是有的任务要求 batch size 不能太大；并且BN 是无法应用到 online learning 中的，因为online 都是单实例更新模型，很难组织起 mini-batch 的结构。 对于相似级别的图像生成任务，BN 效果不佳对于图片分类等任务，只要能够找出关键特征，就能正确分类，这算是一种粗粒度的任务，在这种情形下通常 BN 是有积极效果的。但是对于有些输入输出都是图片的像素级别图片生成任务，比如图片风格转换等应用场景，使用 BN 会带来负面效果，这很可能是因为在 Mini-Batch 内多张无关的图片之间计算统计量，弱化了单张图片本身特有的一些细节信息。 因为输入的 Sequence 序列是不定长的，这源自同一个 Mini-Batch 中的训练实例有长有短。对于类似 RNN 这种动态网络结构，BN 使用起来不方便 训练时和推理时统计量不一致对于 BN 来说，采用 Mini-Batch 内实例来计算统计量，这在训练时没有问题，但是在模型训练好之后，在线推理的时候会有麻烦。因为在线推理或预测的时候，是单实例的，不存在 Mini-Batch。虽说实际使用并没大问题，但是确实存在训练和推理时刻统计量计算方法不一致的问题。 Layer normalization 也是一种正则化手段 BN vs LN ： 从图中看可以知道 batch是“竖”着来的，各个维度做归一化，所以与batch size有关系。 layer是“横”着来的，对一个样本，不同的神经元neuron间做归一化。显示了同一层的神经元的情况。假设这个mini-batch一共有N个样本，则Batch Normalization是对每一个维度进行归一。而Layer Normalization对于单个的样本就可以处理。 相同点： BN 和LN 都是可以很好的一直梯度消失和梯度爆炸的。 实践证明， LN 更加适合 RNN ，BN 更加适合CNN 至于各种 Normalization 的适用场景，可以简洁归纳如下：对于 RNN 的神经网络结构来说，目前只有 LayerNorm 是相对有效的；如果是 GAN 等图片生成或图片内容改写类型的任务，可以优先尝试 InstanceNorm；如果使用场景约束 BatchSize 必须设置很小，无疑此时考虑使用 GroupNorm；而其它任务情形应该优先考虑使用 BatchNorm。 参考文献：深度学习中的Normalization模型 深度学习中的模型Dropout 或者Dropconnect. 其理念就是在训练中随机让神经元无效（即dropout）或让网络中的连接无效（即dropconnect）。这个有点类似集成学习，提高网络模型的泛化性能，减少过拟合的问题。（类似bagging 的思想，使用不同的网络结构在不同的训练集上进行训练）.如果从集成学习角度理解dropout，那么resnet 网络是不是也有点集成学习的味道。 Dropout 的具体流程 首先随机（临时）删掉网络中一半的隐藏神经元，输入输出神经元保持不变（图3中虚线为部分临时被删除的神经元） 然后把输入x通过修改后的网络前向传播，然后把得到的损失结果通过修改的网络反向传播。一小批训练样本执行完这个过程后，在没有被删除的神经元上按照随机梯度下降法更新对应的参数（w，b）。 然后继续重复这一过程 恢复被删掉的神经元（此时被删除的神经元保持原样，而没有被删除的神经元已经有所更新） 从隐藏层神经元中随机选择一个一半大小的子集临时删除掉（备份被删除神经元的参数）。 对一小批训练样本，先前向传播然后反向传播损失并根据随机梯度下降法更新参数（w，b） （没有被删除的那一部分参数得到更新，删除的神经元参数保持被删除前的结果）。 Dropout vs Dropconnect： 它们的区别大体在于，在训练过程中dropout是随机drop掉一些节点，而dropconnect则是随机drop掉一些边。 使用多种模型Bagging： 最典型的就是 随机森林( Random Forest)， 通过不相关的决策树在不同的数据集上进行训练，最后的每个模型使用相同的权重来“融合”。Boosting: 在简单的网络上不断提升。]]></content>
      <categories>
        <category>Deep learning</category>
      </categories>
      <tags>
        <tag>overfit</tag>
        <tag>dropout</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[markdown 和latex 常见的命令]]></title>
    <url>%2F2019%2F07%2F21%2Fmarkdown-latex%2F</url>
    <content type="text"><![CDATA[主要记录总结 latex 在markdown 中的使用，防丢失。 latex 单独使用和 在markdown 中的使用还是不太一样的，所以记下常用的几个。 多行公式对齐 源码： 1234567$$\begin&#123;split&#125;a &amp;= b \\\\c &amp;= d \\\\e &amp;= f \end&#123;split&#125;\tag&#123;1.3&#125;$$ 效果如下：$$\begin{split}a &amp;= b \\c &amp;= d \\e &amp;= f\end{split}\tag{1.1}$$ 分段函数 源码： 1234$$ BP = \begin&#123;cases&#125;1 &amp; c &gt;r \\\\e ^ &#123; ( 1 - r / c ) &#125; &amp; c &lt;= r\end&#123;cases&#125;$$ 效果如下：$$ BP = \begin{cases}1 &amp; c &gt;r \\e ^ { ( 1 - r / c ) } &amp; c &lt;= r\end{cases}\tag{1.2}$$ markdown 页内跳转 源码123&lt;center&gt; &lt;span id=&apos;jump&apos;&gt; 图 1 &lt;/span&gt;&lt;/center&gt; 如[图 1](#jump) ，当 test error 增加的时候，那么模型就应该停止了。 表格 table源码：12345| Tables | Are | Cool || ------------- |:-------------:| -----:|| col 3 is | right-aligned | $1600 || col 2 is | centered | $12 || zebra stripes | are neat | $1 | 效果如下： Tables Are Cool col 3 is right-aligned $1600 col 2 is centered $12 zebra stripes are neat $1 markdown 中的图片大小的设定 源码： 12&lt;img src=&quot;https://upload.cc/i1/2019/08/09/ApFiTC.png&quot; width=&quot;50%&quot; height=&quot;50%&quot;&gt;]]></content>
      <categories>
        <category>others</category>
      </categories>
      <tags>
        <tag>markdown</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Interview Questions]]></title>
    <url>%2F2019%2F07%2F21%2Finterview-questions%2F</url>
    <content type="text"><![CDATA[主要总结一下面试问题。包括一些非技术问题比如说你喜不喜欢加班之类的、期待的薪资呀… 和一些简历上的技术类小问题问题。 非技术类问题面试中最重要的是和面试官进行互动，互相了解的过程。如果一方说的过多，那么这个面试仍然是不成功的。在互动的过程中，也是可以学习到很多的。 “你有什么问题想要问我吗？” 不要做的事情如下： 千万不要问小白问题（凡是官网上、度娘上能够获取的信息都是小白问题）。但是你基于获得的知识，进一步提出自己的观点和看法，那么这个就是一个比较好的问题。 不要问对方无法给出你答案的问题。比如不要问 HR部门的具体业务，不要问技术面试官 关于薪酬的问题。 大的前提是根据面试官的身份，围绕着应聘职位进行提问。比如说问HR 公司是否有针对技术方面的一些培训制度，团建之类的，想要了解一下公司的氛围和企业文化 “面试中的自我评价” 主要从以下的三个方面入手： 有自己的优点，比如说自己开朗，具有合作精神，那么最好有事例进行说明 和招聘岗位匹配程度 个人的缺点，当然这个缺点不是致命的，是可以进行转折的那种 “为什么选你，而不是别人” 这本来是面试官的职责，那么既然问了，其实是创新性的帅锅给了候选人，面试官带着“证明你配得上这分工作”的心态去提问，要么是懒，要么是暗示这个岗位很热门。 既然是这种情况，我们能做就是专注于我可以怎么样… 因为别人的信息你也是无法回答的。有以下几个反套路出发点 从工作内容出发如果你已经面了一面、二面，对于自己做的事情有了一个比较清晰的认识，那么你可以说… 这个职位的工作内容，正是我比较熟悉和擅长的。需要使用具体的事实案例进行证明。 真诚反问，创造互动如果不是很清楚岗位的需求，那么上面的方式就不行，这个时候需要创造互动。简单自我介绍一下，然后说“我正想和您探讨一下， 您认为做好这个工作，候选人应该具备怎样的条件？” 如果提到的是自己的优势，那么就使用事例证明一下；如果是自己的弱项，那么简单的说一下，然后说自己是如何提高和改进，展示的好学的一面，最好是有例子证明。 最后如果聊得比较来，自己答题不错的话，那么可以在结束的时候补上一句：“我冒昧问一句，您也面试了一些候选人，您觉得我的机会大吗？” “你最大的缺点是什么“ 招聘本身就是用人之长，弥补自身的缺陷。面试官究竟想要什么？ 候选人在回答这种刁钻问题时，很喜欢避重就轻：“我最大的缺点，就是太较真，对细节要求太高”“我最大的缺点是太拼了，不注意身体”额……这恰恰中了面试官的套路：“这位候选人不够真诚。” 对于候选人来说，我推荐两种更加真诚的回答套路（对，套路也可以真诚）。正确的方式- 讲一个真实存在的缺陷，但强调你已经意识到，并已经在改善了。….. （需要有事例进行说明）这种方式，既能让面试官感受到你的真诚，也能让面试官觉得你是对自己有清醒的认识同时已经开始行动，是一个比较踏实的候选人。 说到底，企业招人是招人之长，如果你的长处是企业急需的，那么你的弱点并非致命，面试官还是很希望得到你的。 ”请简单的介绍一下自己“ 当面试官问出这种问题的时候，我的第一反应就是：这个面试官没有提前阅读我的简历。我希望面试官是阅读了的，然而，我也清楚这几率很低。 所以见证套路的时候到了，简单的自我介绍，我来自… 之前去… 做过…工作，然后在… 看到你们招聘… 岗位，觉得这个是一个不错的机会，所以投了简历。我想简单问您一个问题，可以吗？您这个职位是新设置，还是之前的同事离职呢？ ”要和面试官进行互动，面试官说的越多，那么成功的可能性是越大的，不是一昧的听，而是创造和面试官之间的有机的互动” 好的面试如同老友重逢在Central Perk，互相尊重互相理解，即使做不成同事，没准还可以做朋友。我们平时和小伙伴们谈笑风生，可以友好的提问，也可以开善意的玩笑。但到了面试时，为什么要抱着死板的心态生硬尴聊呢？在这一点上我倒是很赞赏部分互联网公司的高管， 他们抱着开放的态度和候选人聊工作甚至聊人生，也愿意给候选人提供自己的建议。 “你能在公司待多久” 对这个公司发展的前景比较感兴趣，我也希望能给公司带来点什么，能够发挥作用；只要双方都觉得有收获，做的事情有挑战有意义，就都是ok的。 “你希望这个职位的薪水是多少” 这个是一个微妙的问题，在条件允许的情况下，尽可能的拖延给出一个精确的数据来回答这个问题。你可以说，我知道这个工作的薪水大概范围是.. 到…。 或者您能否透露一下公司中对相似职位的工作的薪水大概是什么样子的呢？ 如果面试官继续追问，那么你说“我现在的薪水是… 和其他人一样，我希望能够提升这个数字，但我主要的兴趣还是在工作本身” 要记住新的工作本身并不会使得你赚到更多的钱。 在你面试过程中的最后一个阶段之前，少谈薪水的问题。因为到了那个时候你就清楚，如果公司对你有很大的兴趣，那么这个时候薪水待遇就有很大的余地。 “如何看待加班？”如果特殊时期项目需要上线之类，那么偶尔加班是可以接受的，但是如果强制长期加班，那还是算了。当然，如果自己负责的事情出了问题，即使公司没有要求加班，那么自己也是会主动加班把事情弄好，尽自己最大的努力保证项目不延期。 最好的方式是凭着工作能力说话，让别人觉得能力ok 不加班也是把工作做完做好的。 目前团队工作氛围怎么样？ 加班情况如何？之类的是可以提前问一下的。如果自己急需工作，先拿下工作之后再说，等自己缓和一下再谋出路。 最后的发问？你们小组是干什么，有什么产品？实习内容主要干什么？（这两个问题是不一样的） 小抄（简历上的面试问题）腾讯新闻的实习经历 绝对提高和相对提高：假如之前的水平是a，然后之后的水平是b，那么绝对提高就是 (b -a); 相对提高 (b -a) /a *100% faiss 的介绍首先使用 index对于向量进行预处理，然后选择不同的模式. 主要讲的是三种模式，一个维度是简单模式，适合在小数据上进行计算 欧氏距离；一个维度是加快检索速度，这种模式下是需要提前的train，其基本的思路对向量进行聚类，当然文中说的是 “细胞”，建立倒排索引，然后检索的时候，搜索这个“细胞”内 和周围的“细胞” 的id 的集合，就可以返回前 K 个最相近的结果；最后一个维度是减少内存的使用，上面两种都是使用的完整的向量，这个模式下是使用的压缩向量，可以使用PCA 进行实现，当然这个模式下得到的结果也是近似解。还有两种计算的上的优化，对于向量进行分段计算，这种可以实现并行，并且支持任务在GPU 上进行运算。 什么是 AB test?在对产品进行A/B测试时，我们可以为同一个优化目标（例如优化购买转化率）制定两个方案（比如两个页面），让一部分用户使用A 方案，同时另一部分用户使用 B 方案，统计并对比不同方案的转化率、点击量、留存率等指标，以判断不同方案的优劣并进行决策，从而提升转化率。 过渡到我这个项目，就是一部分用户使用原来的版本的 topK 的模型，另外一部分用户使用我这个版本 topk 的模型。 经常使用的框架 和模型？中文 nltk (Natural Language Toolkit — NLTK 3.4.4 documentation), 中文分词 jieba hanlp stanford nlp bert英文 nltk genism bertnltk 和 gensim 自然语言工具包 实现了很多nlp 中的算法，比如tfidf 深度学习框架keras tensorflow pytorch 句子向量的训练过程？word embedding 表示的句子的缺点：没有词序；没有上下文；依赖前期的处理 SIF 这种句子向量 （无监督，非网络结构） 如何进行 sentence 训练?无监督模型，通过skip-though 举例说明：skip-gram 根据中心词汇预测上下文skip-thought 根据中心句 预测上下句encoder -decoderencoder 就是一个 特征提取，然后又两个decoder， 是语言模型 ，分别对应着 上一句和下一句统计语言模型是一个单词序列上的概率分布，对于一个给定长度为m的序列，它可以为整个序列产生一个概率 P(w_1,w_2,…,w_m) 。 词汇扩展：使用 word2vec 得到的词向量 来补充 encoder 过程中 该问题的词向量，要求前者的规模是远远大于后者的。通过这种方法补充 encoder 过程中的词汇。 该模型要求：数据集中的句子是有衔接关系的，论文中的数据集是 google news dataset。要求有一定的逻辑的文章。 有监督模型：infersent 综述类型，找到了 nlp 领域的imagenet 和 inception network。网络结构模型是 使用一个encoder 得到向量表示，然后记性 u 相加 和点乘 三种操作，得到 fully-connection 最后是softmax。encoder 是 bi-lstm 并且使用 max pooling 进行网络结构的优化。数据集是 句子对组成的数据集，标注信息是数据间的关系（蕴含，对立和中性）数据量 570k。 具体实验对比的是 RNN lstm gru 然后搭配着 单向or 双向 max pooling or average pooling 进行做的实验。 universal sentence encoder 上面那个是 facebook 团队出的，然后这个是 google 出的，主要的区别在于使用 transformer 代替bi-lstm 作为encoder。 训练的时候，既进行了有监督的学习，又进行了无监督的训练。 图像处理和nlp 的差别：图像是连续实数空间，而nlp 处理的是离散的数据。图像中的pixel 是连续的，而文本是离散的token 表示。 对于 bert transformer 的介绍？ 如果想要讲解 bert，那么就得讲解 transformer，如果想要讲解 transformer，那么就得讲解 self-attention，如果想要讲解 self-attention，那么就得说下 attention机制。 直接从 transformer 说起，大的结构还是 encoder、decoder 的结构。encoder 是6 个两层结构， decoder 也是 6个三层结构。 bert 中有几个技术点是需要讲解： layer normalization （主要 vs batch normalization）、position embedding （在encoder 中是结合了 word embedding 一块作为输入的）、masked attention（当predict t 时刻的word的时候，只能看到 t 之前的情况，将 t 之后的word 进行了 mask）、multi-head attention（类似 图像中的多个 filter ）、self-attention 就不多说了 从 transformer 到 bert 又是一个比较复杂的模型组合的工作。 文章去重 参加的一个课题项目，针对某个特定的行业，比如房地产、互联网，基于大数据，对网页或者文章进行分析，使用数据挖掘的手段，捕捉热点，然后为企业或者个人提供事件或者品牌的情感分析、品牌分析。这个是大的项目背景。 我参与的是在于对网页或者网页的去重工作，因为之后的文章分析的指标计算，比如流行度，都是比较依赖于数据的准确性。数据的来源：新闻（搜狐网、新浪网）电子报纸，app 新闻，分成不同类别的新闻，比如科技、教育、金融和体育。每条数据由标题、站点、url、发布时间、内容、阅读量、评论量等组成，数据量是百万级别的。其中内容部分是由500 到2000不等的字数组成。步骤的话，先是文本的预处理，提取关键词，然后使用simhash 或者minhash 进行计算，重复文章的集合，最后去掉重复的文章。 优化：多线程，字典（有一些品牌类的，人名类，需要进行人工的添加、新词发现，网络词汇） 如何去考察最后去重的效果，一般来说，simhash 中的距离函数是汉明距离，这个在3 以内，都是可以认为是相似的文章。当然对于定量的评价的话，有一个标注的小的测试集，然后可以在上面进行 baseline的验证。 Home credit default risk 尽可能的从业务本身，提取更多可解释、稳定的特征。比如在 6张子表中，是用户的行为数据，如果对于最后的违约概率来说，时间就显得比较重要，用户的最近几个月的消费水平和长期的消费水平分别对应的是近期行为和长期行为，这个对于 违约概率的影响也是不同的。所以需要尽可能地懂数据，然后从这些维度提取可解释的特征。当然最原始的数据中有一些未标明的数据，进行一下交叉和组合也是统计学中经常使用到的手段。 调参也是技术活，常常使用Grid search 和Random search 这个也是可以展开的：domain： 所有的超参数及其值 的dictionary（键值对）Optimization algorithm: 如何去选择下一组 超参数。目标函数：这个是模型中的目标函数results history： 目标函数和 一组超参数的对应关系 关于 results history 的使用就是 random search 和基于贝叶斯方式搜索的区别，前者没有使用这种对应的关系，后者有利用这种关系。 贝叶斯，p(目标函数 | 一组超参数)。 pygen 项目 faker 中生成的信息是单列的，个人信息之间是没有联系的，所以想要用在机器学习训练的时候比较难。关键技术：中文名字有很强的性别属性。例如名字中带有“杰”“志”“宏”等字的一般为男性，带有“琬”“佩”“梅”等字的一般为女性。当然也有一些比较中性的字，例如“文”“安”“清”等，比较难猜测性别，关于这点会在另一个博客中展开，请期待。 gan 论文 D 网络在判别的时候，是从逼真程度上进行判别的，并不是从生成图像多样性上进行判别的，并且是无法从多样性上进行判别的，因为输入到D 网络中的是一个样本或者说是一个 batch的样本，是很难得到生成图像的多样性这个角度的。 同理 G 网络也只是考虑单个的生成图像，对于G 网络图像的多样性是没有考虑在内的。 有时候也会在训练过程中也会利用这种性质，比如说只是为了得到一张或者几张非常逼真好看的图像，而不会很在乎最后是否生成了多样性的图像。 孪生网络最初是用来做指纹的验证，然后扩展到 cv 和nlp 中都有用来做相似度方面的计算。在论文中的是用来作为生成图像之间相似度的计算，该网络中两个 CNN 用于提取特征，然后基于 欧式距离进行计算，输出label 是0-1 之间的数字， 0表示最相近，1表示不想近。损失函数使用的是 对比损失函数，有点类似交叉熵的感觉，分段函数， y =0 的时候是平方损失函数，y =1 的时候，是合页损失函数。 主要的是做了以下的优化：label 浮点化，Label Smoothing :这种距离的计算是通过聚类实现的，在同一个簇中或者相邻簇中的距离小，向着0 靠近；在不同的簇之间的相距大，不超过1. 网络结构上有两点优化：对于对抗生成网络的D 网路的权重进行正则化，具体来说是 谱归一化，使其符合lipschitz 约束，其中超参数 K 取1.即 D(x) -D(y) 的欧式距离是不大于 x-y 的欧式距离的。 加入了 self -attention机制。 大疆笔试题图像领域中的数据增强? 一种是收集更多的数据，一种是根据现有的数据进行增强。数据增强，常用的方式，就是旋转图像，剪切图像，改变图像色差,扭曲图像特征，改变图像尺寸大小，增强图像噪音（一般使用高斯噪音，盐椒噪音）等. 逻辑回归的目标函数：log 似然函数$ p(y)$， 先验概率：一个事件发生的概率$ p(y|x) $，后验概率：一个事件在另一个事件发生条件下的条件概率 计算三个稠密矩阵 A、B、C 的乘积 ABC，假定三个矩阵的尺寸分别为 $m \times n $, $n \times p $, $p \times q$，且 $m&lt;n&lt;p&lt;q $,以下计算效率最高的是解释：$a \times b$ 矩阵和$ b \times c $矩阵相乘，每次行乘列运算需要 b 次乘法和 b-1 次加法，即总共需要 $(2b-1) \times a \times c $ 次运算，时间复杂度为$ O(a \times b \times c) $1.AB 的时间复杂度为 $O(m \times n \times p)$ ，此时形成 $m \times p$ 的新矩阵2.(AB)C 的时间复杂度为 $O(m \times p \times q) $ 结论：首先使较小的矩阵相乘，最后乘较大的矩阵可以减少运算量。 下列哪些项所描述的相关技术是对的？A AdaGrad和L-BFGS使用的都是一阶差分B AdaGrad和L-BFGS使用的都是二阶差分C Adagrad使用的是一阶差分，L-BFGS使用的是二阶差分D Adagrad使用的是二阶差分，L-BFGS使用的是一阶差分答案： C 牛顿法不仅使用了一阶导信息，同时还利用了二阶导来更新参数,L-BFGS算法是一种在牛顿法基础上提出的一种求解函数根的算法 对于一个分类任务，如果开始时神经网络的权重不是随机赋值的，二是都设成0，下面哪个叙述是正确的？（C）C 神经网络可以训练，但是所有的神经元最后都会变成识别同样的东西令所有权重都初始化为0这个一个听起来还蛮合理的想法也许是一个我们假设中最好的一个假设了, 但结果是错误的，因为如果神经网络计算出来的输出值都一个样，那么反向传播算法计算出来的梯度值一样，并且参数更新值也一样( $w=w−α∗dw $)。更一般地说，如果权重初始化为同一个值，网络即是对称的, 最终所有的神经元最后都会变成识别同样的东西。 pca 之前是需要进行正则化，然后求解 bagging（random forest 是其的一种特例，防止过拟合的手段）bagging 中的bag 是袋子的意思，有点通过 random 然后得到一堆东西，这个是可以防止过拟合的stacking (stack 是堆 的意思)，这里以论文审稿为例，首先是三个审稿人分别对论文进行审稿，然后分别返回审稿意见给总编辑，总编辑会结合审稿人的意见给出最终的判断，即是否录用。对应于stacking，这里的三个审稿人就是第一层的模型，其输出（审稿人意见）会作为第二层模型（总编辑）的输入，然后第二层模型会给出最终的结果。boosting 算法（gbdt、adaboost 和 xgboost） BN 算法：在将所有的输入传递到下一层之前对其进行归一化（更改） 以下()属于线性分类器最佳准则？机器学习 ML模型 易A.感知准则函数B.贝叶斯分类C.支持向量机D.Fisher准则正确答案：ACD线性分类器有三大类：感知器准则函数、SVM、Fisher准则，而贝叶斯分类器不是线性分类器。 感知准则函数 ：准则函数以使错分类样本到分界面距离之和最小为原则。其优点是通过错分类样本提供的信息对分类器函数进行修正，这种准则是人工神经元网络多层感知器的基础。 支持向量机 ：基本思想是在两类线性可分条件下，所设计的分类器界面使两类之间的间隔为最大，它的基本出发点是使期望泛化风险尽可能小。（使用核函数可解决非线性问题） Fisher 准则 ：更广泛的称呼是线性判别分析（LDA），将所有样本投影到一条远点出发的直线，使得同类样本距离尽可能小，不同类样本距离尽可能大，具体为最大化“广义瑞利商”。根据两类样本一般类内密集，类间分离的特点，寻找线性分类器最佳的法线向量方向，使两类样本在该方向上的投影满足类内尽可能密集，类间尽可能分开。这种度量通过类内离散矩阵 Sw 和类间离散矩阵 Sb 实现。 以下说法中正确的是() 机器学习 ML模型 中A.SVM对噪声(如来自其他分布的噪声样本)鲁棒B.在AdaBoost算法中,所有被分错的样本的权重更新比例相同C.Boosting和Bagging都是组合多个分类器投票的方法,二都是根据单个分类器的正确率决定其权重D.给定n个数据点,如果其中一半用于训练,一般用于测试,则训练误差和测试误差之间的差别会随着n的增加而减少解释：A. SVM解决的是结构风险最小, 经验风险处理较弱, 所以对数据噪声敏感. (基于树的结构gbdt 对于噪声是具有鲁棒性的，SVM 在没有噪声的数据下效果更好) B. AdaBoost算法中, 每个迭代训练一个学习器并按其误分类率得到该学习器的权重alpha, 这个学习器的权重算出两个更新比例去修正全部样本的权重: 正样本是exp(-alpha), 负样本是exp(alpha). 所以所有被分错的样本的权重更新比例相同. C. bagging的学习器之间无权重不同, 简单取投票结果; Boosting的adaboost根据误分类率决定权重, boosting的gbdt则是固定小权重(也称学习率), 用逼近伪残差函数本身代替权重. D: 根据中心极限定律, 随着n的增加, 训练误差和测试误差之间的差别必然减少 – 这就是大数据训练的由来 关于支持向量机SVM,下列说法错误的是（）A.L2正则项，作用是最大化分类间隔，使得分类器拥有更强的泛化能力B.Hinge 损失函数，作用是最小化经验分类错误C.分类间隔为1/||w||，||w||代表向量的模D.当参数C越小时，分类间隔越大，分类错误越多，趋于欠学习 C错误。间隔应该是2/||w||才对，后半句应该没错，向量的模通常指的就是其二范数。（根据两侧的距离） SVM 分类和深度学习逻辑回归分类的区别逻辑回归是线性分类，SVM 可以应用于线性分类和非线性分类问题，取决于核函数的选取。 SVM 是凸问题，深度学习都是非凸问题正确，深度学习算法的目标函数，几乎全都是非凸的。]]></content>
      <categories>
        <category>others</category>
      </categories>
      <tags>
        <tag>interview</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Recursive & Recursion]]></title>
    <url>%2F2019%2F07%2F17%2Frecursive-recursion%2F</url>
    <content type="text"><![CDATA[递归实现指数型枚举 从 1~n 这 n 个整数中随机选取任意多个，输出所有可能的选择方案。 原题连接 递推： 符合人的思路，比如由前两项计算第三项，求解斐波那契数列，递归：从后往前算，把一个大的问题分解成小的问题，然后计算小的问题，最后把所有的结果整合起来。从递归转成非递归，这个有时候比较麻烦，但是有时候必须这么做，因为可能递归的形式可能造成栈溢出。 dfs 就是类似一种树的结构，每次分支都类似两种选择，最后的叶子节点就是方案从第0个数字开始枚举， state 表示状态位置，使用二进制表示，如果是二进制是1 那么表示当前的数字是存在的如果是0 则表示不存在的 1234567891011121314151617181920212223242526272829// 因为数据量比较小 2^15 就算是三万吧, 所以直接dfs()#include&lt;iostream&gt;using namespace std;int n; // 定义全局变量是不用在递归中传下去了void dfs(int u, int state )&#123; // 边界条件 if (u ==n) &#123; for(int i =0; i&lt;n ;i++) if( state &gt;&gt; i &amp;1) cout &lt;&lt; i+1 &lt;&lt;" "; cout &lt;&lt; endl; return ; &#125; dfs(u+1, state); dfs(u+1, state | 1 &lt;&lt;u);&#125;int main()&#123; cin &gt;&gt;n; // 枚举的当前的数字 和状态表示（这个状态是可以使用 数组来表示，但是这里二进制来表示） dfs(0 , 0); return 0;&#125; 递归实现组合型枚举 从 1~n 这 n 个整数中随机选出 m 个，输出所有可能的选择方案。 输出格式 按照从小到大的顺序输出。 第一个参数表示当前枚举到了哪个数字，第二个参数表示选了多少个数字，第三个参数表示选择了哪些数字（二进制数，是0 表示没有选，是 1表示选择了）。 1234567891011121314151617181920212223242526272829#include&lt;iostream&gt;using namespace std;int n, m;void dfs(int v, int sum, int state)&#123; //边界条件 if (sum + n-v &lt; m) return ; // 一种方案数 if (sum ==m) &#123; for (int i =0; i&lt;n ;i++) //这个是遍历总数个的，因为其中有的是选择有的是不选择的 if (state &gt;&gt;i &amp;1 ) cout &lt;&lt; i+1 &lt;&lt;" "; cout &lt;&lt;endl; return ; &#125; // 从小到大排序，所以尽可能的选择，这样前面的是比较小的 dfs(v +1, sum +1, state | 1 &lt;&lt;v); dfs(v +1, sum , state); &#125;int main()&#123; cin &gt;&gt; n&gt;&gt; m; dfs(0, 0, 0); // 初始化 return 0;&#125; 排列与组合的共同点是从 n 个不同的元素中，任取 m 个元素，不同点在于排列是按照一定的顺序排成一列，组合是无论怎样的顺序并成一组。因此，“有序” 和“无序” 是区分排列和组合的重要标志。 递归实现排列型枚举 把 1~n 这 n个整数排成一行后随机打乱顺序，输出所有可能的次序。 这个是有序的，所以不能只是一个 二进制数进行判断，应该使用数组进行保存结果。 123456789101112131415161718192021222324252627282930313233343536// 因为是有顺序的，所以使用path 进行保存结果，// 还是使用一个 二进制数字 表示是否选择过#include&lt;iostream&gt;#include&lt;vector&gt;using namespace std;int n;vector&lt;int&gt; path;void dfs(int u, int state)&#123; //跳出条件 if (u ==n) &#123; for(auto x : path) cout &lt;&lt; x&lt;&lt; " "; cout &lt;&lt;endl; return; &#125; for(int i =0 ; i&lt;n ;i++) &#123; if ( !( state &gt;&gt; i &amp;1)) // 如果没有遍历过，那么就访问 &#123; path.push_back(i+1); // 加入到path 中 dfs(u+1, state | 1 &lt;&lt;i); path.pop_back(); // 返回到现场 &#125; &#125;&#125;int main()&#123; cin &gt;&gt;n; dfs(0, 0); //从 0开始枚举，使用二进制数字进行标记是否出现过 return 0;&#125; 费解的开关 原题这个题目有点难了 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980#include&lt;iostream&gt;#include&lt;cstring&gt;using namespace std;const int INF = 1000000;char g[10][10];// 上一行的某一个位置为0 那么下一行的对应的位置就要置为1，否则上一行就不能变了。// 所以基本的思路就是固定前4行，判断第5 行如果全部是1 那么就是能够变成1，否则该方案就不成立。// 对于turn 的小的技巧// 中间， 上 右边 下面，左边// 中间 上面 右边 下面 左边// 这个只要保证当前的不变，int dx[5] =&#123; 0, 1, 0, -1, 0&#125;, dy[5] =&#123;0, 0, -1, 0, 1&#125;;void turn(int x, int y)&#123; for(int i =0; i &lt;5; i++) &#123; int a =x +dx[i], b = y+ dy[i]; if( a&gt;= 0 &amp;&amp; a&lt; 5 &amp;&amp; b&gt;=0 &amp;&amp; b&lt;5) g[a][b] ^=1; //对于01 相互转换，这种操作是可行的 &#125;&#125;int work()&#123; int ans =INF; for (int k =0; k&lt; 1&lt;&lt;5; k++) &#123; int res =0; char backup[10][10]; memcpy(backup, g, sizeof(g)); // 备份 g 到 backup for(int j =0; j&lt; 5 ; j++) if (k &gt;&gt; j &amp;1) &#123; res ++; turn(0, j); &#125; for(int i =0; i&lt;4; i++) &#123; for(int j =0; j&lt;5 ;j++) if(g[i][j] =='0') &#123; res ++; turn(i+1, j); &#125; &#125; bool is_successful =true; for(int j =0; j&lt;5 ;j++) if( g[4][j]== '0') &#123; is_successful =false; break; &#125; if(is_successful) ans =min(ans, res); memcpy(g, backup, sizeof(g)); &#125; if (ans &gt;6) ans =-1; return ans; &#125;int main()&#123; int T; cin &gt;&gt; T; while( T --) &#123; for(int i =0; i &lt;5; i++) cin &gt;&gt; g[i]; // 类似行向量的感觉,g 是一个二维的向量 cout &lt;&lt; work() &lt;&lt;endl; &#125; return 0;&#125; 代码是错误的，但是上面的注释是正确的。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889#include&lt;iostream&gt;#include&lt;cstring&gt;using namespace std;const int INF =100000;char g[10][10];// 中间， 上 右边 下面，左边// 中间 上面 右边 下面 左边int dx[5] =&#123;0, 1, 0, -1, 0&#125;, dy[5] =&#123;0, 0, -1, 0, 1&#125;;void turn(int x, int y)&#123; for(int i =0; i&lt;5 ;i++) &#123; int a =x +dx[i], b =y+dy[i]; if(a &gt;=0 &amp;&amp; a&lt;5 &amp;&amp; b&gt;=0 &amp;&amp; b&lt;5 ) g[a][b] ^=1; &#125; &#125;int work()&#123; int ans =INF; for(int k =0; k&lt; 1&lt;&lt;5; k++) &#123; int res =0; // 当前方案的操作数 char backup[10][10]; // 如果你backup 一定是在某个时间点是用于恢复的 memcpy(backup, g, sizeof(g)); // 先把第一行枚举操作完，然后固定第一行 for(int j =0; j&lt;5; j++) &#123; res ++; if(k &gt;&gt;j &amp;1) turn(0,j); // 为什么是1的时候，需要turn 一下，不是0吗？ &#125; // 固定了第一行，然后遍历前4 行 for(int i =0; i&lt;4; i++) &#123; for(int j =0; j&lt;5; j++) if( g[i][j] =='0') &#123; res ++; turn(i +1, j); &#125; &#125; bool is_successful =true; for(int j =0; j&lt;5; j++) if( g[4][j] =='0') &#123; is_successful =false; break; &#125; if( is_successful) ans =min(ans, res); memcpy(g, backup, sizeof(g)); &#125; if (ans&gt;6) ans =-1; return ans; &#125;int main()&#123; int T; cin &gt;&gt; T; // 是每组每组进行判断的额 while( T--) &#123; for (int i =0; i&lt;5; i++) cin&gt;&gt; g[i]; cout &lt;&lt; work() &lt;&lt;endl; &#125; return 0;&#125;]]></content>
      <categories>
        <category>算法</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Knapsack]]></title>
    <url>%2F2019%2F07%2F15%2Fknapsack%2F</url>
    <content type="text"><![CDATA[01背包 有$ N $件物品和一个容量是 $V $的背包。每件物品只能使用一次。第 $i$ 件物品的体积是 $v_i$，价值是 $w_i$。求解将哪些物品装入背包，可使这些物品的总体积不超过背包容量，且总价值最大。输出最大价值。 Tips: 这个是最基础的背包问题。 背包问题属于动态规划的一种， $f[i] $表示体积为 $i $背包的最大的价值。那么转移方程就比较简单了，对于物品(v, w) 有选和不选两种方案。$ f[i] =max( f[i], f[i-v] +w) $ 这样的式子就是比较nice的选择。 01 背包问题的体积是从大到小进行枚举的。 12345678910111213141516171819202122232425262728// 01 背包问题， f[i] =max(f[i], f[i -w]+ v)#include&lt;iostream&gt;#include&lt;cstring&gt;#include&lt;algorithm&gt;using namespace std;const int N= 1010;int f[N];int v[N], w[N];int n, m;int main()&#123; cin&gt;&gt; n&gt;&gt; m; for(int i =0; i&lt; n; i++) &#123; int v, w; cin &gt;&gt; v&gt;&gt; w; for (int j =m ; j&gt;=v; j--) f[j] =max(f[j], f[j -v]+w ); &#125; cout &lt;&lt; f[m]&lt;&lt;endl; return 0;&#125; 完全背包问题 有$ N $件物品和一个容量是 $V $的背包。每件物品都有无限件可用。第 $i$ 件物品的体积是 $v_i$，价值是 $w_i$。求解将哪些物品装入背包，可使这些物品的总体积不超过背包容量，且总价值最大。输出最大价值。 Tips： 这个跟上面的不同点在于，物品是无限件可用，01 背包是只能用一个。从代码实现上看， 01背包问题的 体积是从大到小遍历的，然后完全背包问题是从小到大遍历的。 01 背包问题和 完全背包问题的区别在于前者物品只有选择与否两种方案。而完全背包对于每件物品有无限件可以选择。对于体积v 是从小到大进行枚举的。 1234567891011121314151617181920212223242526272829// 转移方程 f[i] =max(f[i], f[i -v]+w) 没有变，实现的时候有差别#include&lt;iostream&gt;#include&lt;cstring&gt;#include&lt;algorithm&gt;using namespace std;const int N =1010;int f[N];int n, m;int main()&#123; cin &gt;&gt; n&gt;&gt;m; for(int i =0; i&lt; n ;i++) &#123; int v, w; cin &gt;&gt; v&gt;&gt;w; for(int j =v; j&lt;=m ;j++) // 注意这个是 &lt;= 号; 这个起始点是假设放入了体积为v 的物品 f[j] =max(f[j], f[j -v]+w); &#125; cout&lt;&lt; f[m]&lt;&lt;endl; return 0;&#125; 多重背包问题 1 有$ N $件物品和一个容量是 $V $的背包。每件物品都有无限件可用。第 $i$ 件物品最多有 $s_i$ 件, 每件体积是 $v_i$，价值是 $w_i$。求解将哪些物品装入背包，可使这些物品的总体积不超过背包容量，且总价值最大。输出最大价值。 $$\begin{split}0&lt; &amp; N \leq 100 \\0&lt; &amp; V \leq 100 \\0&lt; &amp; v_{i}, w_{i}, s_{i} \leq 100\end{split}$$ Tips： 多重背包问题是转换成 01 背包问题进行解决的，这个大的思路是没有问题的。只是对于不同的数据规模有两种不同的实现手段。当数据规模比较小的时候，三重循环就搞定了；当数据规模比较大的时候，需要使用一个结构体进行操作。 （多重背包问题对物品的个数进行了限制，不是无限个，转换成01 背包问题，体积是从大到小进行枚举） 12345678910111213141516171819202122232425262728#include&lt;iostream&gt;#include&lt;cstring&gt;#include&lt;algorithm&gt;using namespace std;const int N =110;int n,m;int f[N];int main()&#123; cin &gt;&gt;n&gt;&gt;m; for(int i =0;i&lt; n;i++) &#123; int v, w, s; cin &gt;&gt;v &gt;&gt;w&gt;&gt;s; for (int j =m; j&gt;=0; j--) for(int k =1; k&lt;=s &amp;&amp; k*v &lt;=j ; k++) f[j] =max(f[j], f[j -k*v] +k*w); &#125; cout &lt;&lt;f[m]&lt;&lt; endl; return 0;&#125; 多重背包问题 2 有$ N $件物品和一个容量是 $V $的背包。每件物品都有无限件可用。第 $i$ 件物品最多有 $s_i$ 件, 每件体积是 $v_i$，价值是 $w_i$。求解将哪些物品装入背包，可使这些物品的总体积不超过背包容量，且总价值最大。输出最大价值。 数据范围: $$\begin{split}0&lt; &amp; N \leq 1000 \\0&lt; &amp; V \leq 2000 \\0&lt; &amp; v_{i}, w_{i}, s_{i} \leq 2000\end{split}$$ Tips: 主要区别在于数据的维度。 这个是转换成 成 01 背包问题，上一道题目是在 01 背包问题的基础上（框架上）进行修改。 这个从时间复杂度上的优化：$1000 \times 2000 \times 2000$ -&gt; $1000 \times 2000 \times \log(2000) $ 。经过优化 基本上在 $ 10^7 $，所以这个速度是可以接受的。 c++ 在 1s 中就是可以计算 $10^7 $ 这样运算。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354#include&lt;iostream&gt;#include&lt;cstring&gt;#include&lt;algorithm&gt;#include&lt;vector&gt;using namespace std;const int N =2010 ; // 这个数量是 v的最大值int f[N];int n, m;struct Good&#123; int v, w; &#125;;vector&lt;Good&gt; goods;int main()&#123; cin &gt;&gt; n&gt;&gt;m; for(int i =0; i&lt;n ; i++) &#123; int v, w, s; cin &gt;&gt; v&gt;&gt; w&gt;&gt;s; for (int k =1; k&lt;=s; k *=2) &#123; s -=k; goods.push_back(&#123;v*k, w*k&#125;); &#125; if(s &gt;0) goods.push_back(&#123;v*s, w*s&#125;); &#125; // 01 hnapsack // goods 的长度不一定是 n， 所以不能这样进行遍历 /* for(int i =0; i&lt;n; i++)// 这个 for(int j =m ; j&gt;= goods[i].v; j--) &#123; f[j] =max(f[j], f[j -goods[i].v] +goods[i].w); &#125; */ for(auto good : goods) for(int j =m ; j&gt;= good.v; j--) f[j] =max(f[j], f[j -good.v]+ good.w); cout &lt;&lt; f[m]&lt;&lt; endl; return 0;&#125; 混合背包问题 有 $ N$种物品和一个容量是 $V$的背包。 物品一共有三类： 第一类物品只能用1次（01背包）； 第二类物品可以用无限次（完全背包）； 第三类物品最多只能用 $s_i $次（多重背包）； 每种体积是 $v_i$，价值是 $w_i$。求解将哪些物品装入背包，可使物品体积总和不超过背包容量，且价值总和最大。输出最大价值。 Tips： 分类讨论，多重背包问题转成 01背包问题，所以最后是有两类。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364#include&lt;iostream&gt;#include&lt;algorithm&gt;#include&lt;vector&gt;#include&lt;cstring&gt;using namespace std;/*分成两类： 01 背包问题和完全背包问题；对于多重背包问题使用 二进制进行优化，得到01 背包问题。所以需要一个新的结构体 01 背包问题只有两个循环，一个是物品的循环，一个是商品的循环*/const int N =1010;int f[N];int n, m;struct Thing&#123; int v, w, s;&#125;;vector&lt;Thing&gt; things;int main()&#123; cin &gt;&gt;n&gt;&gt;m; for(int i =0; i&lt;n ;i++) &#123; int v, w, s; cin &gt;&gt; v&gt;&gt; w&gt;&gt;s; if(s ==-1) things.push_back(&#123;v, w, -1&#125;); else if (s ==0) things.push_back(&#123;v, w, 0&#125;); else &#123; for(int k =1; k&lt;=s; k *=2) &#123; s -=k; things.push_back(&#123;v*k, w*k, -1&#125;); &#125; if (s &gt;0) things.push_back(&#123;v*s, w*s, -1&#125;); &#125; &#125; // 01 背包问题 for(auto thing: things) &#123; if(thing.s ==-1) &#123; for( int j =m ; j&gt;= thing.v; j--) f[j] =max(f[j], f[j -thing.v]+ thing.w); &#125; else &#123; for(int j =thing.v ; j&lt;= m; j++) f[j] =max(f[j], f[j -thing.v] +thing.w); &#125; &#125; cout &lt;&lt; f[m]&lt;&lt;endl; return 0;&#125; 二维费用的背包问题 有 N 件物品和一个容量是 V 的背包，背包能承受的最大重量是 M。每件物品只能用一次。体积是$ v_i$， 重量是 $m_i$，价值是$ w_i$。求解将哪些物品装入背包，可使物品总体积不超过背包容量，总重量不超过背包可承受的最大重量，且价值总和最大。输出最大价值。 Tips: 一般来说背包问题只有一个限制条件：V。但是这种是可以扩展到多维，即有多个限制条件，比如重量。很简单的扩展是加上一个循环。同理 f 这样的函数是可以扩展成二维的。 12345678910111213141516171819202122232425262728293031#include&lt;iostream&gt;#include&lt;cstring&gt;#include&lt;algorithm&gt;using namespace std;const int N =1010;int f[N][N];int n,v, m;int main()&#123; cin &gt;&gt; n&gt;&gt; v&gt;&gt;m; for (int i =0 ; i&lt; n; i++) &#123; int a, b, c; cin &gt;&gt; a&gt;&gt; b&gt;&gt;c; for(int j =v ;j&gt;=a ; j--) for(int k =m ; k&gt;= b; k--) &#123; f[j][k] =max(f[j][k], f[j- a][k -b] +c); &#125; &#125; cout &lt;&lt; f[v][m]&lt;&lt; endl; return 0;&#125;]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>knapsack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[All you need to know about seq2seq]]></title>
    <url>%2F2019%2F07%2F12%2Fseq2seq-theory%2F</url>
    <content type="text"><![CDATA[介绍 sequence to sequence 的定义、两种不同的结构、训练过程、注意力机制和常见的一些问题。本篇主要是理论，代码demo 可以参考另一篇。 定义和结构 定义 语言模型：对于任意的词序列，它能够计算出这个序列是一句话的概率。 Sequence-to-sequence (seq2seq) 模型，顾名思义，其输入是一个序列，输出也是一个序列. 目前来说，对于Seq2Seq生成模型来说，主要的思路是将该问题作为条件语言模型，在已知输入序列和前序生成序列的条件下,最大化下一目标词的概率，而最终希望得到的是整个输出序列的生成出现的概率最大：$$P ( Y | X ) = \sum _ { t = 1 } ^ { T } \log P \left( y _ { t } | y _ { 1 : t - 1 } , X \right)$$ seq2seq 可以用在很多方面：机器翻译、QA 系统、文档摘要生成、Image Captioning (图片描述生成器)。 Figure 1 介绍了一个标准的seq2seq模型。 其中红色的是encoder RNN, 绿色的是decoder RNN. 他们之间 有一个连线， 也就是encoder states 传给decoder RNN，当做initial state。 两种不同的结构 第一种， seq2seq 模型可简单理解为由三部分组成：Encoder、Decoder 和连接两者的 State Vector (中间状态向量) C 。 第二种结构是最简单的结构，和第一种结构相似，只是 Decoder 的第一个时刻只用到了 Encoder 最后输出的中间状态变量 ： 在两种不同的结构时候，两者的区别在 C (context) 向量是否多次使用，前一种在生成每个word的时候，都使用到了 context，而后者则没有。 训练过程 encoder 激活函数: softmax() 1234crossent = tf.nn.spase_softmax_cross_entropy_with_logits( labels=decoder_outputs, logits=logits)train_loss = (tf.reduce_sum(crossent * target_weights) / batch_size) loss 计算: 交叉熵是损失函数，针对单个样本；目标函数是针对整个数据集的损失函数+ 正则项。 decoder 当你训练你的 NMT 模型时（并且一旦你已经训练了模型），可以在给定之前不可见的源语句的情况下获得翻译。这一过程被称作推理。训练与推理之间有一个明确的区分（测试）：在推理时，我们只访问源语句，即 encoder_inputs。解码的方式有很多种，常见的有greedy 解码和束搜索解码（beam-search）。 我们知道在Seq2Seq模型的最终目的是希望生成的序列发生的概率最大，也就是生成序列的联合概率最大。 实际做的时候有两种算法进行 decoder。 greedy decoding：（贪心算法思维，得到是一种局部最优解） Fig 1 描述的是Inference 的状态， decode 输出来的每一个单词都会当做下一个时刻的输入，来进行翻译。 而在训练过程中， 因为知道翻译出来的单词是什么，就会把这个单词当做输入进行训练。 图中 inference的过程中用到了argmax, 这个函数，也就是每次都选择概率最大的那个单词当做翻译。 这个叫做greedy decoding. 这个不是optimal solution。 贪心搜索只选择了概率最大的一个，而集束搜索 (beam search)则选择了概率最大的前k个。这个k值也叫做集束宽度（Beam Width），算法复杂度是O(nKv), v 是字典的大小， n 是输出序列的长度, k 表示保留的解的个数。 Beam search decoding： 这个方法在每个翻译的步骤 都保存k 个最可能的选择， k 就是beam size, 这个方法虽然不能保证最优解， 但是效率高了很多。 k 一般就是5-10. 上图就是一个 beam size =2 的例子。 在T=0 的时候，选择概率最大的两个。 在T =1 的时候，分别将 the 和 a 输入得到两个概率模型，然后选择概率和最大的两个序列。以此类推，最终得到两个序列。然后前者的概率和大于后者，所以就选择的上面的那个，其他的路径就可以丢掉了。 注意力机制Encoder-Decoder 模型的局限性（有两点） 中间语义向量无法完全表达整个输入序列的信息。 随着输入信息长度的增加，由于向量长度固定，先前编码好的信息会被后来的信息覆盖，丢失很多信息。 Attention 模型的特点是 Decoder 不再将整个输入序列编码为固定长度的中间语义向量 Ｃ ，而是根据当前生成的新单词计算新的 $C_{i}$ ，使得每个时刻输入不同的 Ｃ，这样就解决了单词信息丢失的问题。引入了 Attention 的 Encoder-Decoder 模型如下图： 应用在英文翻译中，将英文输入到 Encoder 中，Decoder 输出中文。在图像标注中，将图像特征输入到 Encoder 中，Decoder 输出一段文字对图像的描述。在 QA 系统中，将提出的问题输入 Encoder 中，Decoder 输出对于问题的回答。 常见问题 Exposure Bias： 问题描述：Seq2Seq模型训练的过程中，编码部分的下一个时刻的输出，是需要根据上一个时刻的输出和上一个时刻的隐藏状态和语义变量Ci.此时上一个时刻的输出使用的是真实的token。而在验证Seq2Seq模型的时候，由于不知道上一个时刻的真实token，上一个时刻的输出使用的是上上个时刻的预测的输出token。这将引发Exposure Bias(曝光偏差问题)。 使用真实 token 本身的行为叫做 teacher forcing。 一种解决思路：使用Beam Search的Encoder的方式也能一定程度上降低Exposure Bias问题 （该问题可以理解为过分的依赖上一个真实的 token 或者说标签），因为其考虑了全局解码概率，而不仅仅依赖与前一个词的输出，所以模型前一个预测错误而带来的误差传递的可能性就降低了 exposure bias 的定义：RNN 在 training 时接受 ground truth input，但 testing 时却接受自己之前的 output，这两个 setting不一致会导致 error accumulate解决思路：为了解决 exposure bias 问题，提出了 scheduled sampling，即在先前的 GroundTrue 单词和先前产生的单词之间随机选择，已经成为适合基于RNN的模型的当前主要训练程序。 然而，它只能减轻 exposure bias，但不能很大程度上解决它。 OOV 和低频词 问题描述：OOV表示的是词汇表外的未登录词，低频词则是词汇表中的出现次数较低的词。在Decoder阶段时预测的词来自于词汇表，这就造成了未登录词难以生成，低频词也比较小的概率被预测生成。 一种解决思路：如果 focus 在文本摘录领域，由于其任务的特点，很多OOV 或者不常见的的词其实可以从输入序列中找到，因此一个很自然的想法就是去预测一个开关（switch）的概率 $P(si=1)=f(hi,yi-1,ci) $，如果开关打开了，就是正常地预测词表；如果开关关上了，就需要去原文中指向一个位置作为输出。 连续生成重复词的问题 问题需要从本质上进行理解，为什么生成了重复复的词语，就是在目前生成的词语上，下一个词语发现某个词一直是概率最大的。所以一种方式是通过 beam search，给出了多种不同的候选词汇，然后计算整体的句子的概率。 一种方式是判断然后惩戒，发现生成了和上一个词语相同的词语，那么拒绝这个词语。 teaching forcing其实RNN存在着两种训练模式(mode): free-running mode: 就是大家常见的那种训练网络的方式: 上一个state的输出作为下一个state的输入。问题： 如果一个unit 出现了garbage ，那么必定是会影响后面一片 unit 的学习效果；造成了不收敛 teacher-forcing mode: 使用目标语言中的实际输出作为 decoder 的输入。优点： 训练变得稳定，快速的收敛缺点：效果并不是那么好，因为在 test 的时候，是没有ground true 的支持，所以模型就效果很差。 常见的几种处理手段：集束搜索（beam search）：产生多个候选词，优化输出序列。实际上是一种剪枝后的深度优先搜索算法。计划学习（curriculum learning）：总的思想，训练开始是 teacher forcing mode，然后随着训练的过程，慢慢降低 ground true的频率。 定期采样Scheduled Sampling：（这个又是一种中和策略，需要调参） 随机的使用目标语言的输出和 decoder 预测的输出。 相对于 beam search而言，每个时刻只是选择 top1的那种，类似一种求解局部最优解，实际上是一种贪心算法。 复习 有两种encoder-decoder 结构，区别在于中间状态信息(context) 是否被使用多次。一种是在生成每个词语的时候都用到了 context 信息，一种是只有在decoder 第一个word 的时候使用context信息。 损失函数、代价函数和目标函数的区别。损失函数是针对单个样本， 代价函数是针对整个数据集，目标函数是代价函数+ 正则项。目标函数是可以最大化或者最小化，但是代价函数是需要最小化的函数。 在encoder-decoder 的模型中希望生成序列发生的概率最大，也就是生成序列的联合概率最大。有两种算法来记性decoder，一种是贪心搜索，得到的是局部最优解；一种是 beam search。前者是根据模型和数据每次选择概率最大的一个；后者是选择概率最大的前K 个，这个参数称为 beam width。算法复杂度相对前者是K 倍的。 exposure bias 是指在训练和predict 过程中模型输入不一致而导致的问题。比如说，在训练过程中，使用上一个真实的词输入（使用真实token 本身的行为叫做teacher forcing）到下一个预测的神经元中，而在预测的时候是没有真实词的输入。这会导致错误的累计。解决思路：(1) 使用 scheduled sampling，在训练的时候 真实单词和之前产生的单词随机选择，使得模型不过分依赖真实单词。(2): 使用beam search 的思想，通过多条路径而非单一路径缓解。 连续生成重复词语问题的解决思路: (1) beam search 的思路，给出多个不同的候选词 (2) 判断惩戒，如果生成了相同的上一个词语，那么拒绝这个词语。 对于decoder 中神经元的输入存在着两种模式：(1) free-running mode， 使用上一个状态的输出组委下一个状态的输入；(2) teacher forcing: 使用 真实结果进行指导，这样使得训练变得稳定，快速的收敛。]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>teacher_forcing</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[A Gentle Introduction of GBDT]]></title>
    <url>%2F2019%2F07%2F12%2Fgbdt%2F</url>
    <content type="text"><![CDATA[介绍GBDT 的定义、训练过程、优缺点和常见的问题。 Boosting、bagging和stacking是集成学习的三种主要方法。不同于bagging方法，boosting方法通过分步迭代（stage-wise）的方式来构建模型，在迭代的每一步构建的弱学习器都是为了弥补已有模型的不足，通过弱学习器提升为 强学习器的算法。Boosting族算法的著名代表是AdaBoost 和GBDT。 定义由于GBDT的学习过程是通过多轮迭代，每次都在上一轮训练结果的残差（如果是回归问题，使用平方误差作为loss function）的基础上进行学习（基函数的线性组合），来对数据进行分类或者回归。训练的过程是通过不断降低偏差来提高最后分类器的精度。 理解GBDT要分为两步，第一步是理解什么叫做用决策树去拟合当前模型的残差，第二步是理解为什么以及如何用损失函数的负梯度去替代当前模型的残差。 GDBT 处理分类问题GBDT在解决分类问题时有两种办法，一个是选择指数损失函数作为损失函数，此时GBDT退化为AdaBoost算法，另一个是选择类似于逻辑回归的对数似然损失函数。（逻辑回归使用的是对数似然函数） 当损失函数取平方误差损失函数和指数损失函数时，每一步的优化还算简单，可是如果损失函数是其他一般损失函数时，那可就难了。类似逻辑回归中的求解过程，使用梯度下降的方式简化了优化过程，学者使用梯度上升的方式近似求解提升树的优化过程。GBDT这个算法中最关键的一点就是用损失函数在当前模型中的负梯度值，即： 使用平方损失函数时候，GBDT算法的每一步在生成决策树只是拟合前面模型的残差，（y- y^）残差是梯度的一个特例。而当损失函数是其他的形式时候，下一次迭代是使用的负梯度值。 对于一般损失函数，为了使其取得最小值，通过梯度下降算法，每次朝着损失函数的负梯度方向逐步移动，最终使得损失函数极小的方法（此方法要求损失函数可导）。 总的来说，第一颗树是由基尼系数决定? ，之后所有的树的决策全是由残差来决定。 GBDT的思想可以用一个通俗的例子解释，假如有个人30岁，我们首先用20岁去拟合，发现损失有10岁，这时我们用6岁去拟合剩下的损失，发现差距还有4岁，第三轮我们用3岁拟合剩下的差距，差距就只有一岁了。如果我们的迭代轮数还没有完，可以继续迭代下面，每一轮迭代，拟合的岁数误差都会减小。 训练过程How Gradient Boosting WorksGradient boosting involves three elements: A loss function to be optimized. A weak learner to make predictions. An additive model to add weak learners to minimize the loss function. loss functionThe loss function used depends on the type of problem being solved.It must be differentiable, but many standard loss functions are supported and you can define your own.For example, regression may use a squared error and classification may use logarithmic loss.A benefit of the gradient boosting framework is that a new boosting algorithm does not have to be derived for each loss function that may want to be used, instead, it is a generic enough framework that any differentiable loss function can be used. log loss主要用来衡量二分类，cross entroy主要用来衡量多分类，前者是后者在二分类下的特例交叉熵公式 (cross entroy) $- \sum _ { i = 1 } ^ { K } p _ { i } \log q _ { i }$ 其中 $p_i$ 表示真实的分布， $q_i$ 表示预测分布, $K$ 表示分类数; 当 $K =2$ 交叉熵退化成 log loss: $- [ y \log \hat { y } + (1-y)log(1 -\hat{y}) ]$ cross entroy与logloss主要用来衡量分类算法性能，因为cross entroy意义是衡量真实分布p和预测分布q的分布差异程度；mse主要用来衡量回归算法性能； weak learner弱分类器一般选择 CART （分类回归树） Classification And Regression Tree(CART)是决策树的一种，并且是非常重要的决策树， Decision trees are used as the weak learner in gradient boosting.Specifically regression trees are used that output real values for splits and whose output can be added together, allowing subsequent models outputs to be added and “correct” the residuals in the predictions. It is common to constrain the weak learners in specific ways, such as a maximum number of layers, nodes, splits or leaf nodes.This is to ensure that the learners remain weak, but can still be constructed in a greedy manner. additive modelTrees are added one at a time, and existing trees in the model are not changed.A gradient descent procedure is used to minimize the loss when adding trees. Traditionally, gradient descent is used to minimize a set of parameters, such as the coefficients in a regression equation or weights in a neural network. After calculating error or loss, the weights are updated to minimize that error.Instead of parameters, we have weak learner sub-models or more specifically decision trees. After calculating the loss, to perform the gradient descent procedure, we must add a tree to the model that reduces the loss (i.e. follow the gradient). We do this by parameterizing the tree, then modify the parameters of the tree and move in the right direction by (reducing the residual loss.Generally this approach is called functional gradient descent or gradient descent with functions. GBDT学习过程：先使用一个初始值来学习一个决策树，叶子可以得到预测的值，以及残差，然后后面的决策树是基于前面的决策树的残差进行学习，直到残差为0.对于测试样本的预测值，就是许多决策树预测值的累加。 GBDT通过多轮迭代,每轮迭代产生一个弱分类器，每个分类器在上一轮分类器的残差基础上进行训练。对弱分类器的要求一般是足够简单，并且是低方差和高偏差的。因为训练的过程是通过降低偏差来不断提高最终分类器的精度。 损失函数：二分 log loss ，多分 交叉熵loss，回归 平方损失弱学习器（weak learner）：加法模型： 使用梯度下降的方式去减少loss， 当增加一个新的树A gradient descent procedure is used to minimize the loss when adding trees. 具体说这个弱学习器，GBDT 选择特征的过程就是 CART (分类回归树)选择特征的过程。选择特征是：遍历每个特征和每个特征的所有切分点，找到最优的特征和最优的切分点。多个CART TREE 生成过程中，选择最优特征切分较多的特征就是重要的特征。 目前GBDT 的算法比较好的库是 xgboost 优缺点优点（相对于 LR or SVM）： 可以灵活的处理各种类型的数据，包括连续型和离散型，处理分类和回归问题 继承了树模型的优点： 对异常值鲁棒，处理缺省值 即使是大量数据，也可以方便的处理，相对比SVM 来说。 缺点： 无法并行 在不带噪声的数据上，分类效果不如LR或者 SVM 常见的问题 为什么GBDT要把CART回归树树分成m棵二叉树去求（每棵树只有两个叶子节点），而不是求一棵二叉树，这棵树有m+1（最多有2m个叶子节点）层呢？ 这是为了解决过拟合问题，基学习器要具有简单、高偏差和低方差的特点，因此每棵CART回归树的深度不会很深。 为什么第m次学习的目标，是前m-1棵树预测值的累加和的残差？ 一方面通过分步求解，一步步逼近目标值，比一步到位要简单；另一方面每一步的残差计算其实变相地增大了被分错的实例的权重，因为被分错的实例其残差较大，而已经分对的实例的残差趋近于0。这样后面的树就能越来越专注于前面被分错的实例了。提升树是迭代多棵回归树来共同决策。当采用平方误差损失函数时，每一棵回归树学习的是之前所有树的结论和残差，拟合得到一个当前的残差回归树，残差的意义如公式：残差 = 真实值 - 预测值 。提升树即是整个迭代过程生成的回归树的累加。 AdaBoost 算法和 GBDT 算法的区别 Boosting族算法的著名代表是AdaBoost，AdaBoost算法通过给已有模型预测错误的样本更高的权重，使得先前的学习器做错的训练样本在后续受到更多的关注的方式来弥补已有模型的不足。与AdaBoost算法不同，梯度提升方法在迭代的每一步构建一个能够沿着梯度最陡的方向降低损失（steepest-descent）的学习器来弥补已有模型的不足。经典的AdaBoost算法只能处理采用指数损失函数的二分类学习任务2，而梯度提升方法通过设置不同的可微损失函数可以处理各类学习任务（多分类、回归、Ranking等），应用范围大大扩展。另一方面，AdaBoost算法对异常点（outlier）比较敏感，而梯度提升算法通过引入bagging思想、加入正则项等方法能够有效地抵御训练数据中的噪音，具有更好的健壮性。这也是为什么梯度提升算法（尤其是采用决策树作为弱学习器的GBDT算法）如此流行的原因，有种观点认为GBDT是性能最好的机器学习算法，这当然有点过于激进又固步自封的味道，但通常各类机器学习算法比赛的赢家们都非常青睐GBDT算法，由此可见该算法的实力不可小觑。 损失函数：adaboost 使用的是指数损失函数的算法，而gbdt 可以使用不同的可微的损失函数进行分类和回归问题 adaboost 一般是用于分类，gbt 一般用于回归。 为什么是低方差？gbdt通过多轮迭代,每轮迭代产生一个弱分类器，每个分类器在上一轮分类器的残差基础上进行训练。对弱分类器的要求一般是足够简单，并且是低方差和高偏差的。因为训练的过程是通过降低偏差来不断提高最终分类器的精度，（此处是可以证明的）。通俗的理解，方差比较低就是模型表现的稳定与否。 什么是NP 问题?NP问题是指可以在多项式的时间里验证一个解的问题。NP问题的另一个定义是，可以在多项式的时间里猜出一个解的问题。 Bagging 算法? Bagging算法是这样做的：每个分类器都随机从原样本中做有放回的采样，然后分别在这些采样后的样本上训练分类器，然后再把这些分类器组合起来。简单的多数投票一般就可以。其代表算法是随机森林 AdaBoostAdaBoost，是英文”Adaptive Boosting”（自适应增强）的缩写。它的自适应在于：前一个基本分类器分错的样本会得到加强，加权后的全体样本再次被用来训练下一个基本分类器。同时，在每一轮中加入一个新的弱分类器，直到达到某个预定的足够小的错误率或达到预先指定的最大迭代次数。 具体说来，整个Adaboost 迭代算法就3步： 初始化训练数据的权值分布。如果有N个样本，则每一个训练样本最开始时都被赋予相同的权值：1/N。 训练弱分类器。具体训练过程中，如果某个样本点已经被准确地分类，那么在构造下一个训练集中，它的权值就被降低；相反，如果某个样本点没有被准确地分类，那么它的权值就得到提高。然后，权值更新过的样本集被用于训练下一个分类器，整个训练过程如此迭代地进行下去。 将各个训练得到的弱分类器组合成强分类器。各个弱分类器的训练过程结束后，加大分类误差率小的弱分类器的权重，使其在最终的分类函数中起着较大的决定作用，而降低分类误差率大的弱分类器的权重，使其在最终的分类函数中起着较小的决定作用。换言之，误差率低的弱分类器在最终分类器中占的权重较大，否则较小。 对于决策树，Adaboost分类用了CART分类树，而Adaboost回归用了CART回归树。 AdaBoost 的优点： 作为分类器，分类精度比较高 简单的二元分类器，构造简单，结果可理解 不容易发生过拟合 缺点： 对异常样本敏感，异常样本在迭代过程中可能获得较高的权重，影响最终的强学习器的预测准确率。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>gbdt</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Text Summarization]]></title>
    <url>%2F2019%2F06%2F30%2Ftext-summarization%2F</url>
    <content type="text"><![CDATA[Text Summarization Text summarization is the process of distilling the most important information from a source (or sources) to produce an abridged version for a particular user (or users) and task (or tasks). There are many reasons why Automatic Text Summarization is useful: Summaries reduce reading time. Automatic summarization algorithms are less biased than human summarizers. Personalized summaries are useful in question-answering systems as they provide personalized information. Text summarization methods can be classified into different types. 虽然是可以从不同的角度进行划分，但最常见的分类角度是 based on output type: extractive and abstractive. All extractive summarization algorithms attempt to score the phrases or sentences in a document and return only the most highly informative blocks of text. Abstractive text summarization actually creates new text which doesn’t exist in that form in the document. Abstractive summarization is what you might do when explaining a book you read to your friend, and it is much more difficult for a computer to do than extractive summarization. extractive 是从源句子中找关键句的过程， abstractive 是概括，生成对于文章总结的过程。 Computers just aren’t that great at the act of creation. To date, there aren’t any abstractive summarization techniques which work suitably well on long documents. The best performing ones merely create a sentence based upon a single paragraph, or cut the length of a sentence in half while maintaining as much information as possible. Often, grammar suffers horribly. They’re usually based upon neural network models. What is ROUGE?To evaluate the goodness of the generated summary, the common metric in the Text Summarization space is called Rouge score. ROUGE stands for Recall-Oriented Understudy for Gisting Evaluation It works by comparing an automatically produced summary or translation against a set of reference summaries (typically human-produced). It works by matching overlap of n-grams of the generated and reference summary. Extractive Techniques LexRank SummarizerLexRank is an unsupervised approach that gets its inspiration from the same ideas behind Google’s PageRank algorithm. It finds the relative importance of all words in a document and selects the sentences which contain the most of those high-scoring words. NLTK Summarizer 将句子的重要程度下放到词语 word 的关键程度。先是文本的预处理，然后挑选关键词，然后挑选关键句。 Although the technique is basic, we found that it did a good job at creating large summaries. Gensim Summarizer （pagerank 的思想很简单，每一个网页都是一种投票，然后被投票的重要程度越高，那么这个网页或者网站的重要性就越高，最后的排名就越靠前）TextRank is based on PageRank algorithm that is used on Google Search Engine. In simple words, it prefers pages which has higher number of pages hitting it. TextRank is a bit more simplistic than LexRank; although both algorithms are very similar, LexRank applies a heuristic post-processing step to remove sentences with highly duplicitous. The gensim algorithm does a good job at creating both long and short summaries. Another cool feature of gensim is that we can get a list of top keywords chosen by the algorithm. This feature can come in handy for other NLP tasks, where we want to use “TextRank” to select words from a document instead of “Bag of Words” or “TF-IDF”. Gensim also has a well-maintained repository and has an active community which is an added asset to using this algorithm. Sentence Embeddings（当源文本比较短的时候，比如 review 或者email， sentence embedding ，然后使用聚类的方式，得到不同的簇之后，每个簇可以选择一个句子，类似于降维的思想） We wanted to evaluate how text summarization works on shorter documents like reviews, emails etc. We used K-means clustering to summarize the types of documents following the aforementioned structure. Then, all of the sentences in a document are clustered in k = sqrt(length of document) clusters. Each cluster of sentence embeddings can be interpreted as a set of semantically similar sentences whose meaning can be expressed by just one candidate sentence in the summary. Candidate sentences corresponding to each cluster are then ordered to form a summary for an email. The order of the candidate sentences in the summary is determined by the positions of the sentences in their corresponding clusters in the original document. Abstraction techniques Pointer — Generator Networks Compared to the sequence-to-sequence-with-attention system, the pointer-generator network does a better job at copying words from the source text. Additionally it also is able to copy out-of-vocabulary words allowing the algorithm to handle unseen words even if the corpus has a smaller vocabulary. Hence we can think of pointer generator as a combination approach combining both extraction (pointing) and abstraction (generating). Drawbacks of Abstractive summarization Firstly, training the model requires a lot of data and hence time. An inherent problem with abstraction is that the summarizer reproduces factual details incorrectly. For instance, if the article talks about Germany beating Argentina 3–2, the summarizer may replace 3–2 by 2–0 Repetition is another problem faced by the summarizer. As we can see in the second example above, some phrases are repeated in the summary To Summarize..Given the architecture of RNNs and the current computing capabilities, we observed that extractive summarization methods are faster, but equally intuitive as abstractive methods. A few other observations: The network fails to focus on the core of the source text and summarizes a less important, secondary piece of information The attention mechanism, by revealing what the network is “looking at”, shines some precious light into the black box of neural networks, helping us to debug problems like repetition and copying. To make further advances, we need greater insight into what RNNs are learning from text and how that knowledge is represented. Case Study: Text Summarization on EmailsUnsupervised Text Summarization using Sentence Embeddings Step-1: Email CleaningAs salutation and signature lines (称谓签名行) can vary from email to email and from one language to the other, removing them will require matching against a regular expression. Hi Jane, Thank you for keeping me updated on this issue. I&apos;m happy to hear that the issue got resolved after all and you can now use the app in its full functionality again. Also many thanks for your suggestions. We hope to improve this feature in the future. In case you experience any further problems with the app, please don&apos;t hesitate to contact me again. Best regards, John Doe Customer Support 1600 Amphitheatre Parkway Mountain View, CA United States Step-2: Language Detection As the emails to be summarized can be of any language, the first thing one needs to do is to determine which language an email is in. I used langdetect for my purpose and it supports 55 different languages. Step-3: Sentence TokenizationOnce the languages identification is performed for every email, we can use this information to split each email into its constituent sentences using specific rules for sentence delimiters (分隔符) for each language. Step-4: Skip-Thought EncoderWe need a way to generate fixed length vector representations for each sentence in our emails. Step-5: Clustering（对于 最后summary 的长度，这个是一个很好的baseline，给出了一个定量的结果，使用 $ \sqrt{N}$, N 表示原始段落或者文章的长度）The number of clusters will be equal to desired number of sentences in the summary. I chose the numbers of sentences in the summary to be equal to the square root of the total number of sentence in the email. Step-6: SummarizationEach cluster of sentence embeddings can be interpreted as a set of semantically similar sentences whose meaning can be expressed by just one candidate sentence in the summary. The candidate sentence is chosen to be the sentence whose vector representation is closest to the cluster center. Candidate sentences corresponding to each cluster are then ordered to form a summary for an email. PageRank 算法和TextRank 介绍 PageRank 算法 PageRank通过互联网中的超链接关系来确定一个网页的排名，其公式是通过一种投票的思想来设计的。整个互联网可以看作是一张有向图，网页是图中的节点，网页之间的链接就是图中的边。如果网页 A 存在到网页 B 的链接，那么就有一条从网页 A 指向网页 B 的有向边。构造完图后，使用下面的公式来计算网页$ i$的重要性（PR值）： $$S \left( V _ { i } \right) = ( 1 - d ) + d \cdot \sum _ { j \in I n \left( V _ { i } \right) } \frac { 1 } { \left| O u t \left( V _ { j } \right) \right| } S \left( V _ { j } \right)$$ $d$ 是阻尼系数，一般设置为0.85. $\operatorname { In } \left( V _ { i } \right)$ 是指向网页 $i$ 的链接的网页集合。 $\operatorname { Out } \left( V _ { j } \right)$ 是网页 $j$ 中的链接指向的网页的集合。 $\left| O u t \left( V _ { j } \right) \right|$ 是集合中元素的个数。 PageRank 需要多次迭代才能得到最后的结果。 (公式看着比较复杂，但是原理非常简单，如果计算网页 i的 pagerank，那么对于所有指向该网页的网站集合，进行权重的分摊。也就是说某个pageranking 很高的网站指向了某个网站，那么这个网站的pageranking 会变高，类似带有权重的投票) 假设我们有4个网页——w1，w2，w3，w4。这些页面包含指向彼此的链接。有些页面可能没有链接，这些页面被称为悬空页面。 Webpage Links w1 [w4, 22] w2 [w3, w1] w3 [] w4 [w1] 在本例中初始化成这样： 最后，这个矩阵中的值将以迭代的方式更新，以获得网页排名。 （上面的pageranking 的讲解是为了 下面的textranking 的理解） TextRank 关键词提取 两者的相似之处： In place of web pages, we use sentences Similarity between any two sentences is used as an equivalent to the web page transition probability The similarity scores are stored in a square matrix, similar to the matrix M used for PageRank 不同之处在于后者使用句子之间的相似度作为 weights. $W S \left( V _ { i } \right) = ( 1 - d ) + d \cdot \sum _ { V _ { j } \in \operatorname { In } ( V i ) } \frac { w _ { j i } } { \sum _ { V _ { k } \in O u t \left( V _ { j } \right) } w _ { j k } } W S \left( V _ { j } \right)$ $w_{ij}$ 就是图中节点 $V_i$ 到$V_j$ 的边的权值， 就是两个句子 $S_i$ 和句子 $S_j$ 的相似程度。下面的代码中使用的是 cosine 函数来表示这种相似度。 步骤： The first step would be to concatenate all the text contained in the articles Then split the text into individual sentences In the next step, we will find vector representation (word embeddings) for each and every sentence Similarities between sentence vectors are then calculated and stored in a matrix The similarity matrix is then converted into a graph, with sentences as vertices and similarity scores as edges, for sentence rank calculation Finally, a certain number of top-ranked sentences form the final summary 这里使用的 cosine similarity scores，计算句子之间的相似度，句子embedding 是通过 word embedding 相加而成。1234for i in range(len(sentences)): for j in range(len(sentences)): if i != j: sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))[0,0] Applying PageRank Algorithm123import networkx as nxnx_graph = nx.from_numpy_array(sim_mat)scores = nx.pagerank(nx_graph) TF-IDF 和 TextRank 对比总结： TextRank与TFIDF均严重依赖于分词结果——如果某词在分词时被切分成了两个词，那么在做关键词提取时无法将两个词黏合在一起（TextRank有部分黏合效果，但需要这两个词均为关键词）。因此是否添加标注关键词进自定义词典，将会造成准确率、召回率大相径庭。 TextRank的效果并不优于TFIDF。 TextRank虽然考虑到了词之间的关系，但是仍然倾向于将频繁词作为关键词。 由于TextRank涉及到构建词图及迭代计算，所以提取速度较慢。 自然语言处理有两个切入点，一个是频率一个是语义。上述两种方法本质上还是基于词频的。如何进行方法的评价，如果是基于概率 （频率）进行计算，那么一个切入点是否能够体现上下文关系，是否有语义信息。如果是基于网络，那么一个方法就是内存和训练时间上是否可以在大规模的工业界展开使用。 Referenceshttps://medium.com/jatana/unsupervised-text-summarization-using-sentence-embeddings-adb15ce83db1 https://towardsdatascience.com/data-scientists-guide-to-summarization-fc0db952e363 https://www.jiqizhixin.com/articles/2018-12-28-18 https://www.analyticsvidhya.com/blog/2018/11/introduction-text-summarization-textrank-python/ https://xiaosheng.me/2017/04/08/article49/ 复习总结 文本摘要有两种方法，一种是从源句子中抽取的过程，一种是概括的过程，生成对于文章总结的过程。 pagerank 算法和 textrank 算法对于简历上的内容相关性不大，就先不复习了]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>text-summarization</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NLP Papers Reading-Machine Translation]]></title>
    <url>%2F2019%2F06%2F29%2Fnlp-papers-reading-machine-translation%2F</url>
    <content type="text"><![CDATA[简单介绍一下 翻译系统的发展过程 然后附上两篇该方面论文的阅读笔记， 即Sequence to Sequence Learning with Neural Network 和 Effective Approaches to Attention-based Neural Machine Translation。 五个主要的发展过程why difficult? Machine translation is challenging given the inherent ambiguity and flexibility of human language. Statistical machine translation replaces classical rule-based systems with models that learn to translate from examples. Neural machine translation models fit a single model rather than a pipeline of fine-tuned models and currently achieve state-of-the-art results. Rule-based Machine Translation Classical machine translation methods often involve rules for converting text in the source language to the target language. The rules are often developed by linguists and may operate at the lexical, syntactic, or semantic level. This focus on rules gives the name to this area of study: Rule-based Machine Translation, or RBMT. The key limitations of the classical machine translation approaches are both the expertise required to develop the rules, and the vast number of rules and exceptions required. Statistical Machine Translation Statistical machine translation, or SMT for short, is the use of statistical models that learn to translate text from a source language to a target language given a large corpus of examples. Given a sentence T in the target language, we seek the sentence S from which the translator produced T. We know that our chance of error is minimized by choosing that sentence S that is most probable given T. Thus, we wish to choose S so as to maximize $P_r(S|T)$. The approach is data-driven, requiring only a corpus of examples with both source and target language text. This means linguists are not longer required to specify the rules of translation. Although effective, statistical machine translation methods suffered from a narrow focus on the phrases being translated , losing the broader nature of the target text. The hard focus on data-driven approaches also meant that methods may have ignored important syntax distinctions known by linguists. Finally, the statistical approaches required careful tuning of each module in the translation pipeline. Neural Machine Translation The key benefit to the approach is that a single system can be trained directly on source and target text, no longer requiring the pipeline of specialized systems used in statistical machine learning. As such, neural machine translation systems are said to be end-to-end systems as only one model is required for the translation. Encoder-Decoder Model Multilayer Perceptron neural network models can be used for machine translation, although the models are limited by a fixed-length input sequence where the output must be the same length. These early models have been greatly improved upon recently through the use of recurrent neural networks organized into an encoder-decoder architecture that allow for variable length input and output sequences. The key to the encoder-decoder architecture is the ability of the model to encode the source text into an internal fixed-length representation called the context vector. Interestingly, once encoded, different decoding systems could be used, in principle, to translate the context into different languages. The power of this model lies in the fact that it can map sequences of different lengths to each other. Encoder-Decoders with Attention Although effective, the Encoder-Decoder architecture has problems with long sequences of text to be translated. The problem stems from the fixed-length internal representation that must be used to decode each word in the output sequence. The solution is the use of an attention mechanism that allows the model to learn where to place attention on the input sequence as each word of the output sequence is decoded. The encoder-decoder recurrent neural network architecture with attention is currently the state-of-the-art on some benchmark problems for machine translation. And this architecture is used in the heart of the Google Neural Machine Translation system, or GNMT, used in their Google Translate service. Although effective, the neural machine translation systems still suffer some issues, such as scaling to larger vocabularies of words and the slow speed of training the models. There are the current areas of focus for large production neural translation systems, such as the Google system. Attention VS LSTM A limitation of the LSTM architecture is that it encodes the input sequence to a fixed length internal representation. This imposes limits on the length of input sequences that can be reasonably learned and results in worse performance for very long input sequences. After reading this, you will know: This (LSTM) is believed to limit the performance of these networks, especially when considering long input sequences, such as very long sentences in text translation problems. Put another way, each item in the output sequence is conditional on selective items in the input sequence. 而对于 Attention 而言: Each time the proposed model generates a word in a translation, it (soft-) searches for a set of positions in a source sentence where the most relevant information is concentrated. The model then predicts a target word based on the context vectors associated with these source positions and all the previous generated target words.… it encodes the input sentence into a sequence of vectors and chooses a subset of these vectors adaptively while decoding the translation. This frees a neural translation model from having to squash all the information of a source sentence, regardless of its length, into a fixed-length vector. And, This (Attention) increases the computational burden of the model, but results in a more targeted and better-performing model. 同样，除了在 machine translation 中有应用， 在其他领域如image description, CNN 中同样有借鉴的意义。Convolutional neural networks applied to computer vision problems also suffer from similar limitations, where it can be difficult to learn models on very large images. 参考文献 introduction-neural-machine-translationattention-long-short-term-memory-recurrent-neural-networks 两篇machine translation 论文 Sequence to Sequence Learning with Neural Network 研究目的：CNNs需要输入、输出维度是已知和固定的。而语音识别、机器翻译、问答系统等序列到序列问题的序列长度是未知的。CNN有一个明显的缺陷：CNN只能处理输入、输出向量维度是定长的情形。对于输入、输出可变长的情况，使用RNN-Recurrent Neural Network更易求解。 论文贡献之一在于网络结构上：通过学习编码一个可变长度的序列成一个固定长度的向量表示，解码一个给定的固定长度的向量成一个可变长度的序列。实现的时候首先将source sequence通过一个encode LSTM map成一个vector，然后再通过另一个decoder LSTM进行翻译得出output，这也恰恰是image caption里的思想呀（通过CNN将输入图像conv成一个vector或者feature map，然后再输入LSTM），原来大体是这样，接着看。 另外还有一个小的策略：LSTM在长句翻译中的表现也不俗。这归功于对源序列中词序的逆转。虽然LSTM能够基于长期的相关性处理问题，但我们发现在把原句序列逆转的情况下LSMT能学习得更加出色。逆转之后，LSTM测试的复杂度从5.8降至4.7，并且在BLEU上的得分从25.9提升至30.6。 不足之处：其他方面都比较普通，或者说很多论文中都有提到过，比如LSTM可以解决vanishing的问题但没法解决gradient exploding的问题，因此採取gradient crop。模型採用了SGD without momentum。实用的LSTM结构式Grave的《Generating sequence from RNN》中的LSTM结构，等等。 总结：总体来说，这个模型还是採取了贪婪的算法，换句话说，后面的预测对前面的状态有极强的依赖，一旦前面的预测出现问题，后面的预测就不可靠了，这也是一个值得思考和改进的地方。 Effective Approaches to Attention-based Neural Machine Translation 这篇文章的核心在于 attention。 Attention 的作用可以看作是一个对齐模型，传统 SMT 我们用 EM 算法来求解对齐，这里做一个隐式的对齐，将 alignment model 用一个 feedforward neural network 参数化，和其他部分一起训练，神经网络会同时来学习 翻译模型(translation) 和 对齐模型(alignment)。 Attention 可以分成 hard and soft两种模型，简单理解 hard attention 就是从 source sentence 中找到一个能产生单词 $t^{th}$ 对齐的特定单词，把 $s_{t,i}$ 设为1，其他所有单词硬性的认为其概率为0; soft attention 对于source sentence中每个单词都给出一个对齐概率，得到一个概率分布，context vector 就是这些概率分布的一个加权和，整个模型是平滑的且处处可分。 而在该篇论文中提出了一个新的 attention 机制 local attention，在得到 context vector 时，我们不想看所有的 source hidden state，而是每次只看一个 hidden state 的子集(subset)，这样的 attention 其实更集中，也会有更好的结果。Global attention 其实就是 soft attention， local model 实际相当于 hard 和 soft attention 的一个混合或者说折中，主要是用来降低 attention 的花费，简单来说就是每次计算先用预测函数得到 source 相关信息的窗口。 soft or hard attention 还是 global or local attention是从不同的角度进行分类的，前者是在概率分布上，后者是在 context上。 这个是global attention： 这个是 local attention 总结 三种不同的attention 种类 attention 分成 hard 和soft 两种模式，简单理解 hard attention 就是从source sentence中国找到一个能够产生 $t^{th}$ 的特定单词，把这个单词设置成1，其他单词设置成0；而soft attention （global attention）是把source sentence中每个单词都给出一个对齐的概率模型，得到一个分布，然后整个模型是处处可分的。而前者不是处处可分的。local attention是相对于 global attention而言的，不是得到一个全局的attention，而是全局的子集，这样可以降低计算attention 的花费，得到一个相关信息的窗口。 nlp 中一般的发展路径: 显示基于 rule-base，然后是基于概率分布的，接着是NN， encoder-decoder框架，可能加上一些小的trick 比如说attention 机制来处理比较长的句子。]]></content>
      <categories>
        <category>NLP</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Seq2Seq Translation (English to Chinese)]]></title>
    <url>%2F2019%2F06%2F28%2Fseq2seq-translation-codes%2F</url>
    <content type="text"><![CDATA[基于 tensorflow 的英文文本的预处理和基于 keras 的中英文本预处理。主要是代码，辅助注释。预处理比较详细的步骤： unicode to ascii normalize string tokenization (choose one-hot or not) padding (find proper length of tokenization) 需要选择一个框架 tensorflow or keras 去实现。大量数据建议选择 tensorflow，小模型使用 keras 就行。 热身Text data typically requires some cleanup before it can be embedded in vector space and fed to a machine learning model. Remove tags. For example, “&lt;i>Hello&lt;/i> &lt;b>World&lt;/b>!” is converted to “Hello World!” Remove repeating whitespace characters (spaces, tabs, line breaks). Convert tabs and line breaks to spaces. Remove stopwords. These include the most commonly occurring words in a language, like “the,” “on,” “is,” etc. NLP libraries like gensim provide a default list of stopwords. Convert all text to lowercase. Perform Porter stemming. Porter stemming reduces inflections like “fishing,” “fished,” and “fisher” to the root “fish.” This makes it easier for an ML model to learn how to glean meaning or intent form a sequence of words. 调用 gensim 框架实现预处理： 12345678from gensim.parsing.preprocessing import preprocess_string, strip_tags, strip_multiple_whitespaces, remove_stopwords, stem_textcustom_filters = [strip_tags, strip_multiple_whitespaces, remove_stopwords, stem_text]# 生成器函数def get_tokenized_questions(X): series = pd.Series(pd.concat([X['question1'], X['question2']]),dtype=str) series.dropna() for question in series: yield preprocess_string(question, custom_filters) All by yourself: 将英文 punctuation characters 和字母以空格隔开。 输入： s = &apos;bla. bla? bla.bla! bla...&apos; 输出： bla . bla ? bla . bla ! bla . . . python2 版本，基于库函数 re 实现。 1234567891011s = 'bla. bla? bla.bla! bla...'import re# 这个符号是可以选择的s = re.sub('([.,!?()])', r' \1 ', s) # use a regular expression to match the punctuation characters you are interested and surround them by spaces,s = re.sub('\s&#123;2,&#125;', ' ', s) # use a second step to collapse multiple spaces anywhere in the document:print(s)# replacing everything with space except (a-z, A-Z, ".", "?", "!", ",")w = re.sub(r"[^a-zA-Z?.!,¿]+", " ", w)w = w.rstrip().strip()w = '&lt;start&gt; ' + w + ' &lt;end&gt;' # 这个是可选的，在首尾加上 'start' or 'end' python3 版本，基于 translate实现。 123# 在python3 中可以使用 translate() 这个方法import stringtext = text.translate(str.maketrans(&#123;key: " &#123;0&#125; ".format(key) for key in string.punctuation&#125;)) tensorflow text preprocessing基于 tensorflow 的文本预处理， 适合大量数据，可以使用batch 输入到模型中去。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374import tensorflow as tfimport numpy as npimport unicodedataimport reraw_data = ( ('What a ridiculous concept!', 'Quel concept ridicule !'), ('Your idea is not entirely crazy.', "Votre idée n'est pas complètement folle."), ("A man's worth lies in what he is.", "La valeur d'un homme réside dans ce qu'il est."), ('What he did is very wrong.', "Ce qu'il a fait est très mal."), ("All three of you need to do that.", "Vous avez besoin de faire cela, tous les trois."), ("Are you giving me another chance?", "Me donnez-vous une autre chance ?"), ("Both Tom and Mary work as models.", "Tom et Mary travaillent tous les deux comme mannequins."), ("Can I have a few minutes, please?", "Puis-je avoir quelques minutes, je vous prie ?"), ("Could you close the door, please?", "Pourriez-vous fermer la porte, s'il vous plaît ?"), ("Did you plant pumpkins this year?", "Cette année, avez-vous planté des citrouilles ?"), ("Do you ever study in the library?", "Est-ce que vous étudiez à la bibliothèque des fois ?"), ("Don't be deceived by appearances.", "Ne vous laissez pas abuser par les apparences."), ("Excuse me. Can you speak English?", "Je vous prie de m'excuser ! Savez-vous parler anglais ?"), ("Few people know the true meaning.", "Peu de gens savent ce que cela veut réellement dire."), ("Germany produced many scientists.", "L'Allemagne a produit beaucoup de scientifiques."), ("Guess whose birthday it is today.", "Devine de qui c'est l'anniversaire, aujourd'hui !"), ("He acted like he owned the place.", "Il s'est comporté comme s'il possédait l'endroit."), ("Honesty will pay in the long run.", "L'honnêteté paye à la longue."), ("How do we know this isn't a trap?", "Comment savez-vous qu'il ne s'agit pas d'un piège ?"), ("I can't believe you're giving up.", "Je n'arrive pas à croire que vous abandonniez."),)# convert the unicode file to ascii, 主要是统一编码方式，然后去除 重音符号def unicode_to_ascii(s): return ''.join( c for c in unicodedata.normalize('NFD', s) # UCD是Unicode字符数据库（Unicode Character DataBase）的缩写。 if unicodedata.category(c) != 'Mn') # 去除 重音符号# 类似于热身中的功能def normalize_string(s): s = unicode_to_ascii(s) s = re.sub(r'([!.?])', r' \1', s) # 如果是这三个符号，那么是需要前面加上一个空格 s = re.sub(r'[^a-zA-Z.!?]+', r' ', s) # 除去不是这些符号的字符 s = re.sub(r'\s+', r' ', s) # 出现多个空格，就去除直到1个 return sraw_data_en, raw_data_fr = list(zip(*raw_data)) # 变量名称前加 *，表示传入的是一个元组，两个星号表示是一个dictionary# 从运行的结果看，由原来的 tuple of tuple 变成了两个string of tuple，并没有list 什么事情raw_data_en, raw_data_fr = list(raw_data_en), list(raw_data_fr) # from tuple to list 这个是转换了raw_data_en = [normalize_string(data) for data in raw_data_en]# 这个是decoder的输入，decoder 是有两个输入的，一个是encoder的输出，一个是 其中一个start destination sentence， 最后是一个 end# 是用来计算 loss的raw_data_fr_in = ['&lt;start&gt; ' + normalize_string(data) for data in raw_data_fr]raw_data_fr_out = [normalize_string(data) + ' &lt;end&gt;' for data in raw_data_fr] # 这种操作比较简洁哈en_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')# 默认是会把 ? . or ! 去掉, 因为我们不想让其 filter 掉 上述三个字符，所有自己进行了处理en_tokenizer.fit_on_texts(raw_data_en)data_en = en_tokenizer.texts_to_sequences(raw_data_en)# 这个padding 是为了之后创建 tf.data.Dataset object 使用的，所以还是比较nice的data_en = tf.keras.preprocessing.sequence.pad_sequences(data_en, padding='post')fr_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')# 都是先使用 fit_on_texts() 然后才使用 texts_to_sequence() ，前者相当于训练，后者是输出的结果，我的理解# A mid-way notice though, we can call fit_on_texts multiple times on different corpora and it will update vocabulary automatically.fr_tokenizer.fit_on_texts(raw_data_fr_in)fr_tokenizer.fit_on_texts(raw_data_fr_out)data_fr_in = fr_tokenizer.texts_to_sequences(raw_data_fr_in)data_fr_in = tf.keras.preprocessing.sequence.pad_sequences(data_fr_in, padding='post')data_fr_out = fr_tokenizer.texts_to_sequences(raw_data_fr_out)data_fr_out = tf.keras.preprocessing.sequence.pad_sequences(data_fr_out, padding='post') in addition： 1234567891011121314# 真正在做实验的时候需要注意的事情：# 一种常见的手段就是 limit the size of the dataset to experiment faster (optimal)# 使用tensorflow 中的dataset 的时候，有意识的 shuffle() 数据集 并且使用batch 的思想dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)# 这个和上面的语句是搭配使用的example_input_batch, example_target_batch = next(iter(dataset))example_input_batch.shape, example_target_batch.shape# 就是在真正的实验的过程中， 网络中的shape (batch_size, embedding_size, )# 超级常用的处理的手段， preprocess_sentence() 是一个函数, apply(lambda )data["eng"] = data.eng.apply(lambda w: preprocess_sentence(w))data["es"] = data.es.apply(lambda w: preprocess_sentence(w)) keras text preprocessing这个版本的代码适用于小的数据量，因为当数据量达到百万的时候，应该使用batch 去训练模型，不应一下子读入到内存中，容易爆内存。比较有特点的 filter 中文的字符使用translate 进行处理。一般从经验上讲是不建议 filter 掉 “？。，” 这三个中文字符的，其他的可以filter 掉，对应英文中的 “? , .” 这三个字符。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788import stringimport refrom numpy import array, argmax, random, takeimport pandas as pdfrom keras.models import Sequentialfrom keras.layers import Dense, LSTM, Embedding, Bidirectional, RepeatVector, TimeDistributedfrom keras.preprocessing.text import Tokenizerfrom keras.callbacks import ModelCheckpointfrom keras.preprocessing.sequence import pad_sequencesfrom keras.models import load_modelfrom keras import optimizersimport matplotlib.pyplot as pltfrom sklearn.model_selection import train_test_splitimport numpy as npdef read_text(filename): # open the file file = open(filename, mode='rt', encoding='utf-8') # read all text text = file.read() file.close() return textdef to_lines(text): sents = text.strip().split('\n') sents = [i.split('\t') for i in sents] return sentsdef to_array(path, debug): data =read_text(path) import gc eng_ch =to_lines(data) del data gc.collect() #import ipdb #ipdb.set_trace() eng_ch =np.asarray(eng_ch) # max memory #eng_ch =np.asarray(eng_ch[:5000000]) if debug: #eng_ch =eng_ch[:4, :]# just for chinese dict test eng_ch =eng_ch[:2000, :] return eng_chdef pre_process(eng_ch): import jieba cn_punctuation = "！？｡ ？。? ＃＄％＆ !（）. ＊＋－／：；＜＝＞＠［＼］＾＿｀｛｜｝～｟｠｢｣､、〃》「」『』【】〔〕〖〗〘〙〚〛〜〝〞〟〰〾 〿–—‘ ’ ‛ “ ” „ ‟ …‧﹏" eng_ch[:, 0] = [s.translate(str.maketrans('', '', string.punctuation)) for s in eng_ch[:, 0]] eng_ch[:, 1] = [s.translate(str.maketrans('', '', cn_punctuation)) for s in eng_ch[:, 1]] for i in range(len(eng_ch)): eng_ch[i, 0] =eng_ch[i, 0].lower() for i in range(len(eng_ch)): seg_list =jieba.cut(eng_ch[i, 1]) eng_ch[i, 1] =' '.join(seg_list) return eng_chdef sentence_length(eng_ch): eng_l =[] ch_l =[] # 这里需要看一下 english的数据是否前后有 空格 for i in eng_ch[:, 0]: eng_l.append(len(i.split())) for i in eng_ch[:,1]: ch_l.append(len(i)) length_df =pd.DataFrame(&#123;'eng': eng_l, 'ch':ch_l&#125;) length_df.hist(bins =50) plt.savefig('data-dist-cn.png')def tokenization(lines): tokenizer = Tokenizer() tokenizer.fit_on_texts(lines) return tokenizerdef encode_sequences(tokenizer, length, lines): seq =tokenizer.texts_to_sequences(lines) seq = pad_sequences(seq, maxlen=length, padding='post') return seq]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>seq2seq</tag>
        <tag>translation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Bairong internship]]></title>
    <url>%2F2019%2F06%2F27%2Fbairong%2F</url>
    <content type="text"><![CDATA[总结在百融的实习经历。侧重学到的东西。 如果需要打上tag，那么就是“机器学习+ 金融+ 风控”。 金融方面术语整个风控是分成三个阶段：要不要放贷，额度策略和定价策略。 风控的重要性：因为大部分的平台都是靠高利率来覆盖高风险，自己本身没有做好足够的风险控制。但从今往后，哪个平台的风控能够做的好，就能在长周期严监管的前提下生存下去，而不能仅仅依靠高利率了。 额度策略 目标：金额坏账率小于账户坏账率。如何实现上述的目标呢？总的策略是对于用户信用评分进行排序，给高风险的人低额度，反之成立。 模型从理论上讲，是存在极端最优解的，但是在实际中往往不是这样做的。比较常见的模型： 线性模型：理论上分得越细，效果是越好。 比较光滑的指数模型 幂函数 sigmoid 函数： sigmoid 函数形状是由两个参数进行控制的。 可以使用sigmoid 函数近似的去不断的接近最优解。最优模型的形状一部分是min，一部分是 max，中间的斜率比较大。（和sigmoid 的形状是比较像） 上面使用的是个人的信用评分，得到是一个基础额度。最后的额度还需要考虑个人资质等一系列的因素。 定价策略 这里的定价是利率的定价，还款利率的高低。主要要提供差异化定价策略。在这个讨论中提到一个专业的名词，贴水。你得到的利率是包含你的风险。基本的原则，风险越高，利率越高。 如何收益的增加？回去借贷高风险，对于利率不太敏感，适当增加的利率或者贴水。对于资质比较好的，对于利率比较敏感，那么维持较低的利率。在高风险人群得到收益。 评分模型 分成贷前、贷中和贷后三个阶段。 KS 曲线对于预测能力指标：ROC/ AUC, K-S指标 和GINI系数。 KS(Kolmogorov-Smirnov)：KS用于模型风险区分能力进行评估， 指标衡量的是好坏样本累计部分之间的差值。好坏样本累计差异越大，KS指标越大，那么模型的风险区分能力越强。数学表达式为： $K-S = max( TPR- FPR)$。 KS值的取值范围是 $[0, 1]$。 下面是图解。 计算步骤 计算每个评分区间的好坏账户数。 计算每个评分区间的累计好账户数占总好账户数比率(good%)和累计坏账户数占总坏账户数比率(bad%)。 计算每个评分区间累计坏账户占比与累计好账户占比差的绝对值（累计good%-累计bad%），然后对这些绝对值取最大值即得此评分卡的K-S值。 在完成一个模型后，将测试模型的样本平均分成10组，以好样本占比降序从左到右进行排列，其中第一组的好样本占比最大，坏样本占比最小。这些组别的好坏样本占比进行累加后得到每一组对应的累计的占比。好坏样本的累计占比随着样本的累计而变化（图中Good/Bad两条曲线），而两者差异最大时就是我们要求的K-S值（图中比较长的直线箭头的那个位置）。 这两条曲线之间的差值，就是K-S曲线。如图1所示，给定一个通过率20%（拒绝率80%），则该模型可以挑出来60%的好人，同时漏进来8%的坏人（92%的坏人都被拒绝掉了）。那么K-S曲线在这个通过率上的值，就是60%-8%=0.52。(好好看，这个坐标的对应关系) K-S曲线主要是验证模型的区分能力，通常是在模型预测全体样本的信用评分后，将全体样本按违约与非违约分为两部分，然后用K-S统计量来检验这两组样本信用评分的分布是否有显著差异。选择最大间隔对于的横轴概率阈值为最佳概率阈值。 KS值 含义 $&gt; 0.3$ 模型预测性较好 $0.2 \sim 0.3 $ 模型可用 $0 \sim 0.2$ 模型预测能力较差 $&lt; 0$ 模型错误 K-S值一般是很难达到0.6的，在0.2~0.6之间都不错。一般如果是如果负样本对业务影响极大，那么区分度肯定就很重要，此时K-S比AUC更合适用作模型评估，如果没什么特别的影响，那么用AUC就很好了。 K-S 值越大，表明分类器对正 负类的区分能力越好。并非所有的情况KS都是越高越好的，尤其在征信模型中。征信模型中，最期望得到的信用分数分布是正态分布，对于正负样本分别而言，也都期望是呈正态分布的样子。如果KS值过大，一般超过0.9，就可以认为正负样本分得过开了，不太可能是正态分布的，反而是比较极端化的分布状态（U字形，两边多，中间少），这样的分数就很不好，基本可以认为不可用。但如果模型的目的就是完美区分正负样本，那么KS值越大就表明分隔能力越突出。另外，KS值所代表的仅仅是模型的分隔能力，并不代表分隔的样本是准确的。换句话说，正负样本完全分错，但KS值可以依旧很高。(在实习的过程中，对于正太分布的执着真是非常执着的) KS曲线和ROC 曲线的区别和联系： K-S曲线其实数据来源和本质和ROC曲线是一致的，只是ROC曲线是把真正率和假正率当作横纵轴，而K-S曲线是把真正率和假正率都当作是纵轴，横轴则由选定的阈值来充当。不同的是，ROC曲线用FPR作为横轴，TPR作为纵轴，采用描点法绘制，图中总共是一条线；而KS曲线的横轴则是不同的概率判断阈值，图中一共有两条线，分别代表了FPR值和TPR值。（具体可以参考上文的图像讲解） 要弄明白ks值和auc值的关系首先要弄懂roc曲线和ks曲线是怎么画出来的。其实从某个角度上来讲ROC曲线和KS曲线是一回事，只是横纵坐标的取法不同而已。拿逻辑回归举例，模型训练完成之后每个样本都会得到一个类概率值（注意是类似的类），把样本按这个类概率值排序后分成10等份，每一份单独计算它的真正率和假正率，然后计算累计概率值，用真正率和假正率的累计做为坐标画出来的就是ROC曲线，用10等分做为横坐标，用真正率和假正率的累计值分别做为纵坐标就得到两个曲线，这就是KS曲线。AUC值就是ROC曲线下放的面积值，而ks值就是ks曲线中两条曲线之间的最大间隔距离。由于ks值能找出模型中差异最大的一个分段，因此适合用于cut_off，像评分卡这种就很适合用ks值来评估。但是ks值只能反映出哪个分段是区分最大的，而不能总体反映出所有分段的效果，因果AUC值更能胜任。 好的信用风控模型一般从准确性、稳定性和可解释性来评估模型。 参考资料：深入理解KS GINI系数 还记得经济学中那个著名的基尼系数吗？下图应该可以让你回忆起来。将一个国家所有的人口按最贫穷到最富有进行排列，随着人数的累计，这些人口所拥有的财富的比例也逐渐增加到100%，按这个方法得到图中的曲线，称为洛伦兹曲线。基尼系数就是图中A/B的比例。可以看到，假如这个国家最富有的那群人占据了越多的财富，贫富差距越大，那么洛伦茨曲线就会越弯曲，基尼系数就越大。 同样的，假设我们把100个人的信用评分按照从高到低进行排序，以横轴为累计人数比例，纵轴作为累计坏样本比例，随着累计人数比例的上升，累计坏样本的比例也在上升。如果这个评分的区分能力比较好，那么越大比例的坏样本会集中在越低的分数区间，整个图像形成一个凹下去的形状。所以洛伦兹曲线的弧度越大，基尼系数越大，这个模型区分好坏样本的能力就越强。 风控模型：ks， 基尼系数 -&gt;质量psi -&gt;稳定性PSI即Population stability index： 模型稳定性 其他经济学概念股市和债市的区别 股市和债市的关系 当股市开始走牛时，债市牛市进入最后阶段，当大部分人都知道股市牛市来临时，债市开始走熊。 当股市从牛市转为熊市时，债市跌最后一波，当大部分人都知道股市熊市来临时，债市开始走牛。 这是因为经济周期决定的。经济进入复苏期后，社会各行业平均利润率逐步升高，而贷款利率其实就是各行业平均利润率，所以银行利率停止下降，但此时银行利率还在最低点，社会流动性也充足，债市靠惯性进入牛市的最后一波。因为经济复苏，股市开始进入牛市。股市走牛一段时间后，因为资本的逐利性，大量投资者开始从债市撤退；债市经过长期上涨，其收益率也远远低于股票的分红率；利率的开始上涨导致债市基本面逆转。债市开始走熊，而股市继续走牛。随着经济由复苏进入繁荣，社会各行业平均利润率到达高峰，股市进入牛市最后阶段，债市继续走熊。当经济由繁荣进入危机期，为了应对危机，国家还在不断收紧流动性，社会各行业平均利润率也开始下降，经济滞涨，股市和债市同时下跌。当经济由危机进入萧条期，国家开始逐步放松货币，但社会各行业平均利润率仍然在下滑，贷款需求下降，大量宽松出来的银行资金首先进入债市；经过漫长下跌，债市的收益率在历史高位，具有投资价值；当大部分人都知道熊市来后，资金开始从股市向债市转移，导致债市开始走牛。去年美国国债居然上涨了18%，5年期国债年收益率只有0.65%，主要就是因为美国经济去年在萧条期，今年美国经济开始复苏，股票就成为最好的投资标的。 股市和债市的定义： 股票市场是已经发行的股票转让、买卖和流通的场所，包括交易所市场和场外交易市场两大类别。由于它是建立在发行市场基础上的，因此又称作二级市场。股票市场的结构和交易活动比发行市场（一级市场）更为复杂，其作用和影响力也更大。股票市场的前身起源于1602年荷兰人在阿姆斯特河大桥上进行荷属东印度公司股票的买卖，而正规的股票市场最早出现在美国。股票市场是投机者和投资者双双活跃的地方，是一个国家或地区经济和金融活动的寒暑表，股票市场的不良现象例如无货沽空等等，可以导致股灾等各种危害的产生。股票市场唯一不变的就是：时时刻刻都是变化的。中国有上交所和深交所两个交易市场。债券市场是发行和买卖债券的场所，是（金融市场）一个重要组成部分。债券市场是一国金融体系中不可或缺的部分。 一个统一、成熟的债券市场可以为全社会的投资者和筹资者提供低风险的投融资工具；债券的收益率曲线是社会经济中一切金融商品收益水平的基准，因此债券市场也是传导中央银行货币政策的重要载体。可以说，统一、成熟的债券市场构成了一个国家金融市场的基础。 多投: 投入了多家进行借贷之类的 对于第三方，就一个大数据公司，还是要体现数据优势，数据质量，匹配程度。贷前是风险控制， 贷中是价值挖掘，发现优质客户。维度：三方数据、工商基本信息和自有数据。 白户和纯白户：（信用记录是不全的）白户是指有申请记录，但最后申请失败没有下卡，时间半年以上。 纯白户是指，没有申请过信用卡与贷款，个人信用空白的叫纯白户。 纯白户和白户都是指没有银行信用记录的客户群体，此类人群在办理贷款时，难免会卡壳遇阻，究其原因，为了把控风险，部分银行只将受众锁定在了有信用记录且良好的人群身上。 Excel 的使用 vloopup 函数用于匹配查找。 四个参数：(查找的关键字，在哪个区域查找，返回往右边数第几列，精确查找/ 模糊查找)第一个参数就是关键字，第二个参数是关键字所在列为最左侧的列的区域全选的时候，选中标题行，ctrl shift + down +F4这样就选中了所有下面的数据相对于关键字第三参数：以关键词所在的列为第一列，然后是要的第几列的数据第四个参数使用 0 或者 false，表示精确查找 透视表/透视图 进行数据的筛选和统计 在excel 中可以使用 “=left” 进行字符串的切分 会议记录书写模板 双方参与人员：时间：地点 会议的主要内容： … … …. 后续的计划安排 也是很重要，一定要写好。 机器学习关于规则还是模型？稳定的特征是在模型中，不太稳定的是在规则中。 常常使用的工具：xgblightgbmspss 商业软件 数据分析软件 个人信息三要素：姓名、手机号和身份证号。 只要将这些信息进行加密or 缺省，那么一般是不会造成数据的缺失的。 风控的流程图：]]></content>
      <categories>
        <category>others</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Similarity Measures]]></title>
    <url>%2F2019%2F06%2F21%2Fsimilarity-measures%2F</url>
    <content type="text"><![CDATA[The similarity measure is the measure of how much alike two data objects are. Similarity measure in a data mining context is a distance with dimensions representing features of the objects. If this distance is small, it will be the high degree of similarity where large distance will be the low degree of similarity. The similarity is subjective and is highly dependent on the domain and application. For example, two fruits are similar because of color or size or taste. Care should be taken when calculating distance across dimensions/features that are unrelated. The relative values of each element must be normalized, or one feature could end up dominating the distance calculation. Similarity are measured in the range 0 to 1 [0,1]. Euclidean distanceEuclidean distance is the most common use of distance. In most cases when people said about distance, they will refer to Euclidean distance. Euclidean distance is also known as simply distance. When data is dense or continuous, this is the best proximity measure. Applications: Where data is continuous or numerical . Also knows as L2 Norm famously. In an n dimensional space between two vectors x and y the formula is simply the square root of the sum of the square distance: $$d \left( \left[ x _ { 1 } , x _ { 2 } , \ldots , x _ { n } \right] , \left[ y _ { 1 } , y _ { 2 } , \ldots , y _ { n } \right] \right) = \sqrt { \sum _ { i = 1 } ^ { n } \left( x _ { i } - y _ { i } \right) ^ { 2 } }$$ 123456from math import * def euclidean_distance(x,y): return sqrt(sum(pow(a-b,2) for a, b in zip(x, y))) print euclidean_distance([0,3,4,5],[7,6,3,-1]) 适用范围：通用型，在连续稠密的向量计算中相比更好。使用这个 matrix 的时候，最好是将数据规范化，一种原因在于 这个是乘方的运算，规范化之后误差是不至于太大。 Manhattan distanceThis Manhattan distance metric is also known as Manhattan length, rectilinear distance, L1 distance or L1 norm, city block distance, Minkowski’s L1 distance, taxi-cab metric, or city block distance. Applications: can be used if the dimensions are continuous or numeric. This distance measure is very similar to Euclidean but it is a sum of the absolute difference of every dimension rather than the sum of squares. $$d = \sum _ { i = 1 } ^ { n } \left| x _ { i } - y _ { i } \right|$$ 123456from math import *def manhattan_distance(x,y): return sum(abs(a-b) for a,b in zip(x,y)) print manhattan_distance([10,20,10],[10,20,20]) 适用范围：可以处理异常值、具有特征选择的功能，可以有多个解（而 Euclidean distance 只有一个最优解） Minkowski distanceMinkowski distance is the generalized distance metric. 当p =1 和2 时，恰好是 Euclidean distance 和Manhattan distance. $$\left( \sum _ { i = 1 } ^ { n } \left| x _ { i } - y _ { i } \right| ^ { p } \right) ^ { 1 / p }$$ 1234567891011121314 from math import *from decimal import Decimal def nth_root(value, n_root): root_value = 1/float(n_root) return round (Decimal(value) ** Decimal(root_value),3) def minkowski_distance(x,y,p_value): return nth_root(sum(pow(abs(a-b),p_value) for a,b in zip(x, y)),p_value) print minkowski_distance([0,3,4,5],[7,6,3,-1],3) Cosine similarityCosine similarity is particularly used in positive space, where the outcome is neatly bounded in [0,1]. One of the reasons for the popularity of cosine similarity is that it is very efficient to evaluate, especially for sparse vectors. Applications: Used in identifying document similarity, product recommendations, Information retrieval and works efficiently with high-dimensional sparse data. It is an angle between two data points in the vector space. Given a vector A and B, the cosine distance is the dot product of x and y divided by Euclidean distance. $$\text { similarity } = \cos ( \theta ) = \frac { \mathbf { A } \cdot \mathbf { B } } { | \mathbf { A } | | \mathbf { B } | } = \frac { \sum _ { i = 1 } ^ { n } A _ { i } B _ { i } } { \sqrt { \sum _ { i = 1 } ^ { n } A _ { i } ^ { 2 } } \sqrt { \sum _ { i = 1 } ^ { n } B _ { i } ^ { 2 } } }$$ 12345678910111213from math import * def square_rooted(x): return round(sqrt(sum([a*a for a in x])),3) # round(num, ndigits) 在python2 中是四舍五入 def cosine_similarity(x,y): numerator = sum(a*b for a,b in zip(x,y)) denominator = square_rooted(x)*square_rooted(y) return round(numerator/float(denominator),3) print cosine_similarity([3, 45, 7, 2], [2, 54, 13, 15]) Cosine similarity vs Euclidean distance首先从公式上说， cosine similarity 考虑的是角度 (angle)而不是 magnitude，可以排除文章长度的干扰。 Mathematically, it measures the cosine of the angle between two vectors projected in a multi-dimensional space. The cosine similarity is advantageous because even if the two similar documents are far apart by the Euclidean distance (due to the size of the document). 如果涉及到（使用 tf or tf-idf）生成了向量表示，使用 cosine similarity 比 euclidean 更好。 如果使用 word2vec 生成的向量，那么 euclidean 是不是更好的选择。Cosine is mostly used on very sparse, discrete domains such as text. Here, most dimensions are 0 and do not matter at all. 如果你想要 magnitude，那么使用 ED。Euclidean is commonly used on dense, continuous variables. There every dimension matters。 这里有一个重要的观点， Cosine is essentially the same as Euclidean on normalized data. 在很高的维度，两者都是不行的，这就是 Curse of Dimensionality. 适用范围： sparse, discrete 这个特点决定了在 nlp 中使用是比较广泛的； 而Euclidean distance 在图像中（稠密连续值）中使用比较广泛。 Jaccard similarityJaccard Distance measures how close two sets are. It is simply a ratio of the intersection of the sets to the Union. Can be used when the datatypes are categorical . Example: Products purchased/viewed by customers. Typically used in Product recommendation, Clustering customers based on purchase/engagement patterns. Please note Jaccard distance is a dissimilarity metric and Jaccard coefficient, J(A,B) is a similarity metric. $$d _ { J } ( A , B ) = 1 - J ( A , B ) = \frac { | A \cup B | - | A \cap B | } { | A \cup B | }$$ 123456789from math import * def jaccard_similarity(x,y): intersection_cardinality = len(set.intersection(*[set(x), set(y)])) # 参数前一个 *表示传入的是一个元祖 tuple union_cardinality = len(set.union(*[set(x), set(y)])) return intersection_cardinality/float(union_cardinality) print jaccard_similarity([0,1,2,5,6],[0,2,3,5,7,9]) 适用范围：set() 中计算。 Edit Distance Edit distance is used when the comparing strings. Ideal use cases would be auto spell check, meta data correction etc. The distance between two strings are smaller if the number of corrections ( insertions or deletions ) needed to perfectly match are smaller. 常见的dp 解法。 123456789101112131415161718192021222324class Solution(object): def minDistance(self, word1, word2): """ :type word1: str :type word2: str :rtype: int """ m = len(word1) n = len(word2) table = [[0] * (n + 1) for _ in range(m + 1)] for i in range(m + 1): table[i][0] = i for j in range(n + 1): table[0][j] = j for i in range(1, m + 1): for j in range(1, n + 1): if word1[i - 1] == word2[j - 1]: table[i][j] = table[i - 1][j - 1] else: table[i][j] = 1 + min(table[i - 1][j], table[i][j - 1], table[i - 1][j - 1]) return table[-1][-1] 适用范围： string 中的距离。 Kullback–Leibler divergenceKullback–Leibler divergence (KL 散度) (also called relative entropy 相对熵)， 是衡量两个分布之间的差异性指标。 可以写成这样： $$D_{\mathrm{KL}}(P | Q)=-\sum_{x \in \mathcal{X}} P(x) \log \left(\frac{Q(x)}{P(x)}\right)$$ 也可以写成这样： $$D_{\mathrm{KL}}(P | Q)=\sum_{x \in \mathcal{X}} P(x) \log \left(\frac{P(x)}{Q(x)}\right)$$ KL 散度是熵和交叉熵之差。推导如下：$$\begin{split}D_{K L}(p | q) &amp;= H(p) -H(p, q) \\ &amp; =-\int p(x) \log q(x)-\left(-\int p(x) \log p(x)\right) \\ &amp;=-\int p(x) \log \frac{q(x)}{p(x)} d x\end{split}$$ 特点： 不具有对称性 当两个分布不相交时候，距离趋向无穷，无法反应距离关系]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>similarity-measures</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ResNet and Inception V3 Understanding]]></title>
    <url>%2F2019%2F06%2F21%2Fresnet-understanding%2F</url>
    <content type="text"><![CDATA[本文介绍 ResNet的背景，想要解决的问题，基本思路和框架 和对于其的一种解读方式。另外，和 Resnet 思路相反的是 Inception 系列。 背景网络的深度是容易出现梯度爆炸和梯度消失，造成网络的不收敛。一些方法已经在很大程度上可以缓解这个问题，比如使用 ReLU激活函数、 良好的权值初始化方法 、还有 intermediate normalization layers(即网络中间的batch normalization)。并且对于网络过程中的过拟合问题，也提出了一些办法如，使用 regularization、权值衰减和dropout方法。 但解决了深度网络收敛问题之后，又出现了另外一个问题。 残差网络要解决的问题一般来说在没有过拟合的情况下，可以逐步增加网络的深度。但在实验中发现了这样的问题。网络退化： 网络越深，训练误差越大。（accuracy开始饱和，原文中这样说的）这种退化并不是由于过拟合造成的，并且在适当深度模型中增加更多的层会导致更多的训练误差 基本思路和结构作者基于增加层如果为恒等映射那么更深层网络不应该比浅层网络产生更高错误率的思想 如图所示左边的是传统的plain networks的结构，右边的是修改为ResNet的结构。改变前目标： 训练 F(x) 逼近 H(x)改变后目标：训练 F(x) 逼近 H(x) -x 即增加一个identity mapping（恒等映射），将原始所需要学的函数H(x)转换成F(x)+x，而作者认为这两种表达的效果相同，但是优化的难度却并不相同，作者假设F(x)的优化 会比H(x)简单的多。这一想法也是源于图像处理中的残差向量编码，通过一个reformulation，将一个问题分解成多个尺度直接的残差问题，能够很好的起到优化训练的效果 在论文中尝试了 skip 2层或者 3层 一种解读方式残差网络单元其中可以分解成右图的形式，从图中可以看出，残差网络其实是由多种路径组合的一个网络，直白了说，残差网络其实是很多并行子网络的组合，整个残差网络其实相当于一个多人投票系统（Ensembling） 从这可以看出其实ResNet是由大多数中度网络和一小部分浅度网络和深度网络组成的， 说明虽然表面上ResNet网络很深，但是其实起实际作用的网络层数并没有很深我们可以看出大多数的梯度其实都集中在中间的路径上，论文里称为effective path。 (网络越深，也是容易出现梯度消失或者梯度爆炸，这个是没有问题的)ResNet其实就是一个多人投票系统。 现在深度网络基本上分成两个方向，一个像 resnet 向着”深度“发展，一个向着”宽度“的inception network Inception V3如果 ResNet 是为了更深，那么 Inception 家族就是为了更宽。第一个见解与对层的操作有关。在传统的卷积网络中，每一层都会从之前的层提取信息，以便将输入数据转换成更有用的表征。 见解 1：为什么不让模型选择？ 这种模型架构的信息密度更大了，这就带来了一个突出的问题：计算成本大大增加。不仅大型（比如 5×5）卷积过滤器的固有计算成本高，并排堆叠多个不同的过滤器更会极大增加每一层的特征映射的数量。而这种计算成本增长就成为了我们模型的致命瓶颈。 这就涉及到了见解 2：使用 1×1 卷积来执行降维。为了解决上述计算瓶颈，Inception 的作者使用了 1×1 卷积来「过滤」输出的深度。一个 1×1 卷积一次仅查看一个值，但在多个通道上，它可以提取空间信息并将其压缩到更低的维度。比如，使用 20 个 1×1 过滤器，一个大小为 64×64×100（具有 100 个特征映射）的输入可以被压缩到 64×64×20。通过减少输入映射的数量，Inception 可以将不同的层变换并行地堆叠到一起，从而得到既深又宽（很多并行操作）的网络。 Inception Net v3 incorporated all of the above upgrades stated for Inception v2, and in addition used the following: RMSProp Optimizer. Factorized 7x7 convolutions. BatchNorm in the Auxillary Classifiers. Label Smoothing (A type of regularizing component added to the loss formula that prevents the network from becoming too confident about a class. Prevents over fitting).]]></content>
      <categories>
        <category>Deep learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[tensorflow learning]]></title>
    <url>%2F2019%2F06%2F17%2Ftf-learning%2F</url>
    <content type="text"><![CDATA[linear_regression 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071import numpy as npimport matplotlib.pyplot as pltimport tensorflow as tfimport xlrd # 即xlrd是读excel，xlwt是写excel的库。import osfrom sklearn.utils import check_random_staten =50XX =np.arange(n)rs =check_random_state(0)YY =rs.randint(-10, 10, size=(n, ))+ 2.0* XXdata =np.stack([XX,YY], axis= 1) #沿着的纵轴num_epochs =50# 变量的初始化W =tf.Variable(0.0, name="weights")b =tf.Variable(0.0, name ="bias")# 定义数据 placeholddef inputs(): X =tf.placeholder(tf.float32, name ="X") Y =tf.placeholder(tf.float32, name ="Y") return X,Ydef inference(X): return X* W+bdef loss(X, Y): Y_predicted =inference(X) # square 是平方，平方差公式 return tf.reduce_sum(tf.squared_difference(Y, Y_predicted))(2.0* data.shape[0])# train 的过程就是 minimize loss 的过程def train(loss): learning_rate =0.0001 return tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)with tf.Session() as sess: sess.run(tf.global_variables_initializer()) X, Y =inputs() train_loss =loss(X, Y) train_op =train(train_loss) for epoch_num in range(num_epochs): loss_value, _ =sess.run([train_loss, train_op], feed_dict=&#123;X :data[:, 0], Y:data[:, 1]&#125;) print("epoch %d, loss =%f"%(epoch_num+1, loss_value)) # 已经 train之后，那么这个 run weights 和 bias 的意义 wcoeff, bias =sess.run([ W, b])# show resultsInput_values =data[:, 0]Labels =data[:, 1]Prediction_values =data[:, 0] *wcoeff +biasplt.plot(Input_values, Labels, 'ro', label ="main")plt.plot(Input_values, Prediction_values, label ="predicted")plt.legend()plt.show()plt.close() tensorflow 中图的概念 Tensorflow有图的概念，Operations会被添加到图中，作为图的节点。在添加某个Operation的时候，不会立即执行该Operation。Tensorflow会等所有Operation添加完毕，然后优化该计算图，以便决定如何执行计算。 而Tensor则是代码中的变量和常量。所有变量都需要在开始执行图计算之前进行初始化，通过调用tf.initialize_all_variables().run()来初始化所有变量。 使用Tensorflow，一般需要三个步骤： 创建Tensor； 添加Operations（Operations输入Tensor，然后输出另一个Tensor）； 执行计算（也就是运行一个可计算的图）。 Tensorflow的图必须在一个会话(Session)中来计算。Session提供了Operation执行和Tensor求值的环境 tensorflow 中name scope 和 variable scope 的区别 TensorFlow提供了通过变量名称（name）来创建或者获取一个变量的机制， 是通过加上前缀来进行变量的管理。 variable scope为了实现tensorflow中的变量共享机制：即为了使得在代码的任何部分可以使用某一个已经创建的变量，TF引入了变量共享机制，使得可以轻松的共享变量，而不用传一个变量的引用。 TensorFlow提供了两种创建变量的方法，一种是tf.Variable()，另一种是tf.get_variable()。 tf.get_variable()除了可以创建变量外，还能获取变量。 给出一个案例进行学习。 1234567891011121314151617import tensorflow as tfwith tf.name_scope('name_scope_test'): v1 = tf.get_variable('v', shape=[1], initializer=tf.constant_initializer(1.0)) v2 = tf.Variable(tf.constant(1.0, shape=[1]), name='v') v3 = tf.Variable(tf.constant(1.0, shape=[1]), name='v')with tf.Session() as sess: init_op = tf.global_variables_initializer() sess.run(init_op) print('the name of v1:', v1.name) print('the name of v2:', v2.name) print('the name of v3:', v3.name)#输出为#the name of v1: v:0#the name of v2: name_scope_test/v:0#the name of v3: name_scope_test/v_1:0 tf.ConfigProto() 创建session() 的时候，用来对 session() 进行参数配置。简单的举个例子. 123config = tf.ConfigProto(allow_soft_placement=True, allow_soft_placement=True)config.gpu_options.per_process_gpu_memory_fraction = 0.4 #占用40%显存sess = tf.Session(config=config) 常见的参数： 记录设备指派情况 : tf.ConfigProto(log_device_placement=True) 自动选择运行设备 ： tf.ConfigProto(allow_soft_placement=True) 限制GPU资源使用： 动态的申请 123config = tf.ConfigProto()config.gpu_options.allow_growth = Truesession = tf.Session(config=config) 限制GPU使用率 123config = tf.ConfigProto()config.gpu_options.per_process_gpu_memory_fraction = 0.4 #占用40%显存session = tf.Session(config=config) 或者 123gpu_options=tf.GPUOptions(per_process_gpu_memory_fraction=0.4)config=tf.ConfigProto(gpu_options=gpu_options)session = tf.Session(config=config) 设置使用哪块GPU 方法一： 在程序中设置 123os.environ['CUDA_VISIBLE_DEVICES'] = '0' #使用 GPU 0os.environ['CUDA_VISIBLE_DEVICES'] = '0,1' # 使用 GPU 0，1 方法二： 在 shell 脚本中设置 1CUDA_VISIBLE_DEVICES=0,1 python yourcode.py 保存和恢复 保存：创建 Saver（使用 tf.train.Saver()）来管理模型中的所有变量。 恢复: tf.train.Saver 对象不仅将变量保存到检查点文件中，还将恢复变量。请注意，当您恢复变量时，您不必事先将其初始化。 变量的种类和使用范围 TensorFlow 支持占位符placeholder。占位符并没有初始值，它只会分配必要的内存。在会话中，占位符可以使用 feed_dict 馈送数据。feed_dict是一个字典，在字典中需要给出每一个用到的占位符的取值。 那么，什么时候该用tf.placeholder，什么时候该使用tf.Variable之类直接定义参数呢？答案是，tf.Variable适合一些需要初始化或被训练而变化的权重或参数，而tf.placeholder适合通常不会改变的被训练的数据集。 源码也可以参考这里 logistic regression 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150import numpys as npimport matplotlib.pyplot as pltimport tensorflow as tfimport tempfileimport urllibimport pandas as pdimport osfrom tensorflow.examples.tutorials.mnist import input_data# tempfile 这个模块主要是用来创建临时文件和目录，用完后会自动删除，省的你自己去创建一个文件、使用这个文件、再删除这个过程了。max_num_checkpoint =10num_classes =2batch_size =512num_epochs =10# learning rateinitial_learning_rate =0.001learing_rate_decay_factor =0.95num_epochs_per_decay =1# statusis_training =Falsefine_tuning =Falseonline_test =Trueallow_soft_placement =Truelog_device_placement =False# Download and get MNIST dataset(available in tensorflow.contrib.learn.python.learn.datasets.mnist)# It checks and download MNIST if it's not already downloaded then extract it.# The 'reshape' is True by default to extract feature vectors but we set it to false to we get the original images.mnist = input_data.read_data_sets("MNIST_data/", reshape=True, one_hot=False)# data processing# 这种键值对还能写成这种形式，我草data =&#123;&#125;data["train/image"] =mnist.train.imagesdata["train/label"] =mnist.train.labelsdata["test/image"] =mnist.test.imagesdata["test/label"] =mnist.test.labelsdef extract_samples_Fn(data): index_list =[] for sample_index in range(data.shape[0]): label =data[sample_index] if label ==1 or label ==0: index_list.append(sample_index) return index_listindex_list_train =extract_samples_Fn(data["train/label"])index_list_test =extract_samples_Fn(data["test/label"])data["train/image"] =mnist.train.images[index_list_train]data["train/label"] =mnist.train.labels[index_list_train]data["test/image"] =mnist.test.images[index_list_test]data["test/label"] =mnist.test.labels[index_list_test]dimenionality_train =data["train/image"].shapenum_train_samples =dimenionality_train[0]num_features =dimenionality_train[1]graph =tf.Graph()with graph.as_default(): global_step =tf.Variable(0, name= ="global_step", trainable =False) # decay steps 是和 训练样本数量， batch size相关的 decay_steps = int(num_train_samples /batch_size * num_epochs_per_decay) learing_rate =tf.train.exponential_decay(initial_learning_rate, global_step, decay_steps, learing_rate_decay_factor, staircase=True, name="exponential_decay_learning_rate") # define placeholder image_place =tf.placeholder(tf.float32, shape=([None, num_features]), name= "image") label_place =tf.placeholder(tf.int32, shape=([None, ]), name = "gt") label_one_hot =tf.one_hot(label_place, depth= num_classes, axis= -1) # 这个是按照最后一个轴进行操作 dropout_param =tf.placeholder(tf.float32) # 我的理解这个不是一种常量吗， 为什么还要使用 placeholder # fully connnected logits =tf.contrib.layers.fully_connected(inputs =image_place, num_outputs =num_classes, scope ="fc") # define loss, with tf.name_scope("loss"): loss_tensor =tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits =logits, labels= label_one_hot)) # argmax(, 0) 返回行中的最大值的索引， 返回列中的最大索引 # tf.cast(predict_correct, tf.float32) 类型转换 prediction_correct =tf.equal(tf.argmax(logits, 1), tf.argmax(label_one_hot, 1)) accuracy =tf.reduce_mean(tf.cast(prediction_correct, tf.float32)) # optimizer, 上面是手动decay的，现在有了一个自动调整 learning rate 的工具 optimizer =tf.train.AdamOptimizer(learning_rate= learing_rate) with tf.name_scope("train_op"): # 加上了一个前缀 gradients_and_variables =optimizer.compute_gradients(loss_tensor) train_op =optimizer.apply_gradients(gradients_and_variables, global_step= global_step) # run the session session_conf =tf.ConfigProto( allow_soft_placement =allow_soft_placement, log_device_placement =log_device_placement, ) sess =tf.Session(graph= graph, config= session_conf) with sess.as_default(): saver =tf.train.Saver() sess.run(tf.global_variables_initializer()) checkpoint_prefix ="model" if fine_tuning: saver.restore(sess, os.path.join(checkpoint_path, checkpoint_prefix)) print(" Model restored for fine-tuning") test_accuracy =0 for epoch in range(num_epochs): # batch 的次数 是和 data 和 batch_size 相关的 total_batch_training =int(data["data/image"].shape[0] /batch_size) for batch_num in range(total_batch_training): start_idx =batch_num *batch_size end_idx =(batch_num +1) * batch_size train_batch_data, train_batch_label =data['train/image'][start_idx: end_idx], data["train/label"][start_idx: end_idx] batch_loss, _, training_step = sess.run([ loss_tensor, train_op, global_step], feed_dict=&#123;image_place: train_batch_data, label_place: train_batch_label, dropout_param: 0.5&#125;) # 使用占位符进行的输出 print("Epoch "+ str(epoch +1) +", Training loss =" + "&#123;:.5f&#125;".format(batch_loss)) # 每进行一个batch 那么是需要重新计算一下 acc的 test_accuracy = 100* sess.run(accuracy, feed_dict=&#123; image_place: data["test/image"], label_place: data["test/label"], dropout_param :1.&#125;) # 在进行test 的时候是不需要进行 dropout的 print(" Final Test Accuracy is %% %.2f" %test_accuracy) RNN 在 tensorflow 中的使用 学习单步执行的RNN ： RNNCell 每个RNNCell都有一个call方法，使用方式是：(output, next_state) = call(input, state)。借助图片来说可能更容易理解。假设我们有一个初始状态h0，还有输入x1，调用call(x1, h0)后就可以得到(output1, h1)： 再调用一次call(x2, h1)就可以得到(output2, h2)： 也就是说，每调用一次RNNCell的call方法，就相当于在时间上“推进了一步”，这就是RNNCell的基本功能。 除了call方法外，对于RNNCell，还有两个类属性比较重要： state_size output_size 前者是隐层的大小，后者是输出的大小。比如我们通常是将一个batch送入模型计算，设输入数据的形状为(batch_size, input_size)，那么计算时得到的隐层状态就是(batch_size, state_size)，输出就是(batch_size, output_size)。 一次执行多步： tf.nn.dynamic_rnn tf.nn.dynamic_rnn函数，使用该函数就相当于调用了n次call函数。即通过{h0,x1, x2, …., xn}直接得{h1,h2…,hn}。 1234# inputs: shape = (batch_size, time_steps, input_size) # cell: RNNCell# initial_state: shape = (batch_size, cell.state_size)。初始状态。一般可以取零矩阵outputs, state = tf.nn.dynamic_rnn(cell, inputs, initial_state=initial_state) 此时，得到的outputs就是time_steps步里所有的输出。它的形状为(batch_size, time_steps, cell.output_size)。state是最后一步的隐状态，它的形状为(batch_size, cell.state_size)。 tf.nn.dynamic_rnn 函数是tensorflow封装的用来实现递归神经网络（RNN）的函数。 1234567891011tf.nn.dynamic_rnn( cell, inputs, sequence_length=None, initial_state=None, dtype=None, parallel_iterations=None, swap_memory=False, time_major=False, scope=None) 重要参数介绍： （个人感觉这参数还是挺重要的，可以放任何东西）cell：LSTM、GRU等的记忆单元。cell参数代表一个LSTM或GRU的记忆单元，也就是一个cell。例如，cell = tf.nn.rnn_cell.LSTMCell((num_units)，其中，num_units表示rnn cell中神经元个数，也就是下文的cell.output_size。返回一个LSTM或GRU cell，作为参数传入。 inputs：输入的训练或测试数据，一般格式为[batch_size, max_time, embed_size]，其中batch_size是输入的这批数据的数量，max_time就是这批数据中序列的最长长度，embed_size表示嵌入的词向量的维度。 sequence_length：是一个list，假设你输入了三句话，且三句话的长度分别是5,10,25,那么sequence_length=[5,10,25]。 如果有更多的时间，那么是可以学习一下 charRNN 这个模型，还是比较有意思的。 RNN 的一个例子1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586import tensorflow as tfimport numpy as npimport matplotlib.pyplot as pltimport argparse# argparse 解析命令行参数learing_rate =0.001seed =111# trainingbatch_size =128num_epoch =10hidden_size =128# 这个是很好的方式，可以用来 重现实验结果的# Reset the graph set the random numbers to be the same using "seed"tf.reset_default_graph()tf.set_random_seed(seed)np.random.seed(seed)# divde 28*28 images to rows of data to feed to RNN as sequantial informationstep_size= 28input_size =28output_size =10X =tf.placeholder(tf.float32, [None, step_size, input_size])y =tf.placeholder(tf.int32, [None])# RNNcell =tf.nn.rnn_cell.BasicRNNCell( num_units= hidden_size)output, state =tf.nn.dynamic_rnn(cell, X, dtype =tf.float32)# dynamic_rnn(cell, inputs)# state 表示最后一层的隐状态，output 这个还是有点模糊的# forward pass and loss calculationlogits = tf.layers.dense(state, output_size)# 只有在实现的时候，才能切身的感受到 loss function 是什么cross_entropy =tf.nn.sparse_softmax_cross_entropy_with_logits(labels= y, logits =logits)loss =tf.reduce_mean(cross_entropy)optimizer =tf.train.AdamOptimizer(learning_rate=learing_rate).minimize(loss)# Prediction,prediction =tf.nn.in_top_k(logits, y, 1) # 这个语句感觉是说不通的accuracy =tf.reduce_mean(tf.cast(prediction, tf.float32))# input datafrom tensorflow.examples.tutorials.mnist import input_datamnist =input_data.read_data_sets("MNIST_data/")# process MNISTX_test =mnist.test.imagesX_test =X_test.reshape([-1, step_size, input_size])y_test =mnist.test.labelsinit =tf.global_variables_initializer()loss_train_list =[]acc_train_list =[]with tf.Session() as sess: sess.run(init) n_batches =mnist.train.num_examples / batch_size for epoch in range(num_epoch): # 迭代的次数 for batch in range(n_batches): # 这个是一个 batch的东西 X_train, y_train =mnist.train.next_batch(batch_size) X_train =X_train.reshape([-1, step_size, input_size]) # 第一个参数有时候还是不太一样的， 并不总是 loss sess.run(optimizer, feed_dict=&#123;X: X_train, y: y_train&#125;) loss_train, acc_train = sess.run([loss, accuracy],feed_dict=&#123;X: X_train, y: y_train&#125;) loss_train_list.append(loss_train) acc_train_list.append(acc_train) # 每一个 epoch 之后是进行输出的 print("Epoch :&#123;&#125;, Train loss :&#123;:.3f&#125;, Train Acc &#123;:.3f&#125;".format(epoch+1, loss_train, acc_train)) loss_test, acc_test =sess.run([loss, acc_test], feed_dict=&#123;X: X_test, y: y_test&#125;) print("Test Loss: &#123;:.3f&#125;, Test ACC: &#123;:.3f&#125;".format(loss_test, acc_test))]]></content>
      <categories>
        <category>Deep learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[LeetCode-Others]]></title>
    <url>%2F2019%2F06%2F16%2Fleetcode-others%2F</url>
    <content type="text"><![CDATA[LeetCode 刷题总结（四），使用Python 实现。该篇题目类型主要包含无法归类到上述三篇的题目。 Reverse Integer Given a 32-bit signed integer, reverse digits of an integer. Input: 120 Output: 21 Input: 123 Output: 321 https://leetcode.com/problems/reverse-integer/ Tips： 这个只是reverse 操作，注意一些细节比如负号和数字 0的处理。对于最大数的表示 pow(2, 31)。 通过 % 求余获得最小位。 12345678910111213141516class Solution(object): def reverse(self, x): """ :type x: int :rtype: int """ result = 0 if x &lt; 0: symbol = -1 x = -x else: symbol = 1 while x: result = result * 10 + x % 10 x /= 10 return 0 if result &gt; pow(2, 31) else result * symbol String to Integer (atoi) Implement atoi which converts a string to an integer. Tips: 涉及到bit 级别数字处理的一般都会用到 res =res 10 + something 这样的东西。对于能够表示的数字的判断 max(-pow(2, 31), min(ressign, pow(2, 31) -1)) 这个还是挺经典的代码的。 https://leetcode.com/problems/string-to-integer-atoi/ 1234567891011121314151617181920212223class Solution(object): def myAtoi(self, str): """ :type str: str :rtype: int """ ls =list(str.strip()) if len(ls) == 0: return 0 sign = -1 if ls[0] == '-' else 1 index=0 # 有一个index 是贯穿始终的 res =0 if ls[index] in ['-', '+']: index +=1 for i in range(index, len(ls)): if ls[i].isdigit(): res =res *10 + ord(ls[i]) -ord('0') else: # case "words and 987" 是不能有 字母的 break return max(-pow(2, 31), min(sign*res, pow(2,31) -1)) Palindrome Number Determine whether an integer is a palindrome. An integer is a palindrome when it reads the same backward as forward. Tips： 回文数。代码写的很巧妙，整体上说是通过 / 和 % 获得数字的头和尾，在实现的时候有若干细节。 1234567891011121314151617181920class Solution(object): def isPalindrome(self, x): """ :type x: int :rtype: bool """ if x &lt; 0: return False ranger = 1 while x / ranger &gt;= 10: ranger *= 10 while x: left = x / ranger right = x % 10 if left != right: return False x = (x % ranger) / 10 ranger /= 100 return True Integer to Roman Roman numerals are represented by seven different symbols: I, V, X, L, C, D and M. Tips： 基于roman 数字规则的转换。代码实现角度 tuple都是要好于 list （内存和速度方面都是），如果你想要存储的是静态的可以遍历的数据，不需要每次进行修改的话，why not 123456789101112131415161718class Solution(object): def intToRoman(self, num): if num &lt;= 0: return "" digits = &#123; "I":1, "V":5, "X":10, "L":50, "C":100, "D":500, "M":1000 &#125; nums = (1000, 900, 500, 400, 100, 90, 50, 40, 10, 9, 5, 4, 1) chs = ("M", "CM", "D", "CD", "C", "XC", "L", "XL", "X", "IX", "V", "IV", "I") len = 13 s = "" while num &gt; 0: for i in range(0, len): if num &gt;= nums[i]: num -= nums[i] s += chs[i] break return s Roman to Integer Roman numerals are represented by seven different symbols: I, V, X, L, C, D and M. Tips： 和上一道题目相似。 1234567891011121314151617181920class Solution(object): def romanToInt(self, s): """ 这个是输入时 string，所以是 index 遍历的在 dict 中进行访问，但是上一个题目是总的 number，是没有办法的 """ digits = &#123; "I":1, "V":5, "X":10, "L":50, "C":100, "D":500, "M":1000 &#125; len_s = len(s) num = 0 # 这个少遍历了一个 ，因为其中有 i+1 的存在 for i in range(0, len_s - 1): cur = digits[s[i]] next_s = digits[s[i + 1]] if cur &gt;= next_s: num += cur else: num -= cur # 处理的是最后一个 num += digits[s[len_s - 1]] return num Letter Combinations of a Phone Number Given a string containing digits from 2-9 inclusive, return all possible letter combinations that the number could represent.A mapping of digit to letters (just like on the telephone buttons) is given below. Note that 1 does not map to any letters. Input: &quot;23&quot; Output: [&quot;ad&quot;, &quot;ae&quot;, &quot;af&quot;, &quot;bd&quot;, &quot;be&quot;, &quot;bf&quot;, &quot;cd&quot;, &quot;ce&quot;, &quot;cf&quot;]. Tips：使用循环的方式表示层级的关系（使用二层循环，第一层表示外围a， 第二层表示 def 等）。 1234567891011121314151617181920212223242526272829303132333435363738class Solution: def letterCombinations(self, digits): """ :type digits: str :rtype: List[str] """ if not digits or digits == "": return [] # 命名很到位 maps =&#123; '1': (), '0': (), '2': ('a', 'b', 'c'), '3': ('d', 'e', 'f'), '4': ('g', 'h', 'i'), '5': ('j', 'k', 'l'), '6': ('m', 'n', 'o'), '7': ('p', 'q', 'r', 's'), '8': ('t', 'u', 'v'), '9': ('w', 'x', 'y', 'z') &#125; results = [""] for digit in digits: tuple1 = maps[digit] tmp =[] if len(tuple1) == 0: continue # 二重循环 for prefix in results: for suffix in tuple1: tmp.append(prefix + suffix) results = tmp return results Divide Two Integers Given two integers dividend and divisor, divide two integers without using multiplication, division and mod operator.Return the quotient after dividing dividend by divisor.The integer division should truncate toward zero. Tips： 不让使用乘除和 mod 操作，只能使用位运算符了。在python 中0 == False 这个在逻辑判断中是等价的（1 ==True）。计算的时候使用 &lt;&lt; （不断的*2）, ,maybe 是二分 12345678910111213141516171819202122class Solution(object): def divide(self, divident, divisor): sign =-1 if divident* divisor&lt;0 else 1 divident, divisor =abs(divident), abs(divisor) ans =0 while divisor &lt;= divident: div =divisor tmp =1 while (div &lt;&lt;1) &lt;= divident: div &lt;&lt;= 1 tmp &lt;&lt;= 1 divident -= div ans += tmp return max(-pow(2, 31), min(ans*sign, pow(2, 31) -1)) Pow(x, n) Implement pow(x, n), which calculates x raised to the power n (xn) Tips： 简单的递归。 1234567891011121314151617181920class Solution(object): # 递归写起来比较好些，但是有时候比较难理解这个运行的过程 def myPow(self, x,n): """ :type x: float :type n: int :rtype: float """ if n ==0: return 1 # 求解pow() 都是正数，如果n &lt;0,那么需要做的是 取导数 elif n &lt;0: return 1.0/self.myPow(x, -n) else: half =self.myPow(x, n&gt;&gt;1) if n%2 ==0: return half *half else: return x *half*half N-Queens The n-queens puzzle is the problem of placing n queens on an n×n chessboard such that no two queens attack each other. Given an integer n, return all distinct solutions to the n-queens puzzle.Each solution contains a distinct board configuration of the n-queens’ placement, where ‘Q’ and ‘.’ both indicate a queen and an empty space respectively. Input: 4Output: [ [“.Q..”, // Solution 1 “…Q”, “Q…”, “..Q.”], [“..Q.”, // Solution 2 “Q…”, “…Q”, “.Q..”]]Explanation: There exist two distinct solutions to the 4-queens puzzle as shown above. Tips： N-皇后，行列对角线是不能出现重复。中规中矩的递归解法，board 是使用的一维向量， 这样去理解 比如board=[1, 3, 0, 2]，这是4皇后问题的一个解，意思是：在第0行，皇后放在第1列；在第1行，皇后放在第3列。 check函数表示 第k 个 皇后是否能够放在第j 个位置。 123456789101112131415161718192021222324252627class Solution(object): def __init__(self): self.board =[] def check(self, k,j): for i in range(k): # 如果之前的皇后已经放到了这个位置，或者两者在一条直线上，这个abs 用的比较牛逼 if self.board[i] ==j or abs(k -i) ==abs(self.board[i] -j): return False return True def dfs(self, depth, valuelist, n, res): if depth ==n: res.append(valuelist) return for i in range(n): if self.check(depth, i): self.board[depth] =i s ='.'*n self.dfs(depth +1, valuelist+[s[:i] +'Q'+s[i+1:]], n,res) def solveNQueens(self, n): self.board =[-1 for i in range(n)] res =[] self.dfs(0, [], n, res) return res N-Queens II The n-queens puzzle is the problem of placing n queens on an n×n chessboard such that no two queens attack each other. Tips: 这个相对于上一个要简单一些，因为最后的结果要的是 counts 而不是 list of path。按照道理讲是不用记录path 的。 123456789101112131415161718192021222324252627282930313233class Solution(object): def __init__(self): self.board =[] self.count =0 def check(self, k,j): """ check if the kth queen can be put in column j :param k: :param j: :return: """ for i in range(k): if self.board[i] ==j or abs(k -i) ==abs(self.board[i] -j): return False return True def dfs(self, depth, valuelist, n, res): if depth ==n: #res.append(valuelist) self.count +=1 return for i in range(n): if self.check(depth, i): self.board[depth] =i s ='.'*n self.dfs(depth +1, valuelist+[s[:i] +'Q'+s[i+1:]], n,res) def totalNQueens(self, n): self.board =[-1 for i in range(n)] res =[] self.dfs(0, [], n, res) return self.count Add Binary Given two binary strings, return their sum (also a binary string).The input strings are both non-empty and contains only characters 1 or 0. Tips：二级制相加，从后往前走，使用 carry 位置记录进位数。 12345678910111213141516171819202122class Solution(object): def addBinary(self, a, b): """ :type a: str :type b: str :rtype: str """ i, j, carry, res =len(a) -1, len(b) -1, 0, '' while i&gt;=0 or j &gt;=0 or carry: if i &gt;=0: carry += int(a[i]) i -=1 if j&gt;=0: carry += int(b[j]) j -=1 res =str(carry%2) +res carry //=2 return res Max Points on a Line Given n points on a 2D plane, find the maximum number of points that lie on the same straight line. Tips：当使用除法的时候，因为精度问题造成的误差，所以这里使用最大的公约数进行化简。使用以下三种方式处理。 Map from (a,b,c,d) representing y=(a/b)x+(c/d) to set of indices of points that are on that line. a/b and c/d are reduced, i.e. a and b are divided by their GCD and so are c and d. Vertical lines are represented by a tuple with 1 element, the x-axis value Single points are represented by a 2-tuple (x, y). 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647import collectionsimport mathdef fraction(x, y): if x &lt; 0: x, y = -x, -y gcd = math.gcd(x, y) return x // gcd, y // gcd class Solution: def maxPoints(self, points): if not points: return 0 if len(points) == 1: return 1 aligned_points = collections.defaultdict(set) duplicates = collections.defaultdict(set) for i, p in enumerate(points): for j, q in enumerate(points[i + 1:], start=i + 1): # x 是否相同 if q[0] == p[0]: if q[1] == p[1]: duplicates[i].add(j) key = tuple(p) else: key = (q[0],) else: a, b = fraction(q[1] - p[1], q[0] - p[0]) # k斜率 c, d = fraction(p[1] * q[0] - q[1] * p[0], q[0] - p[0]) # b 位移 key = (a, b, c, d) #aligned_points[key] = aligned_points[key] or &#123;i, j&#125; # 因为之前定义的是set aligned_points[key] |= &#123;i, j&#125; for p, dups in duplicates.items(): for key in aligned_points: if p in aligned_points[key]: #aligned_points[key] = aligned_points[key] or dups # 这个or 不是选择的意思，是两者都要的意思 aligned_points[key] |= dups max_points = 0 for aliged in aligned_points.values(): max_points = max(max_points, len(aliged)) return max_points Happy Number Write an algorithm to determine if a number is “happy”.A happy number is a number defined by the following process: Starting with any positive integer, replace the number by the sum of the squares of its digits, and repeat the process until the number equals 1 (where it will stay), or it loops endlessly in a cycle which does not include 1. Those numbers for which this process ends in 1 are happy numbers. Input: 19Output: trueExplanation:12 + 92 = 8282 + 22 = 6862 + 82 = 10012 + 02 + 02 = 1 Tips: 使用set 的思路，没有重复之前就一直遍历；对于数字转成 string 逐个进行处理。 1234567891011121314class Solution(object): def isHappy(self, n): """ :type n: int :rtype: bool """ visited =set() # 这个set 每次add 都是add 到最前面一个，是规律吧 while n not in visited: visited.add(n) n =sum((int(x) **2 for x in str(n)))# () 要比 [] 使用较少的内存 return n ==1 Sum of Two Integers Calculate the sum of two integers a and b, but you are not allowed to use the operator + and -. Input: a = 1, b = 2Output: 3 Tips: 下面的add()函数只是在以下场景中 work: a*b&gt;=0 , or the negative number has a larger absolute value( a &lt; 0 and abs(a) &gt; b &gt; 0 , or b &lt; 0 and abs(b) &gt; a &gt; 0) 123456789101112131415161718192021class Solution(object): def getSum(self, a, b): """ :type a: int :type b: int :rtype: int """ def add(a, b): if not a or not b: return a or b # # ^ get different bits and &amp; gets double 1s, &lt;&lt; moves carry ， 这个可能是加法吧， return add(a^b, (a&amp;b) &lt;&lt; 1) if a*b &lt; 0: # assume a &lt; 0, b &gt; 0 if a &gt; 0: return self.getSum(b, a) if -a == b: return 0 if -a &lt; b: return -add(-a, -b) return add(a, b) Fizz Buzz * Write a program that outputs the string representation of numbers from 1 to n. n = 15,Return:[ “1”, “2”, “Fizz”, “4”, “Buzz”, “Fizz”, “7”, “8”, “Fizz”, “Buzz”, “11”, “Fizz”, “13”, “14”, “FizzBuzz”] Tips: 这是一个 for 循环就可以解决的问题 12345678910111213141516171819class Solution(object): def fizzBuzz(self, n): """ :type n: int :rtype: List[str] """ # 使用一个 result append一下 result = [] for i in xrange(1, n + 1): if i % 3 != 0 and i % 5 != 0: result.append(str(i)) elif i % 3 == 0 and i % 5 != 0: result.append("Fizz") elif i % 3 != 0 and i % 5 == 0: result.append("Buzz") else: result.append("FizzBuzz") return result Find Median from Data Stream Median is the middle value in an ordered integer list. If the size of the list is even, there is no middle value. So the median is the mean of the two middle value. For example,[2,3,4], the median is 3[2,3], the median is (2 + 3) / 2 = 2.5 Tips: 主要难点在于数据流，难在数据结构，使用最小根堆实现。这个是需要寻找 median （中位数），使用大小根堆，分别存储较小的一半 和 较大的一半。那么大根堆的堆顶就对应着较小一半的最大值，小根堆对应着较大部分的最小值。所以中位数就可以 快速的从两个堆顶元素中获得。 https://leetcode.com/problems/find-median-from-data-stream/ 简单说一下在python 中heapq -小根堆的实现。将数值转成负数，那么就可以使用小根堆来mimic 大根堆，因为堆顶是负数最小的。（对应正数最大的） heap.heappush(heap, item), 把一个item 添加到heap中 heap.heappushpop(heap, item), 先把item 放入到堆中，然后再pop() , 这样比 heappush() 然后再heappop() 快一些 push item on the heap, then pop and return the smallest item from the heap. The combined action runs more efficiently than heappush followed by a separate call to heappop() Note that the heapq in python is a min heap, thus we need to invert the values in the smaller half to mimic a &apos;max heap&apos; the add operation is O(nlogn), the find operation is O(1) 123456789101112131415161718from heapq import *class MedianFinder(object): def __init__(self): self.small =[] # max heap (the smaller half of the list), 大根堆存放的是小值，然后根存放的就是最大值, 这个转成-num 当然就是 smaller part self.large =[] # min heap (the larger half of the list), 小根堆存放的是大值，然后根就是存放的最小值 def addNum(self, num): if len(self.small) ==len(self.large): heappush(self.large, -heappushpop(self.small, -num)) else: heappush(self.small, -heappushpop(self.large, num)) def findMedian(self): if len(self.small) ==len(self.large): return float(self.large[0] -self.small[0])/2.0 else: return float(self.large[0]) Flatten Nested List Iterator Given a nested list of integers, implement an iterator to flatten it.Each element is either an integer, or a list – whose elements may also be integers or other lists. Tips: 迭代器和 generator 感觉都是差不多的操作，产生一个数字。这个看懂 API 更重要。只要数字内容，不要嵌套的关系。yield 关键字，调用完之后，程序不结束。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950# """# This is the interface that allows for creating nested lists.# You should not implement it, or speculate about its implementation# """#class NestedInteger(object):# def isInteger(self):# """# @return True if this NestedInteger holds a single integer, rather than a nested list.# :rtype bool# """## def getInteger(self):# """# @return the single integer that this NestedInteger holds, if it holds a single integer# Return None if this NestedInteger holds a nested list# :rtype int# """## def getList(self):# """# @return the nested list that this NestedInteger holds, if it holds a nested list# Return None if this NestedInteger holds a single integer# :rtype List[NestedInteger]# """class NestedIterator(object): def __init__(self, nestedList): def gen(nestedList): for x in nestedList: if x.isInteger(): yield x.getInteger() else: for y in gen(x.getList()): yield y self.gen = gen(nestedList) def next(self): return self.value def hasNext(self): try: self.value = next(self.gen) return True except StopIteration: return False# Your NestedIterator object will be instantiated and called as such:# i, v = NestedIterator(nestedList), []# while i.hasNext(): v.append(i.next()) Insert Delete GetRandom O(1) Design a data structure that supports all following operations in average O(1) time. insert(val): Inserts an item val to the set if not already present. remove(val): Removes an item val from the set if present. getRandom: Returns a random element from current set of elements. Each element must have the same probability of being returned. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546class RandomizedSet(object): # 就是一个维持list 的东西 def __init__(self): """ Initialize your data structure here. """ self.list =[] def insert(self, val): """ Inserts a value to the set. Returns true if the set did not already contain the specified element. :type val: int :rtype: bool """ if val not in self.list: self.list.append(val) return True return False def remove(self, val): """ Removes a value from the set. Returns true if the set contained the specified element. :type val: int :rtype: bool """ if val in self.list: self.list.remove(val) return True return False def getRandom(self): """ Get a random element from the set. :rtype: int """ length =len(self.list) import random index =random.randint(0,length-1) return self.list[index] # Your RandomizedSet object will be instantiated and called as such:# obj = RandomizedSet()# param_1 = obj.insert(val)# param_2 = obj.remove(val)# param_3 = obj.getRandom() Perfect Squares Given a positive integer n, find the least number of perfect square numbers (for example, 1, 4, 9, 16, …) which sum to n. Tips: 再次理解一下二重循环，如果第二层中的遍历次数和 第一层是有关系的，往往是 O(N*N/2) 的复杂度（这种记法是错误的），用于遍历前 i 个元素。 ···pythonclass Solution(object): # dp[i] 的定义表示 i 这个数字最少使用的 squares 数量 def numSquares(self, n): &quot;&quot;&quot; :type n: int :rtype: int &quot;&quot;&quot; dp = [float(&apos;inf&apos;)]*(n+1) dp[0]=0 count = 2 for i in range(1,n+1): if i&gt;=count*count: count += 1 for j in range(1,count): dp[i] = min(dp[i],dp[i-j*j]+1) return dp[n] &quot;&quot;&quot; # 这样做是不可行的，在于可能重复使用一个，如果没有最大的合适的话 square =[pow(num, 2) for num in range(1, int(math.sqrt(n) +1))] square =square[::-1] count =0 for sq in square: if n-sq &gt;0: n -= sq count +=1 return count &quot;&quot;&quot; 123456789101112131415161718192021222324** Factorial Trailing Zeroes**&gt; Given an integer n, return the number of trailing zeroes in n!.Tips: 数学问题···pythonclass Solution(object): # 这里的 += n/div 已经就表示了 5的个数， 这样是可以加快运算的 def trailingZeroes(self, n): &quot;&quot;&quot; :type n: int :rtype: int &quot;&quot;&quot; div =5 res =0 while div &lt;=n: res += n/div div =div *5 return res Count Primes Count the number of prime numbers less than a non-negative number, n. Tips: 主要是质数倍数的优化，否则是time out 123456789101112131415161718192021class Solution(object): # prime 质数， 1 既不是质数也不是 合数 # 当前数为质数时，排除掉剩下的数中该数的整倍数。遍历过所有的数之后剩下的数全是质数。提升效率的方法是减少遍历的长度。 # 还有一个优化点，可以不必从2~m-1，只需遍历2 ~ √m.因为如果m能被2 ~ m-1之间任一整数整除，其二个因子必定有一个小于或等于√m，另一个大于或等于√m。例如16能被2,4,8整除 # 质数定义为在大于1的自然数中，除了1和它本身以外不再有其他因数。 # 最后 return 的是 counter，个数 而不是具体的数字 def countPrimes(self, n): """ :type n: int :rtype: int """ if n &lt;= 2: return 0 prime = [True] * n prime[:2] = [False, False] for base in range(2, int((n ) ** 0.5) + 1): # 时间上是 [2, sqrt(m)] ，但是在python 中实现是这样的 if prime[base]: prime[base ** 2::base] = [False] * len(prime[base ** 2::base]) return sum(prime) Missing Number Given an array containing n distinct numbers taken from 0, 1, 2, …, n, find the one that is missing from the array. Tips: 使用公式 index 和前 n数的问题。 1234567891011class Solution(object): # 如果没有限制内存，那么是可以使用dict，然后根据index 和value 进行判断的 # 凡是和对应的index 发生关系，那么这个就有优化的可能，就变得比较有意思 def missingNumber(self, nums): """ :type nums: List[int] :rtype: int """ n =len(nums) return n*(n+1)/2 -sum(nums) Power of Three Given an integer, write a function to determine if it is a power of three. Tips: 求余 and /= 相结合的常见手法 12345678910111213141516171819class Solution(object): # 这个是在考察除法的运算过程 def isPowerOfThree(self, n): """ :type n: int :rtype: bool """ if n &lt;=0: return False if n ==1: return True while n &gt;1: if n %3 !=0: return False n /= 3 return True Implement Trie (Prefix Tree) Implement a trie with insert, search, and startsWith methods. Tips: 这个数据结构很有用，字典树。 123456789101112131415161718192021222324252627282930313233343536373839404142434445class TrieNode: # Initialize your data structure here. # https://www.cnblogs.com/beiyeqingteng/p/5625540.html，这里面有个图 # 还是挺形象的 def __init__(self): # 这个是什么数据结构呀 self.children = collections.defaultdict(TrieNode) # 这种设置还是理解不够深刻 self.is_word = False# 考察的是字典树，这种数据结构# 保存字母的话，是 26叉树，保存数字的话10 叉树class Trie(object): def __init__(self): self.root =TrieNode() def insert(self, word): current =self.root for letter in word: current =current.children[letter] current.is_word =True def search(self, word): current =self.root for letter in word: current =current.children.get(letter) if not current: return False return current.is_word def startsWith(self, prefix): current =self.root for letter in prefix: current =current.children.get(letter) # dictionary 操作，得到的是值 if not current: return False return True# Your Trie object will be instantiated and called as such:# obj = Trie()# obj.insert(word)# param_2 = obj.search(word)# param_3 = obj.startsWith(prefix)]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode-Recursion]]></title>
    <url>%2F2019%2F06%2F16%2Fleetcode-recursion%2F</url>
    <content type="text"><![CDATA[LeetCode 刷题总结（三）， 使用Python 实现。该篇题目类型主要包括：recursion, iteration 和dynamic programming。 Regular Expression Matching Given an input string (s) and a pattern (p), implement regular expression matching with support for ‘.’ and ‘ * ‘ .‘.’ Matches any single character.‘*‘ Matches zero or more of the preceding element. Tips：典型的dp，二维数组是常见的方式。 如果看不懂注释，可以看这里 1234567891011121314151617181920212223242526272829303132333435class Solution(object): """ dp, dp[i][j] means the match status between p[:i] and s[:j] """ def isMatch(self, s, p): dp =[[False]*(len(s)+1) for _ in range(len(p) +1)] dp[0][0]= True # case, of when s is an empty string but p is not, # since each * can eliminate character before it for i in range(2, len(p)+1): dp[i][0] =dp[i-2][0] and p[i-1] =="*" for i in range(1, len(p)+1): for j in range(1, len(s)+1): if p[i-1] =='*': # elimination or propagations dp[i][j] =dp[i-2][j] or dp[i-1][j] # another case, propagations if p[i-2] ==s[j-1] or p[i-2] =='.': # 下面两种写法都是可以 # dp[i][j] = dp[i][j] or dp[i][j-1] dp[i][j] |= dp[i][j-1] else: # 对于and 这个语句就类似 if 语句, 下面两种写法都是可以的 #dp[i][j] =dp[i-1][j-1] and (p[i-1] ==s[j-1] or p[i-1] =='.') if p[i-1] ==s[j-1] or p[i-1] =='.': dp[i][j] =dp[i-1][j-1] return dp[-1][-1] Wildcard Matching Given an input string (s) and a pattern (p), implement wildcard pattern matching with support for ‘?’ and ‘*‘.‘?’ Matches any single character.‘*‘ Matches any sequence of characters (including the empty sequence). Input:s = “aa”p = “a”Output: falseExplanation: “a” does not match the entire string “aa”. Tips: Wildcard 通配符，这个和上一个基本相同啊， 1234567891011121314151617181920212223242526272829303132333435363738394041424344class Solution(object): """ 这个更加简单，while 就是能够搞定的，然后对于 特殊符号特殊判断。使用两个指针进行操作. s[i] ==p[j] 和 p[j] =='?' ，这个是可以放到同一个 if 条件下的。两者是等价的。 关键是 * 的匹配 2，在 p 中出现 * 时，记录 p 中 * 的位置，同时记录此时 s 的位置。 3，从 * 的后面的第一个字符开始匹配。如果匹配失败，返回 s 处，从 s++ 开始重新匹配。 """ def isMatch(self, s, p): """ :type s: str :type p: str :rtype: bool """ j = i = ss = 0; star = -1 # 首先把 string 中的字符比完 while i &lt; len(s): if j &lt; len(p) and (s[i] == p[j] or p[j] == '?'): i += 1; j += 1 continue # star 记录的是 j 的位置，相应的 ss 是记录的i (string) 中的位置 if j &lt; len(p) and p[j] == '*': star = j; j += 1; ss = i; continue # 如果已经有了 star 的出现， 到这里已经说明 star的下一个和 string 中的位置元素不是exact 的匹配 # 所以这里进行了 ss +=1 的操作是为了，相当于把 string 中的char 使用 * 进行了代替 # 好好理解一下 if star != -1: j = star + 1; ss += 1; i = ss continue return False # string 已经比较完了，如果只剩下 * 那么是可以行的，否则是不可行的 while j &lt; len(p) and p[j] == '*': j += 1 if j == len(p): return True return False Valid Parentheses Given a string containing just the characters ‘(‘, ‘)’, ‘{‘, ‘}’, ‘[‘ and ‘]’, determine if the input string is valid. Tips: 必须要使用 len(stack) 进行检测，因为中间的时候也可能 len(stack) 是等于0的，这时候只能是 append() ，不能访问 stack[-1] 123456789101112131415161718192021class Solution(object): def isMatch(self, l, r): return l =='[' and r==']' or l =='(' and r ==')' or l =='&#123;' and r =='&#125;' def isValid(self, s): len_s =len(s) if len_s ==0: return True stack =[] for ch in s: if len(stack) ==0 or not self.isMatch(stack[-1], ch): stack.append(ch) else: stack.pop() return len(stack) ==0 Generate Parentheses Given n pairs of parentheses, write a function to generate all combinations of well-formed parentheses. For example, given n = 3, a solution set is: [ “((()))”, “(()())”, “(())()”, “()(())”, “()()()”] Tips: dfs, left_count 表示是 ‘(‘ 的总数， left_remain 表示 left- right 的差值. 12345678910111213141516171819202122232425class Solution(object): def DogenerateParenthesis(self, n, left_count, left_remain, prefix): if n ==left_count and left_remain ==0: return [prefix] left =[] right =[] if left_count &lt;n: left =self.DogenerateParenthesis(n, left_count+1, left_remain+1, prefix+'(') if left_remain&gt;0: right =self.DogenerateParenthesis(n, left_count, left_remain-1, prefix+')') return left +right def generateParenthesis(self, n): """ :type n: int :rtype: List[str] """ if n ==0: return [] else: list =self.DogenerateParenthesis(n ,0, 0, "") return list Combination Sum Given a set of candidate numbers (candidates) (without duplicates) and a target number (target), find all unique combinations in candidates where the candidate numbers sums to target.The same repeated number may be chosen from candidates unlimited number of times. Tips: 这种找到所有符合题目要求的解，十之八九都是要使用递归。最优解（最值）一般是使用dp，减少子问题的运算。这里给出了 list 和dfs 的结合使用，通过传入 start index来解决是否遍历过的问题。 示意图： 12345678910111213141516171819202122class Solution(object): def dfs(self, candidates, target, start, intermedia, res): # target 这个变量调节了是 继续deeper or return， 每一次都是在变化的。如果 ==0，那么就return 了 if target ==0: res.append(intermedia) return for i in range(start, len(candidates)): if target &lt; candidates[i]: return self.dfs(candidates, target-candidates[i], i, intermedia+[candidates[i]], res) def combinationSum(self, candidates, target): """ :type candidates: List[int] :type target: int :rtype: List[List[int]] """ candidates.sort() res =[] self.dfs(candidates, target, 0, [], res) return res Combination Sum II Given a collection of candidate numbers (candidates) and a target number (target), find all unique combinations in candidates where the candidate numbers sums to target.Each number in candidates may only be used once in the combination. Tips： 这里给出了另外一种遍历list 和dfs 的方法，传入的是部分 list，上面那道题传入了完整的list。 123456789101112131415161718192021222324252627class Solution(object): # @param &#123;integer[]&#125; candidates # @param &#123;integer&#125; target # @return &#123;integer[][]&#125; def combinationSum2(self, candidates, target): candidates.sort() # 排序不影响 时间复杂度的，因为时间复杂度大于排序的时间复杂度 #res=set() res =[] self.findcombination(candidates,target,[],res) #return [list(i) for i in res] return res def findcombination(self,candidates,target,ls,res): if target==0 and ls not in res: # 对于 set() 中使用 add() ，list 中使用 append() #res.add(tuple(ls)) res.append(ls) return # 下面这个判断用和不用 都是相同的效果(时间和空间复杂度上) if target&lt;0: return # not use: c72 ms,11.7M for i in range(len(candidates)): if target&lt;candidates[i]: return self.findcombination(candidates[i+1:],target-candidates[i],ls+[candidates[i]],res) Permutations Given a collection of distinct integers, return all possible permutations. Input: [1,2,3]Output:[ [1,2,3], [1,3,2], [2,1,3], [2,3,1], [3,1,2], [3,2,1]] Tips : extend 是因为list of list ，而不是单独的list，这样能保证最后的结果还是 list of list 123456789101112131415161718192021class Solution(object): def permute(self, nums): return self.doPermute(nums) def doPermute(self, num_list): if len(num_list) ==1: return [num_list] res_list =[] for i in range(len(num_list)): num_list[0], num_list[i] =num_list[i], num_list[0] sub_list =self.doPermute(num_list[1:]) list_head =[num_list[0]] #new_list =list_head+ sub_list new_list = [list_head + list1 for list1 in sub_list] # 可以理解这个是 sub_list 是有一系列的解， 然后再每个解上都加上一个头元素 res_list.extend(new_list) # extend，The list.extend method extends a list by appending elements from an iterable # append 是当做一个整体进行操作 return res_list Permutations II Given a collection of numbers that might contain duplicates, return all possible unique permutations. Tips： 这个 duplicates 是通过 sort 函数，然后在选择 某个index 时候，进行判断一下是否和第一个重合，这样的方式去handle。 12345678910111213141516171819202122232425262728class Solution(object): def doPermuteUnique(self, nums): if len(nums) ==1: return [nums] res_list =[] for i in range(len(nums)): if i&gt;0 and nums[0] ==nums[i]: continue nums[0], nums[i] =nums[i], nums[0] sub_list =self.doPermuteUnique(nums[1:]) list_head =[nums[0]] new_list =[list_head +list1 for list1 in sub_list] res_list.extend(new_list) return res_list def permuteUnique(self, nums): """ :type nums: List[int] :rtype: List[List[int]] """ nums.sort() return self.doPermuteUnique(nums) Climbing Stairs You are climbing a stair case. It takes n steps to reach to the top.Each time you can either climb 1 or 2 steps. In how many distinct ways can you climb to the top?Note: Given n will be a positive integer. Tips： 数学题，斐波那契数列。 解法一： 12345678910111213141516171819202122class Solution(object): # 可以换成数学模型，发现就是 斐波那契数列 # 不使用数字，使用三个变量也是可以的额 def climbStairs(self, n): """ :type n: int :rtype: int """ if n ==1: return 1 elif n ==2: return 2 arr = [0] *(n+1) arr[1] =1 arr[2] =2 for i in range(3, n+1): arr[i] =arr[i-1] +arr[i-2] return arr[n] 解法二123456789101112def climbStairs(self, n): if n ==1: return 1 elif n ==2: return 2 a, b =1,2 c =0 for i in range(3, n+1): c = a+b a, b =b,c return c Combinations Given two integers n and k, return all possible combinations of k numbers out of 1 … n. Tips： 这个是处理的list 和 dfs()的问题，然后使用的传入 index和完整的 list 来控制进度。 12345678910111213141516171819202122class Solution(object): """ 好好理解递归这种逐渐加深的层次 """ def combine(self, n, k): res =[] self.dfs(list(range(1, n+1)), k, 0, [], res) return res def dfs(self, nums, k, index, path, res): # backtracking #if k &lt;0: #return # 这种 return 和result 结合使用的操作是经常常见的 if k ==0: res.append(path) return # 这个index 是很重要的， 在这个index 的基础上选择的 for i in range(index, len(nums)): self.dfs(nums, k-1, i +1, path+ [nums[i]], res) Subsets Given a set of distinct integers, nums, return all possible subsets (the power set). Tips: 使用的是第二种方式，传入部分list，从而由大问题转移成小问题。 12345678910111213141516class Solution(object): # 这种是最简单的深度优先的搜索了， def subsets(self, nums): res =[] self.dfs(nums, [], res) return res def dfs(self, nums, path, res): # 一般来说这个是有跳出条件，回溯的，但是这种情况是没有的，只有最后一个 # [[],[1],[1,2],[1,2,3],[1,3],[2],[2,3],[3]]， 当输出 [1, 2,3] 的时候，return，但是这个return 到了 [1, 3] 这个层次 res.append(path) for i in range(len(nums)): self.dfs(nums[i+1:], path+[nums[i]], res) Subsets II Given a collection of integers that might contain duplicates, nums, return all possible subsets (the power set). Tips： 这个含有duplicates，使用功能sort 然后在 for 循环 的时候进行判断一下。 123456789101112131415161718192021222324class Solution(object): # 递归 def subsetsWithDup(self, nums): """ :type nums: List[int] :rtype: List[List[int]] """ res =[] nums.sort() self.dfs(nums, 0, [], res) return res def dfs(self, nums, index, path, res): if path not in res: res.append(path) #res.append(path) for i in range(index, len(nums)): if i &gt; index and nums[i] ==nums[i-1]: continue self.dfs(nums, i+1, path+[nums[i]], res) # 下面的代码是错误的 memory 但是不知道为什么 Decode Ways A message containing letters from A-Z is being encoded to numbers using the following mapping: &apos;A&apos; -&gt; 1 &apos;B&apos; -&gt; 2 ... &apos;Z&apos; -&gt; 26 Given a non-empty string containing only digits, determine the total number of ways to decode it. Tips： 多少种解码方式。本质是裴波拉契数列, 感觉自己并没有get 到这个本质上是 该数列 1234567891011121314151617181920212223242526class Solution(object): """ DP[i] = DP[i-1] + DP[i-2] \ \___________(if str[i-2] exists and 10&lt;= int(str[i-1] + str[i]))&lt;=26 ) \___________(If str[i-1] exists and str[i] != '0' ) """ def numDecodings(self, s): """ :type s: str :rtype: int """ if not s: return 0 if s =='10': return 1 dp =[0] *(len(s) +1) dp[0] =1 for i in range(1, len(s)+1): if s[i-1] !='0': dp[i] +=dp[i-1] if i &gt;1 and '10' &lt;=s[i-2:i] &lt;='26': dp[i] += dp[i-2] return dp[-1] Binary Tree Inorder Traversal Given a binary tree, return the inorder traversal of its nodes’ values. Tips： 递归。 1234567891011121314151617# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution(object): # inorder 中序遍历, recursive 递归， iterative 迭代 # 这个是递归的 recursively def helper(self, root, res): if root: self.helper(root.left, res) res.append(root.val) self.helper(root.right, res) Tips： 递归的容易写，循环的也好会，使用的stack 先保存左子树，然后不断的node 其右子树。 12345678910111213141516def inorderTraversal(self, root): # 如果使用 迭代，那么就是 stack结构了 res, stack =[], [] while True: while root: stack.append(root) root =root.left if not stack: return res node =stack.pop() res.append(node.val) root =node.right Validate Binary Search Tree Given a binary tree, determine if it is a valid binary search tree (BST).Assume a BST is defined as follows: The left subtree of a node contains only nodes with keys less than the node’s key. The right subtree of a node contains only nodes with keys greater than the node’s key. Both the left and right subtrees must also be binary search trees. Tips： 二叉搜索树的特点，中序遍历，先得到遍历结果，然后判断是否是不减的（只是需要O(N)）. 1234567891011121314151617181920212223242526272829303132# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution(object): def isValidBST(self, root): """ :type root: TreeNode :rtype: bool """ output =[] self.inOrder(root, output) for i in range(1, len(output)): if output[i-1] &gt;= output[i]: return False return True def inOrder(self, root, output): if not root: return self.inOrder(root.left, output) output.append(root.val) self.inOrder(root.right, output) Same Tree Given two binary trees, write a function to check if they are the same or not.Two binary trees are considered the same if they are structurally identical and the nodes have the same value. Tips: 对应的值相同，对应的结构相同。 1234567891011121314151617181920212223242526# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution(object): # 根据上一一个题目的要求，这个是也是可以先进行遍历，然后再比较最后的遍历结果吗 def isSameTree(self, p, q): """ :type p: TreeNode :type q: TreeNode :rtype: bool """ if not p and not q: return True elif not p or not q: return False if p.val ==q.val: return self.isSameTree(p.left, q.left) and self.isSameTree(p.right, q.right) else: return False Symmetric Tree Given a binary tree, check whether it is a mirror of itself (ie, symmetric around its center). Tip： 对称和 same 是在于比较的方式是不一样的。 1234567891011121314151617181920212223242526272829303132# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution(object): # 该题目和 isSameTree 是有点相似的，只是修改部分代码就可以 def isSymmetric(self, root): """ :type root: TreeNode :rtype: bool """ # 这个如果是 [] 或者 None， 是返回true， 因为输入的形式是 list ，所以判断条件是 if root ==[], 这样形式 if not root: return True return self.dfs(root.left, root.right) def dfs(self, p, q): if not p and not q: return True elif not p or not q: return False if p.val == q.val: return self.dfs(p.left, q.right) and self.dfs(p.right, q.left) else: return False Binary Tree Zigzag Level Order Traversal Given a binary tree, return the zigzag level order traversal of its nodes’ values. (ie, from left to right, then right to left for the next level and alternate between). For example:Given binary tree [3,9,20,null,null,15,7], 3 / \ 9 20 / \ 15 7结果是这样的：[ [3], [20,9], [15,7]] Tips： 一行是从左往右，一行是从右往左。层序遍历的变体。从左向右使用 append() ，然后从右向左使用 insert()，这个是没有问题的。 12345678910111213141516171819202122232425262728# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution(object): # 层次遍历 + 奇偶性来决定是否 reverse def zigzagLevelOrder(self, root): """ :type root: TreeNode :rtype: List[List[int]] """ res = [] self.dfs(root, 0, res) return res def dfs(self, root, level, res): if root: if len(res) &lt; level + 1: res.append([]) if level % 2 == 0: res[level].append(root.val) else: res[level].insert(0, root.val) self.dfs(root.left, level+1, res) self.dfs(root.right, level+1, res) Maximum Depth of Binary Tree Given a binary tree, find its maximum depth.The maximum depth is the number of nodes along the longest path from the root node down to the farthest leaf node.Note: A leaf is a node with no children. Tips: 左右子树的max+1，这个是树的深度。 12345678910111213141516171819# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution(object): # 递归， 最简单的方式 def maxDepth(self, root): """ :type root: TreeNode :rtype: int """ if not root: return 0 # 这个是最简单的代码了 return 1 +max(self.maxDepth(root.left), self.maxDepth(root.right)) Binary Tree Level Order Traversal II Given a binary tree, return the bottom-up level order traversal of its nodes’ values. (ie, from left to right, level by level from leaf to root). Tips: 层序遍历，但是 res 需要存储成list of list，这样最后进行reverse，能够表示出 层数的信息。 1234567891011121314151617181920212223242526272829# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution(object): # 需要有一个 level的index， 然后翻转就行了 def levelOrderBottom(self, root): """ :type root: TreeNode :rtype: List[List[int]] """ res =[] self.dfs(root, 0, res) #res 这个是整体的导致，一层 element的倒置，不涉及 element内部的倒置 return res[::-1] def dfs(self, root, level, res): if root: if len(res) &lt; level+1: res.append([]) # 这个是append 一个空的 [] 这种结构，然后下面使用该 list；否则的话 直接进行append res[level].append(root.val) # 这个很重要哦 self.dfs(root.left, level+1, res) self.dfs(root.right, level +1, res) Convert Sorted Array to Binary Search Tree Given an array where elements are sorted in ascending order, convert it to a height balanced BST.For this problem, a height-balanced binary tree is defined as a binary tree in which the depth of the two subtrees of every node never differ by more than 1. Tips： 不减的array 就是 binary search tree 中的中序遍历的结果。递归思想，先要找到 root，然后划分左右子树。递归进行。 1234567891011121314151617181920212223242526# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution(object): # balance tree 这种是递归进行定义的，左右子树相差最多为1 # 主要是不太清楚 如何保证这种 balanced tree def sortedArrayToBST(self, nums): """ :type nums: List[int] :rtype: TreeNode """ if not nums: return None mid =len(nums)//2 root =TreeNode(nums[mid]) root.left =self.sortedArrayToBST(nums[:mid]) root.right =self.sortedArrayToBST(nums[mid+1:]) return root Balanced Binary Tree Given a binary tree, determine if it is height-balanced.For this problem, a height-balanced binary tree is defined as:a binary tree in which the depth of the two subtrees of every node never differ by more than 1. Tip：左右子树的差值不能大于1 为balanced，这个盘别题目，比较容易，重点是 getHeight() 的实现、根节点是 balanced，左右子树也是balanced 1234567891011121314151617181920212223242526# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution(object): # 使用了之前的 求解 树的height 的东西，然后使用定义进行做题 def isBalanced(self, root): """ :type root: TreeNode :rtype: bool """ if not root: return True return abs(self.getHeight(root.left) -self.getHeight(root.right))&lt;2 and self.isBalanced(root.left) and self.isBalanced(root.right) def getHeight(self, root): if not root: return 0 return 1 +max(self.getHeight(root.left), self.getHeight(root.right)) Minimum Depth of Binary Tree Given a binary tree, find its minimum depth.The minimum depth is the number of nodes along the shortest path from the root node down to the nearest leaf node.Note: A leaf is a node with no children. Tip：求 height的变形，如果是叶子节点，那么返回 1+max(left, right) 否则的话，返回左右子树中较小的高度。如果是求高度，那么就不管了什么情况下都是返回 max()+1 1234567891011121314151617181920212223# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution(object): # 求解根节点的左右子树的最小高度 # 如果左右子树都有，那么就是调用该函数， 如果有一个又，那么直接求解高度就行了 def minDepth(self, root): """ :type root: TreeNode :rtype: int """ if not root: return 0 if not root.left or not root.right: # 这个是求解高度的 return 1 +max(self.minDepth(root.left), self.minDepth(root.right)) else: return min(self.minDepth(root.left), self.minDepth(root.right))+1 Path Sum Given a binary tree and a sum, determine if the tree has a root-to-leaf path such that adding up all the values along the path equals the given sum.Note: A leaf is a node with no children. Tips: 有条件的dfs(), 有条件的进行树的路径，树在加深的同时，target 数字也是不断的减少，最后如果相等，那么就是一个合适的解。 12345678910111213141516171819202122232425262728293031323334353637383940# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution(object): # 树的路径 # 总结一下 any all 这种到自己的博客 """ any: Returns true if any of the items is True. It returns False if empty or all are false. Any can be thought of as a sequence of OR operations on the provided iterables. all: Returns true if all of the items are True (or if the iterable is empty). All can be thought of as a sequence of AND operations on the provided iterables. It also short circuit the execution i.e. stop the execution as soon as the result is known. """ def hasPathSum(self, root, sum): """ :type root: TreeNode :type sum: int :rtype: bool """ res =[] self.dfs(root, sum, res) return any(res) def dfs(self, root, target, res): if not root: return False # 对于叶子结点的定义 if not root.left and not root.right: if root.val == target: res.append(True) if root.left: self.dfs(root.left, target-root.val, res) if root.right: self.dfs(root.right, target-root.val, res) Path Sum II Given a binary tree and a sum, find all root-to-leaf paths where each path’s sum equals the given sum.Note: A leaf is a node with no children. Tips： 这个和上面的区别在于，一个是 true or false，一个find all paths，所以需要有一个变量去存储正确的路径。 12345678910111213141516171819202122232425262728293031# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution(object): # 上一道题目是 return true or false，这个是找到所有的路径 def pathSum(self, root, sum): """ :type root: TreeNode :type sum: int :rtype: List[List[int]] """ res =[] self.dfs(root, sum, [], res) return res def dfs(self, root, target, path, res): if not root: return [] if not root.left and not root.right: if root.val == target: res.append(path+[root.val]) if root.left: #这种条件是可以减少迭代的次数 self.dfs(root.left, target-root.val, path+[root.val], res) if root.right: self.dfs(root.right, target-root.val, path+[root.val], res) Flatten Binary Tree to Linked List Given a binary tree, flatten it to a linked list in-place. Tips： 比较有意思，将 tree 的左右子树 flatten 成 linked list的左右结点。其中的 self.pre 就类似一种全局变量，将整个遍历， 1234567891011121314151617181920212223242526272829# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution(object): # 这种 flatten 就是 拉平（先序遍历）， 然后转成linkedlist # 并且这种操作是要求 in-place的 def __init__(self): self.pre =TreeNode('dummy') def flatten(self, root): """ :type root: TreeNode :rtype: None Do not return anything, modify root in-place instead. """ if not root: return tmp =root.right # 这个保存下来，是为了下面的flatten 使用 self.pre.right =root self.pre.left =None self.pre =root self.flatten(root.left) self.flatten(tmp) Populating Next Right Pointers in Each Node You are given a perfect binary tree where all leaves are on the same level, and every parent has two children. The binary tree has the following definition: Tip，属于树的结构的优化，多了一个next 指针指向的是同层的右节点。这个树的操作一般是 in-place，所以在某个递归过程中 return 是不必return value，本生就是在修改。 123456789101112131415161718192021222324252627282930"""# Definition for a Node.class Node(object): def __init__(self, val, left, right, next): self.val = val self.left = left self.right = right self.next = next"""class Solution(object): # perfect binary tree, # 题目的要求， populate each next pointer to its next right node def helper(self, left, right): if not left or not right: return left.next = right # 三种关系，先后顺序是没有关系的 self.helper(left.left, left.right) self.helper(left.right, right.left) self.helper(right.left, right.right) def connect(self, root): if not root: return self.helper(root.left, root.right) return root Populating Next Right Pointers in Each Node II Given a binary tree struct Node { int val; Node left; Node right; Node *next; }Populate each next pointer to point to its next right node. If there is no next right node, the next pointer should be set to NULL. Tips：注意从图片上观察这一题和上一题的区别，这图中表民一个子树的左子树是可以指向另一个子树的右子树，说明这个明显类似层次遍历，而不像上一题那样。 123456789101112131415161718192021222324252627282930313233"""# Definition for a Node.class Node(object): def __init__(self, val, left, right, next): self.val = val self.left = left self.right = right self.next = next"""class Solution(object): # 这个是不太明白的 # 这个相对于上一道题目，只是少了 perfect binary tree, 翻译成中文，满二叉树（完美二叉树），包括最后一层都是满的 def connect(self, root): if root is None: return None queue = [root] while queue: prev,curr = None,None size = len(queue) # 有点类似层次遍历的意思 for i in range(size): curr = queue.pop(0) # 这个 if 只有在for 之内才是有效的，第一次是无效的 if prev : prev.next = curr if curr.left: queue.append(curr.left) if curr.right: queue.append(curr.right) prev = curr curr.next = None return root Binary Tree Maximum Path Sum Given a non-empty binary tree, find the maximum path sum.For this problem, a path is defined as any sequence of nodes from some starting node to any node in the tree along the parent-child connections. The path must contain at least one node and does not need to go through the root. Input: [-10,9,20,null,null,15,7] -10 / \ 9 20 / \ 15 7Output: 42 Tips： 这个的难点在于，可以从任意点开始，然后再任意点结束，并且过不过根节点都是可以的。 分制到底部，在返回的时候传入左右任意一遍最大值加上目前root.val:cur = max(left, right) + root.val 这种情况处理了从Root到左右任意一边的最大值，也就是 root.val + left 和 root.val + right； 还有一种情况就是当最大值 = root.val + left + right， 我们在放入global变量的时候何其比较。 对于最底部叶子节点传上来的值，我们将其设置成0: return cur if cur &gt; 0 else 0 12345678910111213141516171819202122232425262728# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution(object): # 根据以往的经验，树的递归解法一般都是递归到叶节点，然后开始边处理边回溯到根节点。 # 但是这个题目不是， 这个是可以任意 start， 任意 end，然后不一定要经过根节点 def maxPathSum(self, root): """ :type root: TreeNode :rtype: int """ # 使用 self 标志 就意味这个是一种全局的变量， 类似在 init 中进行初始化的 self.res = - float('inf') self.dfs(root) return self.res def dfs(self, root): if not root: return 0 left = self.dfs(root.left) right = self.dfs(root.right) self.res = max(self.res, left + right + root.val) cur = max(left, right) + root.val return cur if cur &gt; 0 else 0 Sum Root to Leaf Numbers Given a binary tree containing digits from 0-9 only, each root-to-leaf path could represent a number.An example is the root-to-leaf path 1-&gt;2-&gt;3 which represents the number 123.Find the total sum of all root-to-leaf numbers.Note: A leaf is a node with no children. Input: [1,2,3] 1 / \ 2 3Output: 25Explanation:The root-to-leaf path 1-&gt;2 represents the number 12.The root-to-leaf path 1-&gt;3 represents the number 13.Therefore, sum = 12 + 13 = 25. Tips: 路径组成的数字代表一个数字，然后所有的路径和相加起来。关键代码只要 cur =pre*10 + root.val， 还是树的路径的遍历吧。 123456789101112131415161718192021222324252627282930313233343536# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution(object): # 对于树的 类型，大概就是这样了， 递归，找出递归的跳出的条件，然后处理保存结果 def sumNumbers(self, root): """ :type root: TreeNode :rtype: int """ self.result =0 self.sumNum(root, 0) return self.result def sumNum(self, root, pre): if not root: return cur = pre *10 +root.val if not root.left and not root.right: self.result += cur return if root.left: self.sumNum(root.left, cur) if root.right: self.sumNum(root.right, cur) Binary Tree Preorder Traversal Given a binary tree, return the preorder traversal of its nodes’ values. Tips：非递归版本（迭代），使用栈（递归的思想就是栈的思想）。 1234567891011121314151617181920212223242526272829# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution(object): # Recursive solution is trivial, could you do it iteratively? def preorderTraversal(self, root): """ :type root: TreeNode :rtype: List[int] """ if not root: return [] res, queue =[], [root] while queue: cur =queue.pop() if cur: res.append(cur.val) queue.append(cur.right) queue.append(cur.left) #queue.append(cur.right) return res LRU Cache Design and implement a data structure for Least Recently Used (LRU) cache. It should support the following operations: get and put.get(key) - Get the value (will always be positive) of the key if the key exists in the cache, otherwise return -1.put(key, value) - Set or insert the value if the key is not already present. When the cache reached its capacity, it should invalidate the least recently used item before inserting a new item.The cache is initialized with a positive capacity. Tips：这个题目纯粹手解，太麻烦了，在python3 中有 collections.OrderedDict() 的实现，这是作弊的写法。特点在于dict +队列（不完全是队列，因为访问之后还会放到队列的最后，而不是弹出）。因为一般的dict 存储的时候是无序（不是按照放入的先后书序），ordereddict 是按照放入的先后顺序进行存储的。题目本身就是先进先出的队列，只不过存储的是 (key, value) 这样的键值对。使用get 的时候，get到一个不能删除，应该放到最后；put的时候在 OrderedDict.popitem()有一个可选参数last（默认为True），当last为True时它从OrderedDict中删除最后一个键值对并返回该键值对，当last为False时它从 OrderedDict中删除第一个键值对并返回该键值对。 123456789101112131415161718192021222324252627282930class LRUCache(object): # python3 environment def __init__(self, capacity): self.size =capacity self.cache = collections.OrderedDict() def get(self, key): if key not in self.cache: return -1 val =self.cache[key] self.cache.move_to_end(key) # Python &gt;= 3.2 return val def put(self, key, val): if key in self.cache: del self.cache[key] self.cache[key] =val if len(self.cache) &gt; self.size: self.cache.popitem(last= False) # Your LRUCache object will be instantiated and called as such:# obj = LRUCache(capacity)# param_1 = obj.get(key)# obj.put(key,value) Insertion Sort List Sort a linked list using insertion sort. Tips： linkedlist 擅长于修改元素（直接修改指向），其中的 if while 是经常搭配使用，发现.. 然后就处理… 1234567891011121314151617181920212223242526272829303132# Definition for singly-linked list.# class ListNode(object):# def __init__(self, x):# self.val = x# self.next = Noneclass Solution(object): # 前插 def insertionSortList(self, head): """ :type head: ListNode :rtype: ListNode """ p =dummy =ListNode(0) cur =dummy.next =head while cur and cur.next: next_val =cur.next.val if cur.val &lt;= next_val: cur =cur.next continue # the sequence is not sorted inorder # find the proper situation # 从头开始找 if p.next.val &gt; next_val: p =dummy while p.next.val &lt;= next_val: p =p.next p.next, cur.next.next, cur.next =cur.next, p.next, cur.next.next return dummy.next Sort List Sort a linked list in O(n log n) time using constant space complexity. Input: 4-&gt;2-&gt;1-&gt;3 Output: 1-&gt;2-&gt;3-&gt;4 Tips: mergesort 的思想，显示把list 分成left and right（分），然后最后merge 算法 1234567891011121314151617181920212223242526272829303132333435# Definition for singly-linked list.# class ListNode(object):# def __init__(self, x):# self.val = x# self.next = Noneclass Solution(object): def merge(self, h1,h2): dummy =tail =ListNode(-1) while h1 and h2: if h1.val &lt; h2.val: tail.next, h1 =h1, h1.next else: tail.next, h2 =h2, h2.next tail =tail.next tail.next =h1 or h2 return dummy.next def sortList(self, head): if not head or not head.next: return head pre, slow, fast =None, head, head # slow fast 直接是两种快慢的不影响的index 遍历方式，slow 是下一个链表的结点 while fast and fast.next: pre, slow, fast =slow, slow.next, fast.next.next pre.next =None # 下面的两种写法是等价的 return self.merge(self.sortList(head), self.sortList(slow)) # return self.merge(*map(self.sortList, (head, slow))) Number of Islands Given a 2d grid map of ‘1’s (land) and ‘0’s (water), count the number of islands. An island is surrounded by water and is formed by connecting adjacent lands horizontally or vertically. You may assume all four edges of the grid are all surrounded by water. Tips: 这个dfs 跟之前的不一样之处在于，需要对于每个点进行 dfs() ，其他的还好。提供了两种解法，第一种比较代码比较少。比较喜欢第一种代码的风格，这样两个函数看起来比较均衡。 https://leetcode.com/problems/number-of-islands/ 解法一：1234567891011121314151617181920212223242526272829class Solution(object): def numIslands(self, grid): """ :type grid: List[List[str]] :rtype: int """ if not grid: return 0 count =0 for i in range(len(grid)): for j in range(len(grid[0])): if grid[i][j] =='1': self.dfs(grid, i,j) # 写法比较巧妙 count +=1 return count def dfs(self, grid, i,j): if i &lt;0 or j&lt;0 or i&gt;=len(grid) or j&gt;= len(grid[0]) or grid[i][j] !='1': return grid[i][j] ='0' self.dfs(grid, i+1, j) self.dfs(grid, i-1, j) self.dfs(grid, i, j+1) self.dfs(grid, i, j-1) 解法二：两种思想一样，写法不一样。 12345678910111213141516171819202122232425262728293031323334353637383940class Solution(object): def numIslands(self, grid): """ :type grid: List[List[str]] :rtype: int """ if not grid: return 0 used =[ [False]* len(grid[0]) for _ in range(len(grid))] count =0 for i in range(len(grid)): for j in range(len(grid[0])): num =self.dfs(grid, used, len(grid)-1, len(grid[0])-1, i, j) if num&gt;0: count +=1 return count def dfs(self, grid, used, row, col, x, y): if grid[x][y] =='0' or used[x][y]: return 0 used[x][y] =True num =1 if x!=0: num += self.dfs(grid, used, row, col, x -1, y) if x !=row: num += self.dfs(grid, used, row, col, x +1,y) if y!=0: num += self.dfs(grid, used, row, col, x, y-1) if y !=col: num += self.dfs(grid, used, row, col, x, y+1) return num Super Egg Drop You are given K eggs, and you have access to a building with N floors from 1 to N.Each egg is identical in function, and if an egg breaks, you cannot drop it again.You know that there exists a floor F with 0 &lt;= F &lt;= N such that any egg dropped at a floor higher than F will break, and any egg dropped at or below floor F will not break. 1234567891011121314151617181920212223class Solution(object): def gameOfLife(self, board): # 纯粹的count，之后的判断是下面决定的 def count(x, y): res =0 # 遍历 点的四周 for r in range(x-1, x+2): for c in range(y-1, y+2): if (r!= x or c!=y) and 0&lt;= r &lt; len(board) and 0&lt;= c &lt; len(board[0]) and board[r][c] &gt;0: res +=1 return res for x in range(len(board)): for y in range(len(board[0])): board[x][y] =count(x, y) +1 if board[x][y] ==1 else -count(x, y) # if board[x][y] == 1, change its value to count(x,y) + 1, the reason I add 1 is to keep it positive for x in range(len(board)): for y in range(len(board[0])): board[x][y] = 1 if board[x][y] in &#123;3, 4, -3&#125; else 0 # &#123;2, 3, -3&#125; Kth Smallest Element in a BST Given a binary search tree, write a function kthSmallest to find the kth smallest element in it. Tips: 二分查找树，中序遍历就是不减的list .有递归，迭代两个版本，共三种实现。倾向于使用第二个版本。迭代，然后使用k 进行及时的跳出。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475# Definition for a binary tree node.class TreeNode(object): def __init__(self, x): self.val = x self.left = None self.right = Noneclass Solution(object): # BST 中序遍历 得到一个不减的list，然后就可得第k 小的元素 # 下面是递归版本 ''' def kthSmallest(self, root, k): """ :type root: TreeNode :type k: int :rtype: int """ if not root: return res =[] self.dfs(root, res) if len(res) +1&lt;k: return return res[k-1] def dfs(self, root, res): if not root: return #res.append(root.val) self.dfs(root.left, res) res.append(root.val) self.dfs(root.right, res) ''' ''' # 相比于第一种方式，时间上是有减少的 def kthSmallest(self, root,k ): stack =[] node =root while True: if node: stack.append(node) node =node.left else: node =stack.pop() # 使用计数的方式进行访问，减少了空间复杂度 k -=1 if not k: break node =node.right return node.val ''' # 这个代码就是有点 抖机灵的那种，如果使用了 try ..catch.. 那么exception 就不会报错 def kthSmallest(self, root, k): def inorder(root, k): if root: inorder(root.left, k) if k ==1: raise Exception(root.val) inorder(root.right, k-1) #return k try: inorder(root, k) except Exception as e: return e.message Lowest Common Ancestor of a Binary Tree Given a binary tree, find the lowest common ancestor (LCA) of two given nodes in the tree.According to the definition of LCA on Wikipedia: “The lowest common ancestor is defined between two nodes p and q as the lowest node in T that has both p and q as descendants (where we allow a node to be a descendant of itself).” Given the following binary tree: root = [3,5,1,6,2,0,8,null,null,7,4] Tips: 这道Follow Up没有BST的特性，所以要对几种case一个一个进行测试。Condition为两种：如果没找到，返回None，找到则返回当前的root(因为找到一个root就不需要继续深入)比对方式： 如果parent的左右孩子都有返回，说明parent就是LCA 如果左边没有返回：则右边返回的就是LCA 如果右边没有返回：则左边返回的就是LCA 讲解 https://leetcode.com/problems/lowest-common-ancestor-of-a-binary-tree/ 1234567891011121314151617181920212223242526272829# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution(object): def lowestCommonAncestor(self, root, p, q): """ :type root: TreeNode :type p: TreeNode :type q: TreeNode :rtype: TreeNode """ if not root: return None if p ==root or q ==root: return root left =self.lowestCommonAncestor(root.left, p, q) right =self.lowestCommonAncestor(root.right, p, q) if left and right: return root if not left: return right if not right: return left Serialize and Deserialize Binary Tree Serialization is the process of converting a data structure or object into a sequence of bits so that it can be stored in a file or memory buffer, or transmitted across a network connection link to be reconstructed later in the same or another computer environment. Tips: 序列化主要是用在 存储和传输上吧. 基于 队列进行实现。队列可以两边进行修改。先序遍历 https://leetcode.com/problems/serialize-and-deserialize-binary-tree/ 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Codec: # 先序遍历 def serialize(self, root): if not root: return "" queue =collections.deque([root]) res =[] # 这个就是一种循环先序遍历二叉树 while queue: # 使用 pop 和 append() 操作右边 node =queue.popleft() # 在队列中使用 popleft 和appendleft() 直接操作队列的左边的增减 if node: queue.append(node.left) queue.append(node.right) res.append(str(node.val) if node else '#') # 使用 # 表示是一种none return ','.join(res) # 使用, 隔开每个node def deserialize(self, data): if not data: return None nodes =data.split(',') root =TreeNode(int(nodes[0])) queue =collections.deque([root]) index =1 # 作为string 的index while queue: node =queue.popleft() if nodes[index] != "#": # nodes[index] is not '#' 这样写也是可以的 node.left =TreeNode(int(nodes[index])) queue.append(node.left) index +=1 if nodes[index] != "#": node.right =TreeNode(int(nodes[index])) queue.append(node.right) index +=1 return root # Your Codec object will be instantiated and called as such:# codec = Codec()# codec.deserialize(codec.serialize(root)) Binary Tree Maximum Path Sum Given a non-empty binary tree, find the maximum path sum.For this problem, a path is defined as any sequence of nodes from some starting node to any node in the tree along the parent-child connections. The path must contain at least one node and does not need to go through the root. Tips: 这个不是树的路径，可以从任意非根节点出发。 1234567891011121314151617181920212223242526272829# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution(object): # 根据以往的经验，树的递归解法一般都是递归到叶节点，然后开始边处理边回溯到根节点。 # 但是这个题目不是， 这个是可以任意 start， 任意 end，然后不一定要经过根节点 def maxPathSum(self, root): """ :type root: TreeNode :rtype: int """ # 这种全局变量的设置确实是必须的，当携带变量的时候就出错了 self.res = - float('inf') self.dfs(root) return self.res def dfs(self, root): if not root: return 0 left = self.dfs(root.left) right = self.dfs(root.right) self.res = max(self.res, left + right + root.val) cur = max(left, right) + root.val return cur if cur &gt; 0 else 0 Number of Islands Given a 2d grid map of ‘1’s (land) and ‘0’s (water), count the number of islands. An island is surrounded by water and is formed by connecting adjacent lands horizontally or vertically. You may assume all four edges of the grid are all surrounded by water. Tips: dfs 12345678910111213141516171819202122232425262728293031class Solution(object): # dfs 是一个中规中矩的算法 # 这种方式更加简洁一点，直接使用 grid[i][j] 是否等于1 进行操作， # 然后如果能返回，在主程序中进行计数，最后的结果比较nice def numIslands(self, grid): """ :type grid: List[List[str]] :rtype: int """ if not grid: return 0 count =0 for i in range(len(grid)): for j in range(len(grid[0])): if grid[i][j] =='1': self.dfs(grid, i,j) count +=1 return count def dfs(self, grid, i, j): if i&lt;0 or j&lt;0 or i&gt;=len(grid) or j &gt;=len(grid[0]) or grid[i][j]!='1': return grid[i][j] ='#' self.dfs(grid, i+1, j) self.dfs(grid, i-1, j) self.dfs(grid, i,j +1) self.dfs(grid, i, j-1) Course Schedule There are a total of n courses you have to take, labeled from 0 to n-1.Some courses may have prerequisites, for example to take course 0 you have to first take course 1, which is expressed as a pair: [0,1]Given the total number of courses and a list of prerequisite pairs, is it possible for you to finish all courses? Tips: dfs 先修课程 12345678910111213141516171819202122232425262728293031323334353637383940414243class Solution(object): """ 这种解释是比较nice的，使用 0 -1 和1 分别表示初始化，正在访问和已经完成 if node v has not been visited, then mark it as 0. if node v is being visited, then mark it as -1. If we find a vertex marked as -1 in DFS, then their is a ring. if node v has been visited, then mark it as 1. If a vertex was marked as 1, then no ring contains v or its successors. """ def canFinish(self, numCourses, prerequisites): """ :type numCourses: int :type prerequisites: List[List[int]] :rtype: bool """ graph = [[] for _ in range(numCourses)] visited = [0 for _ in range(numCourses)] # create graph for pair in prerequisites: x, y = pair graph[x].append(y) # visit each node for i in range(numCourses): if not self.dfs(graph, visited, i): return False return True def dfs(self, graph, visited, i): # if ith node is marked as being visited, then a cycle is found if visited[i] == -1: return False # if it is done visted, then do not visit again if visited[i] == 1: return True # mark as being visited visited[i] = -1 # visit all the neighbours for j in graph[i]: if not self.dfs(graph, visited, j): return False # after visit all the neighbours, mark it as done visited visited[i] = 1 return True Course Schedule II There are a total of n courses you have to take, labeled from 0 to n-1.Some courses may have prerequisites, for example to take course 0 you have to first take course 1, which is expressed as a pair: [0,1]Given the total number of courses and a list of prerequisite pairs, return the ordering of courses you should take to finish all courses.There may be multiple correct orders, you just need to return one of them. If it is impossible to finish all courses, return an empty array. Tips: 和上一题相似，dfs 求解的是路径问题，而不是最值。 1234567891011121314151617181920212223242526272829303132333435363738class Solution(object): # 上一题是true or false 这个题目要求给个能够完成的路径，哎 # 不得不说这个是图的知识点呀 def findOrder(self, numCourses, prerequisites): """ :type numCourses: int :type prerequisites: List[List[int]] :rtype: List[int] """ def dfs(i, visited, graph, res): if visited[i] ==1: return True if visited[i] ==-1: return False visited[i] =-1 for n in graph[i]: if not dfs(n, visited, graph, res): return False res.append(i) visited[i] =1 return True visited =[0] * numCourses graph =&#123;x :[] for x in range(numCourses)&#125; # 注意这个顺序，因为最后要的是路径，所以这样是更加合理的 for p in prerequisites: graph[p[1]].append(p[0]) res =[] for i in range(numCourses): if not dfs(i, visited, graph, res): return [] return res[::-1]]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode-List]]></title>
    <url>%2F2019%2F06%2F16%2Fleetcode-list%2F</url>
    <content type="text"><![CDATA[LeetCode 刷题总结（二）， 使用Python 实现。该篇题目类型主要是： list, linkedList 还有简单的 tree。 Add Two Numbers You are given two non-empty linked lists representing two non-negative integers. The digits are stored in reverse order and each of their nodes contain a single digit. Add the two numbers and return it as a linked list. Input: (2 -&gt; 4 -&gt; 3) + (5 -&gt; 6 -&gt; 4)Output: 7 -&gt; 0 -&gt; 8Explanation: 342 + 465 = 807. Tips: 加法的过程，使用 \% 和 整除进行求解，使用linkedList 进行存储。https://leetcode.com/problems/add-two-numbers/ 123456789101112131415161718192021222324252627# Definition for singly-linked list.# class ListNode(object):# def __init__(self, x):# self.val = x# self.next = Noneclass Solution(object): def addTwoNumbers(self, l1, l2): """ :type l1: ListNode :type l2: ListNode :rtype: ListNode """ dummy = cur = ListNode(0) carry = 0 while l1 or l2 or carry: if l1: carry += l1.val l1 = l1.next if l2: carry += l2.val l2 = l2.next cur.next = ListNode(carry%10) cur = cur.next carry //= 10 return dummy.next Remove Nth Node From End of List Given a linked list, remove the n-th node from the end of list and return its head. Tips: 可以使用”两指针“ 方法进行求解，前后两指针相差 N步数。 12345678910111213141516171819202122232425262728293031323334353637# Definition for singly-linked list.# class ListNode(object):# def __init__(self, x):# self.val = x# self.next = Noneclass Solution(object): def removeNthFromEnd(self, head, n): """ :type head: ListNode :type n: int :rtype: ListNode """ if not head or not head.next: return None new_head =ListNode(-1) new_head.next =head fast =new_head # 先走n 步 for i in range(n): # 这里很好的处理了 n &gt; 链表长度 情况 if fast.next: fast =fast.next else: return head slow =new_head # 一块走 # 因为是 fast 先走到none，所以这个判断条件 while fast.next: fast =fast.next slow =slow.next slow.next =slow.next.next # 这个指向是经验性，还是超级nice的 return new_head.next Merge Two Sorted Lists Merge two sorted linked lists and return it as a new list. The new list should be made by splicing together the nodes of the first two lists. Tips: 归并操作中的”并“ 操作。 https://leetcode.com/problems/merge-two-sorted-lists/ 123456789101112131415161718192021222324252627282930313233343536373839404142# Definition for singly-linked list.# class ListNode(object):# def __init__(self, x):# self.val = x# self.next = Noneclass Solution(object): def mergeTwoLists(self, l1, l2): """ :type l1: ListNode :type l2: ListNode :rtype: ListNode """ if not l1 and not l2: return None elif not l1 or not l2: return l1 or l2 # 这里新建了 node，作为最后返回list 中的head，因为使用哪个子list 都是不确定的 new_node =ListNode(-1) cur =new_node head1 =l1 head2 =l2 while head1 and head2: if head1.val &lt; head2.val: cur.next =head1 head1 =head1.next else: cur.next =head2 head2 =head2.next cur =cur.next # 对于这种 if else 需要是相当的清楚 if head1: cur.next =head1 elif head2: cur.next =head2 return new_node.next Swap Nodes in Pairs Given a linked list, swap every two adjacent nodes and return its head.You may not modify the values in the list’s nodes, only nodes itself may be changed. Given 1-&gt;2-&gt;3-&gt;4, you should return the list as 2-&gt;1-&gt;4-&gt;3. Tips: 常见的类型，使用三个指针修改指向。经常创建 dummy指针，如果head 可能被改变的话。 1234567891011121314151617181920212223242526272829303132# Definition for singly-linked list.# class ListNode(object):# def __init__(self, x):# self.val = x# self.next = Noneclass Solution(object): """ 三个指针的修改，应该是没有问题的 """ def swapPairs(self, head): """ :type head: ListNode :rtype: ListNode """ if not head: return head dummy=pre =ListNode(-1) dummy.next =head while True: cur =pre.next if not cur: break nex =cur.next if not nex: break pre.next, cur.next, nex.next, pre =nex, nex.next, cur, cur return dummy.next Count and Say The count-and-say sequence is the sequence of integers with the first five terms as following: 1. 1 2. 11 3. 21 4. 1211 5. 111221 1 is read off as &quot;one 1&quot; or 11. 11 is read off as &quot;two 1s&quot; or 21. 21 is read off as &quot;one 2, then one 1&quot; or 1211. Tips: 这个是属于循环，字符串处理。 12345678910111213141516171819202122232425262728class Solution(object): def doCountAndSay(self, string): char =string[0] num =0 result ="" for c in string: if char ==c: num +=1 else: result += (str(num)+ char) char =c num =1 result += (str(num) +char) return result def countAndSay(self, n): if 0 ==n: return "" elif 1== n: return "1" result ='1' for i in range(1, n): result =self.doCountAndSay(result) return result Trapping Rain Water Given n non-negative integers representing an elevation map where the width of each bar is 1, compute how much water it is able to trap after raining. Tips: 算法比较巧妙，左右两边进行遍历找出”累计“最高点，在O(N) 时间内完成。能装的水，取决于左右两边( neighbor) 的小值- height[i]。 https://leetcode.com/problems/trapping-rain-water/ 1234567891011121314151617181920212223242526class Solution(object): def trap(self, height): if not height: return 0 len_h =len(height) leftmax=[0]* len_h max_h=0 for i in range(len_h): if height[i] &gt;max_h: max_h =height[i] leftmax[i] =max_h rightmax =[0] *len_h max_h =0 for i in range(len_h-1, -1, -1): if height[i]&gt; max_h: max_h =height[i] rightmax[i] =max_h result =0 for i in range(len_h): result += (min(leftmax[i], rightmax[i])- height[i]) return result Maximum Subarray Given an integer array nums, find the contiguous subarray (containing at least one number) which has the largest sum and return its sum. Input: [-2,1,-3,4,-1,2,1,-5,4], Output: 6 Explanation: [4,-1,2,1] has the largest sum = 6. Tips: 一维数组返回最大的子数组，字符串处理。https://leetcode.com/problems/maximum-subarray/ 1234567891011121314151617181920212223class Solution(object): def maxSubArray(self, nums): """ :type nums: List[int] :rtype: int """ len_n =len(nums) if len_n ==1: return nums[0] max_n =nums[0] # 如果提前初始化，那么之后在更新max_n 时候就不用进行 None 的判断了 sum_n =0 for num in nums: sum_n += num if sum_n&gt; max_n: max_n =sum_n if sum_n &lt;0: sum_n =0 # continue return max_n Insert Interval Given a set of non-overlapping intervals, insert a new interval into the intervals (merge if necessary).You may assume that the intervals were initially sorted according to their start times. Input: intervals = [[1,3],[6,9]], newInterval = [2,5] Output: [[1,5],[6,9]] Tips: 使用( 0, 1) 去区分 start, end 这种点，类似一种数据结构的样子, 排序之后结果的start index 和end index 分别出现在首段和尾段，使用栈进行存储 start index 就行了。https://leetcode.com/problems/insert-interval/ 123456789101112131415161718192021222324252627282930313233343536class Solution(object): def insert(self, intervals, newinterval): """ 这个每个都是有 数据类型的，这个就是 LeetCode 的优点 :type intervals: List[List[int]] :type newInterval: List[int] :rtype: List[List[int]] """ if not intervals: return [newinterval] datas =[] if intervals: datas.append((newinterval[0], 0)) datas.append((newinterval[1], 1)) for interval in intervals: datas.append((interval[0], 0)) datas.append((interval[1], 1)) datas.sort() # sort() 是一个骚操作, 默认的排序显示根据 tuple[0] 进行排序，如果相同，那么根据 tuple[1] 进行排序 # 所以排在前面的一定是 start index。 merged =[] stack =[datas[0]] for i in range(1, len(datas)): data =datas[i] if data[1] ==0: stack.append(data) elif data[1] ==1: if stack: start =stack.pop() if len(stack) ==0: # 这个时候 data 是 end point merged.append((start[0], data[0])) return merged Rotate List Given a linked list, rotate the list to the right by k places, where k is non-negative. Tips： 细节在于k 可能大于 list 的长度。 123456789101112131415161718192021222324252627282930313233343536# Definition for singly-linked list.# class ListNode(object):# def __init__(self, x):# self.val = x# self.next = Noneclass Solution(object): def rotateRight(self, head, k): if k ==0: return head if not head: return head # 头指针 dummy =ListNode(-1) dummy.next =head p =dummy count =0 while p.next: p =p.next count +=1 # 指向了头指针，连成了一个环，下一步开始找头指针 p.next =dummy.next step =count -(k% count) for i in range(step): p =p.next # 找到了头指针，那么下一个就是尾指针 head =p.next p.next =None return head Unique Paths A robot is located at the top-left corner of a m x n grid (marked ‘Start’ in the diagram below).The robot can only move either down or right at any point in time. The robot is trying to reach the bottom-right corner of the grid (marked ‘Finish’ in the diagram below).How many possible unique paths are there? Tips： 最后求解的是unique paths 的数量，而不是具体的路径，所以可以不使用 dp，成了一道模拟排列组合的数学题。 12345678910111213141516171819202122class Solution(object): # 这个总的步数是一定的 m+n -1 ，然后下行和右行也是一定的， # 所以这个是模拟的“排列组合” 的思想 def uniquePaths(self, m, n): """ :type m: int :type n: int :rtype: int """ if m ==0 or n ==0: return 1 up =1 # 这个是分子 for i in range(m+n-2, n -1, -1): up *=i down =1 # 这个是分母 for j in range(1, m): down *= j return up/down Sort Colors Given an array with n objects colored red, white or blue, sort them in-place so that objects of the same color are adjacent, with the colors in the order red, white and blue.Here, we will use the integers 0, 1, and 2 to represent the color red, white, and blue respectively. Tips: 快排思想， 中间数字1 当做key index，左右两边分别是left，right index。 1234567891011121314151617class Solution(object): """ 0, 1, 2 (red, white, blue) """ def sortColors(self, nums): # zero and r record the position of "0" and "2" respectively index, two, zero = 0, len(nums) - 1, 0 while index &lt;= two: if nums[index] == 0: nums[index], nums[zero] = nums[zero], nums[index] index += 1; zero += 1 elif nums[index] == 2: nums[index], nums[two] = nums[two], nums[index] two -= 1 else: index += 1 Remove Duplicates from Sorted List II Given a sorted linked list, delete all nodes that have duplicate numbers, leaving only distinct numbers from the original list. Tips: 常规题，经常出现这样的逻辑， if… while ，如果发现有重复的，那么一直就找到不重复为止。 12345678910111213141516171819202122232425262728# Definition for singly-linked list.# class ListNode(object):# def __init__(self, x):# self.val = x# self.next = Noneclass Solution(object): """ 就是在删除节点的时候，如果head 节点也得删除，这个时候 常常创建一个 dummy 结点 求解的是distinct 的list """ def deleteDuplicates(self, head): dummy =pre =ListNode(0) dummy.next =head while head and head.next: if head.val ==head.next.val: while head and head.next and head.val ==head.next.val: head =head.next head =head.next pre.next =head else: # 这个更新很有意思， head =head.next, pre.next =head pre =pre.next head =head.next return dummy.next Partition List Given a linked list and a value x, partition it such that all nodes less than x come before nodes greater than or equal to x.You should preserve the original relative order of the nodes in each of the two partitions. Tips: 新建了两个结点，分别连接小于 x 和不小于 x 的结点，最后两个结点相连。 list 是直接进行交换位置，但是linkedList 不是这样的。 123456789101112131415161718192021222324252627282930# Definition for singly-linked list.# class ListNode(object):# def __init__(self, x):# self.val = x# self.next = Noneclass Solution(object): # 局部排序，不是全局排序 def partition(self, head, x): """ :type head: ListNode :type x: int :rtype: ListNode """ small =l1 =ListNode(0) great =l2 =ListNode(0) while head: if head.val &lt;x : l1.next =head l1 =l1.next else: l2.next =head l2 =l2.next head =head.next # 这个是一个细节， 最后l2 是需要一个none 进行结束标记 l2.next =None l1.next =great.next return small.next Reverse Linked List II Reverse a linked list from position m to n. Do it in one-pass.Note: 1 ≤ m ≤ n ≤ length of list. Tips: 局部进行reverse，找到该节点，然后迭代进行就可以了。 1234567891011121314151617181920212223242526# Definition for singly-linked list.# class ListNode(object):# def __init__(self, x):# self.val = x# self.next = Noneclass Solution(object): # The idea is simple and intuitive: find linkedlist [m, n], reverse it, then connect m with n+1, connect n with m-1 def reverseBetween(self, head, m, n): pre =dummy =ListNode(0) dummy.next =head #cur, pre =head, dummy for _ in range(m-1): #cur =cur.next pre =pre.next cur =pre.next for _ in range(n-m): tmp =cur.next cur.next =tmp.next tmp.next =pre.next pre.next =tmp return dummy.next Unique Binary Search Trees Given n, how many structurally unique BST’s (binary search trees) that store values 1 … n? Tips: 从处理子问题的角度来看，选取一个结点为根，就把结点切成左右子树，以这个结点为根的可行二叉树数量就是左右子树可行二叉树数量的乘积，所以总的数量是将以所有结点为根的可行结果累加起来。 123456789101112131415161718class Solution(object): # 二叉搜索树，当且仅当中序遍历的时候是单调非减的时候。 # 数学问题 def numTrees(self, n): """ :type n: int :rtype: int """ arr =[0]*(n+1) arr[0] =1 for i in range(1, n+1): for j in range(1, i+1): # 处理的是左右子树的乘积 arr[i] += arr[j-1] *arr[i-j] return arr[-1] Best Time to Buy and Sell Stock Say you have an array for which the ith element is the price of a given stock on day i.If you were only permitted to complete at most one transaction (i.e., buy one and sell one share of the stock), design an algorithm to find the maximum profit.Note that you cannot sell a stock before you buy one. Tips: 虽然这个说是最多一次买入卖出，但是这个价格变化是”连续“的，所以只要是下一个大于上一个就是可以 += profit 中的。 12345678910111213141516class Solution(object): # 从代码上来看，毫无算法可言 def maxProfit(self, prices): """ :type prices: List[int] :rtype: int """ if not prices or len(prices) ==1: return 0 profit =0 for i in range(1, len(prices)): if prices[i] &gt; prices[i-1]: profit += prices[i]-prices[i-1] return profit Best Time to Buy and Sell Stock II Say you have an array for which the ith element is the price of a given stock on day i.Design an algorithm to find the maximum profit. You may complete as many transactions as you like (i.e., buy one and sell one share of the stock multiple times).Note: You may not engage in multiple transactions at the same time (i.e., you must sell the stock before you buy again). Tips: 可以进行多次买卖。和上面的区别在于 下一个只要不小于上一个就是可以累加的。 123456789101112131415class Solution(object): # 可以多次买卖， 买一次然后卖一次。不能多次买入 # 这种就如同寻找的是 增序列。 def maxProfit(self, prices): """ :type prices: List[int] :rtype: int """ total =0 for i in range(1, len(prices)): if prices[i]&gt;= prices[i-1]: total += prices[i]-prices[i-1] return total Longest Consecutive Sequence Given an unsorted array of integers, find the length of the longest consecutive elements sequence.Your algorithm should run in O(n) complexity. Tips： 这种方法很巧妙，如果x-1 not in，那么去 try x+1，然后计数。 123456789101112131415161718class Solution(object): # 第一印象是 先进行排序， 然后选择的过程， 但是限制条件是 O(n) 的复杂度 def longestConsecutive(self, nums): """ :type nums: List[int] :rtype: int """ nums =set(nums) best =0 for x in nums: if x-1 not in nums: y =x+1 while y in nums: y +=1 best =max(best, y-x) return best Candy There are N children standing in a line. Each child is assigned a rating value.You are giving candies to these children subjected to the following requirements: Each child must have at least one candy. Children with a higher rating get more candies than their neighbors.What is the minimum candies you must give? Tips: 因为涉及到 neighbors，所以左右两边进行遍历，因为如果ratings 大的话，那么结果一定得大，所以返回的是较大者。 1234567891011121314151617181920class Solution(object): def candy(self, ratings): """ :type ratings: List[int] :rtype: int """ # 满足第一个条件，at least one candy res =len(ratings) *[1] # left to right, higher then more candies for i in range(1, len(ratings)): if ratings[i] &gt; ratings[i-1]: # 这个是严格的 &gt; res[i] = res[i-1] +1 # right to left, higher then more candy, neighbors for i in range(len(ratings)-1, 0, -1): if ratings[i-1] &gt; ratings[i]: res[i-1] =max(res[i-1], res[i] +1) return sum(res) Single Number Given a non-empty array of integers, every element appears twice except for one. Find that single one. Tips: 异或的性质 1234567891011121314class Solution(object): # 分分钟 异或就出来了 # integers, -2 -1 0 1 2 这样的数字 def singleNumber(self, nums): """ :type nums: List[int] :rtype: int """ standard =0 # 如果正好是其中的 0 出现一次，也是没有关系的， 因为初始化的 0 和 list 中的单数 0 正好匹配，异或操作之后相同为 0 # 结果上是没有什么问题的 for num in nums: standard = num^ standard return standard Single Number II Given a non-empty array of integers, every element appears three times except for one, which appears exactly once. Find that single one. Tips：这个 three times不能使用 异或，从二进制的角度进行考虑，以二进制的形式，将数字存储起来，如果是出现了 3次，那么 %3 结果就是0，最后只是剩下了 那个出现一次的数字 1234567891011121314151617181920212223242526class Solution(object): # 只是出现的一个的single one， 其他的出现三次 # var |= value is short for var = var | value def singleNumber(self, nums): """ :type nums: List[int] :rtype: int """ bit = [0] * 32 for num in nums: for i in range(32): bit[i] += num &gt;&gt; i &amp; 1 # 这个就是从左往右的顺序，先是进行 &gt;&gt; 运算，然后是 &amp; 运算 # 可以想象这个重复计算比较多，因为每次都需要 num &gt;&gt; i 进行位运算 res = 0 for i, val in enumerate(bit): # if the single numble is negative, # this case should be considered separately , 补码 和原码的转换关系 if i == 31 and val % 3: res = -((1 &lt;&lt; 31) - res) else: res |= (val % 3) * (1 &lt;&lt; i) # | 这个是位操作，更加类似不断的取 1 的过程， 然后和该位置的权重相乘 return res Copy List with Random Pointer A linked list is given such that each node contains an additional random pointer which could point to any node in the list or null.Return a deep copy of the list. Tips：在剑指offer 上是通过 指针操作进行做题，但是使用 defaultdict 基本上就不用出，使用dict 来处理这种关系，最后返回的是根节点、 123456789101112131415161718192021222324252627282930"""# Definition for a Node.class Node(object): def __init__(self, val, next, random): self.val = val self.next = next self.random = random"""# 使用dict 有没有感觉在作弊class Solution(object): # 做过这个 def copyRandomList(self, head): """ :type head: Node :rtype: Node """ import ipdb dic = collections.defaultdict(lambda: Node(0, None, None)) # 这个就是给定了一个默认的值, 直接初始化dict 中的value 为这个node # dict 的本身就是存储一种node 的关系，所以dict[n].val , next, random 可以这样进行操作 dic[None] = None n = head while n: dic[n].val = n.val dic[n].next = dic[n.next] dic[n].random = dic[n.random] n = n.next #ipdb.set_trace() return dic[head] Linked List Cycle Given a linked list, determine if it has a cycle in it.To represent a cycle in the given linked list, we use an integer pos which represents the position (0-indexed) in the linked list where tail connects to. If pos is -1, then there is no cycle in the linked list. Tips：快慢两个指针的问题，给了两种方法来实现。 1234567891011121314151617181920212223242526272829303132333435# Definition for singly-linked list.# class ListNode(object):# def __init__(self, x):# self.val = x# self.next = Noneclass Solution(object): # O(1) == constant memory, def hasCycle(self, head): """ :type head: ListNode :rtype: bool """ ''' # 这种不用进行判断 fast 是否可以访问的原因在于 try except 的使用 try: slow = head fast = head.next while slow is not fast: slow = slow.next fast = fast.next.next return True except: return False ''' # 这个是比较中规中矩的写法 slow = fast = head # 注意使用的是 fast进行判断，因为这个走的快 while fast and fast.next: fast = fast.next.next slow = slow.next if slow == fast: return True return False Reorder List Given a singly linked list L: L0→L1→…→Ln-1→Ln,reorder it to: L0→Ln→L1→Ln-1→L2→Ln-2→…You may not modify the values in the list’s nodes, only nodes itself may be changed. Tips： 从中间断开，后半部分翻转，然后和前半部分轮流连接 12345678910111213141516171819202122232425262728293031323334353637383940414243# Definition for singly-linked list.# class ListNode(object):# def __init__(self, x):# self.val = x# self.next = Noneclass Solution(object): def reorderList(self, head): """ :type head: ListNode :rtype: None Do not return anything, modify head in-place instead. """ if not head: return fast, slow = head.next, head # first part has the same or one more node while fast and fast.next: fast = fast.next.next slow = slow.next # reverse the send half p = slow.next slow.next = None node = None # 类似上一个结点， p 是cur 的结点 while p: nex = p.next p.next = node node = p p = nex # combine head part and node part p = head while node: tmp = node.next node.next = p.next # 两个 next 指向操作, 需要next 两次 p.next = node p =p.next.next node = tmp ** Majority Element Given an array of size n, find the majority element. The majority element is the element that appears more than ⌊ n/2 ⌋ times. Output: 2Input: K = 1, N = 2Explanation:Drop the egg from floor 1. If it breaks, we know with certainty that F = 0.Otherwise, drop the egg from floor 2. If it breaks, we know with certainty that F = 1.If it didn’t break, then we know with certainty F = 2.Hence, we needed 2 moves in the worst case to know what F is with certainty. Tips: 可以查看 solution 中的(讲解)[https://leetcode.com/problems/super-egg-drop/] 123456789101112131415161718class Solution(object): def majorityElement(self, nums): """ :type nums: List[int] :rtype: int """ # 初始化 counts =0 for num in nums: if counts ==0: majority =num counts =1 elif majority ==num: counts +=1 else: counts -=1 return majority Maximum Product Subarray Given an integer array nums, find the contiguous subarray within an array (containing at least one number) which has the largest product. Tips: 在于左右两遍遍历，分别得到 prefix和 suffix 的乘积。这个速度上比较快在于存储了之前的结果。实现的时候利用了 1 or prefix[i-1] 这种技巧。 12345678910111213class Solution(object): def maxProduct(self, nums): """ :type nums: List[int] :rtype: int """ prefix =nums suffix =prefix[::-1] for i in range(1, len(prefix)): prefix[i] *= 1 or prefix[i-1] suffix[i] *= 1 or suffix[i-1] return max(prefix+ suffix) Majority Element Given an array of size n, find the majority element. The majority element is the element that appears more than ⌊ n/2 ⌋ times. You may assume that the array is non-empty and the majority element always exist in the array. Tips: majority 的counts 的总数是大于其他所有counts相加之和的 123456789101112131415161718class Solution(object): def majorityElement(self, nums): """ :type nums: List[int] :rtype: int """ majority =None counts =0 for num in nums: if not majority or counts ==0: majority =num counts =1 elif num ==majority: counts +=1 else: counts -=1 return majority Rotate Array Given an array, rotate the array to the right by k steps, where k is non-negative. Tips： 对于python 而言，是不存在切分字符串算法的，一步操作。小的细节是 k %len(array) 更加合理 12345678910111213class Solution(object): def rotate(self, nums, k): """ :type nums: List[int] :type k: int :rtype: None Do not return anything, modify nums in-place instead. """ # 这个in-place 操作是不需要 return k =k%len(nums) # 这个是一个细节吧 #nums[:] =nums[-k:] +nums[:k+1] nums[:] =nums[-k:] +nums[:-k] # 左边有时候是nums 就行，有时候必须nums[:] 表示index的操作，因为环境的问题 # 这种 对称的切分真的是比较好看 Contains Duplicate Given an array of integers, find if the array contains any duplicates.Your function should return true if any value appears at least twice in the array, and it should return false if every element is distinct. Tips: 有很多种方法，比如 dictionary or set，这个简单之处最后返回的是 true or false，不是要找出来。 方法一：使用set，根据length判断。 1234567891011class Solution(object): def containsDuplicate(self, nums): """ :type nums: List[int] :rtype: bool """ # 想法一，排序之后判断， # 想法二：使用dictionary, 在建立的过程中就可以判断，没有必要建立完之后遍历，from collections import Counter # 想法三： 使用set，道理和dictionary 基本上是相同的 return len(nums) !=len(set(nums)) 方法二：使用dictionary，不需要建完之后再判断。 ···pythonclass Solution(object): def containsDuplicate(self, nums): “”” :type nums: List[int] :rtype: bool “”” dic ={} for num in nums: if num in dic: return True else: dic[num] =1 return False 12345678910111213141516171819202122232425** Move Zeroes **&gt; Given an array nums, write a function to move all 0&apos;s to the end of it while maintaining the relative order of the non-zero elements.&gt; Input: [0,1,0,3,12]Output: [1,3,12,0,0]Tips: 双指针问题，pre 指向的是0 ，index是遍历的发现如果不是0，那么进行操作```pythonclass Solution(object): def moveZeroes(self, nums): &quot;&quot;&quot; :type nums: List[int] :rtype: None Do not return anything, modify nums in-place instead. &quot;&quot;&quot; # in-place 操作 pre = 0 for index in range(0, len(nums)): if nums[index]: if not nums[pre]: nums[index], nums[pre] =nums[pre], nums[index] pre +=1 Shuffle an Array Shuffle a set of numbers without duplicates. Tips: 使用库函数randomint， 有 shuffle 和reset 两种操作，前者使用randomint 可以得到一个number，后者使用 list 备份。 12345678910111213141516171819202122232425class Solution(object): def __init__(self, nums): self.original =nums[:] self.nums =nums def reset(self): self.nums =self.original[:] # 这个应该 id() 是不同的 return self.nums def shuffle(self): tmp =self.nums[:] for i in range(len(self.nums)): rand =random.randint(0, len(tmp)-1) self.nums[i] =tmp[rand] del tmp[rand] return self.nums # Your Solution object will be instantiated and called as such:# obj = Solution(nums)# param_1 = obj.reset()# param_2 = obj.shuffle() Intersection of Two Arrays II Given two arrays, write a function to compute their intersection. https://leetcode.com/problems/intersection-of-two-arrays-ii/ Tips: 多看看题意，如果想要映射成dictionary，那么result 就是 min(dict1[i], dict1[j]), 两个dictionary 中values 的最小值。 123456789101112131415161718class Solution(object): def intersect(self, nums1, nums2): """ :type nums1: List[int] :type nums2: List[int] :rtype: List[int] """ # 存储dict 然后如果都存在，那么选择values 较小者 为好 # 说一下几种不同的思路 dic1 =collections.Counter(nums1) res =[] for num in nums2: if dic1[num] &gt;0: res += [num] dic1[num] -=1 return res Increasing Triplet Subsequence Given an unsorted array return whether an increasing subsequence of length 3 exists or not in the array. Input: [1,2,3,4,5]Output: true Tips: 首先学会在python 中表示最大数字(float(‘inf’)), 然后这个技巧相当于选择排序中一次遍历选择最小的那个。代码比较简洁哈 12345678910111213141516class Solution(object): def increasingTriplet(self, nums): """ :type nums: List[int] :rtype: bool """ first = second = float('inf') for n in nums: if n &lt;= first: first = n elif n &lt;= second: second = n else: return True return False Product of Array Except Self Given an array nums of n integers where n &gt; 1, return an array output such that output[i] is equal to the product of all the elements of nums except nums[i]. Tips:左右两遍，这个是一维的还是比较nice，换成 m*n 也是基本的思路吧。 12345678910111213141516171819202122class Solution(object): def productExceptSelf(self, nums): """ :type nums: List[int] :rtype: List[int] """ # 我记得使用一个数组存储起来中间的结果，然后进行操作的 len_n =len(nums) res =[1] *len_n # from left to right for i in range(1, len_n): res[i] =res[i-1] * nums[i-1] tmp =1 # from right to left for i in range(len_n-2, -1, -1): tmp *= nums[i+1] res[i] *= tmp return res Min Stack Design a stack that supports push, pop, top, and retrieving the minimum element in constant time. push(x) – Push element x onto stack. pop() – Removes the element on top of the stack. top() – Get the top element. getMin() – Retrieve the minimum element in the stack. Tips: 使用两个list，在push 和pop 的时候要维护栈，始终保持这stack[-1] 是最小值。getMin() 就直接调用结果就行。 https://leetcode.com/problems/min-stack/ 123456789101112131415161718192021222324252627class MinStack(object): def __init__(self): self.stack =[] self.min =[] def push(self, x): if not self.min: self.min.append(x) else: if x &lt;= self.min[-1]: self.min.append(x) self.stack.append(x) def pop(self): tmp =self.stack.pop() # 这个是合理的，因为只有pop 掉了最小值，然后 min list 才需要改变，之前min list 也只是存储的最小值序列 if tmp ==self.min[-1]: self.min.pop() def top(self): return self.stack[-1] def getMin(self): return self.min[-1] Kth Largest Element in an Array Find the kth largest element in an unsorted array. Note that it is the kth largest element in the sorted order, not the kth distinct element. Tips: 这个很巧妙，是快排的思想，每一次的pivot 是不是第 K大。 12345678910111213141516171819202122232425262728293031323334353637class Solution(object): def findKthLargest(self, nums, k): """ :type nums: List[int] :type k: int :rtype: int """ left, right =0, len(nums)-1 while True: index = self.partition(nums, left, right) if index ==k-1: return nums[index] elif index &lt; k-1: left =index+1 else: right =index -1 def partition(self, nums, left, right): pivot =nums[left] p1, p2 =left+1, right #找出 pivot 这个number 的位置 while p1 &lt;=p2: if nums[p1] &lt; pivot and nums[p2]&gt; pivot: nums[p1], nums[p2] =nums[p2], nums[p1] p1 +=1 p2 -=1 elif nums[p1] &gt;= pivot: p1 +=1 elif nums[p2] &lt;=pivot: p2 -=1 nums[left], nums[p2] =nums[p2] , nums[left] return p2 Min Stack Design a stack that supports push, pop, top, and retrieving the minimum element in constant time. push(x) – Push element x onto stack. pop() – Removes the element on top of the stack. top() – Get the top element. getMin() – Retrieve the minimum element in the stack. Tips: 需要维持两个stack， 一个是日常的，一个是min_stakc, 在push or pop 的过程中需要日常性维护 min_stack . 123456789101112131415161718192021222324252627class MinStack(object): def __init__(self): self.stack =[] self.min =[] def push(self, x): if not self.min: self.min.append(x) else: if x &lt;= self.min[-1]: self.min.append(x) self.stack.append(x) def pop(self): tmp =self.stack.pop() # 这个是合理的，因为只有pop 掉了最小值，然后 min list 才需要改变，之前min list 也只是存储的最小值序列 if tmp ==self.min[-1]: self.min.pop() def top(self): return self.stack[-1] def getMin(self): return self.min[-1] Kth Largest Element in an Array Find the kth largest element in an unsorted array. Note that it is the kth largest element in the sorted order, not the kth distinct element. Tips： 快排中的 pivot 如果恰好是 这个 kth，那么就成了。因为快排以 pivot 为分界点，左边是大于，右边是小于（假设是递减排序的话）；使用二分的方法，去寻找这个 pivot。属于一道比较好的题目。 ···pythonclass Solution(object): # 实际上是在试探 快排中pivot 这个点是否是第k 大值 def findKthLargest(self, nums, k): &quot;&quot;&quot; :type nums: List[int] :type k: int :rtype: int &quot;&quot;&quot; left, right =0, len(nums)-1 while True: index = self.partition(nums, left, right) if index ==k-1: return nums[index] elif index &lt; k-1: left =index+1 else: right =index -1 def partition(self, nums, left, right): pivot =nums[left] p1, p2 =left+1, right #找出 pivot 这个number 的位置 while p1 &lt;=p2: if nums[p1] &lt; pivot and nums[p2]&gt; pivot: nums[p1], nums[p2] =nums[p2], nums[p1] p1 +=1 p2 -=1 elif nums[p1] &gt;= pivot: p1 +=1 elif nums[p2] &lt;=pivot: p2 -=1 nums[left], nums[p2] =nums[p2] , nums[left] return p2 12345678910111213141516171819202122232425262728** Top K Frequent Elements**&gt; Given a non-empty array of integers, return the k most frequent elements. Input: nums = [1,1,1,2,2,3], k = 2 Output: [1,2]Tips: 如果是使用 dictionary 进行计数，那么直接调用 counter 是一个不错的选择； 下面是超级nice的代码。```pythonclass Solution(object): # 一种很简单的方法，就是放到dictionary 中，然后根据values从大到小排序，然后返回相应的keys # python 中的 sort() 函数默认是从小到大进行排序的 def topKFrequent(self, nums, k): &quot;&quot;&quot; :type nums: List[int] :type k: int :rtype: List[int] &quot;&quot;&quot; from collections import Counter # 本质上就是一个dictionary freq =Counter(nums) # 这个dictionary， 然后访问的时候freq 操作的是键，然后freq[x] 是值 #counters =sorted(counters, key =counters[1], reverse =True) uniques=sorted(freq,key=lambda x:freq[x],reverse=True) # 最后返回是一个list，只是按照value 进行排序，返回的是key的列表 return uniques[:k] 4Sum II Given four lists A, B, C, D of integer values, compute how many tuples (i, j, k, l) there are such that A[i] + B[j] + C[k] + D[l] is zero.To make problem a bit easier, all A, B, C, D have same length of N where 0 ≤ N ≤ 500. All integers are in the range of $-2^{28} $ to$ 2^{28} - 1 $ and the result is guaranteed to be at most $2^{31} - 1$. Tips: 思路和 2 sum 是一样，放到dictionary 中去。defaultdict 和dict 的唯一差别在于前者不用记性 key in dict 的判断。 12345678910111213141516171819202122232425class Solution(object): def fourSumCount(self, A, B, C, D): from collections import defaultdict length, dic, res =len(A), defaultdict(int), 0 for a in A: for b in B: dic[a+b] +=1 """ if a+b not in dic: dic[a+b] =1 else: dic[a+b] +=1 """ for c in C: for d in D: if -(c+d) in dic: res += dic[-(c+d)] return res Sliding Window Maximum Given an array nums, there is a sliding window of size k which is moving from the very left of the array to the very right. You can only see the k numbers in the window. Each time the sliding window moves right by one position. Return the max sliding window. Tips: 滑动窗口，然后窗口中的max(). 谁能想得到 python 是擅长处理 list，然后max() 函数就解决了呢 1234567891011121314class Solution(object): def maxSlidingWindow(self, nums, k): """ :type nums: List[int] :type k: int :rtype: List[int] """ if nums ==[]: return () res =[] for i in range(len(nums)-k +1): res.append(max(nums[i:i+k])) return res The Skyline Problem A city’s skyline is the outer contour of the silhouette formed by all the buildings in that city when viewed from a distance. Now suppose you are given the locations and height of all the buildings as shown on a cityscape photo (Figure A), write a program to output the skyline formed by these buildings collectively (Figure B). Tips: 关键点就是记录： 轮廓上升和轮廓下降的点，分别对应着 left 的上升和 right 的下降。评论区讲解，虽然比较难看懂 https://leetcode.com/problems/the-skyline-problem/ 12345678910111213141516171819202122232425262728293031323334from heapq import heappush, heappopclass Solution(object): def getSkyline(self, buildings): """ :type buildings: List[List[int]] :rtype: List[List[int]] """ events =[ (left, -height, right) for left, right, height in buildings] events += list((right, 0, 0) for _, right, _ in buildings) events.sort() # 先是按照left 升序排序，然后是 right 降序排序( 这个就是为什么时候 -right) res =[[0, 0]] # 最小堆，保存当前最高的轮廓 (-Height, right)， 使用-H 转换成最大堆，R 的作用是记录轮廓的有效长度 heap =[(0, float('inf'))] for left, height, right in events: # 如果轮廓上升 if height: heappush(heap, (height, right)) while heap[0][1] &lt;=left: heappop(heap) if res[-1][1] != -heap[0][0]: res += [[left, -heap[0][0]]] return res[1:] Wiggle Sort II Given an unsorted array nums, reorder it such that nums[0] &lt; nums[1] &gt; nums[2] &lt; nums[3]…. Tips: 算法题目被 python 中的lsit 操作给毁了, 可以学习以下 list[::-1], list[::2], 这种是模式化的操作，不是偶然。 12345678910111213141516class Solution(object): def wiggleSort(self, nums): """ :type nums: List[int] :rtype: None Do not return anything, modify nums in-place instead. """ #nums.sort() #half =len(nums)/2 #nums[::2], nums[1::2] =nums[:half][::-1], nums[half:][::-1] nums.sort() half = len(nums[::2]) # 注意这个是 half 必须是这样写的 # 这里面倒过来的原因， 前半个永远不大于后半个，所以这样能保证 波动 nums[::2], nums[1::2] = nums[:half][::-1], nums[half:][::-1] Find Peak Element A peak element is an element that is greater than its neighbors.Given an input array nums, where nums[i] ≠ nums[i+1], find a peak element and return its index. Input: nums = [1,2,3,1]Output: 2Explanation: 3 is a peak element and your function should return the index number 2. Tips: 二分查找， 如果是两个 if 那么就是两个步骤，如果 if else 那么就是一种选择。给定的条件中相邻的元素是不相同的。找到一个解进行了。。 https://leetcode.com/problems/find-peak-element/ 123456789101112131415161718192021class Solution(object): def findPeakElement(self, nums): """ :type nums: List[int] :rtype: int """ if not nums: return -1 if len(nums) ==1: return 0 left, right =0, len(nums)-1 while left &lt; right: mid =(left +right) /2 if nums[mid] &gt; nums[mid+1]: right =mid elif nums[mid] &lt; nums[mid+1]: left =mid +1 return left Find the Duplicate Number Given an array nums containing n + 1 integers where each integer is between 1 and n (inclusive), prove that at least one duplicate number must exist. Assume that there is only one duplicate number, find the duplicate one. Tips: 有两种思路。一种是二分法，一种是两个 pointer 的方法。后者类似linked list 中的操作。好好看看代码， fast =nums[nums[fast]] 这个操作就是 fast =fast.next.next 有木有很神奇的样子。 ···pythonclass Solution(object): # 感觉这个从时间和空间复杂度上限制的好多呀，如果满足这两个维度的，一般是先进行排序，O（nlgn） 时间，然后遍历找出重复的数字 # 基本上有两种思路，一种是 index(faster, slower point)， 一种是二分法 # 根据 indics 是有序的，然后使用二分查找 # The array is not sorted - but the indices of the array are sorted - #Insight &apos;&apos;&apos; def findDuplicate(self, nums): &quot;&quot;&quot; :type nums: List[int] :rtype: int &quot;&quot;&quot; if len(nums) == 0: return 0 low = 0 high = len(nums)-1 # 需要访问两个指针 while low &lt; high: mid = low + int((high-low)&gt;&gt;1) count = 0 for x in nums: if x &lt;= mid: count = count + 1 if count &gt; mid: high = mid else: low = mid+1 return low &apos;&apos;&apos; # 这两种方法的根本依据是 长度为n 包含n+1 个整数，并且只有一个 duplicate def findDuplicate(self, nums): slow = fast = finder = 0 while True: slow = nums[slow] fast = nums[nums[fast]] if slow == fast: while finder != slow: finder = nums[finder] slow = nums[slow] return finder 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960** Count of Smaller Numbers After Self**&gt; You are given an integer array nums and you have to return a new counts array. The counts array has the property where counts[i] is the number of smaller elements to the right of nums[i].&gt; Input: [5,2,6,1]Output: [2,1,1,0] Explanation:To the right of 5 there are 2 smaller elements (2 and 1).To the right of 2 there is only 1 smaller element (1).To the right of 6 there is 1 smaller element (1).To the right of 1 there is 0 smaller element.Tips: 这个本身的应用还是挺有意思的。python 中的库函数bisort (binary sort) 了解一下。逆序遍历，找到合适的位置，插进去，然后index 计数。```pythonclass Solution(object): # The problem is equal to find each number&apos;s inversion count. Actually there are three kinds of solutions: BST, mergeSort, and BITree. While the first two answer&apos;s time complexity is O(nlogn), and BITree time comlexity is O( nlog(maximumNum) ). # 太难了 # 两种解法 &apos;&apos;&apos; def merge(self,left,right,res): i,j=0,0 new_array=[] while i&lt;len(left) and j&lt;len(right): if left[i][1]&gt;right[j][1]: new_array+=[left[i]] res[left[i][0]]+=len(right)-j i+=1 else: new_array+=[right[j]] j+=1 new_array+=left[i:] new_array+=right[j:] return new_array def merge_sort(self,nums,res): if len(nums)&lt;2: return nums mid=len(nums)//2 left=self.merge_sort(nums[:mid],res) right=self.merge_sort(nums[mid:],res) return self.merge(left,right,res) def countSmaller(self, nums): res=[0]*len(nums) self.merge_sort([(i,num) for i,num in enumerate(nums)],res) return res &apos;&apos;&apos; def countSmaller(self, nums): count,sorted=[],[] for num in nums[::-1]: index=bisect.bisect_left(sorted,num) sorted.insert(index,num) count+=[index] return count[::-1] Longest Consecutive Sequence Given an unsorted array of integers, find the length of the longest consecutive elements sequence.Your algorithm should run in O(n) complexity. Tips: 这种解题的方式，是比较新颖的。 12345678910111213141516171819class Solution(object): # 第一印象是 先进行排序， 然后选择的过程， 但是限制条件是 O(n) 的复杂度 def longestConsecutive(self, nums): """ :type nums: List[int] :rtype: int """ nums =set(nums) best =0 # 这种方式真的很简洁 for x in nums: if x-1 not in nums: y =x+1 while y in nums: y +=1 best =max(best, y-x) return best House Robber Tips: dp 的思想运用到极致就是这个样子。使用两个变量句可以搞定。 12345678910111213141516class Solution(object): """ f(0) = nums[0] f(1) = max(num[0], num[1]) f(k) = max( f(k-2) + nums[k], f(k-1) ) """ def rob(self, nums): """ :type nums: List[int] :rtype: int """ # dp 有时候就能这样优化到使用两个变量 last, now =0, 0 for num in nums: last, now =now, max(last +num, now) return now Longest Increasing Subsequence Given an unsorted array of integers, find the length of longest increasing subsequence. Tips: f(i) 表示以 nums[i]为结尾的 longest encreasing subsequence( 第二个for 对比的对象是 f[:i] 使用j 进行遍历 ) 12345678910111213141516171819202122class Solution(object): # 求解最值 唯一解都是可以使用这样的方式的哦 # 不能使用 in 那种骚操作了， 只能踏踏实实的 dp # 这个版本的dp 没有优化好 def lengthOfLIS(self, nums): """ :type nums: List[int] :rtype: int """ if not nums: return 0 dp = [1] * len(nums) """ """ for i in range(len(nums)): for j in range(i): if nums[i] &gt; nums[j]: dp[i] = max(dp[i], dp[j] + 1) return max(dp) Coin Change You are given coins of different denominations and a total amount of money amount. Write a function to compute the fewest number of coins that you need to make up that amount. If that amount of money cannot be made up by any combination of the coins, return -1. Tips: dp问题 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647from sys import maxintclass Solution(object): """ :type coins: List[int] :type amount: int :rtype: int """ ''' # 这种方法不行 if len(coins) ==1: if amount % coins[0] ==0: return amount /coins[0] else: return -1 coins.sort(reverse =True) res =0 for i in range(len(coins)): res += amount /coins[i] amount %= coins[i] return res # table 作为dp， table[i] 表示前i 个数字使用数量最少的硬币能够 表示 # 多看几遍就能理解了 def coinChange(self, coins, amount): table = [0]*(amount + 1) for i in range(1, amount+1): minimum = maxint # 有好几种对于最小值和最大的初始化了 for j in coins: if i &gt;= j and table[i-j] != -1: minimum = min(minimum, table[i-j] + 1) table[i] = -1 if minimum == maxint else minimum return table[amount] ''' # python3 不能使用python2,python2 能使用 python3? def coinChange(self, coins, amount): table = [0 ] *(amount + 1) for i in range(1, amount +1): #minimum =maxint minimum =float('inf') for j in coins: if i &gt;= j and table[ i -j] != -1: minimum = min(minimum, table[ i -j] + 1) table[i] = -1 if minimum == float('inf') else minimum return table[amount]]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode- Array]]></title>
    <url>%2F2019%2F06%2F16%2Fleetcode-array%2F</url>
    <content type="text"><![CDATA[LeetCode 刷题总结（一）， 使用Python 实现。该篇题目类型主要是： array 和string 的相关处理。 Two Sum Given an array of integers, find two numbers such that they add up to a specific target number. Tips： 返回的是 index，所以 dict 中存储的 (num, index) 这样的组合, 是两个不相同的数字的index https://leetcode.com/problems/two-sum/ 12345678910111213class Solution(object): # 不能重复使用一个，return index def twoSum(self, nums, target): """ :type nums: List[int] :type target: int :rtype: List[int] """ cache = &#123;&#125; for i in range(len(nums)): if target - nums[i] in cache and cache[target - nums[i]] != i: # if else 用的是比较简洁的 return [cache[target - nums[i]], i] cache[nums[i]] = i Median of Two Sorted Arrays There are two sorted arrays nums1 and nums2 of size m and n respectively. Find the median of the two sorted arrays. The overall run time complexity should be O(log (m+n)). Tips: 先是 merge，然后选择 median，常规做法，时间0(mn)，不是最优的，还可以达到O(min(m, n) ) 这样的时间复杂度 123456789101112131415161718192021222324252627282930313233343536373839class Solution(object): # 使用 merge() 操作，然后根据，然后取得中位数 def median(self, nums): len_n =len(nums) if len_n &amp;1 ==1: return nums[len_n//2] else: return float(nums[len_n//2]+nums[len_n//2-1])/2 def findMedianSortedArrays(self, nums1, nums2): """ :type nums1: List[int] :type nums2: List[int] :rtype: float """ res =[] if not nums1 or not nums2: res =nums1 or nums2 return self.median(res) else: left, right =0, 0 len_1, len_2 =len(nums1)-1, len(nums2)-1 while left&lt;= len_1 and right &lt;=len_2: if nums1[left] &lt;nums2[right]: res.append(nums1[left]) left +=1 else: res.append(nums2[right]) right +=1 if left &lt;= len_1: res.extend(nums1[left:]) if right &lt;=len_2: res.extend(nums2[right:]) return self.median(res) ZigZag Conversion The string “PAYPALISHIRING” is written in a zigzag pattern on a given number of rows like this: (you may want to display this pattern in a fixed font for better legibility) P A H NA P L S I I GY I R And then read line by line: “PAHNAPLSIIGYIR” Tips: 字符串处理 12345678910111213141516171819202122class Solution(object): def convert(self, s, numRows): """ :type s: str :type numRows: int :rtype: str """ if numRows == 1 or numRows &gt;= len(s): # string of list return s L = [''] * numRows # string of list row, step = 0, 1 for x in s: L[row] += x if row == 0: step = 1 elif row == numRows -1: step = -1 row += step return ''.join(L) # array (list) 转成string 常用的方法 Container With Most Water Given n non-negative integers a1, a2, …, an , where each represents a point at coordinate (i, ai). n vertical lines are drawn such that the two endpoints of line i is at (i, ai) and (i, 0). Find two lines, which together with x-axis forms a container, such that the container contains the most water. The above vertical lines are represented by array [1,8,6,2,5,4,8,3,7]. In this case, the max area of water (blue section) the container can contain is 49. Tips: 左右双指针问题。首先移动高度较小的点，因为两者的距离是肯定变小，如果移动高度大的，那么最后的面积肯定变小；但是如果 移动高度较小的点，那么最后的面积是有可能变大的。所以这个是一个可能性的东西。 1234567891011121314151617181920212223class Solution(object): def maxArea(self, height): len_h = len(height) if len_h == 1: return 0 max_area = 0 left = 0 right = len_h - 1 # left, right =0, len_h -1 while left &lt; right: if height[left] &lt; height[right]: area = (right - left) * height[left] left += 1 else: area = (right - left) * height[right] right -= 1 if area &gt; max_area: max_area = area return max_area Longest Common Prefix Write a function to find the longest common prefix string amongst an array of strings.If there is no common prefix, return an empty string “”. Input: [&quot;flower&quot;,&quot;flow&quot;,&quot;flight&quot;] Output: &quot;fl&quot; Tips: 一个个寻找交集，最朴素的想法、 123456789101112131415161718192021222324252627282930313233class Solution(object): # 时间复杂度是 O(N^2), N=len(strs), 笼统的说 def longestCommonPrefix(self, strs): """ :type strs: List[str] :rtype: str """ if not strs: return "" n =len(strs) if n ==0: return "" elif n ==1: return strs[0] predix =strs[0] for s in strs[1:]: # 这种结构见过了，就是不断迭代，不断地的去寻找 ”交集“ predix =self.findPrefix(predix, s) if "" ==predix: break return predix def findPrefix(self, s1, s2): min_len =min(len(s1), len(s2)) # 这个if 和 return 写的都是很巧妙的 for i in range(0, min_len): if s1[i] != s2[i]: return s1[:i] return s1[:min_len] Remove Duplicates from Sorted Array Given a sorted array nums, remove the duplicates in-place such that each element appear only once and return the new length.Do not allocate extra space for another array, you must do this by modifying the input array in-place with O(1) extra memory. Tips: 前后两个指针覆盖的思想，最后返回的是index，如果发现了后者覆盖前者 12345678910111213141516171819202122class Solution(object): def removeDuplicates(self, nums): """ :type nums: List[int] :rtype: int """ if None == nums: return 0 len_n = len(nums) if len_n &lt;= 1: return len_n m = 0 n = 1 while n &lt; len_n: if nums[m] != nums[n]: m += 1 if m != n: nums[m] = nums[n] n += 1 return m + 1 Remove Element Given an array nums and a value val, remove all instances of that value in-place and return the new length.Do not allocate extra space for another array, you must do this by modifying the input array in-place with O(1) extra memory. Tips: in-place 表示不能创建数组，可以使用（临时）变量。通过双指针进行处理，想想为什么可以使用这么简洁的代码进行处理呢。m n 分别从左到右、从右到左进行遍历，将和 val 相同的元素都放到右边，不相同的放到左边 1234567891011121314151617181920212223242526class Solution(object): def removeElement(self, nums, val): """ :type nums: List[int] :type val: int :rtype: int """ if not nums: return 0 len_n =len(nums) m,n =0, len_n-1 # 注意跳出条件，遍历的方向和跳出条件是相关的, 这个 等号取于不取 一是比较难把握，可以具体带个值 while m &lt;=n: if val ==nums[m]: if val !=nums[n]: # 这个是不能使用 while 找，因为有比较多的case 需要考虑，所以使用 if 进行单步操作 nums[m], nums[n] =nums[n], nums[m] m +=1 n -=1 else: n -=1 else: m +=1 return m # 因为 m 是从0开始的 Search in Rotated Sorted Array Suppose an array sorted in ascending order is rotated at some pivot unknown to you beforehand.(i.e., [0,1,2,4,5,6,7] might become [4,5,6,7,0,1,2]).You are given a target value to search. If found in the array return its index, otherwise return -1. Tips：一定是 二分思想，关键是判断 增序列 和 target 的关系，所以有两层 if 判断条件，一层是增序列， 一层是 target 是否在增序列下面这个观点是要有的： 整个数组由两个有序的子序列构成，且左子序列中的每个元素都&gt;,右子序列中的每个元素。 123456789101112131415161718192021222324class Solution(object): # @param nums, a list of integers # @param target, an integer to be searched # @return an integer def search(self, nums, target): left = 0; right = len(nums) - 1 # 这个 == 是不容易进行取舍的， while left &lt;= right: mid = (left + right) / 2 if target == nums[mid]: return mid # 我当时面试的时候就是这种思路，一定要有条理就是了 # 左边是增序列 if nums[mid] &gt;= nums[left]: if target &lt; nums[mid] and target &gt;= nums[left]: right = mid - 1 else: left = mid + 1 elif nums[mid] &lt; nums[right]: if target &gt; nums[mid] and target &lt;= nums[right]: left = mid + 1 else: right = mid - 1 return -1 Search Insert Position Given a sorted array and a target value, return the index if the target is found. If not, return the index where it would be if it were inserted in order.You may assume no duplicates in the array. Tips： 二分查找，之前是found or not found，现在如果没有找见返回的是 index，没有什么本质区别。 1234567891011121314151617181920212223242526272829class Solution(object): def searchInsert(self, nums, target): """ :type nums: List[int] :type target: int :rtype: int """ if not nums: return None len_n =len(nums) if nums[0] &gt; target: return 0 if nums[-1] &lt; target: return len_n left, right =0, len_n-1 while left&lt;=right: mid =(left +right)/2 if nums[mid] ==target: return mid elif nums[mid] &lt;target: left =mid +1 else: right =mid -1 return left Rotate Image You are given an n x n 2D matrix representing an image.Rotate the image by 90 degrees (clockwise). Given input matrix =[ [1,2,3], [4,5,6], [7,8,9]],rotate the input matrix in-place such that it becomes:[ [7,4,1], [8,5,2], [9,6,3]] Tips: 好好观察四个等式，就是行变列，然后其他的一个坐标是对称的，这个就是旋转 90度；然后有五个变量（有重复的） 1234567891011 class Solution(object): def rotate(self, matrix): n = len(matrix) if 1 == n: return round = int(n / 2) for x in range(0, round): for y in range(x, n - x - 1): matrix[n - y - 1][x], matrix[n - x - 1][n - y - 1], matrix[y][n - x - 1], matrix[x][y] = matrix[n - x - 1][n - y - 1], matrix[y][n - x - 1], matrix[x][y], matrix[n - y - 1][x] Spiral Matrix Given a matrix of m x n elements (m rows, n columns), return all elements of the matrix in spiral order.螺旋形 Input:[ [ 1, 2, 3 ], [ 4, 5, 6 ], [ 7, 8, 9 ]]Output: [1,2,3,6,9,8,7,4,5] Tips: 在处理”行“ 信息的时候，是可以数组切割的。在处理列信息的时候，需要一个个append() 123456789101112131415161718192021222324252627282930313233343536class Solution(object): # 这个不是跟那个 剑指offer 中的顺时针打印输出一样吗， # 这个版本是比较容易理解，所以选择这个版本 def spiralOrder(self, matrix): if matrix ==[]: return [] top, bottom =0, len(matrix)-1 left, right =0, len(matrix[0])-1 res =[] # 当有一个等于的时候就应该跳出来了 while left &lt;right and top&lt; bottom: # 对于行 处理python 是有比较简单的方式的 res += matrix[top][left: right+1] for x in range(top+1, bottom): res.append(matrix[x][right]) res += matrix[bottom][left:right+1][::-1] # 倒叙 for x in range(bottom-1, top, -1): res.append(matrix[x][left]) top, bottom, left, right =top+1, bottom-1, left+1, right-1 if top ==bottom: res += matrix[top][left:right] elif left ==right: for x in range(top, bottom+1): res.append(matrix[x][right]) return res Spiral Matrix II Given a positive integer n, generate a square matrix filled with elements from 1 to $n^2$ in spiral order. Tips : 注意边角的细节。初始化赋值的应该是常见的操作，这里的cur 是比较核心的东西。 123456789101112131415161718192021222324252627282930313233343536class Solution(object): # version 1 是遍历获取， version 2 是填充。这个真是有趣的东西 # 还是设置上下左右四个坐标进行遍历的处理 def generateMatrix(self, n): ans =[ [0] *n for _ in range(n)] top, bottom, left, right =0, n-1, 0, n-1 cur =1 while left &lt;= right and top &lt;= bottom: for i in range(left, right+1): ans[top][i] =cur cur +=1 top +=1 # 根据问题需求，是可以在题目中 设置这种break，不需要等到 while 的判断 if top &gt; bottom: break for i in range(top, bottom+1): ans[i][right] =cur cur +=1 right -=1 if left &gt; right: break # 好好体会这个连接点的处理，左边是能够访问到的，右边为了能够访问到 # 进行了 -1 的操作 for i in range(right, left-1, -1): ans[bottom][i] =cur cur +=1 bottom -=1 if bottom &lt;top: break for i in range(bottom, top-1, -1): ans[i][left] =cur cur +=1 left +=1 return ans Merge Intervals Given a collection of intervals, merge all overlapping intervals. Tips: 需要区分区间的start 和end 点，分别使用 (0 1) 进行区分，然后 sort() ，那么那么start 就出现了最前面，end 就出现了最后面。默认的sort 是先按照 第一个元素排序，然后按照第二个元素排序，所以标识 (0, 1) 这个是没有收到影响的。 12345678910111213141516171819202122232425class Solution(object): def merge(self, intervals): if not intervals: return [] data = [] for interval in intervals: data.append((interval[0], 0)) data.append((interval[1], 1)) data.sort() merged = [] stack = [data[0]] for i in range(1, len(data)): d = data[i] if d[1] == 0: # this is a lower bound, push this onto the stack stack.append(d) elif d[1] == 1: if stack: start = stack.pop() if len(stack) == 0: # we have found our merged interval merged.append( (start[0], d[0])) return merged Length of Last Word Given a string s consists of upper/lower-case alphabets and empty space characters ‘ ‘, return the length of last word in the string.If the last word does not exist, return 0. Tips: 不能使用 split() 因为太多的case需要单独的处理，所以应该使用字母为基本，一个个处理。等不等于 ‘ ‘进行的切分。 1234567891011121314151617class Solution(object): # 这个是需要从 字母角度考虑，而不是从单词角度考虑 def lengthOfLastWord(self, s): len_s =len(s) if 0==len_s: return 0 index =len_s -1 # 找到第一个不是 ' '的字母 while index&gt;=0 and ' ' ==s[index]: index -=1 len_last_word =0 while index &gt;=0 and ' ' != s[index]: index -=1 len_last_word +=1 return len_last_word Valid Number Validate if a given string can be interpreted as a decimal number. Tips :对于小数(decimal ) 各种 case 的熟知程度 123456789101112131415161718192021222324252627class Solution(object): def isNumber(self, s): """ :type s: str :rtype: bool """ s = s.strip() digits = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'] met_dot = met_e = met_digit = False for i, char in enumerate(s): if char in ['+', '-']: if i &gt; 0 and s[i-1] != 'e': return False elif char == '.': if met_dot or met_e: return False met_dot = True elif char == 'e': if met_e or not met_digit: return False met_e, met_digit = True, False #elif char.isdigit(): elif char in digits: met_digit = True else: return False return met_digit Plus One Given a non-empty array of digits representing a non-negative integer, plus one to the integer.The digits are stored such that the most significant digit is at the head of the list, and each element in the array contain a single digit.You may assume the integer does not contain any leading zero, except the number 0 itself. Input: [1,2,3] Output: [1,2,4] Explanation: The array represents the integer 123. Tips: 重点在于加法的处理，一般使用 求余得到digit，然后使用carry 得到进位。 1234567891011121314151617181920class Solution(object): def plusOne(self, digits): """ :type digits: List[int] :rtype: List[int] """ len_s =len(digits) carry =1 for i in range(len_s-1, -1, -1): total =digits[i] +carry digit =int(total %10) carry =int(total //10) digits[i] =digit # 这个是最后一个进位 if carry ==1: digits.insert(0, 1) return digits Simplify Path Given an absolute path for a file (Unix-style), simplify it. Or in other words, convert it to the canonical path. Tips： 12345678910111213141516171819class Solution(object): # 这个从考点上是 stack，但是使用python字符串处理更好 # 按照 '/' 进行split() def simplifyPath(self, path): """ :type path: str :rtype: str """ stack = [] for token in path.split('/'): if token in ('', '.'): pass # continue 这两个是一样的效果， pass 就类似一种占位符，在测试的时候常见 elif token == '..': if stack: stack.pop() else: stack.append(token) return '/' + '/'.join(stack) Edit Distance Given two words word1 and word2, find the minimum number of operations required to convert word1 to word2. Tips: 动态规划就通过存储子问题结果来加快运算，但一个好的动态规划算法会尽量减少空间复杂度。 然后是可以继续优化的，使用 O(n) 的空间的复杂度. 真正的写出来之后，发现代码是比想法更加简单的。 提供了两种解法，第一种比较常规 dp，比较容易理解。123456789101112131415161718192021222324class Solution(object): def minDistance(self, word1, word2): """ :type word1: str :type word2: str :rtype: int """ m = len(word1) n = len(word2) table = [[0] * (n + 1) for _ in range(m + 1)] for i in range(m + 1): table[i][0] = i for j in range(n + 1): table[0][j] = j for i in range(1, m + 1): for j in range(1, n + 1): if word1[i - 1] == word2[j - 1]: table[i][j] = table[i - 1][j - 1] else: table[i][j] = 1 + min(table[i - 1][j], table[i][j - 1], table[i - 1][j - 1]) return table[-1][-1] 第二种就是参考一下吧。 1234567891011121314151617class Solution(object): # 从实现的角度讲，这个是需要把握住有一个 word的index 是不变的 def minDistance(self, word1, word2): l1, l2 = len(word1)+1, len(word2)+1 pre = [0 for _ in range(l2)] for j in range(l2): pre[j] = j for i in range(1, l1): cur = [i]*l2 for j in range(1, l2): cur[j] = min(cur[j-1]+1, pre[j]+1, pre[j-1]+(word1[i-1]!=word2[j-1])) #pre = cur[:] pre =cur return pre[-1] Set Matrix Zeroes Given a m x n matrix, if an element is 0, set its entire row and column to 0. Do it in-place. Tips: 使用第一行和第一列作为标记，使用 0作为标记。 123456789101112131415161718192021class Solution(object): def setZeroes(self, matrix): """ :type matrix: List[List[int]] :rtype: None Do not return anything, modify matrix in-place instead. """ firstRowHasZero = not all(matrix[0]) # all() 只有所有的不为0 返回的才不为0，否则返回0 for i in range(1,len(matrix)): for j in range(len(matrix[0])): if matrix[i][j] == 0: # 这种遍历并标记的方法还是比较优秀的 matrix[0][j] = 0 matrix[i][0] = 0 for i in range(1,len(matrix)): for j in range(len(matrix[0])-1,-1,-1): # 注意是从后往前标记的 if matrix[0][j] == 0 or matrix[i][0] == 0: matrix[i][j] = 0 if firstRowHasZero: matrix[0] = [0]*len(matrix[0]) #最后处理第一行 Remove Duplicates from Sorted List Given a sorted linked list, delete all duplicates such that each element appear only once. Tips: 注意这道题和上面有道题是有差别的，这个是 delete all duplicates 1234567891011121314151617181920212223# Definition for singly-linked list.# class ListNode(object):# def __init__(self, x):# self.val = x# self.next = Noneclass Solution(object): """ 使用两个 while 是因为，逻辑上简单 """ def deleteDuplicates(self, head): """ :type head: ListNode :rtype: ListNode """ cur =head while cur: while cur.next and cur.val == cur.next.val: cur.next =cur.next.next # skip duplicates cur =cur.next return head Merge Sorted Array Given two sorted integer arrays nums1 and nums2, merge nums2 into nums1 as one sorted array. The number of elements initialized in nums1 and nums2 are m and n respectively. You may assume that nums1 has enough space (size that is greater or equal to m + n) to hold additional elements from nums2. Tips: 题目中说了 nums1 是不会出现 index 访问报错的。从后往前遍历，因为这个是要求 merge 2 into 1的。 1234567891011121314151617181920212223242526class Solution(object): def merge(self, nums1, m, nums2, n): """ :type nums1: List[int] :type m: int :type nums2: List[int] :type n: int :rtype: None Do not return anything, modify nums1 in-place instead. """ i, j, k =m-1, n-1, m+n-1 while i &gt;=0 and j&gt;=0: if nums1[i] &gt; nums2[j]: nums1[k] =nums1[i] i -=1 else: nums1[k] =nums2[j] j -=1 k -=1 #import ipdb #ipdb.set_trace() # 如果这是 if 那么使用的就是字符串的切割，如果是while 那么就是一个个操作 if j&gt;=0: nums1[:k+1] =nums2[:j+1] Restore IP Addresses Given a string containing only digits, restore it by returning all possible valid IP address combinations. Tips： 细节比较多，在进行 dfs 的时候 123456789101112131415161718192021222324252627282930class Solution(object): def restoreIpAddresses(self, s): """ :type s: str :rtype: List[str] """ ans = [] self.helper(ans, s, 4, []) # ans 中的item 之间使用 . 进行隔开，这种技术，是非常常见的 # 这个是 list of list， 然后转换成了 list of string return ['.'.join(x) for x in ans] def helper(self, ans, s, k, temp): if len(s) &gt; k * 3: return if k == 0: #ans.append(temp[:]) ans.append(temp) else: for i in range(min(3, len(s) - k + 1)): # s 是一个字符串，当只有一位时，0可以成某一段，如果有两位或三位时，像 00， 01， 001， 011， 000等都是不合法的， # 只能是 0.1.0.0 而不能是00.1.0.0 ，这个是ip语法 # 这个是有连个并列的判断条件 if i == 2 and int(s[:3]) &gt; 255 or i &gt;0 and s[0] =='0': continue self.helper(ans, s[i + 1:], k - 1, temp + [s[:i + 1]]) Interleaving String Given s1, s2, s3, find whether s3 is formed by the interleaving of s1 and s2. Tips： interleaving 插入；s3 是否能够用s1 和s2 组成, len(s1) +len(s2) == len(s3) 这个样子的。行列分别表示s1 和s2 中的字母，然后 (x, y) 值表示当前的能够”走“ 通的路径。 12345678910111213141516171819202122232425262728293031class Solution(object): """ interleaving, 判断s3是否由s1和s2交叉构成， """ def isInterleave(self, s1, s2, s3): """ :type s1: str :type s2: str :type s3: str :rtype: bool """ r, c, l =len(s1), len(s2), len(s3) if r+c !=l: return False # 0 行和 0列的初始化，使用 true or false 来进行表示结果 dp =[ [True] * (c+1) for _ in range(r+1)] for i in range(1, r+1): dp[i][0] =dp[i-1][0] and s1[i-1] == s3[i-1] for j in range(1, c+1): dp[0][j] =dp[0][j-1] and s2[j-1] ==s3[j-1] # 看到代码之后觉得很简单， for i in range(1, r+1): for j in range(1, c+1): dp[i][j] = dp[i-1][j] and s1[i-1] ==s3[i+j-1] or dp[i][j-1] and s2[j-1] ==s3[i+j-1] return dp[-1][-1] 方法二：12345678910111213141516# 从运行的结果来说，内存下降了0.1M， 但是这个时间却商城了def isInterleave(self, s1, s2, s3): r, c, l= len(s1), len(s2), len(s3) if r+c != l: return False dp = [True for _ in range(c+1)] for j in range(1, c+1): dp[j] = dp[j-1] and s2[j-1] == s3[j-1] for i in range(1, r+1): dp[0] = (dp[0] and s1[i-1] == s3[i-1]) for j in range(1, c+1): dp[j] = (dp[j] and s1[i-1] == s3[i-1+j]) or (dp[j-1] and s2[j-1] == s3[i-1+j]) return dp[-1] Pascal’s Triangle Given a non-negative integer numRows, generate the first numRows of Pascal’s triangle. Tips: 这个是小学数学题，变成了编程题、对应好index 进行了。最后 res 可能不是正三角形（直角三角形）但一定是可以这样做的。 123456789101112131415class Solution(object): def generate(self, numRows): """ :type numRows: int :rtype: List[List[int]] """ res = [[1 for _ in range(i+1)] for i in range(numRows)] for i in range(2, numRows): for j in range(1, i): # 这个就是一个数学问题 # 就是上一行(i-1) 的 j-1 和j 元素的相加 res[i][j] =res[i-1][j-1] + res[i-1][j] return res Given a non-negative index k where k ≤ 33, return the kth index row of the Pascal’s triangle.Note that the row index starts from 0. Tips： 相比于上一个，这个只是返回了最后一行。 123456789101112131415class Solution(object): # 相对比上一个，只是输出最后一行的信息， rowIndex def getRow(self, rowIndex): """ :type rowIndex: int :rtype: List[int] """ res =[ [1 for _ in range(i+1) ] for i in range(rowIndex+1)] for i in range(2, rowIndex+1): for j in range(1, i): res[i][j ] = res[i-1][j] + res[i-1][j-1] return res[-1] Valid Palindrome Given a string, determine if it is a palindrome, considering only alphanumeric characters and ignoring cases.Note: For the purpose of this problem, we define empty string as valid palindrome. Input: &quot;A man, a plan, a canal: Panama&quot; Output: true Tips： 建议使用 is.alnum() 这个python 中自带的函数，因为这种判断还是挺常见的。回文数。先是预处理，然后才是 lower() 判断。 1234567891011121314151617181920212223242526class Solution(object): # palindrome 回文数， alphanumeric ， 字母与数字并用的; # 预处理之后，然后比较前后两个字符的异同 # Python isalnum() 方法检测字符串是否由字母和数字组成，这种函数只有在 歪果仁的代码中常见 # s[i] &gt;= 'a' and s[i] &lt;= 'z' or s[i] &gt;= '0' and s[i] &lt;= '9' or s[i] &gt;= 'A' and s[i] &lt;= 'Z':, 这个是国人的写法 # a=''.join([x for x in s if x.isalpha() or x.isdigit()]).lower() 或者这样 # 喜欢写源码 def isPalindrome(self, s): """ :type s: str :rtype: bool """ left, right =0, len(s)-1 while left &lt; right: while left &lt; right and not s[left].isalnum(): left +=1 while left &lt; right and not s[right].isalnum(): right -=1 if s[left].lower() != s[right].lower(): return False left +=1 right -=1 return True Gas Station There are N gas stations along a circular route, where the amount of gas at station i is gas[i].You have a car with an unlimited gas tank and it costs cost[i] of gas to travel from station i to its next station (i+1). You begin the journey with an empty tank at one of the gas stations.Return the starting gas station’s index if you can travel around the circuit once in the clockwise direction, otherwise return -1. Input:gas = [1,2,3,4,5]cost = [3,4,5,1,2]Output: 3Explanation:Start at station 3 (index 3) and fill up with 4 unit of gas. Your tank = 0 + 4 = 4Travel to station 4. Your tank = 4 - 1 + 5 = 8Travel to station 0. Your tank = 8 - 2 + 1 = 7Travel to station 1. Your tank = 7 - 3 + 2 = 6Travel to station 2. Your tank = 6 - 4 + 3 = 5Travel to station 3. The cost is 5. Your gas is just enough to travel back to station 3.Therefore, return 3 as the starting index. Tips: 看着挺吓人的，但是落实到代码上，就是一个循环，当不能出发时， r (rest) 是为0，然后寻求下一个可以出发的点。 123456789101112class Solution(object): def canCompleteCircuit(self, gas, cost): if sum(gas) &lt; sum(cost): return -1 index, rest = 0, 0 for i in range(len(gas)): if gas[i] + rest &lt; cost[i]: #这个是需要遍历整个 gas的，因为有可能开始行但是后来不行，所以开始的index 还是无法完成整个遍历 index = i +1 rest = 0 else: rest += gas[i] - cost[i] return index Evaluate Reverse Polish Notation Evaluate the value of an arithmetic expression in Reverse Polish Notation.Valid operators are +, -, *, /. Each operand may be an integer or another expression. Division between two integers should truncate toward zero. The given RPN expression is always valid. That means the expression would always evaluate to a result and there won&apos;t be any divide by zero operation. Tips： 术语，逆波兰表达式（操作数在前，操作符在后）的一种形式。栈是存储操作数 和运算结果的。对于负数不能整除的，向着 0 靠拢 1234567891011121314151617181920212223242526272829class Solution(object): # 计算逆波兰表达式：把操作数放前面，把操作符后置的一种写法 # 这个明显就是 栈的使用呀，两个栈， def evalRPN(self, tokens): """ :type tokens: List[str] :rtype: int """ stack =[] for t in tokens: if t not in "+-*/": stack.append(int(t)) else: right, left =stack.pop(), stack.pop() if t =="+": stack.append(left +right) elif t =="-": stack.append(left-right) elif t =="*": stack.append(left*right) else: # case like 1/(-2) 负数且不能整除 if left*right &lt;0 and left % right!=0: stack.append(left/right +1) else: stack.append(left/right) return stack.pop() Reverse Words in a String Given an input string, reverse the string word by word. Tips： 正常的思路是第一次全翻转，第二次按照 word 进行翻转。但是python 十分擅长 字符串的处理。 123456789class Solution(object): # 两次翻转。第一次是全翻转，然后第二次是word 翻转 # 字符串类型的算法题目，使用python 是无法get 到算法层面的 hahaha def reverseWords(self, s): """ :type s: str :rtype: str """ return " ".join(s.split()[::-1]) Search a 2D Matrix Write an efficient algorithm that searches for a value in an m x n matrix. This matrix has the following properties: Integers in each row are sorted from left to right. The first integer of each row is greater than the last integer of the previous row. Tips: 注意第二个条件，下一行的开头是大于上一行的末尾，所以如果 dense 一下，是可以看成大的有序，所以思路就是二叉排序。 https://leetcode.com/problems/search-a-2d-matrix/ Input:matrix = [ [1, 3, 5, 7], [10, 11, 16, 20], [23, 30, 34, 50]]target = 3Output: true 123456789101112131415161718192021222324252627class Solution(object): def searchMatrix(self, matrix, target): """ :type matrix: List[List[int]] :type target: int :rtype: bool """ # 如果写成 not target 是有问题，当 target ==0 的时候，这个是不成立的，所以需要看一下数据的范围 # 注意区分 if not matrix or target ==None: return False rows, cols=len(matrix), len(matrix[0]) low, high =0, rows*cols-1 # 总的二叉 while low &lt;=high: mid =(low +high) /2 num =matrix[mid/cols][mid%cols] if num ==target: return True elif num&lt; target: low =mid +1 else: high =mid -1 return False Search a 2D Matrix II Write an efficient algorithm that searches for a value in an m x n matrix. This matrix has the following properties: Integers in each row are sorted in ascending from left to right. Integers in each column are sorted in ascending from top to bottom. [ [1, 4, 7, 11, 15], [2, 5, 8, 12, 19], [3, 6, 9, 16, 22], [10, 13, 14, 17, 24], [18, 21, 23, 26, 30]] Tips: 注意区分和上一道题目的第二点的区别。这个只能是一步步走，下面的程序是从右上方开始走，如果 target 大则向下直走，否则左走。可以有两种初始化方式，一种是右上角，一种是左下角。 https://leetcode.com/problems/search-a-2d-matrix-ii/ 1234567891011121314151617181920212223242526272829303132333435class Solution(object): def searchMatrix(self, matrix, target): """ :type matrix: List[List[int]] :type target: int :rtype: bool """ # 一开始的时候不知道使用什么遍历方式，因为 for 好像不太行，应该使用 while 基于条件遍历 if matrix ==[]: return False rows, cols =len(matrix)-1, len(matrix[0])-1 """ row, col = 0, cols # start points while row &lt;= rows and col &gt;= 0: if matrix[row][col] == target: return True elif matrix[row][col] &lt; target: row +=1 else: col -=1 """ # 还有一种初始化方式 row, col =rows, 0 while row &gt;=0 and col &lt;= cols: if matrix[row][col] ==target: return True elif matrix[row][col] &lt; target: col +=1 else: row -=1 return False Kth Smallest Element in a Sorted Matrix Given a n x n matrix where each of the rows and columns are sorted in ascending order, find the kth smallest element in the matrix. Tips: 主要是看到 example 中的数据，有两种类型，一种是可以把matrix dense 之后依然是有序，另一种不是。这个是属于前者。下面这种解法比较新颖，使用heapq 进行操作，遍历k th 就得到了kth 最小。 ···pythonclass Solution(object): # 有一种方法是初始化为右上角，然后小往左走，大往下走 # 这个默认求解的k smallest，所以python 中heapq 默认也是小根堆，所以 def kthSmallest(self, matrix, k): &quot;&quot;&quot; :type matrix: List[List[int]] :type k: int :rtype: int &quot;&quot;&quot; heap, res, n =[(matrix[0][0], 0, 0)], 0, len(matrix) for k in range(1, k+1): # 这个是次数 res, row, col =heapq.heappop(heap) # 问题在于这里并没有体现了 row col相应的变化 +1 这类东西 # 这个是通过 heapq 不断地push 和pop 来得到相应的 row col 然后进行res 的获取的 if not row and col&lt; n-1: heapq.heappush(heap, (matrix[row][col+1], row, col+1)) if row&lt; n-1: heapq.heappush(heap, (matrix[row+1][col], row+1, col)) return res 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748** Evaluate Reverse Polish Notation**&gt; Evaluate the value of an arithmetic expression in Reverse Polish Notation.Valid operators are +, -, *, /. Each operand may be an integer or another expression.Tips: 逆波兰又称之为后缀表达式，操作符置于操作数后面，这个前后是以“操作符” 进行定义的。解题思路，如果是操作数，那么就压栈，如果是操作符，那么弹出进行运算。```pythonclass Solution(object): def evalRPN(self, tokens): stack =[] operators =[&apos;+&apos;, &apos;-&apos;, &apos;*&apos;, &apos;/&apos;] for token in tokens: if token not in operators: #这种比较nice exact stack.append(int(token)) # 细节 string to int else: if len(stack) &lt;2: return False second =stack.pop() first =stack.pop() if token ==&quot;+&quot;: result =first +second elif token ==&quot;-&quot;: result =first -second elif token ==&quot;*&quot;: result =first *second else: # 除法向来处理就比较麻烦 if second ==0: return False # 这个是操作中的abs 没有改变原来的值，所以比较nice result =abs(first)/abs(second) if first *second &lt;0: result =-result stack.append(result) # 最后只有一个result 值，所以十分简洁 if len(stack) !=1: return False return stack[0] Excel Sheet Column Number Given a column title as appear in an Excel sheet, return its corresponding column number. Tips: 在于 char 和num 的对应关系。ord() 用于 char 转成int 这种库函数还是要有的。 可以看成 26 进制。 123456789101112class Solution(object): def titleToNumber(self, s): """ :type s: str :rtype: int """ res =0 for char in s: res =res*26 +(ord(char)- ord('A') +1) return res Largest Number Given a list of non negative integers, arrange them such that they form the largest number. Tips: string 类型组合成的数字是最大的。 在string 里面 ‘9’ &gt; ‘88888’ 这个是成立，所以这个特性可以处理这个题目，很巧妙。 12345678910class Solution(object): def largestNumber(self, nums): """ :type nums: List[int] :rtype: str """ nums =map(str, nums) nums.sort(cmp =lambda a, b :cmp(a+b, b+a), reverse =True) # 降序 # 可能出现 00 这样的字符串，所以是先 int 然后再string，感觉这个不是算法的味道 return str(int(''.join(nums))) Longest Substring with At Least K Repeating Characters Find the length of the longest substring T of a given string (consists of lowercase letters only) such that every character in T appears no less than k times. Tips: 这个codes 中的else 还是相当的牛逼，第一次见这种写法的。如果 for 循环中的条件不成立，else。 12345678910111213141516171819class Solution: def longestSubstring(self, s, k): """ :type s: str :type k: int :rtype: int """ stack = [] stack.append(s) ans = 0 while stack: s = stack.pop() for c in set(s): if s.count(c) &lt; k: stack.extend([z for z in s.split(c)]) break else: ans = max(ans, len(s)) return ans Longest Increasing Path in a Matrix Given an integer matrix, find the length of the longest increasing path.From each cell, you can either move to four directions: left, right, up or down. You may NOT move diagonally or move outside of the boundary (i.e. wrap-around is not allowed). Tips: dfs, 判断条件是 val &gt; matrix[i][j] 12345678910111213141516171819202122232425262728class Solution(object): # 一看这个就是深度优先搜索 # 这种做法更加普世 def longestIncreasingPath(self, matrix): """ :type matrix: List[List[int]] :rtype: int """ # 表示以这点为终点的 路径是有多长 # 这个逻辑上是比较简单的， 就是dfs()，然哦吼如果从任意一点出发 range() range()， # 使用 dfs() ，如果是value &gt; matrix[][]，就直接返回了 dp[i][j] def dfs(i, j): if not dp[i][j]: val = matrix[i][j] # i-1 的时候要大于0 i+1的时候要i &lt; M 这样的操作 dp[i][j] = 1 + max( dfs(i - 1, j) if i and val &gt; matrix[i - 1][j] else 0, dfs(i + 1, j) if i &lt; M - 1 and val &gt; matrix[i + 1][j] else 0, dfs(i, j - 1) if j and val &gt; matrix[i][j - 1] else 0, dfs(i, j + 1) if j &lt; N - 1 and val &gt; matrix[i][j + 1] else 0) return dp[i][j] if not matrix or not matrix[0]: return 0 M, N = len(matrix), len(matrix[0]) dp = [[0] * N for i in range(M)] # 以该点为终点的 increasing path 有多少个 return max(dfs(x, y) for x in range(M) for y in range(N)) Word Ladder Given two words (beginWord and endWord), and a dictionary’s word list, find the length of shortest transformation sequence from beginWord to endWord, such that:Only one letter can be changed at a time.Each transformed word must exist in the word list. Note that beginWord is not a transformed word. Tips: 讲解-tm) https://leetcode.com/problems/word-ladder/ 123456789101112131415161718192021222324252627class Solution(object): # https://leetcode.com/problems/word-ladder/discuss/157376/Python-(BFS)-tm # 写出来之后就比较好理解，可以好好想想 def ladderLength(self, beginWord, endWord, wordList): """ :type beginWord: str :type endWord: str :type wordList: List[str] :rtype: int """ wordList =set(wordList) queue =collections.deque([(beginWord, 1)]) visited =set() alpha =string.ascii_lowercase # 'abcd..z' while queue: word, length =queue.popleft() if word == endWord: return length for i in range(len(word)): for ch in alpha: new_word =word[:i] +ch+word[i+1:] if new_word in wordList and new_word not in visited: queue.append((new_word, length+1)) visited.add(new_word) return 0 Fraction to Recurring Decimal Given two integers representing the numerator and denominator of a fraction, return the fraction in string format.If the fractional part is repeating, enclose the repeating part in parentheses. Tips: 分数变成小数 123456789101112131415161718192021222324252627282930313233343536373839class Solution(object): # 主要是考察分情况讨论，这样是比较多的 # 就是在拼接呀 def fractionToDecimal(self, numerator, denominator): """ :type numerator: int :type denominator: int :rtype: str """ res ='' if numerator % denominator ==0: return str(numerator/denominator) if numerator* denominator &lt;0: res += '-' numerator, denominator =abs(numerator), abs(denominator) res += str(numerator/denominator) res +='.' numerator %= denominator i =len(res) table =&#123;&#125; # 下面描述的就是辗转相除的过程， 使用 &#123;&#125; 进行存储 while numerator !=0: if numerator not in table: table[numerator] =i else: i =table[numerator] res =res[:i] +'('+res[i:]+')' return res numerator =numerator*10 res += str(numerator/denominator) numerator %= denominator i +=1 return res Reverse Bits Reverse bits of a given 32 bits unsigned integer. Tips: 与操作和左移操作 ( &amp; and &lt;&lt;) 是常见的 bit operation中用到的 ···pythonclass Solution: # @param n, an integer # @return an integer # 没有什么说的， 二级制操作，注意输入和输出都是 integer # One small thing is the plus operator can be replaced by &quot;bitwise or&quot;, aka &quot;|&quot;. # Just generate the answer bit by bit, do not use things like &quot;% 2&quot; or &quot;2 ** k&quot; or &quot;bin&quot;. Bit manipulation is a lot faster. def reverseBits(self, n): ans =0 # 从后往前处理，所以这就reverse 了 for i in range(32): # n&amp;1 是取最后一位 # ans &lt;&lt;1 左移一位，类似乘2 ans += n &amp;1 if i ==31: return ans n &gt;&gt;= 1 ans &lt;&lt;= 1 return ans 12345678910111213141516171819202122232425262728293031323334** Word Break**&gt; Given a non-empty string s and a dictionary wordDict containing a list of non-empty words, determine if s can be segmented into a space-separated sequence of one or more dictionary words.Tips: dp 问题```pythonclass Solution(object): # 字符串的处理，感觉有点难呀 # dp的思路， dp[i] 表示 s[:i] 是否可分 def wordBreak(self, s, wordDict): &quot;&quot;&quot; :type s: str :type wordDict: List[str] :rtype: bool &quot;&quot;&quot; dict =&#123;&#125; for w in wordDict: dict[w] =True dp =[False for x in range(len(s)+1)] dp[0] =True for i in range(1, len(s)+1): # 如果出现了 range(i) 这种是常用的处理字符串的手段，看前i 是否符合某种要求 # 前面的可分，后面的看一下是否可分 for j in range(i): if dp[j] and s[j:i] in dict: dp[i] =True break return dp[-1] Word Break II Given a non-empty string s and a dictionary wordDict containing a list of non-empty words, add spaces in s to construct a sentence where each word is a valid dictionary word. Return all such possible sentences. Tips : dfs 1234567891011121314151617181920212223242526272829303132333435class Solution(object): # 上一题是返回 true or false，这个是要求是路径，那么最直接的就是dfs(), 不能使用dp了 def wordBreak(self, s, wordDict): """ :type s: str :type wordDict: List[str] :rtype: List[str] """ return self.dfs(s, wordDict, &#123;&#125;) def dfs(self, s, wordDict, memo): # 这个memo 就是某长度的字符串，在之前的dfs 中是否存在过 # 'penapple': ['pen apple'], 'applepenapple': ['apple pen apple' if s in memo: return memo[s] if not s: return [] res =[] for word in wordDict: # 这种直接从 dictionary 中寻找要比 从string 中拼凑快一些 if not s.startswith(word): # 这个就是最贪婪的找开头的python 句子 continue if len(word) ==len(s): # 包含且长度相同，那么 res.append() 就是这个操作了 res.append(word) else: rest =self.dfs(s[len(word):], wordDict, memo) # 如果原来的 s 比较长 那么就切分了 # 这种不收 递归影响的思维还是挺牛逼的 哈哈 for item in rest: item =word +' '+item res.append(item) memo[s] =res return res Palindrome Partitioning Given a string s, partition s such that every substring of the partition is a palindrome.Return all possible palindrome partitioning of s. Tips: dfs, 其中 ispa 写的是比较简洁的 123456789101112131415161718192021222324252627class Solution(object): # 感觉这个有点难度呀 # 所有值的一般是 dfs() 这个还是得不断的加强认识的， def partition(self, s): """ :type s: str :rtype: List[List[str]] """ res =[] self.dfs(s, [], res) return res def dfs(self, s, path, res): if not s: res.append(path) return # 关键是这里的理解， path 是不断的增加的，并且 s[I:] 这个是不断的介绍的， # 先是要求 s[:i] 是 palindrome 然后递归 s[i:] 是palindrome ，整体上是比较nice的 for i in range(1, len(s)+1): if self.isPal(s[:i]): self.dfs(s[i:], path+[s[:i]], res) def isPal(self, s): return s ==s[::-1] Word Search II Given a 2D board and a list of words from the dictionary, find all words in the board.Each word must be constructed from letters of sequentially adjacent cell, where “adjacent” cells are those horizontally or vertically neighboring. The same letter cell may not be used more than once in a word. Tips: 使用字典树 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354class TrieNode(): def __init__(self): self.children = collections.defaultdict(TrieNode) self.isWord = Falseclass Trie(): def __init__(self): self.root = TrieNode() def insert(self, word): node = self.root for w in word: node = node.children[w] node.isWord = True def search(self, word): node = self.root for w in word: node = node.children.get(w) if not node: return False return node.isWord# 上面在上一道题目中就应该记住，这个是一道经典的题目class Solution(object): def findWords(self, board, words): res = [] trie = Trie() node = trie.root for w in words: trie.insert(w) # 先是insert，然后在每一个点进行查找，最后看res for i in range(len(board)): for j in range(len(board[0])): self.dfs(board, node, i, j, "", res) return res def dfs(self, board, node, i, j, path, res): if node.isWord: res.append(path) node.isWord = False if i &lt; 0 or i &gt;= len(board) or j &lt; 0 or j &gt;= len(board[0]): return tmp = board[i][j] node = node.children.get(tmp) if not node: return board[i][j] = "#" self.dfs(board, node, i + 1, j, path + tmp, res) self.dfs(board, node, i - 1, j, path + tmp, res) self.dfs(board, node, i, j - 1, path + tmp, res) self.dfs(board, node, i, j + 1, path + tmp, res) board[i][j] = tmp Valid Anagram Given two strings s and t , write a function to determine if t is an anagram of s. Tips: dictionary 的应用 12345678910111213141516171819202122232425262728293031323334353637class Solution(object): # 这个和旋转数组 感觉上是差不多的呀 # 解答的时候，应该从 dictionary 的角度考虑 def isAnagram(self, s, t): """ :type s: str :type t: str :rtype: bool """ dic =&#123;&#125; # dic =collections.defaultdic(int) 和上面的唯一差别就是，直接使用 dic[char] +=1 这样的操作 # 不用判断是否存在 这样的操作 for n in s: if n not in dic: dic[n] =1 else: dic[n] +=1 for n in t: if n not in dic: return False else: dic[n] -=1 """ for n in dic: if dic[n]!=0: return False return True """ for value in dic.values(): if value !=0: return False return True First Unique Character in a String Given a string, find the first non-repeating character in it and return it’s index. If it doesn’t exist, return -1. Tips: 注意是第一个 non-repeating的 1234567891011121314151617181920class Solution(object): # 这个不重复 是整体之后的不重复，而不是左右的不重复，是全局的 # 我的想法使用dict def firstUniqChar(self, s): """ :type s: str :rtype: int """ dic =&#123;&#125; seen =set() for index, ch in enumerate(s): if ch not in seen: dic[ch] =index seen.add(ch) elif ch in dic: del dic[ch] # 这个是通过更新index 达到的 # 因为题目中提到的是 第一个 non-repeating Reverse String Write a function that reverses a string. The input string is given as an array of characters char[].Do not allocate extra space for another array, you must do this by modifying the input array in-place with O(1) extra memory. Tips : in-place 操作 pointer 123456789101112131415class Solution(object): # 我反手一个reverse() 过去，有问题吗 # sting is immutable, cannot reverse in-place def reverseString(self, s): """ :type s: List[str] :rtype: None Do not return anything, modify s in-place instead. """ left, right =0, len(s)-1 while left &lt; right: s[left], s[right] =s[right], s[left] left +=1 right -=1]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tmp]]></title>
    <url>%2F2019%2F06%2F16%2Fnlp-tmp%2F</url>
    <content type="text"><![CDATA[学习BERT模型详解 笔记。 （传统的机器学习特征确实是稀疏）Word Embedding解决了传统机器学习方法的特征稀疏问题，它通过把一个词映射到一个低维稠密的语义空间，从而使得相似的词可以共享上下文信息，从而提升泛化能力。 （不同的对象对比，是有不同的侧重点的）Self-Attention的初衷是为了用Attention替代LSTM，从而可以更好的并行(因为LSTM的时序依赖特效很难并行)，从而可以处理更大规模的语料。 transformer 是可以作为一个基本的 encoder 或者decoder 而存在，这个对等的地位是和 lstm 是相同的。 （多任务学习）当然通过Multi-Task Learning，我们可以利用其它相关任务的数据。比如我们要做文本分类，我们可以利用机器翻译的训练数据，通过同时优化两个(多个)目标，让模型同时学到两个任务上的语义信息，因为这两个任务肯定是共享很多基础语义信息的，所以它的效果要比单个任务好。但即使这样，标注的数据量还是非常有限的。 因为数据量的问题，所以转向了无监督学习，这样才能使用大规模的数据集，使用海量的无标注的数据学习模型。 关于bert 的理解还是要多看看的。 而BERT和OpenAI GPT的方法类似，也是Fine-Tuning的思路，但是它解决了OpenAI GPT(包括ELMo)单向信息流的问题，同时它的模型和语料库也更大。依赖Google强大的计算能力和工程能力，BERT横扫了OpenAI GPT。成王败寇，很少还有人记得OpenAI GPT的贡献了。但是BERT的很多思路都是沿用OpenAI GPT的，要说BERT的学术贡献，最多是利用了Mask LM(这个模型在上世纪就存在了)和Predicting Next Sentence这个Multi-task Learning而已。 （首先 transformer 一开始就是用来做 机器翻译的，其次这段落是不太理解的， 来源）之前我们介绍的Transformer模型是用来做机器翻译的，它有一个Encoder和一个Decoder。这里使用的是Encoder，只不过Encoder的输出不是给Decoder使用，而是直接用它来预测下一个词，如下图所示。但是直接用Self-Attention来训练语言模型是有问题的，因为在k时刻𝑝(𝑡𝑘|𝑡1,..,𝑡𝑘−1)，也就是计算𝑡𝑘的时候只能利用它之前的词(或者逆向的语言模型只能用它之后的词)。但是Transformer的Self-Attention是可以利用整个句子的信息的，这显然不行，因为你让它根据”it is a”来预测后面的词，而且还告诉它整个句子是”it is a good day”，它就可能”作弊”，直接把下一个词输出了，这样loss是零。 对于相似度的计算是有两种途径： 通过无监督的学习得到词向量，然后使用词向量计算句子的相似度（无监督的） 如果有标注的训练集的话，那么是可以使用 seq2seq 这样的模型 进行有监督的学习的 ELMo和GPT最大的问题就是传统的语言模型是单向的——我们是根据之前的历史来预测当前词。但是我们不能利用后面的信息。比如句子”The animal didn’t cross the street because it was too tired”。我们在编码it的语义的时候需要同时利用前后的信息，因为在这个句子中，it可能指代animal也可能指代street。根据tired，我们推断它指代的是animal，因为street是不能tired。但是如果把tired改成wide，那么it就是指代street了。传统的语言模型，不管是RNN还是Transformer，它都只能利用单方向的信息。比如前向的RNN，在编码it的时候它看到了animal和street，但是它还没有看到tired，因此它不能确定it到底指代什么。如果是后向的RNN，在编码的时候它看到了tired，但是它还根本没看到animal，因此它也不能知道指代的是animal。Transformer的Self-Attention理论上是可以同时attend to到这两个词的，但是根据前面的介绍，由于我们需要用Transformer来学习语言模型，因此必须用Mask来让它看不到未来的信息，所以它也不能解决这个问题的。 如下图所示，BERT能够同时利用前后两个方向的信息，而ELMo和GPT只能使用单个方向的。 BERT仍然使用的是Transformer模型，那它是怎么解决语言模型只能利用一个方向的信息的问题呢？答案是它的pretraining训练的不是普通的语言模型，而是Mask语言模型。在介绍Mask语言模型之前我们先介绍BERT的输入表示。 输入表示 BERT的输入表示如图下图所示。比如输入的是两个句子”my dog is cute”，”he likes playing”。后面会解释为什么需要两个句子。这里采用类似GPT的两个句子的表示方法，首先会在第一个句子的开头增加一个特殊的Token [CLS]，在cute的后面增加一个[SEP]表示第一个句子结束，在##ing后面也会增加一个[SEP]。注意这里的分词会把”playing”分成”play”和”##ing”两个Token，这种把词分成更细粒度的Word Piece的方法在前面的机器翻译部分介绍过了，这是一种解决未登录词的常见办法，后面的代码部分也会简单介绍。接着对每个Token进行3个Embedding：词的Embedding；位置的Embedding和Segment的Embedding。词的Embedding大家都很熟悉了，而位置的Embedding和词类似，把一个位置(比如2)映射成一个低维稠密的向量。而Segment只有两个，要么是属于第一个句子(segment)要么属于第二个句子，不管那个句子，它都对应一个Embedding向量。同一个句子的Segment Embedding是共享的，这样它能够学习到属于不同Segment的信息。对于情感分类这样的任务，只有一个句子，因此Segment id总是0；而对于Entailment任务，输入是两个句子，因此Segment是0或者1。 BERT模型要求有一个固定的Sequence的长度，比如128。如果不够就在后面padding，否则就截取掉多余的Token，从而保证输入是一个固定长度的Token序列，后面的代码会详细的介绍。第一个Token总是特殊的[CLS]，它本身没有任何语义，因此它会(必须)编码整个句子(其它词)的语义。 Mask LM 为了解决只能利用单向信息的问题，BERT使用的是Mask语言模型而不是普通的语言模型。Mask语言模型有点类似与完形填空——给定一个句子，把其中某个词遮挡起来，让人猜测可能的词。这里会随机的Mask掉15%的词，然后让BERT来预测这些Mask的词，通过调整模型的参数使得模型预测正确的概率尽可能大，这等价于交叉熵的损失函数。这样的Transformer在编码一个词的时候会(必须)参考上下文的信息。 但是这有一个问题：在Pretraining Mask LM时会出现特殊的Token [MASK]，但是在后面的fine-tuning时却不会出现，这会出现Mismatch的问题。因此BERT中，如果某个Token在被选中的15%个Token里，则按照下面的方式随机的执行： 80%的概率替换成[MASK]，比如my dog is hairy → my dog is [MASK] 10%的概率替换成随机的一个词，比如my dog is hairy → my dog is apple 10%的概率替换成它本身，比如my dog is hairy → my dog is hairy 这样做的好处是，BERT并不知道[MASK]替换的是哪一个词，而且任何一个词都有可能是被替换掉的，比如它看到的apple可能是被替换的词。这样强迫模型在编码当前时刻的时候不能太依赖于当前的词，而要考虑它的上下文，甚至更加上下文进行”纠错”。比如上面的例子模型在编码apple是根据上下文my dog is应该把apple(部分)编码成hairy的语义而不是apple的语义。 预测句子关系 在有些任务中，比如问答，前后两个句子有一定的关联关系，我们希望BERT Pretraining的模型能够学习到这种关系。因此BERT还增加了一个新的任务——预测两个句子是否有关联关系。这是一种Multi-Task Learing。BERT要求的Pretraining的数据是一个一个的”文章”，比如它使用了BookCorpus和维基百科的数据，BookCorpus是很多本书，每本书的前后句子是有关联关系的；而维基百科的文章的前后句子也是有关系的。对于这个任务，BERT会以50%的概率抽取有关联的句子(注意这里的句子实际只是联系的Token序列，不是语言学意义上的句子)，另外以50%的概率随机抽取两个无关的句子，然后让BERT模型来判断这两个句子是否相关。比如下面的两个相关的句子： [CLS] the man went to [MASK] store [SEP] he bought a gallon [MASK] milk [SEP] 下面是两个不相关的句子： [CLS] the man [MASK] to the store [SEP] penguin [MASK] are flight ##less birds [SEP] ELMO 和OpenAI GPT 的区别： UMLFiT比较复杂，而且效果也不是特别好，我们暂且不提。ELMo和OpenAI GPT的思想其实非常非常简单，就是用海量的无标注数据学习语言模型，在学习语言模型的过程中自然而然的就学到了上下文的语义关系。它们都是来学习一个语言模型，前者使用的是LSTM而后者使用Transformer，在进行下游任务处理的时候也有所不同，ELMo是把它当成特征。拿分类任务来说，输入一个句子，ELMo用LSTM把它扫一次，这样就可以得到每个词的表示，这个表示是考虑上下文的，因此”He deposited his money in this bank”和”His soldiers were arrayed along the river bank”中的两个bank的向量是不同的。下游任务用这些向量来做分类，它会增加一些网络层，但是ELMo语言模型的参数是固定的。而OpenAI GPT不同，它直接用特定任务来Fine-Tuning Transformer的参数。因为用特定任务的数据来调整Transformer的参数，这样它更可能学习到与这个任务特定的上下文语义关系，因此效果也更好。 Skip Thought Vector 在训练语料上的区别 和训练Word2Vec不同，Word2Vec只需要提供句子，而Skip Thought Vector需要文章(至少是段落)。论文使用的数据集是BookCorpus(http://yknzhu.wixsite.com/mbweb)，目前网站已经不提供下载了。BookCorpus的统计信息如下图所示，有一万多本书，七千多万个句子。 词汇扩展： 使用一个word2vec 的词向量去映射在训练句子向量中得到的向量，其中的映射关系是 $w$, 这个参数可以通过共现词进行学习。 ELMo 这篇论文的想法其实非常非常简单，但是取得了非常好的效果。它的思路是用深度的双向RNN(LSTM)在大量未标注数据上训练语言模型。然后在实际的任务中，对于输入的句子，我们使用这个语言模型来对它处理，得到输出的向量，因此这可以看成是一种特征提取。]]></content>
      <categories>
        <category>NLP</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[BERT学习笔记(2)]]></title>
    <url>%2F2019%2F06%2F16%2Fnlp_bert(2)%2F</url>
    <content type="text"><![CDATA[学习BERT模型详解 笔记。 （传统的机器学习特征确实是稀疏）Word Embedding解决了传统机器学习方法的特征稀疏问题，它通过把一个词映射到一个低维稠密的语义空间，从而使得相似的词可以共享上下文信息，从而提升泛化能力。 （不同的对象对比，是有不同的侧重点的）Self-Attention的初衷是为了用Attention替代LSTM，从而可以更好的并行(因为LSTM的时序依赖特效很难并行)，从而可以处理更大规模的语料。 transformer 是可以作为一个基本的 encoder 或者decoder 而存在，这个对等的地位是和 lstm 是相同的。 当然通过Multi-Task Learning，我们可以利用其它相关任务的数据。比如我们要做文本分类，我们可以利用机器翻译的训练数据，通过同时优化两个(多个)目标，让模型同时学到两个任务上的语义信息，因为这两个任务肯定是共享很多基础语义信息的，所以它的效果要比单个任务好。但即使这样，标注的数据量还是非常有限的。 因为数据量的问题，所以转向了无监督学习，这样才能使用大规模的数据集，使用海量的无标注的数据学习模型。多任务学习也是可以被用来做词汇补充的。 关于bert 的理解还是要多看看的。 而BERT和OpenAI GPT的方法类似，也是Fine-Tuning的思路，但是它解决了OpenAI GPT(包括ELMo)单向信息流的问题，同时它的模型和语料库也更大。依赖Google强大的计算能力和工程能力，BERT横扫了OpenAI GPT。成王败寇，很少还有人记得OpenAI GPT的贡献了。但是BERT的很多思路都是沿用OpenAI GPT的，要说BERT的学术贡献，最多是利用了Mask LM(这个模型在上世纪就存在了)和Predicting Next Sentence这个Multi-task Learning而已。 （首先 transformer 一开始就是用来做 机器翻译的，其次这段落是不太理解的， 来源）之前我们介绍的Transformer模型是用来做机器翻译的，它有一个Encoder和一个Decoder。这里使用的是Encoder，只不过Encoder的输出不是给Decoder使用，而是直接用它来预测下一个词，如下图所示。但是直接用Self-Attention来训练语言模型是有问题的，因为在k时刻𝑝(𝑡𝑘|𝑡1,..,𝑡𝑘−1)，也就是计算𝑡𝑘的时候只能利用它之前的词(或者逆向的语言模型只能用它之后的词)。但是Transformer的Self-Attention是可以利用整个句子的信息的，这显然不行，因为你让它根据”it is a”来预测后面的词，而且还告诉它整个句子是”it is a good day”，它就可能”作弊”，直接把下一个词输出了，这样loss是零。 对于相似度的计算是有两种途径： 通过无监督的学习得到词向量，然后使用词向量计算句子的相似度（无监督的） 如果有标注的训练集的话，那么是可以使用 seq2seq 这样的模型 进行有监督的学习的 ELMo和GPT最大的问题就是传统的语言模型是单向的——我们是根据之前的历史来预测当前词。但是我们不能利用后面的信息。比如句子”The animal didn’t cross the street because it was too tired”。我们在编码it的语义的时候需要同时利用前后的信息，因为在这个句子中，it可能指代animal也可能指代street。根据tired，我们推断它指代的是animal，因为street是不能tired。但是如果把tired改成wide，那么it就是指代street了。传统的语言模型，不管是RNN还是Transformer，它都只能利用单方向的信息。比如前向的RNN，在编码it的时候它看到了animal和street，但是它还没有看到tired，因此它不能确定it到底指代什么。如果是后向的RNN，在编码的时候它看到了tired，但是它还根本没看到animal，因此它也不能知道指代的是animal。Transformer的Self-Attention理论上是可以同时attend to到这两个词的，但是根据前面的介绍，由于我们需要用Transformer来学习语言模型，因此必须用Mask来让它看不到未来的信息，所以它也不能解决这个问题的。 如下图所示，BERT能够同时利用前后两个方向的信息，而ELMo和GPT只能使用单个方向的。 BERT仍然使用的是Transformer模型，那它是怎么解决语言模型只能利用一个方向的信息的问题呢？答案是它的pretraining训练的不是普通的语言模型，而是Mask语言模型。在介绍Mask语言模型之前我们先介绍BERT的输入表示。 输入表示 BERT的输入表示如下图所示。比如输入的是两个句子”my dog is cute”，”he likes playing”。后面会解释为什么需要两个句子。这里采用类似GPT的两个句子的表示方法，首先会在第一个句子的开头增加一个特殊的Token [CLS]，在cute的后面增加一个[SEP]表示第一个句子结束，在##ing后面也会增加一个[SEP]。注意这里的分词会把”playing”分成”play”和”##ing”两个Token，这种把词分成更细粒度的Word Piece的方法在前面的机器翻译部分介绍过了，这是一种解决未登录词的常见办法，后面的代码部分也会简单介绍。接着对每个Token进行3个Embedding：词的Embedding；位置的Embedding和Segment的Embedding。词的Embedding大家都很熟悉了，而位置的Embedding和词类似，把一个位置(比如2)映射成一个低维稠密的向量。而Segment只有两个，要么是属于第一个句子(segment)要么属于第二个句子，不管那个句子，它都对应一个Embedding向量。同一个句子的Segment Embedding是共享的，这样它能够学习到属于不同Segment的信息。对于情感分类这样的任务，只有一个句子，因此Segment id总是0；而对于Entailment任务，输入是两个句子，因此Segment是0或者1。 BERT模型要求有一个固定的Sequence的长度，比如128。如果不够就在后面padding，否则就截取掉多余的Token，从而保证输入是一个固定长度的Token序列，后面的代码会详细的介绍。第一个Token总是特殊的[CLS]，它本身没有任何语义，因此它会(必须)编码整个句子(其它词)的语义。 Mask LM 为了解决只能利用单向信息的问题，BERT使用的是Mask语言模型而不是普通的语言模型。Mask语言模型有点类似与完形填空——给定一个句子，把其中某个词遮挡起来，让人猜测可能的词。这里会随机的Mask掉15%的词，然后让BERT来预测这些Mask的词，通过调整模型的参数使得模型预测正确的概率尽可能大，这等价于交叉熵的损失函数。这样的Transformer在编码一个词的时候会(必须)参考上下文的信息。 但是这有一个问题：在Pretraining Mask LM时会出现特殊的Token [MASK]，但是在后面的fine-tuning时却不会出现，这会出现Mismatch的问题。因此BERT中，如果某个Token在被选中的15%个Token里，则按照下面的方式随机的执行： 80%的概率替换成[MASK]，比如my dog is hairy → my dog is [MASK] 10%的概率替换成随机的一个词，比如my dog is hairy → my dog is apple 10%的概率替换成它本身，比如my dog is hairy → my dog is hairy 这样做的好处是，BERT并不知道[MASK]替换的是哪一个词，而且任何一个词都有可能是被替换掉的，比如它看到的apple可能是被替换的词。这样强迫模型在编码当前时刻的时候不能太依赖于当前的词，而要考虑它的上下文，甚至更加上下文进行”纠错”。比如上面的例子模型在编码apple是根据上下文my dog is应该把apple(部分)编码成hairy的语义而不是apple的语义。 预测句子关系 在有些任务中，比如问答，前后两个句子有一定的关联关系，我们希望BERT Pretraining的模型能够学习到这种关系。因此BERT还增加了一个新的任务——预测两个句子是否有关联关系。这是一种Multi-Task Learing。BERT要求的Pretraining的数据是一个一个的”文章”，比如它使用了BookCorpus和维基百科的数据，BookCorpus是很多本书，每本书的前后句子是有关联关系的；而维基百科的文章的前后句子也是有关系的。对于这个任务，BERT会以50%的概率抽取有关联的句子(注意这里的句子实际只是联系的Token序列，不是语言学意义上的句子)，另外以50%的概率随机抽取两个无关的句子，然后让BERT模型来判断这两个句子是否相关。比如下面的两个相关的句子： [CLS] the man went to [MASK] store [SEP] he bought a gallon [MASK] milk [SEP] 下面是两个不相关的句子： [CLS] the man [MASK] to the store [SEP] penguin [MASK] are flight ##less birds [SEP] ELMO 和OpenAI GPT 的区别： UMLFiT比较复杂，而且效果也不是特别好，我们暂且不提。ELMo和OpenAI GPT的思想其实非常非常简单，就是用海量的无标注数据学习语言模型，在学习语言模型的过程中自然而然的就学到了上下文的语义关系。它们都是来学习一个语言模型，前者使用的是LSTM而后者使用Transformer，在进行下游任务处理的时候也有所不同，ELMo是把它当成特征。拿分类任务来说，输入一个句子，ELMo用LSTM把它扫一次，这样就可以得到每个词的表示，这个表示是考虑上下文的，因此”He deposited his money in this bank”和”His soldiers were arrayed along the river bank”中的两个bank的向量是不同的。下游任务用这些向量来做分类，它会增加一些网络层，但是ELMo语言模型的参数是固定的。而OpenAI GPT不同，它直接用特定任务来Fine-Tuning Transformer的参数。因为用特定任务的数据来调整Transformer的参数，这样它更可能学习到与这个任务特定的上下文语义关系，因此效果也更好。 Skip Thought Vector 在训练语料上的区别 和训练Word2Vec不同，Word2Vec只需要提供句子，而Skip Thought Vector需要文章(至少是段落)。论文使用的数据集是BookCorpus(http://yknzhu.wixsite.com/mbweb)，目前网站已经不提供下载了。BookCorpus的统计信息如下图所示，有一万多本书，七千多万个句子。 词汇扩展： 使用一个word2vec 的词向量去映射在训练句子向量中得到的向量，其中的映射关系是 $w$, 这个参数可以通过共现词进行学习。 笔记BERT 的创新点在于 mask LM和 predicting next sentence 这个多任务学习。mask 语言模型有点像完形填空，对一个句子，会随机mask 掉15% 的词语，让bert 来预测这些词语。这些词80% 是被替换成 [MASK], 10% 是被随机替换成一个词语，10% 是替换成单词本身。在预测句子关系中，随机50% 抽取两个无关的句子，50% 抽取两个有关联的句子，然后使用bert 模型判断这两个句子的相关性。在训练的时候是多任务学习。 BERT 和OpenAI GPT 的区别： 从训练目标上讲，bert 是同时上下文信息，而其他的都是单向的。]]></content>
      <categories>
        <category>NLP</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[逻辑回归概念]]></title>
    <url>%2F2019%2F06%2F09%2Flr%2F</url>
    <content type="text"><![CDATA[本文主要介绍逻辑回归（logistics regression）和决策树（Decision Tree）。逻辑回归从线性回归出发到逻辑回归，然后手推公式和相关的一些特点；介绍一下决策树的特点。 逻辑回归逻辑回归是线性模型，虽然叫做”回归“，究其原因 逻辑回归从线性回归引申而来，对回归的结果进行 logistic 函数运算，将范围限制在[0,1]区间，并更改损失函数为二值交叉熵损失，使其可用于2分类问题(通过得到的概率值与阈值比较进行分类)。逻辑回归是广义上的线性模型，然后最后的sigmoid 加入了非线性。是处理线性问题的。 公式推导从线性回归问题到逻辑回归过程的推导。 线性二分模型： $$f ( x ) = \theta ^ { T } x$$ 逻辑回归决策函数是将此线性二分类嵌套一个sigmoid函数： $$ f ( x ) = \frac { 1 } { 1 + e ^ { - \theta ^ { T } x } }$$ 损失函数：如果用平方误差（MSE）作为逻辑回归的损失函数,那么函数曲线将是跳跃式的,非凸的(non-convex),原因是logistic函数将数据范围限制在[0,1]区间,而真实标签值非0即1.最小化 MSE 损失容易陷入局部极小点.逻辑回归损失是如下的分情况的凸函数(单个x与y的损失)。 $$P ( y = 1 | x ; \theta ) = h _ { \theta } ( x )$$ $$P ( y = 0 | x ; \theta ) = 1 - h _ { \theta } ( x )$$最初是上述的分段函数，合并成下面的函数，方便计算。$$p ( y | x ; \theta ) = \left( h _ { \theta } ( x ) \right) ^ { y } \left( 1 - h _ { \theta } ( x ) \right) ^ { 1 - y }$$使用最大似然的思想求解。假设我们有n个独立的训练样本{(x1, y1) ,(x2, y2),…, (xn, yn)}，y={0, 1}。那每一个观察到的样本(xi, yi)出现的概率是： 上述似然函数乘法太难算了，然后使用log 将其改为加法，变成了对数似然函数。$$J( \theta ) = \log ( L ( \theta ) ) = \sum _ { i = 1 } ^ { m } y ^ { ( i ) } \log \left( h \left( x ^ { ( i ) } \right) \right) + \left( 1 - y ^ { ( i ) } \right) \log \left( 1 - h \left( x ^ { ( i ) } \right) \right)$$ 求导优化问题sigmoid 函数的特殊性质：$$\sigma ^ { \prime } ( x ) = \sigma ( x ) ( 1 - \sigma ( x ) )$$ 分成三部分求导： 用L(θ)对θ求导，得到：$$\begin{split}\frac { d } { d \theta _ { i } } \operatorname { loss } ( \theta ) &amp;= \left( y \frac { 1 } { \sigma \left( \theta ^ { T } x \right) } - ( 1 - y ) \frac { 1 } { 1 - \sigma \left( \theta ^ { T } x \right) } \right) \frac { d } { d \theta _ { i } } \sigma \left( \theta ^ { T } x \right) \\&amp;= \left( y \frac { 1 } { \sigma \left( \theta ^ { T } x \right) } - ( 1 - y ) \frac { 1 } { 1 - \sigma \left( \theta ^ { T } x \right) } \right) \sigma \left( \theta ^ { T } x \right) \left( 1 - \sigma \left( \theta ^ { T } x \right) \right) \frac { d } { d \theta _ { i } } \theta ^ { T } x \\&amp;= \left( y \left( 1 - \sigma \left( \theta ^ { T } x \right) \right) - ( 1 - y ) \sigma \left( \theta ^ { T } x \right) \right) x _ { i } \\&amp;= \left( y - h _ { \theta } ( x ) \right) x _ { i }\end{split}$$ 注意一会儿有 $\sum$ 一会儿没有的，其实我们更倾向于不用，采用矩阵相乘的方式更加简洁。只是在表达似然函数，使用$ \sum$更加直观$$\theta _ { i } : = \theta _ { j } + \alpha \left( y ^ { ( i ) } - h _ { \theta } \left( x ^ { ( i ) } \right) \right) x _ { j } ^ { ( i ) }$$ 逻辑回归的特点 优点： LR 能以概率的形式输出结果,而非只是 0,1 判定， 可以做 ranking model； LR 的可解释性强,可控度高； 训练快 缺点： 容易欠拟合，一般准确度不太高 只能处理两分类问题. (可以应用多个逻辑回归实现多分类,类似SVM的方式; 另外对于父子类别同时分类的情况,使用逻辑回归要比Softmax等方式效果好) “海量离散特征+简单模型” 同“少量连续特征+复杂模型”的权衡。既可以离散化用线性模型，也可以用连续特征加深度学习 为什么对特征进行离散化 特征从连续变量状态到离散化的初衷在于我们认为不同的区间对于最后的结果的重要性是不同的。同样在工业界，很少直接将连续值作为逻辑回归模型的特征输入，而是将连续特征离散化为一系列0、1特征(one-hot编码)交给逻辑回归模型，这样做的优势有以下几点 离散特征的增加和减少都很容易，易于模型的快速迭代； 稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展； 离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄&gt;30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰； 单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合 离散化后可以进行特征交叉 究其原因，使用 LR+离散模型在于可控可解释。而GBDT 直接使用连续的变量，一方面的原因在于如果特征过多，那么GBDT 是跑不动的。 为什么LR模型的损失函数是交叉熵,而线性回归模型的损失函数却是最小二乘呢？能否随意确定一个损失函数作为目标呢？ 模型的损失函数由各自的响应变量y的概率分布决定，对于线性回归模型，其输出是连续值，所以我们对于该问题假设y服从正态分布；相对的，LR模型一般用来解决二分类问题，所以其输出是0/1，故而我们假设其输出服从伯努利分布；而进一步地，两者的损失函数都是通过极大似然估计推导的来的，所以模型的损失函数并非随意确定。分类模型与回归模型之间有种种联系,比如 SVM 模型可以看作逻辑回归加L2正则项, 并使用了不同的损失函数. 为什么不使用回归模型来做分类?这是一种不好的做法, 因为阈值不好确定, 随着数据集的变动, 阈值也需要有较大变化. 正则项 L2 解决过拟合 L1 解决数据稀疏性 L1和L2正则先验分别服从什么分布从信息论的角度看，向系统加入了正确先验这个信息，肯定会提高系统的性能。两者的差别感性的理解？L1是拉普拉斯分布，L2是高斯分布。拉普拉斯分布：$$f ( x | \mu , b ) = \frac { 1 } { 2 b } e ^ { - \frac { | x - \mu | } { b } }$$高斯分布：$$f \left( x | \mu , \sigma ^ { 2 } \right) = \frac { 1 } { \sqrt { 2 \pi \sigma ^ { 2 } } } e ^ { - \frac { ( x - \mu ) ^ { 2 } } { 2 \sigma ^ { 2 } } }$$ Decision tree主要介绍一下 决策树的特点。 从这次学习中明显的感受到这个 decision tree 是非常容易过拟合的。 We can make our tree more complex by increasing its size , which will result in more and more partitions trying to emulate the circular boundary. 优点在于：可以handle 非线性的变化。decision tree 给人的感觉就是线性或者离散的(category)的都是可以使用，因为decision tree 得到就是一个离散的结果，最大的缺点就是容易过拟合。 This brings us to the biggest problem associated with Decision Trees, that is, they are highly biased class of models. You can make a decision tree model on your training set which might outperform all other algorithms but it’ll prove to be a poor predictor on your test set. You’ll have to rely heavily on pruning and cross validation to get a non-over-fitting model with Decision Trees. 过拟合是可以通过剪枝或者 cross validation 进行缓解 overfit的效果的或者使用 random forest随机性进行”中和“。 This problem of over-fitting is overcome to large extent by using Random Forests, which are nothing but a very clever extension of decision trees. But random forest take away easy to explain business rules because now you have thousands of such trees and their majority votes to make things complex. Also by decision trees have forced interactions between variables , which makes them rather inefficient if most of your variables have no or very weak interactions. 使用范围：1 数值型：数值型目标变量则可以从无限的数值集合中取值，如0.100，42.001等 (数值型目标变量主要用于回归分析)2 标称型：标称型目标变量的结果只在有限目标集中取值，如真与假(标称型目标变量主要用于分类) 决策树的工作原理原始的数据集： 编号 色泽 根蒂 敲声 纹理 脐部 触感 好瓜1 青绿 蜷缩 浊响 清晰 凹陷 硬滑 是2 乌黑 蜷缩 沉闷 清晰 凹陷 硬滑 是3 乌黑 蜷缩 浊响 清晰 凹陷 硬滑 是4 青绿 蜷缩 沉闷 清晰 凹陷 硬滑 是5 浅白 蜷缩 浊响 清晰 凹陷 硬滑 是6 青绿 稍蜷 浊响 清晰 稍凹 软粘 是7 乌黑 稍蜷 浊响 稍糊 稍凹 软粘 是8 乌黑 稍蜷 浊响 清晰 稍凹 硬滑 是9 乌黑 稍蜷 沉闷 稍糊 稍凹 硬滑 否10 青绿 硬挺 清脆 清晰 平坦 软粘 否11 浅白 硬挺 清脆 模糊 平坦 硬滑 否12 浅白 蜷缩 浊响 模糊 平坦 软粘 否13 青绿 稍蜷 浊响 稍糊 凹陷 硬滑 否14 浅白 稍蜷 沉闷 稍糊 凹陷 硬滑 否15 乌黑 稍蜷 浊响 清晰 稍凹 软粘 否16 浅白 蜷缩 浊响 模糊 平坦 硬滑 否17 青绿 蜷缩 沉闷 稍糊 稍凹 硬滑 否 信息熵和信息增益 熵（entropy）： 熵指的是体系的混乱的程度信息增益（information gain）： 在划分数据集前后信息发生的变化称为信息增益，信息增益越大，确定性越强。 决策树的生成过程就是 最优划分属性的选择的过程。 一共有三种策略可以进行选择，大致分为了 ID3 [Quinlan, 1986]、C4.5 [Quinlan, 1993]、CART [Breiman et al., 1984] 三种。一般来说我们希望选择一个属性之后，其分支节点所包含的样本尽可能属于同一个类别，即结点的纯度 (purity)越来越高。 ID3 树中最优划分属性计算举例 信息增益使用上面的数据集进行说明。在决策树学习开始时，根结点包含 𝐷 中所有样例，正例占 $p _ { 1 } = \frac { 8 } { 17 }$，反例占$ p_2=\frac{9}{17}$。根结点的信息熵（下面我们都以比特为单位计算）为： $$H ( D ) = - \sum _ { k = 1 } ^ { 2 } p ( k ) \log _ { 2 } p ( k ) = - \left( \frac { 7 } { 17 } \log _ { 2 } \frac { 7 } { 17 } + \frac { 9 } { 17 } \log _ { 2 } \frac { 9 } { 17 } \right) = 0.998$$ 然后我们要计算出与当前属性集合 {色泽、根蒂、敲声、纹理、脐部、触感}中每个属性的 信息增益，也就是对应的互信息。以属性“色泽”为例，它有 3 个可能取值：{青绿、乌黑、浅白}。以该属性对数据集进行划分，可以得到 3 个子集，分别为：$D_1(色泽=青绿) $、$ D_2(色泽=乌黑)$ 、$D_3(色泽=浅白)$。 对子集$ D_1$ 来说，包含了编号为 {1,4,6,10,13,17} 的 6 个样例，其中正例为 {1,4,6}，占$ p_1=36$ ；反例为 {10,13,17}，占 $p_1=36$。计算其熵为：$$H \left( D _ { 1 } \right) = - \left( \frac { 3 } { 6 } \log _ { 2 } \frac { 3 } { 6 } + \frac { 3 } { 6 } \log _ { 2 } \frac { 3 } { 6 } \right) = 1.000$$ 依次可以计算另外两个子集的信息熵为：$$H \left( D _ { 2 } \right) = 0.918$$$$H \left( D _ { 3 } \right) = 0.722$$最终可以计算数据集 $D$ 的类别信息在属性“色泽”熵的信息增益（也可以理解为类别与“色泽”属性之间的互信息）为： $$\begin{split}Gain(D, 色泽) &amp;= H ( D ) - \sum _ { v = 1 } ^ { 3 } p ( v ) H \left( D _ { v } \right) \\&amp;= 0.998 - \left( \frac { 6 } { 17 } \times 1.000 + \frac { 6 } { 17 } \times 0.918 + \frac { 5 } { 17 } \times 0.722 \right) \\&amp;= 0.109\end{split}$$重复上述的计算步骤，我们可以计算出其他属性的信息增益： $$\begin{split}Gain(𝐷,根蒂) &amp;=0.143 \\Gain(𝐷,敲声) &amp;= 0.141 \\Gain(𝐷,纹理) &amp;= 0.381\\Gain(𝐷,脐部) &amp;= 0.289 \\Gain(𝐷,触感) &amp;=0.006\end{split}$$ 经过比较，发现采用“纹理”进行划分得到的信息增益最大，于是它被选为划分属性。下图给出了根据“纹理”属性划分之后的数据子集： 对每一个数据子集按照上边的步骤继续划分下去就能得到最终的决策树（需要注意的是每次样例子集中的属性不包含父结点中划分所依赖的属性），如下图所示： 信息增益率 采用信息增益来进行划分属性的决策有一个潜在的问题，当某一个属性的取值种类非常多时，对应每一个属性取值的样本子集，其分类的信息熵可能会变得很小。 $$\begin{split}Gain ( D , a ) &amp;= H ( D ) - \sum _ { v = 1 } ^ { V } \frac { \left| D _ { v } \right| } { | D | } H \left( \left| D _ { v } \right| \right) \\&amp;= - \frac { 8 } { 17 } \log _ { 2 } \frac { 8 } { 17 } - \frac { 8 } { 17 } \log _ { 2 } \frac { 8 } { 17 } - \sum _ { v = 1 } ^ { 17 } \frac { 1 } { 17 } \times 0 \\&amp;= 0.9975\end{split}$$最后计算出来的信息增益很大。但是显然，用“编号”属性来作为结点的划分是没有意义的。思考其中的问题在于，对数函数并不是线性的，信息量的减少速度大于类别数量的增加速度。信息增益准则对取值数目较多的属性有所偏好，为了减小这种偏好，C4.5 决策树 采用 信息增益率 (gain ratio) 来选择最优划分属性。其定义如下：$$Gain_ { \mathrm { ratio } } ( D , a ) = \frac { \operatorname { Gain } ( \mathrm { D } , \mathrm { a } ) } { \mathrm { IV } ( \mathrm { a } ) }$$ 其中$$\mathrm { IV } ( a ) = - \sum _ { v = 1 } ^ { V } \frac { \left| D _ { v } \right| } { | D | } \log \frac { \left| D _ { v } \right| } { | D | } = H ( a )$$到这里，我们就可以发现，信息增益率是用属性分类的信息熵对由属性分类引起的互信息熵进行了归一。属性的种类越多其信息熵通常也会越大。 最后一点需要注意的是，增益率准则虽然减少了对取值数目较多的属性依赖，但是增加了对取值数目较少的属性偏好。因此， C4.5 并没有直接选择增益率最大的候选划分属性，而是使用了一个启发式：先从候选划分属性中找出 信息增益 高于 平均水平 的属性，再从中选择 增益率 最高的。 基尼指数 - CART 最后介绍一种选择划分属性的依据是使用 基尼指数 (Gini index)。数据集合 𝐷 的纯度可用基尼指数来度量：$$Gini ( D ) = \sum _ { k = 1 } ^ { | \mathcal { Y } | } \sum _ { k ^ { \prime } \neq k } p _ { k } p _ { k ^ { \prime } } = 1 - \sum _ { k = 1 } ^ { \mathcal { V } } p _ { k } ^ { 2 }$$直观来看，$Gini(𝐷) $ 反映了从数据集 𝐷 中随机抽取两个样本，其类别标记不一致的概率。因此，$Gini(𝐷) $ 越小，则数据集 𝐷 的纯度越高。 对特定属性 𝑎 的基尼指数定义如下：$$Gini_{index}( D , a ) = \sum _ { v = 1 } ^ { V } \frac { \left| D _ { v } \right| } { | D | } \operatorname { Gini } \left( D _ { v } \right) $$我们在候选属性集合 𝐴 中，选择那个使得划分后基尼指数最小的属性作为最优划分属性，即： $$a _ { * } = \arg \min _ { a \in A } Gini_{index } ( D , a )$$ 采用基尼指数作为划分属性的判据的决策树是一种 CART 决策树。 决策树的优缺点优点： 决策树易于理解（能够写出判断的路径，逻辑表达式）和实现， 人们知道该如何去优化。可以处理缺省数据，意味着数据的准备工作是比较简单的。相对比其他的技术，往往需要一般化之类的操作。能够处理数值型和常规性属性。 缺点： 容易过拟合对于各个类别不一致的数据，决策树当中信息增益的结果偏向于那些具有更多数值的特征。 参考文献 决策树及决策树生成与剪枝]]></content>
      <tags>
        <tag>LR</tag>
        <tag>Decision tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SVM All You Need to Know]]></title>
    <url>%2F2019%2F06%2F08%2Fsvm-all-you-need%2F</url>
    <content type="text"><![CDATA[本文主要介绍SVM 相关内容，包括理论原理、在线性可分条件下的公式推导和 SVM的应用特点。最后综合 LR 和 Decision Tree的这篇博客，给出了一些小的建议。 SVM 理论支持向量机分为三个部分，线性可分支持向量机、线性支持向量机、非线性支持向量机。 SVM 原理SVM 是一种二类分类模型。它的基本模型是在特征空间中寻找间隔最大化的分离超平面的线性分类器。 当训练样本线性可分时，通过硬间隔最大化，学习一个线性分类器，即线性可分支持向量机；当训练数据近似线性可分时，引入松弛变量，通过软间隔最大化，学习一个线性分类器，即线性支持向量机；当训练数据线性不可分时，通过使用核技巧及软间隔最大化，学习非线性支持向量机。以上各种情况下的数学推到应当掌握，硬间隔最大化（几何间隔）、学习的对偶问题、软间隔最大化（引入松弛变量）、非线性支持向量机（核技巧）。 SVM 为什么采用间隔最大化当训练数据线性可分时，存在无穷个分离超平面可以将两类数据正确分开。感知机利用误分类最小策略，求得分离超平面，不过此时的解有无穷多个。线性可分支持向量机利用间隔最大化求得最优分离超平面，这时，解是唯一的。另一方面，此时的分隔超平面所产生的分类结果是最鲁棒的，对未知实例的泛化能力最强。可以借此机会阐述一下几何间隔以及函数间隔的关系。 为什么要将求解 SVM 的原始问题转换为其对偶问题一是对偶问题往往更易求解，当我们寻找约束存在时的最优点的时候，约束的存在虽然减小了需要搜寻的范围，但是却使问题变得更加复杂。为了使问题变得易于处理，我们的方法是把目标函数和约束全部融入一个新的函数，即拉格朗日函数，再通过这个函数来寻找最优点。二是可以自然引入核函数，进而推广到非线性分类问题。 为什么 SVM 要引入核函数当样本在原始空间线性不可分时，可将样本从原始空间映射到一个更高维的特征空间，使得样本在这个特征空间内线性可分。而引入这样的映射后，所要求解的对偶问题的求解中，无需求解真正的映射函数，而只需要知道其核函数。核函数的定义：K(x,y)=&lt;ϕ(x),ϕ(y)&gt;，即在特征空间的内积等于它们在原始样本空间中通过核函数 K 计算的结果。一方面数据变成了高维空间中线性可分的数据，另一方面不需要求解具体的映射函数，只需要给定具体的核函数即可，这样使得求解的难度大大降低。 为什么SVM对缺失数据敏感这里说的缺失数据是指缺失某些特征数据，向量数据不完整。SVM 没有处理缺失值的策略。而 SVM 希望样本在特征空间中线性可分，所以特征空间的好坏对SVM的性能很重要。缺失特征数据将影响训练结果的好坏。 SVM 核函数之间的区别一般选择线性核和高斯核，也就是线性核与 RBF 核。 线性核：主要用于线性可分的情形，参数少，速度快，对于一般数据，分类效果已经很理想了。 RBF 核：主要用于线性不可分的情形，参数多，分类结果非常依赖于参数。有很多人是通过训练数据的交叉验证来寻找合适的参数，不过这个过程比较耗时。 如果 Feature 的数量很大，跟样本数量差不多，这时候选用线性核的 SVM。 如果 Feature 的数量比较小，样本数量一般，不算大也不算小，选用高斯核的 SVM。 以上是几个问题在面试中遇到 SVM 算法时，几乎是必问的问题，另外，大家一定要做到自己可以推导集合间隔、函数间隔以及对偶函数，并且理解对偶函数的引入对计算带来的优势。 支持向量 如图所示，上面只有三个点与求解的优化问题有关，它们就叫做支持向量。 SVM公式推导(线性可分条件下)假定样本空间如下$${ ( x _ { 1 } , y _ { 1 } ) , ( x _ { 2 } , y _ { 2 } ) , \ldots , ( x _ { N } , y _ { N } ) }$$ 共有N个向量，其中$x_k$是一个特征向量而不是一个单一数值。 这是一个二分类问题，所以$y=+1 $或者$ y=−1$。那么我们就可以得到 那么，我们可以得到$$y _ { i } \cdot ( w ^ { T } x _ { i } + b ) \geq 1 , i = 1,2 , \ldots , N$$ 因为我们现在只讨论线性可分情况下的支持向量机，那么在这个样本空间中一定存在一个超平面可以将样本集按照y的值分割城两个部分，这个超平面可以表示为$$w ^ { T } x + b = 0$$ 根据这个超平面的表达式以及第一步推到中我们得到的结果，可以得到这个样本集中任意一个样本点距离超平面的距离：$$\gamma = \frac { | w ^ { T } x + b | } { | w | } \geq \frac { 1 } { | w | }$$由此，我们还可以进一步得到整个margin的宽度：$$\gamma = \frac { 2 } { | w | }$$ 由此，根据第一步和第三步的结果，我们可以得到最基本的目标函数： $$\arg \max _ { w , b } \frac { 2 } { | w | } , \text {s.t. } y _ { i } ( w ^ { T } x _ { i } + b ) \geq 1 , i = 1,2 , \ldots , N$$ 我们还可以对这个目标函数进一步做变化： $$\arg \min _ { w , b } \frac { 1 } { 2 } | w | ^ { 2 }, \text {s.t. } y _ { i } ( w ^ { T } x _ { i } + b ) \geq 1 , i = 1,2 , \ldots , N $$ 我们无法继续直接进行计算了，因此引入拉格朗日乘子$$L ( w , b , \alpha ) = \frac { 1 } { 2 } | w | ^ { 2 } + \sum _ { i } \alpha _ { i } [ 1 - y _ { i } ( w ^ { T } x _ { i } + b ) ]$$ 对w和b分别求L的偏导，并令其偏导数等于0：$$\frac { \partial L } { \partial w } = w - \sum _ { i } \alpha _ { i } y _ { i } x _ { i } = 0 \Rightarrow w = \sum _ { i } \alpha _ { i } y _ { i } x _ { i }$$$$\frac { \partial L } { \partial b } = \sum _ { i } \alpha _ { i } y _ { i } = 0$$ 将第七步得到的w和b代入L函数 至此，我们的目标函数已经变成了$$\arg \max _ { \alpha } ( \sum _ { i } \alpha _ { i } - \frac { 1 } { 2 } \sum _ { i } \sum _ { j } \alpha _ { i } \alpha _ { j } y _ { i } y _ { j } x _ { i } ^ { T } x _ { j } )$$$$\text { s.t. } \sum _ { i } \alpha _ { i } y _ { i } = 0$$$$\alpha _ { i } \geq 0 , i = 1,2 , \ldots , N$$ 用数值方法解出α以后，我们带入式子 7就可以得到 $$w^ { * } = \sum _ { i } \alpha _ { i } ^ { * } y _ { i } x _ { i }$$ SVM的特点SVM的优点：就是当大量的特征出现的时候，使用SVM handle large feature spaces; 然而此时 LR 不是一个很好的选择。 SVM can handle large feature spaces which makes them one of the favorite algorithms in text analysis which almost always results in huge number of features where logistic regression is not a very good choice. SVM Pros: Can handle large feature space Can handle non-linear feature interactions Do not rely on entire data SVM Cons: Not very efficient with large number of observations It can be tricky to find appropriate kernel sometimes TakeOff首先使用 LR 进行尝试，不妨试一下 DT，然后 如果特征比较多，但是数据量不是很多，这个时候使用SVM。Always start with logistic regression, if nothing then to use the performance as baselineSee if decision trees (Random Forests) provide significant improvement. Even if you do not end up using the resultant model, you can use random forest results to remove noisy variablesGo for SVM if you have large number of features and number of observations are not a limitation for available resources and time附上链接：https://www.edvancer.in/logistic-regression-vs-decision-trees-vs-svm-part2/]]></content>
      <tags>
        <tag>SVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NLP Papers Reading-Sentence Embedding]]></title>
    <url>%2F2019%2F06%2F01%2Fnlp-papers-reading-sentence-embedding%2F</url>
    <content type="text"><![CDATA[Why Consider Sentence Embedding?One simple way you could do this is by generating a word embedding for each word in a sentence, adding up all the embeddings and divide by the number of words in the sentence to get an “average” embedding for the sentence. 句子embedding的表示 = words emebdding，可能使用 TF-IDF or SIF 作为weights 进行优化。 Alternatively, you could use a more advanced method which attempts to add a weighting function to word embeddings which down-weights common words. This latter approach is known as Smooth Inverse Frequency (SIF). These methods can be used as a successful proxy for sentence embeddings. However, this “success” depends on the dataset being used and the task you want to execute. So for some tasks these methods could be good enough 上述方法的主要缺点：语序；文字在上下文中才有意义；阅读理解，不同的句子是相同的意思，却得到不同的embedding；依赖于前期处理，如分词。 However, there are a number of issues with any of these types of approaches: They ignore word ordering. This is obviously problematic. It’s difficult to capture the semantic meaning of a sentence. The word crash can be used in multiple contexts, e.g. I crashed a party, the stock market crashed, or I crashed my car. It’s difficult to capture this change of context in a word embedding. Sentence length becomes problematic. With sentences we can chain them together to create a long sentence without saying very much, The Philadelphia Eagles won the Super Bowl, The Washington Post reported that the Philadelphia Eagles won the Super Bowl, The politician claimed it was fake news when the Washington Post reported that the Philadelphia Eagles won the Super Bowl, and so on. All these sentences are essentially saying the same thing but if we just use word embeddings, it can be difficult to discover if they are similar. They introduce extra complexity. When using word embeddings as a proxy for sentence embeddings we often need to take extra steps in conjunction with the base model. For example, we need to remove stop words, get averages, measure sentence length and so on. sentence embedding的应用场景：Similar approaches can be used to go beyond representations and semantic search, to document classification and understanding and eventually document summarizing or generation. Words Embed平均词向量与TFIDF加权平均词向量SIF加权平均词向量来自论文 A simple but tough-to-beat baseline for sentence embeddings，更多信息可以参考这里。在大家都从无监督学习走向有监督学习的时候，这个无监督的方法和神经网络的效果是旗鼓相当的。 利用n-grams embeddingfasttext 介绍。简单说 n-gram 是一种概念，可以细化成两部分：character-level 和word-level，前者是可以用来补充词汇，加强对于不常见词的表示能力，后者是对于词序的补充。 DAN（Deep Unordered Composition Rivals Syntactic Methods for Text Classification）其实DAN(Deep Averaging Networks)应该属于Bag of Words类的算法。因为比较特殊，单独列出来。 它是在对所有词语取平均后，在上面加上几层神经网络。特殊的地方在于它在sentiment analysis中表现也不错，这在BOW类方法中比较罕见。 文中提出了DAN(Deep average network)，说白了就是对于一个句子或者一个段落，把每个单词的embedding进行平均求和，得到第一层固定维度的向量，然后在套几层全连接神经网络。本质来讲，这个模型没有考虑单词之间的顺序，not在第一个位置和在最后一个位置对于DAN来讲输入都是一样的，所以自然conver不住这种情况。这是模型本身的问题，没有办法改进，除非换模型，比如textcnn就能很好的解决这种情况对于否定词敏感，比如but,not等，常常判断为negative。训练速度快，且结果较好，和Syntactic Composition性能差不多，但是消耗的计算资源少作为有监督学习任务来讲，可以试一试。但是由于全连接层，无法进行无监督学习。相反，NBOW可以无监督学习，比如文本相似度计算等。当然。对于DAN而言，可以通过迁移学习，预训练好全连接参数，实现无监督学习 总结一下：对简单的任务来说，用简单的网络结构进行处理基本就够了，但是对比较复杂的任务，还是依然需要更复杂的网络结构来学习sentence representation的。 Unsupervised Sentence Embed基于Encoder-decoder的Skip-Thought Vectorsskip-thoughts 中提及一个重要的方法是 词汇扩展。具体来说我们可以先用海量的语料训练一个Word2Vec，这样可以把一个词映射到一个语义空间，我们把这个向量叫做 $ V_{w 2 v} $。而我们之前训练的得到的输入向量也是把一个词映射到另外一个语义空间，我们记作$V_{r n n}$. 我们假设它们之间存在一个线性变换$ f: V_{w 2 v} \rightarrow V_{r n n}$。这个线性变换的参数是矩阵W，使得$V_{r n n}$= $W V_{w 2 v} $。那怎么求这个变换矩阵W呢？因为两个训练语料会有公共的词(通常训练word2vec的语料比skip vector大得多，从而词也多得多)。因此我们可以用这些公共的词来寻找 $W$。寻找的依据是：遍历所有可能的W，使得$ W V_{w 2 v} $和$V_{rnn}$尽量接近。 对于双向训练（不是使用了两个模型，而是使用正反两种不同顺序不同的训练数据集）此外还训练了bi-skip向量，它是这样得到的：首先训练1200维的uni-skip，然后句子倒过来，比如原来是”aa bb”、”cc dd”和”ee ff”，我们是用”cc dd”来预测”aa bb”以及”ee ff”，现在反过来变成”ff ee”、”dd cc”和”bb aa”。这样也可以训练一个模型，当然也就得到一个encoder(两个decoder不需要了)，给定一个句子我们把它倒过来然后也编码成1200为的向量，最后把这个两个1200维的向量拼接成2400维的向量。 模型训练完成之后还需要进行词汇扩展。通过BookCorpus学习到了20,000个词，而word2vec共选择了930,911词，通过它们共同的词学习出变换矩阵W，从而使得我们的Skip Thought Vector可以处理930,911个词。（最终得到还是语言模型） 这篇论文中的要点 训练的时候在远大于 word2vec 的训练集上展开。如 skip-thought 中使用的 BookCorpus 数据集中是千万级别的 words(984,846,357)，在word2vec 中英文的 wikicorpus 是百万 (120 million words)， 所以相差一个数量级。 objective function 是语言模型（类似 machine translation), 给定上下一个句子，然后预测中心句子的概率，不断的最大化的过程。 词汇扩展，使用了一个word2vec 中的embedding 补充在训练数据集中没有出现过的 word，从word2vec 中的word 到该模型中的 word embedding 是学习了一个matrix，转换关系。缓解了oov 问题；关于oov 问题，句子的相似度不是exact的相似，而是一种 在语法和语义上的近似相似。 Continuing the tour of older papers that started with our ResNet blog post, we now take on Skip-Thought Vectors by Kiros et al. Their goal was to come up with a useful embedding for sentences that was not tuned for a single task and did not require labeled data to train. They took inspiration from Word2Vec skip-gram (you can find my explanation of that algorithm here) and attempt to extend it to sentences. Changing a single word has had almost no effect on the meaning of that sentence. To account for these word level changes, the skip-thought model needs to be able to handle a large variety of words, some of which were not present in the training sentences. The authors solve this by using a pre-trained continuous bag-of-words (CBOW) Word2Vec model and learning a translation from the Word2Vec vectors to the word vectors in their sentences. Below are shown the nearest neighbor words after the vocabulary expansion using query words that do not appear in the training vocabulary: 论文描述了一种通用、分布式句子编码器的无监督学习方法。使用从书籍中提取的连续文本，训练了一个编码器-解码器模型，借鉴了word2vec中skip-gram模型，通过一句话来预测这句话的上一句和下一句。语义和语法属性一致的句子被映射到相似的向量表示。接着引入一个简单的词汇扩展方法来编码不再训练集内的单词，令词汇量扩展到一百万词。本文的模型被称为skip-thoughts，生成的向量称为skip-thought vector。模型采用了当下流行的端到端框架，通过搜集了大量的小说作为训练数据集，将得到的模型中encoder部分作为feature extractor，可以给任意句子生成vector。 skip-thought模型结构借助了skip-gram的思想。在skip-gram中，是以中心词来预测上下文的词；在skip-thought同样是利用中心句子来预测上下文的句子。 论文采用了GRU-RNN作为encoder和decoder，encoder部分的最后一个词的hiddenstate作为decoder的输入来生成词。这里用的是最简单的网络结构，并没有考虑复杂的多层网络、双向网络等提升效果。decoder部分也只是一个考虑了encoder last hidden state的语言模型，并无其他特殊之处，只是有两个decoder，是一个one maps two的情况，但计算方法一样。模型中的目标函数也是两个部分，一个来自于预测下一句，一个来自于预测上一句。 传说中的 objective function or loss function 是下面这个样子： 我们将构造一个类似于自编码器的序列到序列结构，但是它与自编码器有两个主要的区别。第一，我们有两个 LSTM 输出层：一个用于之前的句子，一个用于下一个句子；第二，我们会在输出 LSTM 中使用教师强迫（teacher forcing）。这意味着我们不仅仅给输出 LSTM 提供了之前的隐藏状态，还提供了实际的前一个单词。 看上去，Skip-thought和Skip-gram挺象。唯一的遗憾是Skip-thought的decoder那部分，它是作为language modeling来处理的. 从这里的讲解知道这个是不存在 ”正负“样本的， 这个的损失函数是 正确的上下句和生成的上下句之间的reconstruction error。 The end product of Skip-Thoughts is the Encoder. The Decoders are thrown away after training. The trained encoder can then be used to generate fixed length representations of sentences which can be used for several downstream tasks such as sentiment classification, semantic similarity, etc. The encoder utilises a word embedding layer that serves as a look up table. This converts each word in the input sentence to its corresponding word embedding, effectively converting the input sentence into a sequence of word embeddings. This embedding layer is also shared with both of the decoders. The model is then trained to minimise the reconstruction error of the previous and next sentences using the resulting embedding h(i) generated from sentence s(i) after it is passed through the encoder. Back propagating the reconstruction error from the decoder allows the encoder to learn the best representation of the input sentence while capturing the relation between itself and the surrounding sentences.Skip-Thoughts is designed to be a sentence encoder and the result is that the decoders are actually discarded after the training process. The encoder along with the word embedding layer is used as a feature extractor able to encode new sentences that are fed through it. Using cosine similarity on the resulting encoded sentence embeddings, provides a powerful semantic similarity mechanism, where you can measure how closely two sentences relate in terms of meaning as well as syntax. Encoder Network: The encoder is typically a GRU-RNN which generates a fixed length vector representation h(i) for each sentence S(i) in the input. The encoded representation h(i) is obtained by passing final hidden state of the GRU cell (i.e. after it has seen the entire sentence) to multiple dense layers. Decoder Network: The decoder network takes this vector representation h(i) as input and tries to generate two sentences — S(i-1) and S(i+1), which could occur before and after the input sentence respectively. Separate decoders are implemented for generation of previous and next sentences, both being GRU-RNNs. The vector representation h(i) acts as the initial hidden state for the GRUs of the decoder networks. 词汇扩展 当初有个面试官问道的训练样本不足的问题，现在给出答案，一种是大量的数据集，因为这个是无监督的学习，不需要标签，所以使用了大量的小说作为训练集；对于相似句子的定义，不是 exact的相似，只要在语法或者语义上相似，那么这个就可以看做相同的样本；然后还使用了 预训练的 模型进行 vocabulary 中词汇的补充。 作者在训练完过后用在Google News dataset上预训练的模型对Vocabulary进行了词汇扩展主要是为了弥补我们的 Decoder 模型中词汇不足的问题。具体的做法就是：(from https://www.cnblogs.com/jiangxinyang/p/9638991.html) 该思路借鉴于Tomas Mikolov的一篇文章Exploiting Similarities among Languages for Machine Translation中解决机器翻译missing words问题的思路，对训练集产生的词汇表V(RNN)进行了扩展，具体的思路可参考Mikolov的文章，达到的效果是建立了大数据集下V(Word2Vec)和本文V(RNN)之间的映射，V(Word2Vec)的规模远远大于V(RNN)，论文中V(RNN)包括了20000个词，V(Word2Vec)包括了930000多个词，成功地解决了这一问题，使得本文提出的无监督模型有大的应用价值。 评价观点这个方法只是适用于长文本，要求是至少有两个衔接的句子，思想和skip-gram 比较相近。 源码google 的实现作者的实现论文 Quick-Thought vectorsintuition: 基于生成的model， the models are trained to reconstruct the surface form of a sentence, but sometimes words are irrelevant to the meaning of the sentence as well 生成模型的计算成本比较高， 而文中的模型是一种判别模型，所以从原理上这种计算的效率就远远高于生成模型。 Viewing generation as choosing a sentence from all possible sentences, this can be seen as a discriminative approximation to the generation problem. 在实践中经验之谈， 不是要求分类器去判别 positive / negative, 只是要求分类器对于 ground-truth contexts than contrastive contexts more plausible。这个比前者在实验结果上是好的。 论文中的一些观点：encoder-decoder based sequence models 虽然效果好，但是 slow to train on large amounts of data. 另一方面, bag-of-words 虽然高效，但是无法捕捉到 word order 信息。 2018年发表的论文An efficient framework for learning sentence representations提出了一种简单且有效的框架用于学习句子表示。和常规的编码解码类模型（如skip-thoughts和SDAE）不同的是，本文采用一种分类器的方式学习句子表示。具体地，模型的输入为一个句子$s$以及一个候选句子集合$S_{cand}$，其中$S_{cand}$包含一个句子$s_{ctxt}$是$s$的上下文句子（也就是$s $)的前一个句子或后一个句子）以及其他不是$s$上下文的句子。模型通过对$s$以及$S_{cand}$中的每个句子进行编码，然后输入到一个分类器中，让分类器选出$S_{cand}$中的哪个句子是$s_{ctxt}$。实验设置候选句子集合大小为3，即$S_{cand}$包含1个上下文句子和两个无关句子。模型结构如下： 模型有如下两个细节需要注意：模型使用的分类器（得分函数）$c$非常简单，是两个向量内积，即$c(u, v)=u^Tv$，计算$s$的embedding与所有$S_{cand}$中的句子向量内积得分后，输入到softmax层进行分类。使用简单分类器是为了引导模型着重训练句子编码器，因为我们的目的是为了得到好的句子向量表示而不是好的分类器。虽然某些监督任务模型如文本蕴含模型是参数共享的，$s$的编码器参数和候选句子编码器参数是不同的（不共享），因为句子表示学习往往是在大规模语料上进行训练，不必担心参数学习不充分的问题 。测试时，给定待编码句子$s$，通过该模型得到的句子表示是两种编码器的连结 $[ f ( s ) ;g ( s ) ]$。 看上去，Skip-thought和Skip-gram挺象。唯一的遗憾是Skip-thought的decoder那部分，它是作为language modeling来处理的。QT针对这个问题，对decoder部分做了大的调整，它直接把decoder拿掉，取而代之的是一个classifier。这个classifier负责预测哪些句子才是context sentences。 QT的classifier取代了Skip-thought的Decoder。这样做的好处是运行的速度大大提升了，用判别问题取代了生成式问题（这个是才是速度提升的原因）。有趣的是，虽然QT出现的比Skip-thought更晚，但是方法更简单，也更加接近Word2Vec算法。 QT是一种新的state-of-art的算法。它不光效果好，而且训练时间要远小于其他算法。在算法方法上和效果上，都可称为是句子表征界的Word2Vec一般的存在。和前面几篇介绍的不同算法放在一起比较，同样都是为了找到好的句子表征，它们采取了不同的路径：InferSent在寻找NLP领域的ImageNet, 它的成功更像是在寻找数据集和任务上的成功，当然它成功的找到了SNLI; Concatenated p-means在寻找NLP领域的convolutional filter; QT则是直接在算法层面上，寻找句子级别的Word2Vec, 算法上的改进让它受益。我们看到不同的方法在不同的方向上都作出了努力和取得了成效，很难讲哪种努力会更有效或者更有潜力。 Supervised Sentence EmbedInferSent来自论文Supervised Learning of Universal Sentence Representations from Natural Language Inference Data，更多信息参考 这里 Multi-task learning Sentence EmbedUniversal Sentence Encoder来自论文 Universal Sentence Encoder，更多信息参考 Universal Sentence Encoder sentence embedding为什么考虑sentence embedding? 语序（语义）在word embedding 中不能体现 word embedding 的效果依赖于分词 SIF 是一种无监督的学习方式，但是最后的效果和NN 是相当的。 训练样本不足的问题 词汇扩展：使用word embedding补充 sentence embedding 集成学习：使用多个不同的模型或者大的数据集进行补充 词向量]]></content>
      <categories>
        <category>NLP</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[NLP Papers Reading- BERT (***)]]></title>
    <url>%2F2019%2F05%2F27%2Fpaper-reading-bert%2F</url>
    <content type="text"><![CDATA[attention is all you needSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed cy a compatibility function of the query with the corresponding key. 中文的理解：深度学习里的Attention model其实模拟的是人脑的注意力模型，举个例子来说，当我们观赏一幅画时，虽然我们可以看到整幅画的全貌，但是在我们深入仔细地观察时，其实眼睛聚焦的就只有很小的一块，这个时候人的大脑主要关注在这一小块图案上，也就是说这个时候人脑对整幅图的关注并不是均衡的，是有一定的权重区分的。这就是深度学习里的AttentionModel的核心思想。所谓注意力机制，就是说在生成每个词的时候，对不同的输入词给予不同的关注权重。通过注意力机制，我们将输入句子编码为一个向量序列，并自适应地选择这些向量的一个子集，同时对译文进行译码，例如where are you——&gt;你在哪？现在我们在翻译“你”的时候给”you”更多的权重，那么就可以有效的解决对齐问题。 Background: 主要是面临的三个问题。 Transformer 的结构示意图:(transformer 就是讨论了如何实现上述的 self-attention 结构) Encoder: encoder由6个相同的层堆叠而成，每个层有两个子层。第一个子层是多头自我注意力机制(multi-head self-attention mechanism)，第二层是简单的位置的全连接前馈网络(position-wise fully connected feed-forward network)。在两个子层中会使用一个残差连接，接着进行层标准化(layer normalization)。也就是说每一个子层的输出都是LayerNorm(x + sublayer(x))。网络输入是三个相同的向量q, k和v，是word embedding和position embedding相加得到的结果。为了方便进行残差连接，我们需要子层的输出和输入都是相同的维度。 Decoder: decoder也是由N（N=6）个完全相同的Layer组成，decoder中的Layer由encoder的Layer中插入一个Multi-Head Attention + Add&amp;Norm组成。输出的embedding与输出的position embedding求和做为decoder的输入，经过一个Multi-HeadAttention + Add&amp;Norm（（MA-1）层，MA-1层的输出做为下一Multi-Head Attention + Add&amp;Norm（MA-2）的query（Q）输入，MA-2层的Key和Value输入（从图中看，应该是encoder中第i（i = 1,2,3,4,5,6）层的输出对于decoder中第i（i = 1,2,3,4，5,6）层的输入）。MA-2层的输出输入到一个前馈层（FF），经过AN操作后，经过一个线性+softmax变换得到最后目标输出的概率。 对于decoder中的第一个多头注意力子层，需要添加masking，确保预测位置i的时候仅仅依赖于位置小于i的输出。 层与层之间使用的Position-wise feed forward network。 这里应该有一个小问题： masked attention 和 attention 有什么区别？ transformer 的结构谈及 transformer，首先应该提到是 计算效率的大大提高，从原先的RNN 的线性O(N)提升的很多，这个的实现是基于多线程的。而后者是因为是有顺序的线性模型，所以是无法使用并行运算的。 对于 RNN 来说，句首的信息要传递到句尾，需要经过 n 次 RNN 的计算；而 Self-Attention 可以直接连接任意两个节点. 从整体上来看，Transformer依旧是一个“Sequence to Sequence”框架，拥有Encoder和Decoder两部分： transformer 的结构 论文中encoder层由6个encoder堆叠在一起，decoder层也一样。 每一个 encoder 和 decoder 的内部简图如下： encoder 部分 对于encoder，包含两层，一个self-attention层和一个前馈神经网络，self-attention能帮助当前节点不仅仅只关注当前的词，从而能获取到上下文的语义。decoder也包含encoder提到的两层网络，但是在这两层中间还有一层attention层，帮助当前节点获取到当前需要关注的重点内容。 self-attention 先说一下 attention 机制的实现： 当使用 self的时候，query, key and value 这三个就都是相同的。经过softmax() 得到就是一个权重，用于标记和 当前处理的词语的关系。self-attention是Transformer用来将其他相关单词的“理解”转换成我们正在处理的单词的一种思路， Multi-Headed Attention 我的理解就是 在CNN中使用多个filter 的类似产物。该机制理解起来很简单，就是说不仅仅只初始化一组Q、K、V的矩阵，而是初始化多组，tranformer是使用了8组，所以最后得到的结果是8个矩阵。 这样做的主要目的是从不同的语义空间投射原文本，能够从更多的角度表征，并且能够拓展模型对不同位置的关注能力。 这给我们留下了一个小的挑战，前馈神经网络没法输入8个矩阵呀，这该怎么办呢？所以我们需要一种方式，把8个矩阵降为1个，首先，我们把8个矩阵连在一起，这样会得到一个大的矩阵，再随机初始化一个矩阵和这个组合好的矩阵相乘，最后得到一个最终的矩阵。这个就是 multi-head attention 机制的全部的流程了。 Positional Encoding transformer给encoder层和decoder层的输入添加了一个额外的向量Positional Encoding，维度和embedding的维度一样，这个向量采用了一种很独特的方法来让模型学习到这个值，这个向量能决定当前词的位置，或者说在一个句子中不同的词之间的距离。这个位置向量的具体计算方法有很多种，论文中的计算方法如下： $$P E ( p o s , 2 i ) = \sin \left( p o s / 10000 ^ { 2 i } / d _ { m } \text {odel} \right)$$ $$P E ( p o s , 2 i + 1 ) = \cos \left( p o s / 10000 ^ { 2 i } / d _ { m } o d e l \right)$$其中pos是指当前词在句子中的位置，i是指向量中每个值的index，可以看出，在偶数位置，使用正弦编码，在奇数位置，使用余弦编码. 最后把这个Positional Encoding与embedding的值相加，作为输入送到下一层。 layer normalization Normalization有很多种，但是它们都有一个共同的目的，那就是把输入转化成均值为0方差为1的数据。我们在把数据送入激活函数之前进行normalization（归一化），因为我们不希望输入数据落在激活函数的饱和区。 batch normalization 和layer normalization 的区别，简单来说前者依赖于 batch size，是在不同的样本的同一个特征上进行归一化，在CNN 上的效果更好，后者在一个样本上进行归一化， 在 RNN的网络结果中效果更好。更多详细的内容可以参考这篇博客. BN的主要思想就是：在每一层的每一批数据上进行归一化。我们可能会对输入数据进行归一化，但是经过该网络层的作用后，我们的数据已经不再是归一化的了。随着这种情况的发展，数据的偏差越来越大，我的反向传播需要考虑到这些大的偏差，这就迫使我们只能使用较小的学习率来防止梯度消失或者梯度爆炸。可以看到，右半边求均值是沿着数据 batch_size的方向进行的 不过 LN 是在每一个样本上计算均值和方差，而不是BN那种在批方向计算均值和方差！ decoder 部分 decoder部分其实和encoder部分大同小异，不过在最下面额外多了一个masked mutil-head attetion，这里的mask也是transformer一个很关键的技术。 Transformer 模型里面涉及两种 mask，分别是 padding mask 和 sequence mask。其中 padding mask 在所有的 scaled dot-product attention 里面都需要用到，而 sequence mask 只有在 decoder 的 self-attention 里面用到。前者就是一种填充技术，使得 不定长的sequence 变成定长的sequence之后做出的一些处理。 Padding Mask 什么是 padding mask 呢？因为每个批次输入序列长度是不一样的也就是说，我们要对输入序列进行对齐。具体来说，就是给在较短的序列后面填充 0。但是如果输入的序列太长，则是截取左边的内容，把多余的直接舍弃。因为这些填充的位置，其实是没什么意义的，所以我们的attention机制不应该把注意力放在这些位置上，所以我们需要进行一些处理。具体的做法是，把这些位置的值加上一个非常大的负数(负无穷)，这样的话，经过 softmax，这些位置的概率就会接近0！ Sequence masksequence mask 是为了使得 decoder 不能看见未来的信息。也就是对于一个序列，在 time_step 为 t 的时刻，我们的解码输出应该只能依赖于 t 时刻之前的输出，而不能依赖 t 之后的输出。因此我们需要想一个办法，把 t 之后的信息给隐藏起来。那么具体怎么做呢？也很简单：产生一个上三角矩阵，上三角的值全为1。把这个矩阵作用在每一个序列上，就可以达到我们的目的。 缺点： 问题一： 长输入 在文本只要等篇章级别的任务重， transformer 因为计算量的复杂性，所以速度回急速变慢。所以短期内，这些方面仍然是RNN 或者CNN的应用场景（虽然两者做的也不是很好）。 transformer 的改进思路： 比如可以把长输入切断分成K份，强制把长输入切短，再套上Transformer作为特征抽取器，高层可以用RNN或者另外一层Transformer来接力，形成Transformer的层级结构，这样可以把n平方的计算量极大减少。（分而治之的思路是真的比较常见呀） 问题二： 网络结构过于复杂 如何更深刻认识它的作用机理，然后进一步简化它，这也是一个好的探索方向。 上面在做语义特征抽取能力比较时，结论是对于距离远与13的长距离特征，Transformer性能弱于RNN 分界线 - - - - - – - - - - - - - - - - - 分 界线（另外的解读方式） Encoder和Decoder的内部结构： 模型的特点：Positional embedding；（位置嵌入向量——其实类似word2vec，处理的语序的信息）。multi-head attention; (多头注意力机制——点乘注意力的升级版本， 这个就类似ensemble的思想，不同的子空间的attention 进行融合）Position-wise Feed-Forward Networks（位置全链接前馈网络——MLP变形） 有两种常用的注意力函数，一种是加法注意力(additive attention)，另外一种是点乘注意力(dot-productattention)，论文所采用的就是点乘注意力，这种注意力机制对于加法注意力而言，更快，同时更节省空间。 加法注意力还是以传统的RNN的seq2seq问题为例子，加性注意力是最经典的注意力机制，它使用了有一个隐藏层的前馈网络（全连接）来计算注意力分配： 公式:$$\alpha _ { i j } = \frac { \exp \left( e _ { i j } \right) } { \sum _ { k = 1 } ^ { L } e _ { i k } }$$ Scaled Dot-Product这篇论文计算query和key相似度使用了dot-product attention，即query和key进行点乘（内积）来计算相似度。 Multi-Head Attention:（将单个计算组成矩阵运算，有利于并行运算）在实际中为了并行计算，可以在一组queries上计算注意力函数，将多个query堆叠成Q，同理keys和values也被堆叠成K和V，通过下面的公式来计算矩阵输出:self-attention 模型就是自己对自己求attention，即𝑄=𝐾=𝑉$$\text { Attention } ( Q , K , V ) = \operatorname { softmax } \left( \frac { Q K ^ { T } } { \sqrt { d _ { k } } } \right) V$$之所以用内积除以维度的开方，论文给出的解释是：假设Q和K都是独立的随机变量，满足均值为0，方差为1，则点乘后结果均值为0，方差为dk。也即方差会随维度dk的增大而增大，而大的方差导致极小的梯度(我认为大方差导致有的输出单元a（a是softmax的一个输出）很小，softmax反向传播梯度就很小（梯度和a有关））。为了避免这种大方差带来的训练问题，论文中用内积除以维度的开方，使之变为均值为0，方差为1。 除了计算一个单独的注意力函数，论文提出对queries，keys和values做h次不同的投影, 然后都经过Scaled Dot-Product Attention，将结果拼接在一起，最后通过一个线性映射输出，通过多头注意力，模型能够获得不同子空间下的位置信息。如下图所示，公式如下:$$\text {MultiHead} ( Q , K , V ) =Concat(head_1, head_2, …, head_h) W ^ { o }$$ Self-Attention那么首先要明白什么是Attention。从语言学的角度，它是表示词与词之间的关联关系（这种关系是通过反向传播学习到的）。而 self-attention 表示句子内部词于词之间的关联关系，如下图中的it 和其他位置词的关系，颜色越深表示关系越紧密， 从图中可以看到 it 正确的关联到了 animal 它所指代的一个词。 Positional Encodingtransformer是使用 positional encoding 加入了位置信息，保持了词语之间的上下文关系。实现的的时候，在偶数位置，使用正弦编码，在奇数位置，使用余弦编码. Residual connection和layer-normalization 对于学习CV的人估计对这个结构一点也不陌生，Residual connection是对于较为深层的神经网络有比较好的作用，比如网络层很深时，数值的传播随着weight不断的减弱，Residual connection是从输入的部分，就是图中虚线的部分，实际连到它输出层的部分，把输入的信息原封不动copy到输出的部分，减少信息的损失。 layer-normalization这种归一化层是为了防止在某些层中由于某些位置过大或者过小导致数值过大或过小，对神经网络梯度回传时有训练的问题，保证训练的稳定性，这是神经网络设计比较常用的case。 结论：self-attention层的好处是能够一步到位捕捉到全局的联系，解决了长距离依赖，因为它直接把序列两两比较（代价是计算量变为 O(n2)，当然由于是纯矩阵运算，这个计算量相当也不是很严重），而且最重要的是可以进行并行计算，因为这个操作是可以使用矩阵运算的。相比之下，RNN 需要一步步递推才能捕捉到，并且对于长距离依赖很难捕捉。而 CNN 则需要通过层叠来扩大感受野（感受野的概念，更像是 最后经过CNN 的一个点在原始的图像中是多大的面积，这种管中窥豹的感觉），这是 Attention 层的明显优势。 Deep Contextualized Word Representations（可以得到有上下文关系的词向量， 这个特点是相对于 word2vec 或者 glove 的）这篇论文的想法其实非常非常简单，但是取得了非常好的效果。它的思路是用深度的双向RNN(LSTM)在大量未标注数据上训练语言模型，如下图所示。然后在实际的任务中，对于输入的句子，我们使用这个语言模型来对它处理，得到输出的向量，因此这可以看成是一种特征提取。但是和普通的Word2Vec或者GloVe的pretraining不同，ELMo得到的Embedding是有上下文的。比如我们使用Word2Vec也可以得到词”bank”的Embedding，我们可以认为这个Embedding包含了bank的语义。但是bank有很多意思，可以是银行也可以是水边，使用普通的Word2Vec作为Pretraining的Embedding，只能同时把这两种语义都编码进向量里，然后靠后面的模型比如RNN来根据上下文选择合适的语义——比如上下文有money，那么它更可能是银行；而如果上下文是river，那么更可能是水边的意思。但是RNN要学到这种上下文的关系，需要这个任务有大量相关的标注数据，这在很多时候是没有的。而ELMo的特征提取可以看成是上下文相关的，如果输入句子有money，那么它就(或者我们期望)应该能知道bank更可能的语义，从而帮我们选择更加合适的编码。 我们把这两个方向的RNN合并起来就得到Bi-LSTM。我们优化的损失函数是两个LSTM的交叉熵加起来是最小的： 主要贡献： 提出了一个双向训练的 language model，使用前K-1 个词语训练 第K 个词语，然后使用后 N-K+1 个词语训练第K 个词语，所以第 K 个词语是combine 了上下文的信息的。 word embedding 的表示是不同layer 累加的结果，weights 的设定是学习而得。 Why do we need contextualized representations? 词语的意思是由上下文所决定的。所以一个固定的 word embedding 不能准确的表示不同场景下 word 的含义。 As an illustrative example, take the following two sentences: “The bank on the other end of the street was robbed”“We had a picnic on the bank of the river” Both sentences use the word “bank”, but the meaning of the word differs completely between them. This phenomenon where two identical words change meaning depending on the context is known as “polysemy“, and has been an issue in the NLP deep learning community ever since word embeddings really took off. Most current neural networks are bad at handling polysemy because they use a single vector to represent the meaning of the word “bank”, regardless of the context. In reality, the vector representing any word should change depending on the words around it. 之前的做法的缺点是对于每一个单词都有唯一的一个embedding表示, 而对于多义词显然这种做法不符合直觉, 而单词的意思又和上下文相关, ELMo的做法是我们只预训练language model, 而word embedding是通过输入的句子实时输出的, 这样单词的意思就是上下文相关的了, 这样就很大程度上缓解了歧义的发生. 这种算法的特点是：每一个word representation都是整个输入语句的函数。具体做法就是先在大语料上以language model为目标训练出bidirectional LSTM模型，然后利用LSTM产生词语的表征。ELMo故而得名(Embeddings from Language Models)。为了应用在下游的NLP任务中，一般先利用下游任务的语料库(注意这里忽略掉label)进行language model的微调,这种微调相当于一种domain transfer; 然后才利用label的信息进行supervised learning。 ELMo表征是“深”的，就是说它们是biLM的所有层的内部表征的函数。这样做的好处是能够产生丰富的词语表征。高层的LSTM的状态可以捕捉词语意义中和语境相关的那方面的特征(比如可以用来做语义的消歧)，而低层的LSTM可以找到语法方面的特征(比如可以做词性标注)。如果把它们结合在一起，在下游的NLP任务中会体现优势。 所以，最后的 embedding 使用不同层进行weights 的累加，这种理论上是站得住脚的。上面的描述和 CV 是惊人的相似。 Salient featuresELMo representations are: Contextual: The representation for each word depends on the entire context in which it is used. Deep: The word representations combine all layers of a deep pre-trained neural network. Character based: ELMo representations are purely character based, allowing the network to use morphological clues to form robust representations for out-of-vocabulary tokens unseen in training. related work: 针对传统词向量是固定的，与上下文语境无关的缺点，先前的工作多通过两种方式来解决： (1) 通过引入字符级(subword)信息丰富词向量表达； (2) 学习每个单词不同含义的独立向量； ELMo也利用了字符卷积（Character-Convolutions）引入字符级信息，并同时结合了深度双向语言模型的各层隐状态来丰富词向量表达。 P.s.：基于字符的模型不仅能够通过引入字符级信息丰富词向量表达，也能够在很大程度上解决NLP领域的OOV（Out-Of-Vocabulary）问题。 ELMo用到上文提到的双向的language model, 给定N个tokens (t1, t2,…,tN), language model通过给定前面的k-1个位置的token序列计算第k个token的出现的概率:$$p \left( t _ { 1 } , t _ { 2 } , \ldots , t _ { N } \right) = \prod _ { k = 1 } ^ { N } p \left( t _ { k } | t _ { 1 } , t _ { 2 } , \ldots , t _ { k - 1 } \right)$$后向的计算方法与前向相似:$$p \left( t _ { 1 } , t _ { 2 } , \ldots , t _ { N } \right) = \prod _ { k = 1 } ^ { N } p \left( t _ { k } | t _ { k + 1 } , t _ { k + 2 } , \ldots , t _ { N } \right)$$biLM训练过程中的目标就是最大化:$$\sum _ { k = 1 } ^ { N } \left( \log p \left( t _ { k } | t _ { 1 } , \ldots , t _ { k - 1 } ; \Theta _ { x } , \vec { \Theta } _ { L S T M } , \Theta _ { s } \right) + \log p \left( t _ { k } | t _ { k + 1 } , \ldots , t _ { N } ; \Theta _ { x } , \overline { \Theta } _ { L S T M } , \Theta _ { s } \right) \right)$$ELMo对于每个token $t_k$, 通过一个L层的biLM计算出2L+1个表示:$$R_{ k } = { x _ { k } ^ { L M } , \vec { h } _ { k , j } ^ { L M } , h _ { k , j } ^ { L M } | j = 1 , \ldots , L } = { h _ { k , j } ^ { L M } | j = 0 , \ldots , L }$$其中$h _ { k , 0 } ^ { L M }$是对token进行直接编码的结果(这里是字符通过CNN编码), $h _ { k , j } ^ { L M } = \left[ \vec { h } _ { k , j } ^ { L M } ; \overline { h } _ { k , j } \right]$ 是每个biLSTM层输出的结果. 在实验中还发现不同层的biLM的输出的token表示对于不同的任务效果不同. 应用中将ELMo中所有层的输出R压缩为单个向量, ELMok=E(Rk;Θϵ), 最简单的压缩方法是取最上层的结果做为token的表示:$E \left( R _ { k } \right) = h _ { k , L } ^ { L M }$ 更通用的做法是通过一些参数来联合所有层的信息:$$E L M o _ { k } ^ { t a s k } = E \left( R _ { k } ; \Theta ^ { t a s k } \right) = \gamma ^ { t a s k } \sum _ { j = 0 } ^ { L } s _ { j } ^ { t a s k } h _ { k , j } ^ { L M }$$ 其中$s_j$是一个softmax出来的结果, $γ$是一个任务相关的scale参数, 我试了平均每个层的信息和学出来$s_j$发现学习出来的效果会好很多. 文中提到$γ$在不同任务中取不同的值效果会有较大的差异, 需要注意, 在SQuAD中设置为0.01取得的效果要好于设置为1时. ELMo: Context Matters Instead of using a fixed embedding for each word, ELMo looks at the entire sentence before assigning each word in it an embedding. It uses a bi-directional LSTM trained on a specific task to be able to create those embeddings. ELMo provided a significant step towards pre-training in the context of NLP. The ELMo LSTM would be trained on a massive dataset in the language of our dataset, and then we can use it as a component in other models that need to handle language. What’s ELMo’s secret? ELMo gained its language understanding from being trained to predict the next word in a sequence of words - a task called Language Modeling. This is convenient because we have vast amounts of text data that such a model can learn from without needing labels. We can see the hidden state of each unrolled-LSTM step peaking out from behind ELMo’s head. Those come in handy in the embedding proecss after this pre-training is done. ELMo actually goes a step further and trains a bi-directional LSTM – so that its language model doesn’t only have a sense of the next word, but also the previous word.ELMo comes up with the contextualized embedding through grouping together the hidden states (and initial embedding) in a certain way (concatenation followed by weighted summation). lstm-based language modelIn case you are unfamiliar with language models, a language model is simply a model that can predict how “likely” a certain sequence of words is to be a real piece of text. This is generally done by training a model to take a part of sentence (say, the first n words) and predict the next word – or more precisely, output the probability of each word in the vocabulary being the next word (In this blog post, we’ll focus on LSTM-based language models which are the focus of this paper). One trick that this paper uses is to train a language model with reversed sentences that the authors call the “backward” language model.这种模型：上一个模型的输出到下一个模型输入Furthermore, instead of using a single-layer LSTM, this paper uses a stacked, multi-layer LSTM. Whereas a single-layer LSTM would take the sequence of words as input, a multi-layer LSTM trains multiple LSTMs to take the output sequence of the LSTM in the previous layer as input (of course, the first layer takes the sequence of words as input). This is best illustrated in the following illustration: 最后的embedding 是是将不同的层 combination起来，这个系数是通过学习出来的。In ELMo, the part that is task specific is the combination of the task-agnostic representations. The weight is learned for each task and normalized using the softmax function. The parameter $\gamma$ is a task-dependent value that allows for scaling the entire vector, which is important during optimization. 优缺点： 在ELMo中，嵌入基于一个双层的双向语言模型（biLM）的内部状态计算，ELMo也是因此得名的：Embeddings from Language Models（来自语言模型的嵌入）。ELMo的特性：ELMo的输入是字符而不是单词。这使得它可以利用子字（sub-word）单元为词汇表以外的单词计算有意义的表示（和FastText类似）。ELMo是biLM的多层激活的连接（concatenation）。语言模型的不同层编码了单词的不同信息。连接所有层使得ELMo可以组合多种词表示，以提升下游任务的表现。 OpenAI GPT它的思想其实也很简单，使用Transformer来学习一个语言模型，对句子进行无监督的Embedding，然后根据具体任务对Transformer的参数进行微调。 这篇论文中的 多任务学习是如何体现的呢？首先是无监督的pretraining 中有一个语言模型，需要优化一个最大似然估计 L1，然后再监督的fine-tuning 中有一个交叉熵损失函数，这里也是有一个loss ，记为L2。正常情况下，我们应该调整参数最大化L2， 但是我们使用的是多任务学习，同时让它最大似然L1 和L2。$$L_{3}(\mathcal{C})=L_{2}(\mathcal{C})+\lambda \times L_{1}(\mathcal{C})$$ BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding嵌入（Embedding）的新时代到目前为止，词嵌入一直是影响NLP模型处理语言的主要力量。但是我们使用GloVe，那么“stick”这个词将由一个向量表示，无论上下文是什么。所以词嵌入的方式并没有解决上下文的问题。ELMo 解决了这个问题：捕捉单词的上下文信息。ELMo不是对每个单词使用固定嵌入，而是在为其中的每个单词分配嵌入之前查看整个句子，它使用在特定任务上训练的双向LSTM来创建这些嵌入。 BERT全称是Bidirectional Encoder Representations from Transformers，取了核心单词的首字母而得名，从名字我们能看出该模型两个核心特质：依赖于Transformer以及双向。 首先介绍BERT的两种型号： BERT BASE：与OpenAI Transformer的尺寸相当，性价比很高； BERT LARGE：一个非常庞大的模型，它的性能最好； 一般来说使用BASE 版本就足以。 预训练过程BERT为了能够在大规模语料上进行无监督学习，非常巧妙的设计了两个预训练任务：一个是随机遮蔽（mask）掉一个句子中的词，利用上下文进行预测 （换句话说，为了训练深度双向Transformer表示）；另一个是预测下一个句子（类似QA场景） BERT 的目标是生成语言模型，所以只需要 encoder 机制。Transformer 的 encoder 是一次性读取整个文本序列，而不是从左到右或从右到左地按顺序读取，这个特征使得模型能够基於单词的两侧学习，相当于是一个双向的功能。这个是解决了 word embedding的问题，但是在其他的任务中 word2vec 的作用是有限的。所以BERT 提供了两种策略： 蒙面语言模型（NLM：Masked Language Model）和 两个句子的任务（Two-sentence Tasks）。 比较 ELMO 和BERT: 从结构上我们可以看出ELMo的基础是使用了LSTM，而BERT使用了Transformer作为基本模型 核心的是两者的目标函数是不一致的 ELMO: $P \left( w _ { i } | w _ { 1 } , \ldots , w _ { i - 1 } \right) $ 和 $P \left( w _ { i } | w _ { i + 1 } , \ldots , w _ { n } \right)$ BERT：$$P \left( w _ { i } | w _ { 1 } , \ldots , w _ { i - 1 } , w _ { i + 1 } , \ldots , w _ { n } \right)$$ 所以BERT 在训练的时候和 ELMO 是不太一样的，前者使用了 masked LM tricks. Masked LM (MLM) Input:the man [MASK1] to [MASK2] storeLabel:[MASK1] = went; [MASK2] = store 该任务就是BERT为了做到双向深度上下文表示设计的预训练trick任务，而在mask单词的时候，作者也采用了一些技巧，随机mask掉15%的token，最终的损失函数只计算mask掉的token。而对于被mask掉的词也并非简单粗暴的将全部替换成[MASK]标签完事，会遵循如下步骤： 80%即大部分情况下，被mask掉的词会被[MASK]标签代替； 10%的情况下，将该词用一个随机的词替换掉； 10%的情况下，保留该词在原位置。 这样做的目的是偏向代表实际观察到的词。另外模型在预训练时，Transformer编码器并不知道哪些词被mask掉了，所以模型对每个词都会关注。同时，因为随机替换仅发生在所有词的1.5％（即15％*10％），对模型的语言理解能力影响很小。Transformer编码器不知道它将被要求预测哪些单词，或者哪些已经被随机单词替换，因此它必须对每个输入词保持分布式的上下文表示。此外，由于随机替换在所有词中只发生1.5%，所以并不会影响模型对于语言的理解。 Next Sentence Prediction (NSP) Input:the man went to the store [SEP] he bought a gallon of milkLabel:IsNextInput:the man went to the store [SEP] penguins are flightless birdsLabel:NotNext 由于在LM的下游任务还会涉及到问答（Question Answering (QA) ）和推理（ Natural Language Inference (NLI)）的任务，这需要LM有理解句子间关系的能力，所以作者新增了一个预训练任务，输入句子A和B，预测B是否为A的下一个句子，以50%的概率配对A和B，即50%B是真的，50%B是随机选取的一个句子。 所以作者提示在选取预训练语料时，要尽可能选取document-level的语料而非segment-level混合在一起的语料 在 BERT 的训练过程中，模型接收成对的句子作为输入，并且预测其中第二个句子是否在原始文档中也是后续句子。在训练期间，50％ 的输入对在原始文档中是前后关系，另外 50％ 中是从语料库中随机组成的，并且是与第一句断开的。 在训练 BERT 模型时，Masked LM 和 Next Sentence Prediction 是一起训练的，目标就是要最小化两种策略的组合损失函数。 BERT 模型的输入 输入表示可以在一个词序列中表示单个文本句或一对文本(例如，[问题，答案])。对于给定的词，其输入表示是可以通过三部分Embedding求和组成。Embedding的可视化表示如下图所示： token Embeddings表示的是词向量，第一个单词是CLS标志，可以用于之后的分类任务，对于非分类任务，可以忽略词向量； Segment Embeddings用来区别两种句子，因为预训练不只做语言模型还要做以两个句子为输入的分类任务； Position Embeddings是通过模型学习得到的。 应用BERT 可以使用在各种NLP 任务中，只需要在核心模型中添加一个层，比如： 在分类任务中，例如情感分析等，只需要在 Transformer 的输出之上加一个分类层 在问答任务（例如SQUAD v1.1）中，问答系统需要接收有关文本序列的 question，并且需要在序列中标记 answer。 可以使用 BERT 学习两个标记 answer 开始和结尾的向量来训练Q＆A模型。 在命名实体识别（NER）中，系统需要接收文本序列，标记文本中的各种类型的实体（人员，组织，日期等）。 可以用 BERT 将每个 token 的输出向量送到预测 NER 标签的分类层。 句子分类如果使用B ERT 模型进行句子分类，那么在训练阶段模型发生的变化很小，这个过程称为微调。 机器翻译 事实上Transformer比LSTM更好地处理长期依赖性，使其非常适合机器翻译。从一种语言到另一种语言的转换。 参考文献 Transformer &amp; BERTBERT解析及文本分类应用 A simple but tough-to-beat baseline for sentence embeddingsTaking the average of the word embeddings in a sentence tends to give too much weight to words that are quite irrelevant, semantically speaking. Smooth Inverse Frequency tries to solve this problem in two ways: Weighting: like our tf-idf baseline above, SIF takes the weighted average of the word embeddings in the sentence. Every word embedding is weighted by a/(a + p(w)), where a is a parameter that is typically set to 0.001 and p(w) is the estimated frequency of the word in a reference corpus. (使用新的词权重计算方法，不是tf-idf, 频率越高，权重越低，抑制高频词) Common component removal: next, SIF computes the principal component of the resulting embeddings for a set of sentences. It then subtracts from these sentence embeddings their projections on their first principal component. This should remove variation related to frequency and syntax that is less relevant semantically.As a result, SIF downgrades unimportant words such as but, just, etc., and keeps the information that contributes most to the semantics of the sentence. 第一步中的$p(w) $ 是在语料中的词频，第二步中对整个句子集合进行一次PCA，然后对每个句子上面得到的向量减去它在第一奇异向量或者说主成分上的投影。最后初步的句子向量减去对应句子向量的共性成分(起到平滑作用),得到最后的独有的句子向量(使得各个句子向量间的耦合度降低,增强句子的鲁棒性).耦合性越低（模块之间的关联性越小） 作用： 第一步骤中的超参数 $a $ 是一种平滑项，对于低频词的支持，即使出现的次数少，也还是权重的。$ ( \alpha p(w)) $,其中$(p(w))$是单词 $(w) $在整个语料中出现的概率(词频角度), $ (\alpha) $是一个超参数. 这样, 即使和 $ (c_s) $的内积很小, 这个单词也有概率出现. 第二步骤中的减去 主成分，可以理解为让各个词向量更好的分开，减去公共的部分，减少耦合性，使得相似的句子聚类在一起。因为这个主成分是整个语料库中的主成分. 另外论文中还提到了这种方法的鲁棒性: 使用不同语料(多种领域)训练得到的不同的word embedding, 均取得了很好的效果, 说明了对各种语料的友好. 使用不同语料得到的词频, 作为计算词权重的因素, 对最终的结果影响很小. 对于方法中的超参数, 在很大范围内, 获得的结果都是区域一直的, 即超参数的选择没有太大的影响. 尽管长期以来句子的无监督表示学习是主流，最近几个月（2017年末/2018年初），我们看到了许多非常有趣的工作，显示了向监督学习和多任务学习（不同的任务学习到不同的维度，然后组合）转向的趋势。 强力/迅速的基线：FastText、词袋（Bag-of-Words） 当前最先进模型：ELMo、Skip-Thoughts、Quick-Thoughts、 InferSent、MILA/MSR的General Purpose Sentence Representations、Google的Universal Sentence Encoder 关于nlp 中的word embedding 是可以有 phrases, sentences, and paragraphs 三个不同类别的 embedding，所以还是挺好的。 优点： 程序的运行只需要十几分钟，效果和神经网络是相当的 属于无监督的学习，可以对大规模的语料进行利用，相对于有监督的学习方式，这个是优势 缺点： 缺点就是没有考虑句子的语序,导致不能辨别(“我爱你”还是”你爱我”), 只是字意的表达，并没有体现了句意 对于短文本上的word2vec， SIF 效果很好，但是涉及到语意理解的时候，这种方式效果就一般了，而这个时候就应该使用 elmo，transformer or bert 等模型了 Supervised Learning of Universal Sentence Representations from Natural Language Inference Data文章成功的找到了NLP领域的ImageNet — SNLI (Stanford Natural Language Inference dataset), 并且试验了不同的深度学习模型，最终确定bi-LSTM max pooled 为最佳模型。 域 数据 任务 模型(编码器) CV ImageNet image classification Le-Net, VGG-Net, Google-Net, ResNet, DenseNet NLP SNLI NLI ? 基于监督学习方法学习sentence embeddings可以归纳为两个步骤：第一步选择监督训练数据，设计相应的包含句子编码器Encoder的模型框架；第二步选择（设计）具体的句子编码器，包括DAN、基于LSTM、基于CNN和Transformer等。 数据集： 本文采用的是Stanford Natural Language Inference Datasets，简称SNLI （NLP领域的ImageNet ）。SNLI包含570K个人类产生的句子对，每个句子对都已经做好了标签，标签总共分为三类：蕴含、矛盾和中立（Entailment、contradiction and neutral）。下面是这些数据集的一个例子： 从上图可以看出，每个句子对为（text, hypothesis）,中间的judgments为它们的标签。可以看到标签是综合了5个专家的意见，根据少数服从多数的原则得到的。 7种不同的architectures： standard recurrent encoders with LSTM ，取最后一个隐状态 standard recurrent encoders with GRU ，取最后一个隐状态上述两种是基础的recurrent encoder，在句子建模中通常将网络中的最后一个隐藏状态作为sentence representation； conncatenation of last hidden states of forward and backward GRU这种方法是将单向的网络变成了双向的网络，然后用将前向和后向的最后一个状态进行连接，得到句子向量； Bi-directional LSTMs (BiLSTM) with mean pooling Bi-directional LSTMs (BiLSTM) with max pooling这两种方法使用了双向LSTM结合一个pooling层的方法来获取句子表示，具体公式如下： self-attentive network这个网络在双向LSTM的基础上加入了attention机制，具体网络结构如下： hierarchical convolutional networks Now that we have discussed the various sentence encoding architectures used in the paper, let’s go through the part of the network which takes these sentence embeddings and predicts the output label. After the sentence vectors are fed as input to this model, 3 matching methods are applied to extract relations between the text, u and hypothesis, v – concatenation of the two representations (u, v) element-wise product u * v and, absolute element-wise difference |u – v | The resulting vector captures information from both the text, u and the hypothesis, v, and is fed into a 3-class classifier consisting of multiple fully connected layers followed by a softmax layer. Universal Sentence Encoder这篇文章基于InferSent， 也是想找到一个universal encoder。不同之处在于文章把InferSent的bi-lstm换成了DAN（或者Transformer)，而使用DAN这样“简单”的encoder的效果竟然相当好（尤其是时间和内存消耗和其他算法比小很多。） The Google Sentence Encoder is Google’s answer to Facebook’s InferSent. It comes in two forms: an advanced model that takes the element-wise sum of the context-aware word representations produced by the encoding subgraph of a Transformer model. a simpler Deep Averaging Network (DAN) where input embeddings for words and bigrams are averaged together and passed through a feed-forward deep neural network.The Transformer-based model tends to give better results, but at the time of writing, only the DAN-based encoder was available. In contrast to InferSent, the Google Sentence Encoder was trained on a combination of unsupervised data (in a skip-thought-like task) and supervised data (the SNLI corpus). DAN其实DAN(Deep Averaging Networks)应该属于Bag of Words类的算法。因为比较特殊，单独列出来。 它是在对所有词语取平均后，在上面加上几层神经网络。特殊的地方在于它在sentiment analysis中表现也不错，这在BOW类方法中比较罕见。 新方法 类型 基于的旧算法 贡献 SIF 无监督 BOW 一个简单而有效的baseline算法 InferSent 监督 NA 找到了NLP领域的ImageNet – SNLI， 并给出了一个state-of-art 算法 P-mean 无监督 BOW 比SIF更简单且有效的一个算法且适用于cross-lingual Universal-sentence-encoder 监督 InferSent 更加简单的encoder 文章共提出两种基于不同网络架构的Universal Sentence Encoder：Transformer and Deep Averaging Network (DAN).Our two encoders have different design goals. One based on the transformer architecture targets high accuracy at the cost of greater model complexity and resource consumption. The other targets efficient inference with slightly reduced accuracy. NLP 是如何体现了多任务训练的？ 该篇论文在前人的研究基础上，综合利用无监督训练数据和有监督训练数据，进行多任务训练，从而学习一个通用的句子编码器。无监督训练数据包括问答(QA)型网页和论坛，Wikipedia, web news，有监督训练数据为SNLI。多任务模型设计如下图所示，其中灰色的encoder为共享参数的句子编码器。 论文对比了DAN和Transfomer这两种编码器。得出如下结论： Transformer 模型在各种任务上的表现都优于简单的 DAN 模型，且在处理短句子时只稍慢一些。（更高的精度） DAN模型也能具有很不错的表现，并且相较于Transformer模型，训练时间和内存的开销都更小，尤其是当句子较长时。（更快的速度） 总结： Sentence Embedding的质量往往由训练数据和Encoder共同决定。Encoder不一定是越复杂越好，需要依据下游任务、计算资源、时间开销等多方面因素综合考虑。 BERT的理解词汇扩展： 对于词向量中 OOV 问题的处理方法： 如果使用词向量分词的话，一种常见的是字节维度 n-gram 模型，也就是把一个单词分成多个部分，比如说playing 分成play he ##ing 两个token，这种更加细粒度的划分是一种常见的处理oov 的方式。 但是在工业界，经常使用多个不同的语言模型得到word2vec， 可以分成两类，一类是针对该任务训练的word2vec，一类是在通用的模型下进行训练的word2vec， 并且当这种训练方法不同的时候，最后得到的结果也是不同的。通过不同的任务进行补充。在一定程度上是可以缓解 oov 问题的。 在句子向量中进行词汇扩展的方式：常见的是使用 word2vec的词向量来进行扩展句子向量中的词向量。 ELMo 思路是使用双向RNN 在大量未标注数据上训练语言模型，对于之后特定的任务，我们使用这个语言模型进行特征提取得到输出的向量。和 word2vec 不同的是，这个embedding 是有上下文的。比如说 bank的embedding 的上下文如果有river 那么就是水边的意思；如果上下文有money 那么更可能是银行的意思。 实现： 基于lstm 进行实现的，总的loss 是前后两个loss的相加，优化的时候，两个lstm的交叉熵加起来是最小的。 openai 出的 GPT (generative pre-training)， 得到的语言模型中的参数不是固定的，是可以根据特定的任务进行微调，使得词向量更加匹配特定的任务。思想也是很简单，使用transformer学习一个语言模型，对句子进行无监督的embedding，然后根据特定的任务对transformer的参数进行微调。 无监督的预训练：最初的时候 transformer是用来进行机器翻译的，encoder 得到的输出输入到decoder中去。但是在GPT 中的模型，encoder 是用来预测下一个词的。但是基于self-attention的基本结构，它是能够看中心词汇左右两边的上下文，这个特点和想要达到的任务是不符合要求的。所以这里使用到了 mask的原理，将中心词后面的词汇遮住，然后进行训练。 有监督的fine-tuning当只有一个句子（分类问题） 使用简单的分类问题作为一个例子，给定一个句子(x1, …xn)，然后给定标签。然后再最上层加上一个softmax，使用交叉熵损失函数计算loss，从而根据数据调整之前 transformer 和softmax 中weights 的参数。 本来按照elmo 的思想，我们可以fixed（固定）encoder 这个语言模型中的参数，然后只是训练最后softmax 中weights 的参数，但是这里同时去优化 encoder 和最后softmax 的参数，就类似一种多任务学习，同时优化了两个loss，并且这两个loss 中是可以设置权重的，所以从理论上讲模型是具有更好的泛化性能的。（对于前一个loss 的训练，因为前者是无监督的，所以这里只是使用了其中的x，而没有使用其中的y）并且训练速度也会提高，为什么这么说呢？因为是基于大的已经训练好的数据集上进行fine tune，得到的结果可以用不会太差来进行描述，那么你的loss 也会相应的不会太大，所以需要的迭代的次数也不会很多。 当有两个句子的时候（比如相似度的计算或者问答系统）。需要使用特殊的技巧将两个序列变成一个输入序列。（上面有个图是可以非常清楚的展示如何处理多种不同输入） 对于只有一个序列的任务，可以在前后加上两个特殊token，”start” 和”extract”，分别表示开始和结束；对于两个序列，可以在中间加上一个特殊的token, “delim”，输出是三分类标签中的一个。如果是相似度计算，因为对称性，可以把他们交换顺序，然后输入两个transformer。 好终于进入了bert 的学习： 问题： 传统的ELMo 或者GPT 最大的问题是语言模型是单向，不同同时得到前后两个方向的信息。注意transformer中的self-attention 从理论上是可以同时handle 前后上下文的，但是这里使用了mask 机制，所以这种方式也是不行的。防止过拟合的方法：通过对网络结构的约束，比如CNN的局部特效，RNN的时序特效，多层网络的层次结构，对它进行了很多约束，从而使得能够收敛到最佳的参数。 解决方案：在BERT之前，LM 通常是单向的，常见的做法是分别训练正向和反向的LM，然后再做一个ensemble得到的上下文相关表示。这样的做法是会有信息缺失的问题的。 BERT 是 “Bidirectional Encoder Representations from Transformers” 的缩写，B表示模型能够同时利用前后两个方向的信息，而ELMo和GPT 只能是单个方向的。 而bert 仍然使用的是 transformer模型，那么是如何解决语言模型中的只利用一个方向的问题呢？因为bert 不是普通的语言模型，而是一种mask 语言模型。 bert 的输入表示：输入是两个句子，然后是对于每个token 进行3 个embedding：词的embedding， 位置的embedding和segment 的embedding。词语的embedding 是非常常见的，位置embedding引入了词语的顺序信息，segment 的embedding可以学习到不同的segment的信息。位置向量是因为transformer 不像传统的RNN 那样能够很好的处理时序，所以人为加入了表示位置的向量。 这种海量数据还是很重要的。 bert 模型是需要有一个固定的sequence的长度，比如说是128，如果不够了会padding，如果多了会进行裁剪。 Mask LM 和NSP 分别对应的是词级别和句子级别的任务，效果很好。bert也是一种语言模型，在语料训练过程中，是把这两个任务的损失函数相加，同时学习这两个任务。所以这个就是一种多任务学习方式。BERT是通过两个辅助任务训练语言模型。 Mask LM（bert 的第一个重点终于来了） mask语言模型类似完形填空，给定一个句子，然后把其中的某个词遮挡起来，让人猜测可能的词语。这个会随机mask 15%的词， 然后让bert 来预测这些mask 的词，同归调整模型的参数使得模型预测正确的概率尽可能的大，这个等价于交叉熵的损失函数。这样的transformer在编码一个词的时候（必须）参考上下文的信息。 但是还有一个问题，就是在pretraineing mask LM 时候会出现一些特殊的token，但是在fine-tuning 时候并不会出现，这个时候就出现了mismatch 的问题。因此在bert中，如果某个token 被选中之后， 会随机按照以下的方式随机的执行： 80%随机替换成mask 10%替换成随机的一个词 10%概率替换成单词本身 因此，当他看到了 [mask或者apple 的时候，强迫模型在编码的时候不能太依赖当期的词，而是要考虑上下文，甚至进行上下文的“纠错”。 预测句子关系（bert模型中第二个训练任务） 在问答中，前后两个句子有一定的关联关系，我们希望bert pretraining 的模型能够学习到这种关系。因此bert 增加了一种新的额学习任务–预测两个句子是否有关联关系。这个训练集要求是文章（有上下文关系的句子）。对于这个任务，bert 以50%的概率随机抽取两个无关的句子，50%的概率抽取有关联的句子。（这个句子是经过token处理的句子） 实验证明该项任务是可以明显给QA和NLI 类任务带来提升的。 fine-tuning 共有四种任务， （使用过的是两种任务，a和b，分别进行 sentence pair classification 和single sentence classification）对于普通的分类任务，输入是一个序列，如图右上所示，所有的token 都是属于同一个segment，然后再模型的最后一层接上一个softmax进行分类， 用分类数据进行fine tuning 对于相似度计算等输入为两个序列的任务，过程如左上所示；两个序列的token 是对应着不同的segment(id =0/1)。在最后一层加上softmax 进行分类，然后使用分类数据进行fine-tuning 第三类任务是序列标注，比如命名实体识别，使用右下的方式进行训练。 第四类是问答类问题，输入是一个问题和一段很长包含答案文字（paragraph），输出在这段文字里找到的问题的答案。 在参数设置上，作者建议大部分的参数不用变，只是修改batch size， learning rate 和 number of epochs 就可以了：batch size: 16,32learning rate(adam): 5e-5, 3e-5, 2e-5number of epochs: 3, 4并且训练数据集越大，对超参数就越不敏感，而且fine tune 一般来说收敛的是比较快的。 对于中文来说，bert对中文提供的模型是基于字的，而word2vec 是基于词的，所以当word2vec的词向量效果越好，那么这个差距是越大的。 有监督的模型效果好，但是有标签的数据获取非常难。一种有效的解决方案是采用多任务学习(multi task learning MLT), 一方面可以在数据集标注较少的情况下利用其它相似任务的标注数据，另一方面可以降低针对特定任务的过拟合，起到正则化的作用。 Resnet, BERT都告诉我们： 更大的数据规模，更多样性的数据和更高的数据质量。数据还是比较关键的。 bert 模型的缺点： 对于篇章级别的任务，transformer的计算量复杂，速度是变得很慢。解决方案是进行长输入的切分 网络结构的过于复杂 transformer 的理解Transformer 其采用 Self Attention 来学习序列的表示, 具体的是: Scaled Dot-Product Attention. 为解决位置信息 (Position Information) 丢失问题, 模型将 Positional Encpding 与 Input Embedding 结合；为防止 decoder 中后续位置 (模型可并行计算) 对前面位置的影响, 模型在 decoder 中使用了 Mask 以使位置 ii 处的预测只依赖于前面的输出. Transformer由且仅由self-Attenion和Feed Forward Neural Network组成。一个基于Transformer的可训练的神经网络可以通过堆叠Transformer的形式进行搭建.作者的实验是通过搭建编码器和解码器各6层，总共12层的Encoder-Decoder(这个只是一个通用的框架，实际上是可以根据自己的需求进行不同的层的增减) Multi-Head Attention相当于 多个不同的self-attention的集成（ensemble）。 如何表示位置信息？ 常见的模式有：a. 根据数据学习；b. 自己设计编码规则。在这里作者采用了第二种方式。编码公式如下： $$P E(\text { pos, } 2 i)=\sin \left(\frac{\text { pos }}{10000^{\frac{2 i}{\text { model }}}}\right)$$ $$P E(p o s, 2 i+1)=\cos \left(\frac{p o s}{10000^{\frac{2 i}{d_{m o d e l}}}}\right)$$ 上式中，$pos$ 表示当前单词在句子中的位置，可以看出对于偶数位，使用正弦编码，对于奇数位使用余弦编码.。$d$ 表示模型的维度。除了单词的绝对位置，单词的相对位置也非常重要。这就是为什么使用正弦和余弦函数。根据公式$\sin (\alpha+\beta)=\sin \alpha \cos \beta+\cos \alpha \sin \beta$ 和$\cos (\alpha+\beta)=\cos \alpha \cos \beta-\sin \alpha \sin \beta$，这表明位置$k+p$ 的位置向量可以表示为位置 $k$的特征向量的线性变化，这为模型捕捉单词之间的相对位置关系提供了非常大的便利。 谷歌还特意将这种方式构造的向量和学习得到的向量作对比，发现效果接近，然后谷歌就用这个构造式的，因为虽然效果接近，但这种构造式的更能在使用中适应不同长度序列。 将位置向量和词向量进行加和得到最终输入向量，所以前面我们看到词向量和位置向量维度是相同的。 两种mask 技术 padding mask：mask对某些值进行掩盖，使其不产生效果。我们每次批处理序列的长度是不一样的，所以我们需要对齐，具体来说是在较短序列中填充0. 而attention机制不应该把注意力放在这些位置上。具体做法是这些位置上加上一个非常大的负数，这样经过softmax，这些位置的概率就会接近0. 在 encoder 和decoder 中都使用。 sequence mask：是为了decoder 不能看见未来的信息，进行预测的时候只是依赖前$i$ 个单词的信息。具体做法，产生一个上三角形，上三角形值全部为1，下三角形和对角线都是0，作用在序列上就可以达到目的。只是在decoder 中使用。 对于 attention 机制的分类 可以从多角度对 Attention 进行分类，如从信息选择的方式上，可以分为 Soft attention 和 Hard attention。从信息接收的范围上可分为 Global attention 和 Local attention。 global attention 中所有的信息都要参与计算，这样计算的开销就比较大，而别当encoder 的句子比较长时，如一段话或一篇文章。所以提出了 local attention的概念 笔记attention 机制是对于不同的词，在处理的时候给予不同的权重。最初是使用在机器翻译上，可以用来解决对齐问题。多头注意力机是点乘的升级版本，类似集成（ensemble） 的思想， 不同子空间的融合。 SIF 中第一个公式中 $\alph$ 是一个超参数，一种平滑项，对于低频词的支持，即使出现的次数少，也还是有权重的。 语序、语义和位置信息，这个都是相同的含义的不同表达。 transformer 相关总结 transformer 的结构：encoder 是由相同的6 个层组成，每个层是由多头自我注意力机制和全连接网络组成；层和层之间使用残差网络连接。decoder 也是由相同的6个层组成，decoder 是由masked 多头机制，用来保证预测第 $i$ 个位置的时候，只是依赖小于$i$ 的输入。多头attention 机制，原理很简单，想要从不同的方面维度处理信息，类似CNN中的filter 的概念，可以得到不同语义空间投射文本，然后将多个矩阵连在一起。transformer中使用padding mask 和sequence mask 两种mask 技术，分别是padding mask和 sequence mask，其中padding mask 是一种填充技术，使得不定长的sequence变成定长的sequence。 transformer 的输入和输出都是词向量+位置向量。对于位置信息有两种处理手段，一种是通过网络学习，一种是 设计编码规则。作者使用的是后者。使用单词的位置和单词的维度对单词进行位置编码，分别使用sin()和 cosine()函数计算位置信息，编码是一个长度为$d_{\text {model}}$ 的特征向量。该向量除了考虑绝对位置，还可以考虑到单词的相对位置。 transformer 的计算上的优势，之前的RNN 是序列线性结构，句首的信息要经过n 次RNN 才能到达句尾，无法并行运算。而self-attention可以连接任意两个节点，所以可以进行并行运算。 batch normalization 和 layer normalization 的区别？前者依赖于 batch size，经常使用在CNN 中，是不同样本在同一个特征上的归一化；后者是在样本上不同特征进行的归一化，经常在RNN中使用。 ELMo (1 ) 得到的特征是有上下文的，模型上使用的是bi-lstm 进行训练。好处，对于多义词有不同的embedding，缓解了歧义的发生。实现：正向一个loss，反向一个loss，相加，然后一起进行最小化。 (2 ) ELMo 也引入了字符级信息，不仅丰富了词向量的表达，也很大程度上解决了NLP 领域中OOV 问题。 OpenAI GPT 基于 transformer学习一个语言模型，对句子进行无监督的embedding，然后根据具体的任务对参数进行微调。 BERT 两个比较巧妙的预训练模型：mask learning 和 predicting next sentence。通过这两个模型在大语料中进行无监督的学习。 bert 的输入： (1) 词向量 (2) segment embedding用来区别两个句子，因为bert 语言模型还要做两个句子输入的分类模型 (3) position embedding 通过公式计算的。 bert 和ELMo 的区别: (1) ELMo 的基础是LSTM，而BERT使用 transformer 作为基本模型(2) 目标函数不同，ELMo是两个单个loss 的相加，后者是给定上下文，然后预测中心词，这种是同时的。]]></content>
      <categories>
        <category>NLP</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Dynamic Programming Examples(1)]]></title>
    <url>%2F2019%2F05%2F27%2FDynamic-Programming-Examples%2F</url>
    <content type="text"><![CDATA[Dynamic Programming is a method for solving a complex problem by breaking it down into a collection of simpler subproblems, solving each of those subproblems just once, and storing their solutions using a memory-based data structure (array, map, etc.). So the next time the same sub-problem occurs, instead of recomputing its solution, one simply looks up the previously computed solution, thereby saving computation time. knapsack problem: partition-equal-subset-sum 网站链接 python 版本 12345678910111213141516171819202122class Solution(object): def canPartition(self, nums): """ :type nums: List[int] :rtype: bool """ n =len(nums) s = sum(nums) if s &amp;1 ==1: return False dp =[0 for _ in range(s+1)] dp[0] =1 #import ipdb for num in nums: for i in range(s, -1, -1): # ipdb.set_trace() if dp[i]: dp[i+num] =1 if dp[s//2]: return True return False C++ 版本 12345678910111213141516171819202122232425262728293031323334#include&lt;iostream&gt;#include&lt;vector&gt;#include&lt;algorithm&gt;using namespace std;// dp, 01 背包问题，sum表示体积 从大到小进行枚举， dp[i] 选择第i 个数组成的和, 转移 dp[i],int main()&#123; int n; cin&gt;&gt; n; vector&lt;bool&gt; f(n+1, false); vector&lt;int&gt; arr; //for(int i =0; i&lt;n; i++) cin&gt;&gt;arr[i]; arr=&#123;1, 5, 11, 5&#125;; int sum =0; for(auto u: arr) sum += u; if(sum &amp;1) cout&lt;&lt;"false"&lt;&lt;endl; f[0] =true; int half =sum/2; for(auto num: arr) for(int j =half; j&gt;= num; j--) &#123; f[j] =f[j] || f[j-num]; // 01背包问题，选择与否的关系 &#125; cout&lt;&lt;f[half]&lt;&lt;endl; return 0;&#125; longest commone substring Given two strings ‘X’ and ‘Y’, find the length of the longest common substring. Input : X = “GeeksforGeeks”, y = “GeeksQuiz”Output : 5 原文链接 首先审题是需要注意的，需要注意是公共子串还是子序列，前者要求连续index 而后者不要求。 1234567891011121314151617181920212223242526272829"""Given two strings ‘X’ and ‘Y’, find the length of the longest common substring."""class Solution(object): def minDistance(self, word1, word2): m =len(word1) n =len(word2) #dp=[ [None] for _ in range(n+1) for _ in range(m+1)] dp = [[None] *(n +1) for _ in range(m+1) ] for i in range(m+1): for j in range(n+1): if i ==0 or j ==0: dp[i][j] =0 # 这个是python 中语法决定的 word1[len(word1)] 是访问不到的，这个访问是从0开始的，所以只能是这样的 elif word1[i-1] == word2[j-1]: dp[i][j] =dp[i-1][j-1] +1 else: dp[i][j] =max(dp[i-1][j], dp[i][j-1]) return dp[m][n]solution =Solution()word1 ='abcdaf'word2 ='acbcf'result =solution.minDistance(word1, word2)print(result) c++ 版本。当使用 f(n+1, m+1)， 然后在遍历的时候，从1 开始遍历的时候，在if 进行判断的时候， text1[i-1] == text2[j-1] 这个条件是经常弄错的。一定要从 i-1开始计算。 1234567891011121314151617181920212223242526272829303132class Solution &#123;public: int longestCommonSubsequence(string text1, string text2) &#123; int n, m; n =text1.size(), m =text2.size(); vector&lt;vector&lt;int&gt;&gt; f(n+1, vector&lt;int&gt;(m+1)); f[0][0] =0; for(int i=1; i&lt;=n; i++) &#123; for(int j =1;j&lt;=m ; j++) &#123; if(text1[i-1] ==text2[j-1] ) f[i][j]=f[i-1][j-1] +1; else &#123; f[i][j] =max(f[i-1][j], f[i][j-1]); &#125; &#125; &#125; return f[n][m]; //cout&lt;&lt;f[n][m]&lt;&lt;endl; &#125;&#125;;]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Beyond Word Embedding]]></title>
    <url>%2F2019%2F05%2F22%2Fbeyond-word-embedding%2F</url>
    <content type="text"><![CDATA[从one-hot 到 word2vec， 到elmo，简单介绍一下 NLP中词向量的过程。 进入正题之前，思考为什么要将词用向量来表示呢？这样可以给词语一个数学上的表示，使之可以适用于某些算法或数学模型。通常将词语表示成向量有如下两种方法: one-hot and distributed 表示法。one-hot 只有一个位置是1 其他的位置都是0，最大的特点就是数据变得稀疏，而后者属于稠密向量。前者的缺点向量是相互独立的，无法通过距离函数比如 cosine 进行相似度的比较，并且如果维度 N非常大，那么高纬度的表示也可能引发维度灾难。于是接着往下看吧… Traditional Word VectorsBefore diving directly into Word2Vec it’s worth while to do a brief overview of some of the traditional methods that pre-date neural embeddings. 这个是用来描述文章的，有一个大的dict，然后一片文章是如何进行表示、Bag of Words or BoW vector representations are the most common used traditional vector representation. Each word or n-gram is linked to a vector index and marked as 0 or 1 depending on whether it occurs in a given document. An example of a one hot bag of words representation for documents with one word. 局限性: 一方面只是一种counter，没有考虑语义信息；另一方面有些 words 是明显的 relevant than others.BoW representations are often used in methods of document classification where the frequency of each word, bi-word or tri-word is a useful feature for training classifiers. One challenge with bag of word representations is that they don’t encode any information with regards to the meaning of a given word.In BoW word occurrences are evenly weighted independently of how frequently or what context they occur. However in most NLP tasks some words are more relevant than others. 这个是可以认识是对于 bag of words “relevant” 上的改进：使得 选择的words 更加的 “representative” 文章的调性。TF-IDF, short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word or n-gram is to a document in a collection or corpus. They provide some weighting to a given word based on the context it occurs.The tf–idf value increases proportionally to the number of times a word appears in a document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently than others. 但是对于 bag of words 中“没有体现语义” 的缺陷还是没有 deal with。However even though tf-idf BoW representations provide weights to different words they are unable to capture the word meaning. 这个名字只是因为有定义而存在的名字（网络模型 or 深度网络的出现就是为了 handle 语义信息）Distributional Embeddings enable word vectors to encapsulate contextual context. Each embedding vector is represented based on the mutual information it has with other words in a given corpus.重点就是这种方式是要 predict a target word from context words，一定是要能够体现语境的。Predictive models learn their vectors in order to improve their predictive ability of a loss such as the loss of predicting the vector for a target word from the vectors of the surrounding context words. word2vec 两种类型 word2vec 是一种思想，有两种CBOW 和skip-gram 两种实现。Word2Vec is a predictive embedding model. There are two main Word2Vec architectures that are used to produce a distributed representation of words: Continuous bag-of-words (CBOW) — The order of context words does not influence prediction (bag-of-words assumption). Continuous skip-gram weighs nearby context words more heavily than more distant context words. While order still is not captured each of the context vectors are weighed and compared independently vs CBOW which weighs against the average context. word2vec 就是2 种算法+ 2种模型，总共是四种实现。 展示一下结构图： word2vec 的训练过程： 中文分词，然后保存所有的语料到一个文件中，可以使用 换行符进行分开 扫描语料库统计词频，取词频最高的V 个词，构成词汇表，one-hot 编码， 词的维度就是词典的大小，其余（出现频率很低）的词都用一个特殊符号代替掉。 词向量是从输入层到隐藏层的weights，随着初始化而存在，然后之后是不断优化的产物 训练的目标的，以skip-gram 为例，输入中心词然后最大化输出周围的词 (context )词汇。 输入层的输入：每个词存在一个one-hot向量，向量的维度是V（词典大小），如果该词在词汇表中出现过，则向量中词汇表中对应的位置为1，其他位置全为0。如果词汇表中不出现，则向量为全0 负采样（Negative Sample）和层次softmax（Hierarchical Softmax）则是两种加速训练的方法。都是优化最后的softmax 层（输出层），因为这个大小就是词典的大小，计算量太大了，如果知道softmax，是存在指数计算的。 loss function 在cbow模型中，所有的词被编码成ont-hot向量，V为总词语数。input层的one-hot vector经过 $W_{VXN} $矩阵后，被压缩为只有N个元素的向量h，之后经过W′矩阵出来，得到u。于是根据公式，有 $$p \left( w _ { t } | w _ { \text {input} } \right) = y _ { j } = \frac { \exp \left( u _ { j } \right) } { \sum \exp \left( u _ { j } \prime \right) }$$ 最大化该条件概率，得到 $$\max p \left( w _ { t } | w _ { \text {input} } \right) = \max \log y _ { j } = u _ { j } - \operatorname { log } \sum \exp \left( u _ { j } \right)$$于是得到了 词袋模型的 loss function： （关于网络中的 loss function 还是要多留意一下的）$$E = - \log p \left( w _ { t } | w _ { \text {input} } \right) = \log \sum \exp \left( u _ { j } \right) - u _ { j }$$这里，$u _ { j }$ 表示第 $j$ 个词向量， 有了 loss function，就可以进行词向量的训练了。 层次softmax 比如说一个二叉树结构，“我”肯定是第一层叶子节点，“涮羊肉”肯定是在最后一层的叶子节点。在 word2vec 中输入输出的编码都是使用的 one-hot 进行数字化表示的。 这个存储的目的是遍历的次数少了，因为是使用二分类去做多分类，如果词频高的编码少，那么最后的结果是比较少的。word2vec训练的时候按照词频将每个词语Huffman编码，由于Huffman编码中词频越高的词语对应的编码越短。所以越高频的词语在 Hierarchical Softmax过程中经过的二分类节点就越少，整体计算量就更少了。 总的特点：使用 context words 去predict 中心词 or 相反的过程，最大化这种概率关系。CBOW is faster while skip-gram is slower but does a better job for infrequent words.那么为什么快呢？ 答： cbow只要 把窗口内的其他词相加一次作为输入来预测 一个单词。不管窗口多大，只需要一次运算。而skip-gram直接受窗口影响，窗口越大，需要预测的周围词越多。在训练中，通过调整窗口大小明显感觉到训练速度受到很大影响。前者是复杂度大概是O(V)，后者的时间的复杂度为O(KV）(假设K 是窗口的大小) 为什么skip-gram 的准确率高一些，对于生僻词的效果更好一些？ 在skip-gram当中，每个词都要收到周围的词的影响，每个词在作为中心词的时候，都要进行K次的预测、调整。因此， 当数据量较少，或者词为生僻词出现次数较少时， 这种多次的调整会使得词向量相对的更加准确。因为尽管cbow从另外一个角度来说，某个词也是会受到多次周围词的影响（多次将其包含在内的窗口移动），进行词向量的跳帧，但是他的调整是跟周围的词一起调整的，grad的值会平均分到该词上， 相当于该生僻词没有收到专门的训练，它只是沾了周围词的光而已。 算法参数总体上对于效果影响不大，最重要的是语料。相对来说，比较重要的常用的参数： min-count： （这个思想有点意思呀，切词切错了，那么在计算的时候就不要了） 最小词频训练阀值，这个根据训练语料大小设置，只有词频超过这个阀值的词才能被训练。根据经验，如果切词效果不好，会切错一些词，比如 “在深圳”，毕竟切错的是少数情况，使得这种错词词频不高，可以通过设置相对大一点的 min-count 过滤掉切错的词。（这种是对于新词处理的一种补救方法） 向量维度： 如果词量大，训练得到的词向量还要做语义层面的叠加，比如 句子 的向量表示 用 词的向量叠加，为了有区分度，语义空间应该要设置大一些，所以维度要偏大。一般来说，中文 180 就差不多了。 负采样： 负采样（negative sampling）解决了这个问题，它是用来提高训练速度并且改善所得到词向量的质量的一种方法。不同于原本每个训练样本更新所有的权重，负采样每次让一个训练样本仅仅更新一小部分的权重，这样就会降低梯度下降过程中的计算量。 在论文中，作者指出指出对于小规模数据集，选择5-20个negative words会比较好，对于大规模数据集可以仅选择2-5个negative words。 任何采样算法都应该保证频次越高的样本越容易被采样出来。基本的思路是对于长度为1的线段，根据词语的词频将其公平地分配给每个词语：$$\operatorname { len } ( w ) = \frac { \operatorname { counter } ( w ) } { \sum _ { u \in \mathcal { D } } \operatorname { counter } ( u ) }$$ 在word2vec中，该“刻度尺”对应着table数组。具体实现时，对词频取了0.75次幂：$$\operatorname { len } ( w ) = \frac { [ \operatorname { counter } ( w ) ] ^ { 0.75 } } { \sum _ { u \in \mathcal { D } } [ \operatorname { counter } ( u ) ] ^ { 0.75 } }$$这个幂实际上是一种“平滑”策略，能够让低频词多一些出场机会，高频词贡献一些出场机会，劫富济贫。 （这个是一种计算上的优化，通过选取一部分结点（词汇）更新权重）负采样越低，对高频词越不利，对低频词有利。可以这么理解，本来高频词 词被迭代50次，低频词迭代10次，如果采样频率降低一半，高频词失去了25次迭代，而低频词只失去了5次。一般设置成le-5. ( 这个就是 $10^{-5}$ ) 在 fasttext 实现的时候 使用下面的超参数记性控制。 -neg number of negatives sampled [5] 窗口大小： 窗口大小影响 词 和前后多少个词的关系，和语料中语句长度有关，建议可以统计一下语料中，句子长度的分布，再来设置window大小。一般设置成8。（参数也是一个技术活，参数的设置和原始训练数据集 和其他的参数的配合是相关联的。如果句子比较长，那么window size 就不要太小） 负采样 vs 窗口大小 负采样主要是为了降低模型计算量。如果没有负采样，模型需要把词汇表中没有出现在滑动窗口的词语当作负样本。然而在实际训练过程中，并不需要这么多的负样本，过多的负样本会导致模型学偏。（窗口的大小是正采样的个数，那么 负采样的个数和窗口的大小尽量是保持了 1：1 的关系，这样是比较好的） 负采样的个数和滑动窗口的比例尽量控制在0.1-10之间，滑动窗口决定了正样本的数量，负采样的个数决定了负样本的个数，正负样本尽量不要差距太大，建议负采样的个数和滑动窗口的比例控制为1：1。 比较详细的介绍可以查看这里 如何评估 word2vec 训练的好坏？ 词聚类 （可以采用 kmeans 聚类，看聚类簇的分布） 词cos 相关性（查找cos相近的词） Analogy对比 （man-king， woman-queen） 使用tnse，pca等降维可视化展示 更多的评价方法可以参见这里. glove （g lou v） （复习到这里了） Intuition: Both CBOW and Skip-Grams are “predictive” models, in that they only take local contexts into account. word2vec does not take advantage of global context.(细节 能看懂就看)GloVe embeddings by contrast leverage the same intuition behind the co-occurrence matrix (共生矩阵) used distributional embeddings, but uses neural methods to decompose the co-occurrence matrix into more expressive and dense word vectors. 模型目标：进行词的向量化表示，使得向量之间尽可能多地蕴含语义和语法的信息。输入：语料库输出：词向量方法概述：首先基于语料库构建词的共现矩阵，然后基于共现矩阵和GloVe模型学习词向量。 下面是一个例子： sparse vectors 词-文档矩阵(Term-document matrix) 和 词共现矩阵(Term-term matrix)。Term-document matrix表示每个单词在文档中出现的次数(词频)，每一行是一个 term，每一列是一个 document两篇文档的向量相似 =&gt; 两篇文档相似，如上图 doc3 和 doc4，我们就认为它们是相似的。两个单词的向量相似 =&gt; 两个单词相似，如上图的 fool 和 clown，就是相似的。 Term-term matrix然后我们可以考虑更小的粒度，更小的上下文，也就是不用整篇文档，而是用段落(paragraph)，或者小的窗口(window of ±4 words)，所以这个时候，向量就是对上下文单词的计数，大小不再是文档长度 |D|，而是窗口长度 |V| 了，所以现在 word-word matrix 是 |V|*|V| 而 word2vec 得到的向量 dense vectors。不使用 negative sampling 的Wordvec 非常快，但准确率不高（57.4\%）,毕竟模型没有告诉什么是无关的word，模型很难对无关词汇进行惩戒，提高准确率。对于 synonym 问题，word2vec 是好于 glove，但从最终的效果上看，两者是不分彼此的。glove 使用了整体的信息，word2vec 只是使用了局部信息（local context）。 word2vec 和glove 的区别：Predictive的模型，如Word2vec，根据context预测中间的词汇，要么根据中间的词汇预测context，分别对应了word2vec的两种训练方式cbow和skip-gram。 Count-based模型，如GloVe，本质上是对共现矩阵进行降维。首先，构建一个词汇的共现矩阵，每一行是一个word，每一列是context。共现矩阵就是计算每个word在每个context出现的频率。由于context是多种词汇的组合，其维度非常大，我们希望像network embedding一样，在context的维度上降维，学习word的低维表示。该向量表示属于 sparse vectors 和word2vec 的效果比较：glove 和word2vec 相比没有 definitively better results，还是通过实验进行说话吧。While GloVe vectors are faster to train, neither GloVe or Word2Vec has been shown to provide definitively better results rather they should both be evaluated for a given dataset. fasttext这个主要是 each word + n-gram within each word， 最后的效果是好于 word2vec 的。FastText, builds on Word2Vec by learning vector representations for each word and the n-grams found within each word. The values of the representations are then averaged into one vector at each training step. While this adds a lot of additional computation to training it enables word embeddings to encode sub-word information. FastText vectors have been shown to be more accurate than Word2Vec vectors by a number of different measures. 简单说 fasttext 和word2vec 模型上的不同有两点： 模型的输出层：word2vec的输出层，对应的是每一个term，计算某term的概率最大；而fasttext的输出层对应的是 分类的label。不过不管输出层对应的是什么内容，起对应的vector都不会被保留和使用； fasttext则充分利用了h-softmax的分类功能，遍历分类树的所有叶节点，找到概率最大的label（一个或者N个） 模型的输入层：word2vec的输出层，是 context window 内的term；而fasttext对应的整个sentence的内容，包括term，也包括 n-gram的内容 更多信息可以参看博客。 overview of Neural NLP ArchitecturesDeep Feed Forward Networks 1D CNNs RNNs (LSTM/GRU) encoder- decoder 结构 attention and copy mechanisms这个是 attention 机制提出的背景：解决 句子中的长依赖；contextual impact (specific words may carry more importance at different steps)While in theory they can capture long term dependencies they tend to struggle modeling longer sequences, this is still an open problem. One cause for sub-optimal performance standard RNN encoder-decoder models for sequence to sequence tasks such as NER or translation is that they weight the impact each input vector evenly on each output vector when in reality specific words in the input sequence may carry more importance at different time steps.Attention mechanisms provide a means of weighting the contextual impact of each input vector on each output prediction of the RNN. These mechanisms are responsible for much of the current or near current state of the art in Natural language processing. attention In sum, algorithms can allocate attention, and they can learn how to do so, by adjusting the weights they assign to various inputs. Imagine a heat map over a photo. The heat is attention. 这个是要引出来 context word embeddings.One of the limits of traditional word vectors is that they presume that a word’s meaning is relatively stable across sentences.并不是物理上的二维关系能够表示词语之间的 relationship，有时候是需要高纬空间进行表示的。In fact, the strongest relationships binding a given word to the rest of the sentence may be with words quite distant from it.从 credit assignment的角度阐述了 neural networks 就是 allocating importance to input features。The fundamental task of all neural networks is credit assignment. Credit assignment is allocating importance to input features through the weights of the neural network’s model. Learning is the process by which neural networks figure out which input features correlate highly with the outcomes the net tries to predict, and their learnings are embodied in the adjusted quantities of the weights that result in accurate decisions about the data they’re exposed to.这个是传统的 LSTM （encoder -decoder） 模型，问题在于当句子过长（比如说大于20 words）之后，encoder 是无法 memory 之前的所有 words，所以效果就会变得差一些。 但是 attention 就是模仿了人翻译过程，一段作为一个单位，然后进行翻译。这样就可以持续保证较高中确率的输出。In neural networks, attention primarily serves as a memory-access mechanism. 每次的输出都是关注不同的地方，但是至于哪里更加重要，这个交给了 feedback mechanism 反向传播。下面的图片十分清晰的展示了 在翻译的过程中 “focus” 是不断地变化的。Above, a model highlights which pixels it is focusing on as it predicts the underlined word in the respective captions. Below, a language model highlights the words from one language, French, that were relevant as it produced the English words in the translation. As you can see, attention provides us with a route to interpretability. We can render attention as a heat map over input data such as words and pixels, and thus communicate to human operators how a neural network made a decision. (This could be the basis of a feedback mechanism whereby those humans tell the network to pay attention to certain features and not others.) (This could be the basis of a feedback mechanism whereby those humans tell the network to pay attention to certain features and not others.) Additionally in Machine Reading Comprehension and Summarization systems RNNs often tend to generate results, that while on first glance look structurally correct are in reality hallucinated or incorrect. One mechanism that helps mitigate some of these issues is the Copy Mechanism.copy mechanism 简单说来就是 word embedding or raw text.The copy mechanism is an additional layer applied during decoding that decides whether it is better to generate the next word from the source sentence or from the general embedding vocabulary. reading comprehension and summary 上面是说的在 machine translation，下面说的是 阅读理解 和 summary领域。Additionally in Machine Reading Comprehension and Summarization systems RNNs often tend to generate results, that while on first glance look structurally correct are in reality hallucinated or incorrect. One mechanism that helps mitigate some of these issues is the Copy Mechanism.copy mechanism 简单说来就是 decide word embedding from model or raw text.The copy mechanism is an additional layer applied during decoding that decides whether it is better to generate the next word from the source sentence or from the general embedding vocabulary. Taming Recurrent Neural Networks for Better Summarization有两种不同的 summarization:Two types of summarization：Extractive （You might think of these approaches as like a highlighter.） Abstractive（By the same analogy, these approaches are like a pen.）The great majority of existing approaches to automatic summarization are extractive – mostly because it is much easier to select text than it is to generate text from scratch.但是一个问题在于，只是使用 extrative way 可能得到相同的words，Problem 1: The summaries sometimes reproduce factual details inaccurately (e.g. Germany beat Argentina 3-2). This is especially common for rare or out-of-vocabulary words such as 2-0.Problem 2: The summaries sometimes repeat themselves (e.g. Germany beat Germany beat Germany beat…)Easier Copying with Pointer-Generator Networks。这个跟 attention 不是很相关，简单说就是In this way, the pointer-generator network is a best of both worlds, combining both extraction (pointing) and abstraction (generating). To tackle Problem 2 (repetitive summaries), we use a technique called coverage. The idea is that we use the attention distribution to keep track of what’s been covered so far, and penalize the network for attending to same parts again. elmo (e l mo)elmo 产生一个 embedding 是根据 context 产生的。ELMo is a model generates embeddings for a word based on the context it appears thus generating slightly different embeddings for each of its occurrence.（感觉理解一个概念都是 根据其 for example 进行理解的）For example, the word “play” in the sentence above using standard word embeddings encodes multiple meanings such as the verb to play or in the case of the sentence a theatre production. In standard word embeddings such as Glove, Fast Text or Word2Vec each instance of the word play would have the same representation. 参考blog:https://towardsdatascience.com/beyond-word-embeddings-part-2-word-vectors-nlp-modeling-from-bow-to-bert-4ebd4711d0ec http://www.abigailsee.com/2017/04/16/taming-rnns-for-better-summarization.html https://skymind.ai/wiki/word2vec 复习笔记 评价 word embedding的方面：是否能体现语义信息；是否比其他词语更加具有代表性； word2vec(或者说 CBOW or Skip-gram )的特点，输入和输出都是 one-hot 的表示，长度都是字典的长度，在某一个位置是1，其他的位置都是0. 训练过程：分词之后组成预料库，扫描所有的词频，取词频最高的V个词语，构成词汇表，词频低的就会被丢弃（所以词语的学习需要多次的，有上下文的）， 都被用一个特殊符号代替掉。损失函数是概率模型，给定上下文然后中心词出现的概率，概率是需要加上log，就组成了loss function，训练的过程中是最大化概率模型（最小化loss）。CBOW和Skip-gram 的区别：CBOW在训练的过程中将上下文加和成一个向量表示，所以最后的训练速度是快于skip-gram 但是效果没有skip-gram。后者是一个个单词进行训练的（当然训练时间也是比较长的）。对于中文的embedding size，一般180 就OK了。在负采样中负样本是根据词频挑选的，但公式中使用了一种平滑机制，使得低频词也有了一些机会，对高频词语进行了一些约束。 word2vec 和glove 的区别：前者是预测类的模型，后者是cout-base的模型，本质上是对共现矩阵的降维处理。每一行表示一个word，每一列表示context，共现矩阵就是计算每个word 在每个context 中出现的频率。 fasttext 和word2vec 的区别：输入层，word2vec 是一个term，而fasttext中包含的是term和对应的n-gram的特征；输出层， 在分类中fasttext的输出是label 信息。]]></content>
      <categories>
        <category>NLP</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Hyper-parameter Optimization for Machine Learning]]></title>
    <url>%2F2019%2F05%2F22%2FHyperparameter-optimization-for-machine-learning%2F</url>
    <content type="text"><![CDATA[Following are four common methods of hyper-parameter optimization for machine learning in order of increasing efficiency: Manual Grid search Random search Bayesian model-based optimization 简单说一下前两种， manual 适合比较有经验的人进行调参，整一个模型好久了，对参数的设置比较熟悉；Grid Search 最好的理解方式就是 枚举，如果时间充足那么可以找到 search space 中的最优解。但如果是 limited time and space， 那么请使用 random search 或者基于贝叶斯的开源包。 Random SearchFirst we will implement a common technique for hyper-parameter optimization: random search. Each iteration, we choose a random set of model hyper-parameters from a search space. Random search uses the following four parts: Domain: values over which to search Optimization algorithm: pick the next values at random! (yes this qualifies as an algorithm) Objective function to minimize: in this case our metric is cross validation ROC AUC Results history that tracks the hyper-parameters tried and the cross validation metric Random search can be implemented in the Scikit-Learn library using RandomizedSearchCV, however, because we are using Early Stopping (to determine the optimal number of estimators), we will have to implement the method ourselves (more practice!). This is pretty straightforward, and many of the ideas in random search will transfer over to Bayesian hyper-parameter optimization. Empirically, random search is very effective, returning nearly as good results as grid search with a significant reduction in time spent searching. However, it is still an uninformed method in the sense that it does not use past evaluations of the objective function to inform the choices it makes for the next evaluation. Case Study of random search 123456789101112131415161718192021222324252627282930313233343536373839# Load librariesfrom scipy.stats import uniformfrom sklearn import linear_model, datasetsfrom sklearn.model_selection import RandomizedSearchCV# data and model# Load datairis = datasets.load_iris()X = iris.datay = iris.target# Create logistic regressionlogistic = linear_model.LogisticRegression()# Create hyper-parameter Search Space# Create regularization penalty space# 如果比较少，那么久枚举出来penalty = ['l1', 'l2']# 如果是有规律的连续的，就使用这种方式列举出来# Create regularization hyper-parameter distribution using uniform distributionC = uniform(loc=0, scale=4)# Create hyper-parameter optionshyper-parameters = dict(C=C, penalty=penalty)# cv: cross validation, This approach involves randomly dividing the set of observations into k groups, or folds, of approximately equal size. The first fold is treated as a validation set, and the method is fit on the remaining k − 1 folds.# Create randomized search 5-fold cross validation and 100 iterationsclf = RandomizedSearchCV(logistic, hyper-parameters, random_state=1, n_iter=100, cv=5, verbose=0, n_jobs=-1)# Fit randomized searchbest_model = clf.fit(X, y)# View best hyper-parameters# 注意这种获取best params 的方式print('Best Penalty:', best_model.best_estimator_.get_params()['penalty'])print('Best C:', best_model.best_estimator_.get_params()['C'])# Predict target vectorbest_model.predict(X) Comparison between Grid Search and Random Search 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364import numpy as npfrom time import timefrom scipy.stats import randint as sp_randintfrom sklearn.model_selection import GridSearchCVfrom sklearn.model_selection import RandomizedSearchCVfrom sklearn.datasets import load_digitsfrom sklearn.ensemble import RandomForestClassifier# get some datadigits = load_digits()X, y = digits.data, digits.target# build a classifierclf = RandomForestClassifier(n_estimators=20)# Utility function to report best scoresdef report(results, n_top=3): for i in range(1, n_top + 1): candidates = np.flatnonzero(results['rank_test_score'] == i) for candidate in candidates: print("Model with rank: &#123;0&#125;".format(i)) print("Mean validation score: &#123;0:.3f&#125; (std: &#123;1:.3f&#125;)".format( results['mean_test_score'][candidate], results['std_test_score'][candidate])) print("Parameters: &#123;0&#125;".format(results['params'][candidate])) print("")# specify parameters and distributions to sample fromparam_dist = &#123;"max_depth": [3, None], "max_features": sp_randint(1, 11), "min_samples_split": sp_randint(2, 11), "bootstrap": [True, False], "criterion": ["gini", "entropy"]&#125;# run randomized searchn_iter_search = 20random_search = RandomizedSearchCV(clf, param_distributions=param_dist, n_iter=n_iter_search, cv=5, iid=False)start = time()random_search.fit(X, y)print("RandomizedSearchCV took %.2f seconds for %d candidates" " parameter settings." % ((time() - start), n_iter_search))report(random_search.cv_results_)# use a full grid over all parametersparam_grid = &#123;"max_depth": [3, None], "max_features": [1, 3, 10], "min_samples_split": [2, 3, 10], "bootstrap": [True, False], "criterion": ["gini", "entropy"]&#125;# run grid searchgrid_search = GridSearchCV(clf, param_grid=param_grid, cv=5, iid=False)start = time()grid_search.fit(X, y)print("GridSearchCV took %.2f seconds for %d candidate parameter settings." % (time() - start, len(grid_search.cv_results_['params'])))report(grid_search.cv_results_) Random search without in-built function: An example of Random Search all by yourself Bayesian hyper-parameter OptimizationThe one-sentence summary of Bayesian hyper-parameter optimization is: build a probability model of the objective function and use it to select the most promising hyper-parameters to evaluate in the true objective function. The basic idea is: spend a little more time selecting the next hyper-parameters in order to make fewer calls to the objective function. In the case of hyper-parameter optimization, the objective function is the validation error of a machine learning model using a set of hyper-parameters. The aim is to find the hyper-parameters that yield the lowest error on the validation set in the hope that these results generalize to the testing set. Evaluating the objective function is expensive because it requires training the machine learning model with a specific set of hyper-parameters. Ideally, we want a method that can explore the search space while also limiting evaluations of poor hyper-parameter choices. Bayesian hyper-parameter tuning uses a continually updated probability model to “concentrate” on promising hyper-parameters by reasoning from past results. 有很多基于这种思想的实现，hyperopt 只是其中一种There are several Bayesian optimization libraries in Python which differ in the algorithm for the surrogate of the objective function. In this article, we will work with Hyperopt, which uses the Tree Parzen Estimator (TPE) Other Python libraries include Spearmint (Gaussian Process surrogate) and SMAC (Random Forest Regression). There are four parts to a Bayesian Optimization problem: Objective Function: what we want to minimize, in this case the validation error of a machine learning model with respect to the hyper-parameters （原来model 中的 objective function） Domain Space: hyper-parameter values to search over （调参空间） Optimization algorithm: method for constructing the surrogate model and choosing the next hyper-parameter values to evaluate （loss 和调参空间的 新的关系） Result history: stored outcomes from evaluations of the objective function consisting of the hyper-parameters and validation loss （result 没有什么好说的） 其中的 Bayesian hyper-parameter Optimization using Hyperopt是可以好好学习的。 data scientists 这种东西更加贴近于 data scientist 真的。 给出两个参考代码:链接一链接二]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Introduction to Natural Language Processing for Text]]></title>
    <url>%2F2019%2F05%2F21%2FIntroduction-to-Natural-Language-Processing-for-Text%2F</url>
    <content type="text"><![CDATA[Natural Language Processing is used to apply machine learning algorithms to text and speech. For example, we can use it to create systems like speech recognition, document summarization, machine translation, spam detection, named entity recognition, question answering, autocomplete, predictive typing and so on. NLTK (Natural Language Toolkit) is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to many corpora and lexical resources. Also, it contains a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning. Best of all, NLTK is a free, open source, community-driven project. In this article, we’ll cover the following topics.这些功能都是可以使用nltk 进行实现的。text Lemmatization 比如，单词“cars”词形还原后的单词为“car”，单词“ate”词形还原后的单词为“eat”。 Sentence Tokenization 段落成句。Sentence tokenization (also called sentence segmentation) is the problem of dividing a string of written language into its component sentences. The idea here looks very simple. In English and some other languages, we can split apart the sentences whenever we see a punctuation mark.（标点符号） Word Tokenization 句子成词，颗粒度变得更小。Word tokenization (also called word segmentation) is the problem of dividing a string of written language into its component words. In English and many other languages using some form of Latin alphabet, space is a good approximation of a word divider. Text Lemmatization 词性还原 and Stemming 词干提取 这种操作如果被认为是一种 normalization，那么一个优点就是加快了运行的速度。从不同的形式到统一的形式，这可以认为减少了变量。感觉这个更加涉及语法，语法树之类的东西。For grammatical reasons, documents can contain different forms of a word such as drive, drives, driving. Also, sometimes we have related words with a similar meaning, such as nation, national, nationality. Stemming and lemmatization are special cases of normalization. However, they are different from each other. Stemming usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes.Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma. Stop Words因为 stop words往往是带了 noise rather than useful information，所以这个是要去掉的。Stop words are words which are filtered out before or after processing of text. When applying machine learning to text, these words can add a lot of noise. That’s why we want to remove these irrelevant words. stop words dictionary 可以理解成一种过滤词表，是可以根据应用的不同，然后 change的。Stop words usually refer to the most common words such as “and”, “the”, “a” in a language, but there is no single universal list of stopwords. The list of the stop words can change depending on your application. 在存储 stopword 的时候使用 set rather than list 主要原因是 much faster than search operations in a set.You might wonder why we convert our list into a set. Set is an abstract data type that can store unique values, without any particular order. The search operation in a set is much faster than the search operation in a list. For a small number of words, there is no big difference, but if you have a large number of words it’s highly recommended to use the set type. Regex A kind of search pattern. A regular expression, regex, or regexp is a sequence of characters that define a search pattern. Let’s see some basics. 12345678910. - match any character except newline\w - match word\d - match digit\s - match whitespace\W - match not word\D - match not digit\S - match not whitespace[abc] - match any of a, b, or c[^abc] - not match a, b, or c[a-g] - match a character between a &amp; g 这个解释说明了为什么在正则表达式 中使用 r”” 作为一种前缀。因为正则表达是中 ”\“ 的使用和 python 中的”\” 使用有冲突。简而言之，如果加上了 r”” 那么这个就是一种完全的 正则表达式的语法了。 Regular expressions use the backslash character (‘\’) to indicate special forms or to allow special characters to be used without invoking their special meaning. This collides with Python’s usage of the same character for the same purpose in string literals; for example, to match a literal backslash, one might have to write ‘\\‘ as the pattern string, because the regular expression must be \, and each backslash must be expressed as \ inside a regular Python string literal.The solution is to use Python’s raw string notation for regular expression patterns; backslashes are not handled in any special way in a string literal prefixed with ‘r’. So r”\n” is a two-character string containing ‘\’ and ‘n’, while “\n” is a one-character string containing a newline. Usually, patterns will be expressed in Python code using this raw string notation. An example, 1234import resentence = "The development of snowboarding was inspired by skateboarding, sledding, surfing and skiing."pattern = r"[^\w]"print(re.sub(pattern, " ", sentence)) Bag of words Machine learning algorithms cannot work with raw text directly, we need to convert the text into vectors of numbers. This is called feature extraction.The bag-of-words model is a popular and simple feature extraction technique used when we work with text. It describes the occurrence of each word within a document. 这个是 bag of words的”特点“： order or structure of words 没有体现出来。Any information about the order or structure of words is discarded. That’s why it’s called a bag of words. This model is trying to understand whether a known word occurs in a document, but don’t know where is that word in the document. The intuition is that similar documents have similar contents. Also, from a content, we can learn something about the meaning of the document. To use this model, we need to: Design a vocabulary of known words (also called tokens) Choose a measure of the presence of known words 1) 最简单的方式是 “occurrence” ，如果出现了 标为1 否则标为0；这种是最为简单的 bag of words 最的方式，这四个是一一对应的。注意体会。 The complexity of the bag-of-words model comes in deciding how to design the vocabulary of known words (tokens) and how to score the presence of known words. bag of words 中使用 “occurrence” 的方式的缺点：稀疏矩阵（当dict 很大的时候，文章的 representation中有相当成分的0）。 In some cases, we can have a huge amount of data and in this cases, the length of the vector that represents a document might be thousands or millions of elements. Furthermore, each document may contain only a few of the known words in the vocabulary.Therefore the vector representations will have a lot of zeros. These vectors which have a lot of zeros are called sparse vectors. They require more memory and computational resources.We can decrease the number of the known words when using a bag-of-words model to decrease the required memory and computational resources. We can use the text cleaning techniques we’ve already seen in this article before we create our bag-of-words model: 减少 dictionary size 的方式。 Ignoring punctuationRemoving the stop words from our documentsReducing the words to their base form (Text Lemmatization and Stemming)Fixing misspelled words n-gram 的思想是很广泛：通过 sequence of words，这个是可以增加文本的表达力的。An n-gram is a sequence of a number of items (words, letter, numbers, digits, etc.). In the context of text corpora, n-grams typically refer to a sequence of words. A unigram is one word, a bigram is a sequence of two words, a trigram is a sequence of three words etc. 关于如何去 score the presence of word： 这里是有三种方式的。We saw one very simple approach - the binary approach (1 for presence, 0 for absence).Some additional scoring methods are:2) Counts. Count the number of times each word appears in a document.3) Frequencies. Calculate the frequency that each word appears in document out of all the words in the document. TF-IDF 这个语境 是相对于 frequency 而言的，关键词是不一定有 频率所决定，而一些 rarer or domain-specific words 可能是更加常见的。One problem with scoring word frequency is that the most frequent words in the document start to have the highest scores. These frequent words may not contain as much “informational gain” to the model compared with some rarer and domain-specific words. One approach to fix that problem is to penalize words that are frequent across all the documents. This approach is called TF-IDF. TF-IDF 的关键在于体现了“语料库”。TF-IDF, short for term frequency-inverse document frequency is a statistical measure used to evaluate the importance of a word to a document in a collection or corpus. 参考资料https://towardsdatascience.com/introduction-to-natural-language-processing-for-text-df845750fb63]]></content>
      <categories>
        <category>NLP</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[剑指offer-其他]]></title>
    <url>%2F2019%2F05%2F17%2F%E5%89%91%E6%8C%87offer-%E5%85%B6%E4%BB%96%2F</url>
    <content type="text"><![CDATA[这是剑指offer 系列四部曲中的最后一部，因为有些算法题目类别数量太少就汇总到了”其他“, 比如位运算、正则匹配等。第一部关于字符串和数组，第二部是栈、队列、链表和树， 第三部递归、回溯和动态规划。 二进制中1的个数 输入一个整数，输出该数二进制表示中1的个数。其中负数用补码表示。 Tips：首先处理正数, bitwise and operation， 很简单。对于负数，需要转换成 正数然后进行处理，math。 123456789101112131415161718192021class Solution: def NumberOf1(self, n): # write code here if n == 0: return 0 if n &gt; 0: counts = self.number_of_positive(n) else: n = abs(n) - 1 counts = 32 - self.number_of_positive(n) return counts def number_of_positive(self, n): if n == 0: return 0 counts = 0 while n: counts += (n &amp; 1) n = n &gt;&gt; 1 return counts 数值的整数次方 给定一个double类型的浮点数base和int类型的整数exponent。求base的exponent次方。 Tips: 次方使用乘法来进行累乘 1234567891011121314151617181920212223242526272829303132333435class Solution: """ 这个就是边界条件比较多而已，需要分别判断 base 和 exponent 的正负 """ def Power(self, base, exponent): # write code here if base == 0 and exponent != 0: return 0 if base != 0 and exponent == 0: return 1 flag = 1 if base &lt;= 0 and (exponent % 2 == 1): flag = -1 base = abs(base) result = 1 if exponent &gt; 0: reverse = 0 else: reverse = 1 exponent = abs(exponent) if exponent % 2 == 0: result = base * base for i in range(exponent // 2 - 1): result = result * result else: result = base * base for i in range(exponent // 2 - 1): result = result * result result = result * base if reverse: result = 1.0 / result return result * flag 最小的K个数 输入n个整数，找出其中最小的K个数。例如输入4,5,1,6,2,7,3,8这8个数字，则最小的4个数字是1,2,3,4,。 Tips： 这个有点投机取巧了，使用了 “heapq” 的库函数。这个题目跟 第 K个 smallest 是有差别的，快排中的 partition 是找到了 一个数字在最后排序结果中的位置。对于有”累加“前 K个数字还是要使用常规的排序。比较好的就是堆排序。 123456789class Solution: # 想说的是既然是使用这种开源的库函数 那么就记住这种函数名字 def GetLeastNumbers_Solution(self, tinput, k): # write code here if len(tinput) &lt; k: return [] import heapq res = heapq.nsmallest(k, tinput) return res The function partition puts the numbers smaller than nums[left] to its left and then returns the new index of nums[left]. The returned index is actually telling us how small nums[left] ranks in nums. 123456789101112131415161718192021222324252627282930313233343536class Solution(object): def findKthLargest(self, nums, k): """ :type nums: List[int] :type k: int :rtype: int """ left, right = 0, len(nums) - 1 while True: pos = self.partition(nums, left, right) # 这个在排序的时候，是把大的数字放到前面，而前面是pos 是从0 开始的， # 所以这里是 k-1 if pos == k - 1: return nums[pos] # 左边的并不足以构成k 个， 那么在右边 elif pos &lt; k - 1: left = pos + 1 else: right = pos - 1 def partition(self, nums, left, right): # choose nums[left] as pivot pivot = nums[left] # p1, p2就类似 working 中的left right p1, p2 = left + 1, right while p1 &lt;= p2: if nums[p1] &lt; pivot and nums[p2] &gt; pivot: nums[p1], nums[p2] = nums[p2], nums[p1] p1, p2 = p1 + 1, p2 - 1 elif nums[p1] &gt;= pivot: p1 += 1 else: #nums[p2] &lt;= pivot: p2 -=1 nums[left], nums[p2] = nums[p2], nums[left] return p2 整数中1出现的次数（从1到n整数中1出现的次数） 求出1~13的整数中1出现的次数,并算出100~1300的整数中1出现的次数？为此他特别数了一下1~13中包含1的数字有1、10、11、12、13因此共出现6次,但是对于后面问题他就没辙了。ACMer希望你们帮帮他,并把问题更加普遍化,可以很快的求出任意非负整数区间中1出现的次数（从1 到 n 中1出现的次数）。 Tips：math, 计数原理，按位统计该位为1时可能包含的数字总数.由低位向高位依次遍历数字n的每一位curn。记当前位数为c，curn左边（高位）的数字片段为highn，cur右边（低位）的数字片段为lown，lowc = 10 ^ c 若curn = 0，则高位范围为0 ~ highn - 1，低位0 ~ lowc - 1 若curn = 1，则高位范围为0 ~ highn - 1，低位0 ~ lowc - 1；或者 高位为highn， 低位0 ~ lown 若curn ＞ 1，则高位范围为0 ~ highn， 低位为0 ~ lowc - 1 一个小的例子 N= abcde,分别是各个位数上的数字。如果要统计 百位上出现 1的次数，那么他将受到三个因素的影响： 百位上的数字，百位以下的数字(低位) 和百位以上的数字( 高位)。 如果百位上的数字是 0 ， 高位数字 * 当前的数字 如果百位上的数字是1，高位数字 * 当前数字 + ( 低位数字+1) 如果百位数字大于1 (2-9), （高位数字+1 )* 当前的数字 初级版本: 通过不断的求余 和整除，计算每个位置上 1的个数。12345678910111213141516171819int Count1InInteger(int n)&#123; int counts =0; while (n!=0) &#123; counts += (n%10) ==1? 1:0; n/=10; &#125; return counts;&#125;int main()&#123; int total =0; for (int i=0; i&lt;N; i++) &#123; total += Count1InInteger(i) &#125; return total;&#125; 进阶版本：python 版本，思路见上面分析。1234567891011121314151617181920212223242526272829303132333435363738class Solution: # 数字的基本结构分成 weights +working_num + n%base 这三个部分 # 然后一个while 循环是处理一个数字 def NumberOf1Between1AndN_Solution(self, n): # write code here if n &lt; 1: return 0 num = n counts = 0 base = 1 while num: cur = num % 10 num = num // 10 #高位数字 counts += base * num if cur == 1: counts += (n % base) + 1 elif cur &gt; 1: counts += base base *= 10 return counts ``` - 把数组排成最小的数&gt; 输入一个正整数数组，把数组里所有数字拼接起来排成一个数，打印能拼接出的所有数字中最小的一个。例如输入数组&#123;3，32，321&#125;，则打印出这三个数字能排成的最小数字为321323。Tips：使用sorted() 函数， string 类型的排序 和 int 类型的排序是一样的，在python 里面来说。```pythonclass Solution: def PrintMinNumber(self, numbers): # write code here sorted_list = sorted(numbers, cmp=lambda a, b: cmp(str(a) + str(b), str(b) + str(a))) # 这个时候已经排好序，然后只要一个个连接起来就行了 return ''.join(map(str, sorted_list)) 丑数 把只包含质因子2、3和5的数称作丑数（Ugly Number）。例如6、8都是丑数，但14不是，因为它包含质因子7。 习惯上我们把1当做是第一个丑数。求按从小到大的顺序的第N个丑数。 Tips：之后的丑数肯定是2，3或5 的倍数，分别单独计数，然后选择最小的。 123456789101112131415161718192021class Solution: # 在进行 append 操作的时候去重， def GetUglyNumber_Solution(self, index): # write code here if index &lt;1: return 0 list1 = [1] # 意味着只能是 append() 操作了 i, j, k = 0, 0, 0 while len(list1) &lt; index: num = min(list1[i] * 2, list1[j] * 3, list1[k] * 5) if num &gt; list1[-1]: list1.append(num) if num == list1[i] * 2: i += 1 elif num == list1[j] * 3: j += 1 else: k += 1 return list1[-1] 正则表达式匹配 请实现一个函数用来匹配包括’.’和’‘的正则表达式。模式中的字符’.’表示任意一个字符，而’‘表示它前面的字符可以出现任意次（包含0次）。 在本题中，匹配是指字符串的所有字符匹配整个模式。例如，字符串”aaa”与模式”a.a”和”abaca”匹配，但是与”aa.a”和”ab*a”均不匹配 Tips: dp 问题。转换方程 dp[i][j] i 表示 string 的index j表示 pattern 的index， dp[i][j] ==dp[i-1][j-1] or dp[i][j] =dp[i][j-2] or dp[i][j-1] 。 123456789101112131415161718192021222324class Solution: # s, pattern都是字符串 # https://www.youtube.com/watch?v=l3hda49XcDE 心中一定要有这个表格, a[i][j] 这个更像是一种指针 def match(self, s, pattern): if len(s) == 0 and len(pattern) == 0: return True dp = [[False for _ in range(len(pattern) + 1)] for _ in range(len(s) + 1)] dp[0][0] = True for j in range(1, len(pattern) + 1): if pattern[j - 1] == "*": dp[0][j] = dp[0][j - 2] for i in range(1, len(s) + 1): for j in range(1, len(pattern) + 1): if pattern[j - 1] == s[i - 1] or pattern[j - 1] == ".": dp[i][j] = dp[i - 1][j - 1] elif pattern[j - 1] == "*": dp[i][j] = dp[i][j - 2] if s[i - 1] == pattern[j - 2] or pattern[j - 2] == ".": dp[i][j] = dp[i][j] or dp[i - 1][j] else: dp[i][j] = False return dp[len(s)][len(pattern)] 数据流中的中位数 如何得到一个数据流中的中位数？如果从数据流中读出奇数个数值，那么中位数就是所有数值排序之后位于中间的数值。如果从数据流中读出偶数个数值，那么中位数就是所有数值排序之后中间两个数的平均值。我们使用Insert()方法读取数据流，使用GetMedian()方法获取当前读取数据的中位数。 Tips：主要是体现了数据流，要求能够 insert 元素，然后基于当前的状态去 getmedian() ，是动态的，而不是静态的。 1234567891011121314151617181920class Solution: """ 对于数据流 这个应该是第二次接触了，需要使用一个全局变量 """ # 虽然知道这个使用 堆的思想是更优的，搜索时间可以O（1）， 堆的调整是 O(log n) # 但是没有什么很好的教程，所以我也没有学会啊 def __init__(self): self.list1 = [] def Insert(self, num): self.list1.append(num) def GetMedian(self, ch): length = len(self.list1) # 我记得有一个更加快一些 self.list1 = sorted(self.list1) if length % 2 == 0: return (self.list1[length // 2] + self.list1[length // 2 - 1]) / 2.0 else: return self.list1[length // 2] 滑动窗口的最大值 给定一个数组和滑动窗口的大小，找出所有滑动窗口里数值的最大值。例如，如果输入数组{2,3,4,2,6,2,5,1}及滑动窗口的大小3，那么一共存在6个滑动窗口，他们的最大值分别为{4,4,6,6,6,5}； 针对数组{2,3,4,2,6,2,5,1}的滑动窗口有以下6个： {[2,3,4],2,6,2,5,1}， {2,[3,4,2],6,2,5,1}， {2,3,[4,2,6],2,5,1}， {2,3,4,[2,6,2],5,1}， {2,3,4,2,[6,2,5],1}， {2,3,4,2,6,[2,5,1]}。 Tips：使用max(list1) 这样的操作是可行的。 123456789class Solution: # 最简单的模拟滑动窗口 的过程 def maxInWindows(self, num, size): slip = [] if not num or len(num) &lt; size or size == 0: return [] for i in range(len(num) - size + 1): slip.append(max(num[i:i + size])) return slip]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>剑指offer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[剑指offer-递归、回溯和动态规划]]></title>
    <url>%2F2019%2F05%2F17%2F%E5%89%91%E6%8C%87offer-%E9%80%92%E5%BD%92-%E5%9B%9E%E6%BA%AF%E5%92%8C%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%2F</url>
    <content type="text"><![CDATA[这是剑指offer 系列四部曲中的第三部：递归、回溯和动态规划。第一部关于字符串和数组，第二部是栈、队列、链表和树， 最后一部分在这里。 斐波那契数列 大家都知道斐波那契数列，现在要求输入一个整数n，请你输出斐波那契数列的第n项（从0开始，第0项为0）。n&lt;=39 Tips: 简单的递归，可以转换成循环。 1234567891011121314class Solution: # python 中list 的初始化，最开始的是从0 开始，所以是需要多进行一个初始化的 def Fibonacci(self, n): # write code here if n == 0: return 0 if n == 1: return 1 arr = [0] * (n + 1) arr[0] = 0 arr[1] = 1 for i in range(2, n + 1): arr[i] = arr[i - 1] + arr[i - 2] return arr[n] 这个是可以进一步优化到时间 O(N)，空间是 O(1)12345678910111213141516171819202122232425262728293031#include&lt;iostream&gt;#include&lt;algorithm&gt;using namespace std;//0, 1, 1, 2, 3, 5,// 第0项是0，第1项是1int fiber(int n)&#123; if (n ==1) return 1; else if(n ==2) return 1; int a =1, b =1; for(int i =3; i&lt;=n; i++) &#123; a =a+b; b =a; &#125; return b;&#125;int main()&#123; int n; cin &gt;&gt;n; cout&lt;&lt; fiber(n)&lt;&lt;endl; return 0;&#125; 跳台阶 一只青蛙一次可以跳上1级台阶，也可以跳上2级。求该青蛙跳上一个n级的台阶总共有多少种跳法（先后次序不同算不同的结果）。 Tips： 同上。 python 版本123456789101112131415class Solution: def jumpFloor(self, number): # write code here if number == 1: return 1 if number == 2: return 2 arr = [0] * (number + 1) arr[1] = 1 arr[2] = 2 for i in range(3, number + 1): arr[i] = arr[i - 1] + arr[i - 2] return arr[number] c++ 版本12345678910111213141516171819202122232425262728293031#include&lt;iostream&gt;#include&lt;algorithm&gt;#include "vector"using namespace std;//0, 1, 1, 2, 3, 5,// 第0项是0，第1项是1vector&lt;int&gt; f;int jump(int n)&#123; if(n ==1) return 1; if (n ==2) return 2; f[1] =1,f[2] =2; for(int i =3; i&lt;=n;i++) f[i] =f[i-1] +f[i-2]; return f[n]; &#125;int main()&#123; int n; cin &gt;&gt;n; f =vector&lt;int&gt;(n+1); cout&lt;&lt;jump(n)&lt;&lt;endl; return 0;&#125; 跳台阶2 一只青蛙一次可以跳上1级台阶，也可以跳上2级……它也可以跳上n级。求该青蛙跳上一个n级的台阶总共有多少种跳法。 Tips：同上。 12345678910111213class Solution: """ 在使用 for循环的时候，注意 range() 这种取值，究竟是使用 range() 作为次数的计量； 还是要使用range 中的index 。两者是不相同的操作，尤其是对于前后的取值。 """ def jumpFloorII(self, number): # write code here if number == 1: return 1 nums = 1 for i in range(number - 1): nums = nums * 2 return nums 矩形覆盖 我们可以用2*1的小矩形横着或者竖着去覆盖更大的矩形。请问用n个2*1的小矩形无重叠地覆盖一个2*n的大矩形，总共有多少种方法？ Tips: math, 找出递归方程。 1234567891011121314151617"""既然结果只是最后一个解，那么就没有必要保存中间变量，所以只是，所以空间复杂度从O（n） -&gt; O(1) ，这个是超级nice的"""class Solution: def rectCover(self, number): # write code here if number &lt;= 0: return 0 if number &lt;= 2: return number # 只是两个变量罢了 a, b = 1, 2 while number &gt; 2: a, b = b, a + b # python 中独有的比较神奇的操作， 是计算了右边的 b, a+b 然后再统一赋值给 a, b number -= 1 return b 注意在c++ 中是需要使用三个变量的。这样才能一步步的进行转移。 123456789101112131415161718192021class Solution &#123;public: int rectCover(int number) &#123; if(number &lt;=0) return 0; int&amp; n =number; if (n ==1) return 1; if (n ==2) return 2; int a =1, b =2, c; for(int i =3; i&lt;=n; i++) &#123; c =a+b; a =b; b=c; &#125; return c; &#125;&#125;; 机器人的运动范围 (c++ 中一秒能够遍历 $10^8$，遍历一亿个数字) 地上有一个m行和n列的方格。一个机器人从坐标0,0的格子开始移动，每一次只能向左，右，上，下四个方向移动一格，但是不能进入行坐标和列坐标的数位之和大于k的格子。 例如，当k为18时，机器人能够进入方格（35,37），因为3+5+3+7 = 18。但是，它不能进入方格（35,38），因为3+5+3+8 = 19。请问该机器人能够达到多少个格子？ Tips：递归，转移方程不难，在上下左右四个方向进行尝试，需要判断的条件比较多，比如是否访问过，数位之和等一些条件。 123456789101112131415161718192021222324252627282930class Solution: def movingCount(self, threshold, rows, cols): # visited 不一定是二维的，只要是能够"自圆其说" 就行。 visited = [False] * (rows * cols) count = self.movingCountCore(threshold, rows, cols, 0, 0, visited) return count def movingCountCore(self, threshold, rows, cols, row, col, visited): count = 0 # 就是这个访问记录是需要进行变化的， 如果是false ，然后访问之后 是需要设置为 true的 if self.check(threshold, rows, cols, row, col, visited): visited[row * cols + col] = True count = 1 + self.movingCountCore(threshold, rows, cols, row, col - 1, visited) + \ self.movingCountCore(threshold, rows, cols, row, col + 1, visited) + \ self.movingCountCore(threshold, rows, cols, row + 1, col, visited) + \ self.movingCountCore(threshold, rows, cols, row - 1, col, visited) return count def check(self, threshold, rows, cols, row, col, visited): if row &gt;= 0 and row &lt; rows and col &gt;= 0 and col &lt; cols and self.judge(threshold, row, col) and not visited[row * cols + col]: return True else: return False def judge(self, threshold, i, j): if sum(map(int, str(i) + str(j))) &lt;= threshold: return True else: return False 牛客版本 https://www.acwing.com/activity/content/code/content/18362/ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152class Solution &#123;public: // 宽度优先搜索， 使用队列 int get_single_sum(int x) &#123; int s =0; while (x) &#123; s += x%10; x =x/10; &#125; return s; &#125; int get_sum(pair&lt;int, int&gt; p) &#123; return get_single_sum(p.first) +get_single_sum(p.second); &#125; int movingCount(int threshold, int rows, int cols) &#123; int res =0; if(!rows || !cols) return 0; vector&lt;vector&lt;bool&gt;&gt; st(rows, vector&lt;bool&gt;(cols)); int dx[4] =&#123;-1, 0, 1, 0&#125;, dy[4]=&#123;0, 1, 0, -1&#125;; queue&lt;pair&lt;int, int&gt;&gt; q; q.push(&#123;0, 0&#125;); while (q.size()) &#123; auto t =q.front(); q.pop(); if(get_sum(t)&gt; threshold || st[t.first][t.second]) continue; res ++; st[t.first][t.second] =true; for(int i =0; i&lt;4; i++) &#123; int a= t.first+ dx[i], b =t.second+ dy[i]; if(a &gt;=0 &amp;&amp; a&lt;rows &amp;&amp; b&gt;=0 &amp;&amp; b&lt;cols) q.push(&#123;a, b&#125;); &#125; &#125; return res; &#125;&#125;; 矩阵中的路径 请设计一个函数，用来判断在一个矩阵中是否存在一条包含某字符串所有字符的路径。路径可以从矩阵中的任意一个格子开始，每一步可以在矩阵中向左，向右，向上，向下移动一个格子。如果一条路径经过了矩阵中的某一个格子，则之后不能再次进入这个格子。 例如 a b c e s f c s a d e e 这样的3 X 4 矩阵中包含一条字符串”bcced”的路径，但是矩阵中不包含”abcb”路径，因为字符串的第一个字符b占据了矩阵中的第一行第二个格子之后，路径不能再次进入该格子。 Tips： 在二维数组中每个点上都进行尝试，每个点上同样是上下左右进行尝试，返回符合条件的。 1234567891011121314151617181920212223242526class Solution: # 递归 这个是 true or false 判断类型的。 # 思路：先是 rows* cols 这样的全部遍历 def hasPath(self, matrix, rows, cols, path): # 如果使用 [ for _in range(rows) ] for _ in range(cols) ， 这个是有结构的 rows* cols assist = [True] * rows * cols for i in range(rows): for j in range(cols): if self.rightPath(matrix, rows, cols, i, j, path, assist): return True return False def rightPath(self, matrix, rows, cols, i, j, path, assist): if not path: return True index = i * cols + j if i &lt; 0 or i &gt;= rows or j &lt; 0 or j &gt;= cols or matrix[index] != path[0] or assist[index] == False: return False assist[index] = False if (self.rightPath(matrix, rows, cols, i + 1, j, path[1:], assist) or self.rightPath(matrix, rows, cols, i - 1, j, path[1:], assist) or self.rightPath(matrix, rows, cols, i, j - 1, path[1:], assist) or self.rightPath(matrix, rows, cols, i, j + 1, path[1:], assist)): return True assist[index] = True return False]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>剑指offer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[剑指Offer-栈、队列、链表和树]]></title>
    <url>%2F2019%2F05%2F17%2F%E5%89%91%E6%8C%87Offer-%E6%A0%88-%E9%98%9F%E5%88%97-%E9%93%BE%E8%A1%A8%E5%92%8C%E6%A0%91%2F</url>
    <content type="text"><![CDATA[这是剑指offer 系列四部曲中的第二部：栈、队列、链表和树。第一部关于字符串和数组，第三部是递归、回溯和动态规划， 最后一部分在这里。 从尾到头打印链表 输入一个链表，按链表值从尾到头的顺序返回一个ArrayList。 正向遍历之后，使用的是python 中list的特性， list[::-1] 这样进行输出的。 123456789101112131415161718# -*- coding:utf-8 -*-# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: # 返回从尾部到头部的列表值序列，例如[1,2,3] # += , -= 这个都是同一种类型的 def printListFromTailToHead(self, listNode): # write code here arraylist =[] head = listNode while head != None: arraylist += [head.val] # 这个在这里等效于 arraylist.append(head.val) head = head.next return arraylist[::-1] 重建二叉树 输入某二叉树的前序遍历和中序遍历的结果，请重建出该二叉树。假设输入的前序遍历和中序遍历的结果中都不含重复的数字。例如输入前序遍历序列{1,2,4,7,3,5,6,8}和中序遍历序列{4,7,2,1,5,3,8,6}，则重建二叉树并返回。 Tips: 递归，二叉树的题目大多数都是可以使用递归的思想进行解决，因为二叉树本身结构就是递归定义的。递归优点在于代码量比较少。从先序遍历中找出根节点，从中序遍历中找出左右子树。 别怕，手写中如何重建二叉树，在代码中就是如何实现重建二叉树的。首先从前序list 中找到头结点，然后从中序队列中找见对应节点的index，那么前面的就是头结点的左子树，后面的就是右子树。 12345678910111213141516171819202122# -*- coding:utf-8 -*-# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: # 返回构造的TreeNode根节点 # 需要理解在前序遍历中是先遍历左子树的，并且中序和前序中左子树的个数是不会变的 def reConstructBinaryTree(self, pre, tin): # write code here if len(pre) == 0: return None root = TreeNode(pre[0]) # 这个index 函数是需要记住的 index = tin.index(pre[0]) # 这里也是需要修改的 # pre 和 tin都是需要空出一个 root.value 的位置，只不过选择空的位置是不一样的 root.left = self.reConstructBinaryTree(pre[1:index + 1], tin[:index]) root.right = self.reConstructBinaryTree(pre[index + 1:], tin[index + 1:]) return root 用两个栈实现队列 用两个栈来实现一个队列，完成队列的Push和Pop操作。 队列中的元素为int类型。 Tips： 在python 中栈等同于使用list 实现。使用两个栈，意味着一个是push_stack 一个是pop_stack，使用两个栈的“后进先出”表示队列的先进先出（push and pop）从语法上讲 ，if list1 ==[], 那么 list1 ==None, 这两个条件是可以交换判断的。（在list 中） 12345678910111213141516171819class Solution: def __init__(self): self.list1 =[] self.list2 =[] def push(self, node): # write code here self.list1.append(node) def pop(self): # return if not self.list1 and not self.list2 : return None if self.list2 : return self.list2.pop() else: while self.list1: self.list2.append(self.list1.pop()) return self.list2.pop() 链表中倒数第k个结点 输入一个链表，输出该链表中倒数第k个结点。 Tips： 两种解法。一种是遍历存储到list 中，空间复杂度是O(N), 另外一种是两个指针p1，p2，距离相差k，当p2 到达链表尾部，p1 就在导数第k 个位置。 1234567891011121314151617181920212223242526272829303132# -*- coding:utf-8 -*-# class ListNode:# def __init__(self, x):# self.val = x# self.next = None"""尝试使用两个指针版本p1 p2 并且这种 length 在命名上是需要规范的, 并且这种指针操作，最好是拷贝出来进行操作不管怎么说，还是应该求解出来 length of listNode，这种才是正途可以使用两个指针，"""class Solution: def FindKthToTail(self, head, k): # write code here if head == None or k &lt;= 0: return None p1 = head p2 = head len1 = 0 while p1: len1 += 1 p1 = p1.next if k &gt; len1: return None p1 = head while k: p1 = p1.next k -= 1 while p1: p1 = p1.next p2 = p2.next return p2 反转链表 输入一个链表，反转链表后，输出新链表的表头。 Tips： 需要三个指针，cur，next_node, pre。 简单的链表的修改，最后新的表头就是一开始的尾节点。 1234567891011121314151617181920212223242526# -*- coding:utf-8 -*-# class ListNode:# def __init__(self, x):# self.val = x# self.next = None"""修改链表是需要三个指针的 pre, cur, next_node 如果对三个指针名进行命名好了，那么这个就是成功的一般了， 这个不容易想到的是设置pre =None ，这个是一个细节经验性的问题"""class Solution: # 返回ListNode def ReverseList(self, pHead): # write code here if pHead ==None: return None pre =None cur =pHead while cur: next_node =cur.next cur.next =pre pre, cur =cur, next_node return pre 合并两个排序的链表 Tips： 归并排序中的“并” 操作，只不过由原来的list 操作到现在的 linkedlist 操作。 输入两个单调递增的链表，输出两个链表合成后的链表，当然我们需要合成后的链表满足单调不减规则。 123456789101112131415161718192021222324252627282930313233# def __init__(self, x):# self.val = x# self.next = None"""就是在使用两个或者多个 index (p1 or p2) 遍历的时候，一个常见的错误就是忘记了不断更新index"""class Solution: # 返回合并后列表 def Merge(self, pHead1, pHead2): # write code here if pHead1 == None: return pHead2 if pHead2 == None: return pHead1 head = ListNode(-1) head1 = head p1 = pHead1 p2 = pHead2 while p1 and p2: if p1.val &lt; p2.val: head.next = p1 p1 = p1.next else: head.next = p2 p2 = p2.next head = head.next if p1 == None: head.next = p2 if p2 == None: head.next = p1 return head1.next 树的子结构 输入两棵二叉树A，B，判断B是不是A的子结构。（ps：我们约定空树不是任意一个树的子结构） 递归定义，根节点是否相同，左右子树是否相同。 12345678910111213141516171819202122232425262728293031323334# -*- coding:utf-8 -*-# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = None"""分成两部：首先寻找两个根节点的值是否相同；然后判断子树是否完全相同subTree 这个函数就是判断子树是否完全相同的，所以函数的功能一定要搞好"""class Solution: def HasSubtree(self, pRoot1, pRoot2): if not pRoot1: return False if not pRoot2: return False result =False if pRoot1.val ==pRoot2.val: result =self.subTree(pRoot1, pRoot2) if result ==False: result = self.HasSubtree(pRoot1.left, pRoot2) or self.HasSubtree(pRoot1.right, pRoot2) return result def subTree(self, root1, root2): if not root2: return True if not root1: return False if root1.val ==root2.val: return self.subTree(root1.left, root2.left) and self.subTree(root1.right, root2.right) return False 二叉树的镜像 操作给定的二叉树，将其变换为源二叉树的镜像。 Tips：求解二叉树镜像，A 的左子树对应着B 的右子树。 123456789101112131415161718192021# -*- coding:utf-8 -*-# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = None"""就是在某个左（右）子树是None 的情况下，这个也是可以进行交换的，结束的标志应该是根节点是否为空"""class Solution: # 返回镜像树的根节点 def Mirror(self, root): # write code here if not root: return None root.left , root.right =root.right, root.left if root.left: self.Mirror(root.left) if root.right: self.Mirror(root.right) return root 包含min函数的栈 定义栈的数据结构，请在该类型中实现一个能够得到栈中所含最小元素的min函数（时间复杂度应为O（1））。 Tips: 这个跟“使用两个栈表示队列” 是差不多的，就是单独使用一个list 存储min 函数调用的一个列表，这样的话能达到时间复杂度是 O(1). 在原来的基础上，stack 的基础上，使用新的 min_stack 满足这个需求，同样，原来的 push pop 这种操作还是不能少的。所以是维护了两个 list（normal_list, min_list），但是当normal_list pop() 出来的时候，这个min_list 和其pop 出来的不一定是相同的值。（我感觉） 1234567891011121314151617181920212223242526272829303132# -*- coding:utf-8 -*-"""这个栈中最小的元素是变化的，好好理解一下，如果弹出了一个比较大的元素，那么栈中最小的元素是不变的所含元素的最小元素top() and min() 操作是不需要删除元素的， pop 是删除了元素"""class Solution: def __init__(self): self.all_list = [] self.min_list = [] def push(self, node): # write code here if not self.min_list: self.min_list.append(node) else: self.min_list.append(min(node, self.min())) # 好多思想都是基于之前的结果进行求解 self.all_list.append(node) def pop(self): self.all_list.pop() self.min_list.pop() # write code here def top(self): return self.all_list[-1] # write code here def min(self): return self.min_list[-1] 栈的压入、弹出序列 输入两个整数序列，第一个序列表示栈的压入顺序，请判断第二个序列是否可能为该栈的弹出顺序。假设压入栈的所有数字均不相等。例如序列1,2,3,4,5是某栈的压入顺序，序列4,5,3,2,1是该压栈序列对应的一个弹出序列，但4,3,5,1,2就不可能是该压栈序列的弹出序列。（注意：这两个序列的长度是相等的） Tips: 使用一个list 来模拟压入和弹出过程，遍历弹出序列popV，如果结束，那么return True。 12345678910111213def IsPopOrder( pushV, popV): if not pushV: return False tmp =[] while popV: if tmp and popV[0] == tmp[-1]: popV.pop(0) tmp.pop() elif pushV: tmp.append(pushV.pop(0)) else: return False return True 从上往下打印二叉树 从上往下打印出二叉树的每个节点，同层节点从左至右打印。 Tips： 层次遍历，遍历根节点之后加入左右结点。 123456789101112131415161718192021222324# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: # 返回从上到下每个节点值列表，例：[1,2,3] # 层序遍历二叉树， 这个跟数据结构 队列有类似的 # nodes 装上结点，然后vlaues 装上数值 def PrintFromTopToBottom(self, root): # write code here if not root: return [] nodes =[] values = [] nodes.append(root) while nodes: node = nodes.pop(0) values.append(node.val) if node.left: nodes.append(node.left) if node.right: nodes.append(node.right) return values 二叉搜索树的后序遍历序列 输入一个整数数组，判断该数组是不是某二叉搜索树的后序遍历的结果。如果是则输出Yes,否则输出No。假设输入的数组的任意两个数字都互不相同。 Tips：二叉搜索树，按照中序遍历的话，就是一个排序的二叉树，根节点大于左子树，右子树大于根节点。后序遍历序列中最后一个是根节点，小于根节点是左子树，大于根节点的是右子树，这样进行判断。 123456789101112131415class Solution: # 后序遍历结果， 最后一个是根节点，这个是递归的思想 # 二叉搜索树， 左子树小于根节点，右子树大于根节点 def VerifySquenceOfBST(self, sequence): # write code here if not sequence: return False root = sequence[-1] for i in range(len(sequence)): if sequence[i] &gt; root: break for j in range(i, len(sequence)): if sequence[j] &lt; root: return False return True 二叉树中和为某一值的路径** 输入一颗二叉树的跟节点和一个整数，打印出二叉树中结点值的和为输入整数的所有路径。路径定义为从树的根结点开始往下一直到叶结点所经过的结点形成一条路径。(注意: 在返回值的list中，数组长度大的数组靠前) Tips： 树的遍历，深度优先算法（dfs） 这个是非常典型的 dfs，是值得掌握的。 123456789101112131415161718192021222324252627# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: # 返回二维列表，内部每个列表表示找到的路径 # 深度优先 dfs() 这样的一个算法 def FindPath(self, root, expectNumber): # write code here if not root: return [] self.target = expectNumber paths = [] self.dfs(root, [root.val], paths) return pathsdef dfs(self, root, path, paths): if not root.left and not root.right and sum(path) == self.target: paths.append(path) if root.left: self.dfs(root.left, path + [root.left.val], paths) if root.right: self.dfs(root.right, path + [root.right.val], paths) 复杂链表的复制 输入一个复杂链表（每个节点中有节点值，以及两个指针，一个指向下一个节点，另一个特殊指针指向任意一个节点），返回结果为复制后复杂链表的head。（注意，输出结果中请不要返回参数中的节点引用，否则判题程序会直接返回空） Tips: 先是在原来的链表上进行了相同结点的copy和next 指针的指向，然后是random 指针的指向，最后是将原始链表和copy 的链表进行分离。 思路，是先复制节点和 next 指针，然后再遍历一遍复制 random 指针。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051# -*- coding:utf-8 -*-# class RandomListNode:# def __init__(self, x):# self.label = x# self.next = None# self.random = Noneclass Solution: # 返回 RandomListNode # 首先是结点的复制和 next 指针的连接， 然后是random 指针的连接，最后是选择出复制的结点def Clone(self, pHead): # write code here if not pHead: return None self.clone_nodes(pHead) self.connect_nodes(pHead) return self.select_nodes(pHead)def clone_nodes(self, head): if not head: return None while head: cloned = RandomListNode(head.label) cloned.next = head.next head.next = cloned head = cloned.nextdef connect_nodes(self, head): if not head: return None while head: cloned = head.next if head.random: cloned.random = head.random.next head = cloned.nextdef select_nodes(self, head): if not head: return None cloned =cloned_head =None # 这个if 的作用是为了保存一个 cloned_head的结点， # 一定要从这个功能出发 if head: cloned =cloned_head =head.next head.next =cloned.next head =head.next while head: cloned.next =head.next cloned =cloned.next head.next =cloned.next head =head.next return cloned_head 二叉搜索树与双向链表 复习到这里了 输入一棵二叉搜索树，将该二叉搜索树转换成一个排序的双向链表。要求不能创建任何新的结点，只能调整树中结点指针的指向。 Tips：中序遍历二叉搜索树就是一种排序的树的结点，然后树的左右指针可以作为链表中的指向使用。 12345678910111213141516171819202122232425262728293031323334# -*- coding:utf-8 -*-# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: # 使用的树的结构 表示一种双向链表 # 二叉搜索树 ，左子树小于根节点，右子树大于根节点 # 中序遍历得到就是一种排好序的结构 # 只能调整树中结点指针的指向 def Convert(self, pRootOfTree): # write code here if not pRootOfTree: return None tree = pRootOfTree res = [] self.helper(tree, res) for i in range(len(res) - 1): res[i].right = res[i + 1] res[i + 1].left = res[i] # 这个返回值也是比较鬼畜呀， 就是需要这样返回 return res[0]def helper(self, root, res): if not root: return None if root.left: self.helper(root.left, res) res.append(root) if root.right: self.helper(root.right, res) 两个链表的第一个公共结点 输入两个链表，找出它们的第一个公共结点。 Tips：就是一个 m*n 的问题（m，n 分别代表两个链表的长度） 12345678910111213141516171819# -*- coding:utf-8 -*-# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: # 两个指针指向的是 一个结点，一个内存的两个指向 # 将可能不同长度的两个链表转换成相同长度的两个链表的比较，使用 def FindFirstCommonNode(self, pHead1, pHead2): # write code here if not pHead1 or not pHead2: return None p1 = pHead1 p2 = pHead2 while p1 != p2: # 这个p1 只能指向了最后一个结点，但最后一个节点不一定相同 p1 = pHead2 if not p1 else p1.next p2 = pHead1 if not p2 else p2.next return p1 二叉树的深度 输入一棵二叉树，求该树的深度。从根结点到叶结点依次经过的结点（含根、叶结点）形成树的一条路径，最长路径的长度为树的深度。 Tips：递归，相比于二叉树的路径，这个只是返回一个数值就行。 12345678910111213141516171819# -*- coding:utf-8 -*-# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: """ 分别求解 左右子树的深度，然后max(left, right) 这样的操作 """ def TreeDepth(self, pRoot): if not pRoot: return 0 left = self.TreeDepth(pRoot.left) + 1 right = self.TreeDepth(pRoot.right) + 1 # 这个return 是最后执行一次的，然后上面那个都是不断的在进行递归加深 # 这个 left right 已经完成了，最后的效果只是 返回 max(left, right) 这样子 return max(left, right) 平衡二叉树 输入一棵二叉树，判断该二叉树是否是平衡二叉树。 Tips： 左右子树的深度差最大不超过1。两个递归，一个是计算树的深度的递归，一个是判断左右子树是否是平衡二叉树的递归。 123456789101112131415161718192021222324252627# -*- coding:utf-8 -*-# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: # 递归常见的都会有两个return 跳出条件，一个是异常的条件，一个是正确的返回 def get_depth(self, root): if not root: return 0 left =self.get_depth(root.left) right =self.get_depth(root.right) return max(left, right) +1 def IsBalanced_Solution(self, pRoot): if not pRoot: return True left =self.get_depth(pRoot.left) right =self.get_depth(pRoot.right) if abs(left-right) &gt;1: return False return self.IsBalanced_Solution(pRoot.left) and self.IsBalanced_Solution(pRoot.right) 链表中环的入口结点 给一个链表，若其中包含环，请找出该链表的环的入口结点，否则，输出null。 Tips： 两个快慢指针，开指针在环内相遇慢指针。（两个指针一个需要再环外，一个在环内，然后同样的速度走，最后才能相遇）重置快指针到头结点，两个指针相同速度，当再次相遇时候，那就是入口结点。 123456789101112131415161718192021# -*- coding:utf-8 -*-# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: # 现在长个记性吧，在使用next 这样的时候 要先判断这个是不是存在的 def EntryNodeOfLoop(self, pHead): # write code here if not pHead or not pHead.next or not pHead.next.next: return None twoTimes =pHead.next.next oneTime =pHead.next while twoTimes != oneTime: twoTimes =twoTimes.next.next oneTime =oneTime.next twoTimes =pHead while twoTimes != oneTime: twoTimes =twoTimes.next oneTime =oneTime.next return twoTimes 删除链表中重复的结点 在一个排序的链表中，存在重复的结点，请删除该链表中重复的结点，重复的结点不保留，返回链表头指针。 例如，链表1-&gt;2-&gt;3-&gt;3-&gt;4-&gt;4-&gt;5 处理后为 1-&gt;2-&gt;5 12345678910111213141516171819202122232425262728# -*- coding:utf-8 -*-# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: def deleteDuplication(self, pHead): # write code here head = ListNode(-1) head.next = pHead curr = pHead last = head while curr and curr.next: # val =curr.val # 这个条件比较简单，所以可以放到前面 if curr.val != curr.next.val: curr = curr.next last = last.next else: # 这个条件 curr 还是需要注意一下的 val = curr.val # python 中 condition1 and condition2 这种是有先后顺序的 # 可能是存在短路现象的， 如果 curr 不成立，那么后面的是不会执行的 # 草拟 while curr and val == curr.val: curr = curr.next last.next = curr return head.next 二叉树的下一个结点 给定一个二叉树和其中的一个结点，请找出中序遍历顺序的下一个结点并且返回。注意，树中的结点不仅包含左右子结点，同时包含指向父结点的指针。 Tips：中序遍历的下一个结点，如果存在右节点，那么下一个结点是右节点最左边的一个点；如果该结点是其父节点的左结点，那么下一节点是其父节点，否则一直回溯。 123456789101112131415161718192021222324252627282930# -*- coding:utf-8 -*-# class TreeLinkNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = None# self.next = Noneclass Solution: # https://blog.csdn.net/fuxuemingzhu/article/details/79723819 # 这个是求解中序遍历中某个结点的下一个结点 # 这pNode 就是一个普通的结点 def GetNext(self, pNode): # write code here if not pNode: return None # 如果存在右结点 if pNode.right: pNode = pNode.right while pNode.left: pNode = pNode.left return pNode # 如果是父节点的左子树 else: # 这里使用 pNode.next 表示父节点 while pNode.next: if pNode == pNode.next.left: return pNode.next # 这个是右结点 pNode = pNode.next return None 对称的二叉树 请实现一个函数，用来判断一颗二叉树是不是对称的。注意，如果一个二叉树同此二叉树的镜像是同样的，定义其为对称的。 Tips: 判断镜像和递归生成进行还是不太一样的哈。递归判断，根节点相同，然后左右子树是否是对称。 1234567891011121314151617181920212223242526272829# -*- coding:utf-8 -*-# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: # 镜像的概念 和递归 # isSame() 这个就是判断两个子树是否镜像的操作 def isSame(self, p, q): if not p and not q: return True # 好好思考 下面这两个跳出条件为什么是不合适的 if p and q and p.val == q.val: return self.isSame(p.left, q.right) and self.isSame(p.right, q.left) def isSymmetrical(self, pRoot): # write code here # 最开始的条件 如果都是 none 那么这个是对称的 if not pRoot: return True if pRoot.left and not pRoot.right: return False if not pRoot.left and pRoot.right: return False return self.isSame(pRoot.left, pRoot.right) 按之字形顺序打印二叉树 请实现一个函数按照之字形打印二叉树，即第一行按照从左到右的顺序打印，第二层按照从右至左的顺序打印，第三行按照从左到右的顺序打印，其他行以此类推。 Tips：层序遍历的升级版，有两种思路，一种是使用单独 stack (list) 的思想存储偶数层数，一种是先按照原先层序遍历的思想，最后对于偶数的结果进行“翻转” 处理。选择后者，因为代码上比较简单。 1234567891011121314151617181920212223242526272829303132333435363738# -*- coding:utf-8 -*-# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: # 层序遍历 + 偶数翻转 # https://blog.csdn.net/fuxuemingzhu/article/details/79724959 def level(self, root, level, res): """ root: the root of tree level: res: result """ if not root: return if len(res) == level: res.append([]) res[level].append(root.val) if root.left: self.level(root.left, level + 1, res) if root.right: self.level(root.right, level + 1, res) def Print(self, pRoot): # write code here if not pRoot: return [] res = [] self.level(pRoot, 0, res) for level in range(1, len(res), 2): res[level] = res[level][::-1] return res 把二叉树打印成多行 从上到下按层打印二叉树，同一层结点从左至右输出。每一层输出一行。 Tips: 和上一个题目类似，在遍历二叉树的时候，关键是加入了 [level] 层数这种信息。 123456789101112131415161718192021222324252627282930# -*- coding:utf-8 -*-# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: # 返回二维列表[[1,2],[4,5]] def level(self, root, level, res): # 你这里也没有说要返回值的意思呀，这个直接是 return if not root: return if level == len(res): res.append([]) res[level].append(root.val) if root.left: self.level(root.left, level + 1, res) if root.right: # res[level] =self.level(root.right, level+1, res) # 因为这个是 传的值，所以不需要使用返回值的 self.level(root.right, level + 1, res) def Print(self, pRoot): if not pRoot: return [] res = [] self.level(pRoot, 0, res) return res 序列化二叉树 请实现两个函数，分别用来序列化和反序列化二叉树 Tips：序列号和反序列化只是一种约定的存储的形式。 12345678910111213141516171819202122232425262728293031# -*- coding:utf-8 -*-# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: """ 序列化就是从树结构 转成字符串的结构；反之，也是成立的。 使用先序遍历的方法。 https://suixinblog.cn/2019/03/target-offer-serialize-binary-tree.html#%E4%BB%A3%E7%A0%81 """ def __init__(self): self.flag = -1 def Serialize(self, root): # write code here if not root: return "#" return str(root.val) + "," + self.Serialize(root.left) + "," + self.Serialize(root.right) def Deserialize(self, s): # write code here self.flag += 1 string = s.split(',') if self.flag &gt; len(string): return None root = None if string[self.flag] != '#': root = TreeNode(int(string[self.flag])) root.left = self.Deserialize(s) root.right = self.Deserialize(s) return root 二叉搜索树的第k个结点 给定一棵二叉搜索树，请找出其中的第k小的结点。例如， （5，3，7，2，4，6，8） 中，按结点数值大小顺序第三小结点的值为4。 Tips: 二叉搜索树，中序遍历之后有序，然后取第 k 个结点。 12345678910111213141516171819202122232425# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: def middle(self, root, result): if not root: return if root.left: self.middle(root.left, result) result.append(root) if root.right: self.middle(root.right, result) def KthNode(self, pRoot, k): # write code here if not pRoot: return result = [] self.middle(pRoot, result) if len(result) &lt; k or k &lt; 1: return return result[k - 1]]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>剑指offer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[剑指Offer-字符串和数组]]></title>
    <url>%2F2019%2F05%2F17%2F%E5%89%91%E6%8C%87Offer-%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%92%8C%E6%95%B0%E7%BB%84%2F</url>
    <content type="text"><![CDATA[题目都是来自牛客网在线刷题中的剑指offer。刷题记录，顺便从考察知识点的角度分类整理。主要分成以下四大类： 字符串、数组 链表、树 递归、回溯、动态规划 其他, 比如位运算、正则匹配等 这是剑指offer 系列四部曲中的第一部。第一部关于字符串和数组，第二部是栈、队列、链表和树，第三部递归、回溯和动态规划， 最后一部分在这里。 二维数组中的查找 在一个二维数组中（每个一维数组的长度相同），每一行都按照从左到右递增的顺序排序，每一列都按照从上到下递增的顺序排序。请完成一个函数，输入这样的一个二维数组和一个整数，判断数组中是否含有该整数。 列的递增不是基于上一行中最大的（最右边）进行的递增，而是基于上一行对应的列的递增。所以不是左上角是最小，右下角是最大的。 双指针问题，对于任意的一点，下面的都是比其大，左面的都是比其小，这就决定了做好的出发点是从右上角出发。这样可以不断的进行if … else 的判断，然后找到 target。 1234567891011121314def Find( target, array): rows =len(array) cols =len(array[0]) row =0 col =cols-1 while row &lt; rows and col &gt;=0: if array[row][col] == target: return True elif array[row][col] &gt; target: col -=1 else: row +=1 return False 替换空格 请实现一个函数，将一个字符串中的每个空格替换成“\%20”。例如，当字符串为We Are Happy.则经过替换之后的字符串为We\%20Are\%20Happy。 Tips： 字符串的遍历对于 python 而言是比较简单的。 考察的就是字符串的遍历。如果发现了空格，那么替换成 “%20”，否则就是原字符。 123456789101112class Solution:# s 源字符串def replaceSpace(self, s): # write code here # python 中的 str 就是 array of char converted ="" for ch in s: if ch ==" ": converted += "%20" else: converted += ch return converted 旋转数组的最小数字 把一个数组最开始的若干个元素搬到数组的末尾，我们称之为数组的旋转。 输入一个非减排序的数组的一个旋转，输出旋转数组的最小元素。 例如数组{3,4,5,1,2}为{1,2,3,4,5}的一个旋转，该数组的最小值为1。 NOTE：给出的所有元素都大于0，若数组大小为0，请返回0。 如果是有序的数组，那么查找一个数字，使用 二分查找就OK，这个数字不要求是最小数字。这个是在旋转数组中查找最小元素，而不是查找某个元素，注意两者的区别。这个题目本质上是递归，只不过是能够使用循环的方式进行表达。而递归的本质在于大的问题能够转换成小的问题。 12345678910111213141516171819class Solution: def minNumberInRotateArray(self, arr): if not arr or len(arr) ==0: return min_val =arr[0] left, right =0, len(arr)-1 while right -left &gt;1: mid =(left +right)/2 if arr[left] &lt;= arr[mid]: left =mid elif arr[mid] &lt;=arr[right]: right =mid min_val =arr[right] return min_val 调整数组顺序使奇数位于偶数前面 输入一个整数数组，实现一个函数来调整该数组中数字的顺序，使得所有的奇数位于数组的前半部分，所有的偶数位于数组的后半部分，并保证奇数和奇数，偶数和偶数之间的相对位置不变。 快排是根据阈值分别左右挑选了大于和小于阈值的数组。这道题是可以分别左右分别挑选偶数和奇数，然后交换位置，思路是一样的。 123456789101112131415161718192021def partition(arr): if not arr or len(arr) ==0: return left, right =0, len(arr)-1 while left &lt; right: print(arr[left]) while arr[left] %2 ==0:# 奇偶判断的时候，尽量不要使用整除，python2 python3 的语法环境是不一样的 left +=1 while arr[right] % 2 ==1: right -=1 if (left&lt; right): arr[left], arr[right] =arr[right], arr[left]if __name__ =="__main__": arr =[1,2, 3, 4, 5] partition(arr) print(arr) 上面版本保留了原始数字相对的顺序，下面这个没有保留相对的顺序。前者的空间复杂度是O(N), 后者的空间复杂度是O(1). 1234567891011121314151617class Solution: def reOrderArray(self, array): if len(array) ==0: return [] left =0 right =len(array)-1 while left &lt; right: # 如果是奇数 key =array[left] while left &lt; right and array[right] &amp; 1 == 0: right -= 1 array[left] = array[right] # 如果是偶数 while left &lt; right and array[left] &amp; 1 == 1: left += 1 array[right] = key return array 字符串的排列 输入一个字符串,按字典序打印出该字符串中字符的所有排列。例如输入字符串abc,则打印出由字符a,b,c所能排列出来的所有字符串abc,acb,bac,bca,cab和cba。 动态规划和递归有点搞不太清楚。 1234567891011121314151617class Solution:# 递归： 变换方程： 第一字母和 剩下的所有的字母def Permutation(self, ss): # write code here if not ss: return [] res = [] self.helper(ss, '', res) return sorted(list(set(res)))def helper(self, ss, path, res): if not ss: res.append(path) for i in range(len(ss)): self.helper(ss[:i] + ss[i + 1:], path + ss[i], res) 数组中出现次数超过一半的数字 数组中有一个数字出现的次数超过数组长度的一半，请找出这个数字。例如输入一个长度为9的数组{1,2,3,2,2,2,5,4,2}。由于数字2在数组中出现了5次，超过数组长度的一半，因此输出2。如果不存在则输出0。 Tips： 如果某个数字出现的次数多于一半，那么其他所有非该数字的出现的频数是小于该数字的，所以形成一个二分类。 12345678910111213141516171819class Solution:# 如果存在这样的数字，那么这个数字的频数一定是大于其他所有的频数def MoreThanHalfNum_Solution(self, numbers): # write code here if not numbers: return 0 target = numbers[0] nums = 0 # 统计出现次数最多的数字 for i in numbers: if target == i: nums += 1 elif nums == 0: target = i nums = 1 else: nums -= 1 res = target if numbers.count(target) &gt; len(numbers) // 2 else 0 return res 连续子数组的最大和 HZ偶尔会拿些专业问题来忽悠那些非计算机专业的同学。今天测试组开完会后,他又发话了:在古老的一维模式识别中,常常需要计算连续子向量的最大和,当向量全为正数的时候,问题很好解决。但是,如果向量中包含负数,是否应该包含某个负数,并期望旁边的正数会弥补它呢？例如:{6,-3,-2,7,-15,1,2,2},连续子向量的最大和为8(从第0个开始,到第3个为止)。给一个数组，返回它的最大连续子序列的和，你会不会被他忽悠住？(子向量的长度至少是1) 连续子数组的最大和和数组中超过一半的数字，这种问题都是属于数组的遍历问题。 12345678910111213141516class Solution:# largest , sum 这是两个不同的状态# 注意初始化def FindGreatestSumOfSubArray(self, array): # write code here if not array: return [] largest =array[0] sum_of_array =0 for i in array: sum_of_array += i if sum_of_array &gt; largest: largest =sum_of_array elif sum_of_array &lt;0: sum_of_array =0 return largest 第一个只出现一次的字符 在一个字符串(0&lt;=字符串长度&lt;=10000，全部由字母组成)中找到第一个只出现一次的字符,并返回它的位置, 如果没有则返回 -1（需要区分大小写）. 如果使用了 dictionary，那么这个只是数据结构上的问题，不存在算法上的问题了。 1234567891011121314151617181920212223242526272829303132class Solution: """ Three ways to get dictionary of string s """ def FirstNotRepeatingChar(self, s): if not s: return -1 # get dictionary from collections import defaultdict dict1 =defaultdict(int) for string in s: dict1[string] += 1 # or this way # from collections import Counter # dict1 =Counter(s) # or do it yourself #dict1 =self.Counter_self(s) for index, val in enumerate(s): if dict1[val] == 1: return -1 def Counter_self(self, s): dict1 = &#123;&#125; for val in s: if val not in dict1: dict1[val] = 1 else: dict1[val] = dict1[val] + 1 return dict1 数组中的逆序对? 在数组中的两个数字，如果前面一个数字大于后面的数字，则这两个数字组成一个逆序对。输入一个数组,求出这个数组中的逆序对的总数P。并将P对1000000007取模的结果输出。 即输出P%1000000007 1234567891011121314151617181920212223242526272829303132333435363738394041# -*- coding:utf-8 -*-# 之前在牛客网上是可以通过的，现在因为时间复杂度又没办法通过了class Solution: # 这个一斤难出天机了 先不看了 def InversePairs(self, data): # write code here if not data: return 0 temp = [i for i in data] return self.mergeSort(temp, data, 0, len(data ) -1) % 1000000007 def mergeSort(self, temp, data, low, high): if low &gt;= high: temp[low] = data[low] return 0 mid = (low + high) / 2 # 不懂 data 和 temp 为什么是颠倒顺序 left = self.mergeSort(data, temp, low, mid) right = self.mergeSort(data, temp, mid +1, high) count = 0 i = low j = mid +1 index = low while i &lt;= mid and j &lt;= high: if data[i] &lt;= data[j]: temp[index] = data[i] i += 1 else: temp[index] = data[j] count += mid - i +1 j += 1 index += 1 while i &lt;= mid: temp[index] = data[i] i += 1 index += 1 while j &lt;= high: temp[index] = data[j] j += 1 index += 1 return count + left + right 数字在排序数组中出现的次数? 统计一个数字在排序数组中出现的次数。 想法一：在排序数组中，查找数字，使用的就是二分查找，这里不同的是分别找出左右两个边界的值。因为是连续出现的。（哈哈，在使用 二分查找得到左右边界的时候，写不出来） 想法二： 使用二分查找得到一个数字，如果然后顺序查找。下面是实现。 12345678910111213141516171819202122class Solution:# 二分查找，当 data[mid] ==key 的时候是顺序查找，是递归跳出的条件def GetNumberOfK(self, data, k): # write code here # 这个是有两个跳出条件的，一个是正确的跳出，一个是不正确的跳出 if not data: return 0 mid =len(data) // 2 if data[mid] == k: left = right = mid for i in range(mid - 1, -1, -1): if data[i] == k: left -= 1 for i in range(mid + 1, len(data)): if data[i] == k: right += 1 return right - left + 1 # 一半一半的舍去数据 elif data[mid] &lt; k: return self.GetNumberOfK(data[mid + 1:], k) else: return self.GetNumberOfK(data[:mid - 1], k) 数组中只出现一次的数字 ** 一个整型数组里除了两个数字之外，其他的数字都出现了两次。请写程序找出这两个只出现一次的数字。 Tips: 异或操作，两个十进制数字经过异或（二级制计算过程），最后的结果是10进制的形式。如果两个相同的数字异或，那么最后的结果是0，如果是不同的数字，最后的结果是非0.比如 2^4 # 43^4 # 740^42 # 2 结果的二进制形式一定至少有一个 “1”.使用index 得到两个不同的数字二进制形式下的位置，然后从该位置将原来的数组分成两类，那么每类中只含有一个出现一次的数字，接着使用异或操作。 最初的题目：只有一个数字出现了一次，其他的数字都是出现了偶次，那么一遍遍历数（进行异或）就可以得到这个数字。现在是由两个单独的数字。 与运算是 二进制位运算， ^ 是整数运算。 12345678910111213141516171819202122class Solution:# 返回[a,b] 其中ab是出现一次的两个数字# 使用异或的性质，如果只有一个不同，其他的偶次出现，那么全部异或的结果# 就是那个单一的数字def FindNumsAppearOnce(self, array): # write code here remain, index =0, 1 for num in array: remain = remain ^ num # 找出第一个是1 的位置 # index 都是 while (remain &amp; index) ==0: index = index &lt;&lt;1 res1, res2 =0,0 for num in array: # 这个条件必须是0, 表示两个在这个位数是相同的， # 234 ^0 =234 , if num &amp; index ==0: res1 =res1 ^ num else: res2 =res2 ^ num return [res1, res2] 和为S的连续正数序列 (回去做 ，写另外一种做法) 小明很喜欢数学,有一天他在做数学作业时,要求计算出9~16的和,他马上就写出了正确答案是100。但是他并不满足于此,他在想究竟有多少种连续的正数序列的和为100(至少包括两个数)。没多久,他就得到另一组连续正数和为100的序列:18,19,20,21,22。现在把问题交给你,你能不能也很快的找出所有和为S的连续正数序列? 这个是数学问题，使用等差数列求和。还有一种解法。 12345678910111213141516def FindContinuousSequence(self, tsum): # write code here if tsum &lt; 2: return [] left = 1 right = left + 1 res = [] while left &lt; tsum // 2 + 1: if sum(range(left, right)) == tsum: res.append(range(left, right)) left += 1 elif sum(range(left, right)) &lt; tsum: right += 1 else: left += 1 return res 和为S的两个数字 输入一个递增排序的数组和一个数字S，在数组中查找两个数，使得他们的和正好是S，如果有多对数字的和等于S，输出两个数的乘积最小的。 有序序列，查找两个数字，可以使用二分。 12345678910111213141516class Solution: def FindNumbersWithSum(self, array, tsum): # write code here if len(array) &lt; 2: return [] left = 0 right = len(array) - 1 while left &lt; right: if array[left] + array[right] == tsum: return [array[left], array[right]] elif array[left] + array[right] &lt; tsum: left += 1 else: right -= 1 return [] 左旋转字符串 汇编语言中有一种移位指令叫做循环左移（ROL），现在有个简单的任务，就是用字符串模拟这个指令的运算结果。对于一个给定的字符序列S，请你把其循环左移K位后的序列输出。例如，字符序列S=”abcXYZdef”,要求输出循环左移3位后的结果，即“XYZdefabc”。是不是很简单？OK，搞定它！ Tips: python中字符串的处理是没有压力的 123456class Solution: def LeftRotateString(self, s, n): # write code here if len(s) &lt; n: return '' return s[n:] + s[:n] 翻转单词顺序 牛客最近来了一个新员工Fish，每天早晨总是会拿着一本英文杂志，写些句子在本子上。同事Cat对Fish写的内容颇感兴趣，有一天他向Fish借来翻看，但却读不懂它的意思。例如，“student. a am I”。后来才意识到，这家伙原来把句子单词的顺序翻转了，正确的句子应该是“I am a student.”。Cat对一一的翻转这些单词顺序可不在行，你能帮助他么？ 这个是通过两次翻转就可以解决，第一次是字符串的完全翻转，第二次是单词的完全翻转。start, end 分别标记一个单词的开始和结束。 1234567891011121314151617181920212223242526272829class Solution: def Reverse(self, s, left, right): while left &lt;right: s[left], s[right] = s[right], s[left] left +=1 right -=1 def ReverseSentence(self, s): # write code here if not s: return s # from immutable string to mutable list # s =list(s) self.Reverse(s, 0, len(s) - 1) start, end = 0, 0 # 这个小于号 是python 中特有的坑，真正能够访问的区间是 [0, len(s)-1] 这样的区间 while start &lt; len(s): if s[start] == " ": start += 1 end += 1 elif end == len(s) or s[end] == " ": self.Reverse(s, start, end - 1) # update 操作 end += 1 start = end else: end += 1 return "".join(s) 扑克牌顺子 LL今天心情特别好,因为他去买了一副扑克牌,发现里面居然有2个大王,2个小王(一副牌原本是54张^_^)…他随机从中抽出了5张牌,想测测自己的手气,看看能不能抽到顺子,如果抽到的话,他决定去买体育彩票,嘿嘿！！“红心A,黑桃3,小王,大王,方片5”,“Oh My God!”不是顺子…..LL不高兴了,他想了想,决定大/小 王可以看成任何数字,并且A看作1,J为11,Q为12,K为13。上面的5张牌就可以变成“1,2,3,4,5”(大小王分别看作2和4),“So Lucky!”。LL决定去买体育彩票啦。 现在,要求你使用这幅牌模拟上面的过程,然后告诉我们LL的运气如何， 如果牌能组成顺子就输出true，否则就输出false。为了方便起见,你可以认为大小王是0。 Tips：list 中的空缺数量 需要不大于 大小王总数，这样才能构成顺子。 12345678910111213141516171819202122232425class Solution: """ 空缺是1 意味着这两个数字是连续的 比如说 1 2， 这个big -small ==1, 所以这个空缺是0，不用进行填充。 """ def IsContinuous(self, numbers): # write code here if not numbers: return False numbers.sort() # sort() sorted() 这种怎么使用，返回值是什么，这些基本的东西 zeros =numbers.count(0) gaps = 0 left = zeros # 因为这个是排序之后的结果，所以可以这样进行操作 right = left + 1 # 实际上还是两个指针， 所以可以使用两个指针进行操作 # 本质上是两个 相邻指针在进行移动，因为是排序之后，所以没有问题 while right &lt; len(numbers): if numbers[left] == numbers[right]: return False gaps += numbers[right] - numbers[left] - 1 left = right right += 1 # 这种是真的 很简洁， gaps &lt;= zeros 少去了很多if else的判断 return gaps &lt;= zeros 孩子们的游戏(圆圈中最后剩下的数) 每年六一儿童节,牛客都会准备一些小礼物去看望孤儿院的小朋友,今年亦是如此。HF作为牛客的资深元老,自然也准备了一些小游戏。其中,有个游戏是这样的:首先,让小朋友们围成一个大圈。然后,他随机指定一个数m,让编号为0的小朋友开始报数。每次喊到m-1的那个小朋友要出列唱首歌,然后可以在礼品箱中任意的挑选礼物,并且不再回到圈中,从他的下一个小朋友开始,继续0…m-1报数….这样下去….直到剩下最后一个小朋友,可以不用表演,并且拿到牛客名贵的“名侦探柯南”典藏版(名额有限哦!!^_^)。请你试着想下,哪个小朋友会得到这份礼品呢？(注：小朋友的编号是从0到n-1) Tips: 约瑟夫环的问题， 要求求解的是最后胜利者的编号，所以应用数学技巧就可以了。 $ f(x) = (f( x-1) + m ) % (x) $ , 共有 m 个编号，n 个人 12345678910111213class Solution:# mod 求余 的操作， a mod b ==c ,说明 a除以b 之后余数是c# https://blog.csdn.net/gatieme/article/details/51435055， 从做题思路上讲解的比较好# n 个小朋友，然后是m 个编号def LastRemaining_Solution(self, n, m): # write code here if n&lt; 1 or m&lt;1: return -1 last =0 for i in range(2, n+1): # 这个相当于 是一个 “挑选人” 的逆过程， 因为使用的 mod 操作就是取余的操作 last =(last +m) %i return last 求1+2+3+…+n 求1+2+3+…+n，要求不能使用乘除法、for、while、if、else、switch、case等关键字及条件判断语句（A?B:C）。 Tips： 不使用条件判断，来控制跳出；这里使用的是 “短路条件” 来 控制 递归的跳出。 使用递归的思想，来求解数列的和。 12345678910111213141516class Solution: # 如果你想使用全局变量，那么放在 __init__ 中就是一个很好的方式 def __init__(self): self.ans =0 def Sum_Solution(self, n): # write code here self.recur(n) return self.ans # n&gt;0 就是一个短路条件，这个直接决定了后面递归会不会继续执行下去，也就是跳出的条件 # 至于会不会回到原来最初的状态，这个是不重要的，最后的结果是 self.ans ，false之后直接使用这个就行了 def recur(self, n): self.ans += n n -= 1 return n &gt; 0 and self.Sum_Solution(n) 不用加减乘除做加法 写一个函数，求两个整数之和，要求在函数体内不得使用+、-、*、/四则运算符号。 Tips: 使用 异或和与 来进行 “加”、“减”的操作。加法是分成当前位相加 和进位两个部分的。与操作是作为进位操作，只有 1 &amp;1 才是1，异或作为相加的操作， 0 ^1 是1。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051class Solution: """ 不能使用四则运算符，我们就可以使用位运算符。对这两个数在更底层的角度上进行运算。也就是从 01 这种子串的角度进行考虑 https://blog.csdn.net/derrantcm/article/details/46798763 这个博客对于数的运算过程和 位运算是如何一一对应的。分为不进位相加和进位相加。 """ def add(self, num1, num2): while num2 != 0: carry = num1 &amp; num2 num1 = num1 ^ num2 # 这个应该理解为到高位 而不是*2 这样的操作 num2 = carry &lt;&lt; 1 return num1 def sub(self, num1, num2): while num2 != 0: carry = (~num1) &amp; num2 num1 = num1 ^ num2 num2 = carry &lt;&lt; 1 return num1 def Add(self, num1, num2): if num1 &gt;= 0 and num2 &gt;= 0: result = self.add(num1, num2) elif num1 &gt; 0 and num2 &lt; 0: flag = 1 if num1 &gt; abs(num2) else -1 # num2 =abs(num2) # keep num1 bigger than num2 if num1 &lt; abs(num2): num1, num2 = abs(num2), num1 result = self.sub(num1, abs(num2)) result = result * flag elif num1 &lt; 0 and num2 &gt; 0: flag = 1 if abs(num1) &lt; num2 else -1 if abs(num1) &lt; num2: num1, num2 = num2, abs(num1) result = self.sub(abs(num1), num2) result = result * flag else: flag = -1 num1 = abs(num1) num2 = abs(num2) result = self.add(num1, num2) result = result * flag return result 把字符串转换成整数 将一个字符串转换成一个整数(实现Integer.valueOf(string)的功能，但是string不符合数字要求时返回0)，要求不能使用字符串转换整数的库函数。 数值为0或者字符串不是一个合法的数值则返回0。 Tips：还是python 中处理 string， 使用 dictionary 处理字符串和 数字的匹配。这个有好多个版本，是需要问清题目要求，然后根据进行作答。比如说这个是整数比较简单，如果加上小数，该如何处理。如果是中文，的该如何处理。 12345678910111213141516171819202122class Solution:# 有很多不合法的输入，比如空字符串，正负号，非数字字符 数据溢出，所以从反面考虑更加简单一些# 合法的输入只有数字和符号位 + 和-def StrToInt(self, s): # write code here int_list=['0', '1', '2', '3', '4', '5', '6','7', '8', '9', '+', '-'] if s ==" ": return 0 sum1 =0 flag =1 # 正负号 for string in s: if string not in int_list: return 0 if string =="+": flag =1 continue elif string =="-": flag = -1 continue else: sum1 =sum1 *10 +int_list.index(string) return sum1*flag 在一个长度为n的数组里的所有数字都在0到n-1的范围内。 数组中某些数字是重复的，但不知道有几个数字是重复的。也不知道每个数字重复几次。请找出数组中任意一个重复的数字。 例如，如果输入长度为7的数组**{2,3,1,0,2,5,3}，那么对应的输出是第一个重复的数字2。 没有说找出所有重复的数字，只是说找出任意一个重复的数字。一种方法当然是使用dictionary 的思想，如果已经在dictionary 中存在，那么直接输出；否则继续遍历。还有一种方式，是训练数值和index 之间的关系，因为数值是在 0到n-1 之间，那么没有重复的时候，index 的值就是数值真实的值，所以可以这样进行判断。 123456789101112131415class Solution: # 这里要特别注意~找到任意重复的一个值并赋值到duplication[0] # 函数返回True/False # 第二种方式，如果这个是有序的 那么 numbers[i] ==i 这个是成立的 def duplicate(self, numbers, duplication): # write code here length =len(numbers) for i in range(length): while i != numbers[i]: if numbers[numbers[i]] == numbers[i]: duplication[0] = numbers[i] return True else: numbers[numbers[i]], numbers[i] = numbers[i], numbers[numbers[i]] return False 构建乘积数组 ? 给定一个数组A[0,1,…,n-1],请构建一个数组B[0,1,…,n-1],其中B中的元素B[i]=A[0]A[1]…A[i-1]A[i+1]…A[n-1]。不能使用除法。 Tips: 分成上下三角形进行计算，不能每个B[i] 都进行单独重复计算。下三角形是从上往下遍历，上三角形是从下往上遍历。ans 存储各个不同的结果。 加上链接中的图去理解一下这个过程。 12345678910111213141516171819class Solution: # 思路： 转换成图形的就容易想一些， https://blog.csdn.net/u010005281/article/details/80200398 # 代码：https://blog.csdn.net/fuxuemingzhu/article/details/79718543 # A 是一个list ，只是自己构建的是一个矩阵 def multiply(self, A): # write code here ans =[] tmp =1 length =len(A) # 值得是 rows # 首先是下三角形 各个部分的数值的相乘， 从上往下遍历 for i in range(length): ans.append(tmp) tmp *= A[i] tmp =1 # 上三角形 从下往上进行遍历 for i in range(length-1, -1, -1): ans[i] *= tmp tmp *= A[i] return ans 表示数值的字符串 请实现一个函数用来判断字符串是否表示数值（包括整数和小数）。例如，字符串”+100”,”5e2”,”-123”,”3.1416”和”-1E-16”都表示数值。 但是”12e”,”1a3.14”,”1.2.3”,”+-5”和”12e+4.3”都不是。 这个是判断是否的问题，而不是真正转换成 int的过程，所以子函数是返回的true or false，而不是真正的数值。这个相对于前面那个string2int 就比较难一点了，关键是判断 “e” 的存在与否。这种 str2int or allownum 是非常常见的初始化，是必须要学会。 123456789101112131415161718192021222324252627282930313233343536373839class Solution: # s字符串 # 第一种方法是 float()强转，一种是 re 正则表达式匹配 最后一种逻辑判断之类的 # 以 e 为分割符，分成front and behind 两部分，behind 长度不能为0 或者出现 . # digit的判断，+- 只能出现在首位， . 只能出现一次 """ https://github.com/leeguandong/Interview-code-practice-python/blob/master/%E5%89%91%E6%8C%87offer/%E8%A1%A8%E7%A4%BA%E6%95%B0%E5%80%BC%E7%9A%84%E5%AD%97%E7%AC%A6%E4%B8%B2.py """ def isNumeric(self, s): if not s or len(s) == 0: return s s = [i.lower() for i in s] if 'e' in s: index = s.index('e') front = s[:index] behind = s[index + 1:] if len(behind) == 0 or '.' in behind: return False f = self.Digit(front) b = self.Digit(behind) return f and b else: isNum = self.Digit(s) return isNum def Digit(self, s): dotNum = 0 allowNum = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '.', '+', '-'] for i in range(len(s)): if s[i] not in allowNum: return False if s[i] == '.': dotNum += 1 if s[i] in '+-' and i != 0: return False if dotNum &gt; 1: return False return True 字符流中第一个不重复的字符 请实现一个函数用来找出字符流中第一个只出现一次的字符。例如，当从字符流中只读出前两个字符”go”时，第一个只出现一次的字符是”g”。当从该字符流中读出前六个字符“google”时，第一个只出现一次的字符是”l”。 流操作一般在笔试中不会考察。 12345678910111213141516171819class Solution: # 返回对应char # 这个没有了dict 那么依赖于 count 函数 # 主要差别在于有了一个 字符流，是动态的，所以需要有一个大的存储的list def __init__(self): self.list1 =[] def FirstAppearingOnce(self): # write code here for string in self.list1: if self.list1.count(string) == 1: return string return "#" def Insert(self, char): # write code here self.list1.append(char)]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>剑指offer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pinduoduo]]></title>
    <url>%2F2019%2F05%2F11%2Fprepare_for_interview%2F</url>
    <content type="text"><![CDATA[拼多多提前批笔试。 拼多多机经拼多多的在线编程真的是难啊！ 编程题 最大乘积 给定一个无序数组，包含正数、负数和0，要求从中找出3个数的乘积，使得乘积最大，要求时间复杂度：O(n)，空间复杂度：O(1) 1234567891011输入描述:无序整数数组A[n]输出描述:满足条件的最大乘积输入例子1:3 4 1 2输出例子1:24 123456789class Solution(object): def maximumProduct(self, nums): """ :type nums: List[int] :rtype: int """ nums.sort() return nums[-1] * max(nums[0] * nums[1], nums[-2] * nums[-3]) 数组中的最长山谷 问题描述：LeetCode 845. 数组中的最长山脉 原题是找山脉，这里改成了山谷 1234567示例： 输入： [4,3,2,5,3,1,4,8] 输出： 5 说明： [5,3,1,4,8] “坑”点说明输入就是字符串 “[4,3,2,5,3,1,4,8]” 包括括号和标点问题是，直接返回 0 也有 20% 的正确率，导致我一直没想到是输入上的问题，直到最后都卡在 20%建议所有需要处理字符串的问题，都使用 Python，这里只要 A = eval(input()) 就完事了；而 C++ 如果不熟悉 STL 的话，处理输入都比题目本身难了思路：暴力枚举；看代码更直观C++ 代码 [code]（没做输入处理） 这个是最长的山峰的计算，题目要求是山谷的计算。 12345678910111213class Solution(object): def longestMountain(self, A): """ :type A: List[int] :rtype: int """ up, down = [0] * len(A), [0] * len(A) for i in range(1, len(A)): if A[i] &gt; A[i - 1]: up[i] = up[i - 1] + 1 for i in range(len(A) - 1)[::-1]: if A[i] &gt; A[i + 1]: down[i] = down[i + 1] + 1 return max([u + d + 1 for u, d in zip(up, down) if u and d] or [0]) 字符串构造 问题描述一个长串由一个字串循环构成，即 s[i]=t[i%n]，比如 “abcabc” 由 “abc” 构成 注意：”abcabcab” 也是由 “abc” 构成的，答题时没注意这个又只过了一部分 123456789101112131415161718class Solution: def lengthOfLongestSubstring(self, s): """ :type s: str :rtype: int """ dic = &#123;&#125; res = 0 j = 0 for i in range(len(s)): if s[i] not in dic: dic[s[i]] = i else: j = max(j, dic[s[i]]+1) dic[s[i]] = i res = max(res, i-j+1) return res 到达指定位置 Leetcode 754. 到达终点数字You are standing at position 0 on an infinite number line. There is a goal at position target.On each move, you can either go left or right. During the n-th move (starting from 1), you take n steps.Return the minimum number of steps required to reach the destination. 1234567891011121314class Solution(object): def reachNumber(self, target): """ :type target: int :rtype: int """ count = 0 for i in range(888888): count += i if abs(target) &lt;= abs(count) and target % 2 == count % 2: return i 在 牛客网上，使用这样的语句进行标准的输入和输出。 1234import syshp=int(sys.stdin.readline().strip().split()[0])norm=int(sys.stdin.readline().strip().split()[0])buff=int(sys.stdin.readline().strip().split()[0]) 可以尝试使用python3 进行操作一下 123HP = list(map(int,raw_input().split()))HP = list(raw_input().split())HP = list(raw_input()) 使用这种形式可以得到一个整数 1int(input()) ( 下面没有标准答案的都是学霸批，果然是比较难的哦) 靓号 A 国的手机号码由且仅由 N 位十进制数(0-9)组成，可以有前导 0，比如 000123456。一个手机号码中至少有 K 位数相同则被定义为靓号（不要求连续）。如果想把自己的手机号修改为一个靓号，修改一个数字的金额为新数字与旧数字之间的差的绝对值，比如 1 修改为 6 或 6 修改为 1 都要花 5 块钱。求对给定手机号，修改为靓号最少要花的钱数以及新的号码（如果有多个，输出字典序最小的）。 1234567891011121314输入： 第一行包含两个整数 N 和 K，分别表示 手机号码的位数和靓号要求的位数 第二行为 N 个数字，数字之间没有空白符 数据范围 2 &lt;= K &lt;= N &lt;= 10000示例： 输入： 6 5 785785 输出： 4 777577 说明: 777577 比 777775 字典序小 思路：统计每个数字出现次数counter，以每个数字为基准，按照与基准差值对counter排序，优先替换差值小的数字；关于字典序的问题，如果替换的数比基准大则从前向后替换，如果替换的数比基准大，则从后向前替换，得到的就是字典序最小的字符串，时间复杂度O(n) 参考代码： https://www.nowcoder.com/discuss/87694 https://www.cnblogs.com/LJ-LJ/p/11254783.html http://www.ishenping.com/ArtInfo/398682.html 参考代码（下面三道题的都是可以从这里进行求解） https://blog.csdn.net/u012525096/article/details/97619751 https://www.hytheory.com/algorithm/pdd-exam/ 几乎严格升序 给定两个数组A和B，其中数组A是几乎严格升序排列的，几乎的定义是只需改变其中一个数，即可满足完全升序排列。你的任务是从A中找到这个数组，并从数组B中选取一个数将其代替，使得A是严格升序排列的，请找出B中满足要求的最大数字，并输出有序数组，如不存在则输出NO。 首尾相连的一组字符串 给定一个字符串数组（字符串长度和数组长度均大于1且小于1024），所有字符均为大写字母。请问，给定的字符串数组是否能通过更换数组中元素的顺序，从而首尾相连，形成一个环。 多任务的执行顺序 现在一共有N个待执行的任务，每个任务需要Pi的时间完成执行。同时任务之间可能会有一些依赖关系，比如任务1可能依赖任务2和任务3，那么任务1必须等任务2和任务3执行完成后才能开始执行。为了最小化任务的平均返回时长，请安排所有任务的执行顺序。假设在零时刻，所有N个任务已到达系统。 搭积木 有N个长方体积木， 每个积木的高都是1，长宽都为Li，重量为Wi。现在想要用这些积木搭一个高高的金字塔，每一层由且仅由一块积木组成，同时每一层积木的边长都比下方的积木小，每块积木智能承受自身重量的7倍重量，请计算最高可以搭一个多高的金字塔？ 1234567891011121314151617181920212223242526272829303132333435363738394041424344public class E4 &#123; static class Box &#123; int len; int weight; &#125; public static void main(String[] args) &#123; Scanner in = new Scanner(System.in); while(in.hasNext()) &#123; int n = in.nextInt(); Box[] b = new Box[n+1]; b[0] = new Box(); for(int i = 1; i &lt; n+1; ++i) &#123; b[i] = new Box(); b[i].len = in.nextInt(); &#125; for(int i = 1; i &lt; n+1; ++i) &#123; b[i].weight = in.nextInt(); &#125; Arrays.sort(b, (b1, b2) -&gt; &#123;return b1.len == b2.len ? b1.weight-b2.weight : b1.len - b2.len;&#125;); int res = 0; int[][] dp = new int[n+1][n+1]; for(int i = 0; i &lt; n+1; ++i) &#123; for(int j = 0; j &lt; n+1; ++j) &#123; dp[i][j] = Integer.MAX_VALUE; &#125; &#125; dp[0][0] = 0; for(int i = 1; i &lt; n+1; ++i) &#123; for(int k = 0; k &lt; i; ++k) &#123; if(b[k].len &lt; b[i].len) &#123; for(int j = 0; j &lt;= k; ++j) &#123; if(dp[k][j] != Integer.MAX_VALUE &amp;&amp; dp[k][j] &lt;= b[i].weight*7) &#123; dp[i][j+1] = Math.min(dp[i][j+1], dp[k][j]+b[i].weight); res = Math.max(res, j+1); &#125; &#125; &#125; &#125; &#125; System.out.println(res); &#125; &#125;&#125; 喝可乐复制问题 自动售货机里有N瓶复制可乐.复制可乐非常神奇，喝了它的人会复制出一个自己来！现在有Alice,Bob,Cathy,Dave四个人在排队买复制可乐。买完的人会马上喝掉，然后他和他的副本会重新去队伍的最后面排队买可乐。问：最后一个买到复制可乐的人叫什么名字？ 谁是球王 M 个投票者给 N 个球星评等级，等级由英文字母表示，’a’级最高，’z’级最低，共26个级别。当 [球星X的评级比Y高] 的票数高于 [球星Y的评级比X高] 的票数时，称候选球星X强于候选球星Y。若有一个候选球星强于其他所有球星，则称为球王。根据这个规则至多有一个球王，否则没有球王。请你判断哪位换选球星是球王。 运货物问题 N个货物分别重W1、W2、…Wn，（100&lt;=W&lt;=300）,一辆车可以运重量300的货，问需要多少辆车。思路:先从小到大排序，i、j两个指针，i从0开始遍历去掉3的整数倍个100（每三个100增加一辆车），j从N-1开始去掉大于200的数（大于200的自己一辆车），i、j所指的货物若可以拼车，车数加一，i、j向中间移动，否则，车数加一,j向中间移动。 字符串矩形输出 123456789字符串输出为正方形，如abcdefghijklmnop，输出：abcdep fo gn hmlkji思路：计算出每行长度，然后分四次将字符加到每条边上，注意str.size()/4+1等于矩形边长。 字符串转数字，加小数点 字符串类型的整数（可有前缀），可以分成两部分，可以加小数点，问有多少种组合 思路：首先考虑字符串长度为0和1的情况，输出0；然后是字符串长度&gt;=2的情况，定义函数kind，返回当前字符串可以加小数点的种类数。 12345678910111213141516171819202122232425class Solution(object): def myAtoi(self, str): """ :type str: str :rtype: int """ ls =list(str.strip()) if len(ls) == 0: return 0 sign = -1 if ls[0] == '-' else 1 #if ls[0] in ['-', '+']: del ls[0] index=0 if ls[index] in ['-', '+']: index +=1 res =0 for i in range(index, len(ls)): if ls[i].isdigit(): res =res *10 + ord(ls[i]) -ord('0') else: break return max(-pow(2, 31), min(sign*res, pow(2,31) -1)) 最亲密的非朋友 给定好友关系表，求出制定序号非好友中拥有共同好友最多的那个人； 思路：将输入存储到n*n的数组中，设要求的是i，将i行循环一遍，将每个和i不是朋友的取出来，与i的朋友进行比较，然后找出相似性最大的。 输入的时候，用到getline，要先getline一次，才能正常输入 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152class Solution(object): def findCircleNum(self, M): """ :type M: List[List[int]] :rtype: int """ class UnionFind: def __init__(self, n): self.parent = [i for i in range(n)] def find(self, p): root = p # 只要不是最上层的那个结点，就不停向上找 while root != self.parent[root]: root = self.parent[root] # 此时 root 就是大 boss # 接下来把 p 到 root 沿途所有的结点都指向 root while p != self.parent[p]: temp = self.parent[p] self.parent[p] = root p = temp return root def is_connected(self, p, q): return self.find(p) == self.find(q) def union(self, p, q): p_id = self.find(p) q_id = self.find(q) if p_id == q_id: return self.parent[p_id] = q_id m = len(M) union_find_set = UnionFind(m) # 只看下三角矩阵(不包括对角线) for i in range(m): for j in range(i): if M[i][j] == 1: union_find_set.union(j, i) counter = 0 # print(union_find_set.parent) # 自己的父亲是自己的话，这个结点就是根结点，是老大，是 boss # boss 的特点就是，他上面没有人，例如：李彦宏、马云 # 数一数有几个老大，就有几个朋友圈 for index, parent in enumerate(union_find_set.parent): if index == parent: counter += 1 return counterif __name__ == '__main__': M = [[1, 1, 0], [1, 1, 0], [0, 0, 1]] solution = Solution() result = solution.findCircleNum(M) print(result) 递增递减子序列 一组整数，每次抓取只能升序或者降序抓取，最少多少次抓完 思路：考虑顺序循环两遍，一次保存包含当前位的最长递增子序列长度，一次保存包含当前位的最长递减子序列长度，然后找出当前位的递增和递减长度最大且递增与递减长度差最大的位置，根据递增和递减长度的大小，决定是删除递增序列，还是递减序列，每删除一次，num++。外部加一个while循环，判断原始输入是否已经都删除完。都删除完后，num的值即为操作次数。 贪心算法，应该不是最终解法。 乘积最小值 给出两个数组，求两个数组对应元素乘积的最小值。 题目 一套英文字母卡片序列，将重复字母剔除后字母序最小(不区分大小写)的序列中第一张卡片是哪个字母？ 1xaBXY 剔除重复字母后可以为 xaby 或者 abxy，其中字母序最小的为 abxy，第一张卡片为 a 题目 银行抢劫问题。一个二维数组，第一维代表 n 个银行的位置，第二维代表每个银行可抢劫的金额，两个劫匪抢银行，要求银行距离大于 d，求可以抢劫的金额最大值。 题目 给出两个由圆括号组成的字符串，交错这两个圆括号序列，但要保持每个字符在源字符串中的位置，问总共有多少种交错方式可以得到合法的圆括号表达式。 京东对于数值型，还是使用c++吧。对于字符串类型的，使用 python吧。 逆否命题 原命题：若a，则b。 否命题：若非a，则非b 逆命题：若b， 则a 逆否命题：若非b，则非a 存在两对逆否命题：1 和4，2 和3 互为逆否命题。逆否命题是同真同假。这里的a 和b 只是一种符号，其实表示的关系是可以更加复杂。 使用持续输入的方式，只要是一直有输入，那么while 循环就不会停止。 1234567891011121314151617181920212223242526272829#include&lt;iostream&gt;#include&lt;algorithm&gt;#include&lt;cmath&gt;#include&lt;stdio.h&gt;using namespace std;double n, res;int m;int main()&#123; while(cin &gt;&gt;n) &#123; cin &gt;&gt;m; res =0.0; for(int i =0; i&lt;m; i++) &#123; res +=n; n =sqrt(n); &#125; printf("%.2f\n", res); &#125; return 0;&#125; c++ 库函数 123456789101112#include&lt;algorithm&gt;···```c++#include&lt;cmath&gt;double m, n;double res;res = pow(m, n)res =sqrt(m ,n) 12#include&lt;vector&gt;// 其中 vec.push_back() 相当于 python 中的append() ，是添加到了list 的最后面 c++ 中常见使用vector常见的三种初始化方式 123456789101112131415161718192021222324252627282930313233343536373839404142#include&lt;iostream&gt;#include&lt;algorithm&gt;#include&lt;vector&gt;using namespace std;#define LEN 10vector&lt;int&gt; vect(LEN);int n;// vector&lt;int&gt; vec&#123;10, 20, 30, 40, 50&#125;; 注意符号初始化数列// vector&lt;int&gt; vec(10, 1); 初始化10 个1int main()&#123; vector&lt;int&gt; vec(LEN); int n; cin &gt;&gt;n; vector&lt;int&gt; vec1&#123;10, 20, 30, 40, 50&#125;; vector&lt;int&gt; vec2(10, 1); // “常量” 初始化 for(int i =0; i&lt;5; i++) cout &lt;&lt;vec1[i]&lt;&lt; " "; cout&lt;&lt; endl; // vec全部初始化成一样的数字 for(int i =0; i&lt;10; i++) cout&lt;&lt; vec2[i] &lt;&lt; " "; cout&lt;&lt;endl; // 键盘输入输出初始化 for(int i =0;i&lt;n;i++) cin &gt;&gt; vec[i]; cout&lt;&lt;endl; for(int i =0; i&lt;n; i++) cout &lt;&lt;vec[i] &lt;&lt; " "; cout&lt;&lt;endl; return 0;&#125; 面试的京东的商品推荐的算法岗，在北辰大厦（奥森旁边），面试难度总体来说比较简单。（1） 到公司之后先做了一个小时的笔试题目，包括四个简答题和两个编程题，简答题主要是C语言相关的。 Map和Set的插入和查找的复杂度是多少，Set是否可以存储类 递归和循环可以互相转换吗？ epull和select的原理以及区别？ 什么时候会调用构造函数和析构函数？父类的构造函数可以是virtual的吗？ 编程题也特别简单，一道题是一个有序的数组，寻找数组的中的两个数它们的和是否等 于某个值target，返回True或者False，要求O(n)的复杂度。 另一道题目是一个数组，两个相邻下标的数组的值的差的绝对值为1，然后在这个数组中寻找某个值是否在这个数组中，是的话返回下标，否则返回-1。 （2）第一轮面试也特别简单，主要就是聊了聊自己的论文是怎么做的，实习的时候干了啥，然后再出了一道编程题，一个无序数组，只有0,1,2，然后你要对这个数组进行排序，要求O(n)复杂度。（荷兰国旗问题） （3） 第二轮面试也是主要是聊了聊自己的论文，然后推导SVM的原理，然后logistic和SVM的区别，Attention注意力机制的原理，RNN的公式等。还有一个编程题反转链表。]]></content>
      <categories>
        <category>others</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[pygen]]></title>
    <url>%2F2019%2F04%2F24%2Fpygen%2F</url>
    <content type="text"><![CDATA[pygen功能：有关联的随机生成人名，邮箱，ID Card (ssn)，电话，地址等信息，并且可以选择保存为 pandas dataframe格式, 数据库”.db” 文件, Excel 文件和csv 文件格式，用于机器学习训练。项目地址github。 随机生成虚假个人信息具有很大的应用空间。首先，虚假的生成数据可以用于机器学习模型的“准备数据”，当真实的数据比较少或者难以获得的时候，可以使用生成数据进行训练模型，待模型调通之后，然后使用真实的模型。并且，当真实的数据集中缺少某些特征时候，可以使用这种方法进行特征的填充。比如大的数据集中缺少现居城市地址的时候，可以调用该库中的 “city_real” 进行填充。 当前使用最为广泛的是 Faker 开源库用于个人信息的生成，对于中文姓名、邮箱电话等信息支持有限。并且生成的数据是单列的，数据之间没有联系。比如生成的身份证 (ssn) 和姓名所能体现的性别是不匹配(了解更多可以参考这里)、生成的姓名中缺少复姓和电话邮箱等信息不符合我们的使用习惯等等。所以我将从以下几点改进： 增强数据之间相关性 生成名字的多样性 符合国人使用习惯的邮箱电话 提供保存多种保存文件格式，更加适合机器学习的训练 中文名字有很强的性别属性。例如名字中带有“杰”“志”“宏”等字的一般为男性，带有“琬”“佩”“梅”等字的一般为女性。当然也有一些比较中性的字，例如“文”“安”“清”等，比较难猜测性别，关于这点会在另一个博客中展开，请期待。 faker 对中文的支持有限，比如下面这种情况。 1234from faker import Fakerfake = Faker('zh_CN')for _ in range(10): print(fake.name(),fake.ssn(),fake.phone_number()) 从图中可以明显的看出 “王玉梅”和 “李桂花”都是两个女性，但是这种身份证信息（ssn）都没有体现这点。关于身份证的科普信息可以从这里获得。简单来说倒数第二位表示性别信息，如果是男性就是奇数如果是女性就是偶数。faker 生成的数据是不具有数据之间的相关性的。 基于此，我们进行了改进。首先是姓名的生成，然后是性别的判断，最后再生成相应性别的身份证号码。 123from pygen import pygendb =pygen()db.gen_dataframe(fields =['name', 'ssn', 'phone', 'email']) 效果如下： 红色线条表示姓名和性别对应一致，蓝色线条表示结果不确定（“镜阳炎” 像是一个中性的名字），绿色表示生成了含有复姓的名字，增强了数据的多样性。 从上图的 “mail” 一列可以看出邮箱前缀的命名基本上是中文名字中“姓” 和“民”的拼音组合，加强了数据之间的相关性和真实度。 另外，电话号码按照运营商分为三类：0 表示移动，1表示联通，2表示电信。 print(&apos;移动字段:&apos;) for _ in range(5): print(db.simple_ph_num(types =0)) print(&apos;联通字段:&apos;) for _ in range(5): print(db.simple_ph_num(types =1)) print(&apos;电信字段：&apos;) for _ in range(5): print(db.simple_ph_num(types =2)) 输出： 移动字段: 15023689929 16771753917 16790223946 15950129353 15271129554联通字段: 13869739303 13786227031 13950354445 15137578545 15240836142电信字段： 17172983067 15658567011 18562313243 17073127396 15543448286 最后提供了多种文件保存格式，包括”.csv”, “.db” 和”.xlsx”等格式。可以使用如下： from pygen import pygen db =pygen() db.gen_table(filename =filename, fields =[&apos;name&apos;, &apos;ssn&apos;, &apos;phone&apos;, &apos;email&apos;]) db.gen_excel(filename =filename, fields =[&apos;name&apos;, &apos;ssn&apos;, &apos;phone&apos;, &apos;email&apos;]) db.gen_csv(filename =filename, fields =[&apos;name&apos;, &apos;ssn&apos;, &apos;phone&apos;, &apos;email&apos;])]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Data Enhancement</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mode Collapse in GANs]]></title>
    <url>%2F2019%2F04%2F18%2Fmode-collapse-in-gan%2F</url>
    <content type="text"><![CDATA[Mode collapse, a failure case for GANs where the generator generate a limited diversity of samples, regardless of the input. But what causes the mode collapse? There are four reasons for that. The objective of GANs The generator, generates new data, while the discriminator evaluates it for authenticity but not for the diversity of generated instances. the generator can win by producing a polynomial number of training examples. And a low capacity discriminator cannot detect this process, thus, it cannot guide the generator to approximate the target distribution. Even if a high discriminator identifies and assigns the collapse part a low probability, then the generator will simply move from its collapsed output to focus on another fixed output point. Generator No matter the objective function is, if it only considers individual samples (without looking forward or backward) then the generator is not directly incentivised to produce diverse examples. From [1], standard GAN training corresponds exactly to updating the generator parameters using only the first term in this gradient because of a fixed discriminator during GAN training. Therefore, in standard GAN training, each generator update step is a partial collapse towards a delta function. $$\frac { \mathrm { d } f _ { K } \left( \theta _ { G } , \theta _ { D } \right) } { \mathrm { d } \theta _ { G } } = \frac { \partial f \left( \theta _ { G } , \theta _ { D } ^ { K } \left( \theta _ { G } , \theta _ { D } \right) \right) } { \partial \theta _ { G } } + \frac { \partial f \left( \theta _ { G } , \theta _ { D } ^ { K } \left( \theta _ { G } , \theta _ { D } \right) \right) } { \partial \theta _ { D } ^ { K } \left( \theta _ { G } , \theta _ { D } \right) } \frac { \mathrm { d } \theta _ { D } ^ { K } \left( \theta _ { G } , \theta _ { D } \right) } { \mathrm { d } \theta _ { G } }$$ Some methods have been proposed. Multiple generators and weight-sharing generators are developed to capture more modes of the distribution. Discriminator The mode collapse is often explained as gradient exploding of discriminator, which comes from the imbalance between the discriminator and the generator. For example, the technique of TTUR could help discriminator to keep its optimality. But some researchers believe that this is a desirable goal since a good discriminator can give good feedback and ignore the fact. In addition, the discriminator process each example independently, the generator depends on discriminator, thus no mechanism to tell the outputs of the generator to become more similar to each other. The idea from [2], that we could use mini-batch discrimination to help generator give better feedback A straightforward approach to handle multimodality is to take random noise vectors along with the conditional contexts as inputs, where the contexts determine the main content and noise vectors are responsible for variations.The noise vectors are ignored or of minor impacts, since cGANs pay more attention to learn from the high-dimensional and structured conditional contexts. Another question Mode collapse may happen only partially?since training is stochastic progress, the input of generator network will vary and the sample drawn from the real distribution will also vary But sometimes mode collapse is not all bad news. In style transfer using GAN, we are happy to convert one image to just a good one, rather than finding all variants. Indeed, the specialization in the partial mode collapse sometimes creates higher quality images. referrences [1]. Section 2.4 of Unrolled Generative Adversarial Networks[2]. Section 3.2 of Improved Techniques for Training GANs[3]. Mode Seeking Generative Adversarial Networks for Diverse Image Synthesis[4]. Improving Generalization and Stability of Generative Adversarial Networks]]></content>
      <categories>
        <category>Deep learning</category>
      </categories>
      <tags>
        <tag>GANs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[A Not-So-Gentle Introduction to Hyper-parameters Tuning]]></title>
    <url>%2F2019%2F04%2F17%2Fa-not-so-gentle-introduction-to-hyperparameters-tuning%2F</url>
    <content type="text"><![CDATA[Setting the hyper-parameters seems like a black art that requires years of experience to acquire. Currently, there are no simple and easy ways to set hyper-parameters, especifically, batch size, learning rate, momentum, and weight decay. A grid search or random search maybe sounds like a good idea. In this blog, I’d like to share you my idea from reading papers and my projects. Hyper-parametersBatch SizeLearning rate is maybe the most important hyper-parameters, but we choose batch size firstly because large batch size needs a large learning rate in most circumstances. A general principle is: use as a large batch size as possible to fit your CPU memory or/both GPU memory. There are several reasons: larger batch sizes permit the use of larger learning rates A constant number of iterations favors larger batch sizes However, small batch sizes add regularization while large batch sizes add less. So utilize it while balancing the proper amount of regularization. Learning RateWe will introduce the idea from [Cyclical Learning Rates for Training Neural Networks][1]: Cyclical Learning Rates. Instead of monotonically decreasing the learning rate, this method lets the learning rate cyclically vary between reasonable boundary values. The essence of this learning rate policy comes from the observation that increasing the learning rate might have a short term negative effect and yet achieve a longer term beneficial effect. This observation leads to the idea of letting the learning rate vary within a range of values rather than adopting a stepwise fixed or exponentially decreasing value. That is, one sets minimum and maximum boundaries and the learning rate cyclically varies between these bounds.%From Cyclical Learning Rates for Training Neural Networks An intuitive understanding of why CLR methods work comes from considering the loss function topology. Dauphin et al. argue that the difficulty in minimizing the loss arises from saddle points rather than poor local minima. Saddle points have small gradients that slow the learning process. However, increasing the learning rate allows for more rapid traversal of saddle point plateaus. But the question is that how can we find the Minimum bound and Maximum bound. There is a simple way to estimate the reasonable minimum and maximum boundary values with one training run of the network for a few epochs. It is a “LR range test”; run your model for several epochs while letting the learning rate increase linearly between low and high LR values. For example, set both the step size and maxiter to the same number of iterations. In this case, the learning rate will increase linearly from the minimum value to the maximum value during this short run. Next, plot the accuracy versus learning rate. Note the learning rate value when the accuracy starts to increase and when the accuracy slows, becomes ragged, or starts to fall. These two learning rates are good choices for bounds; that is, set $ lr_{base}$ to the first value and set $ lr_{max} $ to the latter value. MomentumSince learning rate is regarded as the most important hyper-parameter to tune then momentum is also important. Like learning rates, it is valuable to set momentum as large as possible without causing instabilities during training. The large learning rate can deal with local minimum but works fail when it comes to saddle point where momentum comes to rescue. The local minimum is like the following picture.In mathematics, a saddle point or minimax point is a point on the surface of the graph of a function where the slopes (derivatives) in orthogonal directions are all zero (a critical point), but which is not a local extremum of the function. Your first step from the very top would likely take you down, but then you’d be on a flat rice terrace. The gradient would be zero, and you’d have nowhere to go. To remedy this, we employ momentum - the algorithm remembers its last step and adds some psroportion of it to the current step. This way, even if the algorithm is stuck in a flat region, or a small local minimum, it can get out and continue towards the true minimum. In summary: when performing gradient descent, learning rate measures how much the current situation affects the next step, while momentum measures how much past steps affect the next step. Weights DecayWhen training neural networks, it is common to use “weight decay,” where after each update, the weights are multiplied by a factor slightly less than 1. This prevents the weights from growing too large and can be seen as gradient descent on a quadratic regularization term. But why? Large weights might correlate with certain patterns in the input data (x), this means that the model almost hard codes certain values. This then makes our training data fit well but our test data fit less well. The idea of weight decay is simple: to prevent overfitting, every time we update a weight $w$ with the gradient $∇J$ in respect to $w$, we also subtract from it $λ∙w$. This gives the weights a tendency to decay towards zero, hence the name. L2 is a type of weights decay.$$J ( W ; X , y ) + \frac { 1 } { 2 } \lambda \cdot | W | ^ { 2 }$$ But weights decay is not necessarily true for all gradient-base algorithms and was recently shown to not be the case for adaptive gradient algorithms, such as Adam. In addition, weight decay is not the only regularization technique. In the past few years, some other approaches have been introduced such as Dropout, Bagging, Early Stop, and Parameter Sharing which work very well in NNs. Takeoff Batch Size Use as a large batch size as possible to fit your memory Learning Rate Perform a learning rate range test to identify a “large” learning rate. Momentum Test with short runs of momentum values 0.99, 0.97, 0.95, and 0.9 to get the best value for momentum. If using the 1-cycle learning rate schedule, it is better to use a cyclical momentum (CM) that starts at this maximum momentum value and decreases with increasing learning rate to a value of 0.8 or 0.85. Weights Decay A grid search to determine the proper magnitude but usually does not require more than one significant figure accuracy.A more complex dataset requires less regularization so test smaller weight decay values, such as $10^{−4} $, $10^{−5} $, $10^{−6} $, 0.A shallow architecture requires more regularization so test larger weight decay values, such as $10^{−2} $, $10^{−3} $, $10^{−4} $. References[1]. Cyclical Learning Rates for Training Neural Networks[2]. A disciplined approach to neural network hyper-parameters]]></content>
      <categories>
        <category>Deep learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Weights Initialization]]></title>
    <url>%2F2019%2F04%2F17%2Fweights-initialization%2F</url>
    <content type="text"><![CDATA[weights initialization 知识点 和分享两个图像领域的架构（resnet 和 inception v3）。 Training a neural network consists of four steps: initialize weights and biases, forward propagation, compute the loss function and backward propagation. This blog mainly focuses on the first part: weights initialization. After completing this tutorial, you will know: Four main types of weights initialization How to choose between Xavier /zivier/ initialization and He initialization Types of Weights Initialization Initializing weights with zero When you set all weights in a neural network to zero, the derivative with respect to loss function is the same for every $ w$ in the same layer, thus all the weights have the same values in the subsequent iteration, which makes your model equivalent to a linear model. Initializing weights randomly You can get weights like this (Python): w =np.random.randn(layer_size[l],layer_size[l-1]) The weighs follows standard normal distribution while it can potentially lead to two issues: vanishing gradients and exploding gradients.下面的情况是很容易发生，因为网络中特征足够多（网络结构足够宽），所以 random 得到数值有足够的 coverage，所以就会出现 weights too small or too large 这种情况。 If the weights start too small, then the signal shrinks as it passes through each layer until it’s too small to be useful. If the weights start too large, then the signal grows as it passes through each layer until it’s too massive to be useful (big value in sigmoid function). Thus there are two necessary conditions to consider: The values of each activation layer won’t be zero The values of each activation layer won’t go into the area of saturation Xavier/Glorot Initialization For deep networks, we can use a heuristic to initialize the weights depending on the non-linear activation function. This applies to Xavier and He initialization. Xavier/Glorot Initialization initializes the weights in your network by drawing them from a distribution with zero mean and a specific variance.$$ { var } ( w _ { i } ) = \frac { 1 } { layer_{l-1}}$$ w=np.random.randn(layer_size[l],layer_size[l-1])*np.sqrt(1/layer_size[l-1]) In practice, it works better for layers with sigmoid or tanh function. He Initialization Using RELU or Leaky RELU is relatively robust to the vanishing/ exploding gradient issues compared with sigmoid function especially for networks that are not too deep. And it the case of Leaky RELU, it never has zero gradients. For RELU, we multiply the randomly generated values of $w$ by: $$\sqrt { \frac { 2 } { layer _ { [ l - 1 ] } } }$$ w=np.random.randn(layer_size[l],layer_size[l-1])*np.sqrt(2/layer_size[l-1]) Sometimes, we combine the idea of Xavier initialization and He initializaiton so the variance becomes the following: $$\sqrt { \frac { 2 } { layer _ { [ l - 1 ] } + \operatorname { layer } _ { [ l ] } } }$$ w=np.random.randn(layer_size[l],layer_size[l-1])*np.sqrt(2/(layer_size[l-1]+layer_size[l])) The idea behind this is that we set the weights neither too much bigger than 1 nor too much less than 1 so the gradients do not vanish or explode too quickly. TakeoffIn summary, the main difference in machine learning is the following: He initialization works better for layers with ReLu(s) activation. Xavier initialization works better for layers with sigmoid activation. Referrence:He initialization Xavier initialization]]></content>
      <categories>
        <category>Deep learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[CycleGAN & StyleGAN]]></title>
    <url>%2F2019%2F04%2F09%2Fcyclegan-stylegan%2F</url>
    <content type="text"><![CDATA[In the past few yeas, GANs have been used in lots of different applications such as generating synthetic data, style transfer, super-resolution and text2image generation. But we donn’t aim to give a overview of what GANs are made for. There are many great and detailed blogs for your understanding. What this post is about Main ideas of CycleGAN Keypoints in StyleGAN A Gentle Introduction of GANsWe assume the reader has some prior experience with neural networks. In addition, if you are familiar with GANs you can skip this section. The famous minimax objective function can be formulated as following:$$\min _ { \theta _ { g } } \max _ { \theta _ { d } } \left[ \mathbb { E } _ { x \sim p _ { d a t a } } \log D _ { \theta _ { d } } ( x ) + \mathbb { E } _ { z \sim p ( z ) } \log \left( 1 - D _ { \theta _ { d } } \left( G _ { \theta _ { g } } ( z ) \right) \right) \right)$$But in practical, the loss function cannot work very well. So we have alternative objective function: Gradient ascent on discriminator $$\max _ { \theta _ { d } } \left[ \mathbb { E } _ { x \sim p _ { d a t a } } \log D _ { \theta _ { d } } ( x ) + \mathbb { E } _ { z \sim p ( z ) } \log \left( 1 - D _ { \theta _ { d } } \left( G _ { \theta _ { g } } ( z ) \right) \right) \right]$$ Gradient ascent on generator$$\max _ { \theta _ { g } } \mathbb { E } _ { z \sim p ( z ) } \log \left( D _ { \theta _ { d } } \left( G _ { \theta _ { g } } ( z ) \right) \right)$$ The reasoning behind this can be found in original paper. Simplily speaking, we can get higher gradient signal for bad samples, which works much better in practice.bad case 的时候，使用原来的min-max function会使得学习率不够，使用 gradient ascent 就会好一些。 From Stanford CS231 Lecture 13 — Generative Models Main ideas of CycleGANCycleGAN was introduced in 2017 out of Berkeley, Unpaired Image-to-Image Translation Using Cycle-Coonsistent Adversarial Networks. This task is performed on unpaired data. Recent methods such as Pix2Pix depend on the availability of training examples where the samee data is availabel in both domains. However, CycleGAN is able to learning such pair information without one-to-one mapping between training data in source and target domains. Network ArchitectureWe build three networks. A generator $F$ to convert image $y$ to image $ \hat{x}$ A generator $G$ to convert image $\hat{x}$ to image $ \hat{y}$ A discriminator $D$ to identify real image or generated picture Simplified version of CycleGAN architecture can be showed in the following.The function $F$ and $G$ are generator network, which consists of encoder, transformer and decoder. Encoder is extracting the features from an image which is done by convolution networks. Each convolution layer leads to extraction of progressively higher level features. We would like to transform the feature emebdding of an image from domain $X$ to that of domain $Y$. So for this, authors have used 6 layers of ResNet blocks. ResNet block is a neural network layer which consists of two convolutiona layers when a residue of input ia added to the output. This is done to ensure properties of input of previous layers are available for later layers as well. ResNet block can be summarized in following imageThe decoder transfer embedding from $y$ back to original embedding $x$. Loss function（对于loss 实际上只有两个，只不过在第一个loss 中 x 和y 分别使用了两次，所以变成了两个公式。）There are two types of losses in CycleGAN. Besides adversarial loss, we have another loss named reconstruction cost.Adversarial loss is similaity to original GAN.$$\operatorname { Loss } _ { a d v } \left( F , D _ { x } , Y \right) = \frac { 1 } { m } \sum _ { i = 1 } ^ { m } \left( 1 - D _ { x } \left( F \left( y _ { i } \right) \right) \right) ^ { 2 }$$$$\operatorname { Loss } _ { a d v } \left( G , D _ { y } , X \right) = \frac { 1 } { m } \sum _ { i = 1 } ^ { m } \left( 1 - D _ { y } \left( G \left( x _ { i } \right) \right) \right) ^ { 2 }$$However, the adversarial loss alone is not sufficient to produce good looking images, which can not enfore that the input and output are recognizably the same. The cycle consistency loss addresses this issue. It relies on the expectation that if you convert an image to the other domain and back again, and then you should get back something similar to what you put in. It enforces that $F ( G ( x ) ) \approx x$ and $G ( F ( y ) ) \approx y$.$$\operatorname { Loss } _ { c y c } ( G , F , X , Y ) = \frac { 1 } { m } \sum _ { i = 1 } ^ { m } \left[ F \left( G \left( x _ { i } \right) \right) - x _ { i } \right] + \left[ G \left( F \left( y _ { i } \right) \right) - y _ { i } \right]$$We can get the full objective function by putting these two together.$$\mathcal { L } \left( G , F , D _ { x} , D _ { y } \right) = \mathcal { L } _ { \text { GAN } } \left( G , D _ { y } , X , Y \right) + \mathcal { L } _ { \text { GAN } } \left( F , D _ { x } , Y , X \right) + \lambda \mathcal { L } _ { \text { cyc } } ( G , F )$$ Keypoints of StyleGAN(想要解决的问题，ProGAN 是训练过程中不是一个 fine-tune 的过程，而是从一种状态到另一种状态的过程，所以styleGAN 想要 control specific features during training)The StyleGAN offers an upgrade version of ProGAN’s image generator, with a focus on the generator. ProGAN generates high-quality images but, in most models, its ability to control specific features of the generated image is very limited. In other word, the features are entangled and therefore attempting to tweak the input, even a bit, usually affects multiple features at the same time. A good illustrations would be following pictures.Compared with first version (ProGAN), the new generator includes several additions to ProGAN’s generators. Mapping NetworkThe mapping network’s goal in to encode the input vector into an intermediate vector whose different elements control different visual features, which consists of 8 fully connected layers and its output $w$ is of the same size as the input. Style Modules (AdaIN)The AdaIn (Adaptive Instance Normalization) module transfers the encoded information $w$, created by the mapping network, into the generated image. Removing traditional inputSince the encoded information $w$ from mapping network was used into generator image, the traditional random input can be omitted and replaced by constant values.]]></content>
      <categories>
        <category>Deep learning</category>
      </categories>
      <tags>
        <tag>GANs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python from Beginner to Master]]></title>
    <url>%2F2019%2F04%2F09%2Fpython-for-beginners%2F</url>
    <content type="text"><![CDATA[Basic Skillsmodulepython 文件可以当做主文件进行运行或者当做函数的集合进行调用。如果是前者一般是需要包含”__name__ ==”__main__”。对于后者就是在其他的python文件中进行调用。12import my_module # python文件from my_module import my_object packagesfrom packageroot.packagefolder.mod import my_object Note: Ensure each directory within your package import contains a file __init__.py python-pathpython2 和python3 使用不同的解释器，导致在一些函数命名和计算上有一些差别，最好在文件的开头标明使用的解释器。 while or forwhile : provide a condition and run the loop until the condition is not met. for: loop for a number of specific times; loop over items or characters of a string. examples:1234[Variable] AggregateFunction([Value] for [item] in [collection])x =[1, 2,3, 4, 5]y =[ 2*a for a in x if a%2 ==0]y &gt;&gt; [4, 8] 或者可以使用这样更加简洁的语句：12345678910111213 lambda arguments : expression fun1 = lambda a,b,c : a+b+c print(fun1(5,6,2))``` 来个比较复杂的例子:```python nums =[1,2,3,4,5] letters =['a', 'b', 'c','d','e'] # 两个for 循环也是要熟练 nums_letters =[[n, l] for n in nums for l in letters ] nums_letters break, continue, or passThe break, continue, and pass statements in Python will allow you to use for loops and while loops more effectively in your code.12345678910number = 0for number in range(10): number = number + 1 if number == 5: pass # pass here print('Number is ' + str(number))print('Out of loop') The pass statement occurring after the if conditional statement is telling the program to continue to run the loop and ignore the fact that the variable number evaluates as equivalent to 5 during one of its iterations. yield 可以用用作新的 if的测试, return results without termination The pass statement can create minimal classes, or act as a placeholder when working on new code and thinking on an algorithmic level before hammering out details.pass 的存在就是占坑，否则这个地方就是报错（IndentationError）。用于想要扩展的地方，但是现在还没有扩展。比如在某个method 下面或者某个 if 条件下。 yield or return经常被用来作为生成器。 when you call a normal function with a return statement the function is terminated whenever it encounters a return statement. In a function with a yield statement the state of the function is ‘saved’ from the last call and can be picked up the next time you call a generator function. for examples12345678910111213141516171819gen_exp =(x **2 for x in range(10) if x %2 ==0)for x in gen_exp: print(x)def my_gen(): for x in range(5): yield xgen1 =my_gen()next(gen1)def my_generator1(): yield 1 yield 2 yield 3 my_gen =my_generator1()# 使用 next() 进行调用下一个next(my_gen) recursionA function calling itself is known as recursion. list, tuples, or dictionary在python 中是使用频繁的data structure，这个是属于 collection 类别，里面放的是element. list: to add/update/ delete an item of a collection 123456my_list.append('C') #adds at the endmy_list[1] = 'D' #updatemy_list.pop(1) # removesmylist.pop() # 默认就是类似 栈的结构，就是pop 出来最后一个mylist.pop(0) # 当然也可以根据index 指定特定的 pop(delete) 的element or 12del mylist[1:2] # 通过指定 index range 然后进行delmylist.sort() # 支持 sorting 然后是从小到大, 这个sort是一种操作，inplace 的操作 tuples: tuples store a sequence of objects, the object can be of any type. Tuples are faster than lists. dictionary: It stores key/value pair objects. 12345678910111213141516171819202122232425262728293031 my_dict =dict() my_dict['key'] ='value' or my_dict =&#123;'key': 'value', ...&#125; for key in my_dict: # do something if 'some key' in my_dict: # do something``` ### Iterators###```pythonclass yrange: def __init__(self, n): self.i = 0 self.n = n # 这个表明是一个 iterator，make an object iterable def __iter__(self): return self # 这个next 函数就被当做是 class的属性，可以被外部调用的， def next(self): if self.i &lt; self.n: i = self.i self.i += 1 return i else: raise StopIteration() shallow vs deep copypython3 中：对于简单的数据类型，像int ，string，这种 copy() 和copy.deepcopy() 这两者都是相同的，copy 都是一种映射，都是相当于”值“ 上的引用；12345aa =2bb =aaprint(id(aa), id(bb)) # 相同bb =3print(id(aa), id(bb)) # 不同，因为把3 这个值重新复制给了变量bb 对于复杂的数据类型，使用deepcopy() 的时候，本来就是会重新拷贝一份到内存中。在python3 中copy() 和deepcopy() 这个是没有什么区别的。12345list1 =['a', 'b']list2 =list1 # 这个是引用，所以和list1 是相同的list3 =copy.copy(list1) # 这个id 和list1 不同list4 =copy.deepcopy(list1)# 这个id 和list1 不同 print(id(list1), id(list2), id(list3), id(list4)) object oriented design1234567891011121314151617181920212223242526272829303132333435363738394041424344454647 class ParentClass: def my_function(self): print 'I am here' class SubClass1(ParentClass): class SubClass2(ParentClass): ``` 对于多继承的支持 （接口）```python class A(B,C): #A implments B and C``` 如果想要call parent class function then you can dp:```python super(A, self).funcion_name()``` ### garbage collection###all the objects in python are stored in a heap space. Python has an in-built garbage collection mechanism. Memory management in Python involves a private heap containing all Python objects and data structures. The management of this private heap is ensured internally by the Python memory manager.In computer science, a heap is a specialized tree-based data structure which is essentially an almost complete[1] tree that satisfies the heap property: if P is a parent node of C, then the key(the value) of P is either greater than or equal to (in a max heap) or less than or equal to (in a min heap) the key of C.### try...catch###```python # raise exceptions try: raise TyeError except: print('exception') # catching exceptions try: do_something() except: print('exception') # try/ catch /finally try: do_something() except TypeError: print('exception') finally: close_connections() 内置函数中的 all() 和 any()这两个函数的参数都是iterable，也就是为list或者tuple.all() 函数: “全‘真’为True，有‘假’为False” ; 当iterable为空的时候，函数返回值为Trueany() “全‘假’为False，有‘真’为True”. 当iterable为空的时候，函数返回值为False 给出all () 函数的一个简单的例子 123456789101112131415161718&gt;&gt;&gt; all(['a', 'b', 'c', 'd']) # 列表list，元素都不为空或0True&gt;&gt;&gt; all(['a', 'b', '', 'd']) # 列表list，存在一个为空的元素False&gt;&gt;&gt; all([0, 1，2, 3]) # 列表list，存在一个为0的元素False &gt;&gt;&gt; all(('a', 'b', 'c', 'd')) # 元组tuple，元素都不为空或0True&gt;&gt;&gt; all(('a', 'b', '', 'd')) # 元组tuple，存在一个为空的元素False&gt;&gt;&gt; all((0, 1, 2, 3)) # 元组tuple，存在一个为0的元素False &gt;&gt;&gt; all([]) # 空列表True&gt;&gt;&gt; all(()) # 空元组True 总结：all()：”有‘假’为False，全‘真’为True，iterable为空是True”any()：”有‘真’为True，全‘假’为False，iterable为空是False” sort() sorted() 函数在python3 中 sorted() 取消了对cmp 的支持, 所以只能使用 sort() 函数， 比如下面语句，表示先是按照第一个元素进行升序，然后在第一个元素相同的条件下，按照第二个元素进行降序。灰常nice的一种写法。12# type envelopes: List[List[int]]envelopes.sort(key =lambda x:(x[0], -x[1])) list.sort( ) 是in-place 操作， 在python2 中 sorted() 是一种有返回排序好的数组的操作。 Advanced FeaturesLet’s move on to advanced features. Lambda functionsA Lambda Function is a small, anonymous function — anonymous in the sense that it doesn’t actually have a name. A lambda function can take any number of arguments, but must always have only one expression: 1234x = lambda a, b : a * b print(x(5, 6)) # prints '30' # 匿名函数也是函数，调用的时候使用这样的方式 x = lambda a : a*3 + 3 print(x(3)) # prints '12' MapsMap() is a built-in Python function used to apply a function to a sequence of elements like a list or dictionary. It’s a very clean and most importantly readable way to perform such an operation.相对于 lambda, map 使用的频率更少了。 最后返回的是一个list。 1234567891011def square_it_func(a): return a * ax = map(square_it_func, [1, 4, 7])print(x) # prints '[1, 16, 49]'def multiplier_func(a, b): return a * bx = map(multiplier_func, [1, 4, 7], [2, 5, 8])print(x) # prints '[2, 20, 56]' FilteringThe Filter built-in function is quite similar to the Map function in that it applies a function to a sequence (list, tuple, dictionary). The key difference is that filter() will only return the elements which the applied function returned as True. 123456789101112 # Our numbersnumbers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]# Function that filters out all numbers which are odddef filter_odd_numbers(num): if num % 2 == 0: return True else: return Falsefiltered_numbers = filter(filter_odd_numbers, numbers)print(filtered_numbers)# filtered_numbers = [2, 4, 6, 8, 10, 12, 14] 123456from itertools import *def check_for_drop(x): print ('Checking: ', x) return (x &gt; 5)for i in dropwhile(check_for_drop, [2, 4, 6, 8, 10, 12]): print ('Result: ', i) Itertools123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263from itertools import *# zip 就是一块访问的那种形式，返回的是一个tuple 数据类型# zip ,joing two lists into a list of tuples# Easy joining of two lists into a list of tuplesfor i in zip([1, 2, 3], ['a', 'b', 'c']): print (i)# ('a', 1)# ('b', 2)# ('c', 3)# 就是一个count() 计数功能# The count() function returns an interator that # produces consecutive integers, forever. This # one is great for adding indices next to your list # elements for readability and convenience# in python3, no need to import izip, use zip directly# 这个 count() 只有在这里才有意义，如果只是单独调用，没有感觉有多大的意义for i in zip(count(1), ['Bob', 'Emily', 'Joe']): print (i)# (1, 'Bob')# (2, 'Emily')# (3, 'Joe') # check ， becomes false for the first time 这个条件很关键, 可以理解成只是找到第一个false 的条件，然后就不再执行该函数# The dropwhile() function returns an iterator that returns # all the elements of the input which come after a certain # condition becomes false for the first time. def check_for_drop(x): print 'Checking: ', x return (x &gt; 5)for i in dropwhile(check_for_drop, [2, 4, 6, 8, 10, 12]): print 'Result: ', i# 注意理解这个输出# Checking: 2# Result: 2# Result: 4# Result: 6# Result: 8# Result: 10# Result: 12# 我的理解这个 groupby 就和数据库中的groupby 是相同的效果# The groupby() function is great for retrieving bunches# of iterator elements which are the same or have similar # propertiesfrom itertools import groupbythings = [("animal", "bear"), ("animal", "duck"), ("plant", "cactus"), ("vehicle", "speed boat"), ("vehicle", "school bus")]for key, group in groupby(things, lambda x: x[0]): for thing in group: print ("A %s is a %s." % (thing[1], key))#A bear is a animal.#A duck is a animal.#A cactus is a plant.#A speed boat is a vehicle.#A school bus is a vehicle. GeneratorGenerator functions allow you to declare a function that behaves like an iterator, i.e. it can be used in a for loop. This greatly simplifies your code and is much more memory efficient than a simple for loop. 12345678910111213141516numbers = list()# range()for i in range(1000): numbers.append(i + 1)total = sum(numbers)# (2) Using a generatordef generate_numbers(n): num = 0 while num &lt; n: yield num # 这个yield 之后，函数并没有结束，不像 return 那种函数 num += 1total = sum(generate_numbers(1000))print(total)total = sum(range(1000 + 1))print(total) Decorator简单地说，decorator就像一个wrapper一样，在函数执行之前或者之后修改该函数的行为，而无需修改函数本身的代码，这也是修饰器名称的来由。总的来说，decorator在你希望在不修改函数本身代码的前提下扩展函数的功能时非常有用。 python 中函数的”人设“, function 也是一种对象，内部函数可以访问外部的 function的变量，但是权限是”只读“。 123456789def greet(name): def get_message(): return "Hello "+name result = get_message()+name return resultprint(greet("John")) 于是乎， 1234567891011def get_text(name): return "lorem ipsum, &#123;0&#125; dolor sit amet".format(name)def p_decorate(func): def func_wrapper(name): return "&lt;p&gt;&#123;0&#125;&lt;/p&gt;".format(func(name)) return func_wrappermy_get_text = p_decorate(get_text)print(my_get_text("John")) 输出：1&lt;p&gt;Outputs lorem ipsum, John dolor sit amet \&lt;/p&gt; 这就是我们的第一个修饰器。一个函数接收另一个函数作为参数，并且产生一个新的函数，注意观察是如何调用和 调用的顺序，体会 decorator 产生的背景，是可以简化这个调用过程的。 下面代码和上面是相同的功能，p_decorate 就像是 function get_text() 的一个外套， 其作为一种输入到 p_decorate() 中。 12345678910def p_decorate(func): def func_wrapper(name): return "&lt;p&gt;&#123;0&#125;&lt;/p&gt;".format(func(name)) return func_wrapper@p_decoratedef get_text(name): return "lorem ipsum, &#123;0&#125; dolor sit amet".format(name)print (get_text("John")) 输出结果： 1&lt;p&gt;lorem ipsum, John dolor sit amet&lt;/p&gt; 在给个例子，理解调用过程。 123456789def hello_decorator(original_fn): def decorator_fn(): print("Hello from new") original_fn() # original function must be invoked return decorator_fn@hello_decoratordef hello(): print("Hello from original") 输出结果：12Hello from newHello from original 另外，一个函数是可以添加多个 修饰器的，并且修饰器的顺序也是有关系的，如果顺序不同，那么最后的结果也是不同的。 感觉装饰器很难的原因在于没有理清它的逻辑关系，本质上装饰器也是函数，但它是对核心程序的闭包封装，在原有的基础上增加更多的功能。细细回顾几遍上面的例子能够加深对装饰器的理解。 使用修饰器实现单例模式 单例是一种设计模式，应用该模式的类只会生成一个实例。这种方式是可以代替全局变量的。比如一些配置、日志等只需要初始化一次的文件，就可以使用这种方式。 123456789101112131415161718192021# 函数修饰器实现单例def singleton(cls): _instance =&#123;&#125; def inner(): if cls not in _instance: _instance[cls] =cls return _instance[cls] # 函数调用 return inner@singletonclass Cls(object): def __init__(self): passcls1 =Cls()cls2 =Cls()print(id(cls1) ==id(cls2)) List Comprehension常见的几种形式：(An iterable is something you can loop over) list comprehensions vs loops: list comprehensions are more efficient both computationally and coding space Every list comprehension can be rewritten as a for loop, but not every for loop can be rewritten as a list comprehension. 从优化的角度 list comprehensions是优于 for loop 中的if else 操作的。因为前者是 predicatable pattern 是可以预测的。However, keep in mind that list comprehensions are faster because they are optimized for the Python interpreter to spot a predictable pattern during looping. a small code demo:在于使用功能 timeit libary 进行函数的计时比较。12345678910111213import timeitdef squares(size): result = [] for number in range(size): result.append(number * number) return resultdef squares_comprehension(size): return [number * number for number in range(size)] print(timeit.timeit("squares(50)", "from __main__ import squares", number=1_000_000))print(timeit.timeit("squares_comprehension(50)", "from __main__ import squares_comprehension", number=1_000_000)) more complex list comprehensions: 这种if 的写法 是两个进行并列的。其实可以写成 123456numbers = [1, 2, 3, 4, 5, 6, 18, 20]squares = [number for number in numbers if number % 2 == 0 if number % 3 == 0]# orsquares = [number for number in numbers if number % 2 == 0 and number % 3 == 0]print(squares)# output: [6, 18] 在 output expression 中，也是可以使用 if else 进行进一步输出筛选。12345numbers = [1, 2, 3, 4, 5, 6, 18, 20] squares = ["small" if number &lt; 10 else "big" for number in numbers if number % 2 == 0 if number % 3 == 0] print(squares)ouput: ['small', 'big'] converting nested loops into list comprehension代码功能： 都是把二维的 matrix 转成了一个 list （flattened）123456matrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]flattened = []for row in matrix: for item in row: flattened.append(item)print(flattened) 注意这个顺序，先是row in matrix 然后是 item in row.123 matrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]flattened = [item for row in matrix for item in row] print(flattened) ouput matric from nested list comprehensions:12matrix = [[item for item in range(5)] for row in range(3)]print(matrix) 对于 dictionary 的支持： 主要是 dict1.items() 和 key, value 的使用123prices = &#123;"beer": 2, "fish": 5, "apple": 1&#125;float_prices = &#123;key:float(value) for key, value in prices.items()&#125;print(float_prices) 从代码的角度，可以看出，操作和最后的返回的形式是没有很大的关系，上面是 [], 这个是 {}, 分别对应的是 list 和 set 两种不同的格式。123numbers = [10, 10, 20, 30, 12, -20, 0, 1]unique_squares = &#123;number**2 for number in numbers&#125;print(unique_squares) python operatorsPython Arithmetic Operator Addition(+) Subtraction(-) Multiplication(*) Division(/) Exponentiation(**) Floor Division(//) 向下取整 Modulus(%) Python Relational Operator Less than(&lt;) Greater than(&gt;) Less than or equal to(&lt;=) Greater than or equal to(&gt;=) Equal to(= =) Not equal to(!=) Python Assignment Operator Assign(=) Add and Assign(+=) Subtract and Assign(-=) Divide and Assign(/=) Divide and Assign(/=) Modulus and Assign(%=) Exponent and Assign(**=) Floor-Divide and Assign(//=) Python Logical Operator(会有某种机制简化运算，比如 condition1 or condition2 ，如果condition1 是正确的，那么最后的结果就是正确的。)用于逻辑判断 and or not Python Membership Operator in ,not in Python Identity Operator is, is not , Python Bitwise Operator (这其中的 | 表现的是一种二级制’ 和’的 关系，如果在二进制下，0 | 1 那么就是1 ) 属于集合操作。 Binary AND(&amp;) Binary OR(|) Binary XOR(^) Binary XOR(^) Binary Left-Shift(&lt;&lt;) Binary Right-Shift(&gt;&gt;) Working with files Working with CSV, Json and XML Over the years, the list of possible formats that you can store your data in has grown significantly. But, there are 3 that dominate in their everyday usage: CSV, JSON, and XML. In this article, I’m going to share with you the easiest ways to work with these 3 popular data formats in Python! 有两种方式去读写 csv file：一种是 pd.read_csv() ，一种是built-in 的library 中的库函数之前一直使用的pd.read_csv(), 现在才发现python 有built-in 的library。We can do both read and write of a CSV using the built-in Python csv library. Usually, we’ll read the data into a list of lists. python in-built function.12345678910111213141516171819import csv filename = "my_data.csv"fields = [] rows = [] with open(filename, 'r') as csvfile: csvreader = csv.reader(csvfile) # 如果单单是这个for，那么内存是消耗比较大的 # fields = csvreader.next() for row in csvreader: rows.append(row)for row in rows[:5]: print(row)# Writing to csv file with open(filename, 'w+') as csvfile: csvwriter = csv.writer(csvfile) csvwriter.writerow(fields) csvwriter.writerows(rows) 12345678910111213141516171819202122import pandas as pdfrom dicttoxml import dicttoxmlimport json# Building our dataframedata = &#123;'Name': ['Emily', 'Katie', 'John', 'Mike'], 'Goals': [12, 8, 16, 3], 'Assists': [18, 24, 9, 14], 'Shots': [112, 96, 101, 82] &#125;df = pd.DataFrame(data, columns=data.keys())# Converting the dataframe to a dictionary# Then save it to filedata_dict = df.to_dict(orient="records")with open('output.json', "w+") as f: json.dump(data_dict, f, indent=4)# Converting the dataframe to XML# Then save it to filexml_data = dicttoxml(data_dict).decode()with open("output.xml", "w+") as f: f.write(xml_data) 12345678910111213141516171819import jsonimport pandas as pd# Read the data from file# We now have a Python dictionarywith open('data.json') as f: data_listofdict = json.load(f) # We can do the same thing with pandasdata_df = pd.read_json('data.json', orient='records')# We can write a dictionary to JSON like so# Use 'indent' and 'sort_keys' to make the JSON# file look nicewith open('new_data.json', 'w+') as json_file: json.dump(data_listofdict, json_file, indent=4, sort_keys=True)# And again the same thing with pandasexport = data_df.to_json('new_data.json', orient='records') text filepython 中 open( mode =’rt’) 的选项：w,r,wt,rt都是python里面文件操作的模式。w是写模式，r是读模式。t是windows平台特有的所谓text mode(文本模式）,区别在于会自动识别windows平台的换行符。类Unix平台的换行符是\n，而windows平台用的是\r\n两个ASCII字符来表示换行，python内部采用的是\n来表示换行符。rt模式下，python在读取文本时会自动把\r\n转换成\n.wt模式下，Python写文件时会用\r\n来表示换行。 参考资料：https://towardsdatascience.com/the-easy-way-to-work-with-csv-json-and-xml-in-python-5056f9325ca9 输入和输出python2 有 raw_input() 和 input() 函数，前者把所有的接收值当做string，如果想要用int，那么需要自己进行转换。input() 如果得到int，那么就是int，string 类型就是string 类型。python3 中只有input() 函数，所有的接收都是 string，需要自己进行转换。 python3 中的input() 函数就是python2 中的raw_input() 函数。下面的代码使用 python3 实现。 1234567891011121314151617181920212223242526272829if __name__ =="__main__": # 输入输出 n*n 的arrar """ n =int(input()) arr =[[0]*n] *n # 在python3 中需要 exactly 的注意这种分割，输入的时候要及其的小心 for i in range(n): arr[i] = input().split(" ") arr[i] =[int(a) for a in arr[i]] # 转成 int 类型 print(arr) """ # 输入和输出 n*m 的数组 n =int(input()) m =int(input()) arr =[[0]*m]*n for i in range(n): arr[i] =input().split(" ") arr[i] =[int(a) for a in arr[i]] print(arr) 其他python 中 in 操作 在不同的数据集合中的时间复杂度这个是取决于操作的数据结构： list (tuple) -average: O(N) set/dict- average: O(1), worst: O(N) (如果 dictionary 所有的值都相同的话，那么时间复杂度就是 O(n)) The O(n) worst case for sets and dicts is very uncommon, but it can happen if hash is implemented poorly. This only happens if everything in your set has the same hash value. 操作 平均情况 最坏情况 说明 列表 list O(n) O(n) list 是由数组实现的 字典 dict O(1) O(n) 集合 set O(1) O(n) 内部实现和 dict 很像]]></content>
      <categories>
        <category>Python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[The Evaluation of Sentence Similarity]]></title>
    <url>%2F2019%2F04%2F06%2FThe-evaluation-of-sentence-similarity%2F</url>
    <content type="text"><![CDATA[I am trying to write my first english blog based on two reasons: First, the data set used in this blog is english; Second, I’d like to expand my reach and attract more audiences, although I should admit that nobody cares. DataInitially I want to use chinese corpus, but I cannot find a proper one. The data should sound like this one: word1 word2 similarity score阿拉伯人 阿拉伯 7.2畜产 农业 5.6垂涎 崇敬 3.4次序 秩序 4.7定心丸 药品 4.3房租 价格 5.2翡翠 宝石 6.7高科技 技术 7.5购入 购买 8.5观音 菩萨 8.2归并 合并 7.7 not like this: 为何我无法申请开通花呗信用卡收款 支付宝开通信用卡花呗收款不符合条件怎么回事 1花呗分期付款会影响使用吗 花呗分期有什么影响吗 0为什么我花呗没有临时额度 花呗没有临时额度怎么可以负 0能不能开花呗老兄 花呗逾期了还能开通 0我的怎么开通花呗收钱 这个花呗是个什么啥？我没开通 我怎么有账单 0蚂蚁借呗可以停掉么 蚂蚁借呗为什么给我关掉了 0我想把花呗功能关了 我去饭店吃饭，能用花呗支付吗 0为什么我借呗开通了又关闭了 为什么借呗存在风险 0支付宝被冻了花呗要怎么还 支付功能冻结了，花呗还不了怎么办 1 If you can find the dataset where ‘similarity score’ is double, please donot hesitate to email me. So, the choice has to be enlgish corpus. The dataset used in this experiment are STSbenchmark and SICK data. The SICK data contains 10,000 sentence paris labeled with semantic relatedness and entailment relation. Similarity MethodsBaselineAs the baseline, we just take the embedding of the words in sentence, and compute the average, weighted by frequency of each word. 1234567891011121314151617181920212223242526272829303132def run_avg_benchmark(sentences1, sentences2, model=None, use_stoplist=False, doc_freqs=None): if doc_freqs is not None: N = doc_freqs["NUM_DOCS"] sims = [] for (sent1, sent2) in zip(sentences1, sentences2): tokens1 = sent1.tokens_without_stop if use_stoplist else sent1.tokens tokens2 = sent2.tokens_without_stop if use_stoplist else sent2.tokens tokens1 = [token for token in tokens1 if token in model] tokens2 = [token for token in tokens2 if token in model]l if len(tokens1) == 0 or len(tokens2) == 0: sims.append(0) continue tokfreqs1 = Counter(tokens1) tokfreqs2 = Counter(tokens2) weights1 = [tokfreqs1[token] * math.log(N / (doc_freqs.get(token, 0) + 1)) for token in tokfreqs1] if doc_freqs else None weights2 = [tokfreqs2[token] * math.log(N / (doc_freqs.get(token, 0) + 1)) for token in tokfreqs2] if doc_freqs else None embedding1 = np.average([model[token] for token in tokfreqs1], axis=0, weights=weights1).reshape(1, -1) embedding2 = np.average([model[token] for token in tokfreqs2], axis=0, weights=weights2).reshape(1, -1) sim = cosine_similarity(embedding1, embedding2)[0][0] sims.append(sim) return sims Smooth Inverse FrequencyThe baseline, like we did before, is very simple and crude of computing sentence embedding. Word frequency cannot reliably reflect its importance to sentence, semantically speaking. Smooth Inverse Frequency (SIF) tries to solve this problem. SIF is very similar to the weighted average we used before, with the difference that it’s weighted by this formular.$$\operatorname { SIF } ( w ) = \frac { a } { ( a + p ( w ) )}$$where $a$ is a hyper-parameter (set to 0.001 by default) and $ p(w)$ is the estimated word frequency in the corpus. (这个权重和 TF或者 IDF 都是不相同的) we need to perform common component removal: subtract from the sentence embedding obtained above the first principal component of the matrix. This corrects for the influence of high-frequency words that have syntactic or dicourse function, such as ‘but’, ‘and’, etc. You can find more information from this paper. 因为这个的输入直接是句子，没有经过分词的处理，所以不免有 but and 这类的词汇出现。 12345678910111213141516171819202122232425262728293031323334353637def remove_first_principal_component(X): svd = TruncatedSVD(n_components=1, n_iter=7, random_state=0) svd.fit(X) pc = svd.components_ XX = X - X.dot(pc.transpose()) * pc return XXdef run_sif_benchmark(sentences1, sentences2, model, freqs=&#123;&#125;, use_stoplist=False, a=0.001): total_freq = sum(freqs.values()) embeddings = [] # SIF requires us to first collect all sentence embeddings and then perform # common component analysis. for (sent1, sent2) in zip(sentences1, sentences2): tokens1 = sent1.tokens_without_stop if use_stoplist else sent1.tokens tokens2 = sent2.tokens_without_stop if use_stoplist else sent2.tokens tokens1 = [token for token in tokens1 if token in model] tokens2 = [token for token in tokens2 if token in model] weights1 = [a / (a + freqs.get(token, 0) / total_freq) for token in tokens1] weights2 = [a / (a + freqs.get(token, 0) / total_freq) for token in tokens2] embedding1 = np.average([model[token] for token in tokens1], axis=0, weights=weights1) embedding2 = np.average([model[token] for token in tokens2], axis=0, weights=weights2) embeddings.append(embedding1) embeddings.append(embedding2) embeddings = remove_first_principal_component(np.array(embeddings)) sims = [cosine_similarity(embeddings[idx * 2].reshape(1, -1), embeddings[idx * 2 + 1].reshape(1, -1))[0][0] for idx in range(int(len(embeddings) / 2))] return sims Google Sentence EncoderInferSent is a pre-trained encoder that produces sentence embedding, which opensourced by Facebook. The Google Sentence Encoder is Google’s answer to Facebook’s InferSent. In contrast to InferSent, the Google Sentence Encoder was trained on a combination of unsupervised data and supervised data (SNLI corpus), which tends to give better results. The codes can be used in Google Jupyter Notebook 12345678910111213141516171819202122232425import tensorflow_hub as hubtf.logging.set_verbosity(tf.logging.ERROR)embed = hub.Module("https://tfhub.dev/google/universal-sentence-encoder/1")def run_gse_benchmark(sentences1, sentences2): sts_input1 = tf.placeholder(tf.string, shape=(None)) sts_input2 = tf.placeholder(tf.string, shape=(None)) sts_encode1 = tf.nn.l2_normalize(embed(sts_input1)) sts_encode2 = tf.nn.l2_normalize(embed(sts_input2)) sim_scores = tf.reduce_sum(tf.multiply(sts_encode1, sts_encode2), axis=1) with tf.Session() as session: session.run(tf.global_variables_initializer()) session.run(tf.tables_initializer()) [gse_sims] = session.run( [sim_scores], feed_dict=&#123; sts_input1: [sent1.raw for sent1 in sentences1], sts_input2: [sent2.raw for sent2 in sentences2] &#125;) return gse_sims Experiments1234567891011121314def run_experiment(df, benchmarks): sentences1 = [Sentence(s) for s in df['sent_1']] sentences2 = [Sentence(s) for s in df['sent_2']] pearson_cors, spearman_cors = [], [] for label, method in benchmarks: sims = method(sentences1, sentences2) pearson_correlation = scipy.stats.pearsonr(sims, df['sim'])[0] print(label, pearson_correlation) pearson_cors.append(pearson_correlation) spearman_correlation = scipy.stats.spearmanr(sims, df['sim'])[0] spearman_cors.append(spearman_correlation) return pearson_cors, spearman_cors Helper function: 1234567891011import functools as ftbenchmarks = [ ("AVG-GLOVE", ft.partial(run_avg_benchmark, model=glove, use_stoplist=False)), ("AVG-GLOVE-STOP", ft.partial(run_avg_benchmark, model=glove, use_stoplist=True)), ("AVG-GLOVE-TFIDF", ft.partial(run_avg_benchmark, model=glove, use_stoplist=False, doc_freqs=doc_frequencies)), ("AVG-GLOVE-TFIDF-STOP", ft.partial(run_avg_benchmark, model=glove, use_stoplist=True, doc_freqs=doc_frequencies)), ("SIF-W2V", ft.partial(run_sif_benchmark, freqs=frequencies, model=word2vec, use_stoplist=False)), ("SIF-GLOVE", ft.partial(run_sif_benchmark, freqs=frequencies, model=glove, use_stoplist=False)),] Results123import matplotlib.pyplot as pltplt.rcParams['figure.figsize'] = (20,13)spearman[['AVG-GLOVE', 'AVG-GLOVE-STOP','AVG-GLOVE-TFIDF', 'AVG-GLOVE-TFIDF-STOP','GSE']].plot(kind="bar").legend(loc="lower left") Take Off Smooth Inverse Frequency methods are better than baseline, no matter with word2vec or Glove embeddings. Google Sentence Encoder has the similar performance as Smooth Inverse Frequency. Using tf-idf weights does not help and using a stoplist looks like a reasonable choice. Pearson CorrelationSpearman Correlation Full codes can be found in here. 复习笔记 TF-IDF 和 SIF三者的差别 SIF的计算公式：$$\operatorname { SIF } ( w ) = \frac { a } { ( a + p ( w ) )}$$$a$ 是超参数，一般设置为0.001，保证…; $p(w)$ 是word 在预料中出现的频数。 TF 的计算公式： $$ 词频(TF) = 某个词在文章中出现的次数( 频数) $$ 可以进一步标准化（减少文章长度的影响） $$ 词频( TF) = \frac{某次在文中出现的次数}{文章的总词语数} $$ $$ 逆文档频率 (IDF) = log(\frac{语料中的文档总数}{ 包含该词的文档数 +1}) $$]]></content>
      <categories>
        <category>NLP</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[深度网络中的碎碎念]]></title>
    <url>%2F2019%2F03%2F26%2F%E6%B7%B1%E5%BA%A6%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E7%A2%8E%E7%A2%8E%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[介绍深度网络中小的基本概念，比如权重初始化、激活函数和优化器 和常见的技术比如 dropout。 Weights Initializationweights 的初始化在网络的训练起到重要的作用，初始化的好坏能够直接影响到网络是否可以正常收敛。这里的初始化都是指的是weights初始化。bias 表示偏差，噪声，作用在于企图去描述真实的分布（高斯分布），通过引入随机性来表示这个是具有推广性的。主要介绍常见的三种初始化方法和选择方法。 Here’s another trick — before squishing our scalar value (called an activation) into the sigmoid function, we can add a little number called the bias, b. There will be two biases, one for each user and one for each movie. Random Initialization总结来说就是容易出现梯度消失和梯度爆炸，尤其是在layer_size（特征数量） 比较大的时候。从均值方差的角度进行分析。 a) If weights are initialized with very high values the term np.dot(W,X)+becomes significantly higher and if an activation function like sigmoid() is applied, the function maps its value near to 1 where slope of gradient changes slowly and learning takes a lot of time.b) If weights are initialized with low values it gets mapped to 0, where the case is same as above. 1w =np.random.randn(layer_size[l],layer_size[l-1]) 另外的表述方式： If the weights start too small, then the signal shrinks as it passes through each layer until it’s too small to be useful.If the weights start too large, then the signal grows as it passes through each layer until it’s too massive to be useful (big value in sigmoid function). Xavier initialization /ˈzeɪvjər/sFor deep networks, we can use a heuristic to initialize the weights depending on the non-linear activation function. This applies to Xavier and He initialization. Xavier/Glorot Initialization initializes the weights in your network by drawing them from a distribution with zero mean and a specific variance.$$ { var } ( w _ { i } ) = \frac { 1 } { layer_{l-1}}$$ 1w=np.random.randn(layer_size[l],layer_size[l-1])*np.sqrt(1/layer_size[l-1]) In practice, it works better for layers with sigmoid or tanh function. 总的思想原则：They set the weights neither too much bigger that 1, nor too much less than 1.就是本来就是在 (0,1) 标准正太分布出来，然后进行了进一步的约束条件。思想当特征数量越大的时候，weights 的波动情况是成反比的，最后的weights 数值越接近于均值附近。 He InitializationUsing RELU or Leaky RELU is relatively robust to the vanishing/ exploding gradient issues compared with sigmoid function especially for networks that are not too deep. And it the case of Leaky RELU, it never has zero gradients. For RELU, we multiply the randomly generated values of $w$ by: $$\sqrt { \frac { 2 } { layer _ { [ l - 1 ] } } }$$1w=np.random.randn(layer_size[l],layer_size[l-1])*np.sqrt(2/layer_size[l-1]) Sometimes, we combine the idea of Xavier initialization and He initializaiton so the variance becomes the following: $$\sqrt { \frac { 2 } { layer _ { [ l - 1 ] } + \operatorname { layer } _ { [ l ] } } }$$ 12w=np.random.randn(layer_size[l],layer_size[l-1])*np.sqrt(2/(layer_size[l-1]+layer_size[l]))# 代码中 random() 中的两个参数是 shape，最后的np.sqrt 标准差 The idea behind this is that we set the weights neither too much bigger than 1 nor too much less than 1 so the gradients do not vanish or explode too quickly. 所以上述的初始化从数学的角度去理解： 从(0,1) 标准正太分布 转换成了 (0, np.sqrt(2/ size_l -1)) 这样的分布，就是你网络结构是越宽，那么这个方差就是越小的，最后的结果是越集中的，就越集中的 均值 u 左右。从图像的角度看，方差越大，图像越矮胖；方差越小，图像越瘦高。 Takeoff In summary, the main difference in machine learning is the following: He initialization works better for layers with ReLu(s) activation. Xavier initialization works better for layers with sigmoid activation. Activation function总的说可以分为线性和非线性的激活函数， activation function 的作用 就是对于网络的输出 说yes or no. It maps the resulting values in between 0 to 1 or -1 to 1 etc. Sigmoid function (Logistic Activation)the only reason why we use sigmoid is because it exists between 0 to 1. 这个非常有利于 predict probability. 因为自然映射到 0 是不存在 然后1 是存在。而当多分类的时候，使用softmax。 Tanh function The advantage is that the negative inputs will be mapped strongly negative and the zero inputs will be mapped near zero in the tanh graph. Relu (Rectified Linear Unit) Activation本质上是分段函数。 range: [0, infinity]. The function and its derivative both are monotonic. Leaky Relu每当一个伟大的东西产出，总会伴随着比较明显的错误，然后紧跟着就是有一个 rectified(improved) 这种版本。这个相比之前就是修正了 当输入是负数的时候，怎么办的问题。 Softmax这个需要从数学的角度去理解，数学公式就是下面。有一个真实的例子： 如果一张”4” 的图片，输出一个网络，最后是softmax 激活函数，那么最后得到的 “4” 的概率是最大的，但是也是有其他的可能性存在的，这个就是softmax 的效果。 最主要的功能是 convert number into probabilities. 这种效果，不像sigmoid 那样有很明确的数学公式。 $$\sigma ( z ) _ { j } = \frac { e ^ { z _ { j } } } { \sum _ { k = 1 } ^ { K } e ^ { z _ { k } } }$$ Softmax function, a wonderfulactivation function that turns numbers aka logits into probabilities that sum to one.Remember the takeaway is: the essential goal of softmax is to turn numbers into probabilities. OptimizerGradient Descent最基本的应该是 gradient descent，这个算是开山鼻祖。给出了每次梯度变化的方向 Momentum作用：Momentum是为了对冲mini-batch带来的抖动。 个人更喜欢将其翻译成动量，这个gradient 考虑之前的gradient 的方向和大小，有点像动量的的含义。不仅考虑了current step, 而且accumulates gradient of the past steps. A very popular technique that is used along with SGD is called Momentum. Instead of using only the gradient of the current step to guide the search, momentum also accumulates the gradient of the past steps to determine the direction to go. 好处在于: most recent is weighted than the less recent onesthe weightage of the most recent previous gradients is more than the less recent ones.for example: RMSprop作用： RMSprop是为了对hyper-parameter进行归一。 RMSprop, or Root Mean Square Propogation has an interesting history. RMSProp also tries to dampen the oscillations, but in a different way than momentum. Adam这种套路总是一样的，总是有一个中和者，关键是如何整合到一起。 Adam or Adaptive Moment Optimization algorithms combines the heuristics of both Momentum and RMSProp. The hyperparameter beta1 is generally kept around 0.9 while beta_2 is kept at 0.99. Epsilon is chosen to be 1e-10 generally. 对于公式的解释，Eq 1 and Eq 2是come from RMSprop, Eq 3 comes from Adam。最后一步是 update 操作，条理清楚，没有问题。(公式中的v 表示导数，g 表示导数e 不太清楚)Here, we compute the exponential average of the gradient as well as the squares of the gradient for each parameters (Eq 1, and Eq 2). To decide our learning step, we multiply our learning rate by average of the gradient (as was the case with momentum) and divide it by the root mean square of the exponential average of square of gradients (as was the case with momentum) in equation 3. Then, we add the update.) 在目前各种主流的深度学习工具中也都内置了各种常见的优化算法 因此，CNN 中的各种优化算法还是在梯度下降算法上面做手脚。 Batch Gradient Descent 使用我们拿到的所有的数据计算梯度，然后使用这个梯度对参数进行更新。在凸函数中，只要学习率足够小，肯定能够找到全局最优点，在非凸函数中也可以保证找到局部最优. $$\theta = \theta - \eta \cdot \nabla _ { \theta } J ( \theta )$$ 缺点: 训练数据太多的时候无法全部放入内存 训练数据多的时候计算梯度的时间非常久 优点: 方法简单 能够保证收敛到全局最优值 (凸函数)/ 局部最优值 (非凸函数) Stochastic Gradient Descent 和 Batch Gradient Descent 相反，SGD 又走入了另外一个极端，SGD 拿到一个数据之后，马上计算梯度，然后对参数进行更新. $$\theta = \theta - \eta \cdot J \left( \theta ; x ^ { ( i ) } , y ^ { ( i ) } \right)$$ 缺点: 无法利用矩阵操作加速计算过程 优点: 收敛速度快 (因为在 Batch 的方法中，每次计算梯度会计算很多相似的样本得到的梯度，这部分是冗余的) 有可能逃出一个比较差的局部最优而收敛到一个更好的局部最优甚至是全局最优 Mini-batch Gradient Descent Mini-batch 的方法是在上述两个方法中取了个折衷，每次从全部的熟练数据中取一个 mini-batch 的数据计算。 $$\theta = \theta - \eta \cdot J \left( \theta ; x ^ { ( i : i + n ) } , y ^ { ( i : i + n ) } \right)$$ batch size 的选择 n： 一般取值在 50～256 目前，mini-batch 的方法是深度学习中主流方法，各种深度学习工具默认也是这种方法。也可以把上述两种方法看成是 mini-batch 的特例，Batch 的方法，就是 mini-batch size 是整个数据集，SGD 方法就是 min-batch=1 的情况. 目前遇到的问题： 目前有一些通过 schedule 的方式来设置 learning rate 的方法，即预先设计好计算了一定数量的迭代之后减小 learning rate, 但是，由于是在训练之前就要设置好，因此，无法根据训练时的时间情况进行调整。实践中也证明这种 schedule 的方法非常不可靠. 上述方法中，对于每一个参数的 learning rate 都是相同的，这种做法是不合理的。试想，如果训练数据是稀疏的，而且，特征数据出现的频率变化很大，那么，比较合理的做法是，对于出现频率低的特征设置较大的学习速率，对于出现频率较大的特征数据设置较小的学习速率. （这种case 的经验是值得学习的） 近期的的研究表明，深层神经网络之所以比较难训练，并不是因为容易进入 local minimum, 相反，由于网络结构非常复杂，在绝大多数情况下，即使是 local minimum 也可以得到非常好的结果。而之所以难训练是因为学习过程容易陷入到马鞍面中，即在坡面上，一部分点是上升的，一部分点是下降的。而这种情况比较容易出现在平坦区域，在这种区域中，所有方向的梯度值都几乎是 0. Momentum mu 就是我们在各种深度学习框架中 / 论文中见到的 momentum. 一般设置为 0.9, 0.99 等。然而，把 mu 理解为摩擦 (或者加上其它的因素) 更好. mu 的作用是在小球在山坡上滚动的过程中来降低小球的动能。否则，如果没有这个因素，那么，小球永远不会停下来，优化过程也永远不会停止. 作用：可以使得梯度方向不变的维度上速度变快，梯度方向有所改变的维度上的更新速度变慢，这样就可以加快收敛并减小震荡。 $$\begin{split}W : &amp;= W - \alpha d W \\b : &amp;= b - \alpha d b \\\end{split}$$ 超参数设定值: 一般 beta 取值 0.9 左右。beta和1-beta分别代表之前的dW权重和现在的权重。 缺点：这种情况相当于小球从山上滚下来时是在盲目地沿着坡滚，如果它能具备一些先知，例如快要上坡时，就知道需要减速了的话，适应性会更好。 Nesterov Accelerated Gradient NAG 可以使 RNN 在很多任务上有更好的表现。 目前为止，我们可以做到，在更新梯度时顺应 loss function 的梯度来调整速度，并且对 SGD 进行加速。 momentum 和 nesterov accelerated gradient 区别：考虑之前的向量和当前的向量的顺序是不一样的。 momentum首先计算一个梯度(短的蓝色向量)，然后在加速更新梯度的方向进行一个大的跳跃(长的蓝色向量)，nesterov项首先在之前加速的梯度方向进行一个大的跳跃(棕色向量)，计算梯度然后进行校正(绿色梯向量) *** 这里是分界线*‘ 上面中的学习率是需要手动调整，但是这种调整的代价是比较高的，多大的学习率可能出现震荡，过小的学习率学习时间是比较长的，因此需要有一种能够自动调节学习率的手段。 AdaGrad （Adaptive gradient algorithm） 这个算法就可以对低频的参数做较大的更新，对高频的做较小的更新，也因此，对于稀疏的数据它的表现很好，很好地提高了 SGD 的鲁棒性，例如识别 Youtube 视频里面的猫，训练 GloVe word embeddings，因为它们都是需要在低频的特征上有更大的更新。 一般来说，在稀疏的数据上，对于频率低的特征使用较大的learning rate，对于频率高的特征使用较小的 learing rate 优点：这种方法对于训练数据比较稀疏的情况比较适用. 缺点：在深度学习中，这种方法导致学习率的调整太激进，因此常常过早结束了学习过程. RMSProp 另一种加速下降的一个算法RMSprop，全称root mean square prop。 $$\begin{split}S _ { d W } &amp; = \beta S _ { d W } + ( 1 - \beta ) d W ^ { 2 } \\S _ { d b } &amp; = \beta S _ { d b } + ( 1 - \beta ) d b ^ { 2 } \\W : &amp; = W - \alpha \frac { d W } { \sqrt { S } _ { d W } } \\b : &amp; = b - \alpha \frac { d b } { \sqrt { S _ { d b } } } \\\end{split}$$ 其中dW的平方是(dW)^2，db的平方是(db)^2。如果严谨些，防止分母为0，在分数下加上个特别小的一个值epsilon，通常取10^-8。 这个更加有规范化的意思，适合处理非平稳目标 - 对于RNN效果很好 Adam 全称 Adaptive Moment Estimation， 结合了 Momentum 和 RMSprop 两种算法的优点。算法中通常beta_1=0.9,beta_2=0.999。 前者是 Momentum 中的参数，后者是 RMSprop 的参数。 Adam 就是在 RMSprop 的基础上加了 bias-correction 和 momentum，随着梯度变的稀疏，Adam 比 RMSprop 效果会好。该方法和 RMSProp 唯一的区别是 “smooth” 过程，这里使用的是 m 来做 smooth 操作。在实践中，推荐使用 Adam 方法. Adam 算法通常会比 RMSProp 算法效果好。 Adam(Adaptive Moment Estimation)本质上是带有动量项的RMSprop，它利用梯度的一阶矩估计和二阶矩估计动态调整每个参数的学习率。Adam的优点主要在于经过偏置校正后，每一次迭代学习率都有个确定范围，使得参数比较平稳。 特点： 结合了Adagrad善于处理稀疏梯度和RMSprop善于处理非平稳目标的优点对内存需求较小为不同的参数计算不同的自适应学习率也适用于大多非凸优化 - 适用于大数据集和高维空间 如何选择？ 如果数据是稀疏的，就用自适用方法，即 Adagrad, Adadelta, RMSprop, Adam。RMSprop, Adadelta, Adam 在很多情况下的效果是相似的。 很多论文里都会用 SGD，没有 momentum 等。SGD 虽然能达到极小值，但是比其它算法用的时间长，而且可能会被困在鞍点。如果需要更快的收敛，或者是训练更深更复杂的神经网络，需要用一种自适应的算法。 参考文献：深度学习最全优化方法总结比较（SGD，Adagrad，Adadelta，Adam，Adamax，Nadam） 卷积网络一个卷积神经网络由若干卷积层、Pooling层、全连接层组成。 卷积层$Input \rightarrow [Conv] \times N \rightarrow [Pool] \times M \rightarrow [FK] \times K$ 下面的动画显示了包含两个filter的卷积层的计算。我们可以看到 $7 \times 7 \times 3$ 输入，经过两个$3 \times 3 \times 3 $filter的卷积(步幅为2)，得到了$ 3 \times 3 \times 2 $的输出。另外我们也会看到下图的Zero padding是1，也就是在输入元素的周围补了一圈0。Zero padding对于图像边缘部分的特征提取是很有帮助的。 100×100×3，3×3 卷积核，输出是 50×50×10，算进行了多少次乘-加操作？输出的每个像素点都要经过 3×3×3 = 27 次乘-加操作，因此总共需要进行 50×50×10×27 次乘-加操作。 对于包含两个 $333 $的fitler的卷积层来说，其参数数量仅有 $(3 \times 3 \times3+1) \times 2 =56 $个，且参数数量与上一层神经元个数无关。与全连接神经网络相比，其参数数量大大减少了。 卷积层输出大小计算：输入大小： $W 1 \times H 1 \times D 1$超参数（filter信息 +是否填充）： filter 个数( K), filter 大小( F), 步长 (S )，边界填充( P)输出：$$\begin{split}W _ { 2 } &amp; = \left( W _ { 1 } - F + 2 P \right) / S + 1 \\H _ { 2 } &amp;= \left( H _ { 1 } - F + 2 P \right) / S + 1 \\D _ { 2 } &amp;= K\end{split}$$ 注意计算的是宽和高的两个维度。 $$\begin{split}\text {output} _ { w } &amp;= \left\lfloor \frac { i m a g e _ { w } + 2 p a d d i n g - k e r n e l _ { s i z e } } { s t r i d e } \right\rfloor + 1 \\\text {output} _ { h } &amp;= \left\lfloor \frac { i m a g e _ { h } + 2 p a d d i n g - k e r n e l _ { s i z e } } { s t r i d e } \right\rfloor + 1 \\\end{split}$$卷积层参数量计算： 权值共享： 给定一张图，用一个 filter 去扫描这张图， filter 里面的数字叫做权重，这张图么个位置是被同样的filter 扫描的，所以权重是一样的，也就是共享。（从减少参数量的角度去理解）这个概念是和全连接层中的权值进行比较的，简单来说就是降低了权重的使用。权重共享即 filter 值的共享。对于 三维图片来说，每个filter需要FFD1个权重值，总共K个filter，需要FFD1*K权重值。和一维一样，整个滑动过程中filter W0和W1值保持不变，可称作权值共享。而且，补充一句，对于三维的input，权值只是在input的每个depth slice上共享的。对于一层的 filter 只是有一个bias。 for example:Filter个数：32原始图像shape：$224 \times 224 \times 3$卷积核大小为：$2 \times 2$一个卷积核的参数：$ 2 \times 2 \times 3=12 $16个卷积核的参数总额：$ 16 \times 12 + 16 =192 + 16 = 208 $$ weight \times x + bias $根据这个公式，即可算的最终的参数总额为：208 Pooling 层Pooling层主要的作用是下采样，主要有两点作用，一个是提取重要特征，一个是简化网络的计算。Pooling的方法很多，最常用的是Max Pooling。Max Pooling实际上就是在n*n的样本中取最大值，作为采样后的样本值。下图是 max pooling： 除了Max Pooing之外，常用的还有Mean Pooling——取各样本的平均值。 池化层往往在卷积层后面，通过池化来降低卷积层输出的特征向量，同时改善结果（不易出现过拟合）。 池化层参数个数计算： 这个很明显是没有参数的。 卷积层 vs pooling 层主要比较两者在降维、特征提取方面的差别。 pooling 层常见的 max pooling 和 average (mean ) pooling两种。该层是没有参数的。 pooling 的作用主要体现在减少模型去拟合的难度，防止过拟合，节省计算力方面。max pooling 相比于 mean pooling 更加有 提取特征的感觉。抓住比计较显著的特征，AVE pooling 可以带来一定意义上的平滑，可以减小图像尺寸变化的干扰。从效果上讲，前者一半是要好于后者的。 从上面左图可以看到，使用了pool操作其实就是降低图片的空间尺寸。右图使用一个 2 × 2的 池化核（filter），以2为步长（stride），对图片进行max pooling，那么会图片就会尺寸就会减小一半。需要注意，这里是因为 stride = 2，所以图片尺寸才会减少一半的。 不同点：pooling 是没有 weights 或者 parameter 更新，仅仅是下采样convolution layer 则不一样，提取了特征并且进行了下采样。 全连接层连接所有的特征，将输出值送给分类器（如softmax分类器） 比如说上一层（池化层）的输出为：$(111 \times 111 \times 16) $，从第一层到第二层，只是图片大小发生了变化，深度没有发生变化，而Dense对应的神经元个数为133个，那么还是根据公式：$weight \times x + bias$，计算得：$133 \times16+133=2261$ Dropout 概念 dropout 是指在深度学习网络的训练过程中，按照一定的概率将一部分神经网络单元暂时从网络中丢弃，相当于从原始的网络中找到一个更瘦的网络。 why 我们在训练神经网络的时候，会遇到两大缺点： 容易过拟合 费时 dropout 主要是为了在一定程度上减少过拟合。 工作原理 首先随机（临时）删掉网络中一半的隐藏神经元，输入输出神经元保持不变（图中虚线为部分临时被删除的神经元） 然后把输入x通过修改后的网络前向传播，然后把得到的损失结果通过修改的网络反向传播。一小批训练样本执行完这个过程后，在没有被删除的神经元上按照随机梯度下降法更新对应的参数（w，b） 继续重复这一过程：恢复被删掉的神经元（此时被删除的神经元保持原样，而没有被删除的神经元已经有所更新）。从隐藏层神经元中随机选择一个一半大小的子集临时删除掉（备份被删除神经元的参数）。 不断的重复着一过程 怎么理解测试时权重参数w要乘以概率p？ 假设总共有100个神经元，训练的时候我们加上dropout，p=0.5，那么我们就有50个神经元参与训练，那么我们每次50个神经元训练出来的模型参数w是要比直接100个神经元要小的，因为它更新的次数会更少。我们测试的时候100个神经元是都会参与计算的，这就跟训练的时候我们使用50个神经元产生差异了，如果要保证测试的时候每个神经元的关联计算不能少，只能从通过改变w来达到跟训练时一样输出，所以才会有权重参数w乘以p。 为什么 dropout 可以有效的减少过拟合？（类似取平均的活动）因为不同的网络可能产生不同的过拟合，取平均则有可能让一些“相反的”拟合互相抵消。dropout掉不同的隐藏神经元就类似在训练不同的网络，随机删掉一半隐藏神经元导致网络结构已经不同，整个dropout过程就相当于对很多个不同的神经网络取平均。 梯度消失/ 梯度爆炸 首先一个观点，梯度消失和梯度爆炸本质上一回事。 理由：sigmoid 导数的最大值为0.25，通常 abs(w) &lt; 1,则上述分析中的激活函数的导数与权重的积小于0.25，前面的层比后面的层梯度变化更小，故变化更慢，从而引起了梯度消失问题。当权值过大，前面层比后面层梯度变化更快，则引起梯度爆炸问题。所以后面的梯度消失和梯度爆炸只是前面初始化值的一种蝴蝶效应，只是数值问题。 解决方法 重新设计网络结构 使用比较浅的网络结构 使用残差结构, 这种方式在图像处理中更加常见 激活函数使用 relu or leaky relu 而不是 sigmoid or tanh 关于weights 方面 使用梯度截断（Gradient Clipping），检查误差梯度的值是否超过阈值，如果超过，则截断梯度，将梯度设置为阈值。 使用权重正则化（Weight Regularization），常用的是 ，通常使用的是 L1 惩罚项（权重绝对值）或 L2 惩罚项（权重平方）这个是在损失函数上的操作 batch normalization （关于BN 的位置是可以再查一下的，现在有两个位置，一个是在激活函数之前一个是 激活函数之后，个人倾向于激活函数之前，因为这样才有可以减少梯度消失和梯度爆炸的发生呀） dropout一定在激活函数之后 ps: CONV / FC - &gt; BatchNorm - &gt; ReLu（或其他激活） - &gt; Dropout - &gt; CONV / FC对于 cnn 还有一种常见的结构：先卷积，再batchnorm, 然后激活函数，最后pooling在fully connection中的应用，用在全连接层之后激活函数之前 理解 正则化如何减少模型过拟合程度 High Bias（高偏差）就是欠拟合，High Variance（高方差）就是过拟合。 简单来说，正则化就是在原来的 cost function 中添加 正则项。 正则化项能减少模型的非线性程度，从而降低模型的过拟合。从图中来看，正则化项能将过拟合的模型（蓝色）变为Just Right的模型（粉红色）。 为什么正则化是有效的？ 对于线性模型，其添加了正则化项的Cost Function如下图。 现在目标函数有两个目标，第一个是我们想要训练的，使假设更好地去拟合训练数据，第二个目标是我们想要保持参数较小。 $ \lambda$ 用来调节两者之间的平衡，这样理解如果 该值设置的很大，那么前面的参数 $\theta 1 \theta 2 \theta 3 \theta 4$ 就会被非常大的惩罚，这些值就会接近0。如果假设第一个目标是多项式组成的，那么当 $\theta $ 数值变小的时候，这个式子就没有了，就减少了模型的复杂度。 对于神经网络，其激活函数（以tanh为例）如下图 直观的理解，如果我们的正则化系数（lambda）无穷大，则权重w就会趋近于0。权重变小，激活函数输出z变小。z变小，就到了激活函数的线性区域，从而降低了模型的非线性化程度。 感受野的计算在卷积神经网络中，感受野（Receptive Field）的定义是卷积神经网络每一层输出的特征图（feature map）上的像素点在输入图片上映射的区域大小。 这个是一个反向的过程。这个和 CNN 的 不同在于，如果是2 两层，那么这个是连续（持续）的对同一个图像进行采样。接着上一个的结果进行操作。 使用例子说明：两层$3 \times 3$ 卷积操作的有效区域(感受野)是 $5 \times 5 $ (所有filter的stride=1,pad=0)。 卷积和池化操作的计算（例题）卷积和池化的计算方式是一样的，具体可以参考上面小结关于卷积操作的公式。 输入图片大小为200×200，依次经过一层卷积（kernel size 5×5，padding 1，stride 2），pooling（kernel size 3×3，padding 0，stride 1），又一层卷积（kernel size 3×3，padding 1，stride 1）之后，输出特征图大小为：A. 95B. 96C. 97D. 98E. 99F. 100 解答： 第一次卷积后大小：$$\frac { 200 + 2 - 5 } { 2 } + 1 = 99$$ 第一次池化后大小： $$\frac { 99 + 0 - 3 } { 1 } + 1 = 97$$ 第二次卷积后大小： $$\frac { 97 + 2 - 3 } { 1 } + 1 = 97$$ 所以最后的结果是 97]]></content>
      <categories>
        <category>Deep learning</category>
      </categories>
      <tags>
        <tag>optimizer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[siamese network]]></title>
    <url>%2F2019%2F03%2F26%2Fsiamese-network%2F</url>
    <content type="text"><![CDATA[主要是介绍自己论文中的网络结构：siamese network。 但凡优化，无非两条路：在好的基础上更快，在快的基础上效果更好。 Siamese network训练速度快，所以只是需要其在训练效果上更好。 先来分析一下为什么训练速度快。那么不得不分析该网络结构。整个网络的输入是 (img1, img2, y) 这样的三元组，img 表示图片，y表示label。图片可以是同一类别的图片，也可以是不同类别的，y表示两张图片之间的相似程度，y的取值 (0,1)，0表示相似（同一类别），1 表示不相似（不同类别）。并且y 是double 类型，属于0-1 区间任意的数字。网路结构最后的输出是 0-1 区间的任意数字。通常是以0.5 作为分界线，如果小于0.5 那么认为两种图片是属于同一类别，或者说更相似；反之也成立。重要的一点是中间的weights 实现了权值共享，这样可以加快网络的训练速度。 loss function这个是属于经典的 contrastive loss function (对比损失函数)。当y 接近于0的时候，右半部分消失，这个是表示两张图片很是相似，然后就不断使得 欧氏距离减少；当y 接近于1的时候，左半部分消失，这个时候两张图片很不相似，然后右边就是 hinge loss （合页损失函数）。参数m 作为一种margin 是是可以调节，我的实验中 m 取1.总的思想：就是使得相近的图像距离相近，不想近的图像距离变远。 $L ( W , ( Y , X _ { 1 } , X _ { 2 } ) ) = ( 1 - Y ) \frac { 1 } { 2 } ( D _ { W } ) ^ { 2 } + ( Y ) \frac { 1 } { 2 } { \max ( 0 , m - D _ { W } ) } ^ { 2 }$ Spectral Normalization图像输入到网络之前使用正则化，然后输入到激活函数之前也是使用正则化，所以这种效果也是扩展到 weights，直接对 weights 进行正则化使其符合 Lipschitz 约束，避免使用大的gradients。在GAN 网络中的 discriminator 或者 generator 都发现了其可以稳定训练的过程。在实验中，我们扩大了这种使用范围，把其应用到所有的网络的 layer上。 Lipschitz Continuity 在 GAN 中，假设我们有一个判别器 $\mathrm { D } : 1 \rightarrow \mathrm { R }$， 其中 I 是图像空间. 如果判别器是 K-Lipschitz continuous 的, 那么对图像空间中的任意 x 和 y，有 $$| D ( x ) - D ( y ) | \leq K | x - y |$$ 其中 $ | \cdot | $ 为L2 norm，如果K 取到最小值，那么K 被称为 Lipschitz constant。 直观来说，Lipschitz 条件限制了函数变化的剧烈程度，即函数的梯度。在一维空间中，很容易看出 y=sin(x) 是 1-Lipschitz 的，它的最大斜率是 1。 self-attention mechanismAttention 机制自从 “Attention Is All You Need” 开始火爆，并且实验的效果也是很好的，然后在图像领域也开始尝试使用 attention 机制来解决长依赖的问题。应用到图像领域主要是 explore spatial locality information, 说白了就是细节的信息。 If we look at the DCGAN model, we see that regular GANs are heavily based on convolution operations, which use a local receptive field (convolutional kernel) to learn representations. Simple features like edges and corners are learned in the first few layers. Also, ConvNets are able to use these simple representations to learn more complex ones. However, long-range dependency might be hard to learn. Long-range dependency (long-term dependency) is from RNN, which we can say anything larger than trigram as a long term dependency. Thus, most of the image content does not exhibit elaborated shape such as sky or the ocean looks fine. The task of creating geometrically complex forms, such as four-legged animals, is far more challenging. This is where attention comes into play. 而 self-attention 中QKV 三个部分是相同的，对于这种处理方法和Res_block 还是有点相似的。 结果训练数据集使用是 Cifar-10，记录了训练过程中 acc 和loss 的变化情况。除了训练的效果比较好外，训练速度也是非常快的，可以清楚的看到model acc 在接近25 epoches的时候就开始收敛。]]></content>
      <categories>
        <category>Deep learning</category>
      </categories>
      <tags>
        <tag>siamese network</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[fastText & faiss (ongoing)]]></title>
    <url>%2F2019%2F03%2F25%2FfastText-faiss%2F</url>
    <content type="text"><![CDATA[主要介绍 fastText、faiss 两个常用的工具，然后顺带介绍一下容易混淆的概念 k-means 和knn。 fastTextfastText结合了自然语言处理和机器学习中最成功的理念。这些包括了使用词袋以及n-gram袋表征语句，还有使用子字(subword)信息，效果上的提升。另外采用了一个softmax层级(利用了类别不均衡分布的优势)来加速运算过程。 fastText 主要是用来解决 word representations和 sentence classification. 有趣的是前者是无监督的学习方式，后者是有监督的学习方式。分别主要来自 ”Enriching Word Vectors with Subword Information“ 和 “Bag of Tricks for Efficient Text Classification” 两篇论文。并且使用的是 shallow neural network 而不是深度网络。 FastText is a library created by the Facebook Research Team for efficient learning of word representations and sentence classification. Take off:fastText 方法包含三部分：模型架构、层次 Softmax 和 N-gram 特征。fasttext 有两个用处： text classification 和 word embedding 。使用场景：大型数据，高效计算 下面进行细说： 模型架构这个是总的框架图。分为两个部分介绍这个网络结构：从input -&gt; hidden:输入层输入的是一个已经分词后短文本。短文本中每个词的词向量是由该短文本的one-hot矩阵乘以一个初始化的矩阵w得到的。（原理图：下图是fasttext 运行的时候，这个分词是再处理成单词和n-gram 组成的特征，这个是不需要我们进行显性的操作的）从 hidden -&gt; output：插播一句，我们经常使用的预训练模型中的weights 是从input-&gt; hidden。 Hierarchical SoftmaxHierarchical Softmax 不是fasttext 首创，它的改进之处在于实现结构上基于 huffman 树而不是普通的二叉树，属于运算上的优化。具体说来：利用了类别（class）不均衡这个事实（一些类别出现次数比其他的更多），通过使用 Huffman 算法建立用于表征类别的树形结构。对标签进行编码，能够极大地缩小模型预测目标的数量。 这个是softmax 的原始的计算公式：$$p \left( w _ { j } | w _ { I } \right) = y _ { j } = \frac { \exp \left( u _ { j } \right) } { \sum _ { j ^ { \prime } = 1 } ^ { V } \exp \left( u _ { j ^ { \prime } } \right) }$$ 采用二叉树的结构之后，时间上优化不少。$O ( N) \rightarrow O \left( \log _ { 2 } N \right)$。见下图。 和之前的神经网络模型相比，这里的huffmax树的所有内部节点就类似之前神经网络隐藏层的神经元。其中，根节点的词向量对应我们的投影后的词向量，而所有的叶子节点就类似于之前神经网softmax输出层的神经元。叶子节点的个数就是词汇表的大小. 和之前的相比，从隐藏层到输出层的softmax映射不是一下就完成的，而是沿着 huffman树一步步完成的，因此这种 softmax取名为”Hierarchical softmax”. N-gram 特征N-gram是基于这样的思想：某个词的出现依赖于其他若干个词；我们获得的信息越多，预测越准确。我想说，我们每个人的大脑中都有一个N-gram模型，而且是在不断完善和训练的。我们的见识与经历，都在丰富着我们的阅历，增强着我们的联想能力。 N-gram 是一种思想，可以有两种level 的实现，一种是基于 character-level，一种是基于 word-level，前者是扩充了对于”不常见“单词，后者是考虑了部分的词的顺序，都是考虑了”周边“ 信息,用流行的话就是 context 的信息。所以比较难界定 fasttext 训练出来的是不是有比较强的词序。 N-gram模型是一种语言模型（Language Model，LM），语言模型是一个基于概率的判别模型，它的输入是一句话（单词的顺序序列），输出是这句话的概率，即这些单词的联合概率（joint probability）。 这样的作用，使用N-gram来给文本添加额外的特征获得关于局部词顺序的部分信息。举个栗子：对于句子：“我 喜欢 喝 咖啡”, 如果不考虑顺序，那么就是每个词，“我”，“喜欢”，“喝”，“咖啡”这五个单词的word embedding求平均。如果考虑2-gram, 那么除了以上五个词，还有“我喜欢”，“喜欢喝”，“喝咖啡”等词。“我喜欢”，“喜欢喝”，“喝咖啡”这三个词就作为这个句子的文本特征。我们经常见到的场景：输入法的预选词汇。就是可以通过这种方式实现的。 当然使用了更多的特征意味着计算量的增加，计算效率下降，于是该作者提出了两种解决方法： 过滤掉低词频 使用词粒度代替字粒度。 还是使用上面的句子”我喜欢喝咖啡“，如果使用子粒度的2-gram，那么产生的特征是“我喜”，“喜欢”，“欢喝”，“喝咖”，“咖啡”。如果使用词粒度为2-gram，那么产生的特征是“我喜欢”，“喜欢喝”，“喝咖啡”。 补充一句，subwords就是一个词的character-level的n-gram。比如单词”hello”，长度至少为3的char-level的ngram有”hel”,”ell”,”llo”,”hell”,”ello”以及本身”hello”。 Negative Sampling该 technique 主要是减轻计算量的角度考虑的，每次让一个训练样本仅仅更新一部分的权重参数，这个技术不是 fastText 首创的，但是本着总结知识点的，也就放在这里了。 CBOW / Skip-gram模型 （这个论文中）提出了两种方法，一种是Hierarchical Softmax，另一种是Negative Sampling。论文中提出的两种方法都是用来提高计算效率的，下面说一下负采样。 在训练神经网络时，每当接受一个训练样本，然后调整所有神经单元权重参数，来使神经网络预测更加准确。换句话说，每个训练样本都将会调整所有神经网络中的参数。而 Negative Sampling 每次让一个训练样本仅仅更新一小部分的权重参数，从而降低梯度下降过程中的计算量。如果 vocabulary 大小为1万时， 当输入样本 ( “fox”, “quick”) 到神经网络时， “ fox” 经过 one-hot 编码，在输出层我们期望对应 “quick” 单词的那个神经元结点输出 1，其余 9999 个都应该输出0。在这里，这9999个我们期望输出为0的神经元结点所对应的单词我们称为 negative word，随机选择一小部分的 negative words，比如选 10个 negative words 来更新对应的权重参数。 解决的问题，在最后一层 softmax 的计算量太大，相当于每一次word 都是需要整个dict 量的级别的更新。然后选择 k 个negative words，只是计算这些softmax 的值。 Training a neural network means taking a training example and adjusting all of the neuron weights slightly so that it predicts that training sample more accurately. In other words, each training sample will tweak all of the weights in the neural network.As we discussed above, the size of our word vocabulary means that our skip-gram neural network has a tremendous number of weights, all of which would be updated slightly by every one of our billions of training samples!Negative sampling addresses this by having each training sample only modify a small percentage of the weights, rather than all of them. Here’s how it works.When training the network on the word pair (“fox”, “quick”), recall that the “label” or “correct output” of the network is a one-hot vector. That is, for the output neuron corresponding to “quick” to output a 1, and for all of the other thousands of output neurons to output a 0.With negative sampling, we are instead going to randomly select just a small number of “negative” words (let’s say 5) to update the weights for. (In this context, a “negative” word is one for which we want the network to output a 0 for). We will also still update the weights for our “positive” word (which is the word “quick” in our current example).The paper says that selecting 5-20 words works well for smaller datasets, and you can get away with only 2-5 words for large datasets.Recall that the output layer of our model has a weight matrix that’s 300 x 10,000. So we will just be updating the weights for our positive word (“quick”), plus the weights for 5 other words that we want to output 0. That’s a total of 6 output neurons, and 1,800 weight values total. That’s only 0.06% of the 3M weights in the output layer!In the hidden layer, only the weights for the input word are updated (this is true whether you’re using Negative Sampling or not). Positive samples and Negative samplesOne little detail that’s missing from the description above is how do we select the negative samples.（下面说的是如何进行选择negative sample的问题：基本思路是根据出现频率进行选择）The negative samples are chosen using the unigram distribution. Essentially, the probability of selecting a word as a negative sample is related to its frequency, with more frequent words being more likely to be selected as negative samples. Instead of using the raw frequency in the original word2vec paper, each word is given a weight that’s equal to it’s frequency (word count) raised to the 3/4 power. The probability for selecting a word is just it’s weight divided by the sum of weights for all words.$$P \left( w _ { i } \right) = \frac { f \left( w _ { i } \right) ^ { 3 / 4 } } { \sum _ { i = 0 } ^ { n } \left( f \left( w _ { j } \right) ^ { 3 / 4 } \right) }$$ 上述中的函数是幂函数，图像的形状和log 函数差不多，都是从 $y =x$ 进行了一下约束，函数变得更加的平缓。对于高频词进行了约束，对于低频次也有机会出现。 This decision to raise the frequency to the 3/4 power appears to be empirical; as the author claims it outperformed other functions (e.g. just using unigram distribution).Side note: The way this selection is implemented in the original word2vec C code is interesting. They have a large array with 100M elements (which they refer to as the unigram table). They fill this table with the index of each word in the vocabulary multiple times, and the number of times a word’s index appears in the table is given by Then, to actually select a negative sample, we just generate a random integer between 0 and 100M, and use the word at that index in the table. Since the higher probability words occur more times in the table, we’re more likely to pick those. 这个也是有讲 任何进行negative sample的选择http://jalammar.github.io/illustrated-word2vec/一般来说在 word2vec 中context 是会选择到 5，然后这个 positive / negative sample 会是(1/6), 然后 nagative sample 是随机在 dictionary里面选的（所以有可能选到 positive sample）， 这个是这个dictionary 是根据频率，出现次数越多的，被选中的可能性也越大。The number of negative samples is another factor of the training process. The original paper prescribes 5-20 as being a good number of negative samples. It also states that 2-5 seems to be enough when you have a large enough dataset. The Gensim default is 5 negative samples. To address this, we need to introduce negative samples to our dataset – samples of words that are not neighbors. Our model needs to return 0 for those samples. Now that’s a challenge that the model has to work hard to solve – but still at blazing fast speed.This idea is inspired by Noise-contrastive estimation. We are contrasting the actual signal (positive examples of neighboring words) with noise (randomly selected words that are not neighbors). This leads to a great tradeoff of computational and statistical efficiency. Essentially, the probability for selecting a word as a negative sample is related to its frequency, with more frequent words being more likely to be selected as negative samples. They fill this table with the index of each word in the vocabulary multiple times, and the number of times a word’s index appears in the table is given by $P(wi)*P(wi)$ table_size. Then, to actually select a negative sample, you just generate a random integer between 0 and 100M, and use the word at that index in the table. Since the higher probability words occur more times in the table, you’re more likely to pick those. (ps 这种数量比不是 1：1，常常是 positive ： negative =1：5， 这个是经验值，在传统机器学习中可能认为是 data unbalanced)It’s now time to build out our skip-gram generator which will give us pair of words and their relevance (word, word in the same window), with label 1 (positive samples). (word, random word from the vocabulary), with label 0 (negative samples). 使用第一个应用场景：词向量。fastText作为训练词向量认为可以有两种模式，一种是根据周围词语预测中心词汇的CBOW （continuous bag-of-words）模型，另一种是根据中心词汇预测上下文的 skip-gram 模型。 ./fasttext – It is used to invoke the FastText library. skipgram/cbow – It is where you specify whether skipgram or cbow is to be used to create the word representations. -input – This is the name of the parameter which specifies the following word to be used as the name of the file used for training. This argument should be used as is. data.txt – a sample text file over which we wish to train the skipgram or cbow model. Change this name to the name of the text file you have. -output – This is the name of the parameter which specifies the following word to be used as the name of the model being created. This argument is to be used as is. model – This is the name of the model created.Running the above command will create two files named model.bin and model.vec. model.bin contains the model parameters, dictionary and the hyperparameters and can be used to compute word vectors. model.vec is a text file that contains the word vectors for one word per line. 最后生成有两个文件，一个 xxx.bin 文件，一个是 xxx.vec 文件，前者是预训练模型，后者是词向量。这两个可能是最重要的格式了。 The most important parameters of the model are its dimension and the range of size for the subwords. 常见的代码格式： ./fasttext skipgram -input data/fil9 -output result/fil9 -minn 2 -maxn 5 -dim 300 跑偏一下说一下shell的小技巧。使用echo 或者 &lt; 这样进行单个词或者多个词的词向量的查询。 ./fasttext print-word-vectors model.bin &lt; queries.txtecho “word” | ./fasttext print-word-vectors model.bin Finding simialr words: ./fasttext nn model.bin 最重要的几个参数： The most important parameters of the model are its dimension and the range of size for the subwords. The dimension (dim) controls the size of the vectors, the larger they are the more information they can capture but requires more data to be learned. But, if they are too large, they are harder and slower to train. By default, we use 100 dimensions, but any value in the 100-300 range is as popular. The subwords are all the substrings contained in a word between the minimum size (minn) and the maximal size (maxn). By default, we take all the subword between 3 and 6 characters, but other range could be more appropriate to different languages: 1$ ./fasttext skipgram -input data/fil9 -output result/fil9 -minn 2 -maxn 5 -dim 300 The following arguments for the dictionary are optional: -minCount 词出现的最少次数 [5] -minCountLabel 标签出现的最少次数 [0] -wordNgrams 单词 ngram 的最大长度 [1] -bucket 桶的个数 [2000000] -minn char ngram 的最小长度 [3] -maxn char ngram 的最大长度 [6] The following arguments for training are optional -dim 字向量的大小 [100] -ws 上下文窗口的大小 [5] -epoch 迭代次数 [5] -neg 负样本个数 [5] -loss 损失函数 {ns, hs, softmax} [ns] 第二个应用场景：文本分类。 Sentiment analysis and email classification are classic examples of text classification （BERT 也是采用的这种label 的格式）在训练数据集中label 默认是使用 “__label__” 进行表示的，当然也是可以进行自定义的。 ./fasttext supervised -input train.ft.txt -output model_kaggle -label __label__ -lr 0.5 就是进行predict的时候，有时候并不是很能想起来只是predict top 3 这样的东西。 # Predicting on the test dataset ./fasttext predict model_kaggle.bin test.ft.txt # Predicting the top 3 labels ./fasttext predict model_kaggle.bin test.ft.txt 3 fasttext VS. CBOW在标准的多核CPU上， 能够训练10亿词级别语料库的词向量在10分钟之内，能够分类有着30万多类别的50多万句子在1分钟之内。 n-gram n-gram 是一种基于语言模型的算法，基本思想是将文本内容按照字节顺序进行大小为N的滑动窗口操作，最终形成长度为N的字节片段序列。 CBOW 是和词序无关的，实现 n-gram 作为额外的特征可以捕捉一些部分的词序。fastText是一种基于skip-gram模型的新扩展，它会使用subword的信息，将每个词被表示成一个字符级n-gram词袋(a bag of character n-grams)。每个向量表示与每个字符级n-gram相关联，而词(word)则可以看成是这些n-gram向量表示的求和(sum)。fastText在大语料上训练很快。 网络结构方面 输入层：CBOW 的输入层是由目标词汇 $y$ 的上下文单词 ${ x _ { 1 } , \ldots , x _ { c } }$ 组成， $\boldsymbol { x } _ { i }$ 是被 onehot 编码过的 V 维向量，其中 V 是词汇量。而fasttext 的输入是多个单词及其n-gram特征。比如说，对于单词“apple”，假设n的取值为3，则它的trigram有: “&lt;ap”, “app”, “ppl”, “ple”, “le&gt;”其中，&lt;表示前缀，&gt;表示后缀。于是，我们可以用这些trigram来表示“apple”这个单词，进一步，我们可以用这5个trigram的向量叠加来表示“apple”的词向量。这样做有两点好处： 对于低频词生成的词向量效果会更好。因为它们的n-gram可以和其它词共享。 对于训练词库之外的单词，仍然可以构建它们的词向量。我们可以叠加它们的字符级n-gram向量。 从输入层到隐藏层，CBOW会将上下文单词向量叠加起来并经过一次矩阵乘法（线性变化）并应用激活函数，而fastText省略了这一过程，直接将embedding过的向量特征求和取平均； 输出层，一般的CBOW模型会采用Softmax作为输出，而fastText则采用了Hierarchical Softmax，大大降低了模型训练时间；CBOW 的输出层是被onehot编码过的目标词y CBOW的输出是目标词汇，fastText的输出是文档对应的类标。 使用 fasttext 进行文本分类的时候，其核心思想是 将整篇文档的词及n-gram向量叠加平均得到文档向量，然后使用文档向量做softmax多分类。 层次softmax softmax 是在 逻辑回归 （logistic regression） 在多分类任务上的推广，是网络中的最后一层。当 词汇数量V 较大时候，softmax 的计算代价是很大的， O(v) 量级。层次softmax 是将全局多分类转化成了若干个二分类问题，从而将时间复杂度从O(V) 转化成了 O(log V)。 缺点：fastText适用与分类类别非常大而且数据集足够多的情况，当分类类别比较小或者数据集比较少的话，很容易过拟合。 faiss用途：相似度检测和稠密向量的聚类。 Faiss is a library for efficient similarity search and clustering of dense vectors. 之前的实习经历主要是用faiss 处理文本的representation，但是这个是有偏差的，凡是能够打成词向量，都是可以使用faiss 进行计算的，当然这词向量需要满足：相近内容在相近的空间。 Once the vectors are extracted by learning machinery (from images, videos, text documents, and elsewhere), they’re ready to feed into the similarity search library. faiss的实现过程首先使用 index对于向量进行预处理，然后选择不同的模式. 主要讲的是三种模式，一个维度是简单模式，适合在小数据上进行计算 欧氏距离；一个维度是加快检索速度，这种模式下是需要提前的train，其基本的思路对向量进行聚类，当然文中说的是 “细胞”，建立倒排索引，然后检索的时候，搜索这个“细胞”内 和周围的“细胞” 的id 的集合，就可以返回前 K 个最相近的结果；最后一个维度是减少内存的使用，上面两种都是使用的完整的向量，这个模式下是使用的压缩向量，可以使用PCA 进行实现，当然这个模式下得到的结果也是近似解。还有两种计算的上的优化，对于向量进行分段计算，这种可以实现并行，并且支持任务在GPU 上进行运算。 牺牲了一些精确性来使得运行速度更快。 Similarity search can be made orders of magnitude faster if we’re willing to trade some accuracy; that is, deviate a bit from the reference result. For example, it may not matter much if the first and second results of an image similarity search are swapped, since they’re probably both correct results for a given query. Accelerating the search involves some pre-processing of the data set, an operation that we call indexing. ( 下面这句话的观点是什么，感觉不知道逻辑在哪里啊)向量的比较有两种metric：一种是L2 一种是基于consine (点乘)进行检索。前者是求解最小的值，后者是通过inner——product 求解maximum. 并且是支持GPU的，在原来CPU上建立的index，然后很好的迁移到 GPU上。 faiss 中的三种基本索引 IndexFlatL2 基于brute-force计算向量的L2距离，就是暴搜。检索速度慢，适用于小数据量。 在计算上进行了优化，比如使用堆存储结构，寻找最接近的 K 个元素时候后，进行分段计算，把 d 维向量分成几段分别进行计算；建立倒排索引( id -contents) ，先使用聚类，然后再类内和相近的类进行寻找而非整个空间。 12345678910111213141516171819202122232425262728293031323334353637383940import numpy as npd = 64 # 维度nb = 100000 # 数据库大小nq = 10000 # 要搜索的querynp.random.seed(1234) # 确定种子，使随机数可重现xb = np.random.random((nb, d)).astype('float32')xb[:, 0] += np.arange(nb) / 1000. # 每一行的第一个列增加一个等差数列的对应项数xq = np.random.random((nq, d)).astype('float32')xq[:, 0] += np.arange(nq) / 1000.print(xq.shape) # (10000, 64)print(xb.shape) # (100000, 64)import faiss # make faiss availableindex = faiss.IndexFlatL2(d) # 构建FlatL2索引print(index.is_trained)print(index.ntotal)index.add(xb) # 向索引中添加向量。add操作如果没有提供id，则使用向量序号作为id。print(index.ntotal)k = 4 # 搜索多少个临近向量D, I = index.search(xb[:5], k) # 用xb的前五行本身自己搜索自己，完整性检查，用于测试print("I=")print(I)#I=#[[ 0 393 363 78 924]# [ 1 555 277 364 617]# [ 2 304 101 13 801]# [ 3 173 18 182 484]# [ 4 288 370 531 178]]# I输出类似于上面，每行对应着相应向量的搜索结果。k为多少就有多少列，distance低的排在前面。# 可以看到前五行的第一列确实是0~4print("D=")print(D)#[[0. 7.1751733 7.207629 7.2511625]# [0. 6.3235645 6.684581 6.7999454]# [0. 5.7964087 6.391736 7.2815123]# [0. 7.2779055 7.5279865 7.6628466]# [0. 6.7638035 7.2951202 7.3688145]]# 可以看到第一行第一列都是0，意思是向量与自己本身的距离为0D, I = index.search(xq, k) # 搜索print(I[:5]) # 最初五个向量查询的结果print(I[-5:]) # 最后五个向量查询的结果 IndexIVFFlat (加速搜索) 对于暴搜来说，海量数据搜索速度太慢，那么需要预训练把向量都聚类。这里使用IndexIVFFlat来加快搜索速度。IndexIVFFlat是faiss的倒排索引，把数据构成的向量空间切割为Voronoi细胞，每个向量落入其中一个Voronoi细胞中。在搜索时，只有查询x所在细胞中包含的数据库向量y与少数几个相邻查询向量进行比较。 训练的时候还需要有一个量化器，用于决定以什么方式将向量分配给Voronoi细胞。每个细胞由一个质心定义，找到一个向量所在的Voronoi细胞包括在质心集中找到该向量的最近邻居。 搜索方法有两个参数： nlist 划分Voronoi细胞的数量 nprobe 执行搜索访问的单元格数(不包括nlist)，该参数调整结果速度和准确度之间折中的一种方式。如果设置nprobe=nlist则结果与暴搜一致。 加快索引的方式之一，与暴搜对比就是需要train，把向量空间下的数据切割为Voronoi细胞，检索只对向量所在细胞和周围细胞进行检索。 123456789101112131415161718192021222324252627import numpy as npd = 64 # dimensionnb = 100000 # database sizenq = 10000 # nb of queriesnp.random.seed(1234) # make reproduciblexb = np.random.random((nb, d)).astype('float32')xb[:, 0] += np.arange(nb) / 1000.xq = np.random.random((nq, d)).astype('float32')xq[:, 0] += np.arange(nq) / 1000.import faissnlist = 100k = 4quantizer = faiss.IndexFlatL2(d) # 内部的索引方式index = faiss.IndexIVFFlat(quantizer, d, nlist, faiss.METRIC_L2)# here we specify METRIC_L2, by default it performs inner-product searchprint("before train")assert not index.is_trainedindex.train(xb)assert index.is_trainedprint("before add")index.add(xb) # add may be a bit slower as wellD, I = index.search(xq, k) # actual searchprint(I[-5:]) # neighbors of the 5 last queriesindex.nprobe = 10 # default nprobe is 1, try a few moreD, I = index.search(xq, k)print(I[-5:]) # neighbors of the 5 last queries IndexIVFPQ (减少内存使用) 上面两种索引都是存储的完整向量，下面介绍一种压缩向量的方法。IndexIVFPQ基于PQ (Product Quantizer)算法压缩向量。在这种情况下，由于向量没有精确存储，搜索方法返回的距离也是近似值。上面我们看到的索引IndexFlatL2和IndexIVFFlat都会全量存储所有的向量在内存中，为满足大的数据量的需求，faiss提供一种基于Product Quantizer(乘积量化)的压缩算法编码向量大小到指定的字节数。此时，存储的向量时压缩过的，查询的距离也是近似的。 原理：简单来说就是通过PCA将高纬空间转换成低维空间。 原来的数据 train 得到一个转换矩阵P，然后这个矩阵和原来的数据X得到新的降维之后的Y ($PX =Y$)。这样转换过程中信息损失的更少，在faiss 中使用 train() 函数进行实现。 123456789101112131415161718192021222324252627282930313233343536373839import numpy as npd = 64 # dimensionnb = 100000 # database sizenq = 10000 # nb of queriesnp.random.seed(1234) # make reproduciblexb = np.random.random((nb, d)).astype('float32')xb[:, 0] += np.arange(nb) / 1000.xq = np.random.random((nq, d)).astype('float32')xq[:, 0] += np.arange(nq) / 1000.import faissnlist = 100m = 8k = 4quantizer = faiss.IndexFlatL2(d) # 内部的索引方式index = faiss.IndexIVFPQ(quantizer, d, nlist, m, 8)# 每个向量都被编码为8个字节大小index.train(xb)index.add(xb)D, I = index.search(xb[:5], k) # sanity checkprint(I)print(D)#[[ 0 78 714 372]# [ 1 1063 555 277]# [ 2 304 134 46]# [ 3 773 64 8]# [ 4 288 531 827]]#[[1.6675376 6.1988335 6.4136653 6.4228306]# [1.4083313 6.023788 6.025648 6.284443 ]# [1.6988016 5.592166 6.139589 6.6717234]# [1.7987373 6.625978 6.7166452 6.865783 ]# [1.5371588 5.7953157 6.38059 6.4141784]]# 可以看到确实搜索到了正确的结果，但是第一行第一列的distance不为零，属于有损压缩。# 虽然与接下来的几列（其他几个搜索结果）对比还是有几倍的优势。index.nprobe = 10 # 与以前的方法相比D, I = index.search(xq, k) # searchprint(I[-5:]) 在涉及index使用考虑速度，acc和内存大小三个不同的维度。然后不同的index 是有不同的侧重的。 安装参考 Take Off(这个是有三方面需要权衡的： query time、 query accuracy and preprocessing time) As with anything, there is a tradeoff between improving query time versus query accuracy versus preprocessing/index build time versus data storage: no build time, high query time, high storage, exact accuracy: Faiss IndexFlat low build time, med query time, high storage, high accuracy: Faiss IndexIVFFlat med build time, low query time, low-med storage, med-high accuracy: Faiss IndexIVFPQ very high build time, low query time, low-high storage (whether stored as a k-NN graph or raw data), high accuracy: NN-Descent by Dong et al. (e.g., nmslib) IndexIVFPQ with perhaps IMI is typically what we concentrate on, seems to be a reasonable sweet spot for billion-scale datasets. product quantization 算法这里的乘积是指笛卡尔积（Cartesian product），意思是指把原来的向量空间分解为若干个低维向量空间的笛卡尔积，并对分解得到的低维向量空间分别做量化（quantization）。这样每个向量就能由多个低维空间的量化code组合表示。 The idea is to decomposes the space into a Cartesian product of low dimensional subspaces and to quantize each subspace separately. A vector is represented by a short code composed of its subspace quantization indices. Image Vector Dataset: 存储的是离 embedding 最近的centroid (质心) 的编号 而非向量本身。 Let’s say you have a collection of 50,000 images, and you’ve already performed some feature extraction with a convolutional neural network, and now you have a dataset of 50,000 feature vectors with 1,024 components each. The first thing we’re going to do is compress our dataset. The number of vectors will stay the same, but we’ll reduce the amount of storage required for each vector. Note that what we’re going to do is not the same as “dimensionality reduction”! This is because the values in the compressed vectors are actually symbolic rather than numeric, so we can’t compare the compressed vectors to one another directly. Two important benefits to compressing the dataset are that (1) memory access times are generally the limiting factor on processing speed, and (2) sheer memory capacity can be a problem for big datasets. Here’s how the compression works. For our example we’re going to chop up the vectors into 8 sub-vectors, each of length 128 (8 sub vectors x 128 components = 1,024 components). This divides our dataset into 8 matrices that are [50K x 128] each. These centroids are like “prototypes”. They represent the most commonly occurring patterns in the dataset sub-vectors. We’re going to use these centroids to compress our 1 million vector dataset. Effectively, we’re going to replace each subregion of a vector with the closest matching centroid, giving us a vector that’s different from the original, but hopefully still close. Doing this allows us to store the vectors much more efficiently—instead of storing the original floating point values, we’re just going to store cluster ids. For each subvector, we find the closest centroid, and store the id of that centroid. Each vector is going to be replaced by a sequence of 8 centroid ids. I think you can guess how we pick the centroid ids–you take each subvector, find the closest centroid, and replace it with that centroid’s id. Note that we learn a different set of centroids for each subsection. And when we replace a subvector with the id of the closest centroid, we are only comparing against the 256 centroids for that subsection of the vector. Because there are only 256 centroids, we only need 8-bits to store a centroid id. Each vector, which initially was a vector of 1,024 32-bit floats (4,096 bytes) is now a sequence of eight 8-bit integers (8 bytes total per vector!). K-means 算法k-Means算法是一种聚类算法，它是一种无监督学习算法，目的是将相似的对象归到同一个蔟中。蔟内的对象越相似，聚类的效果就越好。聚类和分类最大的不同在于，分类的目标事先已知，而聚类则不一样。其产生的结果和分类相同，而只是类别没有预先定义。 K-means 是一种聚类算法，先说一下什么是聚类：聚类分析是在数据中发现数据对象之间的关系，将数据进行分组，组内的相似性越大，组间的差别越大，则聚类效果越好。 K-Means算法思想：对给定的样本集，事先确定聚类簇数K，让簇内的样本尽可能紧密分布在一起，使簇间的距离尽可能大。该算法试图使集群数据分为n组独立数据样本，使n组集群间的方差相等，数学描述为最小化惯性或集群内的平方和。K-Means作为无监督的聚类算法，实现较简单，聚类效果好，因此被广泛使用。 算法步骤 创建k个点作为k个簇的起始质心（经常随机选择）。 分别计算剩下的元素到k个簇中心的相异度（距离），将这些元素分别划归到相异度最低的簇。 根据聚类结果，重新计算k个簇各自的中心，计算方法是取簇中所有元素各自维度的算术平均值。 将D中全部元素按照新的中心重新聚类。 重复第4步，直到聚类结果不再变化。 最后，输出聚类结果。 K-Means算法优缺点 优点 原理易懂、易于实现； 当簇间的区别较明显时，聚类效果较好； Trains quickly 缺点 当样本集规模大时，收敛速度会变慢； 对孤立点数据敏感，少量噪声就会对平均值造成较大影响, 所以离群点的检测和删除对于最后的结果有很大的帮助。 k的取值十分关键，对不同数据集，k选择没有参考性，需要大量实验 Choosing K The algorithm explained above finds clusters for the number k that we chose. So, how do we decide on that number? 尝试法： 计算每个点到最近的簇的距离的总和，如果增加 k 导致的总和下降不明显，那么就接近临界点了。 To find the best k we need to measure the quality of the clusters. The most traditional and straightforward method is to start with a random k, create centroids, and run the algorithm as we explained above. A sum is given based on the distances between each point and its closest centroid. As an increase in clusters correlates with smaller groupings and distances, this sum will always decrease when k increases; as an extreme example, if we choose a k value that is equal to the number of data points that we have, the sum will be zero. The goal with this process is to find the point at which increasing k will cause a very small decrease in the error sum, while decreasing k will sharply increase the error sum. This sweet spot is called the “elbow point.” In the image below, it is clear that the “elbow” point is at k-3.­ 总的来说faiss 高效实现了PCA 算法, k-means 算法 和PQ 算法。 ref 1ref 2ref 3ref 4 K-means &amp; KNN简而言之，KNN 是有监督分类学习，根据K 个最近邻的类别信息，通过投票的方式决定刚进来的数据点的类别；K-means是无监督聚类，K 表示最后簇的个数。算法步骤：首先随机选择K 个质心，然后分配每个数据点到最近的质心；然后计算不同的簇新的质心，接着重新分配，直到迭代次数足够或者质心的位置不再改变。 In short, the algorithms are trying to accomplish different goals. K-nearest neighbor is a subset of supervised learning classification (or regression) algorithms (it takes a bunch of labeled points and uses them to learn how to label other points). It is supervised because you are trying to classify a point based on the known classification of other points. In contrast, K-means is a subset of unsupervised learning clustering algorithms (it takes a bunch of unlabeled points and tries to group them into clusters). It is unsupervised because the points have no external classification. The $ k $ in each case mean different things. In K-NN, the $ k $ represents the number of neighbors who have a vote in determining a new player’s position. The $ k $ in K-means, determine the number of clusters we want to end up. In a K-NN algorithm, a test sample is given as the class of majority of its nearest neighbours. For example, if we have three classes and the goal is to find a class label for the unknown example $ x_j $ then, by using the Euclidean distance and a value of $ k=5 $ neighbors, the unknown sample is classified to the category of the most voted neighbors. How it works?Step 1: Determine the value for KStep 2: Calculate the distances between the new input (test data) and all the training data. The most commonly used metrics for calculating distance are Euclidean, Manhattan and MinkowskiStep 3: Sort the distance and determine k nearest neighbors based on minimum distance valuesStep 4: Analyze the category of those neighbors and assign the category for the test data based on majority voteStep 5: Return the predicted class The situation with K-means is that, given some data you will cluster them in k-groups or clusters. The initial step of the algorithm is to randomly spawn $ k $ centroids (centers). At every iteration the center of each cluster is moved slightly to minimize the objective function. The algorithm will terminate if the iterations are maximized or if the centroids stop to move. The objective function of K-means is $ J = \sum_{j=1}^{k}\sum_{i=1}^{n}\left |x_i^{j}-c_j \right |^{2} $ How it works?Step 1: Determine K value by Elbow method and specify the number of clusters KStep 2: Randomly assign each data point to a clusterStep 3: Determine the cluster centroid coordinatesStep 4: Determine the distances of each data point to the centroids and re-assign each point to the closest cluster centroid based upon minimum distanceStep 5: Calculate cluster centroids againStep 6: Repeat steps 4 and 5 until we reach global optima where no improvements are possible and no switching of data points from one cluster to other. 上图a表达了初始的数据集，假设k=2。在图b中，我们随机选择了两个k类所对应的类别质心，即图中的红色质心和蓝色质心，然后分别求样本中所有点到这两个质心的距离，并标记每个样本的类别为和该样本距离最小的质心的类别，如图c所示，经过计算样本和红色质心和蓝色质心的距离，我们得到了所有样本点的第一轮迭代后的类别。此时我们对我们当前标记为红色和蓝色的点分别求其新的质心，如图4所示，新的红色质心和蓝色质心的位置已经发生了变动。图e和图f重复了我们在图c和图d的过程，即将所有点的类别标记为距离最近的质心的类别并求新的质心。最终我们得到的两个类别如图f。 复习笔记 使用二叉树的结构，时间复杂度从 $O(N) $ 优化到了$log_2(N)$，当使用huffman 树的时候，这种效果更加明显。层次softmax 不是fasttext 的首创，它的改进之处在实现的时候基于 huffman 树而不是普通的二叉树， 属于运算上的优化。利用了类别不均衡的特点，类别多的路径短，整体上的时间效率会提高。 N-gram 一种是基于character-level 对于不常见单词的扩充，解决的是OOV问题；一种是word-level，考虑的是词语周边的信息，加入了context 的信息，local context 的信息。 negative sampling 是解决最后softmax 层中，不更新所有的negative words，只是更新少部分单词，根据词频选择negative words，并且这种词频是经过约束，主要是使得低频词语也有出现的机会。 调参分为字典相关的参数和训练相关参数 fasttext 的和之前 CBOW的区别：网络结构中的输入层，CBOW是经过one-hot的上下文单词，而fasttext 是单词+ n-gram 的特征，在解决OOV效果比较好；另外在最后的输出层，基于huffman 树实现了层次softmax，对于类别不均衡的训练集来说，训练时间会变得更短。 fasttext 的缺点，使用文本分类的时候，当类别比较多的时候提升效果比较明显，否则是容易过拟合的。 faiss 三种模式或者说索引。一种简单模式在小的数据集上计算欧式距离；一种加快检索的速度，使用聚类算法，检索的时候只是检索id 所在的簇和周围的簇，不过这个过程是需要预训练的；一种是减少内存的时候，如果是求解近似解，那么不必存储完整的向量，使用pca 降维。还有比较通用的加快速度的方式，比如分段计算和使用gpu 进行计算。 关于k-means中选择聚类簇k的个数的算法：尝试法。如果增大k，发现并不能使得指标明显的下降，这个时候就达到了阈值。指标：一个簇内所有的点到簇类中心的距离的总和。 knn 和k-means 的区别，前者是有监督的分类算法，根据测试点周围k 个点的类别信息判断该点的信息；k-means 是无监督算法，属于聚类中的一种。]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>k-means</tag>
        <tag>knn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NLP中的碎碎念(1)]]></title>
    <url>%2F2019%2F03%2F25%2FNLP%E4%B8%AD%E7%9A%84%E7%A2%8E%E7%A2%8E%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[顾名思义，主要整理一下自己在文本处理中遇到的小的知识点，比如关键词提取技术，分词软件包的原理。 关键词提取TF-IDF这个是可以参看之前自己写的一个博客 卡方分布卡方检验是以χ2分布为基础的一种常用假设检验方法。该检验的基本思想是：首先假设H0成立，基于此前提计算出χ2值，它表示观察值与理论值之间的偏离程度。 官方定义： 若k 个随机变量$Z_1$、……、$Z_k $相互独立，且数学期望为0、方差为 1(即服从标准正态分布)，则随机变量X$$X = \sum _ { n = 1 } ^ { k } Z _ { n } ^ { 2 }$$被称为服从自由度为 k 的卡方分布，记作$$X \sim \chi ^ { 2 } ( k )$$ 从直观的角度感受一下自由度k 和图像形状的关系，自由度越大卡方分布越接近正太分布。 用于特征选择 如果文章是否包含“篮球“与文章是否属于体育类别是独立无关的。且一个新闻文章属于体育类别的概率是0.609，那么可以得到下面的表格。假设文章是否包含“篮球“与文章是否属于体育类别是独立无关的，所以不管文章中是不是包含”篮球“，其属于体育类别的概率都是0.609。 列联表 组别 体育 非体育 包含”篮球“ 44 * 0.609 = 26.8 44 * 0.391 = 17.2 不包含”篮球“ 43 * 0.609 = 26.2 43 * 0.391 = 16.8 如果两个分类变量真的是独立无关的，那么四格表的实际值与理论值得差值应该非常小（有差值的原因是因为抽样误差）。那么如何衡量实际值与理论值得差值呢？ 步骤 统计每个词的正文档出现频率（A）、负文档出现频率（B）、正文档不出现频率）、负文档不出现频率。-计算每个词的卡方值 将每个词按卡方值从大到小排序，选取前k个词作为特征，k即特征维数。$$\operatorname { CHI } ( \mathrm { x } , \mathrm { y } ) = \chi ^ { 2 } ( x , y ) = \sum \frac { ( A - T ) ^ { 2 } } { T }$$该公式可以进一步简化成 （其中 x 表示一个特征，y 表示的target）$$\mathrm { CHI } ( \mathrm { x } , \mathrm { y } ) = \chi ^ { 2 } ( x , y ) = \frac { N ( A D - B C ) } { ( A + B ) ( A + C ) ( B + D ) ( C + D ) }$$ 组别 体育 非体育 合计 包含”篮球“ 34 (A) 10 (B) 44 (A+B) 不包含”篮球“ 19 (C) 24 (D) 43 (C+D) 合计 53(A +C) 34 (B+D) 87 (N) 卡方分布的临界值 自由度F = （行数 - 1） * （列数 - 1） = 1，对于四格表，F = 1。 由于自由度F = 1，所以只需要看分布表的第一行。可以看到，随着CHI的增大，原假设成立的概率就越小。因为10.10 &gt; 6.64，所以原假设成立是概率是小于1%。反之，也就是说，原假设不成立（即两个分类变量不是独立无关）的概率大于99% 如何应用于特征选择 CHI值越大，说明两个变量越不可能是独立无关的，也就是说X2越大，两个变量的相关程序也就越高。对于特征变量x1,x2,…,xn，以及分类变量y。只需要计算CHI(x1, y)、CHI(x2, y)、…、CHI(xn, y)，并按照CHI的值从大到小将特征排序，然后选择阈值，大于阈值的特征留下，小于阈值的特征删除。这样就筛选出一组特征子集了，接着使用这组特征子集去训练分类器，然后评估分类器的性能。 因为只要比较CHI值得相对大小，所以上述的分布表就没用了。 使用范围：一般是在离散变量上进行使用。 另外一个例子 In the case of classification problems where input variables are also categorical, we can use statistical tests to determine whether the output variable is dependent or independent of the input variables. If independent, then the input variable is a candidate for a feature that may be irrelevant to the problem and removed from the dataset. The Pearson’s chi-squared statistical hypothesis is an example of a test for independence between categorical variables. Contingency TableFor example, the Sex=rows and Interest=columns table with contrived counts might look as follows:下面是一个列联表123 Science, Math, ArtMale 20, 30, 15Female 20, 15, 30 The Pearson’s Chi-Squared test, or just Chi-Squared test for short, is named for Karl Pearson, although there are variations on the test. 如何去解读这种信息？ We can interpret the test statistic in the context of the chi-squared distribution with the requisite number of degress of freedom as follows: If Statistic &gt;= Critical Value: significant result, reject null hypothesis (H0), dependent.If Statistic &lt; Critical Value: not significant result, fail to reject null hypothesis (H0), independent.The degrees of freedom for the chi-squared distribution is calculated based on the size of the contingency table as: degrees of freedom: (rows - 1) * (cols - 1) In terms of a p-value and a chosen significance level (alpha), the test can be interpreted as follows: If p-value &lt;= alpha: significant result, reject null hypothesis (H0), dependent.If p-value &gt; alpha: not significant result, fail to reject null hypothesis (H0), independent.For the test to be effective, at least five observations are required in each cell of the contingency table. case study： Chi-square Test for feature selection $$X ^ { 2 } = \frac{ {(Observed frequency - Expected frequency)} ^ 2 } { Expected frequency }$$ 12345678910111213141516171819202122# Load libraries from sklearn.datasets import load_iris from sklearn.feature_selection import SelectKBest from sklearn.feature_selection import chi2 # Load iris data iris_dataset = load_iris() # Create features and target X = iris_dataset.data y = iris_dataset.target # Convert to categorical data by converting data to integers X = X.astype(int) # Two features with highest chi-squared statistics are selected chi2_features = SelectKBest(chi2, k = 2) X_kbest_features = chi2_features.fit_transform(X, y) # Reduced features print('Original feature number:', X.shape[1]) print('Reduced feature number:', X_kbest.shape[1]) Original feature number: 4Reduced feature number : 2 而实际应用到特征选择中的时候，我们不需要知道自由度，不要知道卡方分布，我们只需要根据算出来的χ2 进行排序就好了，越大我们就越喜欢！挑选最大的一堆，于是就完成了利用卡方检验来进行特征提取。卡方分布的缺点：没有考虑词频，它只统计文档是否出现词，而不管出现了几次。这会使得他对低频词有所偏袒（因为它夸大了低频词的作用）。 参考一参考二 CBOW和skip-gram举一个简单的小例子说明 CBOW和skip-gram的区别：skip-gram 是根据中心词汇然后预测上下文词汇，这个不是一下子输入上下文词汇的，而是一个过程，中心词汇和上下文词汇1 ，中心词汇和上下文词汇2，这样的进行输入。 cbow 和其的区别，在于简单相加了上下文词汇作为一个整体，然后和中心词汇进行输入，所以最后是这样的结果。 使用skip gram训练的时间更长，但是对于出现频率不高的词汇，效果比较好。但CBOW的训练速度是相对来说比较快一些。 分词软件常见的中文分词服务 分词服务 开源/商业 支持语言 词性标注 命名实体识别 jieba 开源 Python, Java, C++ 无 无 HanLP 开源 Python，Java， C++ 有 有 StandFord CoreNLP 开源 Java 百度NLP 商业 阿里NLP 商业 Stanford NLP是由斯坦福大学的 NLP 小组开源的 Java 实现的 NLP 工具包。可以使用python 调用 Stanford NLP 进行中文分词 HanLP是由一系列模型与算法组成的Java工具包。也可以使用Python调用HanLP进行中文分词。 结巴分词的算法策略 基于前缀词典实现高效的词图扫描，生成句子中汉字所有可能成词情况所构成的有向无环图 (DAG) 采用了动态规划查找最大概率路径, 找出基于词频的最大切分组合 对于未登录词，采用了基于汉字成词能力的 HMM 模型，使用了 Viterbi 算法 如果想更加详细的了解这种策略，可以参考这里. 中文分词主要掌握规则的分词方法（逆向最大匹配法（该方法是优于正向的）、有个词频-内连-外 的分词算法， 总共两种算法就可以了） 中文分词的困难主要体现在：歧义词的切分和未登录词识别。其中后者的影响要远远大于前者的影响。 下面是常见的分词方式，但是并没有说明哪个软件是什么分词方法。（需要掌握两种分词的方式，一种是基于词典的逆向最大匹配，一种是后面新词发现中的基于统计的信息熵算法） 基于规则的分词方法 这种方法又叫作机械分词方法、基于字典的分词方法，它是按照一定的策略将待分析的汉字串与一个“充分大的”机器词典中的词条进行匹配，比如说正向最大匹配法、逆向最大匹配法 基于统计的分词方法 该方法的主要思想：词是稳定的组合，因此在上下文中，相邻的字同时出现的次数越多，就越有可能构成一个词。因此字与字相邻出现的概率或频率能较好地反映成词的可信度。可以对训练文本中相邻出现的各个字的组合的频度进行统计，计算它们之间的互现信息。互现信息体现了汉字之间结合关系的紧密程度。当紧密程 度高于某一个阈值时，便可以认为此字组可能构成了一个词。该方法又称为无字典分词。 比如说CRF 算法。 基于语义的分词方法 语义分词法引入了语义分析，对自然语言自身的语言信息进行更多的处理，如扩充转移网络法、知识分词语义分析法、邻接约束法、综合匹配法、后缀分词法、特征词库法、矩阵约束法、语法分析法等。 逆向最大匹配分词是中文分词基本算法之一，因为是机械切分，所以它也有分词速度快的优点，且逆向最大匹配分词比起正向最大匹配分词更符合人们的语言习惯。逆向最大匹配分词需要在已有词典的基础上，从被处理文档的末端开始匹配扫描，每次取最末端的i个字符（分词所确定的阈值i）作为匹配字段，若匹配失败，则去掉匹配字段最前面的一个字，继续匹配。而且选择的阈值越大，分词越慢，但准确性越好。这种方法经常被用来解决歧义词的问题，所以单独说一下。 算法：事先设置一个k值，下面的程序k值设为5，然后从最后一个字开始向前截取k个字，先把这k个字和字典匹配，看能否找到匹配的词语，若不能，则剔除这k个字最左边的字，然后再把这k-1个字与字典匹配…一直到匹配成功，或者前k-1个字都没匹配成功，那就把第k个字当成一个独立的词，然后再向前移动分出来的词的长度，再截取k个字……一直到全部分好词为止。“我爱北京天安门” 先从后面开始截取k(这里是5)个字，然后把”北京天安门”五个字与字典匹配，字典中没有这个词，然后就去掉”北”字，把剩下的”京天安门”与字典匹配，字典中还是没有这个词，再去掉”京”，然后再把”天安门”与字典匹配，发现匹配到了这个词，于是就把”天安门”划为一个词语，然后指针向前移动三个字。再截取k个字，这里因为就剩下4个字了，所以就截取4个字，把”我爱北京”与字典匹配，没成功，去掉”我”，再把”爱北京”与字典匹配，还是没成功，再去掉”爱”，然后发现”北京”匹配成功，把”北京”划为一个词语，再把指针向前移动两个字， CRF算法基本思路是对汉字进行标注训练，不仅考虑了词语出现的频率，还考虑上下文，具备较好的学习能力，因此其对歧义词和未登录词的识别都具有良好的效果。 近年来，随着硬件计算能力的发展以及词的分布式表示（word embedding）的提出，神经网络可以有效处理许多NLP任务。这类方法对于序列标注任务（如CWS、POS、NER）的处理方式是类似的：将token从离散one-hot表示映射到低维空间中成为稠密的embedding，随后将句子的embedding序列输入到RNN中，用神经网络自动提取特征，Softmax来预测每个token的标签。这种方法使得模型的训练成为一个端到端的过程，而非传统的pipeline （管道，指的是一个包含多步骤的流水式的工作），不依赖于特征工程，是一种数据驱动的方法，但网络种类繁多、对参数设置依赖大，模型可解释性差。 推荐博客讲解 CRF算法，反正我是没有看懂，哈哈哈。【中文分词】条件随机场CRF, 浅谈分词算法（4）基于字的分词方法（CRF） 和 CRF算法学习——自己动手实现一个简单的CRF分词 分词粒度目前，分词主要包含细粒度分词和粗粒度分词两种，在不同的应用场景需要用到不同的粒度。细粒度分词是指将原始语句切分成最基本的词语，而粗粒度分词是指将原始语句中的多个基本词组合起来切成一个词，进而组成语义相对明确的实体。 原始串：浙江大学坐落在西湖旁边 细粒度：浙江/大学/坐落/在/西湖/旁边 粗粒度：浙江大学/坐落/在/西湖/旁边 举例说明分词粒度和应用场景：对于query为“耐克鞋”来说，词典中是包含这个实体词的，分词的切分结果就是“耐克鞋”。但是，考拉用来建索引的商品描述中，这三个字是没有连续出现的，自然就没有“耐克鞋”对应的文档，这个时候就无法召回结果。那么，这时候你会说那就使用最小粒度的分词就解决这个问题了。相反，某一款商品可能有“参数表”这三个字，如果有最小粒度的分词策略，分词的结果为“参数/表”。很不幸，当query为“表”时，你会发现会召回莫名其妙的结果。一般来说，是使用词频表来进行粒度控制，基本可以解决绝大多数问题。 搜索引擎展现与粒度：显而易见，粒度越小，展现就越多，建立倒排索引时，索引的长度就越长;粒度的层次越多，索引的数量就越多。一个多，一个长，就对搜索系统的性能构成了极大的考验。搜索引擎并不会对所有小粒度词都建索引，而是选择“更有可能展现相关结果”的小粒度词。所以在一般情况下，切分文本粒度越大，索引越多，相关性越好，但展现越少;切分文本粒度越小，索引越少，相关性越差，但展现越好。 词频决定分词粒度：具体算法是，当发现一个由多个短词组成的长词时，判断每个短词中最小的词频，如果这个词频还是大于长词的词频，则按该组合进行拆分。如果多种组合，按词频最大的组合拆分。如上面例子，”中央饭店”，中央的词频为1000，饭店为900,饭为200,店为600，而中央饭店为500 。 OOV问题由来：由于词频过小被替换成了 UNK token 或者是在原始的训练数据集中没有出现而在测试集中出现了。 常见的解决方案： subword (n-gram ) 分字和词语两个维度。西欧语系中是有字维度，在中文中没有。对于西欧语系的词汇，是可以通过 n-gram；如果推广到中文，那么更加合适使用分词之后的结果作为一个 word，然后每个字作为一个gram在训练过程中，每个n-gram都会对应训练一个向量，而原来完整单词的词向量就由它对应的所有n-gram的向量求和得到。所有的单词向量以及字符级别的n-gram向量会同时相加求平均作为训练模型的输入。优点：解决了 低频词 oov 问题；缺点：需要估计的参数量变多 transfer learning (fine tune) 使用 context embedding 去表示缺省的 中心词（oov） 文献 新词发现在中文分词的世界里，最主要的挑战有两个：歧义词识别，未登录词（新词）识别。对于歧义词简单说一下，比如“乒乓球拍卖完了”，切分为以下两种情况都是合理的，“乒乓球拍/卖/完了”，“乒乓球/拍卖/完了”。这个就是典型的歧义词。而处理这种问题常用的一种手段是使用逆向最大匹配法去处理歧义词，由于汉语中偏正结构较多，若从后向前匹配，可以适当提高精确度。比如取出“南京市长江大桥”的后四个字“长江大桥”，发现词典中有匹配，切割下来；对剩余的“南京市”进行分词，整体结果为：“南京市/长江大桥”。 新词对最后结果的影响程度是大于歧义词（20倍左右吧）。 so，这里主要讲的是新词的发现 而非处理歧义词的识别。 中英文分词的区别 分词 / 词干提取和词形还原。 中文和英文的nlp各有各的难点，中文的难点在于需要进行分词，将一个个句子分解成一个单词数组。而英文虽然不需要分词，但是要处理各种各样的时态，所以要进行词干提取和词形还原。比如 词干提取主要采取“缩减”的方式，“cats” 处理为 “cat”， 将“effective” 处理为“effect”； 词性还原主要采取转变的方式，“drove” 处理成“drive”， 将“driving” 处理成“drive”。 对于中文，一个Word可以是一个单词，也可以是一个词。 分词的重要性 在处理文本对象时，非常关键的问题在于“切词”这个环节，几乎所有的后续结果都依赖第一步的切词。因此切词的准确性在很大程度上影响着后续的处理，切词结果的不同，也就影响了特征的提取，跟数据挖掘一样，特征提取的好坏特别重要，不论用什么算法，特征好数据好结果才会好。 分词的目的 当模型的记忆和拟合能力足够强（或者简单点，足够智能）的时候，我们完全可以不用分词的，直接基于字的模型就可以做，比如基于字的文本分类、问答系统等，早已有人在研究。但是，即便这些模型能够成功，也会因为模型复杂而导致效率下降，因此，很多时候（尤其是生产环境中），我们会寻求更简单、更高效的方案。比如之前我们一直是word embedding，但是 sentence embedding 也是在学术界很流行的，最后没有大规模的采用，无非就是工业级要求更加高效的方式，有时候会牺牲一些精度。 目前很多的切词模块可以处理大部分的通用语料，然而有两类文本集仍然处理的不是很好，就是： 网络文档 领域文档 后者可以有对应的专家 handle，但涉及到商用这种人力成本也是比较高的了，所以一般使用基础词汇+ 各领域的常用词汇。即使这样的方案仍然是无法可持续的，所以需要一种算法去判断是否是新词。在当下的互联网时代，人们还会不断的创造出一些新词出来，比如：“神马”、“不明觉厉”等。未登录词辨别未登录词包括是种类繁多，形态组合各异，规模宏大的一个领域。对这些词语的自动辨识，是一件非常困难的事。比如，“美的”、“快的”、“英雄联盟”应该被作为一个词，却被切成了两个词，失去了原有的语义。未登录词（out-of-vocabulary, OOV）笼统地之未在词典中出现的词， 人工标注可以解决很好识别，比如最典型的未登录词就是人名，尤其是明星，然后最简单的是手动的维护，但是人力成本也是比较高昂的。 从分词的角度来看，新词一般表现为细粒度切分后相邻词的组合。 基于统计的新词发现 基于信息熵的新词发现算法，从左右信息熵和互信息入手，成词的标准有两个： 内部凝固度 自由运用程度 内部凝固度和自由运用程度分别考虑是词语内部的紧密程度和外部搭配的丰富性。所谓内部凝固度，用来衡量词搭配（collocation）是否合理。比如，对于“的电影”、“电影院”这两个搭配，直观上讲“电影院”更为合理，即“电影”和“院”凝固得更紧一些。在计算语言学中，PMI (Pointwise mutual information)被用来度量词搭配与关联性，定义如下： $$p m i ( x , y ) = \log \frac { P ( x , y ) } { P ( x ) P ( y ) }$$ 若PMI高，即两个词共现（co-occurrence）的频率远大于两个词自由拼接的乘积概率，则说明这两个词搭配更为合理一些。针对一个词有多种搭配组合，比如“电影院”可以由“电影”+“院”构成，也可以由“电”+“影院”构成，那么取其所有pmi最小值（去掉log）作为内部凝固度：$$\operatorname { solid } \left( c _ { 1 } ^ { m } \right) = \min \frac { P \left( c _ { 1 } ^ { m } \right) } { \prod P \left( c _ { i } ^ { j } \right) } = \frac { P \left( c _ { 1 } ^ { m } \right) } { \max \prod P \left( c _ { i } ^ { j } \right) }$$ 其中， $c _ { 1 } ^ { m } = c _ { 1 } c _ { 2 } \cdots c _ { m }$表示长度为 $m$ 的字符串，$P \left( c _ { 1 } ^ { m } \right)$ 表示$c _ { 1 } ^ { m }$ 的频率。 光看文本片段内部的凝合程度还不够，我们还需要从整体来看它在外部的表现。考虑“被子”和“辈子”这两个片段。我们可以说“买被子”、“盖被子”、“进被子”、“好被子”、“这被子”等，在“被子”前面加各种字；但“辈子”的用法却非常固定，除了“一辈子”、“这辈子”、“上辈子”、“下辈子”，基本上“辈子”前面不能加别的字了。“辈子”这个文本片段左边可以出现的字太有限，以至于直觉上我们可能会认为，“辈子”并不单独成词，真正成词的其实是“一辈子”、“这辈子”之类的整体。 先简单的介绍熵的概念，熵是一种表示信息量的指标，熵越高就意味着信息含量越大，不确定性越高，越难以预测，信息也就越丰富。$$H ( X ) = - \sum _ { x \in X } p ( x ) \log _ { 2 } p ( x )$$ 所以，提出了自由运用程度，用以衡量一个词的左邻字与右邻字的丰富程度。正好信息熵可以完美地诠释了这种丰富程度，熵越大则丰富程度越高。“被子”和“辈子”这两个片段的左邻字熵le与右邻字熵re分别如下 le(被子) = 3.67453re(被子) = 3.8740le(辈子) = 1.25963re(辈子) = 4.11644 可以看出，“被子”的左邻字熵与右邻字熵都较高，而“辈子”的左邻字熵较小，即左邻字非常贫乏。因此，“被子”较“辈子”更有可能成词。自由运用程度的定义如下： 给频数、内部凝固度与自由运用程度设定一个阈值，提取出来符合阈值的候选词，去掉词典中存在的词即为新词了。所以两者都高于某个对应的阈值，那么说明这个是一个新词。 实现： 使用射雕英雄传txt 作为文本，然后词频、内部凝固度 和自由程度进行新词识别。代码。 词频这点很好理解,因为不是词的话出现的频率一般比较低,词的出现频率会比较高.所以可以设置一个词频阀值,高于这个阀值的判断为词,否则判定为不是词. 凝固度 基于上一步词频的挑选。 自由度 基于上述分词进行挑选。 NLP 中的三类特征提取器 NLP 和图像中数据的特征 NLP的输入往往是一句话或者一篇文章，所以它有几个特点：首先，输入是个一维线性序列，这个好理解；其次，输入是不定长的，有的长有的短，而这点其实对于模型处理起来也会增加一些小麻烦；再次，单词或者子句的相对位置关系很重要，两个单词位置互换可能导致完全不同的意思。 NLP 中的四大任务 序列标注： 分词/ POS Tag /NER /语义标注 分类任务： 文本分类/ 情感计算 句子关系判断： Entailment /QA / 自然语言推理 生成式任务： 机器翻译/ 文本摘录 一类是序列标注，这是最典型的NLP任务，比如中文分词，词性标注，命名实体识别，语义角色标注等都可以归入这一类问题，它的特点是句子中每个单词要求模型根据上下文都要给出一个分类类别。第二类是分类任务，比如我们常见的文本分类，情感计算等都可以归入这一类。它的特点是不管文章有多长，总体给出一个分类类别即可。第三类任务是句子关系判断，比如Entailment，QA，语义改写，自然语言推理等任务都是这个模式，它的特点是给定两个句子，模型判断出两个句子是否具备某种语义关系；第四类是生成式任务，比如机器翻译，文本摘要，写诗造句，看图说话等都属于这一类。它的特点是输入文本内容后，需要自主生成另外一段文字。 深度学习最大的优点是 “端到端” NLP中的任务很多，哪些任务是最具有代表性的呢？答案是机器翻译。 回归主题，特征提取器 RNNRNN模型结构参考上图，核心是每个输入对应隐层节点，而隐层节点之间形成了线性序列，信息由前向后在隐层之间逐步向后传递。 RNN也存在问题，它采取线性序列结构不断从前往后收集输入信息，但这种线性序列结构在反向传播的时候存在优化困难问题，因为反向传播路径太长，容易导致严重的梯度消失或梯度爆炸问题。为了解决这个问题，后来引入了LSTM和GRU模型，通过增加中间状态信息直接向后传播，以此缓解梯度消失问题，获得了很好的效果，于是很快LSTM和GRU成为RNN的标准模型。 （Attention机制最早是在视觉图像领域提出来的，但是真正火起来应该算是2014年google mind团队的论文《Recurrent Models of Visual Attention》，他们在RNN模型上使用了attention机制来进行图像分类） 为什么RNN能够这么快在NLP流行并且占据了主导地位呢？主要原因还是因为RNN的结构天然适配解决NLP的问题，NLP的输入往往是个不定长的线性序列句子，而RNN本身结构就是个可以接纳不定长输入的由前向后进行信息线性传导的网络结构，而在LSTM引入三个门后，对于捕获长距离特征也是非常有效的。所以RNN特别适合NLP这种线形序列应用场景，这是RNN为何在NLP界如此流行的根本原因。 那么为什么有衰弱了？RNN本身的序列依赖结构对于大规模并行计算来说相当之不友好。通俗点说，就是RNN很难具备高效的并行计算能力。如果适合在学术界发论文，那么不太可能在工业界广泛的使用。 CNN 特征提取器卷积层本质上是个特征抽取层，可以设定超参数F来指定卷积层包含多少个卷积核（Filter）。对于某个Filter来说，可以想象有一个d*k大小的移动窗口从输入矩阵的第一个字开始不断往后移动，其中k是Filter指定的窗口大小，d是Word Embedding长度。对于某个时刻的窗口，通过神经网络的非线性变换，将这个窗口内的输入值转换为某个特征值，随着窗口不断往后移动，这个Filter对应的特征值不断产生，形成这个Filter的特征向量。这就是卷积核抽取特征的过程。卷积层内每个Filter都如此操作，就形成了不同的特征序列。Pooling 层则对Filter的特征进行降维操作，形成最终的特征。一般在Pooling层之后连接全联接层神经网络，形成最后的分类过程。 CNN 捕捉到的是什么信息？ 关键在于卷积核覆盖的那个滑动窗口，CNN能捕获到的特征基本都体现在这个滑动窗口里了。大小为k的滑动窗口轻轻的穿过句子的一个个单词，荡起阵阵涟漪，那么它捕获了什么?其实它捕获到的是单词的k-gram片段信息，这些k-gram片段就是CNN捕获到的特征，k的大小决定了能捕获多远距离的特征。 卷积操作是通过加深层数，然后获得远距离的特征的。所以有两种解题思路： 一种是增加窗口大小k 增大；一种是加深深度。 简单谈一下CNN的位置编码问题和并行计算能力问题。CNN的卷积层其实是保留了相对位置信息的，只要你在设计模型的时候别手贱，中间层不要随手瞎插入Pooling层，问题就不大，不专门在输入部分对position进行编码也行。至于CNN的并行计算能力，那是非常强的，这其实很好理解。我们考虑单层卷积层，首先对于某个卷积核来说，每个滑动窗口位置之间没有依赖关系，所以完全可以并行计算；另外，不同的卷积核之间也没什么相互影响，所以也可以并行计算。CNN的并行度是非常自由也非常高的，这是CNN的一个非常好的优点。 Transformer看看 Transformer 对于NLP 任务中的解决方案： 不定长的输入：Transformer 一般设置最大的长度，不够了 就padding，然后多了就 去尾。 单词之间的相对位置： Transformer是用位置函数来进行位置编码的，而Bert等模型则给每个单词一个Position embedding，将单词embedding和单词对应的position embedding加起来形成单词的输入embedding 长依赖问题： self attention机制 对于Transformer来说，Multi-head attention的head数量严重影响NLP任务中Long-range特征捕获能力：结论是head越多越有利于捕获long-range特征。（对标filter 的个数） 我个人意见是：这说明Transformer之所以能够效果这么好，不仅仅multi-head attention在发生作用，而是几乎所有构件都在共同发挥作用，是一个小小的系统工程。 参考文献 放弃幻想，全面拥抱Transformer BLEUBLEU (bilingual evaluation understudy) 读音:波勒 not blue. 完美匹配的得分为1.0，而完全不匹配则得分为0.0。 虽然是为机器翻译提出，但后来也广泛用于NLP其他领域。 可以根据一个数值指标来衡量其与一个或多个人工参考翻译的接近程度。 原理介绍下面三个概念： N-gram， 惩罚因子和Bleu 算法。 N-gram N-gram是一种统计语言模型，该模型可以将一句话表示n个连续的单词序列，利用上下文中相邻词间的搭配信息，计算出句子的概率，从而判断一句话是否通顺。 通过例子学习 原文： 猫坐在垫子上机器翻译：The cat sat on the mat.人工翻译：The cat is on the mat. 1 gram 匹配度是 5/6 同理可以得到 2 gram, 3 gram, 4 gram 分别是 3/5, 1/4 和 0. 处理一些特殊的情况： 原文：猫坐在垫子上机器译文： the the the the the the the.参考译文：The cat is on the mat. 所以当 1 gram 的时候，匹配度是 7/7，不合理的。 BLEU修正了这个算法，提出取机器翻译译文N-gram的出现次数和参考译文中N-gram最大出现次数中的最小值的算法，具体如下： $$Count_{clip} =min(Count, Max_Ref_Count)$$ 那么修正之后的 1 gram 的匹配度就是 2/7 论文中 N-gram 的计算公式： $$P_n =\frac{ \sum_{C \in {Candidates} } \sum_{n-gram \in C} Count_{clip}(n-gram)}{\sum_{C \in {Candidates} } \sum_{n-gram \in C^{‘}} Count_{clip}(n-gram^{‘})} $$ 分子表示翻译译文（机器翻译），然后分母表示参考译文（人工翻译） 惩罚因子 如果出现了这样的情况： 机器译文：The cat参考译文：The cat is on the mat. 如果出现这种短句子，你会发现计算n-gram的精度会得很高分，很显然这次的得分为1，但实际上它的得分应该是比较低的。针对翻译译文长度比参考译文要短的情况，就需要一个惩罚的机制去控制。BP 主要是处理句子长度的问题。 $$ BP = \begin{cases}1 &amp; c &gt;r \\e ^ { ( 1 - r / c ) } &amp; c &lt;= r\end{cases}$$ 这里的c是机器译文的词数，r是参考译文的词数 所以计算得：$ BP = e^(1- 6 / 2) = 7.38905609893065 $ BLEU 算法 $$\mathrm { B } \mathrm { LEU } = \mathrm { BP } \cdot \exp \left( \sum _ { n = 1 } ^ { N } w _ { n } \log p _ { n } \right)$$ 对于BP 已经知道，后面的其实就是一些数学运算，它的作用就是让各阶n-gram取权重服从均匀分布，就是说不管是1-gram、2-gram、3-gram还是4-gram它们的作用都是同等重要的。由于随着n-gram的增大，总体的精度得分是呈指数下降的，所以一般N-gram最多取到4-gram. 计算 第一步：计算各阶n-gram的精度第二步：加权求和 (一般就是平均)第三步：求BP最后求BLEU. 优缺点BLEU原理其实并不是很复杂，BLEU容易陷入常用词和短译句的陷阱中， 所以更多的是基于n-gram 基础上的优化。BLEU本身就不追求百分之百的准确性，也不可能做到百分之百，它的目标只是给出一个快且不差的自动评估解决方案。 优点： 计算速度快；容易理解；已经被广泛使用 缺点： 短译句的测评精度有时会较高 它没有考虑句子意义 参考文献机器翻译质量评测算法-BLEU 复习记录： 分词两大问题：歧义词和oov。后者对于最后的效果影响更大。处理OOV 常用的方法：subword；使用多个word embedding（transfer learning）； 使用context embedding进行分词常用的算法：基于规则的逆向最大匹配（中文的特点），频数；基于信息熵的内部凝固度和自由运用程度（分别考虑词语内部的紧密程度和外部搭配的丰富程度）；深度学习+NER进行的词性标注，条件随机场CRF之类的。 中英文语料预处理的区别：中文需要分词，英文需要处理时态，对于名词和形容词处理成标准名词，对于动词进行词性还原。 NLP中数据的特征： 输入是一维线性 输入是不定长 单词或者子句的相对位置很重要 RNN 的缺点，首先是捕捉长距离依赖，可以被LSTM 比较有效的解决；另一个缺点，RNN 这种序列的线性结构对于并行运算是不利的。CNN 的特点关键在于滑动窗口]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>关键词提取</tag>
        <tag>oov</tag>
        <tag>中文分词</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python中的多线程和多进程]]></title>
    <url>%2F2019%2F03%2F25%2FPython%E4%B8%AD%E7%9A%84%E5%A4%9A%E7%BA%BF%E7%A8%8B%E5%92%8C%E5%A4%9A%E8%BF%9B%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[多线程和多进程问题是可以对应到 并发 （cncurrency）和并行(parallelism)上的。 并发，就是一个单核cpu同时开始了多个任务，但是这个任务并不是同时独立进行的，而是通过cpu的不断切换，保存现场，然后重启这样的快速的切换，给用户的感觉是并发，但是实际上是cpu的计算能力受到了限制，用户体验比较好一些。如果在多核cpu （比如我的mac 是一cpu 6核）这样的话完全是可以达到并行的，这个是真正的独立操作(parallelism)，对应的是多进程的。对应python 中的实现多线程是使用threading，处理的是io 响应；多进程是Concurrency，使用multiprocessing包，处理的是多核cpu的操作。 Take off: 如果处理io 响应，那么使用多线程；如果是计算，那么使用多进程。 So, before we go deeper into the multiprocessing module, it’s worthwhile ensuring you know the advantages of using multiprocessing over multithreading. The general rule of thumb is that, if you are trying to improve the performance of CPU-bound tasks, multiprocessing is what you want to use. However, if your particular task is Input/Output bound, then you’ll generally want to use multithreading to improve the performance of your applications. 这个是多线程的demo 响应的io 请求。 1234567891011121314151617181920212223242526272829import threadingclass Worker(threading.Thread): # Our workers constructor, note the super() method which is vital if we want this # to function properly def __init__(self): super(Worker, self).__init__() def run(self): for i in range(10): print(i)def main(): thread1 = Worker() thread1.start() thread2 = Worker() thread2.start() thread3 = Worker() thread3.start() thread4 = Worker() thread4.start()if __name__ == "__main__": main() 下面是多进程的demo响应的计算请求。 123456789101112131415161718import multiprocessing as mpdef my_func(x): print(mp.current_process()) return x ** xdef main(): pool = mp.Pool(mp.cpu_count()) # 这个还是很好的 pool 这个的个数和你的cpu count 是保持一致的 result = pool.map(my_func, [4, 2, 3, 5, 3, 2, 1, 2]) result_set_2 = pool.map(my_func, [4, 6, 5, 4, 6, 3, 23, 4, 6]) print(result) print(result_set_2)if __name__ == "__main__": main()]]></content>
      <categories>
        <category>Python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Data Pre-processing 学习笔记]]></title>
    <url>%2F2019%2F03%2F25%2FData-Pre-processing%2F</url>
    <content type="text"><![CDATA[主要介绍机器学习中的数据预处理，包括 data cleaning、data integration、 data transformation、data reduction、data imbalanced 和一些概念。 data normalization 是经常用到的。 数据预处理的时候需要注意 missing values、noisy data (outlier) ，对于这种现象应该分成要不要处理和如何处理两个问题。 数据清洗 (缺省值、异常值的处理)、数据整合 (子表 合并到 or 成主表)、数据转换 (one-hot or label encoding， 连续数值离散化)、和数据降维 (可以单独的成一章)。这个几个步骤应该是熟记于心的。 Data Cleaning这个步骤主要处理 missing values 和 noisy data (outlier).对于missing values ，可以分成两个问题，要不要处理 和如何处理，具体说来有以下处理手段： ignore the tuple; fill in the missing value manually use a global constant to fill in the missing value use the attribute mean to fill in the missing value (均值) use the most probable value to fill in the missing value (mode 众数) 有时候就是根据某几个特征然后弄一个简单的回归模型，根据模型进行predict 关于这几种方法如何去选择，我如果说 “it depends”，那么其他人不认为这是一个具有说服力的答案，他们更像知道 it depends what, and when and why to use specific method? 我认为应该是根据缺省值程度和重要性进行经验性的选择，这也去就是 empirical study吧。 接着是 noisy data (outlier)，我的观点是首先得认识到这个是错误的数据，不是真实的数据来源，可能是来自人为的笔误 或者仪器记录的问题，这个是需要修改的。可以使用聚类 (clustering) 进行noisy data 的检测，找到之后这个就类似 missing value了，可以采取以上的手段进行操作，应该注意到的这个 noisy data 所占比例不会很高，否则就成了主要的数据分布了。 Data Integration:处理数据库数据，经常是需要处理子表信息的，那么必然存在着主表，而子表系信息往往是主表信息的某一方面的细化。所以有必要将两者连接起来。 Data Transformation:In data transformation, the data are transformed or consolidated into forms appropriate for mining.这里想要澄清的是很多相同的内容都可以用不同的方式表达，并且可以放在数据处理的不同阶段，并且这种工作不是一次性完成的，而是迭代的 until you run out your patience and time.首先我接触的最常见的就是 discrete variables -&gt; continuous variables. 当然对于 discrete variables，基于树结构的机器学习模型是可以处理的，这里想说的是有这种方式。这种 transformation 常见的处理方式: one-hot 或者 label encoding. 如果按照 data transformation的预设，那么 normalization 就也属于该模块的内容。 不论是在 machine learning 还是在 图像处理的时候，对于原始的数据经常采取 normalization. 一方面这个是可以预防梯度消失 或者 gradient exploding, 如果你采用了 Sigmoid的激活函数的话。另一方面我认为更加重要的原因是将 不同的数据放在了同一个尺度下，如果你采取了 normalization之后。 Data Reduction:一般来说很少提及到到 data reduction的必要性，如果非要给出原因，那么可以从时间和空间的角度进行考虑。更加需要关注的是如何做的问题。 我的理解reduction 可以从两个维度进行考虑，假设一个 matrics A 是 m*n，这个是一个二维的矩阵，那么可以从 行列两方面入手。映射到机器学习中一般这样描述 从dimension 和 data两个角度去描述，分别称之为 dimension reduction 和 data compression. 前者指的是特征的选取，后者是数据size的减少。dimension reduction: where irrelevant, weakly relevant, or redundant attributes or dimensions may be detected and removed.data compression: PCA 线性降维 to reduce the data set size. 这个是针对某一个特征展开的。 Data Normalization 是什么？ 中心化： 均值为0，对方差没有要求 $$x ^ { \prime } = x - \mu$$ 标准化：服从正太分布 (0, 1) $$x ^ { \prime } = \frac { x - \overline { x } } { \sigma }$$ 归一化有两种： mean normalization 和 min-max normalization mean normalization$$x ^ { \prime } = \frac { x - \operatorname { mean } ( x ) } { \max ( x ) - \min ( x ) }$$ min-max normalization $$x ^ { \prime } = \frac { x - \operatorname { min } ( x ) } { \max ( x ) - \min ( x ) }$$ 为什么？ 提高模型的准确率比如说两个特征，一个特征的范围是0-100， 另一个是-2000 到2000， 这个使用 欧式距离进行计算的时候，两个特征的差值很大，特征并没有站在同一个维度上。 提高模型的速度还是上面的例子，两个特征x1 x2的取值范围比较大，那么学习率上变得波动，所以学习时间会变长。 深度学习中数据归一化可以防止模型梯度爆炸以sigmoid 函数为例解释就行 怎么做？定义就是表示怎么做 适用范围 概率模型不需要归一化，因为它们不关心变量的值，而是关心变量的分布和变量之间的条件概率，如决策树、rf。而像adaboost、svm、lr、KNN、KMeans之类的最优化问题就需要归一化。 映射到 N(0,1) 的这种行为，叫做归一化。Feature scaling is the method to limit the range of variables so that they can be compared on common grounds. 有三个主要的原因。 Because most of the Machine Learning models are based on Euclidean Distance. Age- 40 and 27Salary- 72000 and 48000 这两个特征，这两个距离相差很大；但是这个并不是我们想要的，我们想要的是相对值，而不是绝对值。 即使最后的loss function不是 euclidean distance，比如说decision tree，实践证明经过正则化的之后的数据的训练速度是快于 没有经过正则化的数据的。 经过归一化之后，数据是不容易出现梯度消失或者梯度爆炸的。 很多模型的基本假设 就是 N(0,1) 高斯分布。 实现的三种手段： rescaling (min-max normalization) $$x ^ { \prime } = \frac { x - \min ( x ) } { \max ( x ) - \min ( x ) }$$ mean normalization $$x ^ { \prime } = \frac { x - \operatorname { average } ( x ) } { \max ( x ) - \min ( x ) }$$ standardization $$x ^ { \prime } = \frac { x - \overline { x } } { \sigma }$$ data imbalanced机器学习中的特征工程是有一定技巧可言，其中我觉得最为有趣的是: generation or you can call it abstraction. 对于特征的泛的提取才是对于问题本身或者特征的理解，这不仅需要积累，更需要对于该问题领域的专业知识， that’s all.举个栗子，在 “Home Credit Default Risk” (kaggle 竞赛)中，原始的训练数据有信贷金额和客户的年收入，这个时候 “credit_income_percent” 就是类似这种性质的提取特征。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Finding Similar Quora Questions]]></title>
    <url>%2F2019%2F03%2F25%2Fquora-questions%2F</url>
    <content type="text"><![CDATA[介绍 kaggle 上 finding similar quora quesitons 竞赛，作为一次总结。 问题描述判断一对问题是否重复。这种问题的研究是非常有价值，如果发现用户提出了一个重复的已经回答过的问题，那么可以直接返回给用户答案，提高了用户体验。 简要说明一下 kaggle 上总的评分机制：在计算得分的时候，Public Leaderboard (LB)和 Private LB 之分。具体而言，参赛选手提交整个测试集的预测结果，Kaggle 使用测试集的一部分计算得分和排名，实时显示在 Public LB上，用于给选手提供及时的反馈和动态展示比赛的进行情况；测试集的剩余部分用于计算参赛选手的最终得分和排名，此即为 Private LB，在比赛结束后会揭晓。 针对于该问题，使用的log loss 作为评分依据，提交的文件中每一对问题并不是给出 {0, 1} 这样的二值，而是给出区间 [0,,1] 的浮点数，然后predict 的结果和真实的结果，计算 log loss。作为最后的得分，该loss 的取值范围是 [0, 1] ，如果结果越小，说明预测越准确。 我的情况： 总共是3000多个队伍吧，最后的排名是在 5% （在150名之内） 数据特点 数据规模 总体： training data: 40 万， 400,000 , 63M test data: 其中有机器生成的虚假的数据，不作为最后的评分，但是存在, 314 M 这个从数量上充分说明了冗余的数据，防止人工作弊的。 training data 的字段： 1234id: Looks like a simple rowIDqid&#123;1, 2&#125;: The unique ID of each question in the pairquestion&#123;1, 2&#125;: The actual textual contents of the questions.is_duplicate: The label that we are trying to predict - whether the two questions are duplicates of each other. 句子的频数 ：使用 qid 统计在训练集中出现的次数。 发现大多数句子的重复率比较低。 1234567891011121314print('Total number of question pairs for training: &#123;&#125;'.format(len(df_train)))print('Duplicate pairs: &#123;&#125;%'.format(round(df_train['is_duplicate'].mean()*100, 2)))qids = pd.Series(df_train['qid1'].tolist() + df_train['qid2'].tolist())print('Total number of questions in the training data: &#123;&#125;'.format(len( np.unique(qids))))print('Number of questions that appear multiple times: &#123;&#125;'.format(np.sum(qids.value_counts() &gt; 1)))plt.figure(figsize=(12, 5))plt.hist(qids.value_counts(), bins=50)plt.yscale('log', nonposy='clip')plt.title('Log-Histogram of question appearance counts')plt.xlabel('Number of occurences of question')plt.ylabel('Number of questions')print() In terms of questions, everything looks as I would expect here. Most questions only appear a few times, with very few questions appearing several times (and a few questions appearing many times). One question appears more than 160 times, but this is an outlier. 训练数据集中句子word 的个数 的分布，可以看出一般是在20 左右的长度 word 的句子是最多的。 训练数据集中 character 个数的分布 注意到两个分布都是右偏态分布（正偏态），从数学的角度理解 mean &gt; median. 同理也有左偏态分布。偏态分布又是相对于正太分布而言的。这种偏态的定义，是以尾部命名，右偏态或者正偏态的尾部，集中在右侧。 由上面的例子可以看出，不管是正偏态分布还是负偏态分布，由于受偏态分布的影响，众数与均值值发生变化，而均值的影响更大。 中位数仅与样本总数有关，即在整个样本的中间位置，而在正偏态分布，大部分值在峰值（众数）的右边，故中位数在众数的右边（众数＜中位数） 中位数与均值的比较了？由于均值受极大值的影响变大而中位数不受影响，故均值大于中位数（中位数＜均值） 所以 众数 &lt; 中位数 &lt; 均值 首先众数与中位数的比较 这个分布是左偏的，大部分值是在峰值（众数）的左边，故众数大于中位数 均值与中位数的比较 由于是负偏态分布，极小值对均值的影响大（极小值对均值拉低），故均值小于中位数 所以 均值＜中位数＜众数 training data 和 test data 中的正负样本有很大的差别 However, before I do this, I would like to rebalance the data that XGBoost receives, since we have 37% positive class in our training data, and only 17% in the test data. 采取的措施 Rebalancing the Data (这里所谓的 rebalance data 就是将原始的一部分数据集去掉，减少 正样例的个数)。 原因有两点： 机器学习算法的基本假设就是独立同分布，如果 train data 和test data 不能保持同分布，那么泛化性能不能要求很高 因为对于 log loss 的计算保持正负样例的比例，对于最后的结果是重要的。 特征主要分成两类：传统的数据挖掘的特征+ NLP embedding 向量特征 传统数据挖掘特征 character of length of questions 1 and 2 number of words in question 1 and 2 number of character in question 1 and question 2 difference of length 字符串的精确匹配和部分匹配（fuzzywuzzy 工具） numbers of capital letters, questions marks, 数字 normalized word share count (非常具有 prediction 的特征)计算实现： 12345678910111213141516def word_match_share(row): q1words = &#123;&#125; q2words = &#123;&#125; for word in str(row['question1']).lower().split(): if word not in stops: q1words[word] = 1 for word in str(row['question2']).lower().split(): if word not in stops: q2words[word] = 1 if len(q1words) == 0 or len(q2words) == 0: # The computer-generated chaff includes a few questions that are nothing but stopwords return 0 shared_words_in_q1 = [w for w in q1words.keys() if w in q2words] shared_words_in_q2 = [w for w in q2words.keys() if w in q1words] R = (len(shared_words_in_q1) + len(shared_words_in_q2))/(len(q1words) + len(q2words)) return R 我认为这个定义应该是两个句子的交集/ 两个句子的并集。 和上面的实现不知道有什么区别？反正这个特征是非常 preditable 的 NLP 方面的处理 基于词向量得到得到的句子向量的相关距离函数的计算 cosine distance cityblock distance jaccard distance euclidean distance 直接是句子向量的上述距离的计算 可以做到的是对比试验，在只是使用 词向量得到的特征最后的test acc 是87%， 而加上了句子向量没有提升，应该是86.3% 之类的。 最好的模型是acc 是 0.88， 当然一切模型都是需要提到线上的public 进行计算的，对应的 log loss 是0.14658。排名是 5% （150~ /3000~） 好的代码pandas 对于数据的常用操作 实用的数据预分析 123456789101112131415# Check for any null valuesprint(train.isnull().sum())print(test.isnull().sum())# Add the string 'empty' to empty stringstrain = train.fillna('empty')test = test.fillna('empty')# 数据的分类统计temp = df.column.value_counts() # 显示整体的df 的信息df.info()# 直接就产生了一个图像，不敢想象， 使用groupby()，然后选择 column 可以学习df.groupby("is_duplicate")['id'].count().plot.bar() 常见的表格的操作 123456789test_cp.rename(columns=&#123;'test_id':'id'&#125;,inplace=True)comb = pd.concat([train_cp,test_cp])ques = pd.concat([train_orig[['question1', 'question2']], \ test_orig[['question1', 'question2']]], axis=0).reset_index(drop='index') # 筛选操作train_comb = comb[comb['is_duplicate'] &gt;= 0][['id','q1_hash','q2_hash','q1_freq','q2_freq','is_duplicate']]test_comb = comb[comb['is_duplicate'] &lt; 0][['id','q1_hash','q2_hash','q1_freq','q2_freq']] 小心使用 12train_questions.drop_duplicates(subset = ['question1'],inplace=True)train_questions.reset_index(inplace=True,drop=True) 其他 1234567891011eng_stopwords = set(stopwords.words('english'))# 主要掌握set.intersection() 交集的使用def q1_q2_intersect(row): return(len(set(q_dict[row['question1']]).intersection(set(q_dict[row['question2']]))))# series 是可以直接 tolist() 的qids = pd.Series(df_train['qid1'].tolist() + df_train['qid2'].tolist())from nltk import word_tokenize, ngrams# word_tokenize 就是将句子打散，成为一个个单词 手写 TF-IDF 12345678910111213141516171819202122232425262728293031323334from collections import Counter# If a word appears only once, we ignore it completely (likely a typo)# Epsilon defines a smoothing constant, which makes the effect of extremely rare words smallerdef get_weight(count, eps=10000, min_count=2): if count &lt; min_count: return 0 else: return 1 / (count + eps)eps = 5000 words = (" ".join(train_qs)).lower().split()counts = Counter(words)weights = &#123;word: get_weight(count) for word, count in counts.items()&#125;def tfidf_word_match_share(row): q1words = &#123;&#125; q2words = &#123;&#125; for word in str(row['question1']).lower().split(): if word not in stops: q1words[word] = 1 for word in str(row['question2']).lower().split(): if word not in stops: q2words[word] = 1 if len(q1words) == 0 or len(q2words) == 0: # The computer-generated chaff includes a few questions that are nothing but stopwords return 0 shared_weights = [weights.get(w, 0) for w in q1words.keys() if w in q2words] + [weights.get(w, 0) for w in q2words.keys() if w in q1words] total_weights = [weights.get(w, 0) for w in q1words] + [weights.get(w, 0) for w in q2words] R = np.sum(shared_weights) / np.sum(total_weights) return R 123# 在代码中使用 ls 的操作，也是比较 666 的from subprocess import check_outputprint(check_output([&quot;ls&quot;, &quot;../input&quot;]).decode(&quot;utf8&quot;)) 在验证 word2vec 时候， 可以使用t-sne 进行可视化 训练得到的model 可以 most_similar() ,定住一个变量，去验证好坏就行 参考资料 kaggle kernel 1kaggle kernel 2代码 https://towardsdatascience.com/finding-similar-quora-questions-with-word2vec-and-xgboost-1a19ad272c0d]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Linear Algebra in ML]]></title>
    <url>%2F2019%2F03%2F25%2FLinear-Algebra-in-ML%2F</url>
    <content type="text"><![CDATA[我觉得到 ML 中的一个难点：就是由原来简单的 linear equations 直接过渡到了 matrics and vectors。这个过程是没有人跟你说的。网络结构可以当作是一个complicated 并且是无法表示的函数，于是很多使用者把它当作黑匣子，关心于输入和输出，中间过程 don’t care. 变量（特征个数）和解的关系多变量和最后target的关系是可以使用 matrices 进行表示的，这就是一种数学公式化。 Broadly speaking, in linear algebra data is represented in the form of linear equations. These linear equations are in turn represented in the form of matrices and vectors. 先直观的感受一下变量和图形（可视化）的关系。两个变量组成的equations 是两条线的相交情况。而三个变量在空间中有三种情况： 相交，平行，不在一个平面上。三个变量组成的equations 是三个面的相交情况。有四种情况 (try hard to figure it out)：No intersection at all.Planes intersect in a line.They can intersect in a plane.All the three planes intersect at a point. 当到达4 dims 的时候，it’s impossible to visulize it. terms in related to matrix这些词汇 (terms) 经常在文献中出现，需要对于其含义有个比较好的认识。Order of matrix – If a matrix has 3 rows and 4 columns, order of the matrix is 34 i.e. rowcolumn. (翻译成 矩阵的阶)Square matrix – The matrix in which the number of rows is equal to the number of columns.Diagonal matrix – A matrix with all the non-diagonal elements equal to 0 is called a diagonal matrix.Upper triangular matrix – Square matrix with all the elements below diagonal equal to 0.Lower triangular matrix – Square matrix with all the elements above the diagonal equal to 0.Scalar matrix – Square matrix with all the diagonal elements equal to some constant k.Identity matrix – Square matrix with all the diagonal elements equal to 1 and all the non-diagonal elements equal to 0.Column matrix – The matrix which consists of only 1 column. Sometimes, it is used to represent a vector.Row matrix – A matrix consisting only of row.Trace – It is the sum of all the diagonal elements of a square matrix.Rank of a matrix – Rank of a matrix is equal to the maximum number of linearly independent row vectors in a matrix.Determinant of a matrix - 矩阵的行列式转置 -在图形 matrix中还是很常见的。$$\mathrm { A } _ { \mathrm { ij } } ^ { \mathrm{T}} = \mathrm { A } _ { \mathrm { ji } }$$ 这个矩阵乘法和元素相称的区别，后者是element-wise 进行的。可以从另外一个角度去列及矩阵相称： This operation on a vector is called linear transformation. 就是后面的vector 映射到了前面的矩阵空间。 特征值和奇异值着两个是分别对应着PCA 和SVD。Eigenvalues and Eigenvectors如公式所示，特征值和特征向量的乘积就是方阵和特征向量的乘积，原先的方阵是可以降维表示成特征向量和特征值的。$ A x = \lambda x $ 对于奇异值分解，最常见的就是这种表达：$A = U \Sigma V ^ { T }$特征值分解和奇异值分解都是给一个矩阵(线性变换)找一组特殊的基，特征值分解找到了特征向量这组基，在这组基下该线性变换只有缩放效果。而奇异值分解则是找到另一组基，这组基下线性变换的旋转、缩放、投影三种功能独立地展示出来了。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Back to my blog]]></title>
    <url>%2F2019%2F03%2F07%2FBack-to-my-blog%2F</url>
    <content type="text"><![CDATA[直到某一天发现个人网站中的图片都显示不出来了，查了一下才发现之前的图床不能用了（点名批评七牛），果断弃之，换了个大厂子产品。证明一下图片是能出来的。ps：之前的图片有时间再整理到新的平台上。]]></content>
      <categories>
        <category>人间不值得</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[基于simhash的大文本相似度比较]]></title>
    <url>%2F2018%2F08%2F23%2F%E5%9F%BA%E4%BA%8Esimhash%E7%9A%84%E6%96%87%E6%9C%AC%E7%9B%B8%E4%BC%BC%E5%BA%A6%E6%AF%94%E8%BE%83%2F</url>
    <content type="text"><![CDATA[本文主要记录使用simhash比较中文大文本的相似度问题。先说一下文本特征，数据属于中文文本，每篇文章的字数大于500,小于2000,基本上属于大文本。步骤如下： 基于tf-idf提取文本的关键词。如果这些关键词在之后的比较中是相同的，那么认为对应的文章也是相同。简而言之，这些提取的关键词可以看做原文章的”代表”. 根据关键字计算simhash编码，然后使用hamming distance进行比较两者的不同。如果对于上述概念比较模糊，建议首先阅读该篇博客。 实战 顺滑过渡到代码实现：123456789101112131415# 常规导包import sys,codecsimport pandas as pdimport numpy as npimport jieba.possegimport jieba.analysefrom sklearn import feature_extractionfrom sklearn.feature_extraction.text import TfidfTransformerfrom sklearn.feature_extraction.text import CountVectorizer# 数据集的路径path =&quot;../tianmao2.csv&quot;names =[&apos;where&apos;, &apos;time&apos;, &apos;title&apos;, &apos;url&apos;, &apos;contents&apos;]data =pd.read_csv(path, delimiter=&apos;\t&apos;, names= names, nrows=200)data[&apos;id&apos;] =data.index+1data.head() 我们使用title和contents 组合作为原始处理的数据，我们认为该数据能够就是文章的内容。1stopkey = [w.strip() for w in codecs.open(&apos;../keyword_extraction/data/stopWord.txt&apos;, &apos;r&apos;).readlines()] 该stop words是中文停用词，就是常见的”的 了”。常见的有百度停用词表、哈尔滨工业大学停用词表以及中科院的停用词表。这里使用的是中科院的停用词。对于停用词的存储，可以使用set ，因为set 要比 list的检索要快。12345678def dataPrepos(text, stopkey): l = [] pos = [&apos;n&apos;, &apos;nz&apos;, &apos;v&apos;, &apos;vd&apos;, &apos;vn&apos;, &apos;l&apos;, &apos;a&apos;, &apos;d&apos;] # 定义选取的词性 seg = jieba.posseg.cut(text) # 分词 for i in seg: if i.word not in stopkey and i.flag in pos: # 去停用词 + 词性筛选 l.append(i.word) return l 我们选择名词作为主要的分析对象。12345678idList, titleList, abstractList = data[&apos;id&apos;], data[&apos;title&apos;], data[&apos;contents&apos;]corpus = [] # 将所有文档输出到一个list中，一行就是一个文档# 这个 虽然使用 &quot; &quot; 进行分割 但是实际上还是一个打的listfor index in range(len(idList)): text = &apos;%s。%s&apos; % (titleList[index], abstractList[index]) # 拼接标题和摘要 text = dataPrepos(text, stopkey) # 文本预处理 text = &quot; &quot;.join(text) # 连接成字符串，空格分隔 corpus.append(text) 这里的corus 是将所有的经过预处理文档作为当前计算 idf 的语料库。123456789vectorizer = CountVectorizer()X = vectorizer.fit_transform(corpus) # 词频矩阵,a[i][j]:表示j词在第i个文本中的词频# 2、统计每个词的tf-idf权值transformer = TfidfTransformer()tfidf = transformer.fit_transform(X)# 3、获取词袋模型中的关键词word = vectorizer.get_feature_names()# 4、获取tf-idf矩阵，a[i][j]表示j词在i篇文本中的tf-idf权重weight = tfidf.toarray() 使用sklearn 内置的函数计算tf-idf。1234567891011121314151617181920212223242526272829topK = 10ids, titles, keys, weights = [], [], [], []for i in range(len(weight)): print(&quot;-------这里输出第&quot;, i + 1, &quot;篇文本的词语tf-idf------&quot;) ids.append(idList[i]) titles.append(titleList[i]) df_word, df_weight = [], [] # 当前文章的所有词汇列表、词汇对应权重列表 for j in range(len(word)): # print(word[j],weight[i][j]) df_word.append(word[j]) df_weight.append(weight[i][j]) df_word = pd.DataFrame(df_word, columns=[&apos;word&apos;]) df_weight = pd.DataFrame(df_weight, columns=[&apos;weight&apos;]) word_weight = pd.concat([df_word, df_weight], axis=1) # 拼接词汇列表和权重列表 word_weight = word_weight.sort_values(by=&quot;weight&quot;, ascending=False) # 按照权重值降序排列 # 在这里可以查看 k的选取的数值应该是多大， # from ipdb import set_trace # set_trace() keyword = np.array(word_weight[&apos;word&apos;]) # 选择词汇列并转成数组格式 word_split = [keyword[x] for x in range(0, topK)] # 抽取前topK个词汇作为关键词 word_split = &quot; &quot;.join(word_split) keys.append(word_split) wei = np.array(word_weight[&apos;weight&apos;]) wei_split = [str(wei[x]) for x in range(0, topK)] wei_split = &quot; &quot;.join(wei_split) weights.append(wei_split) # 这里的命名 容易混淆result = pd.DataFrame(&#123;&quot;id&quot;: ids, &quot;title&quot;: titles, &quot;key&quot;: keys, &apos;weight&apos;: weights&#125;, columns=[&apos;id&apos;, &apos;title&apos;, &apos;key&apos;, &apos;weight&apos;]) 选择前10个频率最高的词语作为该篇文章的代表，当然这个参数是可以调整，需要根据具体的问题和结果进行调整。1result.head() 最后的效果如上。至此我们第一步的提取文章的关键词就已经做完。下面进行相似度的比较。 1234import jiebaimport jieba.analyseimport pandas as pd#日常导包 数据和上述的一样，所以就不截图了。123datasets =pd.read_csv(&quot;../tianmao2-tf-idf.csv&quot;)tokens =datasets[&apos;key&apos;]weights =datasets[&apos;weight&apos;] 提取关键词和对应的权重。123456789101112print(tokens[0], len(tokens[0]))print(weights[0], len(weights[0]))tokens0 =tokens[0].split()weights0 =weights[0].split()len(tokens0)len(weights0)tokens1 =tokens[1].split()weights1 =weights[1].split()import astweights0 =[ ast.literal_eval(i) for i in weights0]weights1 =[ ast.literal_eval(i) for i in weights1] 构造测试用例。因为权重是字符串，所以简单处理转成整数。 12dict0 =dict(zip(tokens0, weights0))dict1 =dict(zip(tokens1, weights1)) 定义一个Simhash，提供对文档的数值映射和文档间相似度计算的功能.123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081class Simhash(object): # 初始化函数 def __init__(self, weights_dict, tokens=&apos;&apos;, hashbits=64): self.hashbits = hashbits self.hash = self.simhash_function(tokens, weights_dict) # toString函数 # 不懂这个 self.hash ，凡是带有self 的函数都是可以类变量，所以这个就是返回的 self.hash这个变量 #凡是使用__str__ 这种类型的函数 都是重写 原来的函数 def __str__(self): return str(self.hash) &quot;&quot;&quot; ord() 函数是 chr() 函数（对于8位的ASCII字符串）或 unichr() 函数（对于Unicode对象）的配对函数，它以一个字符（长度为1的字符串）作为参数，返回对应的 ASCII 数值，或者 Unicode 数值 &quot;&quot;&quot; # 给每一个单词生成对应的hash值 # 这个操作搞懂之后一定很简洁， 但是现在很难理解，因为不是字符串，而是位操作 def _string_hash(self, source): if source == &apos;&apos;: return 0 else: x = ord(source[0]) &lt;&lt; 7 # &lt;&lt; 表示 乘以2^7 ; &gt;&gt; 表示除以 ; ** 表示次方的意思 # ^ : 按位异或 (二进制进行异或)； &amp; 按位进行与 操作 # 左移位操作也是可以理解为 2^x 的操作，因为存储是二进制，这样左移一位 表示×2 一次 m = 1000003 mask = 2 ** self.hashbits - 1 for c in source: x = ((x * m) ^ ord(c)) &amp; mask x ^= len(source) if x == -1: x = -2 return x # 生成simhash值 def simhash_function(self, tokens, weights_dict): v = [0] * self.hashbits # 这种使用 &#123;&#125; dictionary 然后强行得到item 再进行遍历也是牛逼 for key, t in &#123;x: self._string_hash(x) for x in tokens&#125;.items(): for i in range(self.hashbits): bitmask = 1 &lt;&lt; i if t &amp; bitmask: v[i] += weights_dict[key] else: v[i] -= weights_dict[key] fingerprint = 0 for i in range(self.hashbits): if v[i] &gt;= 0: fingerprint += 1 &lt;&lt; i return fingerprint # 求文档间的海明距离 def hamming_distance(self, other): x = (self.hash ^ other.hash) &amp; ((1 &lt;&lt; self.hashbits) - 1 ) tot = 0 while x : tot += 1 x &amp;= x - 1 return tot #求相似度 # 这个相似度的计算，十分简单，如果两个数接近，那么就是认为相似。越是接近1 越是相似， # 不是原先那种以某一个参数整数 如3 为距离的相似度 def similarity(self, other): a = float(self.hash) b = float(other.hash) if a &gt; b: return b / a else: return a / b if __name__ == &apos;__main__&apos;: hash0 = Simhash(weights_dict=dict0, tokens=tokens0) print(hash0) hash1 = Simhash(weights_dict=dict1, tokens=tokens1) print(hash1) print(hash0.hamming_distance(hash1)) print(hash0.similarity(hash1)) 结果如上。可以看出该例子中使用的两两比较的方式，对于大数据来说，一般可能会用到倒排索引和cpu并行技术。]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>simhash</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[文本相似度比较基本知识(1)]]></title>
    <url>%2F2018%2F08%2F23%2F%E6%96%87%E6%9C%AC%E7%9B%B8%E4%BC%BC%E5%BA%A6%E6%AF%94%E8%BE%83%E5%9F%BA%E6%9C%AC%E7%9F%A5%E8%AF%86%2F</url>
    <content type="text"><![CDATA[互联网网页存在大量的重复内容网页，无论对于搜索引擎的网页去重和过滤、新闻小说等内容网站的内容反盗版和追踪，还是社交媒体等文本去重和聚类，都需要对网页或者文本进行去重和过滤。本文介绍的 Locality Sensitive Hashing 是常见的一类hash 函数用于去重。 SimHash是一种局部敏感hash，它也是Google公司进行海量网页去重使用的主要算法。 Locality Sensitive HashingHash 函数能够保证最后的映射空间唯一性和均匀分布，但是不能保证原来相似向量，映射之后也是相似的。但是局部敏感hash（比如 simhash or minhash ）是能够保证这一点的。也可以从降维的角度进行理解，降维之前和降维之后，相似的文档（这里就具体化一个东西）hash 之后也是相似的。 局部敏感哈希的基本思想：在高维数据空间中的两个相邻的数据被映射到低维数据空间中后，将会有很大的概率任然相邻；而原本不相邻的两个数据，在低维空间中也将有很大的概率不相邻。通过这样一映射，我们可以在低维数据空间来寻找相邻的数据点，避免在高维数据空间中寻找，因为在高维空间中会很耗时。有这样性质的哈希映射称为是局部敏感的。simhash 或者minhash是局部敏感hash的一种具体实现。局部敏感哈希是一种思想。 应用：对于高维数据的海量数据近邻查找，局部敏感哈希是一个很好的解决方法。在很多问题中，从海量数据库中寻找到与查询数据相似的数据是一个很关键的问题。可以具体应用到文本相似度检测、网页搜索等领域。 Simhash我们现在处理的是大数据维度上的文本去重，这就对算法的效率有着很高的要求。但是在小的样本上这个是不一定有保证有效的，小文本使用 NLP 相关知识可能得到更好的精度。SimHash算法是Google公司进行海量网页去重的高效算法，它通过将原始的文本映射为64位的二进制数字串，然后通过比较二进制数字串的差异进而来表示原始文本内容的差异。本文服务于该篇博客,主要进行名词解释。 基本概念simhash 也是一种hash，一般的hash 函数映射规则只需要满足以下两个条件： 对很多不同的特征来说，它们对所对应的向量是均匀随机分布的 相同的特征来说对应的向量是唯一 simhash 和传统的 hash 不同点在于前者的01 串是可以表征文本之间的相似度，而后者是不可以的。 简单来说普通的hash映射需要满足随机分布和唯一性两个条件。simhash想要实现的是，如果原来的文本的特征是相似，那么映射之后的编码也是相似。这里使用 hamming distance 进行比较simhash映射之后的距离。根据经验值，对64位的 SimHash值，海明距离在3以内的可认为相似度比较高。编码之后的表示在英文中是 fingerprint(指纹)。 simhash最初被google 用于网页去重，当时使用的fingerprint 是64,所以这里沿用了这个传统。64位的签名可以表示多达$2^64$个象限，因此只保存所在象限的信息也足够表征一个文档了。更进一步，表示的文档的数字最多是多少？这个应该可以准确计算特征的个数应为如果用三位(01) 表示，那么有8种，那么2^64 这么多种特征，所以16*10^18 这么多。 算法步骤Simhash 分为5个步骤：分词、hash（md5 要求均匀映射到某空间就行，不要求反应原始样本的相关性）、加权、合并（列项相加）、降维（正数为1 负数为0），得到每篇文章的simhash 之后，计算两个文章的海明距离（两个字符串对应位置的不同字符的个数）。对于64 位的simhash 值，在3以内就可以认为是比较相似的。 第一步：文本预处理得到分词（去重，去除的了stop words）,然后weight权重可以使用分词的frequency 或者tfidf 进行得到第二步：进行hash 映射（可以使用md5这种传统的映射方式，基本的要求就是均匀映射到一个空间，这种映射并不能反映原始样本的相关性）第三步：hash 映射值和weight 进行相乘，如果原来是1 则乘以1，如果是0 则乘以-1，第四步： 列向相加，得到summing weights,进行降维如果是正数那么为1，如果是负数那么为-1第五步： 计算不同文本之间的 hamming distance。 然后这个simhash就出来了.有图有真相 simhash的局限性：只考虑到文章存在哪些词，没有考虑到词的顺序。不过相应的优点是，可以实现海量文章相似度计算。文章相似度计算忽略词的顺序之后效果更好。所以在处理大文本时候，simhash是有效的，但是在处理小文本，这种效果往往不能被保证。直观上理解，在一片段文章或者段落中，词语出现的顺序还是比较重要的。更加准确的说，这个是用来进行判重的算法，而不是计算相似度的算法。 开源实现：simhash 的实现，调用一个库 https://www.twblogs.net/a/5c178f2cbd9eee5e40bbc9e9/zh-cn simhash 的一个github https://github.com/yanyiwu/simhash 百度去重： top k 最长的语句，作为源数据 minhash总的操作步骤如下： 对一个文档转化为关键词的集合，用这个集合来表示这个文档，叫Shingling。 用MinHashing函数来构造哈希表。 使用LSH来寻找相似的文档。 对于第一点中的Shingling，这个k 是一个关键参数，可以体现上下文的那种。例如：k=2，doc=adcab，这个集合的2-shingles={ab,ba,ca} 。我们对这个字符串进行划分，得到的是ad dc ca ab，由于集合是唯一性的所以不可能有重复的元素。k=2 其实是一个比较糟糕的选择，我们一般选择K在实际情况中一般会选择9或者10，我们要求这个k一般要大于我们文章中出现的单词的长度。这样的选择会比较合理一些。 使用一个具体的例子讲解 minhash 的操作步骤： 第一步： 文档的Shingling：对于中文首先进行分词，得到每篇文章的词语的集合（集合是去重之后的结果），这里是可以做n-gram 的思想的，这个n 的取值越大，越能找到真正相似的文档，代价是dictionary 很大，存储上的 1234&gt; s1 = &quot;我 减肥&quot;&gt; s2= &quot;要&quot;&gt; s3 = &quot;他 减肥 成功&quot;&gt; s4 = &quot;我 要 减肥&quot; 第二步： 文档的矩阵表示（如果keyword 在相应的文章中出现，标记为1 否则标记为0） 元素 S1 S2 S3 S4 我 1 0 0 1 他 0 0 1 0 要 0 1 0 1 减肥 1 0 1 1 成功 0 0 1 0 真正实践中的矩阵应该是十分稀疏的。 第三步： 最小hash定义为：特征矩阵按行进行一个随机的排列后，第一个列值为1的行的行号。举例说明如下，假设之前的特征矩阵按行进行的一个随机排列如下： 元素 S1 S2 S3 S4 我 1 0 0 1 减肥 1 0 1 1 成功 0 0 1 0 他 0 0 1 0 要 0 1 0 1 最小哈希值：h(S1)=1，h(S2)=5，h(S3)=2，h(S4)=1. 从图中可以知道，应该从 input matrix（原始的matrix ）得到新的 signature matrix。 签名的相似性 1/3 ，以此类推我们可以得到我们所求的图中要求的相似性，我们看图中第2列和第3列 Jaccard similarity 就是 1/5，我们签名的相似性就是1/3 。 使用 signature matrix 去近似的表示 Jaccard similarity 这个是第一二列的 input matrix 0 10 01 00 10 01 10 0 这个是signature matrix 3 12 21 5 为什么使用上述方法是有效？ 事实上，两列的最小hash值就是这两列的Jaccard相似度的一个估计，换句话说，两列最小hash值同等的概率与其相似度相等，即P(h(Si)=h(Sj)) = sim(Si,Sj)。为什么会相等？我们考虑Si和Sj这两列，它们所在的行的所有可能结果可以分成如下三类： （1）A类：两列的值都为1； （2）B类：其中一列的值为0，另一列的值为1； （3）C类：两列的值都为0. 特征矩阵相当稀疏，导致大部分的行都属于C类，但只有A、B类行的决定sim(Si,Sj)，假定A类行有a个，B类行有b个，那么sim(si,sj)=a/(a+b)。现在我们只需要证明对矩阵行进行随机排列，两个的最小hash值相等的概率P(h(Si)=h(Sj))=a/(a+b)，如果我们把C类行都删掉，那么第一行不是A类行就是B类行，如果第一行是A类行那么h(Si)=h(Sj)，因此P(h(Si)=h(Sj))=P(删掉C类行后，第一行为A类)=A类行的数目/所有行的数目=a/(a+b)，这就是最小hash的神奇之处。 第四步： 这个只是一次随机采样，根据中心极限定理，只有多次随机重复采样，才能得到比较稳定的结果。那么现在出现另一个问题，将随机排列去排序，这耗费很长的时间。于是这里使用了另一种方式，选择n 个hash 函数 $h_1$, $h_2$, $h_3$ .. $h_n$ 得到不同的签名矩阵，而不是将矩阵进行重新排序。 对于两个document，在Min-Hashing方法中，它们hash值相等的概率等于它们降维前的Jaccard相似度。 就是说，对于两个document，在Min-Hashing方法中，它们hash值相等的概率等于它们降维前的Jaccard相似度。 minhash 的缺点 在工程中，不容易找到一系列的hash 函数，不同的hash 函数之间可能相关 局部敏感哈希是相对的，而且我们所说的保持数据的相似度不是说保持100%的相似度，而是保持最大可能的相似度。对于局部敏感哈希“保持最大可能的相似度”的这一点，我们也可以从数据降维的角度去考虑。数据对应的维度越高，信息量也就越大，相反，如果数据进行了降维，那么毫无疑问数据所反映的信息必然会有损失。哈希函数从本质上来看就是一直在扮演数据降维的角色。 simhash 有两个比较典型的应用：一个是网页抓取的排重，一个是检索时相似doc 的排重 simhash与Minhash的联系和区别： 相同点：simhash和minhash可以做到两个文档Hash之后仍然相似，但是simhash计算相似的方法是海明距离；而minhash计算距离的方式是Jaccard距离。不同点：理论上讲，simhash 的准确率低于minhash。原因有二： simhash 对文本进行分词并统计词频，可以认为是一个词袋模型，没有统计词汇的先后顺序。而minhash 使用滑动窗口的方式，加入了词汇的词序信息。 simhash 对词汇特征向量按列求和符号映射，丢失了文本特征信息。 参考资料： 讲解minhas https://www.cnblogs.com/maybe2030/p/4953039.html 这个也是比较好的：https://www.cnblogs.com/fengfenggirl/p/lsh.html 距离函数这里的距离函数都是用来文本相似度。 Jaccard相似度简单来说交集除以并集。这个集合中存放的是文章或者段落的关键词。 1234567891011def JaccardSim(str_a, str_b): &apos;&apos;&apos; Jaccard相似性系数 计算sa和sb的相似度 len（sa &amp; sb）/ len（sa | sb） &apos;&apos;&apos; seta = splitWords(str_a)[1] setb = splitWords(str_b)[1] sa_sb = 1.0 * len(seta &amp; setb) / len(seta | setb) return sa_sb 可以看到核心代码很简单，经过分词之后，就是seta 和setb 进行的操作。 Jaccard 系数Jaccard相似指数用来度量两个集合之间的相似性,它被定义为两个集合交集元素个数除以两个集合并集元素个数。 $$\mathrm { J } ( \mathrm { A } , \mathrm { B } ) = \frac { | A \cap B | } { | A \cup B | }$$Jaccard距离用来度量两个集合之间的差异性m它是jaccard的相似系数的补集: $$d _ { J } ( A , B ) = 1 - J ( A , B ) = \frac { | A \cup B | - | A \cap B | } { | A \cup B | }$$利用jaccard相似度来衡量文档之间的相似性,使用LSH来实现文档相似度计算。 cosine12345def cos_sim(a, b): a = np.array(a) b = np.array(b) # return &#123;&quot;文本的余弦相似度:&quot;:np.sum(a*b) / (np.sqrt(np.sum(a ** 2)) * np.sqrt(np.sum(b ** 2)))&#125; return np.sum(a * b) / (np.sqrt(np.sum(a ** 2)) * np.sqrt(np.sum(b ** 2))) 将文本的关键词映射成某种高维函数，然后在高维空间中计算两者的相似度。 tf-idf在simhash 中使用 tf-idf作为我们的比较函数。TF-IDF的主要思想就是：如果某个词在一篇文档中出现的频率高，也即TF高；并且在语料库中其他文档中很少出现，即DF的低，也即IDF高，则认为这个词具有很好的类别区分能力。$$ TF-IDF = 词频(TF) x 逆文档频率(IDF) $$ 算法步骤： 计算词频$$ 词频(TF) = 某个词在文章中出现的次数( 频数) $$或者可以进一步进行“标准化”$$ 词频( TF) = \frac{某次在文中出现的次数}{文章的总词语数} $$ 逆文档频率(这对这个术语的还是好好记忆)这个时候需要一个语料库 (corpus)，模拟语言环境$$ 逆文档频率 (IDF) = log(\frac{语料中的文档总数}{ 包含该词的文档数 +1}) $$ TF-IDF 优点是简单快速，比较符合实际。缺点，无法体现词的位置信息，所有的位置都是被认为重要性相同，但是开头结尾，段落的开头和段落的结尾，therefore，so，but这些词语都是没有体现的；还有一个缺点是，是基于统计的，没有表达出词语的语意信息 or context 上下文的信息。 Hamming distancehamming distance就是比较01串的不同，按照位进行比较。算法：异或时，只有在两个比较的位不同时其结果是1 ，否则结果为0，两个二进制“异或”后得到1的个数即为海明距离的大小。 123456789101112131415161718hashbits =64 # 使用64位进行编码def simhash_function(tokens, weights_dict): v = [0] * hashbits # 这种 &#123;key: value&#125;.item() 的操作也是没有了谁了 for key, t in &#123;x: _string_hash(x) for x in tokens&#125;.items(): for i in range(hashbits): bitmask = 1 &lt;&lt; i if t &amp; bitmask: v[i] += weights_dict[key] else: v[i] -= weights_dict[key] fingerprint = 0 for i in range(hashbits): if v[i] &gt;= 0: fingerprint += 1 &lt;&lt; i return fingerprintfingerprint = simhash_function(tokens, weights) 分词在英文中存在天然的空格可以进行分词操作，但是中文的分词就比较复杂了。常用的中文分词开源工具有 jieba和HanLP前者简单易行，容易上手；后者在自然语言处理作为汉语言处理包，可以用于词性标注，命名实体识别等一系列功能。常用的英文分词 corenlp 倒排索引倒排索引使用python在实现上就是一个dictionary 嵌套一个 set(). 一般的索引都是数字或者英文字母映射内容，具体在放到simhash的情景下就是使用文章的序列号对应提取出来的关键词。但是倒排索引就是关键词对应文章的序列号，类似与原来的”值”对应这”键”，所以称之为倒排索引。一般使用在召回的场景下，使用关键词然后出现了该关键词下的index 的集合。可以参考这篇文章。 一般的情况是key 是索引，value 对应的是关键词之列的内容； 但是倒排索引正好相反，关键字作为key，然后索引作为value，所以称之为倒排索引。]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>simhash</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Differences Between l1 and l2 as Loss Function and Regularization]]></title>
    <url>%2F2018%2F07%2F21%2Fdifferences-between-l1-and-l2-as-loss-function-and-regularization%2F</url>
    <content type="text"><![CDATA[L1 和L2 既可以作为 loss function 也可以作为 reguarization，分别介绍了一下两者，最后介绍 overfitting 的概念。 As loss functionloss function or error function 是用来衡量真实$y$ 和生成的$ f(x)$ 之间差距的函数。在模型训练中我们一般情况下不断训练模型使得loss function不断下降（如果task要求loss function是增大，这时候一般加上符号或者转换成 1- loss fucntion，最后实现的还是loss function下降）。好的回到L1 loss function和L2 loss function. 概括：从鲁棒性（对待异常值）的角度看，L1 是比L2 具有更好的性质，因为L2 是把误差进行了平方处理，误差回放大；从稳定性角度（数据集的一个小的移动），L2 是比L1 具有更好的性质，这个是可以从实验的角度进行总结。 L1-norm loss function is also known as least absolute deviations(LAD), least absolute errors. It is basically minimizing the sum of the absolute differences between the target value Y and the estimated values (f(x)). $$S = \sum _ { i = 1 } ^ { n } \left| y _ { i } - f \left( x _ { i } \right) \right|$$ L2-norm loss function is also known as least squares error(LSE). It is basically minimizing the sum of the square of the differences between the target value Y and the estimated values (f(x)). $$S = \sum _ { i = 1 } ^ { n } \left( y _ { i } - f \left( x _ { i } \right) \right) ^ { 2 }$$ 比较L1-norm 和L2-norm在前两个评价指标中的表现： 对于是否具有唯一解，可以从这个角度分析：L1是可以有多种解，而L2只有一种解。 使用范围： L1 适合在稀疏数据上使用，具有特征选择功能，得到的稀疏解；L2 适合在稠密数据上使用，得到的是唯一解，没有特征选择的功能。 As regularization从XGBoost调参指南中我们知道objective function = loss funcion + regularization. 而我们大多数情况下提及的都是loss function,常常忽略了regularization 的作用。 The regularization term controls the complexity of the model, which helps us to avoid overfitting.对于模型训练，一开始的想法是尽量的overfitting, 因为就现在不成熟的经验而言，对于overfitting这个问题有很多处理方法，比如卷积深度神经网络中的dropout, LightGBM中的early stop 和随机采样的思想。 这些方法都是可以缓解overfitting，所以可以出现overfitting。相反，如果你的模型是underfitting，那么你就微显尴尬了。好，收回到L1 and L2。 总结： L1 在系数weights 中使用，L2 在非稀疏的情况下使用更好。可以从计算效率、是否稀疏输出和特征选择进行分析。 计算效率 稀疏结果 特征选择 L1 在稀疏解 上效率比较高，在非稀疏解上效率比较低 产生稀疏解 具有特征选择的功能 L2 在非稀疏解上效率高 不产生稀疏解 没有特征选择的功能 首先理解为什么要正则化？ 如果网络足够的强大，数据量不是那么充足，那么网络完全是可以通过“记住” 这些样本，然后得到很高的 training acc，但是这个 test error 会比较大，这个时候就出现了过拟合。正则化就类似减轻了数据的复杂程度。关于过拟合是可以从两个维度进行解决：网络和数据。 常常使用的 dropout 就是减弱网络复杂性的一种手段，随机减少了某些网络中的节点，那么网络就没有那么强的“记忆”功能，但是最后的loss（要求）还是不变的，所以只能去寻找数据更加“简单普遍”的规律；正则化是对数据和weights 两种类型的，前者是对于数据的操作，然后weights 更加倾向于对于网络结构的操作，使得系数更加“光滑”。 为什么L1 相比于L2 产生了更加稀疏的解？ （这里从数据分布的角度进行解读，L2 的数据来源是高斯分布；L1 可以看成是来自laplace 分布） 这里从数学角度和空间角度进行了解释。 但是从更加 tuition 的角度去理解，L1 没有一个连续的导数，能产生权值为0，所以类似剔除了某些特征，产生了稀疏的权值，而L2 是具有比较连续的导数，产生了比较平滑的权值。 更加数学化的理解： $$f ( x ) = \frac { 1 } { \sqrt { 2 \pi } \sigma } \exp \left( - \frac { ( x - \mu ) ^ { 2 } } { 2 \sigma ^ { 2 } } \right)$$如果把高斯密度函数取对数，那么就只会剩下一个平方项，这个就是L2正则项的来源，所以数据是比较稠密的。 同样，如果是数据比较稀疏，不妨假设来自 laplace 分布，下面是其中的 $u$ 表示位置参数， $ b $ 表示尺度参数, 当 $b$ 越大的时候，图像越低矮，数据越不集中，越类似均匀分布，所以有一种稀疏的感觉。这两个都是可以对标正太分布的。可以看到图像的尾部都是比较平滑，然后大多数是接近于0的。 公式：$$f ( x | \mu , b ) = \frac { 1 } { 2 b } \exp \left( - \frac { | x - \mu | } { b } \right)$$同样取对数，那么得到是L1 正则项。 所以L2 得到的是一个比较平滑的weights 适合处理稠密的向量，而L1 得到是一个稀疏的结果，因为有很大程度的被置为0. 先上公式L1 regularization on least squares:$$\mathbf { w } ^ { * } = \underset { \mathbf { w } } { \arg \min } \sum _ { j } \left( t \left( \mathbf { x } _ { j } \right) - \sum _ { i } w _ { i } h _ { i } \left( \mathbf { x } _ { j } \right) \right) ^ { 2 }$$ L2 regularization on least squares: The difference between their properties can be promptly summarized as follows: 对于第一点computational efficient的理解：平方比绝对值更容易计算，平方可以求导直接求最值，但是绝对值就无法求导。并且L1 regularization在 non-sparse cases中是 computational inefficient，但是在 sparse(0比较多) cases中是有相应的稀疏算法来进行优化的，所以是computational efficient.对于第二点是否具有sparse solution可以从几何意义的角度解读：The green line(L2-norm) is the unique shortest path, while the red, blue yellow(L1-norm) are all same length for the same route. Built-in feature selection is frequently mentioned as a useful property of the L1-norm, which the L2-norm does not. This is actually a result of the L1-norm, which tends to produces sparse coefficients (explained below). Suppose the model have 100 coefficients but only 10 of them have non-zero coefficients, this is effectively saying that “the other 90 predictors are useless in predicting the target values”. L2-norm produces non-sparse coefficients, so does not have this property. 所以表格中第三点也是顺理成章的了。至此，我们区分了L1-norm vs L2-norm loss function 和L1-regularization vs L2-regularization，下面说一下 overfitting的东西。 overfitting现在，我们的训练优化算法是一个由两项内容组成的函数：一个是损失项，用于衡量模型与数据的拟合度，另一个是正则化项，用于衡量模型复杂度。 对于过拟合有两种解读方式，一种是模型是复杂，然后是去拟合了所有的数据，没有了泛化性能； 一种是从数据角度，模型去拟合了noise ，这些random的数据。然后从loss function的角度去优化的话，就是降低模型的复杂度，正则项就是降低模型复杂度的一种手段。所以从这个角度，L2 是降低模型复杂度的一种手段，也是一种减轻overfit的手段，这两者是相辅相成的。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>l1</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LightGBM和XGBoost及其调参]]></title>
    <url>%2F2018%2F07%2F21%2FLightGBM%E5%92%8CXGBoost%E5%8F%8A%E5%85%B6%E8%B0%83%E5%8F%82%2F</url>
    <content type="text"><![CDATA[先主要介绍树的基本知识，然后介绍LightGBM和XGBoost及其调参. 进入正文之前简单的说一下决策树。一棵树很容易过拟合或者欠拟合（根据树的深度），然后需要使用多棵树进行组合预测，而GBDT是实现这个手段的方式之一。sklearn中也是实现了 GBDT 这种思想，但是比较难用，训练速度跟不上。但是 lightGBM 和XGBoost 实现的效果就比较好。 lightGBM调参(常用参数)Since lightGBM is based on decision tree algorithms, it splits the tree with the best fit whereas boosting algorithms split the tree depth wise or level wise rather than leaf-wise. So when growing on the same leaf in Light GBM, the leaf-wise algorithm can reduce more loss than the level-wise algorithm and hence results in much better accuracy which can rarely be achieved by any of the existing boosting algorithms. Also, it is surprisingly very fast, hence the word ‘Light’. Leaf wise splits lead to increase in complexity and may lead to overfitting and it can be overcome by specifying another parameter max_depth parameter. Advantages of LightGBM faster training speed and higher efficiencyLight GBM use histogram based algorithm i.e it buckets continuous feature values into discrete bins which fasten the training procedure. lower memory usageReplaces continuous values to discrete bins which result in lower memory usage. better accuracy than any other boosting algorithmIt produces much more complex trees by following leaf wise split approach rather than a level-wise approach which is the main factor in achieving higher accuracy. compatibility with large datasetsIt is capable of performing equally good with large datasets with a significant reduction in training time as compared to XGBOOST. parallel learning supported lightGBM调参(常用参数) taskdefault= train, option: train, prediction applicationdefault= regression, option: regression, binary, multiclass, lambdarank(lambdarank application) datatraining data, 这个比较诡异，你需要创建一个lightGBM类型的data num_iterationsdefault =100, 可以设置为的大一些，然后使用early_stopping进行调节。 early_stopping_rounddefault =0, will stop training if one metric of one validation data doesn’t improve in last early_stopping_round rounds. num_leavesdefault =31, number of leaves in a tree devicedefault =cpu, options: gpu, cpu, choose gpu for faster training. max_depthspecify the max depth to which tree will grow, which is very important. feature_fractiondefault =1, specifies the fraction of features to be taken for each iteration. bagging_fractiondefault =1, spefifies the fraction of data to be used for each iteration and it generally used to speed up the training and avoid overfitting. max_binmax number of bins to bucket the feature values.因为模型是基于bin训练的，如果bin 数量越多，得到better accuracy,同时更加容易 overfitting. num_threads labelspecify the label columns. categorical_featurespecify the categorical features num_classdefault =1, used only for multi-class classification referrencewhich-algorithm-takes-the-crown-light-gbm-vs-xgboostLightGBM 如何调参官方文档param_tuning官方文档parameter XGBoost调参Advantage of XGBoost regularizationstandard GBM implementation has no regularization, in fact, XGBoost is also known as ‘regularized boosting’ technique. parallel processingwe know that boosting is sequential process so how can it be parallelized? this link to explore further. high flexibilityXGBoost allow users to define custom optimization objectives and evaluation criteria handling missing valuesvery useful property. XGBoost has an in-built routine to handle missing values. User is required to supply a different value than other observations and pass that as a parameter. XGBoost tries different things as it encounter a missing value on each node and learns which path to take for missing values in future. Tree pruningA GBM would stop splitting a node when it encounters a negative loss in the split. Thus it is more of a greedy algorithm. XGBoost on the other hand make splits upon the max_depth specified and then start pruning the tree backwards and remove splits beyond which there is no positive gain. built-in cross-validationThis is unlike GBM where we have to run a grid-search and only a limited values can be tested. continue on existing model XGBoost Parametersgeneral parametersGeneral Parameters: Guide the overall functioning booster:default =gbtree, can be gbtree, gblinear or dart. 一般使用gbtree. silent:default =0, silent mode is activated if set to 1(no running messages will be printed) nthread:default to maximum of threads. booster parametersBooster Parameters: Guide the individual booster (tree/regression) at each step eta(learning rate):default=0.3, typical final values to be used: 0.01-0.2, using CV to tune min_child_weight:minimum sum of instance weight (hessian) needed in a child. If the tree partition step results in a leaf node with the sum of instance weight less than min_child_weight, then the building process will give up further partitioning. In linear regression mode, this simply corresponds to minimum number of instances needed to be in each node. The larger, the more conservative the algorithm will be.default =1,too high values can lead to under-fitting, it should be tuned using CV. 数值越小越容易过拟合，越大越容易 under-fitting. max_depth:default =6, typical values: 3-10, should be tuned using CV. gamma:default =0, Gamma specifies the minimum loss reduction required to make a split.如果在分裂过程中小于该值，那么就不会继续分裂。 subsample:default =1, typical values: 0.5-1. Denotes the fraction of observations to be randomly samples for each tree. colsample_bytree:default =1, typical values: 0.5-1. colsample_bytree和subsample不同点：colsample_by是特征的随机fraction, subsample是rows的随机fraction。 lambda:default =1, L2 regularization term on weights(analogous to Ridge regression). Though many data scientists don’t use it often, it should be explored to reduce overfitting. alpha:default =0, L1 regularization term on weight (analogous to Lasso regression) scale_pos_weight:default =1, a value greater than 0 should be used in case of high class imbalance as it helps in faster convergence. learning task parametersLearning Task Parameters: Guide the optimization performed objectivebinary: logistic- returns predicated probability(not class)multi: softmax- returns predicated class(not probabilities)multi: softprob- returns predicated probability of each data point belonging to each class. eval_metircdefault according to objective(rmse for regression and error for classification), used for validation data.typical values: rmse(root mean square error), mse(mean absolute error), logloss(negative log-likelihood), error(binary classification error rate, 0.5 threshold), auc(area under the curve) seeddefault =0, used for reproducible results and also for parameter tuning. Control OverfittingThere are in general two ways that you can control overfitting in xgboost. The first way is to directly control model complexity. The second way is to add regularization parameters Referrencecomplete guide parameter tuning xgboost with codes python官方文档 param_tuning官方文档 parameter 补充一些理论知识 LightGBM 和 XGBoost 的一些区别 树的增长方式 这个原先是lightGBM所特有的，然后xgboost 在最新的版本上也实现该中方式，这估计就是开源，并不是一成不变的。两者都是基于叶子进行增长的，但是增长的方式是不同的。一种是 level-wise 一种是 leaf-wiseboth xgboost and lightGBM use the leaf-wise growth strategy when growing the decision tree.there are two strategies that can be employed: level-wise and leaf-wise. level-wise maintains a balanced tree（平衡树，左右子树的高度差不超过1）;但是 leaf-wise 这个就比较随意了。这个是这两者的区别：当叶子总数相同的时候，leaf-wise 这种生长方式得到的树的深度是大于 level-wise 的深度的。Compared to the case of level-wise growth, a tree grown with leaf-wise growth will be deeper when the number of leaves is the same. find the best split The key challenge in training a GBDT is the process of finding the best split for each leaf. The computational complexity is thus $O \left( n _ { \text {data} } n _ { \text {features} } \right)$.这两者采用的方式都是：现阶段的中的的数据 数据量和特征量都是很大的，所以这种方式是不可取的。然后这两种方法都是采用了 Histogram-based methods，这样最后的时间复杂度降低到：reducing the computational complexity to $O \left( n _ { d a t a } n _ { b i n s } \right)$. 这个复杂度是取决于number of bins。这个是引入了一个超参数，number of bins ，trade off between precision and time, 当更多的 bins 的时候，这个precision 会提高，但是 time 也会增大。 Ignoring sparse inputs 这个是处理缺省值（或者 0）的手段：两者在split 分裂点的时候，都是先不处理数值 0；然后找到分裂点之后，把0 放到哪边造成的loss 下降的比较大，然后就放到哪边。 Subsampling the data: Gradient-based One-Side Sampling (lightGBM)biased sampling(抽样的基本原则是随机性，但是在抽样过程中由于一系列因素造成偏差抽样，造成样本是不符合真实样本的分布)这个就是缓解，也不是为了彻底让其均衡，lightgbm increases the weigths of the samples.This means that it is more efficient to concentrate on data points with larger gradients.In order to mitigate this problem, lightGBM also randomly samples from data with small gradients.lightGBM increases the weight of the samples with small gradients when computing their contribution to the change in loss (this is a form of importance sampling, a technique for efficient sampling from an arbitrary distribution).]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>XGBoost</tag>
        <tag>LightGBM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[那些年的算法题目（二）]]></title>
    <url>%2F2018%2F07%2F21%2F%E9%82%A3%E4%BA%9B%E5%B9%B4%E7%9A%84%E7%AE%97%E6%B3%95%E9%A2%98%E7%9B%AE%EF%BC%88%E4%BA%8C%EF%BC%89%2F</url>
    <content type="text"><![CDATA[最长01相同子串已知一个长度为N的字符串，只由0和1组成， 求一个最长的子串，要求该子串出现0和1的次数相等。思路：最简单的方式是先生成字串，然后判断每个字串是否满足0的个数和1的个数相同。这种暴力求解时间复杂度O(n^3),明显是不合理的。下面说一下简单的做法：定义一个数组B[N]，B[i]表示从A[0…i]中 num_of_0 - num_of_1，0的个数与1的个数的差 。那么如果A[i] ~ A[j]是符合条件的子串，一定有 B[i] == B[j]，因为中间的部分0、1个数相等，相减等于0。 时间复杂度：O(n)，空间复杂度：O(n) 注意这个是求解的长度，是一个最值问题，而不是最长的01 子串本身。所以使用临时数组记录目前为止的01 的差值，然后选择最大的就行了。时间复杂度是O(N)，空间复杂度是O(N)。 算法的优化的关键在于，可以基于前一步的计算结果继续往下计算。前一步算出了 (i-1) 的01 的差值，那么下一步可以计算前 i 的01 的差值。 12345678910111213141516171819202122232425262728def lengest01SubStr(s): ''' 最长0,1 相等的子串长度 ''' count =[0, 0] B =[0]*len(s) dic =&#123;&#125; # 保存 0 1 的差值 lengest =0 for i in range(len(s)): count[int(s[i])] +=1 B[i] =count[0] - count[1] # start from 0th index if B[i] ==0: lengest +=1 continue if B[i] in dic: # i -dic[B[i]] , not from 0th index lengest =max(lengest, i- dic[B[i]]) else: dic[B[i]] =i return lengesta ='1011010'b ='10110100'print(lengest01SubStr(a)) # 6 # '011010'print(lengest01SubStr(b)) # 8 # '10110100' 顺时针打印矩阵输入一个矩阵，按照从外向里以顺时针的顺序依次扫印出每一个数字。 使用四个循环，就可以解决，这个应该属于找规律的题目。 思路：找到左上角的，一个start_point， 然后根据这个点进行上下左右的循环。 python 中的 range() 是一种左闭右开的区间，并且在逆序遍历的时候，区间的值是不变的，最后使用 -1 就OK了。还有遍历时候的 i, j 都只是一种index，如果一个够用的话，那么就没有必要使用两个。 123456789101112131415161718192021222324252627282930313233343536373839404142def printMatrix(matrix): if not matrix or matrix ==[[]]: return # 第一次见这样判断空的matrix row =len(matrix) column =len(matrix[0]) # 这里的left, right, up, down 都是真实能够access到数据的 left =0 right =column -1 up =0 down =row -1 res =[] while left &lt;right and up &lt;down: # from left to right for i in range(left, right+1): res.append(matrix[up][i]) # from up to down for i in range(up+1, down+1): res.append(matrix[i][right]) # from right to left for i in range(right-1, left-1, -1): res.append(matrix[down][i]) for i in range(down-1, up, -1): res.append(matrix[i][left]) left +=1 right -=1 up +=1 down -=1 # 最后对于这种特殊情况的处理是容易忘记的 # left one row 这种情况很特殊，只是从左往右遍历 if up ==down and left &lt;right: for i in range(left, right+1): res.append(matrix[up][i]) # left one column 只有可能是从上往下遍历 if left ==right and up &lt;down: for i in range(up, down+1): res.append(matrix[i][left]) if up ==down and left ==right: res.append(matrix[left][up]) return resprint(printMatrix(matrix))]]></content>
      <categories>
        <category>算法</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Activation Function and Loss Function]]></title>
    <url>%2F2018%2F07%2F07%2FLoss-Activation-and-Optimisation-Function%2F</url>
    <content type="text"><![CDATA[主要介绍 activation function， loss function 等等概念和分类。 Activation FunctionWhat? It’s just a thing (node) that you add to the output end of any neural network. It is also known as Transfer Function. It can also be attached in between two Neural Networks. $$Output = activation function \left( x _ { 1 } w _ { 1 } + x _ { 2 } w _ { 2 } + \cdots + x _ { n } w _ { n } + b i a s \right)$$ A weighted sum is computed as:$$x _ { 1 } w _ { 1 } + x _ { 2 } w _ { 2 } + \cdots + x _ { n } w _ { n }$$Then, the computed value is fed into the activation function, which then prepares an output.$$activation function \left( x _ { 1 } w _ { 1 } + x _ { 2 } w _ { 2 } + \cdots + x _ { n } w _ { n } + b i a s \right)$$ Think of the activation function as a mathematical operation that normalises the input and produces an output. The output is then passed forward onto the neurons on the subsequent layer. 作用：增加非线性 The thresholds are pre-defined numerical values in the function. This very nature of the activation functions can add non-linearity to the output. Activation Function Types Linear Activation Function: $$ output = k * x$$where $k$ is a scalar value, as an instance 2, and $x$ is the input. Sigmoid or Logistic Activation Function The sigmoid activation function is “S” shaped. It can add non-linearity to the output and returns a binary value of 0 or 1. $$Output = \frac { 1 } { 1 + e ^ { - x } }$$ 这个函数有一个很好的导数形式，在反向传播的时候，效果比较明显。 Tanh Activation Function Tanh is an extension of the sigmoid activation function. Hence Tanh can be used to add non-linearity to the output. The output is within the range of -1 to 1. Tanh function shifts the result of the sigmoid activation function: $$\text { Output } = \frac { 2 } { 1 + e ^ { - 2 x } } - 1$$ Rectified Linear Unit Activation Function (RELU) RELU is one of the most used activation functions. It is preferred to use RELU in the hidden layer. The concept is very straight forward. It also adds non-linearity to the output. However the result can range from 0 to infinity. $$ Output = \max ( 0 , x )$$这个是很高的评价了。If you are unsure of which activation function you want to use then use RELU. Softmax Activation Function Softmax is an extension of the Sigmoid activation function. Softmax function adds non-linearity to the output, however it is mainly used for classification examples where multiple classes of results can be computed. $$Output = \frac { e ^ { x } } { \operatorname { sum } \left( e ^ { x } \right) }$$ 这个一般使用在最后，作为多分类的结束。 Loss Function(Error Function)机器学习中所有的算法都需要最大化或最小化一个函数，这个函数被称为“目标函数”。其中，我们一般把最小化的一类函数，称为“损失函数”。它能根据预测结果，衡量出模型预测能力的好坏。 损失函数 (Loss function) 是用来衡量模型的预测值 $f(x)$ 和真实值 $Y$ 的不一样的程度，通常使用 $L (Y, f(x))$ 来进行表示，损失函数越小，模型的鲁棒性越强。 选择loss 的时候需要考虑两点：分类or 回归问题 和结果的输出情况。 the choice of loss function must match the framing of the specific predictive modeling problem, such as classification or regression. Further, the configuration of the output layer must also be appropriate for the chosen loss function. 总的来说是可以分成三类：回归模型，二分类模型和多分类模型 Regression Loss Functionsa. Mean Squared Error Lossb. Mean Squared Logarithmic Error Lossc. Mean Absolute Error Loss Binary Classification Loss Functionsa. Binary Cross-Entropyb. Hinge Lossc. Squared Hinge Loss Multi-Class Classification Loss Functionsa. Multi-Class Cross-Entropy Lossb. Sparse Multiclass Cross-Entropy Lossc. Kullback Leibler Divergence Loss Regression Loss Function (回归) 如何进行选择？ 对于回归问题，一个baseline 的loss function 是可以选择平方损失函数。从数据的角度进行分析，如果数据服从正太分布，那么平方损失函数没有问题，如果数据有一些 outlier，可以使用 mean squared logarithmic error loss， 先进行 $\hat { y } $然后再计算平方和。如果 outlier 比较多的话，那么使用 mean absolute error loss，计算差值的时候换成绝对值函数。 平方损失函数 定义： Mean Squared Error (MSE), or quadratic, loss function is widely used in linear regression as the performance measure, and the method of minimizing MSE is called Ordinary Least Squares (OSL)。 To calculate MSE, you take the difference between your predictions and the ground truth, square it, and average it out across the whole dataset. $$Loss = \frac { 1 } { n } \sum _ { i = 1 } ^ { n } \left( y ^ { ( i ) } - \hat { y } ^ { ( i ) } \right) ^ { 2 }$$ 在线性回归中，它假设样本和噪声都服从高斯分布（为什么假设成高斯分布呢？其实这里隐藏了一个小知识点，就是中心极限定理），最后通过极大似然估计MLE可以推导出最小二乘式子，即平方损失函数可以通过线性回归在假设样本是高斯分布的条件下推导得到。 $$S E = \sum _ { i = 1 } ^ { n } \left( y _ { i } - y _ { i } ^ { p } \right) ^ { 2 }$$ 为什么选择欧式距离作为误差的度量？ 简单，计算方便 欧式距离是一种很好的相似度衡量标准 在不同的表示域变换之后，特征的性质能够保持不变。 在实际应用中，通常会使用 均方差作为一种衡量指标，就是在上面的公式中除以 N. 使用说明： 如果 target 是服从高斯分布，那么使用 mean squared error 是没有问题；并且没有很好的理由进行替换的话，那么就是他了。 Mathematically, it is the preferred loss function under the inference framework of maximum likelihood if the distribution of the target variable is Gaussian. It is the loss function to be evaluated first and only changed if you have a good reason. Mean Squared Logarithmic Error Loss 和上面的的mse 有一点差别。这个是先记性log 求结果，然后再计算 mse. you can first calculate the natural logarithm of each of the predicted values, then calculate the mean squared error. This is called the Mean Squared Logarithmic Error loss, or MSLE for short. 好处:It has the effect of relaxing the punishing effect of large differences in large predicted values. 使用说明：如果最后的结果的数值有大值，那么可以尝试一下。不是那么符合高斯分布，就可以尝试一下。 Mean Absolute Error Loss 定义：Mean Absolute Error (MAE) is a quantity used to measure how close forecasts or predictions are to the eventual outcomes, which is computed by $$Loss = \frac { 1 } { n } \sum _ { i = 1 } ^ { n } \left| y ^ { ( i ) } - \hat { y } ^ { ( i ) } \right|$$ where $| .|$ denotes the absolute value. Albeit, both MSE and MAE are used in predictive modeling, there are several differences between them. MSE has nice mathematical properties which makes it easier to compute the gradient. However, MAE requires more complicated tools such as linear programming to compute the gradient. Because of the square, large errors have relatively greater influence on MSE than do the smaller error. Therefore, MAE is more robust to outliers since it does not make use of square. On the other hand, MSE is more useful if concerning about large errors whose consequences are much bigger than equivalent smaller ones. MSE also corresponds to maximizing the likelihood of Gaussian random variables. 使用说明： 当有很多的点偏离 mean and variance 的时候，可以尝试使用 mae loss function，这个显著的特点在于 对outlier 是有抵抗作用的。 The Mean Absolute Error, or MAE, loss is an appropriate loss function in this case as it is more robust to outliers. It is calculated as the average of the absolute difference between the actual and predicted values. Binary Classification Loss Function (二分)在二分类的网络结构中经常 出现 Dense() 网络层( 如果使用 keras 进行实现的话), 因为最后想要得到的是一个结点。 Binary Cross-Entropy Loss Cross-entropy loss is often simply referred to as “cross-entropy,” “logarithmic loss,” “logistic loss,” or “log loss” for short. Cross-entropy is the default loss function to use for binary classification problems. 如果没有更好的二分类的选择（理由），那么这个就是首选。 数学定义： Cross-entropy will calculate a score that summarizes the average difference between the actual and predicted probability distributions for predicting class 1. The score is minimized and a perfect cross-entropy value is 0. 二分类问题的交叉熵 loss 主要是有两种形式。第一种是输出的label 是 {0, 1}，也是最为常见的。$$Loss= - [ y \log \hat { y } + ( 1 - y ) \log ( 1 - \hat { y } ) ]$$ 这个公式可以先从一个分段函数推导，然后从最大似然的角度出发，预测类别的概率是:$$P ( y | x ) = \hat { y } ^ { y } \cdot ( 1 - \hat { y } ) ^ { ( 1 - y ) }$$然后取对数：$$log(loss)= - [ y \log \hat { y } + ( 1 - y ) \log ( 1 - \hat { y } ) ]$$ 当 $y =1$ 的时候，$$L = - \log \hat { y }$$因为，$$\hat { y } = \frac { 1 } { 1 + e ^ { - s } }$$所以得到$$L = \log \left( 1 + e ^ { - s } \right)$$loss 的曲线如下图所示： 从图中明显能够看出，s 越大于零，L 越小，函数的变化趋势也完全符合实际需要的情况。（y =1 是这个时候的目标） 当 $y =0$ 的时候：同理可以得到，$$log(loss)= \log \left( 1 + e ^ { s } \right)$$ 从图中明显能够看出，s 越小于零，L 越小，函数的变化趋势也完全符合实际需要的情况。 第二种情况是基于输出label 表示方式 {-1, 1}，这个时候的loss 表达式为: $$loss = \log \left( 1 + e ^ { - y s } \right)$$ 这个时候只不过使用 $ys$ 代替上面的s ，实际上的分析还是一样的。其中 y 表示真实的标签，s 表示 sigmoid 中的s，见下面的公式。$$g ( s ) = \frac { 1 } { 1 + e ^ { - s } }$$当 ys &gt;0 的时候，表示的预测正确，否则是预测错误。 交叉熵损失函数经常使用sigmoid 函数作为激活函数，因为这个可以完美解决平方损失函数中权重更新比较慢点的情况。 使用说明：Mathematically, it is the preferred loss function under the inference framework of maximum likelihood. It is the loss function to be evaluated first and only changed if you have a good reason.It is intended for use with binary classification where the target values are in the set {0, 1}. Hinge Loss An alternative to cross-entropy for binary classification problems is the hinge loss function, primarily developed for use with Support Vector Machine (SVM) models. Hinge Loss，又称合页损失，其表达式如下： $$Loss = \max ( 0,1 - y s )$$其中 y 和 s 的取值范围都是 [-1, 1]. 想想 SVM 中最大化间隔的内容就理解了。图像如下： 如同合起来的书，所以称之为 合页损失。显然，只有当 ys &lt; 1 时，Loss 才大于零；对于 ys &gt; 1 的情况，Loss 始终为零。Hinge Loss 一般多用于支持向量机（SVM）中，体现了 SVM 距离最大化的思想。 而且，当 Loss 大于零时，是线性函数，便于梯度下降算法求导。Hinge Loss 的另一个优点是使得 ys &gt; 0 的样本损失皆为 0，由此带来了稀疏解，使得 SVM 仅通过少量的支持向量就能确定最终超平面。 使用说明：要求 target 转换成 {-1, 1} ，效果有时候比 cross-binary 要好。 It is intended for use with binary classification where the target values are in the set {-1, 1}.Reports of performance with the hinge loss are mixed, sometimes resulting in better performance than cross-entropy on binary classification problems. ( 这句话比较迷离呀) The hinge loss function encourages examples to have the correct sign, assigning more error when there is a difference in the sign between the actual and predicted class values. Squared Hinge Loss (通常上讲，最后的结果更加光滑是没有什么劣势的)A popular extension is called the squared hinge loss that simply calculates the square of the score hinge loss. It has the effect of smoothing the surface of the error function and making it numerically easier to work with. 需要从 {0, 1} -&gt; {-1, 1} 这样target 的转换, 非常容易实现。 123# change y from &#123;0,1&#125; to &#123;-1,1&#125;y[where(y == 0)] = -1` 使用说明： （很强的相关性了）If using a hinge loss does result in better performance on a given binary classification problem, is likely that a squared hinge loss may be appropriate. Multi-Class Classification Loss Functions对于多类问题的定义： The problem is often framed as predicting an integer value, where each class is assigned a unique integer value from 0 to (num_classes – 1). The problem is often implemented as predicting the probability of the example belonging to each known class. Multi-Class Cross-Entropy Loss Cross-entropy is the default loss function to use for multi-class classification problems. （可见 交叉熵对于分类问题的重要性） 同理，如果是最大似然，概率模型，donot hesitate.( 数学基础就是在这里)Mathematically, it is the preferred loss function under the inference framework of maximum likelihood. It is the loss function to be evaluated first and only changed if you have a good reason. Cross-entropy will calculate a score that summarizes the average difference between the actual and predicted probability distributions for all classes in the problem. The score is minimized and a perfect cross-entropy value is 0. 基于 keras实现的时候需要先把 target (label) 转成 one-hot 类型的，当然这个可能造成 loss 曲线的波动（后话）。12# one hot encode output variabley = to_categorical(y) Sparse Multiclass Cross-Entropy Loss 和上面的区别主要在于 label 是不需要转成 one-hot 类型的，保持这原来的 number 形式。Sparse cross-entropy addresses this by performing the same cross-entropy calculation of error, without requiring that the target variable be one hot encoded prior to training. Kullback Leibler Divergence Loss Kullback Leibler Divergence, or KL Divergence for short, is a measure of how one probability distribution differs from a baseline distribution. 数学原理：(以 bit 为单位的 信息熵) A KL divergence loss of 0 suggests the distributions are identical. In practice, the behavior of KL Divergence is very similar to cross-entropy. It calculates how much information is lost (in terms of bits) if the predicted probability distribution is used to approximate the desired target probability distribution. 使用说明：更常使用 在复杂的模型上，比如 dense representation 之列。当然也是可以使用在多分类的情况下，这个时候如同 multi-class cross-entropy. As such, the KL divergence loss function is more commonly used when using models that learn to approximate a more complex function than simply multi-class classification, such as in the case of an autoencoder used for learning a dense feature representation under a model that must reconstruct the original input. In this case, KL divergence loss would be preferred. Nevertheless, it can be used for multi-class classification, in which case it is functionally equivalent to multi-class cross-entropy. 参考文献： How to Choose Loss Functions When Training Deep Learning Neural Networks log 对数损失函数 在逻辑回归的推导中，它假设样本服从伯努利分布（0-1分布），然后求得满足该分布的似然函数，接着取对数求极值等等。 指数损失函数 公式如下：$$loss = e ^ { - y s }$$ 曲线如下： Exponential Loss 与交叉熵 Loss 类似，但它是指数下降的，因此梯度较其它 Loss 来说，更大一些。 Exponential Loss 一般多用于AdaBoost 中。因为使用 Exponential Loss 能比较方便地利用加法模型推导出 AdaBoost算法。 $$L = - \log \frac { e ^ { s } } { \sum _ { j = 1 } ^ { C } e ^ { s _ { j } } } = - s + \log \sum _ { j = 1 } ^ { C } e ^ { s _ { j } }$$ softmax loss 的曲线如下图所示： 上图中，当 s &lt;&lt; 0 时，Softmax 近似线性；当 s&gt;&gt;0 时，Softmax 趋向于零。Softmax 同样受异常点的干扰较小，多用于神经网络多分类问题中。 若我们把 ys 的坐标范围取得更大一些，上面 5 种 Loss 的差别会更大一些，如图： 显然，这时候 Exponential Loss 会远远大于其它 Loss。从训练的角度来看，模型会更加偏向于惩罚较大的点，赋予其更大的权重。如果样本中存在离群点，Exponential Loss 会给离群点赋予更高的权重，但却可能是以牺牲其他正常数据点的预测效果为代价，可能会降低模型的整体性能，使得模型不够健壮（robust）。 相比 Exponential Loss，其它四个 Loss，包括 Softmax Loss，都对离群点有较好的“容忍性”，受异常点的干扰较小，模型较为健壮。 Softmax loss 对于多分类问题，可以使用 softmax loss。 其中，C 为类别个数，小写字母 s 是正确类别对应的 Softmax 输入，大写字母 S 是正确类别对应的 Softmax 输出。 由于 log 运算符不会影响函数的单调性，我们对 S 进行 log 操作。另外，我们希望 log(S) 越大越好，即正确类别对应的相对概率越大越好，那么就可以对 log(S) 前面加个负号，来表示损失函数： 如何选择损失函数？ 对于异常点的处理是一个维度，比如L1 损失函数处理异常点更加稳定，相对L2 损失函数。 what? 衡量模型好坏的 function，如果模型表现好，那么loss 应该是小；如果模型表现不好，那么loss 应该是大的。 At its core, a loss function is incredibly simple: it’s a method of evaluating how well your algorithm models your dataset. If your predictions are totally off, your loss function will output a higher number. If they’re pretty good, it’ll output a lower number. As you change pieces of your algorithm to try and improve your model, your loss function will tell you if you’re getting anywhere. Log Loss (Cross Entropy Loss) Cross-entropy loss, or log loss, measures the performance of a classification model whose output is a probability value between 0 and 1. Cross-entropy loss increases as the predicted probability diverges from the actual label. So predicting a probability of .012 when the actual observation label is 1 would be bad and result in a high loss value. A perfect model would have a log loss of 0. The graph above shows the range of possible loss values given a true observation (isDog = 1). As the predicted probability approaches 1, log loss slowly decreases. As the predicted probability decreases, however, the log loss increases rapidly. Log loss penalizes both types of errors, but especially those predictions that are confident and wrong! Cross-entropy and log loss are slightly different depending on context, but in machine learning when calculating error rates between 0 and 1 they resolve to the same thing. In binary classification, where the number of classes M equals 2, cross-entropy can be calculated as: $$- ( y \log ( y ) + ( 1 - y ) \log ( 1 - y ) )$$ If $ M&gt;$2 (i.e. multiclass classification), we calculate a separate loss for each class label per observation and sum the result. $$- \sum _ { c = 1 } ^ { M } y _ { o , c } \log \left( y _ { o , c } \right)$$ M - number of classes (dog, cat, fish)log - the natural logy - binary indicator (0 or 1) if class label c is the correct classification for observation o 想要表达的是 log loss 是从 Likelihood Loss，改进过来的，有没有发现最大似然的痕迹。log loss 表达式如下：$$\begin{split}P(Y | X) &amp;= P(X_1 | Y) \times P(X_2 | Y) \times \dots \times P(X_n | Y) \times P(Y) = P(Y) \prod_{i}^{n} P(X_i | Y) \\&amp;\Rightarrow log(P(Y | X)) = log(\prod_{i}^{n} P(X_i | Y) \Rightarrow \sum_{i}^{n} log(P(X_i | Y))\end{split}$$交叉熵表达式：$$CE(\hat{y}, y) = - \sum_{i=1}^{n} y_i log(\hat{y}) + (1 - y_i) log(1 - \hat{y})$$ L2 这两个loss function 在这里介绍过，所以本博客中简单说一下。 L2 loss function is the square of the L2 norm of the difference between actual value and predicted value. It is mathematically similar to MSE, only do not have division byn, it is computed by $$ Loss = \sum _ { i = 1 } ^ { n } \left( y ^ { ( i ) } - \hat { y } ^ { ( i ) } \right) ^ { 2 }$$ Kullback Leibler (KL) Divergence（计算的是两个分布的问题）KL Divergence, also known as relative entropy, information divergence/gain, is a measure of how one probability distribution diverges from a second expected probability distribution. KL divergence loss function is computed by$$D _ { K L } ( p | q ) = \sum _ { x } p ( x ) \log \frac { p ( x ) } { q ( x ) }$$ 交叉熵的定义：$$H ( p , q ) = - \sum _ { x } p ( x ) \log q ( x )$$两者的关系推导，$$\begin{split}D _ { K L } ( p | q ) &amp;= \sum _ { x } p ( x ) \log \frac { p ( x ) } { q ( x ) } \\&amp;= \sum _ { x } ( p ( x ) \log p ( x ) - p ( x ) \log q ( x ) ) \\&amp;= - H ( p ) - \sum _ { x } p ( x ) \log q ( x ) \\&amp;= - H ( p ) + H ( p , q )\end{split}$$所以说， cross entropy 也是可以写成这样：$$H ( p , q ) = D _ { K L } ( p | q ) + H ( p )$$ logistic loss 和 cross entropy的关系 当 $ p \in { y , 1 - y }$, $q \in { \hat { y } , 1 - \hat { y } }$ ，cross entropy 可以写成 logistic loss: $$H ( p , q ) = - \sum _ { x } p ( x ) \log q ( x ) = - y \log \hat { y } - ( 1 - y ) \log ( 1 - \hat { y } )$$ 交叉熵函数是怎么来的？从上面可以清楚的了解到，交叉熵函数在分类问题上是 default 的选择，那么我们有没有思考过 这个loss function 的数学基础 ？是怎么来的呢？ 真实的样本标签为 [0, 1], 分别表示负类和正类，模型最终会经过一个 sigmoid 函数，输出一个概率值。sigmoid 函数的表达式如下：$$g ( s ) = \frac { 1 } { 1 + e ^ { - s } }$$ 所以sigmoid 的输出值表示 1的概率：$$\hat { y } = P ( y = 1 | x )$$表示 0的概率：$$1 - \hat { y } = P ( y = 0 | x )$$ 进而，从最大似然的角度出发，将上面的两种情况整合起来： $$P ( y | x ) = \hat { y } ^ { y } \cdot ( 1 - \hat { y } ) ^ { 1 - y }$$ 从另一个角度也可以理解这个公式，分别令 y =0 和y =1， 发现两种情况正好对应着 $P ( y = 0 | x ) = 1 - \hat { y }$ 和 $P ( y = 1 | x ) = \hat { y }$。 我们做的就是把上面两种情况给整合起来了。 接下来引入 log 函数$$\log P ( y | x ) = \log \left( \hat { y } ^ { y } \cdot ( 1 - \hat { y } ) ^ { 1 - y } \right) = y \log \hat { y } + ( 1 - y ) \log ( 1 - \hat { y } )$$我们希望公式越大越好，反过来，希望 $\log \mathrm { P } ( \mathrm { y } | \mathrm { x } )$ 越小越好，于是得到了最终的 损失函数：$$L = - [ y \log \hat { y } + ( 1 - y ) \log ( 1 - \hat { y } ) ]$$ 上面是 单个样本的损失函数，计算N 个样本，只需要将上面的公式叠加起来。$$L = - \sum _ { i = 1 } ^ { N } y ^ { ( i ) } \log \hat { y } ^ { ( i ) } + \left( 1 - y ^ { ( i ) } \right) \log \left( 1 - \hat { y } ^ { ( i ) } \right)$$ ok，这个就是交叉熵损失函数完整的推导过程。 二分类、多分类和多标签问题的区别二分类、多分类与多标签分类问题使用不同的激活函数和损失函数。 基本概念二分类：判别这个水果是苹果还是香蕉。多分类：对于一堆水果，辨别是苹果、梨还是橘子。一个样本只能有一个标签。多标签分类： 给每一个样本一系列的目标标签。比如一个文档有不同的相关话题，需要加上不同的tag 如宗教、政治和教育。 多分类问题常常是可以转换成二分类问题进行处理的，常见有两种策略。 一对一的策略给定数据集D这里有N个类别，这种情况下就是将这些类别两两配对，从而产生 $\frac{N*(N-1)}{2}$个二分类任务，在测试的时候把样本交给这些分类器，然后进行投票。 一对其余策略将每一次的一个类作为正例，其余作为反例，总共训练N个分类器。测试的时候若仅有一个分类器预测为正的类别则对应的类别标记作为最终分类结果，若有多个分类器预测为正类，则选择置信度最大的类别作为最终分类结果。 同样，多标签分类和二分类问题也是有关系的。 面临的问题： 图片的标签数目不是固定的，有的有一个标签，有的有两个标签，但标签的种类总数是固定的，比如为5类。 解决该问题： 采用了标签补齐的方法，即缺失的标签全部使用0标记，这意味着，不再使用one-hot编码。例如：标签为：-1,1,1,-1,1 ;-1表示该类标签没有，1表示该类标签存在。 如何衡量损失？ 计算出一张图片各个标签的损失，然后取平均值。 如何计算精度 计算出一张图片各个标签的精度，然后取平均值。 该处理方法的本质：把一个多标签问题，转化为了在每个标签上的二分类问题。 损失函数的选择 基于逻辑回归的二分类问题：使用逻辑回归二分类loss function的推导，上面的一小节是有详细的介绍的。 基于 Softmax 的多分类问题 softmax层中的softmax 函数是logistic函数在多分类问题上的推广，它将一个N维的实数向量压缩成一个满足特定条件的N维实数向。压缩后的向量满足两个条件： 向量中的每个元素的大小都在[0,1] 向量所有元素的和为 1 因此，softmax适用于多分类问题中对每一个类别的概率判断，softmax的函数公式如下：$$a _ { j } ^ { L } = \frac { e ^ { z _ { j } ^ { L } } } { \sum _ { k } e ^ { z _ { k } ^ { L } } }$$ 基于 Softmax 的多分类问题采用的是 log似然代价函数（log-likelihood cost function）来解决。单个样本的 log似然代价函数的公式为：$$C = - \sum _ { i } \left( y _ { i } \log a _ { i } \right)$$其中， $y_i $表示标签向量的第 i 个分量。因为往往只有一个分量为 1 其余的分量都为 0，所以可以去掉损失函数中的求和符号，化简为， $$C = - \ln a _ { j }$$其中，$ a_j $是向量 y 中取值为 1 对应的第 j 个分量的值。 $$\begin{split}\operatorname { cost } \left( h _ { \theta } ( x ) , y \right) &amp;= - y _ { i } \log \left( h _ { \theta } ( x ) \right) - \left( 1 - y _ { i } \right) \log \left( 1 - h _ { \theta } ( x ) \right)$ \\C &amp;= - \sum _ { i } \left( y _ { i } \log a _ { i } \right)$\\\end{split}$$ 理论上都是使用多类交叉熵函数，但是在实现的时候，深度学习工具keras 是支持两种形式，针对于标签y 的形式，一种是 sparse 一种是 dense分别对应的是 one-hot 形式和 label 的形式。 因为这两个交叉熵损失函数对应不同的最后一层的输出。第一个对应的最后一层是 sigmoid，用于二分类问题，第二个对应的最后一层是 softmax，用于多分类问题。但是它们的本质是一样的，请看下面的分析。 可以看一下交叉熵函数的定义： $$-\int p ( x ) \log g ( x ) d x$$ 交叉熵是用来描述两个分布的距离的，神经网络训练的目的就是使 $g(x)$ 逼近$ p(x)$。 总结 分类问题名称 输出层使用激活函数 对应的损失函数 二分类 sigmoid 二分类交叉熵损失函数（binary_crossentropy） 多分类 softmax 多类别交叉熵损失函数（categorical_crossentropy） 多标签分类 sigmoid函数 二分类交叉熵损失函数（binary_crossentropy）]]></content>
      <categories>
        <category>Deep learning</category>
      </categories>
      <tags>
        <tag>loss function</tag>
        <tag>Activation Function</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[特征工程]]></title>
    <url>%2F2018%2F06%2F29%2F%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[关于特征工程的概念，特征离散化、特征生成、组合特征、特征选取的方法。 特征生成机器学习的一大任务在于手工 create 特征。特征主要来源有两个，一个统计学知识，一个该场景下的特征。前者属于常规操作，后者需要有对业务领域比较熟悉的了解。后者举个例子，比如在衡量个人信用的时候， days_employed / days_birth 就是一个属于金融（保险）领域的较为熟悉，才能理解的一个特征, 这种百分比，占比的思想还是非常常见的。前者同样举个栗子，对于子表（传统机器学习还是有很多来自数据库的信息的）常用的操作是 aggregation 操作（min max sum variance mean）等聚合操作，然后和主表进行连接。一般来说子表是可以广泛的使用 aggregation 操作，但是对于主表来说，这个就取决于信息是什么，特征是什么内容，聚合函数的本质在于”总结“，就是你操作的变量是否有必要这样做，看一下数据是不是流水账。 特征离散化？连续化特征就是一些不可枚举的有理数。那么什么是离散化特征呢？ 离散化特征就是可枚举的特征。离散化的作用是把数据变成可计算状态。而特征工程就是从原始字段中根据业务提取出对模型有效的特征出来。 在线性模型下(w.x)，w已经确定的情况下，x的某个特征的值是20，或者30，w.x的值相差是很大的，但是往往20岁的人跟30岁的人对同一个广告的兴趣差距不会那么大。连续特征离散化的基本假设，是默认连续特征不同区间的取值对结果的贡献是不一样的。离散化和连续化最大的区别是，对一个字段做连续化后的结果就还只是一个特征，而离散化后的这一列有多少个key(字段可能的值)就会抽取出多少个特征。当经过离散化之后，特征各有各的权重，彼此之间就没有关系了。 模型是使用离散特征还是连续特征, 其实是一个“海量离散特征+简单模型” 同 “少量连续特征+复杂模型”的权衡。既可以离散化用线性模型，也可以用连续特征加深度学习。 常用的选取离散点的方法：等距离离散，等样本离散、画图观察趋势和决策树模型(天生就可以对连续特征分段)。 离散化：变量分箱（在风控模型中是这样进行阐述的），是对于连续变量的离散化的一种称呼。分箱的方式，一般有等距离分段，等深分段（先确定分段数量，然后令每个分段中的数据数量大致相等）和最优分段。 在工业界，很少直接将连续值作为特征喂给逻辑回归模型，而是将连续特征离散化为一系列0、1特征交给逻辑回归模型，这样做的优势有以下几点： 单变量离散化为N个后，每个变量有单独的权重，在激活函数的作用下相当于为模型增加了非线性，能够提升模型表达能力，加大拟合。 离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄&gt;30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰，因为特征值的异常会导致权重也就是w的值也会异常。 一定有同学担心特征过多会导致运算缓慢，但是LR是线性模型，我们在内部计算的时候是向量化计算，而不是循环迭代。稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展。 所以不用担心像GBDT算法那样，特征多了就跑不动了(我们都说GBDT不能用离散特征不是因为它处理不了离散特征，而是因为离散化特征后会产生特别多的特征，决策树的叶子节点过多，遍历的时候太慢了)。 所以海量离散特征＋LR是业内常见的一个做法。而少量连续特征+复杂模型是另外一种做法，例如GBDT。 当然也可以通过深度学习来进行特征选择：目前这种手段正在随着深度学习的流行而成为一种手段，尤其是在计算机视觉领域，原因是深度学习具有自动学习特征的能力，这也是深度学习又叫unsupervised feature learning的原因。从深度学习模型中选择某一神经层的特征后就可以用来进行最终目标模型的训练了。 参考文献:https://blog.csdn.net/lujiandong1/article/details/52412123 组合特征先是离散化，然后是特征组合。交叉从理论上而言是为了引入特征之间的交互，也即为了引入非线性。LR(逻辑回归）分类算法:因为线性函数的表达能力有限，所以我们引入激活函数就是给LR增加非线性关系。能让一条直线变成曲线。这样可以拟合出更好的效果。（也由此才后了后来说的过拟合问题而引入了正则化超参数） LR模型之所以很受欢迎，主要是因为LR模型本质是对数线性模型，实现简单，易于并行，大规模扩展方便，迭代速度快，同时使用的特征比较好解释，预测输出在0与1之间契合概率模型。（模型的可解释性举例，比如A-B的权重比较大，A代表用户，B代表物品，那么可以认为A是对B比较感兴趣的）但是，线性模型对于非线性关系缺乏准确刻画，特征组合正好可以加入非线性表达，增强模型的表达能力。另外，广告LR中，基本特征可以认为是用于全局建模，组合特征更加精细，是个性化建模，因为在这种大规模离散LR中，单对全局建模会对部分用户有偏，对每一用户建模又数据不足易过拟合同时带来模型数量爆炸，所以基本特征+组合特征兼顾了全局和个性化。 从统计的角度解释，基本特征仅仅是真实特征分布在低维的映射，不足以描述真实的分布，加入特征在高维空间拟合真实分布，使得预测更加准确。 寻找高级特征最常用的方法有：若干项特征加和： 我们假设你希望根据每日销售额得到一周销售额的特征。你可以将最近的7天的销售额相加得到。若干项特征之差： 假设你已经拥有每周销售额以及每月销售额两项特征，可以求一周前一月内的销售额。若干项特征乘积： 假设你有商品价格和商品销量的特征，那么就可以得到销售额的特征。若干项特征除商： 假设你有每个用户的销售额和购买的商品件数，那么就是得到该用户平均每件商品的销售额。 正则化真正测试一个模型的不是简单与否，更重要在于它在预测新的情况时表现如何。小权重意味着网络的行为不会因为我们随意更改了一些输入而改变太多。 这里是需要使用latex 补充一下公式的。公式是可以参考着上面进行书写。 https://testerhome.com/topics/10811 这是我们加了正则化之后的成本函数，可以看我们后面加入了正则化 λ 的表达式来完善成本函数。为什么加入λ能够减轻过拟合呢？直观一点的解释是设置的λ值越大，那么参数w的值就会被压缩的越小(在梯度下降中, 每次迭代的步长，也就是这个公式w=w - 学习率*成本函数对w的导数， 现在由于成本函数增加了正则项，使得J和w变得数值相关了)。 假设λ设置的足够大，那么w会无限的趋近于0. 把多隐藏层的单元的权重设置为0以后，那么基本上就是消除掉了这些单元的作用，而使得网络模型得到简化，就像下面的图一样。由于正则化的设置，消除了一些隐藏单元的作用。而使得整个模型越来越接近于线性化，也就是从下图中的过拟合往欠拟合偏转。当然我们有一个适合的λ的值，能让我们的拟合状态达到最佳。所以我们在训练模型的时候，往往都会有一个Ｌ２正则项的超参数需要我们设置。 feature selection定义： feature selection 的过程就是dimension reduction的过程。就是说由较多的数据集 映射到 较少的数据集，这种方式就叫做降维。 Using dimensionality reduction techniques, of course. You can use this concept to reduce the number of features in your dataset without having to lose much information and keep (or improve) the model’s performance. 为什么? （必要性分析） 时间角度，空间（内存）角度。减少冗余信息，就减少了模型去拟合”噪声“数据的可能性。 常见的有以下几种方式： 可以归结成几类：特征本身（数据缺省值比较大，数据的波动性比较小），特征和特征之间（特征具有较高的相关性，使用PCA 进行降维），特征和最后的target的关系（机器学习模型的 feature importance，卡方分布检测特征和target 的重要性，Pearson 相关系数：-1 表示负相关，0 表示不相关，1表示正相关）。 可以划分成三类： 一、独立于模型的特征选择（没有在入模时候进行的特征选择）或者叫做 filter： 移除低方差的特征 (Removing features with low variance) 当特征值都是离散型变量的时候这种方法才能用，如果是连续型变量，就需要将连续变量离散化之后才能用。 单变量特征选择 (Univariate feature selection) 对于分类问题(y离散)，可采用：卡方检验，f_classif, mutual_info_classif，互信息对于回归问题(y连续)，可采用：皮尔森相关系数，f_regression, mutual_info_regression，最大信息系数 这里说的是特征选择，但是上面说的都是针对“特征重要性” 这点展开的，但是一个特征入模是一个复杂的过程，需要考虑的因素很多。比如：变量的预测能力，变量之间的相关性，变量的简单性（容易生成和使用），变量的强壮性（不容易被绕过），变量在业务上的可解释性（被挑战时可以解释的通）等等。当然，其中最主要和最直接的衡量标准是变量的预测能力。 尤其是当你使用LR 这类简单的模型的时候，是需要重点的在特征上下功夫的，因为模型是线性的，比较简单，引入特征，加入非线性，然后才能更好的表达实际问题。 二、基于模型选择的特征排序 有些机器学习方法本身就具有对特征进行打分的机制，或者很容易将其运用到特征选择任务中，例如回归模型，SVM，决策树，随机森林等等。这种方法的思路是直接使用你要用的机器学习算法，针对 每个单独的特征 和 响应变量建立预测模型。假如 特征 和 响应变量 之间的关系是非线性的，可以用基于树的方法(决策树、随机森林)、或者 扩展的线性模型 等。 三、无监督的模型选择 聚类，可以从降维的角度理解。可以在机器学习算法中的importance 不是很大，容易被忽略的特征。 Principal Component Analysis (PCA)为什么要进行降维数据的处理？ 可以从 计算和存储的角度分析，还可以从模型和数据的角度分析。 这个属于一句话的的思想：PCA的问题其实是一个基的变换，使得变换后的数据有着最大的方差。 pca 的假设：这个说得很清楚，higher dimensional space is mapped to lower dimension, 然后这种在lower dimensional 中数据的variance应该是保持max的。It works on a condition that while the data in a higher dimensional space is mapped to data in a lower dimension space, the variance of the data in the lower dimensional space should be maximum. 就是需要理解这个主成分是不断的生成的，在前者的基础之上生成的。below are some of the key points you should know about PCA before proceeding further: A principal component is a linear combination of the original variables Principal components are extracted in such a way that the first principal component explains maximum variance in the dataset Second principal component tries to explain the remaining variance in the dataset and is uncorrelated to the first principal component Third principal component tries to explain the variance which is not explained by the first two principal components and so on The first component is the most important one, followed by the second, then the third, and so on. SVDSVD decomposes the original variables into three constituent matrices. It is essentially used to remove redundant features from the dataset. It uses the concept of Eigenvalues and Eigenvectors to determine those three matrices. We will not go into the mathematics of it due to the scope of this article, but let’s stick to our plan, i.e. reducing the dimensions in our dataset. $$\operatorname { Data } _ { m \times n } = U _ { m \times m } \Sigma _ { m \times n } V _ { n \times n } ^ { T }$$SVD将原始的数据集矩阵Data分解成三个矩阵：U、Sigma、VT，如果原始矩阵是m行n列，那么U、Sigma和VT分别就是m行m列、m行n列、n行n列。比较值得一提的是矩阵Sigma，该矩阵只有对角元素，其他元素均为0，有一个惯例是：Sigma的对角元素是从大到小排列的。这些对角元素就称为奇异值. PCA 是方阵是 $ mm $操作，那么SVD 是$mn$ 就是更加广泛的矩阵操作。 投影也是一种降维手段这种思想真的是服气，虽然我也不是很懂，但是思想是很好的By projecting one vector onto the other, dimensionality can be reduced. 当投影到另一个平面的时候，原来的平面维度就消失了，所以只剩下了投影面的维度。 T-sne就是指出 t-sne 这个可以非线性降维。有local approaches 和 global approaches 两种方式，我大概的理解就是：类之间还能尽可能的远离，类内保持差异性。（不知道对不对）So far we have learned that PCA is a good choice for dimensionality reduction and visualization for datasets with a large number of variables. But what if we could use something more advanced? What if we can easily search for patterns in a non-linear way? t-SNE is one such technique. There are mainly two types of approaches we can use to map the data points:Local approaches : They maps nearby points on the manifold to nearby points in the low dimensional representation.Global approaches : They attempt to preserve geometry at all scales, i.e. mapping nearby points on manifold to nearby points in low dimensional representation as well as far away points to far away points. 杂货铺 对于encodding 的理解，从字符转成数字，并且尽可能的保留原来的信息。接触的有三种，一种 label encoding，适合类别信息只有两类。如果大于两类那么就使用one-hot。第三种就是万物可以embedding，使用神经网络的思想。 特征工程分为特征构造 (feature creation or construction)和 特征选择（feature selection） 。 归一化（去中心，方差归一）是属于特征(预)处理:把特征值压缩成0~1的区间。 One-hot（也叫One-of-k）的方法把每个无序特征转化为一个数值向量。比如一个无序特征color有三种取值：red，green，blue。那么可以用一个长度为3的向量来表示它，向量中的各个值分别对应于red，green，blue。如：这种方法在NLP里用的很多，就是所谓的词向量模型。变换后的向量长度对于词典长度，每个词对应于向量中的一个元素。 过拟合和欠拟合问题： 可以从数据和模型两方面考虑。 过拟合 欠拟合 数据 收集更多的数据 或者 数据增强 模型 模型简单化 (dropout, normalizaion )，目标函数加上L1 or L2 ，不同模型取平均 模型变得复杂（如果树的结构，那么树个数增加），loss function 训练 early stop, 随机采样 (加入了随机性) 多训练一下看看效果 待解决的问题问题一： IV 的计算 IV的全称是Information Value，中文意思是信息价值，或者信息量。从直观逻辑上大体可以这样理解“用IV去衡量变量预测能力”这件事情：我们假设在一个分类问题中，目标变量的类别有两类：Y1，Y2。对于一个待预测的个体A，要判断A属于Y1还是Y2，我们是需要一定的信息的，假设这个信息总量是I，而这些所需要的信息，就蕴含在所有的自变量C1，C2，C3，……，Cn中，那么，对于其中的一个变量Ci来说，其蕴含的信息越多，那么它对于判断A属于Y1还是Y2的贡献就越大，Ci的信息价值就越大，Ci的IV就越大，它就越应该进入到入模变量列表中。IV的计算是以WOE为基础的。例中性别变量的信息值为0.0147，表示性别对目标变量的预测能力非常弱。一般说来，信息值0.02以下表示与目标变量相关性非常弱。0.02-0.1很弱；0.1-0.3一般；0.3-0.5强；0.5-1很强。（https://blog.csdn.net/kevin7658/article/details/50780391 这个讲解的是iv 计算过程，可以说是很详细了） 问题二： 非监督模型，比如说聚类是如何真正的在 feature selection 中使用的？ 需要的是例子。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>PCA</tag>
        <tag>SVD</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常见的排序算法总结]]></title>
    <url>%2F2018%2F06%2F29%2F%E5%B8%B8%E8%A7%81%E7%9A%84%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[分类和总结 根据待排序的数据大小不同，使得排序过程中所涉及的存储器不同，可分为内部排序和外部排序。 排序关键字可能出现重复，根据重复关键字的排序情况可分为稳定排序和不稳定排序。冒泡、插入和归并是稳定，在这里提到的其他的几个都是不稳定的。 对于内部排序，依据不同的排序原则，可分为插入排序、交换(快速)排序、选择排序、归并排序和计数排序。 针对内部排序所需的工作量划分，可分为:简单排序 O(n^2)、先进排序 O(nlogn)和基数排序 O(d*n)。常见算法的性质总结： 排序算法实现默认都是升序… 插入排序(Insert Sort)思想： 有序数组+insert one every one time 插入排序的工作方式非常像人们排序一手扑克牌一样。开始时，我们的左手为空并且桌子上的牌面朝下。然后，我们每次从桌子上拿走一张牌并将它插入左手中正确的位置。为了找到一张牌的正确位置，我们从右到左将它与已在手中的每张牌进行比较，如下图所示：步骤： 从第一个元素开始，该元素可以认为已经被排序 取出下一个元素，在已经排序的元素序列中从后向前扫描 如果该元素（已排序）大于新元素，将该元素移到下一位置 重复步骤3，直到找到已排序的元素小于或者等于新元素的位置 将新元素插入到该位置后 重复步骤2~5 123456789101112131415def insert_sort(lists): count = len(lists) for i in range(1, count): key =lists[i] j =i-1 while j &gt;= 0: if lists[j] &gt;key: lists[j+1] =lists[j] lists[j] =key j -= 1 return lists# testlists =[1,2,-3, 90,34]print(insert_sort(lists)) 选择排序(Select Sort)思想和步骤： select one every time 简单选择排序是最简单直观的一种算法，基本思想为每一趟从待排序的数据元素中选择最小（或最大）的一个元素作为首元素，直到所有元素排完为止，简单选择排序是不稳定排序。 分析： 无论什么数据进去都是 O(n²) 的时间复杂度。所以用到它的时候，数据规模越小越好。唯一的好处可能就是不占用额外的内存空间了吧。 代码实现: 12345678910111213def select_sort(lists): count =len(lists) for i in range(0, count): min =i for j in range(i+1, count): if lists[min] &gt;lists[j]: min =j lists[min], lists[i] =lists[i], lists[min] return lists# testlists =[1, 23,45, 0,-1]print(select_sort(lists)) 冒泡排序(Bubble Sort)思想： move smallest one time (假设不减) 一遍排序之后，右边是最大，也就是排好序的。这个规则是以此类推的。 有两种说法 从右往左： 最小值被移到了最左边。 【冒泡法】的本意 从左往右： 最大值被移到了最右边。 此时其实应该叫 【沉石法】 从左向右 沉石法 图解： 分析：平均时间复杂度：O(n^2)最坏空间复杂度： 总共 O(n)，需要辅助空间 O(1) 1234567891011def bubble_sort(lists): count =len(lists) for i in range(0, count): for j in range(i+1, count): if lists[i]&gt; lists[j]: lists[i], lists[j] =lists[j], lists[i] return lists# testlists =[1,34,45,0,89]print(bubble_sort(lists)) 归并排序(Merge Sort)思想： divide-and-conquer +递归 归并排序（MERGE-SORT）是利用归并的思想实现的排序方法，该算法采用经典的分治（divide-and-conquer）策略（分治法将问题分(divide)成一些小的问题然后递归求解，而治(conquer)的阶段则将分的阶段得到的各答案”修补”在一起，即分而治之)。 分析：时间复杂度：O(nlogn)空间复杂度：O(N)，归并排序需要一个与原数组相同长度的数组做辅助来排序 1234567891011121314151617181920212223242526def merge(left, right): i, j =0,0 result =[] while i&lt;len(left) and j &lt;len(right): if left[i] &lt;= right[j]: result.append(left[i]) i +=1 else: result.append(right[j]) j +=1 result += left[i:] result += right[j:] return resultdef merge_sort(lists): if len(lists) &lt;=1: return lists num =int(len(lists)/2) left =merge_sort(lists[:num]) right = merge_sort(lists[num:]) return merge(left, right)# merge_sort 是先切分，然后再整合，quick sort 是两个指针# testlists =[1, 34, 23,45,0,9]print(merge_sort(lists)) 快速排序(Quick Sort) ***思想：任意选择一个key(通常选择a[0])，将比他小的数据放在它的前面，比他大的数字放在它的后面。递归进行。 步骤： 从数列中挑出一个基准值。 将所有比基准值小的摆放在基准前面，所有比基准值大的摆在基准的后面(相同的数可以到任一边)；在这个分区退出之后，该基准就处于数列的中间位置。 递归地把”基准值前面的子数列”和”基准值后面的子数列”进行排序。 分析：快速排序的时间复杂度在最坏情况下是O($N^2$)，平均的时间复杂度是O(N*logN)，采用的是分治的思想，二叉树的结构。 下面以数列 a={30,40,60,10,20,50} 为例，演示它的快速排序过程(如下图)。 这个是经过一个迭代的结果，每一个迭代，都排好了基准数字的位置。按照同样的方法，对子数列进行递归遍历。最后得到有序数组！ 在实现的过程中，key 值的选择 while 的遍历顺序是相关的。如果key =arr[left] 那么第一个while 循环是从right 开始的（从后往前）；如果key =arr[right] 那么是从left 进行遍历的。 12345678910111213141516171819202122232425def quick_sort(lists, left, right): if left &gt;= right: return lists key =lists[left] low =left high =right while left &lt; right: # 因为你最初key 取得是left，然后从右边找到一个比key小的，然后替换left 的位置 while left &lt;right and lists[right]&gt;=key: right -= 1 lists[left] =lists[right] while left &lt; right and lists[left] &lt;= key: left +=1 lists[right] =lists[left] lists[left] =key # 这里的left 和right 都是可以的，因为从while 中出来之后两者是相同的 # 这个步伐是1,所以只能是一个个变化 quick_sort(lists, low, left-1) quick_sort(lists, left+1, high) # 因为left的位置已经被占了，所以只是划分左边一块，右边一块就是可以的 return lists# testlists =[3,2,45, 100,1,56,56]#lists =[1,2,3,2,2,2,5,4,2]print(quick_sort(lists, 0, len(lists)-1)) 给出两种 C ++ 的代码，发现有两点。 分开写之后，可以发现和 下面那个算法题目的关系： 这个基准点就是那个 Kth 的一个参考 关于while 条件的分析， 如果是i &lt;=j 那么返回的是j， 如果是 i&lt;j 那么返回的i 作为基准点 12345678910111213141516171819202122232425262728293031323334void QuickSort(int array[], int start, int last)&#123;int i = start;int j = last;int temp = array[i];if (i &lt; j)&#123;while (i &lt; j)&#123;//while (i &lt; j &amp;&amp; array[j]&gt;=temp )j--;if (i &lt; j)&#123;array[i] = array[j];i++;&#125;while (i &lt; j &amp;&amp; temp &gt; array[i])i++;if (i &lt; j)&#123;array[j] = array[i];j--;&#125;&#125;//把基准数放到i位置array[i] = temp;//递归方法QuickSort(array, start, i - 1);QuickSort(array, i + 1, last);&#125;&#125; 第二种写法： 123456789101112131415161718192021222324252627282930313233int partition(int arr[], int left, int right) //找基准数 划分&#123;int i = left + 1 ;int j = right;int temp = arr[left];while(i &lt;= j)&#123;while (arr[i] &lt; temp)&#123;i++;&#125;while (arr[j] &gt; temp )&#123;j--;&#125;if (i &lt; j)swap(arr[i++], arr[j--]);else i++;&#125;swap(arr[j], arr[left]);return j;&#125;void quick_sort(int arr[], int left, int right) &#123;if (left &gt; right)return;int j = partition(arr, left, right);quick_sort(arr, left, j - 1);quick_sort(arr, j + 1, right);&#125; Kth Largest Element in an Array Find the kth largest element in an unsorted array. Note that it is the kth largest element in the sorted order, not the kth distinct element. Tips: 使用的是 快排的思想，最后的平均时间复杂度是O(N) ，所以是一个不错的算法。 kth 和 ”快排“ 实现的时候稍微有一些区别，前者只有发现 一组数据（一个比pivot 大，一个比 pivot 小）才进行交换，后者是是 两个while， 只要发现一个就进行交换。这个是细微的差别。求解K 最大，那么数组适合的是降序，求解K 最小，那么数组适合升序。关键这个求解的是 K 最大或者最小，那么是不需要整个数组是有序的，而是找出某个数字是，即可。 123456789101112131415161718192021222324252627282930def partition(arr, left, right): key =arr[right] low, high =left, right while left&lt; right: while left &lt;right and arr[left]&gt;=key: left +=1 arr[right] =arr[left] while left&lt;right and arr[right] &lt;=key: right -=1 arr[left] =arr[right] arr[right] =key return leftdef findKthLargest(arr, k): left, right =0, len(arr)-1 while True: pos =partition(arr, left, right) if pos ==k-1: return arr[pos] elif pos &gt; k-1: right =pos -1 else: left = pos+1 将上面的想法和到一起。这两道题目是比较有意思的， 很好哈。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263#include&lt;iostream&gt;#include&lt;vector&gt;using namespace std;int QuickSort(vector&lt;int&gt; &amp;nums, int left, int right, int k) &#123; if (left == right) &#123; return nums[left]; &#125; if(left&lt;right)&#123; //快排思想 从大至小排序 int i=left; int j=right; int key=nums[left]; while(i&lt;j)&#123; while(i&lt;j&amp;&amp;nums[j]&lt;key) j--; if(i&lt;j)&#123; nums[i]=nums[j]; i++; &#125; while(i&lt;j&amp;&amp;nums[i]&gt;=key) i++; if(i&lt;j)&#123; nums[j]=nums[i]; j--; &#125; &#125; //i=j退出循环 nums[i]=key; //进行基准的新下标与k的比较 if(i&gt;k) return QuickSort(nums,left,i-1,k); else if(i&lt;k) return QuickSort(nums,i+1,right,k); else return nums[i]; &#125;&#125;int kthLargestElement(int k, vector&lt;int&gt; nums)&#123; //第k大 下标k-1 return QuickSort(nums, 0, nums.size() - 1, k-1);&#125;int main()&#123; vector&lt;int&gt; arr; int n, k; cin &gt;&gt;k ; cin &gt;&gt; n; for(int i =0;i &lt;n;i++) cin &gt;&gt; arr[i]; int num =kthLargestElement(k, arr); cout &lt;&lt; num&lt;&lt;endl; return 0;&#125; 堆排序参看另外一篇博客 参考文献算法动图效果排序算法分类]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>排序算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[那些年的算法题目（一）]]></title>
    <url>%2F2018%2F06%2F22%2F%E9%82%A3%E4%BA%9B%E5%B9%B4%E7%9A%84%E7%AE%97%E6%B3%95%E9%A2%98%E7%9B%AE%EF%BC%88%E4%B8%80%EF%BC%89%2F</url>
    <content type="text"><![CDATA[完全二叉树插入问题描述已知一个完全二叉树的结构，现在需要将一个节点插入到这颗完全二叉树的最后，使得它还是一个完全二叉树。第一种解法：如果该树为满二叉树或者左子树不为满二叉树，那么就进入左子树，否则进入右子树，递归进行。 二叉树(Binary Tree) 二叉树 任何一个节点的子节点数量不超过2 完全二叉树 所有叶子结点都在最后一层或倒数第二层。 最后一层的叶子结点在左边连续，倒数第二节的叶子结点在右侧连续。 满二叉树 所有叶子结点都在最后一层 结点的总数为 $2^n -1$ (n 为树的高度) 满二叉树是一种特殊的完全二叉树 平衡二叉树 也叫 AVL 树 它是一颗空树或左右两个子树的高度差的绝对值不超过1。 左右两个子树均为平衡二叉树。 二叉搜索树（Binary Search Tree） 也叫二叉查找树、二叉排序树 若子树不空，则子树上所有节点的值均小于或等于根节点的值。 若右子树不空，则右子树所有节点的值均大于或等于根节点的值。 左、右子树也分别为二叉排序树，或是一颗空树。 哈夫曼树 带权路径长度达到最小的二叉树，也叫做最优二叉树。 树的深度和高度：深度是从上往下数；高度是从下往上数 代码实现平滑过渡到本问题的代码实现。树的高度就是树的遍历。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253#include&lt;iostream&gt;using namespace std;typedef struct Node&#123; int value; struct Node *lchild, *rchild;&#125;Tree;int GetLeftDepth(Tree* root)&#123; Tree* pNode =root-&gt;lchild ; int depth =0; while(pNode != NULL) &#123; depth ++; pNode =pNode-&gt;lchild; &#125; return depth;&#125;int GetRightDepth(Tree* root)&#123; Tree* pNode =root-&gt;rchild; int depth =0 ; while(pNode != NULL) &#123; depth ++ ; pNode =pNode-&gt;rchild ; &#125; return depth;&#125;bool IsFullBinaryTree(Tree* root)&#123; return GetLeftDepth(root) == GetRightDepth(root) ;&#125;void insert(Tree* root, Tree * node)&#123; if (IsFullBinaryTree(root) || !IsFullBinaryTree(root-&gt;lchild))&#123; insert(root-&gt;lchild, node); return ; &#125; if (root-&gt;rchild ==NULL)&#123; root-&gt;rchild =node ; return ; &#125; insert(root-&gt;rchild, node) ;&#125;int main()&#123; Node* a = new Node(); a-&gt;value =1;&#125; inplace 去除连续的 0给定一个一维整数数组，不使用额外的空间，本地去掉数组中连续的0。 Tips： 前后两个指针，判断是否是连续的0。第三个指针标记新的数组，前者覆盖后者。 12345678910111213141516171819202122232425262728293031#include&lt;iostream&gt;using namespace std;int RemoveDuplicates(int* sortBuffer,int length)&#123; if(sortBuffer == NULL || length == 0) &#123; return false; &#125; int count = 0; for(int i = 1; i &lt; length; i++) &#123; if(sortBuffer[i] ==0 &amp;&amp; 0 == sortBuffer[i-1]) &#123; continue; &#125; else &#123; sortBuffer[count]=sortBuffer[i]; count++; &#125; &#125; return count; &#125;int main()&#123; int length =sizeof(array)/sizeof(int); &#125; 最大连续子数组和 ??已知一个整数二维数组，求最大的子数组和(子数组的定义从左上角(x0,y0) 到右下角(x1,y1)的数组)先考虑一维整数数组的情况。 12345678910111213141516171819#include&lt;iostream&gt;using namespace std;int Max(int a, int b)&#123; return a&gt;b ?a:b;&#125;int FindGreatestSubarray(int *arr, int n)&#123; int sum =arr[0]; int max =arr[0]; for(int i =1; i&lt;n; i++)&#123; sum =Max(sum+arr[i], arr[i]); max =Max(sum, max) &#125; return max;&#125;int main()&#123; return 0;&#125; 本题目的要求是从二位的数组中求解最大的子矩阵。我们可以将其转化成一维数组的问题。如果是二维数组可以压缩为一维数组（我当时也是不懂这里）。如果最大子矩阵和原矩阵等高，就可以这样压缩。 不是很懂，感觉K 的值应该是具有某种限制，但是这个仍然是 0-k 这样的数字。 12345678910111213141516171819202122232425262728293031323334353637383940414243#include&lt;stdio.h&gt;#include&lt;iostream&gt;#include&lt;vector&gt;using namespace std;#define inf 0x3f3f3f3fint Max(int a, int b)&#123; return a&gt;b? a:b;&#125;// 求解一维数组的最大连续子数列int FindGreatestSubarray(int *arr, int n)&#123; int sum =arr[0]; int max =arr[0]; for(int i =1;i&lt;n;i++)&#123; sum =Max(sum+arr[i], arr[i]) if(sum &gt;=max)&#123; max =sum; &#125; &#125; return max;&#125;int GreatestMatrix(int[][] arr, int rows, int cols)&#123; int maxVal =- inf for(int i =0 ; i &lt;rows; i++)&#123; vector&lt;int&gt; temp(arr[i]); maxVal =Max(maxVal, FindGreatestSubarray(temp)); // 得到第一行的最大和 // 将行的n个元素加到上一行，然后计算最大和 for(int j =i+1; j&lt;rows; j++)&#123; for(int k =0;k&lt;cols ;k++)&#123; temp[k] =arr[j][k]; &#125; // 依次0~k行的最大和 maxVal =Max(maxVal, FindGreatestSubarray(temp)) &#125; &#125;&#125;int main()&#123;&#125; 堆排序堆是一棵顺序存储的完全二叉树，堆排序是一种树形选择排序，其时间复杂度为O(nlogn)，空间复杂度:对于记录较少的文件不推荐使用，对于较大的文件还是有效的.堆分为大根堆和小根堆。大根堆的要求是每个节点的值都不大于其父节点的值，即A[PARENT[i]] &gt;= A[i]。小根堆的要求是每个节点的值都不小于其父节点的值，即A[PARENT[i]] &lt;= A[i]。 堆的每次调整交换堆顶和最后一个元素，然后只是调整堆顶和堆顶的左右孩子树的关系。有建立堆，调整堆和堆排序三个步骤。 这个是讲解视频 1234567891011121314151617181920212223242526272829303132333435# heap modifydef MAX_Heapify(heap, HeapSize, root): left =2* root+1 right = left +1 larger =root if left &lt;HeapSize and heap[larger] &lt;heap[left]: larger =left if right &lt; HeapSize and heap[larger] &lt;heap[right]: larger =right # if modify the larger then exchange it if larger != root: heap[larger], heap[root] =heap[root], heap[larger] MAX_Heapify(heap, HeapSize, larger)# Build the heapdef Build_MAX_Heap(heap): HeapSize =len(heap) # from the end to the begin for i in range((HeapSize -2)//2, -1,-1): MAX_Heapify(heap, HeapSize, i)# sort after building the heapdef HeapSort(heap): Build_MAX_Heap(heap) for i in range(len(heap)-1, -1, -1): heap[0], heap[i] =heap[i], heap[0] MAX_Heapify(heap, i, 0) return heapif __name__ =="__main__": a =[30, 50, 57, 77, 62, 78, 94, 80, 84] print(a) print("without sort but with build heap") Build_MAX_Heap(a) print(a) 补充： stack 栈， FILO，先进后出，栈溢出都是它 heap，大根堆、小根堆，常用在大量的数据进行排序，是一种树形结构 queue 队列，先进先出 KMP（字符串高效查找）**字符串匹配是计算机的基本任务之一。假设两个字符串的长度分别是S和P 的长度分别是m, n(m &gt;n), 那么在字符串S 中查找P，时间复杂度是 O( $m \times n $)。Knuth-Morris-Pratt算法（简称KMP）是最常用的之一。KMP通过一个O(n)的预处理，可以使得时间复杂度降为O(n+m). 所以说 KMP 是一种非常高效的字符串匹配技术。 思想：当字符串S 和字符串P 不匹配的时候，是要利用已知的前面两者匹配的个数的信息，不要把“搜索位置”移回已经比较过的位置，进行一下跳转的移动，这样避免了重复的比较工作。 算法步骤： 假设现在文本串 S 匹配到 i 位置，模式串 P 匹配到 j 位置 如果 j = -1，或者当前字符匹配成功（即 S[i] == P[j]），都令 i++，j++，继续匹配下一个字符； 如果 j != -1，且当前字符匹配失败（即 S[i] != P[j]），则令 i 不变，j = next[j]。此举意味着失配时，模式串 P 相对于文本串 S 向右移动了 j - next [j] 位。换言之，当匹配失败时，模式串向右移动的位数为：失配字符所在位置 - 失配字符对应的 next 值（next 数组的求解会在下文的 3.3.3 节中详细阐述），即移动的实际位数为：j - next[j]，且此值大于等于1。 此也意味着在某个字符失配时，该字符对应的 next 值会告诉你下一步匹配中，模式串应该跳到哪个位置（跳到next [j] 的位置）。如果 next [j] 等于 0 或 -1，则跳到模式串的开头字符，若 next [j] = k 且 k &gt; 0，代表下次匹配跳到 j 之前的某个字符，而不是跳到开头，且具体跳过了 k 个字符。 计算部分匹配表 前缀和后缀的定义：”前缀”指除了最后一个字符以外，一个字符串的全部头部组合；”后缀”指除了第一个字符以外，一个字符串的全部尾部组合。 如果给定的模式串是：“ABCDABD”，从左至右遍历整个模式串，其各个子串的前缀后缀分别如下表格所示： 123456789101112131415161718192021222324def kmp_match(s, p): m, n =len(s) ,len(p) cur =0 table = partial_table(p) while cur &lt;= m-n: for i in range(n): if s[i+cur] != p[i]: cur += max(i -table[i-1], 1) break else: return True return Falsedef partial_table(p): prefix =set() postfix =set() ret =[0] for i in range(1, len(p)): prefix.add(p[:i]) postfix =&#123; p[j:i+1] for j in range(1, i+1)&#125; ret.append(len((prefix &amp; postfix or &#123;''&#125;).pop())) # &amp;两个set求交集 return retprint(partial_table('ABCDABD'))print(kmp_match("BBC ABCDAB ABCDABCDABDE", "ABCDABD")) 参考资料： http://wiki.jikexueyuan.com/project/kmp-algorithm/define.html http://www.ruanyifeng.com/blog/2013/05/Knuth%E2%80%93Morris%E2%80%93Pratt_algorithm.html 二叉树的遍历在python中二叉树的结构: 12345class BinNode(): def __init__(self, val): self.value =val self.lchild =None self.rchild =None 先序遍历(preOrder)第一种思路是递归实现，第二种思路借助栈的结构来实现。栈的大小空间为O(h)，h为二叉树高度；时间复杂度为O(n)，n是树的节点的个数。 递归的写法便于理解，循环的方式内存比较省。 循环的版本从变量命名和结构上都是可以优化的。 123456789101112131415161718192021222324# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution(object): def preorderTraversal(self, root): """ :type root: TreeNode :rtype: List[int] """ res, stack =[], [root] while stack: cur_node =stack.pop() # default pop(-1) 这个时间复杂度是 O(1)，如果是pop(0) 那么时间复杂度是 O(n) if cur_node: res.append(cur_node.val) stack.append(cur_node.right) # 注意这个顺序，这个是正确的姿态 stack.append(cur_node.left) return res 中序遍历（inorder）递归和非递归两种实现思路。入栈的顺序是一样的，只是改变的遍历(print())的顺序. 中序遍历是先把所有的左子树遍历完之后，然后遍历根节点，然后遍历右子树。 12345678910111213141516171819202122# 递归def inOrder(self, root): if root ==None: return self.inOrder(root.lchild) print(root.val) self.inOrder(root.rchild)# 借助栈结构class Solution: def inorderTraversal(self, root): res, stack =[], [] while True: while root: stack.append(root) root =root.left if not stack: return res node =stack.pop() res.append(node.val) root =node.right 后序遍历(post order)仍然是递归和非递归版本，非递归中使用两个stack,两个stack的后进先出等于一个先进先出。 12345678910111213141516171819202122232425262728293031# 递归def postOrder(self, root): if root == None: return self.postOrder(root.lchild) self.postOrder(root.rchild) print(root.val)# 借助栈结构# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution(object): def postorderTraversal(self, root): """ :type root: TreeNode :rtype: List[int] """ res, stack =[], [root] while stack: cur_node =stack.pop() if cur_node: res.append(cur_node.val) stack.append(cur_node.left) # 注意和先序遍历的顺序，还有最后的 reverse 操作 stack.append(cur_node.right) return res[::-1] 层序遍历使用到了队列的思想，先进先出。实际上，用的是Python中list.pop(0).注意默认是list.pop(-1),也就是默认弹出的是最后一个元素。 层序遍历使用 while 循环就比较好理解 1234567891011121314def levelOrder(self, root): if root ==None: return myQueue =[] node =root myQueue.append(node) while myQueue: # remove and return item at index (default last) node =myQueue.pop(0) print(node.val) if node.lchild != None: myQueue.append(node.lchild) if node.rchild != None: myQueue.append(node.rchild) 旋转数组找最小值把一个数组最开始的若干个元素搬到数组的末尾，我们称之为数组的旋转。 输入一个非递减排序的数组的一个旋转，输出旋转数组的最小元素。 例如数组{3,4,5,1,2}为{1,2,3,4,5}的一个旋转，该数组的最小值为1。 NOTE：给出的所有元素都大于0，若数组大小为0，请返回0。 暴力求解因为原来的数组假设是增序，所以如果出现了的某一个元素比上一个元素小，该元素就是这个序列中的最小值。(这个情况具有唯一性吧).时间复杂度O(N)。这个代码就省略了，实际上跟数组的遍历是没有什么区别的。 递归版本对于有序数组 or 部分有序的数组的遍历，二分查找是必然的选择。然后时间复杂度是O(logN) 123456789101112def minNumberInRotateArray(arr): if not arr or len(arr) ==0: return 0 if len(arr) ==2: # 最后肯定是不增的数组 return arr[1] mid =int(len(arr) /2) if arr[mid] &gt; arr[0]: return minNumberInRotateArray(arr[mid:]) elif arr[mid] &lt;arr[0]: return minNumberInRotateArray(arr[:mid+1]) else: return minNumberInRotateArray(arr[1:]) # 防止出现不减的情况 非递归版本递归版本占的内存比较多，改进吧..于是非递归的版本就出来了。需要注意的是该版本的判断比较条件(其中一点是和 arr[right]进行比较)一定要小心，都是小坑…这种解法关键是需要找到非减序列（和原序列相同的形式），然后就变得可预测，可以排除这部分其他的数字。 下面这个确实是是正确的代码，注意体会细节。非递减的array，然后是从后往前比较的。if else中的条件是可以调换的，虽然这话听起来像是废话。这说明两个条件的判断的顺序不应该产生不同的结果。 1234567891011121314def minNumberInRotateArray(rotateArray): arr =rotateArray left =0 right =len(arr) -1 while left &lt; right: mid = int((left+ right)/2) if arr[mid] &gt;arr[right]: left =mid+1 # 不包含mid 因为mid 绝不可能是 最小值 elif arr[mid] &lt;arr[right]: right =mid # 包含mid 因为mid 可能是最小值 else: right -=1 # 这个也是可以换成 left +=1 ，只要是能够渐进的 return arr[left] 文章的小标题是求解最小(大)值，上述讲述的都是最小值。如果求解最大值，稍微修改一下特殊情况的判断条件，将返回的index-1 即可。因为最小值的位置是”某一个元素比上一个元素小”，那么 index-1 之后这个元素就是该数组序列中最大的。 单链表反转单链表的反转有循环迭代和递归两种方法。单链表节点 链表这种数据结构，一个是值一个是指针。 123456class Node(object): def __init__(self): self.value =None self.next =None def __str__(self): return str(self.value) 循环迭代循环迭代需要维持三个变量：pre, head, next。pre是head的pre，next是head的next.(废话) 虽然说操作的是三个变量，但是从实际的效果上讲，操作的是一个指针。所以这种思想很重要。重写代码，换成 pre, cur, nex 三个指针， pre 和nex 都是为cur 服务的。 1234567891011def reverse_Linkedlist(head): if not head or not head.next : #空指针或者只有一个结点 return head pre =None # 需要创建一个None 作为最后的指向 while head: next = head.next head.next =pre pre = head head =next # 最后一次循环迭代 Head==None，而pre指向了头结点 return pre 递归一开始正常情况下不会执行if判断，利用递归走到链表的末端，new_head的值没有发生改变，为链表的最后一个节点，反转之后就成为了新链表的head。 这个递归就没有 循环好理解，因为这个是线性的结构吗？ 12345678def reverse_Linkedlist(head): if not head or not head.next: return head new_head = reverse_Linkedlist(head.next) # 将当前节点设置为后面节点的后续节点 head.next.next =head head.next =None return new_head 测试12345678910111213141516171819202122232425if __name__ == "__main__": three = Node() three.value =3 two =Node() two.value =2 two.next =three one =Node() one.value =1 one.next =two head =Node() head.value =0 head.next =one """ while head: print(head.value, ) head =head.next print("******") """ newhead = reverse_Linkedlist(head) while newhead: print(newhead.value) newhead =newhead.next Minimum Window Substring Given a string S and a string T, find the minimum window in S which will contain all the characters in T in complexity O(n). 12Input: S = &quot;ADOBECODEBANC&quot;, T = &quot;ABC&quot;Output: &quot;BANC&quot; Tips： 类似文章的摘要 双指针 left,right 先是从目标字符串的长度上满足要求，在这个基础上不断的去压缩 left, right 指针 123456789101112131415161718192021222324from collections import Counterclass Solution(object): def minWindow(self, s, t): """ :type s: str :type t: str :rtype: str """ need, missing = Counter(t), len(t) i = start = end = 0 for j, c in enumerate(s, 1): missing -= need[c] &gt; 0 need[c] -= 1 if not missing: while need[s[i]] &lt; 0: need[s[i]] += 1 i += 1 if end ==0 or j - i &lt;= end - start: start, end = i, j need[s[i]] += 1; i += 1; missing += 1 return s[start : end]]]></content>
      <categories>
        <category>算法</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Introduction to Ensemble]]></title>
    <url>%2F2018%2F06%2F13%2FIntroduction-to-Ensemble%2F</url>
    <content type="text"><![CDATA[虽然在Titanic Challenge博客中介绍了模型融合(Ensemble)，但是其是以特征提取为主旋律，所以这篇博客将结合实战从最基本的Ensemble到比较复杂的Ensemble,最后安利一个用于模型融合的package. 在文中可能出现一些概念，如果不太熟悉，可以先去文末找找看，如果我没有补充，那么请自行Google吧。为了不影响阅读的流畅性，我可能在其中不会涉及很多概念解释，比如说ROC曲线和AUC值。好，我们进入正文。 Dataset本文书写过程中沿用参考博客(Introduction to Python Ensembles)的数据集。可以去这里下载，当然推荐使用原作者处理之后的数据集，you can find here。 简单介绍一下这个数据集：Federal Election Commission这个组织收集了2007到2016年的donations记录，最后得出 When Scientists Donate To Politicians, It’s Usually To Democrats这样的结论。 好了，我不想在数据集这里花太多时间，即使你不太明白数据集的具体含义，完全不影响下文的阅读，因为你很快就会发现下文并没有进行很多和原数据集相关的内容，更多的是模型融合。当然你如果能够看懂，可以感受一下上述结论的有趣之处。 Give me codes:1234567891011121314151617181920import numpy as npimport pandas as pdimport matplotlib.pyplot as plt%matplotlib inline# import data# Always good to set a seed for reproducibilitySEED = 222np.random.seed(SEED)df = pd.read_csv(&apos;input.csv&apos;)from sklearn.model_selection import train_test_splitfrom sklearn.metrics import roc_auc_scoredef get_train_test(test_size= 0.95): y =1*(df.cand_pty_affiliation ==&apos;REP&apos;) X = df.drop([&quot;cand_pty_affiliation&quot;], axis=1) X = pd.get_dummies(X, sparse=True) X.drop(X.columns[X.std() == 0], axis=1, inplace=True) return train_test_split(X, y, test_size=test_size, random_state=SEED)xtrain, xtest, ytrain, ytest = get_train_test()df.head() 简单看一下数据长什么样子，虽然有人可能不太懂。 Begin with ensemble之前的博客主要从ensemble分类的角度阐述，现在从概念的角度阐述。Ensemble: combining predictions from several models averages out idiosyncratic(怪异的) errors and yield better overall predictions.(有时候我觉得英文说得很清楚，所以就不翻译成中文了，求不被打。) 简单的说，就是可以防止过拟合。当过拟合时，边界曲线就回去过分考虑某一个或某一些少数的点，这时候 ensemble通过某种combine机制，然后得到一个泛化性能比较好的边界曲线，也就是比较好的模型。 Decision Tree首先我们从 decision tree开始。A decision tree, which is a tree of if-then rules. The deeper the tree, the more complex the patterns it can capture, but the more prone to overfitting it will be. Because of this, we will need an alternative way of building complex models of decision trees, and an ensemble of different decision trees is one such way.我们先使用 depth =112345678from IPython.display import Imagefrom sklearn.metrics import roc_auc_scorefrom sklearn.tree import DecisionTreeClassifiert1 = DecisionTreeClassifier(max_depth=1, random_state=SEED)t1.fit(xtrain, ytrain)p = t1.predict_proba(xtest)[:, 1]print(&quot;Decision tree ROC-AUC score: %.3f&quot; % roc_auc_score(ytest, p)) Decision tree ROC-AUC score: 0.672 发现结果不太理想，加深depth.1234t2 =DecisionTreeClassifier(max_depth=3, random_state=SEED)t2.fit(xtrain, ytrain)p =t2.predict_proba(xtest)[:, 1]print(&quot;Decision tree ROC-AUC score: %.3f&quot; % roc_auc_score(ytest, p)) Decision tree ROC-AUC score: 0.751 由于我们最后要ensemble，而这要求我们要构造有差异但每个不是那么差的模型。首先我们考虑到使用不同的数据集。1234567xtrain_slim =xtrain.drop([&apos;transaction_amt&apos;], axis=1)xtest_slim =xtest.drop([&apos;transaction_amt&apos;], axis=1)t3=DecisionTreeClassifier(max_depth=3, random_state=SEED)t3.fit(xtrain_slim, ytrain)p =t3.predict_proba(xtest_slim)[:,1]print(&apos;Decision tree ROC-AUC score:&#123;&#125;&apos;.format(roc_auc_score(ytest, p))) Decision tree ROC-AUC score:0.7403182587884118通过corr()来检验两者的相关性(差异性)123p1 =t2.predict_proba(xtest)[:,1]p2 =t3.predict_proba(xtest_slim)[:,1]pd.DataFrame(&#123;&apos;full_data&apos;:p1, &apos;red_data&apos;:p2&#125;).corr() 发现有一定的相关性，但是还是可以容忍的。于是开始融合。1234p1 = t2.predict_proba(xtest)[:, 1]p2 = t3.predict_proba(xtest_slim)[:, 1]p = np.mean([p1, p2], axis=0)print(&quot;Average of decision tree ROC-AUC score: %.3f&quot; % roc_auc_score(ytest, p)) Average of decision tree ROC-AUC score: 0.783我们发现：两个旗鼓相当（0.74 0.73）的可以得到一个更好的结果，可以减少决策失误 的平均是0.78。 需要注意的是我们在构造第二个模型时候通过drop()丢掉一个feature，反而得到一个更好的模型。于是乎，我们想通过使用不同的子集（不同的特征）构造不同的模型，是不是能得到更好的模型？ Random Forest(Bagging)A fast approach that works well in practice is to randomly select a subset of features, fit one decision tree on each draw and average their predictions.This process is known as bootstrapped averaging (often abbreviated bagging), and when applied to decision trees, the resultant model is a Random Forest. (我觉得原作者比我说的清楚，借用了)我的理解，在上面小节中我们使用的是Decision Tree,在实际应用中发现有差异的多个子树的效果要更好一些。而实现这个途径快速的方法就是 Random Forest。Random 在这里表示任意几个子树(特征)，然后这些Tree组成了Forest。123456789from sklearn.ensemble import RandomForestClassifierrf =RandomForestClassifier( n_estimators=10, max_features= 3, random_state=SEED)rf.fit(xtrain, ytrain)p =rf.predict_proba(xtest)[:, 1]print(&apos;Average of decision tree ROC-AUC score:&#123;&#125;&apos;.format(roc_auc_score(ytest, p))) Average of decision tree ROC-AUC score:0.844018408542404这就是叫做”质的飞跃”从那个0.783-&gt; 0.844(将近6个百分点，好吧，有点神经质了…)From nobody to somebody, we are on something.. Ensemble of various models可以看出上述模型中，最后的模型（Random Forest）的子模型(Decision Tree)。但是子模型并不是局限树这一种结构，我们更多的选择：linear models, kernel-based models, non-parametric models, neural networks or even other ensembles! 为了避免代码的冗余构造了以下的helper function.123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657# A host of Scikit-learn modelsfrom sklearn.svm import SVC, LinearSVCfrom sklearn.naive_bayes import GaussianNBfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifierfrom sklearn.linear_model import LogisticRegressionfrom sklearn.neighbors import KNeighborsClassifierfrom sklearn.neural_network import MLPClassifierfrom sklearn.kernel_approximation import Nystroemfrom sklearn.kernel_approximation import RBFSamplerfrom sklearn.pipeline import make_pipelinedef get_models(): &quot;&quot;&quot;Generate a library of base learners.&quot;&quot;&quot; nb = GaussianNB() svc = SVC(C=100, probability=True) # C越大边界越复杂，会导致过拟合 knn = KNeighborsClassifier(n_neighbors=3) # KNN算法寻找训练数据中的K个最近的数据，它使用指向最多的那个类别来作为预测的输出。 lr = LogisticRegression(C=100, random_state=SEED) # 对于这个 c 能知道的就是正则化系数 smaller values specify stronger regularization. nn = MLPClassifier((80, 10), early_stopping=False, random_state=SEED) gb = GradientBoostingClassifier(n_estimators=100, random_state=SEED) # 子模型的数量，默认是100, gbc通常 robust to over-fitting, so a large number results in better performance rf = RandomForestClassifier(n_estimators=10, max_features=3, random_state=SEED) models = &#123;&apos;svm&apos;: svc, &apos;knn&apos;: knn, &apos;naive bayes&apos;: nb, &apos;mlp-nn&apos;: nn, &apos;random forest&apos;: rf, &apos;gbm&apos;: gb, &apos;logistic&apos;: lr, &#125; return models def train_predict(model_list): P =np.zeros((ytest.shape[0], len(model_list))) P =pd.DataFrame(P) print(&apos;Fitting models&apos;) cols =list() for i, (name, m) in enumerate(models.items()): print(&quot;%s...&quot; % name, end=&quot; &quot;, flush=False) m.fit(xtrain, ytrain) P.iloc[:, i] =m.predict_proba(xtest)[:, 1] cols.append(name) print(&apos;Done&apos;) P.columns =cols print(&apos;Done.\n&apos;) return P def score_models(P, y): &quot;&quot;&quot;Score model in predication DF&quot;&quot;&quot; print(&apos;Scoring models&apos;) for m in P.columns: score =roc_auc_score(y, P.loc[:, m]) print(&quot;%-26s: %.3f&quot; % (m, score)) print(&apos;Done, \n&apos;) let’s go…123models =get_models()P =train_predict(models)score_models(P, ytest) This is our base line.Gradient Boosting Machine(GBM) 果然名不虚传, does best我们来分析一下模型之间的相关性，原作者使用的mlens package(You can install it with: pip install mlens)，我这里用的是seaborn(install it with: pip install seaborn).在检查相关性(Pearson相关性:衡量两个数据集合的线性相关性)时候，我们使用的是经过处理的相关性。具体说来可以称之为 error correlation,详细见代码。 12345678# look at error correlations is more promising, errors are significantly correlated import seaborn as snsplt.subplots(figsize=(10,8)) corrmat = P.apply(lambda pred: 1*(pred &gt;= 0.5) - ytest.values).corr()sns.set(font_scale=1.5) hm = sns.heatmap(corrmat, cbar=True, annot=True, square=True, fmt=&apos;.2f&apos;, annot_kws=&#123;&apos;size&apos;: 10&#125;) plt.title(&apos;Pearson Correlation of Features&apos;, y=1.05, size=15) plt.show() 预测值和真实值之间的差异称之为error，查看error的 Pearson correlation，效果更加明显。1print(&quot;Ensemble ROC-AUC score: %.3f&quot; % roc_auc_score(ytest, P.mean(axis=1))) Ensemble ROC-AUC score: 0.884结果高于每一个单独的模型，但是不是那么明显。 Visualize Curve ROC(helper function)我们注意到之前使用的所有的结果的评价标准都是 roc_auc_score，但是并没有提及这是什么。当然在正文中也不打算解释，如果不是很清楚，可以查看本文最后补充概念.简单来说AUC可以用来衡量”二分问题”的泛化能力，是一种评价指标。我们这里想说的是 visualize Curve ROC,可视化。12345678910111213141516171819202122# a helper function for roc_curvefrom sklearn.metrics import roc_curvedef plot_roc_curve(ytest, P_base_learners, P_ensemble, labels, ens_label): &quot;&quot;&quot;Plot the roc curve for base learners and ensemble.&quot;&quot;&quot; plt.figure(figsize=(10, 8)) plt.plot([0, 1], [0, 1], &apos;k--&apos;) cm = [plt.cm.rainbow(i) for i in np.linspace(0, 1.0, P_base_learners.shape[1] + 1)] for i in range(P_base_learners.shape[1]): p = P_base_learners[:, i] fpr, tpr, _ = roc_curve(ytest, p) plt.plot(fpr, tpr, label=labels[i], c=cm[i + 1]) fpr, tpr, _ = roc_curve(ytest, P_ensemble) plt.plot(fpr, tpr, label=ens_label, c=cm[0]) plt.xlabel(&apos;False positive rate&apos;) plt.ylabel(&apos;True positive rate&apos;) plt.title(&apos;ROC curve&apos;) plt.legend(frameon=False) plt.show()plot_roc_curve(ytest, P.values, P.mean(axis=1), list(P.columns), &quot;ensemble&quot;) 我们顺便把刚才的模型可视化，发现ensemble的AUC是最大的，意味这泛化性能是最好，这也是符合我们的认知的 。 Beyond ensembles as a simple average我们回到主线上，在上一个模型中我们提及最后的 ensemble的结果是好于每个单独的模型，但是没有那么突出。根据ROC曲线我们也可以看出，有的模型(KNN)在这里表现的没有那么好，我们在想是不是由于因为这个而拉底了最后的结果，当然这只是猜测，于是我们找到了 try的方向。可能第一直觉去掉这个模型再进行融合，在这个实验表明该策略最后的结果0.883，相比与0.884，你懂得，并没有变好。我们还有一种策略:learn a sensible set of weights to use when averaging predictions.让模型自己去学习如何调整各个模型之间的比例。 Learning to combine predications为了让模型自学习各个之间的预测比例，我们引入了 meta learner(meta是元，理解为最基础的) to learn how to best combine these predictions. 除此之外，我们将使用不同的数据集，像Random Forest使用不同的数据子集(不同的特征组成的数据集)。于是我们需要a method for splitting the training data between the base learners and the meta learner.12345678910base_learners =get_models()meta_learner = GradientBoostingClassifier( n_estimators=1000, loss=&quot;exponential&quot;, max_features=4, max_depth=3, subsample=0.5, learning_rate=0.005, random_state=SEED) 使用最强模型GBM作为 meta learner并定义好 base_learners.To keep things simple, we split the full training set into a training and prediction set of the base learners. This method is sometimes referred to as Blending. Unfortunately, the terminology differs between communities, so it’s not always easy to know what type of cross-validation the ensemble is using.12345678910# sefine a procedure for generating train and test setsxtrain_base , xpred_base, ytrain_base, ypred_base =train_test_split(xtrain, ytrain, test_size=0.5, random_state=SEED)def train_base_learners(base_learners, inp, out, verbose =True): if verbose: print(&apos;Fitting models&apos;) for i, (name, m) in enumerate(base_learners.items()): if verbose: print(&quot;%s...&quot; % name, end=&quot; &quot;, flush=False) m.fit(inp, out) if verbose: print(&apos;Done.&apos;)train_base_learners(base_learners, xtrain_base, ytrain_base) (注意我们只是使用了50%的data去train, test_size =0.5)1234567891011121314def predict_base_learners(pred_base_learners, inp, verbose=True): &quot;&quot;&quot;Generate a prediction matrix.&quot;&quot;&quot; P = np.zeros((inp.shape[0], len(pred_base_learners))) if verbose: print(&quot;Generating base learner predictions.&quot;) for i, (name, m) in enumerate(pred_base_learners.items()): if verbose: print(&quot;%s...&quot; % name, end=&quot; &quot;, flush=False) p = m.predict_proba(inp) # With two classes, need only predictions for one class P[:, i] = p[:, 1] if verbose: print(&quot;done&quot;) return PP_base = predict_base_learners(base_learners, xpred_base) 现在我们得到了base_learners的predications，接下来我们应该使用的是这个流程，在base learners的基础上（类似两层结构了 meta learner 学习的如何搭配这些base learner使得最后的结果 predications最大）进行训练，而不是训练原来的数据集。 1meta_learner.fit(P_base, ypred_base) 1234567#meta_learner.fit(P_base, ypred_base)def ensemble_predict(base_learners, meta_learner, inp, verbose=True): &quot;&quot;&quot;Generate predictions from the ensemble.&quot;&quot;&quot; P_pred = predict_base_learners(base_learners, inp, verbose=verbose) return P_pred, meta_learner.predict_proba(P_pred)[:, 1]P_pred, p = ensemble_predict(base_learners, meta_learner, xtest)print(&quot;\nEnsemble ROC-AUC score: %.3f&quot; % roc_auc_score(ytest, p)) 最后的结果是0.881 相比与之前最好的0.884（使用相同的数据集，没有进行 meta_learner的操作）这是因为我们在划分数据集的使用只是使用了0.5的数据集，而前者的模型使用了全部的train sets。有人不免疑问：为什么不使用全部的data？我的理解是划分xtrain_base , xpred_base, ytrain_base, ypred_base使用的是 train_test_split()，总是需要设定一个数值的，即使train_size =0.01，也是没有用到全部的datas. Training with cross-validation我们使用cross-validation 来缓解上面那个问题。During cross-validated training of the base learners, a copy of each base learner is fitted on K−1 folds, and predict the left-out fold. This process is iterated until every fold has been predicted. The more folds we specify, the less data is being left out in each training pass. This makes cross-validated predictions less noisy and a better reflection of performance during test time. The cost is significantly increased training time. Fitting an ensemble with cross-validation is often referred to as stacking, while the ensemble itself is known as the Super Learner. To understand how cross-validation works, we can think of it as an outer loop over our previous ensemble. The outer loop iterates over K distinct test folds, with the remaining data used for training. The inner loop trains the base learners and generate predictions for the held-out data. Here’s a simple stacking implementation:12345678910111213141516171819202122232425262728293031323334353637383940414243from sklearn.base import clonedef stacking(base_learners, meta_learner, X, y, generator): &quot;&quot;&quot;Simple training routine for stacking.&quot;&quot;&quot; # Train final base learners for test time print(&quot;Fitting final base learners...&quot;, end=&quot;&quot;) train_base_learners(base_learners, X, y, verbose=False) print(&quot;done&quot;) # Generate predictions for training meta learners # Outer loop: print(&quot;Generating cross-validated predictions...&quot;) cv_preds, cv_y = [], [] for i, (train_idx, test_idx) in enumerate(generator.split(X)): fold_xtrain, fold_ytrain = X[train_idx, :], y[train_idx] fold_xtest, fold_ytest = X[test_idx, :], y[test_idx] # Inner loop: step 4 and 5 fold_base_learners = &#123;name: clone(model) for name, model in base_learners.items()&#125; train_base_learners( fold_base_learners, fold_xtrain, fold_ytrain, verbose=False) fold_P_base = predict_base_learners( fold_base_learners, fold_xtest, verbose=False) cv_preds.append(fold_P_base) cv_y.append(fold_ytest) print(&quot;Fold %i done&quot; % (i + 1)) print(&quot;CV-predictions done&quot;) # Be careful to get rows in the right order cv_preds = np.vstack(cv_preds) cv_y = np.hstack(cv_y) # Train meta learner print(&quot;Fitting meta learner...&quot;, end=&quot;&quot;) meta_learner.fit(cv_preds, cv_y) print(&quot;done&quot;) return base_learners, meta_learner 尤其在cv_preds和cv_y的维度问题上，注意小心。The basic difference between blending and stacking is therefore that stacking allows both base learners and the meta learner to train on the full data set. Using 2-fold cross-validation, we can measure the difference this makes in our case:1234567from sklearn.model_selection import KFold# Train with stackingcv_base_learners, cv_meta_learner = stacking( get_models(), clone(meta_learner), xtrain.values, ytrain.values, KFold(2))P_pred, p = ensemble_predict(cv_base_learners, cv_meta_learner, xtest, verbose=False)print(&quot;\nEnsemble ROC-AUC score: %.3f&quot; % roc_auc_score(ytest, p)) Ensemble ROC-AUC score: 0.889这是目前为止最好的结果了。Stacking yields a sizeable increase in performance: in fact, it gives us our best score so far. This outcome is typical for small and medium-sized data sets, where the effect of blending can be severe. As the data set size increases, blending and stacking performs similarly. Use packages快要接近尾声了，在文章的开始，我们提及要安利一个resemble好用的package. So, it’s now.Here, we use Ensemble and build our previous generalized ensemble, but now using 10-fold cross-validation:123456789101112131415161718192021from mlens.ensemble import SuperLearner# Instantiate the ensemble with 10 foldssl = SuperLearner( folds=10, random_state=SEED, verbose=2, backend=&quot;multiprocessing&quot;)# Add the base learners and the meta learnersl.add(list(base_learners.values()), proba=True) sl.add_meta(meta_learner, proba=True)# Train the ensemblesl.fit(xtrain, ytrain)# Predict the test setp_sl = sl.predict_proba(xtest)print(&quot;\nSuper Learner ROC-AUC score: %.3f&quot; % roc_auc_score(ytest, p_sl[:, 1])) So simple!1plot_roc_curve(ytest, p.reshape(-1, 1), P.mean(axis=1), [&quot;Simple average&quot;], &quot;Super Learner&quot;) 发现super learner(meta learner)和 basic learner的mean的结果已经不相上下了。super learner得到了很好的训练。 ROC曲线和AUC值ROC全称是“受试者工作特征”（Receiver Operating Characteristic）。ROC曲线的面积就是AUC（Area Under the Curve）。AUC用于衡量“二分类问题”机器学习算法性能（泛化能力）。说到这里不得不提及就是经常使用的符号，TP(True Positive), FP(False, Positive), TN(True Negative),FN(False Negative)。他们是根据原来真实数据和预测类别进行的排列组合（当然这是针对二分问题）。 ROC 曲线: ROC 曲线（接收者操作特征曲线）是一种显示分类模型在所有分类阈值下的效果的图表。该曲线绘制了以下两个参数：真正例率 和 假正例率。 真正例率 (TPR) 是召回率的同义词，因此定义如下：$$T P R = \frac { T P } { T P + F N }$$假正例率 (FPR) 的定义如下：$$F P R = \frac { F P } { F P + T N }$$ ROC 中 TPR =(True positive / ( True positive +False negative)), 那个false negative 也是真实的类别，只不过是错误的当做了 negative（false negative） precision是 在所有预测为真的样本中，真实的情况也是真的比例；recall 是在所有的真实的样本中，预测也是真的比例。陈述这个关键把握住预测结果和真实结果两个维度。 2针对一个二分类问题，将实例分成正类(postive)或者负类(negative)。但是实际中分类时，会出现四种情况. 若一个实例是正类并且被预测为正类，即为真正类(True Postive TP) 若一个实例是正类，但是被预测成为负类，即为假负类(False Negative FN) 若一个实例是负类，但是被预测成为正类，即为假正类(False Postive FP) 若一个实例是负类，但是被预测成为负类，即为真负类(True Negative TN) ROC 曲线是如何绘制的: 采用不同分类阈值时的 TPR 与 FPR。降低分类阈值会导致将更多样本归为正类别，从而增加假正例（FP）和真正例（TP）的个数，可以理解为降低了被认为正确的标准，数量自然就增多了。下图显示了一个典型的 ROC 曲线。注意观察图中TPR 和 FPR是呈正相关的，验证了上述的结论。为了计算 ROC 曲线上的点，我们可以使用不同的分类阈值多次评估逻辑回归模型，但这样做效率非常低。幸运的是，有一种基于排序的高效算法可以为我们提供此类信息，这种算法称为曲线下面积。 ROC曲线，一般适用于你的分类器输出一个“概率值”，即这个样本属于某个类的概率是多少。 如此的话，你就需要设定一个阈值， 大于这个阈值属于正类，小于这个阈值属于负类。 从而，对于这个阈值P0， 就会得到对应的TPR, FPR, 也就是ROC曲线上的一个点，你设置不同的阈值，就会得到不同的TPR, FPR， 从而构成ROC曲线。 通常来说 阈值降低，即进入正类的门槛变低， TPR会变大，但是FPR也会变大， 看他们谁变的快。 1234567891011121314gbc = GradientBoostingClassifier()gbc.fit(x_train, y_train)resu = gbc.predict(x_test) #进行预测y_pred_gbc = gbc.predict_proba(x_test)[:,1] ###这玩意就是预测概率的fpr, tpr, threshold = roc_curve(y_test, y_pred_gbc) ###画图的时候要用预测的概率，而不是你的预测的值plt.plot(fpr, tpr, 'b', label='AUC = %0.2f' % rocauc)#生成ROC曲线plt.legend(loc='lower right')plt.plot([0, 1], [0, 1], 'r--')plt.xlim([0, 1])plt.ylim([0, 1])plt.ylabel('真正率')plt.xlabel('假正率')plt.show() 接着我们计算TRP(True Positive Radio)，FRP(False Positive Ratio)用于描述ROC曲线，分别表示该曲线的Y轴，X轴。TPR=TP/(TP+FN)FPR=FP/(FP+TN)最后就形成了类似这样的图像(来源于上述的训练模型) 我们希望的结果是TRU越大（接近1），FRU越小（接近0）。AUC的值是ROC所覆盖的面积，当AUC越大时候，分类器的效果越好。从图中可以看出模型(ensemble)的面积是最大的，分类效果也是最好的。关于该图像还有一点，如果你的曲线拟合对角线（图中虚线），那么相当于随机猜测。 ROC(receiver operating characteristic curve): ROC曲线的横坐标为false positive rate（FPR,假正率），纵坐标为true positive rate（TPR，真正率，召回率）PRC(precision recall curve): PRC曲线的横坐标为召回率Recall，纵坐标为准确率Precision。 用ROC curve来表示分类器的performance很直观好用。可是，人们总是希望能有一个数值来标志分类器的好坏。通常，AUC的值介于0.5到1.0之间，较大的AUC代表了较好的Performance。一般来说，如果ROC是光滑的，那么基本可以判断没有太大的overfitting（比如图中0.2到0.4可能就有问题，但是样本太少了），这个时候调模型可以只看AUC，面积越大一般认为模型越好。 一般情况，用不同的阀值，统计出一组不同阀值下的精确率和召回率，这就是PRC曲线。如下图：如果是做搜索，那就是保证召回的情况下提升准确率；如果做疾病监测、反垃圾，则是保准确率的条件下，提升召回。 两者的区别：在正负样本分布得极不均匀(highly skewed datasets)的情况下，PRC比ROC能更有效地反应分类器的好坏。 Recall：查全率，正样本中被预测出来是正的比例(越大越好)Precision：查准率，预测的正样本中被正确预测的比例(越大越好)True Positive Rate：跟 Recall 定义一样 （越大越好)FPR : 负样本中被预测为正的比例(越小越好) 对于一个二分类问题，往往要设定一个 threshold，当预测值大于这个 threshold 时预测为正样本，小于这个 threshold 时预测为负样本。如果以 Recall 为横轴，Precision 为纵轴，那么设定一个 threshold 时，便可在坐标轴上画出一个点，设定多个 threshold 则可以画出一条曲线，这条曲线便是 PR 曲线。 PR 曲线是以 Recall 为横轴，Precision 为纵轴；而 ROC曲线则是以 FPR 为横轴，TPR 为纵轴。 定理1：对于一个给定的的数据集，ROC空间和PR空间存在一一对应的关系，因为二者包含完全一致的混淆矩阵。我们可以将ROC曲线转化为PR曲线，反之亦然。 定理 2 中 “曲线A优于曲线B” 是指曲线 B 的所有部分与曲线 A 重合或在曲线 A 之下。而在ROC空间，ROC曲线越凸向左上方向效果越好。与ROC曲线左上凸不同的是，PR曲线是右上凸效果越好。 F1对于PRC就好象AUC对于ROC一样。一个数字比一条线更方便调模型。 这个是得到 阈值的一种方式： 接着来说ROC曲线（Receiver operating characteristic curve），ROC曲线其实是多个混淆矩阵的结果组合，如果在上述模型中我们没有定好阈值，而是将模型预测结果从高到低排序，将每个概率值依次作为阈值，那么就有多个混淆矩阵。 如精准营销领域的商品推荐模型，模型目的是尽量将商品推荐给感兴趣的用户，若用户对推荐的商品不感兴趣，也不会有很大损失，因此此时TPR相对FPR更重要。再比如反欺诈领域的欺诈预测模型，由于模型结果会对识别的坏人进行一定的处置措施，FPR过高会对好人有一定干扰，造成误杀，影响客户体验，因此模型需保证在低于一定FPR的基础上尽量增加TPR。 如果在我们所说的fraud detection 或者癌症检测这一类应用中，我们的倾向肯定是“宁可错杀一千，不可放过一个”呀。所以我们可以设定在合理的precision下，最高的recall作为最优点，找到这个对应的threshold点。 召回率应用场景召回率的应用场景：比如拿网贷违约率为例，相对好用户，我们更关心坏用户，不能错放过任何一个坏用户。因为如果我们过多的将坏用户当成好用户，这样后续可能发生的违约金额会远超过好用户偿还的借贷利息金额，造成严重偿失。召回率越高，代表实际坏用户被预测出来的概率越高，它的含义类似：宁可错杀一千，绝不放过一个。 对于ROC 曲线，我们希望， TPR 越高，同时FPR 越低（ROC 曲线向着左上角扩展，越陡越好） 先看二分类问题。指标的好坏主要取决于分类器的目标。比方说，电子邮件的垃圾过滤，你是希望它更全面（查出所有的垃圾，但是会有大量有用信息也被判为垃圾）呢，还是希望它尽量精准（不要老是将有用的邮件判为垃圾）呢？在这个例子里，显然，我们认为False Positive的伤害要大于False Negative：重要邮件要是被判成垃圾所造成的损失，远大于收件箱里还有一部分的垃圾邮件——前者可能会让你错过重要的工作，后者仅仅是让你在阅读的时候皱皱眉头。在这种情况下，我们会认为Precision的指标会比较重要，或者反应在ROC图上，FPR尽量的小——自然，在保证FPR的基础上，Recall依然还是重要的——毕竟用户购买的是垃圾过滤，如果只是过滤了1条垃圾但是Precision＝100%，这样的东西看起来也没什么用——那么综合起来，我们也可以通过ROC的AUC来进行比较，面积较大的代表同样的FPR下面，recall比较高。 其次是搜索问题。搜索问题其实是一个排序问题，但我们往往会定义Precision@Top K这样的指标，即正确的答案有没有被排在Top K中，如果是的话，就相当于判断为“真”，反之则为“否”。这样搜索问题就转化为了一个二分类问题，唯一的问题是，这是一个典型的数据不均衡的case。很显然，所有的候选集的数量是非常巨大的，但是K的数量不会很大（比如Top 10, Top 20）。所以，在这个问题中，我们会主要看Precision-Recall curve。更重要的是，一般而言，人们看搜索结果都不会太有耐心，所以希望Top K中的有用信息尽量多，换言之，Precision@Top K的指标，是最核心的。 然而如果我们的问题是多分类的问题，实际上这些指标就不适合了，我们需要看的是Confusion Matri: 简单来讲, 尽可能地找到正样例(最关心的类别)时,选recall(immediate from recall定义)尽可能地避免把非正样例预测为正样例时，选precision(immediate from precision定义) 拿楼上的『地震预测』举例子。我的理解是，对模型的要求是不能漏报（recall一定要高），但是不能老是误报（FPR不能太高）。混合的准确率其实没有太大的意义。所以，我会选择ROC。『犯罪检测』也是一样的。要求一样，数据分布也类似（正样本&lt;&lt;负样本） TakeOff If you have an imbalanced dataset accuracy can give you false assumptions regarding the classifier’s performance, it’s better to rely on precision and recall, in the same way a Precision-Recall curve is better to calibrate the probability threshold in an imbalanced class scenario as a ROC curve. ROC Curves: summarise the trade-off between the true positive rate and false positive rate for a predictive model using different probability thresholds. Precision-Recall curves: summarise the trade-off between the true positive rate and the positive predictive value for a predictive model using different probability thresholds. ROC curves are appropriate when the observations are balanced between each class, whereas precision-recall curves are appropriate for imbalanced datasets. In both cases the area under the curve (AUC) can be used as a summary of the model performance. Why a ROC curve cannot measure well? The Receiver Operating Characteristic (ROC) curves plot FPR vs. TPR as shown below. Because TPR only depends on positives, ROC curves do not measure the effects of negatives. The area under the ROC curve (AUC) assesses overall classification performance . AUC does not place more emphasis on one class over the other, so it does not reflect the minority class well. Davis and Goadrich in this paper propose that Precision-Recall (PR) curves will be more informative than ROC when dealing with highly skewed datasets. The PR curves plot precision vs. recall (FPR). Because Precision is directly influenced by class imbalance so the Precision-recall curves are better to highlight differences between models for highly imbalanced data sets. When you compare different models with imbalanced settings, the area under the Precision-Recall curve will be more sensitive than the area under the ROC curve. 最强讲解 how to handle unbalanced data对于这类问题是可以从数据和模型来进行考虑的。 Imbalanced data typically refers to a problem with classification problems where the classes are not represented equally. The accuracy paradox is the name for the exact situation in the introduction to this post. Data approachOversample minority class and Undersample majority class Over-sampling increases the number of minority class members in the training set. The advantage of over-sampling is that no information from the original training set is lost, as all observations from the minority and majority classes are kept. On the other hand, it is prone to overfitting. (You can add copies of instances from the under-represented class called over-sampling or more formally sampling with replacement) Under-sampling, on contrary to over-sampling, aims to reduce the number of majority samples to balance the class distribution. Since it is removing observations from the original data set, it might discard useful information. (You can delete instances from the over-represented class, called under-sampling.) Some Rules of Thumb Consider testing under-sampling when you have an a lot data (tens- or hundreds of thousands of instances or more) Consider testing over-sampling when you don’t have a lot of data (tens of thousands of records or less) Consider testing random and non-random (e.g. stratified) sampling schemes. Consider testing different resampled ratios (e.g. you don’t have to target a 1:1 ratio in a binary classification problem, try other ratios) Try Different Algorithms基于树的这种结构的模型还是表现比较给力的。That being said, decision trees often perform well on imbalanced datasets. The splitting rules that look at the class variable used in the creation of the trees, can force both classes to be addressed. Try Penalized ModelsPenalized classification imposes an additional cost on the model for making classification mistakes on the minority class during training. These penalties can bias the model to pay more attention to the minority class. Try Changing Your Performance Metric 使用 precision and recall curves or F1 去评价你的网络效果 Precision: A measure of a classifiers exactness. Recall: A measure of a classifiers completeness F1 Score (or F-score): A weighted average of precision and recall. 而 ROC curves 通常不是一个最好的选择。 https://medium.com/anomaly-detection-with-python-and-r/sampling-techniques-for-extremely-imbalanced-data-281cc01da0a8 GBC参数这些参数中，类似于Adaboost，我们把重要参数分为两类，第一类是Boosting框架的重要参数，第二类是弱学习器即CART回归树的重要参数。n_estimators: 也就是弱学习器的最大迭代次数，或者说最大的弱学习器的个数。learning_rate: 即每个弱学习器的权重缩减系数ν，也称作步长对于分类模型，有对数似然损失函数”deviance”和指数损失函数”exponential”两者输入选择。默认是对数似然损失函数”deviance”。一般来说，推荐使用默认的”deviance”。它对二元分离和多元分类各自都有比较好的优化。而指数损失函数等于把我们带到了Adaboost算法。对于回归模型，有均方差”ls”, 绝对损失”lad”, Huber损失”huber”和分位数损失“quantile”。默认是均方差”ls”。一般来说，如果数据的噪音点不多，用默认的均方差”ls”比较好。如果是噪音点较多，则推荐用抗噪音的损失函数”huber”。而如果我们需要对训练集进行分段预测的时候，则采用“quantile”。max_features:可以使用很多种类型的值，默认是”None”,意味着划分时考虑所有的特征数.subsample: 选择小于1的比例可以减少方差，即防止过拟合，但是会增加样本拟合的偏差，因此取值不能太低。推荐在[0.5, 0.8]之间，默认是1.0，即不使用子采样。 参考文献GBC参数设置ROC曲线和AUC值Introduction to Python Ensembles]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>ROC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[对抗生成网络实验对比]]></title>
    <url>%2F2018%2F06%2F05%2F%E5%AF%B9%E6%8A%97%E6%80%A7%E7%94%9F%E6%88%90%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[GAN模型是典型的隐式无监督生成模型，建模过程中没有利用到数据的语义标签。但在实际应用中生成模型的可控性至关重要，根据标签生成对可控有样本更具实际应用价值。图像生成模型被广泛应用于数据增强、风格转换和数据补全领域，需要可控且语义完备的生成模型。条件GAN模型在GAN模型建模思路的基础上，将语义标签加入了建模过程，将无监督生成模型转变为有监督的条件生成模型。条件GAN模型具体包括Conditional GAN模型、Semi-GAN模型和AC-GAN模型。 实验模型介绍（1）Conditional GAN模型GAN模型中判别器D对输入的数据样本的来源进行判别，是典型的判别模型流程。如果在GAN框架中加入有监督信息来辅助训练，如图像的类别信息来辅助判别器D进行判别，则会帮助生成更加真实的样本。其中最初的尝试方式是Conditional GAN模型，结构如图所示。Conditional GAN模型在生成器G和判别器D输入中都加入了标签信息，试图让生成器G学习到从数据标签y到样本x的映射，让判别器D学习对样本x和标签y的组合进行判别。与GAN模型相比，Conditional GAN模型增加了标签信息的输入将模型转变为条件生成模型，在一定程度上提高了模型的稳定性。Conditional GAN模型训练过程中，判别器D对样本x和标签y的类别组合进行训练，并没有输入样本x和标签y的类别错误组合进行训练，因此模型并没有学习样本x和标签y的联合分布。（2）Semi-GAN模型Conditional GAN模型利用了标签信息进行建模，但没有对标签语义的信息进行表征，导致模型能够学到的信息有限。在生成模型过程中，如果判别器D能够明确指出生成样本的类别错误，则可为生成器G提供更加精确的梯度信息，最终能生成更加真实的样本。Semi-GAN基于此思路进行改进，具体结构如图9所示。Semi-GAN模型在Conditional GAN模型的基础上，对判别器D的分类输出进行细化加入了半监督学习过程。（3）AC-GAN模型与Conditional GAN模型相比，Semi-GAN模型中判别器D能够判别真实样本的来源，增强了判别器D的判别能力。但研究表明过强的判别信息会影响生产样本的质量，具体原因为Semi-GAN模型的建模过程为半监督分类过程，目标优化函数为无监督分类和有监督分类目标函数之和。若判别器D的监督分类信息过强，则会削弱判别器D对样本来源的判别能力。Conditional GAN模型能够生成指定类别的样本，Semi-GAN模型能够判别样本的类别信息。AC-GAN模型将以上两个模型思路进行整合，得到够进行条件生成的生成器G，和能够判别样本类别和来源信息的判别器D。AC-GAN模型的结构如图10所示。AC-GAN模型在Conditional GAN的基础上，让判别器D在判别样本来源的同时，让样本进行分类。此时的判别器D的输出分为样本来源信息LS和样本分类LC信息。 不同模型比较下面用表格的方式对比在实验中使用的模型的目标函数(ps,图画比较丑，之后再修改) Name Paper Link Value Function GAN Arxiv DCGAN Arxiv Semi-GAN Arxiv 和GAN 模型相同 CGAN Arxiv ACGAN Arxiv our model Arxiv 数据集介绍在常用于图像生成的图像数据集中，大部分数据的标签类型为离散类型。其中MNIST和Fashion-MNIST为常用的灰度图像数据，每类样本分布较为独立，常用于进行图像分类和样本生成的实验；SVHN和CIFAR10为彩色数据集图像像素分布较为复杂，其中CIFAR10常用来检验分类网络性能的评价数据集；CelebA为大规模的人脸识别和属性分类数据集，每幅人脸图像包括40个属性标签；ImageNet为图像分类和识别数据集，数据集类别分布比较复杂具体包括自然图像和人为图像。UnityEyes为人眼视觉合成数据集，数据集标签包括瞳孔标签和视觉方向标签，其中视觉方向标签为连续的语言标签。常见的离散标签图像数据集的样例: 我们的模型在原始GAN模型中，目标函数定义为生成器G和判别器D的博弈过程，定义V(G;D)为模型的目标函数，由生成器G和判别器D组成。语义匹配目标函数FMloss。基于语义匹配的条件生成网络模型的生成器G和判别器D的目标函数分别为：如上式，LS为样本来源，LC为分类结果。判别器D目标为最大化LC+ LS + FMloss，其试图对输入样本进行分类，并通过来源和语义匹配区分生成样本和原始样本。生成器G的目的是最大化LC − LS −FMloss，其试图通过样本分类结果、样本来源和语义匹配结果来欺骗判别器D。LS来源损失与原始GAN模型相同，LC为语义标签分类损失，在类别分类中使用交叉信息熵，在数值回归中则使用均方差回归。 生成结果对比MNIST数据集生成结果Fashion-MNIST数据集生成结果SVHN数据集生成结果CIFAR10数据集生成结果CelebA数据集生成结果UnityEyes数据集生成结果]]></content>
      <categories>
        <category>Deep learning</category>
      </categories>
      <tags>
        <tag>GANs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Titanic Challenge]]></title>
    <url>%2F2018%2F06%2F05%2FTitanic-Challenge%2F</url>
    <content type="text"><![CDATA[用此博客记录自己解决 Kaggle Titanic challenge过程中的个人总结。 问题描述说道Titanic(泰坦尼克号)，最熟悉莫过于Titanic(1997 film),这部由著名导演詹姆斯·卡梅隆执导，莱昂纳多·迪卡普里奥、凯特·温斯莱特领衔主演的电影，经久不衰…但是我们今天的画风不是这样的… 我们今天解决的问题是以该事件问背景，但是没有那么浪漫。关于这个案例的介绍网上有很多内容，为了避免累赘，在这里就不进行详述。总的要求：预测在这个事件中乘客是否死亡。附上对于train sets和 test sets中数据的介绍。更多详细的内容参看：https://www.kaggle.com/c/titanic 数据分析顺滑过渡到第二阶段，数据分析，对于竞赛而言，我感觉对于数据的认识的重要性完全不亚于模型的重要性。之后我们将再次提到这句话。我将结合代码进行数据分析。 pandas 原生数据分析函数1234import pandas as pdtrain =pd.read_csv(&apos;data/train.csv&apos;)test =pd.read_csv(&apos;data/test.csv&apos;)train.describe(include=&apos;all&apos;) 除了上面 train.describe()，下面这两个也是比较常用的12train.head()train.columns 因为总体的数据量比较少，所以我们选择把train set 和test set连接起来进行数据分析和处理。1combined2 = pd.concat([train_data, test_data], axis=0) 数据质量分析 缺省值对于缺省值，常用的手段就是填充，但是针对不同的数据有不同的填充手段，有的是均值填充，有的是默认值填充还有的是根据现有数据训练一个 regression进行拟合(这种情况出现在缺省的数据比较重要，对于结果的预测有比较强的相关性的时候)。1combined2.Embarked.fillna(&apos;S&apos;, inplace=True) Embardked(上船港口)不是那么能表现出和结果(survival)相关的变量，我么可以直接采用某个默认值进行填充。1combined2.Fare.fillna(np.median(combined2.Fare[combined2.Fare.notnull()]), inplace=True) Fare(船票)我们选择使用均值填充 12345678classers = [&apos;Fare&apos;,&apos;Parch&apos;,&apos;Pclass&apos;,&apos;SibSp&apos;,&apos;TitleCat&apos;,&apos;CabinCat&apos;,&apos;Sex_female&apos;,&apos;Sex_male&apos;, &apos;EmbarkedCat&apos;, &apos;FamilySize&apos;, &apos;NameLength&apos;, &apos;FamilyId&apos;]age_et = ExtraTreesRegressor(n_estimators=200)X_train = full_data.loc[full_data.Age.notnull(),classers]Y_train = full_data.loc[full_data.Age.notnull(),[&apos;Age&apos;]]X_test = full_data.loc[full_data.Age.isnull(),classers]age_et.fit(X_train,np.ravel(Y_train))age_preds = age_et.predict(X_test)full_data.loc[full_data.Age.isnull(),[&apos;Age&apos;]] = age_preds 因为在特征提取看来 age 是一个比较重要的属性（下文中使用age来进一步计算性别特征，而性别特征对于survival 是重要的因素），所以需要通过 fit来进行填充 null 值。 异常值异常值的检测12combined2.boxplot()plt.ylim(0, 1000) 异常值处理大多数情况下我们都采取忽视，但是有时候异常值中却跟结果有比较强的相关性，比如说该题目分数在0.9的一位大神在博客中使用的特征包含名字长度。这个在我一开始的特征提取中确实没有太在意名字长度也可以当作一种和结果(servival or dead)相关的特征。 重复值重复值的检测1train[train.duplicated()==True] 如果运行结果为空，那么就是没有重复值,如果有重复值，一般使用下面类似的代码都是可以去除掉的。1df.drop_duplicates() 分布特征分析1234567#分布分析fig ,ax =plt.subplots(2,2, figsize=(8,6))sns.countplot(&apos;Embarked&apos;, data =train, ax =ax[0,0])sns.countplot(&apos;Pclass&apos;, data =train, ax =ax[0,1])sns.violinplot(&apos;Survived&apos;, &apos;Age&apos;, data= train, ax =ax[1,0]).set(ylim =(-10, 80))sns.countplot(x =&apos;Survived&apos;, data= train, ax =ax[1,1])plt.tight_layout() 运行需要导入 seaborn1import seaborn as sns 这里安利一个数据可视化工具-seaborn。回正题 counplot()可以直观的看出单个数据的特征，但是我们更加关心的是数据和数据之间的关系，更准确的是数据和预测数据(survival )之间的关系。所以,我们进行相关性分析。123456plt.subplots(figsize=(10,8)) corrmat = train[train.columns[1:]].corr()sns.set(font_scale=1.5) hm = sns.heatmap(corrmat, cbar=True, annot=True, square=True, fmt=&apos;.2f&apos;, annot_kws=&#123;&apos;size&apos;: 10&#125;) plt.title(&apos;Pearson Correlation of Features&apos;, y=1.05, size=15) plt.show() 从图中可以看出，在原始数据集特征中(为什么这么说，嗯，这意味着我们下文还要进行 generate new features),Fare特征是和 survival最相关的。这从数据角度这你船票价钱越高，你生存的几率就越大(三观尽毁)。嗯，这是符合经济社会的运行规律的。我们在分析 Pearson Correlation的时候，关注的是数值的绝对值，如果是正值，表示正相关；如果是负值，表示负相关。 如果细心的小伙伴发现，这个并没有把所有的变量的相关性表示出来，是的，下文我将给出一个加强版的。 特征工程当我们对数据有了一个初步的认识，这时候就可以进行特征工程了。网上流传很广的一句话”数据特征决定了机器学习的上限，而算法优化只是尽可能逼近这个上限”，我深有体会。因为之前进行特征提取，然后在kaggle的submission score是0.73205,经过模型融合然后达到了0.78947，提高了5个百分点。当自己在思考数据特征重新进行特征提取的时候，最后的score 是0.82296.这都是以10个百分点的提高啊。所以这句话很有道理，我试图找到这句话的出处，以表示我对于版权的尊重，但是科学上网能力有限，没有找见，也许这句话来自群众的智慧吧。图：我在kaggle 的submission 和相应的score 但是我想强调的是特征提取是个很难有模板化的东西，这得看个人对于这个问题的理解和对于数据的理解，对于数据异常值的处理。并且还想说的是这个一个迭代的过程，不是一步到位的。当初步构造好自己特征之后可以使用图形化工具进行简单的分析一下。(下图是我第一次构造的特征工程的图形化)12345678910111213141516171819def correlation_heatmap(df): _ , ax = plt.subplots(figsize =(14, 12)) colormap = sns.diverging_palette(220, 10, as_cmap = True) _ = sns.heatmap( df.corr(), cmap = colormap, square=True, cbar_kws=&#123;&apos;shrink&apos;:.9 &#125;, ax=ax, annot=True, linewidths=0.1,vmax=1.0, linecolor=&apos;white&apos;, annot_kws=&#123;&apos;fontsize&apos;:12 &#125; ) plt.title(&apos;Pearson Correlation of Features&apos;, y=1.05, size=15) train2 =train1.drop([&apos;NullCabin&apos;], axis =1)correlation_heatmap(train2) 上图是第一次特征分析的结果(kaggle score: 0.73205),survival和Pclass和Fare是有较强的正相关性。 该图是第二次特征分析分析结果（kaggle score:0.82296,survival与male_adult(-0.56)、sex_male(-0.54)负相关,和female_adult(0.54)、sex_female(0.52)正相关。你的survival的概率和你的性别和年龄有关，如果你是成年女子，那么你很大的概率不会死亡(像Rose那样)；如果你是成年男子，那么你有很大概率体现英伦的绅士风度，主动(Jack那样)或者被选择死亡。瞬间想起了Titanic电影中Jack和Rose 的场景，好感人啊!!! 模型训练发现写了这么久，还没有开始训练模型。加快脚步…下面的内容以第一次训练模型为例。在建立基本模型之前我们需要先引入评价函数，以评价不同模型性能的好坏。 通过均值和方差来评价模型性能的优劣1234from sklearn import cross_validation def rmsl(clf): s = cross_validation.cross_val_score(clf, X_train, y_train, cv=5) return (s.mean(),s.std()) 建立基本模型1234567891011121314151617181920212223242526272829303132333435363738394041424344from sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis,gaussian_processNLA =[ #ensemble methods ensemble.AdaBoostClassifier(), ensemble.BaggingRegressor(), ensemble.GradientBoostingClassifier(), ensemble.RandomForestClassifier(n_estimators=60), # Gaussian process gaussian_process.GaussianProcessClassifier(), # LM linear_model.LogisticRegression(C=1.0, penalty=&apos;l1&apos;), #Navies Bayes naive_bayes.GaussianNB(), # Nearest Neighbor neighbors.KNeighborsClassifier(n_neighbors=3), # Svm svm.SVC(probability=True), svm.LinearSVC(), #Tree tree.DecisionTreeClassifier(), tree.ExtraTreeClassifier() ]#create table to compare MLAMLA_columns = [&apos;MLA Name&apos;, &apos;MLA Parameters&apos;,&apos;MLA Train Accuracy Mean&apos;, &apos;MLA Test Accuracy Mean&apos;, &apos;MLA Test Accuracy Min&apos; ,&apos;MLA Time&apos;] MLA_compare = pd.DataFrame(columns = MLA_columns) row_index = 0 for alg in MLA: #set name and parameters MLA_compare.loc[row_index, &apos;MLA Name&apos;] = alg.__class__.__name__ MLA_compare.loc[row_index, &apos;MLA Parameters&apos;] = str(alg.get_params()) #score model with cross validation: cv_results = model_selection.cross_validate(alg, X_train, y_train, cv =5,return_train_score=True) MLA_compare.loc[row_index, &apos;MLA Time&apos;] = cv_results[&apos;fit_time&apos;].mean() MLA_compare.loc[row_index, &apos;MLA Train Accuracy Mean&apos;] = cv_results[&apos;train_score&apos;].mean() MLA_compare.loc[row_index, &apos;MLA Test Accuracy Mean&apos;] = cv_results[&apos;test_score&apos;].mean() MLA_compare.loc[row_index, &apos;MLA Test Accuracy Min&apos;] = cv_results[&apos;test_score&apos;].min() #let&apos;s know the worst that can happen! row_index+=1MLA_compare.sort_values(by = [&apos;MLA Test Accuracy Mean&apos;], ascending = False, inplace = True) 当我们发现某个模型效果比较好的时候，我们仍然可以进一步调参。但是这种调参并不是每次会得到better result,有时候只是一个decent result。调参是个技术活。以上图中的 DecisionTreeClassifier为例进行调参。12345678param_grid = &#123;&apos;criterion&apos;: [&apos;gini&apos;, &apos;entropy&apos;], &apos;splitter&apos;: [&apos;best&apos;, &apos;random&apos;], &apos;max_depth&apos;: [None, 2,4,6,8,10], &apos;min_samples_split&apos;: [5,10,15,20,25], &apos;max_features&apos;: [None, &apos;auto&apos;, &apos;sqrt&apos;, &apos;log2&apos;] &#125;tune_model = model_selection.GridSearchCV(tree.DecisionTreeClassifier(), param_grid=param_grid, scoring = &apos;accuracy&apos;, cv = 5) cv_results = model_selection.cross_validate(tune_model, X_train, y_train, cv = 5) 使用的是sklearn 中的model_selection 模块，进行GridSearch，其实只是把调参过程自动化程序化。得到的结果是1230.863068608315 #train mean0.79803322024 # test mean0.77094972067 #test min 我们通过比对发现这个结果和上图的结果是稍微变差的。可视化显示各个算法的效率：1234sns.barplot(x=&apos;MLA Test Accuracy Mean&apos;, y = &apos;MLA Name&apos;, data = MLA_compare, color = &apos;m&apos;) plt.title(&apos;Machine Learning Algorithm Accuracy Score \n&apos;) plt.xlabel(&apos;Accuracy Score (%)&apos;) plt.ylabel(&apos;Algorithm&apos;) 对比之后我们选取几个效果比较“好”的模型，然后进行下一步的模型融合。12345678910111213141516171819MLA_best = [ #Ensemble Methods ensemble.AdaBoostClassifier(), # 0.76076 ensemble.BaggingClassifier(), # 0.72248 ensemble.GradientBoostingClassifier(), # 0.73684 ensemble.RandomForestClassifier(n_estimators = 60), # 0.72727 #GLM linear_model.LogisticRegression(C=1.0, penalty=&apos;l1&apos;, tol=1e-6), # 0.77990 linear_model.RidgeClassifierCV(), # 0.77033 linear_model.LogisticRegressionCV() #0.77033 ] row_index = 0 for alg in MLA_best: algname = alg.__class__.__name__ alg.fit(X_train, y_train) predictions = alg.predict(X_test) result = pd.DataFrame(&#123;&apos;PassengerId&apos;:test[&apos;PassengerId&apos;].as_matrix(), &apos;Survived&apos;:predictions.astype(np.int32)&#125;) result.to_csv(algname+&quot;.csv&quot;, index=False) # save the results row_index+=1 模型融合简单的说模型融合就是通过多个decent模型的结果通过某种方式的结合，产生了比原来单个模型better的结果。关于模型融合的详细内容，请移步另一篇文章模型融合(Ensemble learning)我们这里以stacking(二层)为例说明模型融合。1234567891011121314151617181920212223ntrain = train.shape[0] #891 ntest = test.shape[0] #418 SEED = 0 # for reproducibility NFOLDS = 5 # set folds for out-of-fold prediction kf =model_selection.KFold(n_splits=NFOLDS, random_state=SEED)# 封装算法基本操作 class SklearnHelper(object): def __init__(self, clf, seed=0, params=None): params[&apos;random_state&apos;] = seed self.clf = clf(**params) def train(self, x_train, y_train): self.clf.fit(x_train, y_train) def predict(self, x): return self.clf.predict(x) def fit(self,x,y): return self.clf.fit(x,y) def feature_importances(self,x,y): print(self.clf.fit(x,y).feature_importances_) return self.clf.fit(x,y).feature_importances_ 下面是定义五折交叉验证的方法，默认是三折。123456789101112131415161718def get_oof(clf, x_train, y_train, x_test): oof_train = np.zeros((ntrain,)) oof_test = np.zeros((ntest,)) oof_test_skf = np.empty((NFOLDS, ntest)) for i, (train_index, test_index) in enumerate(kf.split(x_train)): x_tr = x_train[train_index] y_tr = y_train[train_index] x_te = x_train[test_index] clf.train(x_tr, y_tr) oof_train[test_index] = clf.predict(x_te) oof_test_skf[i, :] = clf.predict(x_test) oof_test[:] = oof_test_skf.mean(axis=0) return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1) # 想让z变成只有一列，行数不知道多少 1234567891011121314151617181920212223242526272829303132from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier,AdaBoostClassifier, GradientBoostingClassifier# 定义四个不同的弱分类器的参数值 # Random Forest parameters rf_params = &#123; &apos;n_jobs&apos;: -1,&apos;n_estimators&apos;: 500,&apos;warm_start&apos;: True, &apos;max_depth&apos;: 6,&apos;min_samples_leaf&apos;: 2, &apos;max_features&apos; : &apos;sqrt&apos;,&apos;verbose&apos;: 0#&apos;max_features&apos;: 0.2, &#125; # Extra Trees Parameters et_params = &#123; &apos;n_jobs&apos;: -1,&apos;n_estimators&apos;:500,&apos;max_depth&apos;: 8,&apos;min_samples_leaf&apos;: 2,&apos;verbose&apos;: 0 #&apos;max_features&apos;: 0.5, &#125; # AdaBoost parameters ada_params = &#123; &apos;n_estimators&apos;: 500,&apos;learning_rate&apos; : 0.75 &#125; # Gradient Boosting parameters gb_params = &#123; &apos;n_estimators&apos;: 500,&apos;max_depth&apos;: 5,&apos;min_samples_leaf&apos;: 2, &apos;verbose&apos;: 0 #&apos;max_features&apos;: 0.2, &#125; # Support Vector Classifier parameters # svc_params = &#123; # &apos;kernel&apos; : &apos;linear&apos;,&apos;C&apos; : 0.025 # &#125; # 创建四个若分类器模型 rf = SklearnHelper(clf=RandomForestClassifier, seed=SEED, params=rf_params) et = SklearnHelper(clf=ExtraTreesClassifier, seed=SEED, params=et_params) ada = SklearnHelper(clf=AdaBoostClassifier, seed=SEED, params=ada_params) gb = SklearnHelper(clf=GradientBoostingClassifier, seed=SEED, params=gb_params) 1234567891011121314151617181920#X_train =X_train.values#X_test =X_test.values# 使用五折交叉方法分别计算出使用不同算法的预测结果，这些结果将用于Stacking的第二层预测 et_oof_train, et_oof_test = get_oof(et, X_train, y_train, X_test) # Extra Trees rf_oof_train, rf_oof_test = get_oof(rf,X_train, y_train, X_test) # Random Forest ada_oof_train, ada_oof_test = get_oof(ada, X_train, y_train, X_test) # AdaBoost gb_oof_train, gb_oof_test = get_oof(gb,X_train, y_train, X_test) # Gradient Boost rf_feature = rf.feature_importances(X_train,y_train) et_feature = et.feature_importances(X_train, y_train) ada_feature = ada.feature_importances(X_train, y_train) gb_feature = gb.feature_importances(X_train,y_train) feature_dataframe = pd.DataFrame( &#123;&apos;features&apos;: cols, &apos;Random Forest feature importances&apos;: rf_feature, &apos;Extra Trees feature importances&apos;: et_feature, &apos;AdaBoost feature importances&apos;: ada_feature, &apos;Gradient Boost feature importances&apos;: gb_feature &#125;) 接下来以第一层为为基础训练第二层12345678base_predictions_train = pd.DataFrame( &#123; &apos;RandomForest&apos;: rf_oof_train.ravel(),# # ravel函数在降维时默认是行序优先 &apos;ExtraTrees&apos;: et_oof_train.ravel(), &apos;AdaBoost&apos;: ada_oof_train.ravel(), &apos;GradientBoost&apos;: gb_oof_train.ravel() &#125;) X_train2 = np.concatenate(( et_oof_train, rf_oof_train, ada_oof_train, gb_oof_train), axis=1) X_test2 = np.concatenate(( et_oof_test, rf_oof_test, ada_oof_test, gb_oof_test), axis=1) 使用XGBoost训练第二层的数据。关于XGBoost为什么是有效的和相关的概念，请移步XGBoost123456789101112131415# XGboost import xgboost as xgbgbm = xgb.XGBClassifier( #learning_rate = 0.02, n_estimators= 2000, max_depth= 4, min_child_weight= 2, #gamma=1, gamma=0.9, subsample=0.8, colsample_bytree=0.8, objective= &apos;binary:logistic&apos;, nthread= -1, scale_pos_weight=1).fit(X_train2, y_train) predictions = gbm.predict(X_test2) 最后产生结果文件 StackingSubmission.csv12StackingSubmission = pd.DataFrame(&#123;&apos;PassengerId&apos;:test.PassengerId, &apos;Survived&apos;: predictions &#125;) StackingSubmission.to_csv(&quot;StackingSubmission.csv&quot;, index=False) # 0.78947 参考文献本文在效果可视化中借鉴该博客特征提取参看kaggle多位大神，在这里就谢过…]]></content>
      <categories>
        <category>竞赛</category>
      </categories>
      <tags>
        <tag>Kaggle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[模型融合]]></title>
    <url>%2F2018%2F06%2F05%2F%E6%A8%A1%E5%9E%8B%E8%9E%8D%E5%90%88%2F</url>
    <content type="text"><![CDATA[多个有差异性的模型融合可以提高整体的性能。它能同时降低最终模型的bias 和variance，从而在提高竞赛分数的同时降低overfitting 的风险。 从结果文件中融合这种做法不需要重新训练模型，融合竞赛提交的结果文件就可以，简单便捷。 Voting投票制：少数服从多数。如一个分类问题，多个模型的投票（当然可以设置权重，若没有就是平均投票），最终投票数最多的类就是被预测的类。对于加权表决融合，性能表现较差的模型（权值比较低）只能通过和其他模型保持一致增强自己的说服力。对于结果取平均融合，在不同的评估准则上也能获得不错的效果在于：取均值常常能减少过拟合的现象。如图所示：如果单个模型过拟合产生了绿色的边缘，这时候去平均这种策略使得决策边界变成黑色的边缘，这样的效果更好。机器学习的目的并不是让模型记住训练数据，而是具有更好的泛化性。 RankingRank的思想其实和Averaging一致，但Rank是把排名做平均，对于AUC指标比较有效。 训练模型融合 Bagging:使用训练数据的不同随机子集来训练每个 Base Model，最后进行每个 Base Model 权重相同的 Vote。也即 Random Forest 的原理。 Boosting:Boosting是一种迭代的方法，每一次训练会更关心上一次被分错的样本，比如改变被错分的样本的权重的Adaboost方法。也即 Gradient Boosting，Adaboost 的原理。比 Bagging 效果好，但更容易 Overfit。 Blending用不相交的数据训练不同的 Base Model，将它们的输出取（加权）平均。实现简单，但对训练数据利用少了。 Stackinig(以二层为例)在网上为数不多的关于stacking的内容中，相信你已经看过这张图片：PS:这不是原图，是在原图的基础上经过修改（把最上面标题的model 1，2,3,4,5 修改为model 1,1,1,1,1。因为这个一个model 的不同阶段，不是多个模型） 想比较而言我更加喜欢下面这张图片，因为它把stacking 的不同阶段表达的更加清楚，尤其是经过model 1之后，model 2是在model 1的基础上进行训练的。 对于第二阶段使用的样本集合，上图使用的是第一阶段的结果数据集，当然还有一种方式，如下图所示。 在该图中上一阶段的结果(prob 1-N)列和原始数据集组成新的特征向量，训练第二阶段模型。 名词解释 cross validation交叉验证当评估不同的参数设置，对算法表现的影响时，仍然存在则过拟合的风险。因为在调整参数，优化测试集的算法表现时，测试集的信息已经泄漏进模型中了。为了解决这个问题，需要一部分数据作为验证集(validation set)。 这样，用训练集(Train set)的数据训练模型；用验证集对模型参数调剂，如上述程序中的C值；最后，算法的评价在测试集(Test set)上完成。但是当数据有原来的两份化成三份之后，降低了寻数据量；另外算法的表现依赖于三个数据集的划分。解决上述两个问题的常见方法: cross validation. 测试集仍然单独划分出来，但是 validation set不用单独划分。将训练集划分为k个小的数据集，称之为k-fold CV。对每个fold进行下列过程： 用其他k-1 folds 作为training sets，训练模型 模型的结果用剩下的一个 fold进行评价模型的性能用上述循环中的 k-fold 交叉验证集的平均值表现，这样的做法增加了计算量，但是提高数据的利用效率。 参考文献https://blog.csdn.net/u013395516/article/details/79745063https://blog.csdn.net/u012969412/article/details/76636336https://blog.csdn.net/u012604810/article/details/77579782]]></content>
      <categories>
        <category>竞赛</category>
      </categories>
      <tags>
        <tag>Kaggle</tag>
        <tag>优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[XGBoost]]></title>
    <url>%2F2018%2F06%2F05%2FXGBoost%2F</url>
    <content type="text"><![CDATA[Introduction to XGBoostXGBoost is short for “Extreme Gradient Boosting”, where the term “Gradient Boosting” is proposed in the paper Greedy Function Approximation: A Gradient Boosting Machine, by Friedman. XGBoost is based on this original model. And most of the content is based on the websit(http://xgboost.readthedocs.io/en/latest/model.html) element of Supervised learningXGBoost is used for supervised learning problems, where we use the training data (with multiple features) xi to predict a target variable yi. Objective Function: Training Loss + RegularizationA very important fact about objective functions is they must always contain two parts: training loss and regularization. For example, a commomly used training loss is mean squared error. Another commonly used loss function is logistic loss for logistic regression The regularization term is what people usually forget to add. The regularization term controls the complexity of the model, which helps us to avoid overfitting.The general principle is we want both a simple and predictive model. The tradeoff between the two is also referred as bias-variance tradeoff in machine learning. Decision Tree and random forest Decision Trees: It is one of my favorite techniques. It can be used as a ultimate solution to tackle multiple challenges like missing values, outliers and identifying significant variables. It worked well in our Data Hackathon also. 现在理解可能是基于”增益“ 之类的东西，关于 RF 最后一条。 Random Forest: Similar to decision tree is Random Forest. I would also recommend using the in-built feature importance provided by random forests to select a smaller subset of input features. Just be careful that random forests have a tendency to bias towards variables that have more no. of distinct values i.e. favor numeric variables over binary/categorical values. Tree EnsembleThe tree ensemble model is a set of classification and regression trees(CART). Here is a simple example of a CART that classifies whether someone will like computer games. A CART is a bit different from decision trees, where the leaf only contains decision values. In CART, a real score is associated with each of the leaves, which gives us richer interpretations that go beyond classification. This also makes the unified optimization step easier, as we will see in a later part of this tutorial. Usually, a single tree is not strong enough to be used in practice. What is actually used is the so-called tree ensemble model, which sums the prediction of multiple trees together. Here is an example of a tree ensemble of two trees. The prediction scores of each individual tree are summed up to get the final score. If you look at the example, an important fact is that the two trees try to complement each other. Now here comes the question, what is the model for random forests? It is exactly tree ensembles! So random forests and boosted trees are not different in terms of model, the difference is how we train them. This means if you write a predictive service of tree ensembles, you only need to write one of them and they should directly work for both random forests and boosted trees. One example of why elements of supervised learning rock. Tree BoostingAfter introducing the model, let us begin with the real training part. How should we learn the trees? The answer is, as is always for all supervised learning models: define an objective function, and optimize it! Assume we have the following objective function (remember it always needs to contain training loss and regularization) Additive TrainingFirst thing we want to ask is what are the parameters of trees? You can find that what we need to learn are those functions fi, with each containing the structure of the tree and the leaf scores. This is much harder than traditional optimization problem where you can take the gradient and go. It is not easy to train all the trees at once. Instead, we use an additive strategy: fix what we have learned, and add one new tree at a time. It remains to ask, which tree do we want at each step? A natural thing is to add the one that optimizes our objective. Training modelThe XGBoost model for classification is called XGBClassifier(regression is called XGBRegressor). We can create and and fit it to our training dataset. Models are fit using the scikit-learn API and the model.fit() function.]]></content>
      <categories>
        <category>竞赛</category>
      </categories>
      <tags>
        <tag>XGBoost</tag>
        <tag>Kaggle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[等概率生成器]]></title>
    <url>%2F2018%2F05%2F28%2F%E7%AD%89%E6%A6%82%E7%8E%87%E7%94%9F%E6%88%90%E5%99%A8%2F</url>
    <content type="text"><![CDATA[rand3() -&gt; rand7() 给定一个可以等概率生成1-3的rand3()函数生成器，求解可以随机等概率生成1-7的rand7()函数生成器。 使用多个 rand3()，保证产生的值域是大于后者的，然后再产生的值域上进行裁剪取尾。 3 6 9 1 4 7 10 2 5 8 11 3 6 9 12 转成这种样子： 3 6 9 1 1 4 7 2 2 5 8 3 3 6 9 从以上的两个例子中，这种随机数的套路应该是掌握了吧。如果是 randx() 那么这个生成式子 就是 randx() + randx() *x 这个乘数必须是 x ，要不然就重复生成了一个数字，那么最后就没有办法达到等概率，至于最后的减去的数字则是根据实际情况选择。一定要生成连续不重复的数字，这样才能是等概率的。 减去的数字保证了取余能够整除的。 123456789def rand3_to_rand7(): value =0 for i in range(1,4): for j in range(1, 4): value =i + j*3 -3 if value &gt;7: return return value% 7 rand2() -&gt; rand5()Given rand2(), you should get rand5() 如果只是使用两个 rand2() 那么只能得到四种可能性，不足以生成 rand5() ，所以要使用3 个rand2() ，这个时候得到了 8种可能性，减去3 种，那么就OK 了。( 从这个例题中可以得到是使用多个 randx() 的倍数是 x) 12345int rand7() &#123; int x = rand2() * 4 + rand2() * 2 + rand2(); if (x == 7) return rand7(); // restart else return x;&#125;]]></content>
      <categories>
        <category>算法</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[机器学习概念]]></title>
    <url>%2F2018%2F05%2F28%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[解释一些机器学习中小的基本的概念。梯度下降、牛顿法、凸函数 梯度下降批量梯度下降法是最原始的形式，它是指在每一次迭代时使用所有样本来进行梯度的更新。 优点： （1）一次迭代是对所有样本进行计算，此时利用矩阵进行操作，实现了并行。 （2）由全数据集确定的方向能够更好地代表样本总体，从而更准确地朝向极值所在的方向。当目标函数为凸函数时，BGD一定能够得到全局最优。缺点： （1）当样本数目 m 很大时，每迭代一步都需要对所有样本计算，训练过程会很慢。 随机梯度下降（Stochastic Gradient Descent，SGD） 随机梯度下降法不同于批量梯度下降，随机梯度下降是每次迭代使用一个样本来对参数进行更新。使得训练速度加快。 解释一下为什么SGD收敛速度比BGD要快？答：这里我们假设有30W个样本，对于BGD而言，每次迭代需要计算30W个样本才能对参数进行一次更新，需要求得最小值可能需要多次迭代（假设这里是10）；而对于SGD，每次更新参数只需要一个样本，因此若使用这30W个样本进行参数更新，则参数会被更新（迭代）30W次，而这期间，SGD就能保证能够收敛到一个合适的最小值上了。也就是说，在收敛时，BGD计算了 10×30W 次，而SGD只计算了 1×30W 次。 小批量梯度下降，是对批量梯度下降以及随机梯度下降的一个折中办法。其思想是：每次迭代 使用 batch_size 个样本来对参数进行更新。 上面的图可以看出 SGD 的搜索空间是有点盲目的，但是快。 随机梯度下降和批量梯度下降是两种迭代求解思路 是在梯度下降的基础之上增加了某一些限制来使得其快速运算的目的。引入随机梯度下降法与mini-batch梯度下降法是为了应对大数据量的计算而实现一种快速的求解。 牛顿法 梯度下降 vs 牛顿法 梯度下降：$$x _ { n + 1 } = x _ { n } - \mu f ^ { \prime } \left( x _ { n } \right)$$ 牛顿法递推式： $$x _ { n + 1 } = x _ { n } - \frac { f ^ { \prime } \left( x _ { n } \right) } { f ^ { \prime \prime } \left( x _ { n } \right) }$$ $$f ( x + \Delta x ) = f ( x ) + f ^ { \prime } ( x ) \Delta x + \frac { 1 } { 2 } f ^ { \prime \prime } ( x ) \Delta x ^ { 2 }$$ 对于 两种优化方法的几种解读方式： wiki 上的解释： 牛顿下降法是用二次曲面去拟合当前的局部曲面，而梯度下降法是用平面去拟合当前的局部曲面，一般用二次曲面拟合的更好，所以一般牛顿算法收敛快。 红色为牛顿下降法，绿色为梯度下降法 牛顿法是基于当前位置的切线来确定下一次的位置，所以牛顿法又被很形象地称为是”切线法”。牛顿法的搜索路径（二维情况）如下图所示： 牛顿法搜索动态示例图 数学角度 使用平面去拟合使用泰勒展开式进行解释：$$f ( x + \Delta x ) = f ( x ) + f ^ { \prime } ( x ) * \Delta x$$ 使用曲面去拟合$$f ( x + \Delta x ) = f ( x ) + f ^ { \prime } ( x ) \Delta x + \frac { 1 } { 2 } f ^ { \prime \prime } ( x ) \Delta x ^ { 2 }$$对两边 $\Delta x$进行求导 $$\Delta x = - \frac { f ^ { \prime } ( x ) } { f ^ { \prime \prime } ( x ) }$$进而： $$x _ { n + 1 } = x _ { n } - \frac { f ^ { \prime } \left( x _ { n } \right) } { f ^ { \prime \prime } \left( x _ { n } \right) }$$ 优缺点： 牛顿法起始点不能离局部极小点太远，否则很可能不会收敛。(考虑到二阶拟合应该很容易想象)，所以实际操作中会先使用别的方法，比如梯度下降法，使更新的点离最优点比较近，再开始用牛顿法。 牛顿法每次需要更新一个二阶矩阵，当维数增加的时候是非常耗内存的，所以实际使用是会用拟牛顿法。 梯度下降法在非常靠近最优点时会有震荡，就是说明明离的很近了，却很难到达，因为线性的逼近非常容易一个方向过去就过了最优点(因为只能是负梯度方向)。但牛顿法因为是二次收敛就很容易到达了。 牛顿法最明显快的特点是对于二阶函数(考虑多元函数的话要在凸函数的情况下)，牛顿法能够一步到达，非常有效。 所以这个综合上面两种方法，一般在开始的时候使用 梯度下降，然后快要接近最值点（这个时候使用 梯度下降容易发生震荡），使用牛顿法，牛顿法是需要计算二阶梯度，一般计算量比较大，所以可以考虑使用拟牛顿法。 凸函数对于损失函数无非是凸函数和非凸函数。其中对凸函数一般是可以求得全局最小值的。对于非凸函数来说，其函数是抖动的，可能存在很多局部解，而在对这类问题求解时可能不能有效地找出其全局最小值， 凸优化:对凸优化的问题我们在基础数学上面已经有了很多解决方法，例如可以将凸优化问题Lagerange做对偶化，然后用Newton、梯度下降算法求解。 凸函数：Jacobian矩阵和Hessian矩阵：在向量分析中, 雅可比矩阵是一阶偏导数以一定方式排列成的矩阵, 其行列式称为雅可比行列式。Hessian矩阵： bias and variance个人感觉 variance 更像是一种稳定性的评价，预测结果的波动性，如果variance 小，那么就是稳定性好。 bias variance tradeoff 实际上是有三个名词 bias (偏差)，variance (方差) and noise (噪声)。偏差是预测值和实际值的差值，是准不准的问题；而方差是预测值的属性，如果最后的结果比较分散，那么方差就比较大。所以是两个不同的维度。最好的模型当然是低方差并且低偏差了。一图胜千言 常见的几种说法： 当网络结构比价简单的时候，容易出现高偏差，一般来说这个时候是欠拟合 当网络比较复杂的时候，容易出现高方差，一般来说这个时候是过拟合 如果出现了过拟合，一般说拟合了过多的noise (噪声)，没有抓住问题的本质。 问题 解决方案 高方差 采集更多的样本， 降低特征的维度，降低参数 高偏差 采集更多的特征，增大参数 在训练方法上也是可以改进的， 比如使用 K-Fold 交叉验证。简单的说，就是将训练样本分成k份，每次取其中一份作为验证集，另外 k-1 份作训练集。这样进行 k 次训练得到 k 个模型。这 k 个模型对各自的验证集进行预测，得到 k 个评估值（可以是误差、准确率，或按某种规则计算的得分等等）。注意到每个样本参与了 k-1 个模型的训练（导致模型之间存在关联），每个样本有一次被用作测试（没有用另外的从未见过的测试集数据），所以这与标准的计算过程是不一样的。 当 K值比较大的时候，模型容易过拟合数据集(K-1 数据集占比是接近于1，所以记住了训练数据集)，bias 比较小，但是当 测试集上的时候，variance 比较大； 同理，当K 值标胶小的时候，bias 比较大，没有过拟合数据，因此在 test 数据集上 bias 是比较小的。 偏差和方差又与「欠拟合」及「过拟合」紧紧联系在一起。由于随机误差是不可消除的，所以此篇我们讨论在偏差和方差之间的权衡（Bias-Variance Tradeoff）。 当模型处于欠拟合状态时，训练集和验证集上的误差都很高； 当模型处于过拟合状态时，训练集上的误差低，而验证集上的误差会非常高。 这两个概念和模型的复杂度又有着很深的联系： 生成式模型和判别式比较判别式模型对 $ P(Y|X) $进行建模,目的是找到一个决策边界,根据这个边界来确定新样本的类别.生成式模型对 $P(X,Y) $进行建模,目的是找到每个类别的分布,根据类别的分布情况确定新样本的类别. 如上图所示,当一个新样本进来后,如果是判别式模型,会判断它在决策边界的左边还是右边,即计算P(Y_1|X)与P(Y_2|X)来判断新样本是蓝色还是黄色.如果是生成式模型会根据两个类别分布分别计算属于两个类别的概率,即P(Y_1,X)与P(Y_2,X)来判断新样本是蓝色还是黄色. 举例子说明常见的算法： 线性回归、逻辑回归、神经网络、支持向量机、决策树、 K近邻 都是属于判别模型；最常见的生成模型的例子就是 朴素贝叶斯了。 朴素贝叶斯通过贝叶斯法则,对 $ P(y | x) $进行变换: $$P ( y | x ) = \frac { P ( x , y ) } { P ( x ) }$$ 分母是常数,分子根据条件独立性可以转化为:$$P ( x , y ) = p \left( y , f _ { 1 } , \ldots , f _ { n } \right) = \prod _ { i = 1 } ^ { n } P \left( f _ { i } | y \right)$$ 也就是说朴素贝叶斯相当于为每一个特征建立了一个高斯模型.所以朴素贝叶斯是生成式模型. 对于判别式模型来说求得P(Y|X)，对未见示例X，根据P(Y|X)可以求得标记Y，即可以直接判别出来，如上图的左边所示，实际是就是直接得到了判别边界，所以传统的、耳熟能详的机器学习算法如线性回归模型、支持向量机SVM等都是判别式模型，这些模型的特点都是输入属性X可以直接得到Y。 而生成式模型求得P(Y,X)，对于未见示例X，你要求出X与不同标记之间的联合概率分布，然后大的获胜，如上图右边所示，并没有什么边界存在，对于未见示例（红三角），求两个联合概率分布（有两个类），比较一下，取那个大的。比如朴素贝叶斯算法和隐式马尔科夫模型。 判别式模型举例：要确定一个羊是山羊还是绵羊，用判别模型的方法是从历史数据中学习到模型，然后通过提取这只羊的特征来预测出这只羊是山羊的概率，是绵羊的概率。生成式模型举例：利用生成模型是根据山羊的特征首先学习出一个山羊的模型，然后根据绵羊的特征学习出一个绵羊的模型，然后从这只羊中提取特征，放到山羊模型中看概率是多少，在放到绵羊模型中看概率是多少，哪个大就是哪个。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>bias-variance</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[面试算法题]]></title>
    <url>%2F2018%2F05%2F28%2F%E9%9D%A2%E8%AF%95%E7%AE%97%E6%B3%95%E9%A2%98%2F</url>
    <content type="text"><![CDATA[求解平方根求解平方根的算法主要有两种：二分法(binary search) 和牛顿迭代法(Newton’s Method)。前者是二分查找的第一种形式，后者是数学公式 $k + \frac{n} {k} \geq 2 \times k$。也可以从泰勒展开式中理解牛顿法，牛顿法是泰勒展开式的二阶形式。 1234567891011121314151617181920212223242526272829from math import sqrt# you needn't import sqrt, I do that just for comparing the results of different methodsclass Solution: # in-built function def my_sqrt(self, n): return sqrt(n) def sqrt_binary(self, n): low, high =0,n mid =int((low+high)/2) while abs(mid*mid- n)&gt; 1e-9: if mid*mid&gt;n: high =mid else: low =mid mid =(low+high)/2 return mid # Newton method: math def newton_method(self, n): k =1 while abs(k*k-n) &gt;1e-9: k =(k+n/k)/2 return k if __name__ =="__main__": print(Solution().my_sqrt(2)) print(Solution().sqrt_binary(2)) print(Solution().newton_method(2)) 运行结果 1231.4142135623730951 #built-in function1.4142135623842478 # binary search1.4142135623746899 # Newton method 从结果中看，Newton method比 binary sqrt更加接近系统自带的sqrt function. 并且从数学上可以证明 Newton method 比 binary sqrt需要更少的迭代次数。 Consecutive Numbers SumTips：如果是连续数，那么是可以使用等差数列求和的。并且这个求解的是符合条件的解的个数。时间复杂度是O ($\sqrt{n}$) 。对于一个正整数N，如果能写成K个连续正整数相加的形式，则有， $$\begin{split}N &amp;= ( x + 1 ) + ( x + 2 ) + \dots + ( x + K ) \\N &amp;= K \times x + \frac { ( 1 + K ) \times K } { 2 } \\\end{split}$$ 所以， N 能够被 K个连续正整数相加的条件是， $\left( N - \frac { K * ( K + 1 ) } { 2 } \right)$ 能够被 K 整除。 1234567891011121314151617181920class Solution(object): def consecutiveNumbersSum(self, N): """ :type N: int :rtype: int """ res =0 n =0 while n*(n+1) &lt;= 2*N: top =N -(n *n +n)/2 if top &lt;=0: break elif top %(n +1) ==0: res +=1 n +=1 return res 如何近似的求解 piTips: 随机抽样，注意是均匀分布而不是正太分布，对应python 中的实现是 randrange() 而不是 random() 函数。 12345678910111213from random import randrangetotal =10000count =0while total: x =randrange(0, 10001)/10000.0 y =randrange(0, 10001)/10000.0 if pow(x, 2) +pow(y,2) &lt;=1: count +=1 total -=1 pai =count *4.0/total 中文数字转阿拉伯数字 **Tips: 万、亿、兆可以分别对应着4、8、12个0。关键时其他的进制数字如千、百可以出现在他们之前，所以有一种进制的进制的感觉。总而言之这三个汉字是需要单独处理的。 字符串的处理，从低位到高位逐个进行处理(pop() )。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135CN_NUM = &#123; u'〇': 0, u'一': 1, u'二': 2, u'三': 3, u'四': 4, u'五': 5, u'六': 6, u'七': 7, u'八': 8, u'九': 9, u'零': 0, u'壹': 1, u'贰': 2, u'叁': 3, u'肆': 4, u'伍': 5, u'陆': 6, u'柒': 7, u'捌': 8, u'玖': 9, u'貮': 2, u'两': 2,&#125;CN_UNIT = &#123; u'十': 10, u'拾': 10, u'百': 100, u'佰': 100, u'千': 1000, u'仟': 1000, u'万': 10000, u'萬': 10000, u'亿': 100000000, u'億': 100000000, u'兆': 1000000000000,&#125;def cn2dig(cn): lcn = list(cn) unit = 0 # 当前的单位 ldig = [] # 临时数组 while lcn: cndig = lcn.pop() #if CN_UNIT.has_key(cndig): if cndig in CN_UNIT: unit = CN_UNIT.get(cndig) if unit == 10000: ldig.append('w') # 标示万位 unit = 1 elif unit == 100000000: ldig.append('y') # 标示亿位 unit = 1 elif unit == 1000000000000: # 标示兆位 ldig.append('z') unit = 1 continue else: dig = CN_NUM.get(cndig) if unit: dig = dig * unit unit = 0 ldig.append(dig) # 这个是 if unit == 10: # 处理10-19的数字 ldig.append(10) print(ldig) ret = 0 tmp = 0 import ipdb ipdb.set_trace() while ldig: x = ldig.pop() # 这个进制一定是出现在后面的， tmp *= 10000 是没有问题的 if x == 'w': tmp *= 10000 ret += tmp tmp = 0 elif x == 'y': tmp *= 100000000 ret += tmp tmp = 0 elif x == 'z': tmp *= 1000000000000 ret += tmp tmp = 0 else: tmp += x ret += tmp return ret # ldig.reverse() # print ldigß # print CN_NUM[u'七']if __name__ == '__main__': test_dig = [u'九', u'十一', u'十九万零十九', u'一百二十三', u'一千二百零三', u'一万一千一百零一', u'十万零三千六百零九', u'一百二十三万四千五百六十七', u'一千一百二十三万四千五百六十七', u'一亿一千一百二十三万四千五百六十七', u'一百零二亿五千零一万零一千零三十八', u'一千一百一十一亿一千一百二十三万四千五百六十七', u'一兆一千一百一十一亿一千一百二十三万四千五百六十七', ] for cn in test_dig: print (cn2dig(cn)) 中文转成阿拉伯数字（含有点） 如果有“点”，那么就截断处理，分成小数点之前和之后。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136CN_NUM = &#123; u'〇': 0, u'一': 1, u'二': 2, u'三': 3, u'四': 4, u'五': 5, u'六': 6, u'七': 7, u'八': 8, u'九': 9, u'零': 0, u'壹': 1, u'贰': 2, u'叁': 3, u'肆': 4, u'伍': 5, u'陆': 6, u'柒': 7, u'捌': 8, u'玖': 9, u'貮': 2, u'两': 2,&#125;CN_UNIT = &#123; u'十': 10, u'拾': 10, u'百': 100, u'佰': 100, u'千': 1000, u'仟': 1000, u'万': 10000, u'萬': 10000, u'亿': 100000000, u'億': 100000000, u'兆': 1000000000000,&#125;def cn2dig(cn): lcn = list(cn) unit = 0 # 当前的单位 ldig = [] # 临时数组 while lcn: cndig = lcn.pop() #if CN_UNIT.has_key(cndig): if cndig in CN_UNIT: unit = CN_UNIT.get(cndig) if unit == 10000: ldig.append('w') # 标示万位 unit = 1 elif unit == 100000000: ldig.append('y') # 标示亿位 unit = 1 elif unit == 1000000000000: # 标示兆位 ldig.append('z') unit = 1 continue else: dig = CN_NUM.get(cndig) if unit: dig = dig * unit unit = 0 ldig.append(dig) # 这个是 if unit == 10: # 处理10-19的数字 ldig.append(10) print(ldig) ret = 0 tmp = 0 #import ipdb #ipdb.set_trace() while ldig: x = ldig.pop() # 这个进制一定是出现在后面的， tmp *= 10000 是没有问题的 if x == 'w': tmp *= 10000 ret += tmp tmp = 0 elif x == 'y': tmp *= 100000000 ret += tmp tmp = 0 elif x == 'z': tmp *= 1000000000000 ret += tmp tmp = 0 else: tmp += x ret += tmp return ret # ldig.reverse() # print ldigß # print CN_NUM[u'七']if __name__ == '__main__': #for cn in test_dig: # print (cn2dig(cn)) test_case ='十五点二' if u'点' in test_case: index =test_case.index(u'点') pre =test_case[:index] nex =test_case[index+1:] pre_num =cn2dig(pre) nex_num = cn2dig(nex) res =str(pre_num)+"."+str(nex_num) print(res) else: res =cn2dig(test_case) print(res) 二进制数中的1 题目中说的很清楚，见到了二进制最基础的算法一定是使用位操作，然后 &amp; 和 &gt;&gt; 是不可缺少的。这种算法的复杂度是O(n) ,n 表示二进制的长度。 123456789101112131415161718#include&lt;iostream&gt;using namespace std;// 遍历整个二进制的数组int count(Byte v)&#123; int count =0; while(v)&#123; if (v %2 ==1) &#123; count ++; &#125; v =v/2 &#125; &#125; 使用位操作代替求余和除二 12345678910int count2(Byte v)&#123; int count =0; while(v)&#123; count += v &amp; 0x01; v &gt;&gt;=1; &#125; &#125; 如果1出现的次数比较少， 这个从 O(N) -&gt; O(K) K 表示的 子串中1 的个数， N表示总的子串的长度。这种思想是想要使 二进制子串最高位的1 变成0，所以 n &amp;(n-1) 是一个很好的操作。 1234567891011121314int count3(Byte v)&#123; int count =0; while (v) &#123; v &amp;= (v-1);// 使得最大位的1变成0 ，在二进制的角度上 count +=1 ; &#125;&#125;// 使用空间代替时间, 这个是可以进行穷举的int count4(Byte v)&#123; int countTable [256] =&#123;0, 1,1,2...8&#125; int count =countTable[v]; return count;&#125; 给定一个数字N N的阶乘有多少个0? Tips: 数学问题，N 是可以通过质因数(2, 3, 5)进行分析 $N =(2^x +3^y+ 5^z) $， 然后0的个数是 $min(x, z) -&gt; z $, 最后就相当于每个数字的5的个数。 到最后是在统计 5 的个数，数字N 分解因式之后 5 的个数。 1234567891011int count(Byte， v)&#123; int count; for(int i =0; i&lt;=v; i++) &#123; while (i) &#123; count +=1; i /=5; &#125; &#125; return count;&#125; 123456789int count(Byte, v)&#123; int count =0; while (v &gt;4) &#123; count += v/5; v =v /5; &#125; return count;&#125; 求解 N! 中的二级制表示中最低位1的位置 Tips: 如果能够被2 整除表示二进制表示中是 0，否则就是1，所以只要找到能有多少个被 2整除的个数，然后这个个数+1 就ok了 -&gt; 这个如同就是求解 N！ 中含有质因数 2的个数 题目中关键时求解最低位1 的位置，所以前面都是0，那么在二进制中 0是很好判别的，如果能被整除，那么这个就是 0 123456789int count(int N)&#123; int res =0; while(N)&#123; N &gt;&gt;=1; res += N; &#125; return N;&#125; 寻找每个 ID，该ID 出现的频率是大于 0.5, 出现的次数大于总数的一半 Tips: 如果每次删除两个不同的ID，那么 最多的ID 出现的次数还是大于总数的一般，不断的重复这个过程。时间复杂度是O(n)，想法是上面那种，但是在操作的时候，并不是删除了某个ID。看代码是一清二楚的。 1234567891011121314151617181920212223242526Type Find(Type *ID, int N)&#123; Type candidate; int nTimes =0, i; for(i =0; i&lt; N; i++)&#123; if (nTimes ==0) &#123; candidate =ID[i]; nTimes =1 &#125; else &#123; if(ID[i] ==candidate)&#123; nTimes +=1; &#125; else&#123; nTimes -=1; &#125; &#125; &#125; // 这个应该还是需要最后判断一下 candidate 是不是出现了 [2/N] 次数，因为有一种情况是不存在的&#125; 精确表达浮点数 将小数使用分数的形式进行表示 这个是可以分成两个步骤，一个是将小数形式（整数、有限循环小数、无限循环小数）转成分数形式，然后是求解最大公约数的。还需要再看看，因为在无限循环小数那里不是很懂。小数可以表示成这样的形式： ，然后对于对于循环小数可以这样进行计算，好好体会一下对于无线循环小数，使用的 ($10^m -1 $) 作为分母。 $$\begin{split}X &amp;= 0 . a_1 a_2 \dots a_n ( b_1 b_2 \ldots b_m )&amp;= ( a_1 a_2 \ldots a_n + 0 . ( b_1 b_2 \dots b_m ) ) / 10 ^ { n}&amp;= ( ( a_1 a_2 \dots a_n ) ^ { * } ( 10 ^ { m } - 1 ) + ( b_1 b_2 \ldots b_m ) ) / ( ( 10 ^ { m } - 1 ) ^ { k } 10 ^ { n } )\end{split}$$ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101#include&lt;iostream&gt;#include&lt;stdio.h&gt;using namespace std;//最大公约数(greatest common divisor)// 辗转相除求解的最大公约数，默认第一个参数较大，第二个较小；这种方式的缺点在于减法的次数太多long gcd1(long x, long y)&#123; if (x&lt; y)&#123; return gcd1(y, x); &#125; if (y ==0) &#123; return x; &#125; else return gcd1(x-y, y);&#125;// 使用位运算 如果是偶数，进行优化bool IsEven(long x)&#123; if (x &amp; 0x01) &#123; return false; &#125; else return true;&#125;// 解法二主要是对于偶数进行了优化，使用递归实现long gcd2(long x, long y)&#123; if (x&lt;y) &#123; return gcd2(y, x); &#125; if (y ==0)&#123; return x; &#125; if(IsEven(x)) &#123; if (IsEven(y)) &#123; return (gcd2(x &gt;&gt;1, y &gt;&gt;1)&lt;&lt;1); &#125; else return gcd2(x&gt;&gt;1, y); &#125; else&#123; if(IsEven(y)) return gcd2(x, y&gt;&gt;1); else return gcd2(x-y, y); // 尽量做除法，是在不行就做减法 &#125;&#125;int main()&#123; long a =0, b =0, c =0; cin&gt;&gt;c&gt;&gt;a&gt;&gt;b; // c 是整数部分， a b 分别是有限小数 和 循环小数 if (a==0 &amp;&amp; b ==0) cout &lt;&lt;c; else&#123; long up =c; long down =1; long ta =a; while(ta) &#123; down *=10; ta /=10; &#125; up =c*down +a; if (b!=0) &#123; long wb =1; long tb =b; while(tb) &#123; wb *=10; tb /=10; &#125; up =up *(wb -1)+b ; down =down *(wb -1); &#125; long fac =gcd2(up, down); cout &lt;&lt;up/fac &lt;&lt;"/"&lt;&lt;down/fac&lt;&lt;endl; &#125;&#125; 尾递归尾调用定义: 在某个函数的最后一步调用另一个函数.进入下一个递归之后，该层次的递归已经结束，不需要使用栈空间进行保留信息。 123456function f(x) &#123; if (x &gt; 0) &#123; return m(x) &#125; return n(x); &#125; 上面代码中，函数m和n都属于尾调用，因为它们都是函数f的最后一步操作。不一定是最后一行。 同理是可以推广到尾递归。 12345678910111213141516function f() &#123; let m = 1; let n = 2; return g(m + n);&#125;f();// 等同于function f() &#123; return g(3);&#125;f();// 等同于g(3); 尾递归的意义在于防止栈溢出。因为递归的调用是非常消耗内存的，需要再栈中保存N 个调调用记录，很容易发生栈溢出 (stack overflow) 。但是对于为递归来说，只有一个调用记录，因为上一个已经结束，所以是不会发生 “栈溢出”错误的。 Fibonacci 数列最原始的解法，递归，其中有很多重复的计算子单元。 程序的开始就像百度的面试官那样说的，都是应该考虑一下特殊情况，有可能考虑不全，但是一定要有这个意识。到了递归中，不再是特殊情况这种事情，而是跳出条件。 12345678910111213141516int Fibonacci(int n)&#123; if (n &lt;0) &#123; return 0; &#125; else if(n ==1) &#123; return 1; &#125; else &#123; return Fibonacci(n -1) + Fibonacci(n -2); &#125; &#125; 解法二： 时空都是 O(N)，重点是从递归转成了循环。 123456789101112131415def Fibonacci(n ): if n ==0: return 0 elif n ==1: return 1 arr =[0] *(n+1) arr[0] =0 arr[1] =1 for i in range(2, n+1): arr[i] =arr[i-1] +arr[i-2] return arr[-1] 在上面的基础上进行优化，空间复杂度变成 O(1) a, b =b, a+b # 在python，连续赋值是从左往右的。也就是说， 先保存 b, 和 a+b，然后执行 a =b, b = a+b 所以在python 中交换两个数字可以写成这样： a, b = b, a (python 中是从左往右进行赋值的) 而在C 语言中是从右往左进行赋值的，所以注意这点。 1234567891011121314151617class Solution(object): def fib(self, N): """ :type N: int :rtype: int """ if N &lt;=0: return 0 elif N ==1: return 1 a, b =0, 1 for i in range(N ): a, b =b, a+b # 在python，连续赋值是从左往右的。 return a 寻找数组中的最大值和最小值时间复杂度是 O(N)，使用了两个值进行min_n 和max_n 的判断。 123456789101112131415161718192021222324252627282930313233343536#include&lt;iostream&gt;using namespace std;#define INF 10^9void FindMinMax(int a[], int size, int &amp;min, int &amp;max)&#123; max =-INF; min =INF; for (int i =0; i&lt;size -1; i++) &#123; // 这种判断方式 还是值得学习的，细节就是 index 的边界 if (a[i] &lt; a[i+1]) &#123; if(a[i+1]&gt; max ) max =a[i+1]; if(a[i] &lt; min) min =a[i]; &#125; else &#123; if(a[i] &gt; max) max =a[i] ; if(a[i+1] &lt; min) min =a[i+1]; &#125; &#125;&#125;int main()&#123; int arr[10]; int length =sizeof(arr)/ sizeof(arr[0]); int min, max; FindMinMax(arr, length, min, max); &#125; 寻找最近点对在二维平面上的n个点中，找出最接近的一对点第一次碰到这个题目的时候，还在想图的结构，现在看来只要是有array 的存在，那么这个是一定按照 array 的情况进行遍历求解。 解法一： 暴力法，时间复杂度是$O(n^2)$ 12345678910111213141516171819202122232425262728293031323334353637383940#include&lt;iostream&gt;#include&lt;cmath&gt;using namespace std;typedef struct Point&#123; float x, y;&#125;Point;float countDistance(Point a, Point b)&#123; return sqrt( (a.x -b.x)*(a.x -b.x) + (a.y- b.y)*(a.y -b.y));&#125;double MinDifference(Point point[], int n)&#123; double minDiff =0.0; double tmp =0.0; int firstIndex, secondIndex; for (int i =0; i&lt;n; i++) &#123; for (int j =i+1; j&lt;n ;j++) &#123; tmp =countDistance(point[i], point[j]); if (tmp&lt; minDiff) &#123; minDiff =tmp; firstIndex = i; secondIndex =j; &#125; &#125; &#125; return minDiff ;&#125; 解法二：分治法 参考这里, 最后的时间复杂度可以降低到 $O(nlogn)$，没有看懂。 使用分治法（divide and conquer）解题步骤： divide 将要解决的问题分成若干小规模的同类问题 conquer- 当子问题划分的足够小，能够使用较简单的方式解决 combine 将子问题的解逐层合并构成原问题的解 采用分而治之的思想，分成左右两个子集，$S_l$ 和$S_r$ ，然后分别计算两个子集之内的最小的距离，然后需要计算两个子集边界附近的点的距离。从算法步骤上讲，先存储点 point 这样的一个结构体，然后按照point 中的x 进行排序。 $\delta$ 是从左右两个子集中取得的点与点之间的最小距离，那么意味着两点之间的距离 最小是 $\delta $，所以在正方形 ($2\delta$, $2\delta$ ) 的区间内，左半部分的点到右半部分的点的距离最少是 $\delta$，根据割舍原理，那么最多只需要比较6个点。这里的是每个点，只需要个其他的6个点进行比较，而不是总共只有6个点。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137#include &lt;ctime&gt;#include &lt;cmath&gt;#include &lt;iostream&gt;#include &lt;algorithm&gt;using namespace std;#define INFINITE_DISTANCE 65535 // 无限大距离#define COORDINATE_RANGE 100.0 // 横纵坐标范围为[-100,100]#ifndef Closest_pairtypedef struct Point&#123;// 二维坐标上的点Point double x; double y;&#125;Point;double Distance(Point a, Point b)&#123;//平面上任意两点对之间的距离公式计算 return sqrt((a.x - b.x)*(a.x - b.x) + (a.y - b.y)*(a.y - b.y));&#125;bool compareX(Point a, Point b)&#123;//自定义排序规则：依照结构体中的x成员变量升序排序 return a.x &lt; b.x;&#125;bool compareY(Point a, Point b)&#123;//自定义排序规则：依照结构体中的x成员变量升序排序 return a.y &lt; b.y;&#125;float ClosestPair(Point points[], int length, Point &amp;a, Point &amp;b)&#123;// 求出最近点对记录，并将两点记录再a、b中 double distance; //记录集合points中最近两点距离 double d1, d2; //记录分割后两个子集中各自最小点对距离 int i = 0, j = 0, k = 0, x = 0; //用于控制for循环的循环变量 Point a1, b1, a2, b2; //保存分割后两个子集中最小点对 if (length &lt; 2) return INFINITE_DISTANCE; //若子集长度小于2，定义为最大距离，表示不可达 else if (length == 2) &#123;//若子集长度等于2，直接返回该两点的距离 a = points[0]; b = points[1]; distance = Distance(points[0], points[1]); &#125; else &#123;//子集长度大于3，进行分治求解 Point *pts1 = new Point[length]; //开辟两个子集 Point *pts2 = new Point[length]; sort(points, points + length, compareX); //调用algorithm库中的sort函数对points进行排序，compareX为自定义的排序规则 double mid = points[(length - 1) / 2].x; //排完序后的中间下标值，即中位数 for (i = 0; i &lt; length / 2; i++) pts1[i] = points[i]; for (int j = 0, i = length / 2; i &lt; length; i++) pts2[j++] = points[i]; d1 = ClosestPair(pts1, length / 2, a1, b1); //分治求解左半部分子集的最近点 d2 = ClosestPair(pts2, length - length / 2, a2, b2); //分治求解右半部分子集的最近点 if (d1 &lt; d2) &#123; distance = d1; a = a1; b = b1; &#125; //记录最近点，最近距离 else &#123; distance = d2; a = a2; b = b2; &#125; //merge - 进行子集合解合并 //求解跨分割线并在δ×2δ区间内的最近点对 Point *pts3 = new Point[length]; for (i = 0, k = 0; i &lt; length; i++) //取得中线2δ宽度的所有点对共k个 if (abs(points[i].x - mid) &lt;= distance) pts3[k++] = points[i]; sort(pts3, pts3 + k, compareY); // 以y排序矩形阵内的点集合 for (i = 0; i &lt; k; i++) &#123; if (pts3[j].x - mid &gt;= 0) // 只判断左侧部分的点 continue; x = 0; for (j = i + 1; j &lt;= i + 6 + x &amp;&amp; j &lt; k; j++) //只需与有序的领接的的6个点进行比较 &#123; if (pts3[j].x - mid &lt; 0) &#123;// 假如i点是位于mid左边则只需判断在mid右边的j点即可 x++; continue; &#125; if (Distance(pts3[i], pts3[j]) &lt; distance) &#123;//如果跨分割线的两点距离小于已知最小距离，则记录该距离和两点 distance = Distance(pts3[i], pts3[j]); a = pts3[i]; b = pts3[j]; &#125; &#125; &#125; &#125; return distance;&#125;void SetPoints(Point *points, int length)&#123;//随机函数对点数组points中的二维点进行初始化 srand(unsigned(time(NULL))); for (int i = 0; i &lt; length; i++) &#123; points[i].x = (rand() % int(COORDINATE_RANGE * 200)) / COORDINATE_RANGE - COORDINATE_RANGE; points[i].y = (rand() % int(COORDINATE_RANGE * 200)) / COORDINATE_RANGE - COORDINATE_RANGE; &#125;&#125;int main()&#123; int num; //随机生成的点对个数 Point a, b; //最近点对 double diatance; //点对距离 cout &lt;&lt; "请输入二维点对个数:"; cin &gt;&gt; num; if (num &lt; 2) cout &lt;&lt; "请输入大于等于2的点个数！！" &lt;&lt; endl; else &#123; cout &lt;&lt; endl &lt;&lt; "随机生成的" &lt;&lt; num &lt;&lt; "个二维点对如下：" &lt;&lt; endl; Point *points = new Point[num]; SetPoints(points, num); for (int i = 0; i &lt; num; i++) cout &lt;&lt; "(" &lt;&lt; points[i].x &lt;&lt; "," &lt;&lt; points[i].y &lt;&lt; ")" &lt;&lt; endl; diatance = ClosestPair(points, num, a, b); cout &lt;&lt; endl &lt;&lt; endl &lt;&lt; "按横坐标排序后的点对:" &lt;&lt; endl; for (int i = 0; i &lt; num; i++) cout &lt;&lt; "(" &lt;&lt; points[i].x &lt;&lt; "," &lt;&lt; points[i].y &lt;&lt; ")" &lt;&lt; endl; cout &lt;&lt; endl &lt;&lt; "最近点对为：" &lt;&lt; "(" &lt;&lt; a.x &lt;&lt; "," &lt;&lt; a.y &lt;&lt; ")和" &lt;&lt; "(" &lt;&lt; b.x &lt;&lt; "," &lt;&lt; b.y &lt;&lt; ")" &lt;&lt; endl &lt;&lt; "最近点对距离为：" &lt;&lt; diatance &lt;&lt; endl; &#125; system("pause");&#125;#endif // !Closest_pair 对比一下一维的情况。123456789101112131415161718double MinDifference(double arr[], int n)&#123; if (n &lt;2) return 0; sort(arr, arr+n); // sort(arr, arr+n myfunction) 降序 double minDiff =arr[1] - arr[0]; for (int i =2; i&lt;n;i++) &#123; double tmp =arr[i] -arr[i-1]; if (tmp &lt;minDiff) &#123; minDiff =tmp; &#125; &#125; return minDiff; &#125; 快速寻找满足条件的两个数 第一种解法： 排序之后 然后二分查找 O(nlogn) +O(logn) -&gt; O(nlogn) 在有序的数组中，必然使用二分进行查找，这样查找的效率从O(n) -&gt; O(log n) 123456789101112131415161718192021222324252627282930313233343536373839404142434445#include&lt;iostream&gt;#include&lt;algorithm&gt;using namespace std;#define MAXN 1001int arr[MAXN];bool findSum(int arr[], int n, int sum)&#123; int i =0; int j =n-1; while (i&lt;j) &#123; int tmp =arr[i] +arr[j]; if (tmp ==sum) &#123; printf("(%d, %d)", arr[i], arr[j]); return true; &#125; else if(tmp &lt;sum) i ++; else j --; &#125; return false;&#125;int main()&#123; int n, sum, i,j; cin &gt;&gt; n &gt;&gt;sum; for(i =0; i&lt;n;i++) cin &gt;&gt;arr[i]; sort(arr, arr+n); bool res =findSum(arr, n, sum); return 0;&#125; 第二种解法：使用辅助的空间 dictionary 存储，空间复杂度O(N) + 时间复杂度 O(N) 使用python 中的 dictionary， in 这种操作是非常具有可读性的。并且这种操作的时间复杂度是O(1) ,这个是不容置疑的，这个是字典的特性。 123456789101112131415def twoSum(arr, total): if not arr: return arr from collections import defaultdict dic =defaultdict(int) for num in arr: dic[num] = dic[num] +1 for key in dic: if total -key in dic: return (key, total -key) return (-1, -1) 子数组的最大乘积 给定一个长度为 N 的整数数组，只允许用乘法，不能用除法， 计算任意 ( N-1) 个数的组合中乘积最大的一组。 限制条件： 必须要选出 （N-1）个数字，只能使用乘法。 第一种解法：暴力求解，遍历出n 个，然后需要 n-1 次相乘，所以时间复杂度是 $O(N^2)$ 1234567891011121314151617181920212223242526#include&lt;iostream&gt;using namespace std;class Solution &#123;public:int maxProduct(int *nums) &#123; int res; int max; int i, j; int n= sizeof(nums) /sizeof(nums[0]); for(i =0; i&lt;n; i++) &#123; for(j =0; j&lt;n; j++) // 使用 for 循环的形式将子集相乘 &#123; res =1; res *= ( (i==j) ? 1: nums[j]); if (res ==0) break; &#125; max =( i==0 ? res: max); // 实际上是对于 max 的出释怀 max =( max &lt; res? res: max); // 这种语法是比较简洁的 &#125; return max;&#125;&#125;​ 上述方式进行了很多重复的运算，可以保存下来子问题，减少时间复杂度。实现的时候，两个list 分别从左右两边进行累乘运算。所以第二种解法：时间复杂度从 $O(n^2) $ 降低为 $ O(n)$​1234567891011121314151617181920212223242526272829303132333435363738394041424344454647#include&lt;iostream&gt;using namespace std;int maxProduct(int *arr, int len)&#123; if (arr ==NULL | len&lt;0) return NULL ; int *p =new int[len+1]; // 申请的数组得 长度，加上一个防止越界之类的额 int *q =new int[len+1]; // 这里为什么多申请了一个 和c 一样都是有一个\0 吗, 个人觉得这个是没有必要的 // 指针是在堆上建立的空间，如果是在 main 主函数里面，一定要使用 del [] p; 删除了结点，否则容易溢出 p[0] =1; // 实现的是累乘 for (int i =1; i&lt;=len; i++) &#123; p[i] =p[i-1] *arr[i-1]; &#125; q[len] =1; for (int i =len-1; i&gt;=0; i --) &#123; q[i]= q[i+1] *arr[i] ; &#125; int max = -0x10000; for (int i =0; i&lt;len; i++) &#123; int tmp =p[i] *q[i+1]; max = (max &gt; tmp)? max : tmp; &#125; return max;&#125;int main()&#123; int arr[6] =&#123;1, 4, -2, 5, -3, 3&#125;; int len =sizeof(arr)/sizeof(arr[0]); int res =maxProduct(arr, len); cout &lt;&lt; "max product :" &lt;&lt; res &lt;&lt; endl; return 0;&#125; 最长递增子序列 第一种方式，时间复杂度是 O($n^2$),思想是dp. dp[i] 表示 到目前为止前i 个数字中的 最长递增子序列。 12345678910111213141516171819202122232425#include&lt;iostream&gt;#include&lt;algorithm&gt; // cpp 中的max() 函数是在 algorithm 里面，输入是两个数字#include&lt;vector&gt; // cpp 中的vector 还是比较好用的 vector 跟python 中的list 差不多using namespace std; int lengthOfLIS(vector&lt;int&gt;&amp; nums) &#123; vector&lt;int&gt; dp(nums.size(), 1); int res = 0; for (int i = 0; i &lt; nums.size(); ++i) &#123; for (int j = 0; j &lt; i; ++j) &#123; if (nums[i] &gt; nums[j]) &#123; dp[i] = max(dp[i], dp[j] + 1); &#125; &#125; res = max(res, dp[i]); &#125; return res;&#125;int main()&#123; vector&lt;int&gt; nums = &#123;10,9,2,5,3,7,101,18&#125;; int result =lengthOfLIS(nums); cout &lt;&lt; result &lt;&lt; endl;&#125; 上面的时间复杂度是O($ n^2$), 在于每次都是遍历了之前所有的情况 (dp) ，还有一种方式创建一个辅助数组，用于存储之前已经排好序的元素，遍历的时候使用二分查找进行遍历。于是第二种解法就出来了，时间复杂度是 O(n logn). 这个辅助函数不是从原来的数组不是最长增数组本身，这个函数只能用于计算最后的长度，得不到数组。 123456789101112131415161718192021222324252627282930313233343536373839404142#include&lt;iostream&gt;#include&lt;algorithm&gt;#include&lt;vector&gt;using namespace std;int findFirstBigger(vector&lt;int&gt; h,int left,int right,int target)&#123; if(left==right) return left; int mid=(left+right)/2; if(h[mid]&lt;target) return findFirstBigger(h,mid+1,right,target); else return findFirstBigger(h,left,mid,target);&#125; int lengthOfLIS(vector&lt;int&gt; nums) &#123; if(nums.empty()||nums.size()==0) return 0; //int[] h=new int[nums.length]; vector&lt;int&gt; h(nums.size(), 0); h[0]=nums[0]; int max=0;//最长子序列最右边的位置 for(int i=1;i&lt;nums.size();i++)&#123; if(nums[i]&gt;h[max])&#123; h[++max]=nums[i]; // continue; 这句话是没有用的，因为 if else 本身就是一种排他的 &#125; else&#123; int pos=findFirstBigger(h,0,max,nums[i]); h[pos]=nums[i]; &#125; &#125; return max+1;&#125;int main()&#123; vector&lt;int&gt; nums =&#123;2,1,6,4,5,2,7,4&#125;; int res =lengthOfLIS(nums); cout &lt;&lt; res&lt;&lt; endl; return 0;&#125; 数组循环移位 暴力求解 时间复杂度是O($KN$) K表示移位的次数 N 表示数组的长度 C++ 解法 12345678910111213141516171819202122232425#include&lt;iostream&gt;#include&lt;vector&gt;using namespace std;void rightShift(vector&lt;int&gt; arr, int n , int k)&#123; if (arr.empty() or arr.size() ==0) return ; if (k&gt;n) k =k%n; while(k --) &#123; int temp =arr[n-1]; for(int i =n-1; i&gt;0; i--) arr[i] =arr[i-1]; arr[0] =temp; &#125; &#125;int main()&#123; return 0;&#125; C语言解法 123456789101112131415161718void rightShift(int *arr, int n, int k)&#123; if (arr ==NULL or n ==0) return ; if (k&gt;n) k =k%n; while(k --) &#123; int temp =arr[n-1]; for (int i =n-1; i&gt;0; i--) &#123; arr[i] =arr[i-1]; &#125; arr[0] =temp; &#125;&#125; 解法二： 原来序列 abcd1234 转换成 1234abcd ，分成两部分， 1234 和abcd 看成两个整体，右移就是把数组的两个部分交换一下通过以下的方法进行实现， 逆序1234, 逆序abcd , 逆序整个数组, 整个arr 的 reverse 时间复杂度：这个不是递归，只是同一个函数调用了三次，时间复杂度分别是 O($ \frac{k}{2}$), O( $\frac{N-k}{2}$) 和 O($\frac{N}{2}$)，所以总的时间复杂度是 O(N) 12345678910111213141516void reverse(int *arr, int b, int e)&#123; for (; b&lt;e; b++, e--) &#123; int temp =arr[e]; arr[e] =arr[b]; arr[b] =temp; &#125;&#125;void rightShift(int *arr, int n, int k)&#123; k %=n; reverse(arr, 0, n-k-1); reverse(arr, n-k , n-1); reverse(arr, 0, n-1);&#125; 数组分割?题目概述：有一个没有排序，元素个数为2N的正整数数组。要求把它分割为元素个数为N的两个数组，并使两个子数组的和最接近。 这里的dp[k][s]表示从前k个数中取k个数，且k不超过n，且这些数之和为s的取法是否存在。 算法时间复杂度是 $O(N^2*sum)$ 讲解可以参考这个，反正我是没有很看动画https://blog.csdn.net/linyunzju/article/details/7729774 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354#include&lt;iostream&gt;#include&lt;algorithm&gt;using namespace std;#define MAXN 101#define MAXSUM 100000int A[MAXN];bool dp[MAXN][MAXSUM];// 使用全局变量 解决定义二维的数组的问题void find(int *a, int n, int sum)&#123; int k1, k2, s; for (k1 =1; k1&lt;2*n; k1++) &#123; for(k2 =min(k1, n) ;k2&gt;=1; k2--) &#123; for(s =1; s&lt;=sum/2; s++) &#123; if (s &gt;=A[k1] &amp;&amp; dp[k2-1][s-A[k1]]) dp[k2][s] =true; &#125; &#125; &#125; for (s =sum/2; s&gt;=1 &amp;&amp; !dp[n][s] ; s--) &#123; cout &lt;&lt; sum-2*s&lt;&lt; endl; &#125;&#125;int main()&#123; int n, i; cin &gt;&gt; n; for(i=1; i&lt;=2*n; i++) &#123; cin &gt;&gt; A[i]; &#125; int sum =0; for (i =1; i&lt;=2*n; i++) &#123; sum += A[i]; &#125; memset(A, 0, sizeof(dp)); dp[0][0] =true; find(A, n, sum); return 0;&#125; 只考加法的面试题 写一个程序，对于一个 64位正整数，输出所有可能的连续（两个以上）自然数之和的算式 因为这里涉及到了连续自然数，那么一定要使用等差数列，然后进行枚举。在时间复杂度 O(n) 内解决问题。 数学问题 设存在连续自然数，首项为$A_1$，项数为m，即$$ A_1 + A_2 + \ldots + A_m = { M }$$ 使用求和公式 $$A_1 \times m + \frac{m \times ( m - 1 )}{2} = M$$可以整理得到关于 $m$ 的一元二次方程方程 $$ m^2 +2\times( A_1 -1) \times m -2 \times M =0$$ 解得 $$m = \frac{ (1- 2 \times A_1) \pm \sqrt{( 4*(A_1-1)^2 +8 \times M}}{2}$$ 所以要求 m 是整数，那么需要满足两个条件： $\sqrt{( 4*(A_1-1)^2 +8 \times M}$ 是整数 整个分子是偶数 1234567891011121314151617181920212223242526#include&lt;iostream&gt;#include&lt;cmath&gt;using namespace std;int main()&#123; int n; while (cin &gt;&gt; n) &#123; for (int i =0; i&lt;n/2; i ++) &#123; int a =i; int w =(2*a -1)*(2*a -1) +8*n; int k =(int)sqrt(w); if (k*k != w) continue; int m =k +1-2*a; if (m%2 ==1) continue; else cout &lt;&lt; i &lt;&lt; " "&lt;&lt;i+m/2-1 &lt;&lt; endl; &#125; &#125; return 0;&#125; 字符串移位包含的问题 这种移位并没有说明怎么移位，然后移动几位，所以应该是不能模拟出来的，下面是有一个既定的结论，如果在移位中包含，那么一定是在 strstr 这个中的。于是解法一就出来了。 补充cpp 的一些语法： 1 和true是等价的 字符串和字符数组的定义和初始化 直接进行初始化定义：12char *a ="string1";char b[] ="string2"; 局部变量 a b 都是在栈中，a 是指向了一个常亮， “string1” 而b 是指向了了一个数组。实际上前者的赋值是错误的，因为是其实不可改变的。使用 const char * a=”string1” 更加合适。char 类型转换成 string类型 cpp 中使用 string 第一种方式 12345678910#include&lt;iostream&gt;#include&lt;cstring&gt;using namespace std;int main()&#123; string str; str ="hello world"; cout &lt;&lt; str &lt;&lt;endl; return 0;&#125; 这种是 cpp 中使用string 的第二种方式 1234567891011#include&lt;iostream&gt;using namespace std;int main()&#123; std:: string str; str ="hello world"; cout &lt;&lt; str &lt;&lt;endl; return 0;&#125; 问题的解法： 1234567891011121314151617181920212223242526272829#include&lt;iostream&gt;using namespace std;bool isContain(char *s1, char *s2) &#123; int len = strlen(s1); //假设s1位较长的字符串 char s[len * 2]; for(int i = 0; i &lt; 2; i++) &#123; for(int j = 0; j &lt; len; j++) &#123; s[i * len + j] = s1[j]; &#125; &#125; if(strstr(s, s2) != NULL) &#123; return true; &#125; else &#123; return false; &#125;&#125;int main()&#123; char* s1 ="hello"; char* s2 ="hel"; bool result; result =isContain(s1, s2); cout &lt;&lt; isContain(s1, s2) &lt;&lt;endl; return 0;&#125; 解法二： python 实现，时间复杂度是O(n) 空间复杂度是(1) ，使用取模运算 123456789101112131415161718192021def match(src, des): if not src or not des: return False len1 =len(src) len2 =len(des) if len1 ==0 and len2 ==0: return True for i in range(len1): if des[0] == src[i]: for j in range(1, len2): if des[j] != src[ (i+j)%len1]: break else: return True return False Letter Combinations of a Phone Number 这个是做过的，深度优先算法 12345678910111213141516171819202122232425262728293031323334class Solution: def letterCombinations(self, digits): if not digits or digits == "": return [] # 命名很到位，任何的dictionary 都是可以使用 maps 进行命名的 # 注意从 list 到tuple 减少了内存的使用 maps =&#123; '1': (), '0': (), '2': ('a', 'b', 'c'), '3': ('d', 'e', 'f'), '4': ('g', 'h', 'i'), '5': ('j', 'k', 'l'), '6': ('m', 'n', 'o'), '7': ('p', 'q', 'r', 's'), '8': ('t', 'u', 'v'), '9': ('w', 'x', 'y', 'z') &#125; results = [""] for digit in digits: tuple1 = maps[digit] tmp =[] if len(tuple1) == 0: continue # 二重循环， results 是之前的结果，然后每次都是要在其基础上添加一些东西 for prefix in results: for suffix in tuple1: tmp.append(prefix + suffix) results = tmp return results 求一维子数组的最大和 题目：输入一个整形数组，数组里有正数也有负数。数组中连续的一个或多个整数组成一个子数组，每个子数组都有一个和。求所有子数组的和的最大值。要求时间复杂度为O(n)。 123456789101112131415161718192021222324252627282930313233343536373839404142434445#include&lt;iostream&gt;#include&lt;cstring&gt;#define MAX 10# define MAX_NUM 100000using namespace std;int max_array(int arr[], int n)&#123; int max_num= -MAX_NUM; int cur_num =0; for (int i =0; i&lt;n; i++) &#123; cur_num += arr[i]; if (cur_num &lt; 0) cur_num =0; if( cur_num &gt; max_num) max_num =cur_num; &#125; if (max_num ==0) &#123; max_num =arr[0]; for(int i =1; i&lt;n;i ++) if (arr[i] &gt;max_num) max_num =arr[i]; &#125; return max_num;&#125;int main()&#123; int n; int arr[MAX]; int max_num ; cout &lt;&lt; "input number of array"; cin &gt;&gt; n; for(int i =0; i&lt;n ;i++) &#123; cin &gt;&gt; arr[i]; &#125; max_num =max_array(arr, n); cout &lt;&lt; max_num; return 0;&#125; 二维子数组最大和Tips: 通过枚举矩阵的上下界，然后再用一维情况的方法确定左右边界，就可以得到二维问题的解。二维数组是$ (m \times n) $， 那么该方法的时间复杂度是 $O( n^2 \times m) $.当然也可以枚举左右边界，然后使用一维情况去确定上下界，所以时间复杂度是 $ O( m \times n \times min(m ,n) )$. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162#include&lt;iostream&gt;#include&lt;cstdio&gt;#include&lt;ctime&gt;using namespace std;#define M 4#define N 4#include &lt;memory.h&gt;int maxSubArray(int *arr, int len) //最大子序列和&#123; int i, sum = arr[0], b = 0; for (i = 0; i&lt;len; ++i) &#123; if (b&gt;0) b += arr[i]; else b = arr[i]; if (b&gt;sum) sum = b; &#125; return sum;&#125;int maxSubMatrix(int n, int m, int array[M][N])&#123; int i, j, h, max, sum = -100000; int b[100]; for (i = 0; i &lt; n; i++) &#123; memset(b, 0, sizeof(b)); //初始化b[] for (j = i; j &lt; n; j++) //把第i行到第j行相加,对每一次相加求出最大值 &#123; for (h = 0; h&lt;m; h++) &#123; b[h] += array[j][h]; //二维数组压缩成一维数组，然后求最大子序列和 &#125; max = maxSubArray(b, h); if (max&gt;sum) sum = max; &#125; &#125; return sum;&#125;int main()&#123; int arr[M][N] = &#123; &#123; -15, -21,5, -12 &#125;, &#123; -7, 21, 20, 12 &#125;, &#123; 21, 0, -1, 13 &#125;, &#123; 10, 20, -10, -18 &#125; &#125;; cout &lt;&lt; "随机二维数组为：" &lt;&lt; endl; //srand(time(0)); //for (int i = 0; i &lt; M; i++) //&#123; // for (int j = 0; j &lt; N; j++) // &#123; // arr[i][j] = rand() % 50 - 25; // cout &lt;&lt; arr[i][j] &lt;&lt; " "; // &#125; // cout &lt;&lt; endl; //&#125; cout &lt;&lt; maxSubMatrix(M, N, arr) &lt;&lt; endl; system("pause"); return 0;&#125; 使用C++ 中的vector 进行改写，思路正确，但结果出错。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970#include&lt;iostream&gt;#include&lt;vector&gt;#include&lt;algorithm&gt;using namespace std;#define M 4#define N 4#define MAX 100000int maxSubarray2(vector&lt;int&gt; arr, int n)&#123; int max_num= -MAX; int cur_num =0; for (int i =0; i&lt;n; i++) &#123; cur_num += arr[i]; if (cur_num &lt; 0) cur_num =0; if( cur_num &gt; max_num) max_num =cur_num; &#125; // 这个步骤是处理整数中有正有负的情况 /* if (max_num ==0) &#123; max_num =arr[0]; for(int i =1; i&lt;n;i ++) if (arr[i] &gt;max_num) max_num =arr[i]; &#125; */ return max_num;&#125;int maxSubmatrix(int rows, int cols, vector&lt;vector&lt;int&gt;&gt; arr)&#123; int i, j, k, max=arr[0][0], sum =-10000; vector&lt;int&gt; b; for (i =0;i&lt; rows;i++) &#123; vector &lt;int&gt; b(cols, 0) ; for (j =i; j&lt; rows; j++) &#123; for(k =0;k &lt;cols;k++) b[k] += arr[j][k]; sum =maxSubarray2(b, k); if(sum &gt; max) max =sum; &#125; &#125; return max;&#125;int main()&#123; vector&lt;vector&lt;int&gt;&gt; arr = &#123; &#123; -15, -21,5, -12 &#125;, &#123; -7, 21, 20, 12 &#125;, &#123; 21, 0, -1, 13 &#125;, &#123; 10, 20, -10, -18 &#125; &#125;; //cout &lt;&lt; arr[1][1]&lt;&lt;endl; cout &lt;&lt; maxSubmatrix(M, N, arr) &lt;&lt; endl; vector&lt;int&gt; tmp =&#123;2, 0, -4, 9, -1,2&#125;; cout &lt;&lt; tmp.size()&lt;&lt;endl; cout&lt;&lt; maxSubarray2(tmp, tmp.size())&lt;&lt; endl; return 0;&#125;]]></content>
      <categories>
        <category>算法</category>
      </categories>
  </entry>
</search>
