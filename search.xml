<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[NLP Papers Reading-Sentence Embedding]]></title>
    <url>%2F2019%2F06%2F01%2Fnlp-papers-reading-sentence-embedding%2F</url>
    <content type="text"><![CDATA[Why Consider Sentence Embedding?One simple way you could do this is by generating a word embedding for each word in a sentence, adding up all the embeddings and divide by the number of words in the sentence to get an â€œaverageâ€ embedding for the sentence. Alternatively, you could use a more advanced method which attempts to add a weighting function to word embeddings which down-weights common words. This latter approach is known as Smooth Inverse Frequency (SIF). These methods can be used as a successful proxy for sentence embeddings. However, this â€œsuccessâ€ depends on the dataset being used and the task you want to execute. So for some tasks these methods could be good enough However, there are a number of issues with any of these types of approaches: They ignore word ordering. So your product is easy to use, I do not need any help is identical to I do need help, your product is not easy to use. This is obviously problematic. Itâ€™s difficult to capture the semantic meaning of a sentence. The word crash can be used in multiple contexts, e.g. I crashed a party, the stock market crashed, or I crashed my car. Itâ€™s difficult to capture this change of context in a word embedding. Sentence length becomes problematic. With sentences we can chain them together to create a long sentence without saying very much, The Philadelphia Eagles won the Super Bowl, The Washington Post reported that the Philadelphia Eagles won the Super Bowl, The politician claimed it was fake news when the Washington Post reported that the Philadelphia Eagles won the Super Bowl, and so on. All these sentences are essentially saying the same thing but if we just use word embeddings, it can be difficult to discover if they are similar. They introduce extra complexity. When using word embeddings as a proxy for sentence embeddings we often need to take extra steps in conjunction with the base model. For example, we need to remove stop words, get averages, measure sentence length and so on. sentence embeddingçš„å¼•ç”¨åœºæ™¯ï¼šSimilar approaches can be used to go beyond representations and semantic search, to document classification and understanding and eventually document summarizing or generation. Words Embedå¹³å‡è¯å‘é‡ä¸ŽTFIDFåŠ æƒå¹³å‡è¯å‘é‡SIFåŠ æƒå¹³å‡è¯å‘é‡æ¥è‡ªè®ºæ–‡ A simple but tough-to-beat baseline for sentence embeddingsï¼Œæ›´å¤šä¿¡æ¯å¯ä»¥å‚è€ƒè¿™é‡Œã€‚ åˆ©ç”¨n-grams embeddingfasttext ä»‹ç»ã€‚ DANï¼ˆDeep Unordered Composition Rivals Syntactic Methods for Text Classificationï¼‰å…¶å®žDAN(Deep Averaging Networks)åº”è¯¥å±žäºŽBag of Wordsç±»çš„ç®—æ³•ã€‚å› ä¸ºæ¯”è¾ƒç‰¹æ®Šï¼Œå•ç‹¬åˆ—å‡ºæ¥ã€‚ å®ƒæ˜¯åœ¨å¯¹æ‰€æœ‰è¯è¯­å–å¹³å‡åŽï¼Œåœ¨ä¸Šé¢åŠ ä¸Šå‡ å±‚ç¥žç»ç½‘ç»œã€‚ç‰¹æ®Šçš„åœ°æ–¹åœ¨äºŽå®ƒåœ¨sentiment analysisä¸­è¡¨çŽ°ä¹Ÿä¸é”™ï¼Œè¿™åœ¨BOWç±»æ–¹æ³•ä¸­æ¯”è¾ƒç½•è§ã€‚ æ–‡ä¸­æå‡ºäº†DAN(Deep average network)ï¼Œè¯´ç™½äº†å°±æ˜¯å¯¹äºŽä¸€ä¸ªå¥å­æˆ–è€…ä¸€ä¸ªæ®µè½ï¼ŒæŠŠæ¯ä¸ªå•è¯çš„embeddingè¿›è¡Œå¹³å‡æ±‚å’Œï¼Œå¾—åˆ°ç¬¬ä¸€å±‚å›ºå®šç»´åº¦çš„å‘é‡ï¼Œç„¶åŽåœ¨å¥—å‡ å±‚å…¨è¿žæŽ¥ç¥žç»ç½‘ç»œã€‚æœ¬è´¨æ¥è®²ï¼Œè¿™ä¸ªæ¨¡åž‹æ²¡æœ‰è€ƒè™‘å•è¯ä¹‹é—´çš„é¡ºåºï¼Œnotåœ¨ç¬¬ä¸€ä¸ªä½ç½®å’Œåœ¨æœ€åŽä¸€ä¸ªä½ç½®å¯¹äºŽDANæ¥è®²è¾“å…¥éƒ½æ˜¯ä¸€æ ·çš„ï¼Œæ‰€ä»¥è‡ªç„¶converä¸ä½è¿™ç§æƒ…å†µã€‚è¿™æ˜¯æ¨¡åž‹æœ¬èº«çš„é—®é¢˜ï¼Œæ²¡æœ‰åŠžæ³•æ”¹è¿›ï¼Œé™¤éžæ¢æ¨¡åž‹ï¼Œæ¯”å¦‚textcnnå°±èƒ½å¾ˆå¥½çš„è§£å†³è¿™ç§æƒ…å†µå¯¹äºŽå¦å®šè¯æ•æ„Ÿï¼Œæ¯”å¦‚but,notç­‰ï¼Œå¸¸å¸¸åˆ¤æ–­ä¸ºnegativeã€‚è®­ç»ƒé€Ÿåº¦å¿«ï¼Œä¸”ç»“æžœè¾ƒå¥½ï¼Œå’ŒSyntactic Compositionæ€§èƒ½å·®ä¸å¤šï¼Œä½†æ˜¯æ¶ˆè€—çš„è®¡ç®—èµ„æºå°‘ä½œä¸ºæœ‰ç›‘ç£å­¦ä¹ ä»»åŠ¡æ¥è®²ï¼Œå¯ä»¥è¯•ä¸€è¯•ã€‚ä½†æ˜¯ç”±äºŽå…¨è¿žæŽ¥å±‚ï¼Œæ— æ³•è¿›è¡Œæ— ç›‘ç£å­¦ä¹ ã€‚ç›¸åï¼ŒNBOWå¯ä»¥æ— ç›‘ç£å­¦ä¹ ï¼Œæ¯”å¦‚æ–‡æœ¬ç›¸ä¼¼åº¦è®¡ç®—ç­‰ã€‚å½“ç„¶ã€‚å¯¹äºŽDANè€Œè¨€ï¼Œå¯ä»¥é€šè¿‡è¿ç§»å­¦ä¹ ï¼Œé¢„è®­ç»ƒå¥½å…¨è¿žæŽ¥å‚æ•°ï¼Œå®žçŽ°æ— ç›‘ç£å­¦ä¹  æ€»ç»“ä¸€ä¸‹ï¼šå¯¹ç®€å•çš„ä»»åŠ¡æ¥è¯´ï¼Œç”¨ç®€å•çš„ç½‘ç»œç»“æž„è¿›è¡Œå¤„ç†åŸºæœ¬å°±å¤Ÿäº†ï¼Œä½†æ˜¯å¯¹æ¯”è¾ƒå¤æ‚çš„ä»»åŠ¡ï¼Œè¿˜æ˜¯ä¾ç„¶éœ€è¦æ›´å¤æ‚çš„ç½‘ç»œç»“æž„æ¥å­¦ä¹ sentence representationçš„ã€‚ Unsupervised Sentence EmbedåŸºäºŽEncoder-decoderçš„Skip-Thought VectorsContinuing the tour of older papers that started with our ResNet blog post, we now take on Skip-Thought Vectors by Kiros et al. Their goal was to come up with a useful embedding for sentences that was not tuned for a single task and did not require labeled data to train. They took inspiration from Word2Vec skip-gram (you can find my explanation of that algorithm here) and attempt to extend it to sentences. Changing a single word has had almost no effect on the meaning of that sentence. To account for these word level changes, the skip-thought model needs to be able to handle a large variety of words, some of which were not present in the training sentences. The authors solve this by using a pre-trained continuous bag-of-words (CBOW) Word2Vec model and learning a translation from the Word2Vec vectors to the word vectors in their sentences. Below are shown the nearest neighbor words after the vocabulary expansion using query words that do not appear in the training vocabulary: è®ºæ–‡æè¿°äº†ä¸€ç§é€šç”¨ã€åˆ†å¸ƒå¼å¥å­ç¼–ç å™¨çš„æ— ç›‘ç£å­¦ä¹ æ–¹æ³•ã€‚ä½¿ç”¨ä»Žä¹¦ç±ä¸­æå–çš„è¿žç»­æ–‡æœ¬ï¼Œè®­ç»ƒäº†ä¸€ä¸ªç¼–ç å™¨-è§£ç å™¨æ¨¡åž‹ï¼Œå€Ÿé‰´äº†word2vecä¸­skip-gramæ¨¡åž‹ï¼Œé€šè¿‡ä¸€å¥è¯æ¥é¢„æµ‹è¿™å¥è¯çš„ä¸Šä¸€å¥å’Œä¸‹ä¸€å¥ã€‚è¯­ä¹‰å’Œè¯­æ³•å±žæ€§ä¸€è‡´çš„å¥å­è¢«æ˜ å°„åˆ°ç›¸ä¼¼çš„å‘é‡è¡¨ç¤ºã€‚æŽ¥ç€å¼•å…¥ä¸€ä¸ªç®€å•çš„è¯æ±‡æ‰©å±•æ–¹æ³•æ¥ç¼–ç ä¸å†è®­ç»ƒé›†å†…çš„å•è¯ï¼Œä»¤è¯æ±‡é‡æ‰©å±•åˆ°ä¸€ç™¾ä¸‡è¯ã€‚æœ¬æ–‡çš„æ¨¡åž‹è¢«ç§°ä¸ºskip-thoughtsï¼Œç”Ÿæˆçš„å‘é‡ç§°ä¸ºskip-thought vectorã€‚æ¨¡åž‹é‡‡ç”¨äº†å½“ä¸‹æµè¡Œçš„ç«¯åˆ°ç«¯æ¡†æž¶ï¼Œé€šè¿‡æœé›†äº†å¤§é‡çš„å°è¯´ä½œä¸ºè®­ç»ƒæ•°æ®é›†ï¼Œå°†å¾—åˆ°çš„æ¨¡åž‹ä¸­encoderéƒ¨åˆ†ä½œä¸ºfeature extractorï¼Œå¯ä»¥ç»™ä»»æ„å¥å­ç”Ÿæˆvectorã€‚ skip-thoughtæ¨¡åž‹ç»“æž„å€ŸåŠ©äº†skip-gramçš„æ€æƒ³ã€‚åœ¨skip-gramä¸­ï¼Œæ˜¯ä»¥ä¸­å¿ƒè¯æ¥é¢„æµ‹ä¸Šä¸‹æ–‡çš„è¯ï¼›åœ¨skip-thoughtåŒæ ·æ˜¯åˆ©ç”¨ä¸­å¿ƒå¥å­æ¥é¢„æµ‹ä¸Šä¸‹æ–‡çš„å¥å­ã€‚ è®ºæ–‡é‡‡ç”¨äº†GRU-RNNä½œä¸ºencoderå’Œdecoderï¼Œencoderéƒ¨åˆ†çš„æœ€åŽä¸€ä¸ªè¯çš„hiddenstateä½œä¸ºdecoderçš„è¾“å…¥æ¥ç”Ÿæˆè¯ã€‚è¿™é‡Œç”¨çš„æ˜¯æœ€ç®€å•çš„ç½‘ç»œç»“æž„ï¼Œå¹¶æ²¡æœ‰è€ƒè™‘å¤æ‚çš„å¤šå±‚ç½‘ç»œã€åŒå‘ç½‘ç»œç­‰æå‡æ•ˆæžœã€‚decoderéƒ¨åˆ†ä¹Ÿåªæ˜¯ä¸€ä¸ªè€ƒè™‘äº†encoder last hidden stateçš„è¯­è¨€æ¨¡åž‹ï¼Œå¹¶æ— å…¶ä»–ç‰¹æ®Šä¹‹å¤„ï¼Œåªæ˜¯æœ‰ä¸¤ä¸ªdecoderï¼Œæ˜¯ä¸€ä¸ªone maps twoçš„æƒ…å†µï¼Œä½†è®¡ç®—æ–¹æ³•ä¸€æ ·ã€‚æ¨¡åž‹ä¸­çš„ç›®æ ‡å‡½æ•°ä¹Ÿæ˜¯ä¸¤ä¸ªéƒ¨åˆ†ï¼Œä¸€ä¸ªæ¥è‡ªäºŽé¢„æµ‹ä¸‹ä¸€å¥ï¼Œä¸€ä¸ªæ¥è‡ªäºŽé¢„æµ‹ä¸Šä¸€å¥ã€‚æˆ‘ä»¬å°†æž„é€ ä¸€ä¸ªç±»ä¼¼äºŽè‡ªç¼–ç å™¨çš„åºåˆ—åˆ°åºåˆ—ç»“æž„ï¼Œä½†æ˜¯å®ƒä¸Žè‡ªç¼–ç å™¨æœ‰ä¸¤ä¸ªä¸»è¦çš„åŒºåˆ«ã€‚ç¬¬ä¸€ï¼Œæˆ‘ä»¬æœ‰ä¸¤ä¸ª LSTM è¾“å‡ºå±‚ï¼šä¸€ä¸ªç”¨äºŽä¹‹å‰çš„å¥å­ï¼Œä¸€ä¸ªç”¨äºŽä¸‹ä¸€ä¸ªå¥å­ï¼›ç¬¬äºŒï¼Œæˆ‘ä»¬ä¼šåœ¨è¾“å‡º LSTM ä¸­ä½¿ç”¨æ•™å¸ˆå¼ºè¿«ï¼ˆteacher forcingï¼‰ã€‚è¿™æ„å‘³ç€æˆ‘ä»¬ä¸ä»…ä»…ç»™è¾“å‡º LSTM æä¾›äº†ä¹‹å‰çš„éšè—çŠ¶æ€ï¼Œè¿˜æä¾›äº†å®žé™…çš„å‰ä¸€ä¸ªå•è¯ï¼ˆå¯åœ¨ä¸Šå›¾å’Œè¾“å‡ºæœ€åŽä¸€è¡Œä¸­æŸ¥çœ‹è¾“å…¥ï¼‰ã€‚ çœ‹ä¸ŠåŽ»ï¼ŒSkip-thoughtå’ŒSkip-gramæŒºè±¡ã€‚å”¯ä¸€çš„é—æ†¾æ˜¯Skip-thoughtçš„decoderé‚£éƒ¨åˆ†ï¼Œå®ƒæ˜¯ä½œä¸ºlanguage modelingæ¥å¤„ç†çš„. ä»Žè¿™é‡Œçš„è®²è§£çŸ¥é“è¿™ä¸ªæ˜¯ä¸å­˜åœ¨ â€æ­£è´Ÿâ€œæ ·æœ¬çš„ï¼Œ è¿™ä¸ªçš„æŸå¤±å‡½æ•°æ˜¯ æ­£ç¡®çš„ä¸Šä¸‹å¥å’Œç”Ÿæˆçš„ä¸Šä¸‹å¥ä¹‹é—´çš„reconstruction errorã€‚ The end product of Skip-Thoughts is the Encoder. The Decoders are thrown away after training. The trained encoder can then be used to generate fixed length representations of sentences which can be used for several downstream tasks such as sentiment classification, semantic similarity, etc. The encoder utilises a word embedding layer that serves as a look up table. This converts each word in the input sentence to its corresponding word embedding, effectively converting the input sentence into a sequence of word embeddings. This embedding layer is also shared with both of the decoders. The model is then trained to minimise the reconstruction error of the previous and next sentences using the resulting embedding h(i) generated from sentence s(i) after it is passed through the encoder. Back propagating the reconstruction error from the decoder allows the encoder to learn the best representation of the input sentence while capturing the relation between itself and the surrounding sentences.Skip-Thoughts is designed to be a sentence encoder and the result is that the decoders are actually discarded after the training process. The encoder along with the word embedding layer is used as a feature extractor able to encode new sentences that are fed through it. Using cosine similarity on the resulting encoded sentence embeddings, provides a powerful semantic similarity mechanism, where you can measure how closely two sentences relate in terms of meaning as well as syntax. Encoder Network: The encoder is typically a GRU-RNN which generates a fixed length vector representation h(i) for each sentence S(i) in the input. The encoded representation h(i) is obtained by passing final hidden state of the GRU cell (i.e. after it has seen the entire sentence) to multiple dense layers. Decoder Network: The decoder network takes this vector representation h(i) as input and tries to generate two sentences â€” S(i-1) and S(i+1), which could occur before and after the input sentence respectively. Separate decoders are implemented for generation of previous and next sentences, both being GRU-RNNs. The vector representation h(i) acts as the initial hidden state for the GRUs of the decoder networks. è¯æ±‡æ‰©å±• ä½œè€…åœ¨è®­ç»ƒå®Œè¿‡åŽç”¨åœ¨Google News datasetä¸Šé¢„è®­ç»ƒçš„æ¨¡åž‹å¯¹Vocabularyè¿›è¡Œäº†è¯æ±‡æ‰©å±•ä¸»è¦æ˜¯ä¸ºäº†å¼¥è¡¥æˆ‘ä»¬çš„ Decoder æ¨¡åž‹ä¸­è¯æ±‡ä¸è¶³çš„é—®é¢˜ã€‚å…·ä½“çš„åšæ³•å°±æ˜¯ï¼š(from https://www.cnblogs.com/jiangxinyang/p/9638991.html) è¯¥æ€è·¯å€Ÿé‰´äºŽTomas Mikolovçš„ä¸€ç¯‡æ–‡ç« Exploiting Similarities among Languages for Machine Translationä¸­è§£å†³æœºå™¨ç¿»è¯‘missing wordsé—®é¢˜çš„æ€è·¯ï¼Œå¯¹è®­ç»ƒé›†äº§ç”Ÿçš„è¯æ±‡è¡¨V(RNN)è¿›è¡Œäº†æ‰©å±•ï¼Œå…·ä½“çš„æ€è·¯å¯å‚è€ƒMikolovçš„æ–‡ç« ï¼Œè¾¾åˆ°çš„æ•ˆæžœæ˜¯å»ºç«‹äº†å¤§æ•°æ®é›†ä¸‹V(Word2Vec)å’Œæœ¬æ–‡V(RNN)ä¹‹é—´çš„æ˜ å°„ï¼ŒV(Word2Vec)çš„è§„æ¨¡è¿œè¿œå¤§äºŽV(RNN)ï¼Œè®ºæ–‡ä¸­V(RNN)åŒ…æ‹¬äº†20000ä¸ªè¯ï¼ŒV(Word2Vec)åŒ…æ‹¬äº†930000å¤šä¸ªè¯ï¼ŒæˆåŠŸåœ°è§£å†³äº†è¿™ä¸€é—®é¢˜ï¼Œä½¿å¾—æœ¬æ–‡æå‡ºçš„æ— ç›‘ç£æ¨¡åž‹æœ‰å¤§çš„åº”ç”¨ä»·å€¼ã€‚ è¯„ä»·è§‚ç‚¹è¿™ä¸ªæ–¹æ³•åªæ˜¯é€‚ç”¨äºŽé•¿æ–‡æœ¬ï¼Œè¦æ±‚æ˜¯è‡³å°‘æœ‰ä¸¤ä¸ªè¡”æŽ¥çš„å¥å­ï¼Œæ€æƒ³å’Œskip-gram æ¯”è¾ƒç›¸è¿‘ã€‚ æºç google çš„å®žçŽ°ä½œè€…çš„å®žçŽ°è®ºæ–‡ Quick-Thought vectors2018å¹´å‘è¡¨çš„è®ºæ–‡An efficient framework for learning sentence representationsæå‡ºäº†ä¸€ç§ç®€å•ä¸”æœ‰æ•ˆçš„æ¡†æž¶ç”¨äºŽå­¦ä¹ å¥å­è¡¨ç¤ºã€‚å’Œå¸¸è§„çš„ç¼–ç è§£ç ç±»æ¨¡åž‹ï¼ˆå¦‚skip-thoughtså’ŒSDAEï¼‰ä¸åŒçš„æ˜¯ï¼Œæœ¬æ–‡é‡‡ç”¨ä¸€ç§åˆ†ç±»å™¨çš„æ–¹å¼å­¦ä¹ å¥å­è¡¨ç¤ºã€‚å…·ä½“åœ°ï¼Œæ¨¡åž‹çš„è¾“å…¥ä¸ºä¸€ä¸ªå¥å­$s$ä»¥åŠä¸€ä¸ªå€™é€‰å¥å­é›†åˆ$S_{cand}$ï¼Œå…¶ä¸­$S_{cand}$åŒ…å«ä¸€ä¸ªå¥å­$s_{ctxt}$æ˜¯$s$çš„ä¸Šä¸‹æ–‡å¥å­ï¼ˆä¹Ÿå°±æ˜¯$s $)çš„å‰ä¸€ä¸ªå¥å­æˆ–åŽä¸€ä¸ªå¥å­ï¼‰ä»¥åŠå…¶ä»–ä¸æ˜¯$s$ä¸Šä¸‹æ–‡çš„å¥å­ã€‚æ¨¡åž‹é€šè¿‡å¯¹$s$ä»¥åŠ$S_{cand}$ä¸­çš„æ¯ä¸ªå¥å­è¿›è¡Œç¼–ç ï¼Œç„¶åŽè¾“å…¥åˆ°ä¸€ä¸ªåˆ†ç±»å™¨ä¸­ï¼Œè®©åˆ†ç±»å™¨é€‰å‡º$S_{cand}$ä¸­çš„å“ªä¸ªå¥å­æ˜¯$s_{ctxt}$ã€‚å®žéªŒè®¾ç½®å€™é€‰å¥å­é›†åˆå¤§å°ä¸º3ï¼Œå³$S_{cand}â€‹$åŒ…å«1ä¸ªä¸Šä¸‹æ–‡å¥å­å’Œä¸¤ä¸ªæ— å…³å¥å­ã€‚æ¨¡åž‹ç»“æž„å¦‚ä¸‹ï¼š æ¨¡åž‹æœ‰å¦‚ä¸‹ä¸¤ä¸ªç»†èŠ‚éœ€è¦æ³¨æ„ï¼šæ¨¡åž‹ä½¿ç”¨çš„åˆ†ç±»å™¨ï¼ˆå¾—åˆ†å‡½æ•°ï¼‰$c$éžå¸¸ç®€å•ï¼Œæ˜¯ä¸¤ä¸ªå‘é‡å†…ç§¯ï¼Œå³$c(u, v)=u^Tv$ï¼Œè®¡ç®—$s$çš„embeddingä¸Žæ‰€æœ‰$S_{cand}$ä¸­çš„å¥å­å‘é‡å†…ç§¯å¾—åˆ†åŽï¼Œè¾“å…¥åˆ°softmaxå±‚è¿›è¡Œåˆ†ç±»ã€‚ä½¿ç”¨ç®€å•åˆ†ç±»å™¨æ˜¯ä¸ºäº†å¼•å¯¼æ¨¡åž‹ç€é‡è®­ç»ƒå¥å­ç¼–ç å™¨ï¼Œå› ä¸ºæˆ‘ä»¬çš„ç›®çš„æ˜¯ä¸ºäº†å¾—åˆ°å¥½çš„å¥å­å‘é‡è¡¨ç¤ºè€Œä¸æ˜¯å¥½çš„åˆ†ç±»å™¨ã€‚è™½ç„¶æŸäº›ç›‘ç£ä»»åŠ¡æ¨¡åž‹å¦‚æ–‡æœ¬è•´å«æ¨¡åž‹æ˜¯å‚æ•°å…±äº«çš„ï¼Œ$s$çš„ç¼–ç å™¨å‚æ•°å’Œå€™é€‰å¥å­ç¼–ç å™¨å‚æ•°æ˜¯ä¸åŒçš„ï¼ˆä¸å…±äº«ï¼‰ï¼Œå› ä¸ºå¥å­è¡¨ç¤ºå­¦ä¹ å¾€å¾€æ˜¯åœ¨å¤§è§„æ¨¡è¯­æ–™ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä¸å¿…æ‹…å¿ƒå‚æ•°å­¦ä¹ ä¸å……åˆ†çš„é—®é¢˜ã€‚æµ‹è¯•æ—¶ï¼Œç»™å®šå¾…ç¼–ç å¥å­$s$ï¼Œé€šè¿‡è¯¥æ¨¡åž‹å¾—åˆ°çš„å¥å­è¡¨ç¤ºæ˜¯ä¸¤ç§ç¼–ç å™¨çš„è¿žç»“ $[ f ( s ) ;g ( s ) ]$ã€‚ çœ‹ä¸ŠåŽ»ï¼ŒSkip-thoughtå’ŒSkip-gramæŒºè±¡ã€‚å”¯ä¸€çš„é—æ†¾æ˜¯Skip-thoughtçš„decoderé‚£éƒ¨åˆ†ï¼Œå®ƒæ˜¯ä½œä¸ºlanguage modelingæ¥å¤„ç†çš„ã€‚è€ŒSkip-gramåˆ™æ˜¯åˆ©ç”¨ä¸€ä¸ªclassifieré¢„æµ‹å‘¨å›´çš„è¯(é€šè¿‡hierarchical softmax æˆ–è€…negative samplingï¼‰ã€‚QTé’ˆå¯¹è¿™ä¸ªé—®é¢˜ï¼Œå¯¹decoderéƒ¨åˆ†åšäº†å¤§çš„è°ƒæ•´ï¼Œå®ƒç›´æŽ¥æŠŠdecoderæ‹¿æŽ‰ï¼Œå–è€Œä»£ä¹‹çš„æ˜¯ä¸€ä¸ªclassifierã€‚è¿™ä¸ªclassifierè´Ÿè´£é¢„æµ‹å“ªäº›å¥å­æ‰æ˜¯context sentencesã€‚ QTçš„classifierå–ä»£äº†Skip-thoughtçš„Decoderã€‚è¿™æ ·åšçš„å¥½å¤„æ˜¯è¿è¡Œçš„é€Ÿåº¦å¤§å¤§æå‡äº†ï¼Œç”¨åˆ¤åˆ«é—®é¢˜å–ä»£äº†ç”Ÿæˆå¼é—®é¢˜ã€‚æœ‰è¶£çš„æ˜¯ï¼Œè™½ç„¶QTå‡ºçŽ°çš„æ¯”Skip-thoughtæ›´æ™šï¼Œä½†æ˜¯æ–¹æ³•æ›´ç®€å•ï¼Œä¹Ÿæ›´åŠ æŽ¥è¿‘Word2Vecç®—æ³•ã€‚QTæ˜¯ä¸€ç§æ–°çš„state-of-artçš„ç®—æ³•ã€‚å®ƒä¸å…‰æ•ˆæžœå¥½ï¼Œè€Œä¸”è®­ç»ƒæ—¶é—´è¦è¿œå°äºŽå…¶ä»–ç®—æ³•ã€‚åœ¨ç®—æ³•æ–¹æ³•ä¸Šå’Œæ•ˆæžœä¸Šï¼Œéƒ½å¯ç§°ä¸ºæ˜¯å¥å­è¡¨å¾ç•Œçš„Word2Vecä¸€èˆ¬çš„å­˜åœ¨ã€‚å’Œå‰é¢å‡ ç¯‡ä»‹ç»çš„ä¸åŒç®—æ³•æ”¾åœ¨ä¸€èµ·æ¯”è¾ƒï¼ŒåŒæ ·éƒ½æ˜¯ä¸ºäº†æ‰¾åˆ°å¥½çš„å¥å­è¡¨å¾ï¼Œå®ƒä»¬é‡‡å–äº†ä¸åŒçš„è·¯å¾„ï¼šInferSentåœ¨å¯»æ‰¾NLPé¢†åŸŸçš„ImageNet, å®ƒçš„æˆåŠŸæ›´åƒæ˜¯åœ¨å¯»æ‰¾æ•°æ®é›†å’Œä»»åŠ¡ä¸Šçš„æˆåŠŸï¼Œå½“ç„¶å®ƒæˆåŠŸçš„æ‰¾åˆ°äº†SNLI; Concatenated p-meansåœ¨å¯»æ‰¾NLPé¢†åŸŸçš„convolutional filter, å³æ€Žæ ·æ‰èƒ½æ›´å¥½çš„æç‚¼å‡ºå¥å­çº§åˆ«çš„ç‰¹å¾ï¼Œå®ƒæ‰¾åˆ°äº†p-meansæ“ä½œï¼Œä»¥åŠåˆ©ç”¨äº†ä¸åŒçš„embeddings; QTåˆ™æ˜¯ç›´æŽ¥åœ¨ç®—æ³•å±‚é¢ä¸Šï¼Œå¯»æ‰¾å¥å­çº§åˆ«çš„Word2Vec, ç®—æ³•ä¸Šçš„æ”¹è¿›è®©å®ƒå—ç›Šã€‚æˆ‘ä»¬çœ‹åˆ°ä¸åŒçš„æ–¹æ³•åœ¨ä¸åŒçš„æ–¹å‘ä¸Šéƒ½ä½œå‡ºäº†åŠªåŠ›å’Œå–å¾—äº†æˆæ•ˆï¼Œå¾ˆéš¾è®²å“ªç§åŠªåŠ›ä¼šæ›´æœ‰æ•ˆæˆ–è€…æ›´æœ‰æ½œåŠ›ã€‚å”¯ä¸€å”¯ä¸€å¯ä»¥è‚¯å®šçš„æ˜¯ï¼Œä»Žåº”ç”¨å±‚é¢ä¸Šæ¥è®²ï¼Œåˆé€‚çš„æ‰æ˜¯æœ€å¥½çš„ã€‚ Supervised Sentence EmbedInferSentæ¥è‡ªè®ºæ–‡Supervised Learning of Universal Sentence Representations from Natural Language Inference Dataï¼Œæ›´å¤šä¿¡æ¯å‚è€ƒ è¿™é‡Œ Multi-task learning Sentence EmbedUniversal Sentence Encoderæ¥è‡ªè®ºæ–‡ Universal Sentence Encoderï¼Œæ›´å¤šä¿¡æ¯å‚è€ƒ Universal Sentence Encoder]]></content>
      <categories>
        <category>NLP</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[NLP Papers Reading- BERT]]></title>
    <url>%2F2019%2F05%2F27%2Fpaper-reading-nlp%2F</url>
    <content type="text"><![CDATA[nlp è®ºæ–‡é˜…è¯»ç¬”è®°, éšæ—¶ updateâ€¦ attention is all you needSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The ouput is computed as a weighted sum of the values, where the weight assigned to each value is computed cy a compatibility function of the query with the corresponding key. ä¸­æ–‡çš„ç†è§£ï¼šæ·±åº¦å­¦ä¹ é‡Œçš„Attentionmodelå…¶å®žæ¨¡æ‹Ÿçš„æ˜¯äººè„‘çš„æ³¨æ„åŠ›æ¨¡åž‹ï¼Œä¸¾ä¸ªä¾‹å­æ¥è¯´ï¼Œå½“æˆ‘ä»¬è§‚èµä¸€å¹…ç”»æ—¶ï¼Œè™½ç„¶æˆ‘ä»¬å¯ä»¥çœ‹åˆ°æ•´å¹…ç”»çš„å…¨è²Œï¼Œä½†æ˜¯åœ¨æˆ‘ä»¬æ·±å…¥ä»”ç»†åœ°è§‚å¯Ÿæ—¶ï¼Œå…¶å®žçœ¼ç›èšç„¦çš„å°±åªæœ‰å¾ˆå°çš„ä¸€å—ï¼Œè¿™ä¸ªæ—¶å€™äººçš„å¤§è„‘ä¸»è¦å…³æ³¨åœ¨è¿™ä¸€å°å—å›¾æ¡ˆä¸Šï¼Œä¹Ÿå°±æ˜¯è¯´è¿™ä¸ªæ—¶å€™äººè„‘å¯¹æ•´å¹…å›¾çš„å…³æ³¨å¹¶ä¸æ˜¯å‡è¡¡çš„ï¼Œæ˜¯æœ‰ä¸€å®šçš„æƒé‡åŒºåˆ†çš„ã€‚è¿™å°±æ˜¯æ·±åº¦å­¦ä¹ é‡Œçš„AttentionModelçš„æ ¸å¿ƒæ€æƒ³ã€‚æ‰€è°“æ³¨æ„åŠ›æœºåˆ¶ï¼Œå°±æ˜¯è¯´åœ¨ç”Ÿæˆæ¯ä¸ªè¯çš„æ—¶å€™ï¼Œå¯¹ä¸åŒçš„è¾“å…¥è¯ç»™äºˆä¸åŒçš„å…³æ³¨æƒé‡ã€‚é€šè¿‡æ³¨æ„åŠ›æœºåˆ¶ï¼Œæˆ‘ä»¬å°†è¾“å…¥å¥å­ç¼–ç ä¸ºä¸€ä¸ªå‘é‡åºåˆ—ï¼Œå¹¶è‡ªé€‚åº”åœ°é€‰æ‹©è¿™äº›å‘é‡çš„ä¸€ä¸ªå­é›†ï¼ŒåŒæ—¶å¯¹è¯‘æ–‡è¿›è¡Œè¯‘ç ï¼Œä¾‹å¦‚where are youâ€”â€”&gt;ä½ åœ¨å“ªï¼ŸçŽ°åœ¨æˆ‘ä»¬åœ¨ç¿»è¯‘â€œä½ â€çš„æ—¶å€™ç»™â€youâ€æ›´å¤šçš„æƒé‡ï¼Œé‚£ä¹ˆå°±å¯ä»¥æœ‰æ•ˆçš„è§£å†³å¯¹é½é—®é¢˜ã€‚ Background: ä¸»è¦æ˜¯é¢ä¸´çš„ä¸‰ä¸ªé—®é¢˜ã€‚ Model: Encoder: encoderç”±6ä¸ªç›¸åŒçš„å±‚å †å è€Œæˆï¼Œæ¯ä¸ªå±‚æœ‰ä¸¤ä¸ªå­å±‚ã€‚ç¬¬ä¸€ä¸ªå­å±‚æ˜¯å¤šå¤´è‡ªæˆ‘æ³¨æ„åŠ›æœºåˆ¶(multi-head self-attention mechanism)ï¼Œç¬¬äºŒå±‚æ˜¯ç®€å•çš„ä½ç½®çš„å…¨è¿žæŽ¥å‰é¦ˆç½‘ç»œ(position-wise fully connected feed-forward network)ã€‚åœ¨ä¸¤ä¸ªå­å±‚ä¸­ä¼šä½¿ç”¨ä¸€ä¸ªæ®‹å·®è¿žæŽ¥ï¼ŒæŽ¥ç€è¿›è¡Œå±‚æ ‡å‡†åŒ–(layer normalization)ã€‚ä¹Ÿå°±æ˜¯è¯´æ¯ä¸€ä¸ªå­å±‚çš„è¾“å‡ºéƒ½æ˜¯LayerNorm(x + sublayer(x))ã€‚ç½‘ç»œè¾“å…¥æ˜¯ä¸‰ä¸ªç›¸åŒçš„å‘é‡q, kå’Œvï¼Œæ˜¯word embeddingå’Œposition embeddingç›¸åŠ å¾—åˆ°çš„ç»“æžœã€‚ä¸ºäº†æ–¹ä¾¿è¿›è¡Œæ®‹å·®è¿žæŽ¥ï¼Œæˆ‘ä»¬éœ€è¦å­å±‚çš„è¾“å‡ºå’Œè¾“å…¥éƒ½æ˜¯ç›¸åŒçš„ç»´åº¦ã€‚ Decoder: decoderä¹Ÿæ˜¯ç”±Nï¼ˆN=6ï¼‰ä¸ªå®Œå…¨ç›¸åŒçš„Layerç»„æˆï¼Œdecoderä¸­çš„Layerç”±encoderçš„Layerä¸­æ’å…¥ä¸€ä¸ªMulti-Head Attention + Add&amp;Normç»„æˆã€‚è¾“å‡ºçš„embeddingä¸Žè¾“å‡ºçš„position embeddingæ±‚å’Œåšä¸ºdecoderçš„è¾“å…¥ï¼Œç»è¿‡ä¸€ä¸ªMulti-HeadAttention + Add&amp;Normï¼ˆï¼ˆMA-1ï¼‰å±‚ï¼ŒMA-1å±‚çš„è¾“å‡ºåšä¸ºä¸‹ä¸€Multi-Head Attention + Add&amp;Normï¼ˆMA-2ï¼‰çš„queryï¼ˆQï¼‰è¾“å…¥ï¼ŒMA-2å±‚çš„Keyå’ŒValueè¾“å…¥ï¼ˆä»Žå›¾ä¸­çœ‹ï¼Œåº”è¯¥æ˜¯encoderä¸­ç¬¬iï¼ˆi = 1,2,3,4,5,6ï¼‰å±‚çš„è¾“å‡ºå¯¹äºŽdecoderä¸­ç¬¬iï¼ˆi = 1,2,3,4ï¼Œ5,6ï¼‰å±‚çš„è¾“å…¥ï¼‰ã€‚MA-2å±‚çš„è¾“å‡ºè¾“å…¥åˆ°ä¸€ä¸ªå‰é¦ˆå±‚ï¼ˆFFï¼‰ï¼Œç»è¿‡ANæ“ä½œåŽï¼Œç»è¿‡ä¸€ä¸ªçº¿æ€§+softmaxå˜æ¢å¾—åˆ°æœ€åŽç›®æ ‡è¾“å‡ºçš„æ¦‚çŽ‡ã€‚ å¯¹äºŽdecoderä¸­çš„ç¬¬ä¸€ä¸ªå¤šå¤´æ³¨æ„åŠ›å­å±‚ï¼Œéœ€è¦æ·»åŠ maskingï¼Œç¡®ä¿é¢„æµ‹ä½ç½®içš„æ—¶å€™ä»…ä»…ä¾èµ–äºŽä½ç½®å°äºŽiçš„è¾“å‡ºã€‚ å±‚ä¸Žå±‚ä¹‹é—´ä½¿ç”¨çš„Position-wise feed forward networkã€‚ ä»Žæ•´ä½“ä¸Šæ¥çœ‹ï¼ŒTransformerä¾æ—§æ˜¯ä¸€ä¸ªâ€œSequence to Sequenceâ€æ¡†æž¶ï¼Œæ‹¥æœ‰Encoderå’ŒDecoderä¸¤éƒ¨åˆ†ï¼š Transformerçš„Encoderå…¶å®žæœ‰6å±‚ï¼ŒDecoderä¹Ÿæœ‰6å±‚ï¼Œä»ŽEncoderçš„è§’åº¦ï¼Œä½Žå±‚çš„Encoderæ˜¯è¡¨å±‚çš„è¯æ³•ä¿¡æ¯ï¼Œé€æ­¥å‘ä¸Šè¿›è¡ŒæŠ½è±¡ä¹‹åŽï¼Œåœ¨ä¸Šå±‚å°†è¡¨ç¤ºæŠ½è±¡è¯­ä¹‰ä¿¡æ¯ã€‚Encoderéƒ¨åˆ†è¿˜åœ¨æœ€ä¸Šå±‚è¿žäº†å‡ æ¡çº¿åˆ°æ¯ä¸ªDecoderçš„éƒ¨åˆ†ï¼Œè¿™æ˜¯ä¸ºäº†åœ¨Decoderä¸­è¿›è¡ŒAttentionæ“ä½œï¼ŒDecoderçš„ç½‘ç»œä¸­å’ŒEncoderä¹Ÿæœ‰ä¿¡æ¯ä¼ é€’å’Œäº¤äº’çš„ã€‚æœ€åŽä¸€ä¸ªç‰¹ç‚¹æ˜¯Decoderå’ŒEncoderç”»çš„å¤§å°æ˜¯ä¸€æ ·çš„ï¼Œå› ä¸ºå®ƒä»¬å±‚çš„ç»´åº¦å¤§å°æ˜¯ä¸€æ ·çš„ã€‚ ä¹Ÿå°±æ˜¯è¯´encoderçš„è¾“å‡ºï¼Œä¼šå’Œæ¯ä¸€å±‚çš„decoderè¿›è¡Œç»“åˆã€‚Encoderå’ŒDecoderçš„å†…éƒ¨ç»“æž„ï¼š æ¨¡åž‹çš„ç‰¹ç‚¹ï¼šPositional embeddingï¼›ï¼ˆä½ç½®åµŒå…¥å‘é‡â€”â€”å…¶å®žç±»ä¼¼word2vecï¼Œå¤„ç†çš„è¯­åºçš„ä¿¡æ¯ï¼‰ã€‚multi-head attention; (å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶â€”â€”ç‚¹ä¹˜æ³¨æ„åŠ›çš„å‡çº§ç‰ˆæœ¬ï¼Œ è¿™ä¸ªå°±ç±»ä¼¼ensembleçš„æ€æƒ³ï¼Œä¸åŒçš„å­ç©ºé—´çš„attention è¿›è¡Œèžåˆï¼‰Position-wise Feed-Forward Networksï¼ˆä½ç½®å…¨é“¾æŽ¥å‰é¦ˆç½‘ç»œâ€”â€”MLPå˜å½¢ï¼‰ æœ‰ä¸¤ç§å¸¸ç”¨çš„æ³¨æ„åŠ›å‡½æ•°ï¼Œä¸€ç§æ˜¯åŠ æ³•æ³¨æ„åŠ›(additive attention)ï¼Œå¦å¤–ä¸€ç§æ˜¯ç‚¹ä¹˜æ³¨æ„åŠ›(dot-productattention)ï¼Œè®ºæ–‡æ‰€é‡‡ç”¨çš„å°±æ˜¯ç‚¹ä¹˜æ³¨æ„åŠ›ï¼Œè¿™ç§æ³¨æ„åŠ›æœºåˆ¶å¯¹äºŽåŠ æ³•æ³¨æ„åŠ›è€Œè¨€ï¼Œæ›´å¿«ï¼ŒåŒæ—¶æ›´èŠ‚çœç©ºé—´ã€‚ åŠ æ³•æ³¨æ„åŠ›è¿˜æ˜¯ä»¥ä¼ ç»Ÿçš„RNNçš„seq2seqé—®é¢˜ä¸ºä¾‹å­ï¼ŒåŠ æ€§æ³¨æ„åŠ›æ˜¯æœ€ç»å…¸çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œå®ƒä½¿ç”¨äº†æœ‰ä¸€ä¸ªéšè—å±‚çš„å‰é¦ˆç½‘ç»œï¼ˆå…¨è¿žæŽ¥ï¼‰æ¥è®¡ç®—æ³¨æ„åŠ›åˆ†é…ï¼š å…¬å¼:$$\alpha _ { i j } = \frac { \exp \left( e _ { i j } \right) } { \sum _ { k = 1 } ^ { L } e _ { i k } }$$ Scaled Dot-Productè¿™ç¯‡è®ºæ–‡è®¡ç®—queryå’Œkeyç›¸ä¼¼åº¦ä½¿ç”¨äº†dot-product attentionï¼Œå³queryå’Œkeyè¿›è¡Œç‚¹ä¹˜ï¼ˆå†…ç§¯ï¼‰æ¥è®¡ç®—ç›¸ä¼¼åº¦ã€‚ Multi-Head Attention: åœ¨å®žé™…ä¸­ä¸ºäº†å¹¶è¡Œè®¡ç®—ï¼Œå¯ä»¥åœ¨ä¸€ç»„queriesä¸Šè®¡ç®—æ³¨æ„åŠ›å‡½æ•°ï¼Œå°†å¤šä¸ªqueryå †å æˆQï¼ŒåŒç†keyså’Œvaluesä¹Ÿè¢«å †å æˆKå’ŒVï¼Œé€šè¿‡ä¸‹é¢çš„å…¬å¼æ¥è®¡ç®—çŸ©é˜µè¾“å‡º:self-attention æ¨¡åž‹å°±æ˜¯è‡ªå·±å¯¹è‡ªå·±æ±‚attentionï¼Œå³ð‘„=ð¾=ð‘‰$$\text { Attention } ( Q , K , V ) = \operatorname { softmax } \left( \frac { Q K ^ { T } } { \sqrt { d _ { k } } } \right) V$$ä¹‹æ‰€ä»¥ç”¨å†…ç§¯é™¤ä»¥ç»´åº¦çš„å¼€æ–¹ï¼Œè®ºæ–‡ç»™å‡ºçš„è§£é‡Šæ˜¯ï¼šå‡è®¾Qå’ŒKéƒ½æ˜¯ç‹¬ç«‹çš„éšæœºå˜é‡ï¼Œæ»¡è¶³å‡å€¼ä¸º0ï¼Œæ–¹å·®ä¸º1ï¼Œåˆ™ç‚¹ä¹˜åŽç»“æžœå‡å€¼ä¸º0ï¼Œæ–¹å·®ä¸ºdkã€‚ä¹Ÿå³æ–¹å·®ä¼šéšç»´åº¦dkçš„å¢žå¤§è€Œå¢žå¤§ï¼Œè€Œå¤§çš„æ–¹å·®å¯¼è‡´æžå°çš„æ¢¯åº¦(æˆ‘è®¤ä¸ºå¤§æ–¹å·®å¯¼è‡´æœ‰çš„è¾“å‡ºå•å…ƒaï¼ˆaæ˜¯softmaxçš„ä¸€ä¸ªè¾“å‡ºï¼‰å¾ˆå°ï¼Œsoftmaxåå‘ä¼ æ’­æ¢¯åº¦å°±å¾ˆå°ï¼ˆæ¢¯åº¦å’Œaæœ‰å…³ï¼‰ï¼‰ã€‚ä¸ºäº†é¿å…è¿™ç§å¤§æ–¹å·®å¸¦æ¥çš„è®­ç»ƒé—®é¢˜ï¼Œè®ºæ–‡ä¸­ç”¨å†…ç§¯é™¤ä»¥ç»´åº¦çš„å¼€æ–¹ï¼Œä½¿ä¹‹å˜ä¸ºå‡å€¼ä¸º0ï¼Œæ–¹å·®ä¸º1ã€‚ é™¤äº†è®¡ç®—ä¸€ä¸ªå•ç‹¬çš„æ³¨æ„åŠ›å‡½æ•°ï¼Œè®ºæ–‡æå‡ºå¯¹queriesï¼Œkeyså’Œvaluesåšhæ¬¡ä¸åŒçš„æŠ•å½±, ç„¶åŽéƒ½ç»è¿‡Scaled Dot-Product Attentionï¼Œå°†ç»“æžœæ‹¼æŽ¥åœ¨ä¸€èµ·ï¼Œæœ€åŽé€šè¿‡ä¸€ä¸ªçº¿æ€§æ˜ å°„è¾“å‡ºï¼Œé€šè¿‡å¤šå¤´æ³¨æ„åŠ›ï¼Œæ¨¡åž‹èƒ½å¤ŸèŽ·å¾—ä¸åŒå­ç©ºé—´ä¸‹çš„ä½ç½®ä¿¡æ¯ã€‚å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œå…¬å¼å¦‚ä¸‹:$$\text {MultiHead} ( Q , K , V ) =Concat(head_1, head_2, â€¦, head_h) W ^ { o }$$ Self-Attentioné‚£ä¹ˆé¦–å…ˆè¦æ˜Žç™½ä»€ä¹ˆæ˜¯Attentionã€‚ä»Žè¯­è¨€å­¦çš„è§’åº¦ï¼Œå®ƒæ˜¯è¡¨ç¤ºè¯ä¸Žè¯ä¹‹é—´çš„å…³è”å…³ç³»ï¼Œåƒä¸‹å›¾æ‰€ç¤ºï¼Œè¿™æ˜¯ä¸€ä¸ªSelf-Attentionçš„ç¤ºæ„ï¼Œå®ƒè¿™ä¸ªitä¼šå’Œå…¶ä»–ä½ç½®çš„è¯å‘ç”Ÿå…³ç³»ï¼Œé¢œè‰²è¶Šæ·±çš„æ˜¯è¯´å…³ç³»è¶Šç´§å¯†ï¼Œä»Žä¸­å›¾ä¸­çœ‹åˆ°å®ƒå¾ˆæ­£ç¡®çš„å…³è”åˆ°äº†animalå®ƒå®žé™…æŒ‡ä»£çš„ä¸€ä¸ªè¯ã€‚ä»Žæœºå™¨å­¦ä¹ çš„è§’åº¦ï¼Œè¿™ä¸ªAttentionæ˜¯ç¥žç»ç½‘ç»œéšå±‚ä¹‹é—´ä¸€ä¸ªç›¸ä¼¼åº¦çš„è¡¨ç¤ºï¼Œä»€ä¹ˆæ˜¯Self-Attentionï¼Ÿå°±æ˜¯è¡¨ç¤ºå¥å­å†…éƒ¨è¯ä¸Žè¯ä¹‹é—´çš„å…³è”å…³ç³»ï¼Œå°±åƒè¿™é‡Œçš„itåˆ°animalï¼Œå¯ä»¥ç”¨äºŽæŒ‡ä»£æ¶ˆè§£ç­‰é—®é¢˜ã€‚ Positional EncodingPosition Embeddingï¼Œä¹Ÿå°±æ˜¯â€œä½ç½®å‘é‡â€ï¼Œå°†æ¯ä¸ªä½ç½®ç¼–å·ï¼Œç„¶åŽæ¯ä¸ªç¼–å·å¯¹åº”ä¸€ä¸ªå‘é‡ï¼Œé€šè¿‡ç»“åˆä½ç½®å‘é‡å’Œè¯å‘é‡ï¼Œå°±ç»™æ¯ä¸ªè¯éƒ½å¼•å…¥äº†ä¸€å®šçš„ä½ç½®ä¿¡æ¯ï¼Œè¿™æ ·Attentionå°±å¯ä»¥åˆ†è¾¨å‡ºä¸åŒä½ç½®çš„è¯äº†ã€‚ Residual connectionå’Œlayer-normalizationï¼ˆè¿™ä¸¤ä¸ªæ“ä½œä¸»è¦æ˜¯åº”å¯¹ æ·±åº¦ç½‘ç»œè€Œæå‡ºçš„ï¼‰å¯¹äºŽå­¦ä¹ CVçš„äººä¼°è®¡å¯¹è¿™ä¸ªç»“æž„ä¸€ç‚¹ä¹Ÿä¸é™Œç”Ÿï¼ŒResidual connectionæ˜¯å¯¹äºŽè¾ƒä¸ºæ·±å±‚çš„ç¥žç»ç½‘ç»œæœ‰æ¯”è¾ƒå¥½çš„ä½œç”¨ï¼Œæ¯”å¦‚ç½‘ç»œå±‚å¾ˆæ·±æ—¶ï¼Œæ•°å€¼çš„ä¼ æ’­éšç€weightä¸æ–­çš„å‡å¼±ï¼ŒResidual connectionæ˜¯ä»Žè¾“å…¥çš„éƒ¨åˆ†ï¼Œå°±æ˜¯å›¾ä¸­è™šçº¿çš„éƒ¨åˆ†ï¼Œå®žé™…è¿žåˆ°å®ƒè¾“å‡ºå±‚çš„éƒ¨åˆ†ï¼ŒæŠŠè¾“å…¥çš„ä¿¡æ¯åŽŸå°ä¸åŠ¨copyåˆ°è¾“å‡ºçš„éƒ¨åˆ†ï¼Œå‡å°‘ä¿¡æ¯çš„æŸå¤±ã€‚ layer-normalizationè¿™ç§å½’ä¸€åŒ–å±‚æ˜¯ä¸ºäº†é˜²æ­¢åœ¨æŸäº›å±‚ä¸­ç”±äºŽæŸäº›ä½ç½®è¿‡å¤§æˆ–è€…è¿‡å°å¯¼è‡´æ•°å€¼è¿‡å¤§æˆ–è¿‡å°ï¼Œå¯¹ç¥žç»ç½‘ç»œæ¢¯åº¦å›žä¼ æ—¶æœ‰è®­ç»ƒçš„é—®é¢˜ï¼Œä¿è¯è®­ç»ƒçš„ç¨³å®šæ€§ï¼Œè¿™æ˜¯ç¥žç»ç½‘ç»œè®¾è®¡æ¯”è¾ƒå¸¸ç”¨çš„caseã€‚ ç»“è®ºï¼šself-attentionå±‚çš„å¥½å¤„æ˜¯èƒ½å¤Ÿä¸€æ­¥åˆ°ä½æ•æ‰åˆ°å…¨å±€çš„è”ç³»ï¼Œè§£å†³äº†é•¿è·ç¦»ä¾èµ–ï¼Œå› ä¸ºå®ƒç›´æŽ¥æŠŠåºåˆ—ä¸¤ä¸¤æ¯”è¾ƒï¼ˆä»£ä»·æ˜¯è®¡ç®—é‡å˜ä¸º O(n2)ï¼Œå½“ç„¶ç”±äºŽæ˜¯çº¯çŸ©é˜µè¿ç®—ï¼Œè¿™ä¸ªè®¡ç®—é‡ç›¸å½“ä¹Ÿä¸æ˜¯å¾ˆä¸¥é‡ï¼‰ï¼Œè€Œä¸”æœ€é‡è¦çš„æ˜¯å¯ä»¥è¿›è¡Œå¹¶è¡Œè®¡ç®—ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒRNN éœ€è¦ä¸€æ­¥æ­¥é€’æŽ¨æ‰èƒ½æ•æ‰åˆ°ï¼Œå¹¶ä¸”å¯¹äºŽé•¿è·ç¦»ä¾èµ–å¾ˆéš¾æ•æ‰ã€‚è€Œ CNN åˆ™éœ€è¦é€šè¿‡å±‚å æ¥æ‰©å¤§æ„Ÿå—é‡Žï¼Œè¿™æ˜¯ Attention å±‚çš„æ˜Žæ˜¾ä¼˜åŠ¿ã€‚ A simple but tough-to-beat baseline for sentence embeddingsè¿™ç§motivation è¿˜æ˜¯å¾ˆå€¼å¾—å¥½å¥½çœ‹çš„ï¼ŒéªŒè¯è®ºæ–‡çš„å¥½åæ˜¯å¯ä»¥é€šè¿‡çœ‹æœ€åŽçš„æ•ˆæžœ/ ç»“æžœæ˜¯å¦æŒ‰ç…§ motivation é‚£æ ·çš„ã€‚ â€œrelative weightsâ€ æ˜¯é’ˆå¯¹ word2vec ä¸­çš„æ•ˆæžœæ”¹è¿›çš„ï¼Œä»Žmotivation çš„è§’åº¦æ²¡æœ‰è€ƒè™‘åˆ° melo or BERT ä¸­çš„ context . å½“ç„¶è®ºæ–‡çš„åˆ›æ–°ç‚¹åœ¨äºŽ æ— ç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œå½“åˆ«äººéƒ½åœ¨è½¬å‘æœ‰ç›‘ç£å’Œå¤šä»»åŠ¡çš„æ—¶å€™ï¼Œåœ¨ç¼©çŸ­è¿è¡Œæ—¶é—´çš„åŒæ—¶ï¼Œæœ€åŽçš„æ•ˆæžœå’Œç¥žç»ç½‘ç»œæ——é¼“ç›¸å½“ã€‚ Taking the average of the word embeddings in a sentence tends to give too much weight to words that are quite irrelevant, semantically speaking. Smooth Inverse Frequency tries to solve this problem in two ways: Weighting: like our tf-idf baseline above, SIF takes the weighted average of the word embeddings in the sentence. Every word embedding is weighted by a/(a + p(w)), where a is a parameter that is typically set to 0.001 and p(w) is the estimated frequency of the word in a reference corpus. Common component removal: next, SIF computes the principal component of the resulting embeddings for a set of sentences. It then subtracts from these sentence embeddings their projections on their first principal component. This should remove variation related to frequency and syntax that is less relevant semantically.As a result, SIF downgrades unimportant words such as but, just, etc., and keeps the information that contributes most to the semantics of the sentence. æœ¬æ–‡æ˜¯ç”¨æ— ç›‘ç£æ–¹æ³•åšå¥å­çº§åˆ«çš„ embeddingï¼Œç”¨çš„æ˜¯ä¸€ä¸ªååˆ†ç®€å•ä½†å´åˆå¾ˆæœ‰æ•ˆçš„ä¼ ç»Ÿæ–¹æ³•ï¼Œè¿™åœ¨ç¥žç»ç½‘ç»œæ³›æ»¥çš„å¹´ä»£ç®—æ˜¯ä¸€è‚¡æ¸…æµäº†ã€‚è¿™å¼ å›¾ä¸Šçš„ä¿¡æ¯è¿˜æ˜¯å¾ˆå¤šçš„ï¼Œæ‰€ä»¥å¥½å¥½å½’çº³æ•´ç†ä¸€ä¸‹ã€‚å°½ç®¡é•¿æœŸä»¥æ¥å¥å­çš„æ— ç›‘ç£è¡¨ç¤ºå­¦ä¹ æ˜¯ä¸»æµï¼Œæœ€è¿‘å‡ ä¸ªæœˆï¼ˆ2017å¹´æœ«/2018å¹´åˆï¼‰ï¼Œæˆ‘ä»¬çœ‹åˆ°äº†è®¸å¤šéžå¸¸æœ‰è¶£çš„å·¥ä½œï¼Œæ˜¾ç¤ºäº†å‘ç›‘ç£å­¦ä¹ å’Œå¤šä»»åŠ¡å­¦ä¹ è½¬å‘çš„è¶‹åŠ¿ã€‚ å¼ºåŠ›/è¿…é€Ÿçš„åŸºçº¿ï¼šFastTextã€è¯è¢‹ï¼ˆBag-of-Wordsï¼‰ å½“å‰æœ€å…ˆè¿›æ¨¡åž‹ï¼šELMoã€Skip-Thoughtsã€Quick-Thoughtsã€ InferSentã€MILA/MSRçš„General Purpose Sentence Representationsã€Googleçš„Universal Sentence Encoder å…³äºŽnlp ä¸­çš„word embedding æ˜¯å¯ä»¥æœ‰ phrases, sentences, and paragraphs ä¸‰ä¸ªä¸åŒç±»åˆ«çš„ embeddingï¼Œæ‰€ä»¥è¿˜æ˜¯æŒºå¥½çš„ã€‚ è¿‘äº”å¹´æ¥æå‡ºäº†å¤§é‡è¯åµŒå…¥æ–¹æ³•ã€‚å…¶ä¸­æœ€å¸¸ç”¨çš„æ¨¡åž‹æ˜¯word2vecå’ŒGloVeï¼Œè¿™ä¸¤ä¸ªæ¨¡åž‹éƒ½æ˜¯åŸºäºŽåˆ†å¸ƒå‡è¯´ï¼ˆdistributional hypothesisï¼‰çš„æ— ç›‘ç£æ–¹æ³•ã€‚ï¼ˆæ ¹æ®åˆ†å¸ƒå‡è¯´ï¼Œå‡ºçŽ°åœ¨ç›¸åŒä¸Šä¸‹æ–‡ä¸­çš„å•è¯å€¾å‘äºŽå…·æœ‰ç›¸ä¼¼çš„å«ä¹‰ï¼‰ã€‚å°½ç®¡æœ‰ä¸€äº›å·¥ä½œé€šè¿‡å¹¶å…¥è¯­ä¹‰æˆ–è¯­æ³•çŸ¥è¯†ç­‰å¢žå¼ºè¿™äº›æ— ç›‘ç£æ–¹æ³•ï¼Œçº¯æ— ç›‘ç£æ–¹æ³•åœ¨2017-2018å¹´æœŸé—´å–å¾—äº†æœ‰è¶£çš„è¿›å±•ï¼Œå…¶ä¸­æœ€é‡å¤§çš„æ˜¯FastTextï¼ˆword2vecçš„æ‰©å±•ï¼‰å’ŒELMoï¼ˆå½“å‰æœ€å…ˆè¿›çš„ä¸Šä¸‹æ–‡è¯å‘é‡ï¼‰ã€‚ åœ¨ELMoä¸­ï¼ŒåµŒå…¥åŸºäºŽä¸€ä¸ªåŒå±‚çš„åŒå‘è¯­è¨€æ¨¡åž‹ï¼ˆbiLMï¼‰çš„å†…éƒ¨çŠ¶æ€è®¡ç®—ï¼ŒELMoä¹Ÿæ˜¯å› æ­¤å¾—åçš„ï¼šEmbeddings from Language Modelsï¼ˆæ¥è‡ªè¯­è¨€æ¨¡åž‹çš„åµŒå…¥ï¼‰ã€‚ELMoçš„ç‰¹æ€§ï¼šELMoçš„è¾“å…¥æ˜¯å­—ç¬¦è€Œä¸æ˜¯å•è¯ã€‚è¿™ä½¿å¾—å®ƒå¯ä»¥åˆ©ç”¨å­å­—ï¼ˆsub-wordï¼‰å•å…ƒä¸ºè¯æ±‡è¡¨ä»¥å¤–çš„å•è¯è®¡ç®—æœ‰æ„ä¹‰çš„è¡¨ç¤ºï¼ˆå’ŒFastTextç±»ä¼¼ï¼‰ã€‚ELMoæ˜¯biLMçš„å¤šå±‚æ¿€æ´»çš„è¿žæŽ¥ï¼ˆconcatenationï¼‰ã€‚è¯­è¨€æ¨¡åž‹çš„ä¸åŒå±‚ç¼–ç äº†å•è¯çš„ä¸åŒä¿¡æ¯ã€‚è¿žæŽ¥æ‰€æœ‰å±‚ä½¿å¾—ELMoå¯ä»¥ç»„åˆå¤šç§è¯è¡¨ç¤ºï¼Œä»¥æå‡ä¸‹æ¸¸ä»»åŠ¡çš„è¡¨çŽ°ã€‚ æ™®é€‚å¥åµŒå…¥è¯è¢‹æ–¹æ³•è¿™ä¸€é¢†åŸŸçš„ä¸€èˆ¬å…±è¯†æ˜¯ï¼Œç›´æŽ¥å¹³å‡ä¸€ä¸ªå¥å­çš„è¯å‘é‡è¿™ä¸€ç®€å•æ–¹æ³•ï¼ˆæ‰€è°“è¯è¢‹æ–¹æ³•ï¼‰ï¼Œä¸ºè®¸å¤šä¸‹æ¸¸ä»»åŠ¡æä¾›äº†å¼ºåŠ›çš„åŸºçº¿ã€‚Aroraç­‰åŽ»å¹´åœ¨ICLRå‘è¡¨çš„è®ºæ–‡A Simple but Tough-to-Beat Baseline for Sentence Embeddingsæä¾›äº†ä¸€ä¸ªå¾ˆå¥½çš„ç®—æ³•ï¼šé€‰æ‹©ä¸€ç§æµè¡Œçš„è¯åµŒå…¥ï¼Œç¼–ç å¥å­ä¸ºè¯å‘é‡çš„çº¿æ€§åŠ æƒç»„åˆï¼Œç„¶åŽè¿›è¡Œç›¸åŒæˆåˆ†ç§»é™¤ï¼ˆæ ¹æ®é¦–è¦ä¸»æˆåˆ†ç§»é™¤å‘é‡æŠ•å½±ï¼‰ã€‚è¿™ä¸€é€šç”¨æ–¹æ³•å…·æœ‰æ·±åˆ»è€Œå¼ºå¤§çš„ç†è®ºåŠ¨æœºï¼ŒåŸºäºŽåœ¨è¯­ç¯‡å‘é‡ä¸Šéšæœºè¡Œèµ°ä»¥ç”Ÿæˆæ–‡æœ¬çš„ç”Ÿæˆå¼æ¨¡åž‹ã€‚ ï¼ˆæœ‰äººå®žè·µï¼‰è¿™ä¸ªæ–¹æ³•åœ¨çŸ­æ–‡æœ¬ä¸Šæ•ˆæžœæ›´å¥½ï¼Œåœ¨è¯­æ–™ä¸è¶³çš„æ—¶å€™æ•ˆæžœä¸èƒ½ä¿è¯ã€‚è¿™ç§æ¨¡åž‹æ²¡æœ‰è€ƒè™‘è¯é¡ºåºï¼ˆä¹Ÿå¯ä»¥è¯´åªèƒ½ç†è§£è¯æ„æ€ï¼Œä½†æ˜¯ä¸èƒ½ç†è§£è¯­ä¹‰ï¼‰ï¼Œè€Œæ·±åº¦ç½‘ç»œæ¨¡åž‹æ˜¯å¯ä»¥è€ƒè™‘è¯­ä¹‰çš„ã€‚å¯èƒ½å†ç›¸ä¼¼åº¦é—®é¢˜ä¸Šå¯ä»¥å–å¾—æ¯”è¾ƒå¥½çš„æ•ˆæžœï¼Œä½†æ˜¯åœ¨æ–‡æœ¬åˆ†ç±»ï¼Œæƒ…æ„Ÿåˆ†ç±»ä¸Šæ•ˆæžœä¸€èˆ¬ã€‚ æ€è€ƒï¼šä»Žç›´è§‰ä¸Šç†è§£, çŸ­æ–‡æœ¬ä¸Šçš„ word2vecã€SIF è¿™ç§æ²¡æœ‰ handle è¯­åºçš„æ¨¡åž‹å¾—åˆ°çš„æ•ˆæžœå°±å·²ç»è¶³å¤Ÿçš„å¥½ï¼Œå¯¹äºŽä¸­é•¿æ–‡æœ¬ï¼ˆå¥å­ã€æ®µè½ç­‰ï¼‰elmo å’ŒBERT è¿™ç§æ¨¡åž‹çš„æ•ˆæžœæ˜¯æ›´åŠ çš„ã€‚ ç¨‹åºçš„è¿è¡Œåªéœ€è¦åå‡ åˆ†é’Ÿï¼Œä¸Žç¥žç»ç½‘ç»œçš„æ•ˆæžœæ——é¼“ç›¸å½“ã€‚ Supervised Learning of Universal Sentence Representations from Natural Language Inference Dataæ–‡ç« æˆåŠŸçš„æ‰¾åˆ°äº†NLPé¢†åŸŸçš„ImageNet â€” SNLI (Stanford Natural Language Inference dataset), å¹¶ä¸”è¯•éªŒäº†ä¸åŒçš„æ·±åº¦å­¦ä¹ æ¨¡åž‹ï¼Œæœ€ç»ˆç¡®å®šbi-LSTM max pooled ä¸ºæœ€ä½³æ¨¡åž‹ã€‚ åŸŸ æ•°æ® ä»»åŠ¡ æ¨¡åž‹(ç¼–ç å™¨) CV ImageNet image classification Le-Net, VGG-Net, Google-Net, ResNet, DenseNet NLP SNLI NLI ? åŸºäºŽç›‘ç£å­¦ä¹ æ–¹æ³•å­¦ä¹ sentence embeddingså¯ä»¥å½’çº³ä¸ºä¸¤ä¸ªæ­¥éª¤ï¼šç¬¬ä¸€æ­¥é€‰æ‹©ç›‘ç£è®­ç»ƒæ•°æ®ï¼Œè®¾è®¡ç›¸åº”çš„åŒ…å«å¥å­ç¼–ç å™¨Encoderçš„æ¨¡åž‹æ¡†æž¶ï¼›ç¬¬äºŒæ­¥é€‰æ‹©ï¼ˆè®¾è®¡ï¼‰å…·ä½“çš„å¥å­ç¼–ç å™¨ï¼ŒåŒ…æ‹¬DANã€åŸºäºŽLSTMã€åŸºäºŽCNNå’ŒTransformerç­‰ã€‚ æ•°æ®é›†ï¼š æœ¬æ–‡é‡‡ç”¨çš„æ˜¯Stanford Natural Language Inference Datasetsï¼Œç®€ç§°SNLI ï¼ˆNLPé¢†åŸŸçš„ImageNet ï¼‰ã€‚SNLIåŒ…å«570Kä¸ªäººç±»äº§ç”Ÿçš„å¥å­å¯¹ï¼Œæ¯ä¸ªå¥å­å¯¹éƒ½å·²ç»åšå¥½äº†æ ‡ç­¾ï¼Œæ ‡ç­¾æ€»å…±åˆ†ä¸ºä¸‰ç±»ï¼šè•´å«ã€çŸ›ç›¾å’Œä¸­ç«‹ï¼ˆEntailmentã€contradiction and neutralï¼‰ã€‚ä¸‹é¢æ˜¯è¿™äº›æ•°æ®é›†çš„ä¸€ä¸ªä¾‹å­ï¼š ä»Žä¸Šå›¾å¯ä»¥çœ‹å‡ºï¼Œæ¯ä¸ªå¥å­å¯¹ä¸ºï¼ˆtext, hypothesisï¼‰,ä¸­é—´çš„judgmentsä¸ºå®ƒä»¬çš„æ ‡ç­¾ã€‚å¯ä»¥çœ‹åˆ°æ ‡ç­¾æ˜¯ç»¼åˆäº†5ä¸ªä¸“å®¶çš„æ„è§ï¼Œæ ¹æ®å°‘æ•°æœä»Žå¤šæ•°çš„åŽŸåˆ™å¾—åˆ°çš„ã€‚ 7ç§ä¸åŒçš„architecturesï¼š standard recurrent encoders with LSTM ï¼Œå–æœ€åŽä¸€ä¸ªéšçŠ¶æ€ standard recurrent encoders with GRU ï¼Œå–æœ€åŽä¸€ä¸ªéšçŠ¶æ€ä¸Šè¿°ä¸¤ç§æ˜¯åŸºç¡€çš„recurrent encoderï¼Œåœ¨å¥å­å»ºæ¨¡ä¸­é€šå¸¸å°†ç½‘ç»œä¸­çš„æœ€åŽä¸€ä¸ªéšè—çŠ¶æ€ä½œä¸ºsentence representationï¼› conncatenation of last hidden states of forward and backward GRUè¿™ç§æ–¹æ³•æ˜¯å°†å•å‘çš„ç½‘ç»œå˜æˆäº†åŒå‘çš„ç½‘ç»œï¼Œç„¶åŽç”¨å°†å‰å‘å’ŒåŽå‘çš„æœ€åŽä¸€ä¸ªçŠ¶æ€è¿›è¡Œè¿žæŽ¥ï¼Œå¾—åˆ°å¥å­å‘é‡ï¼› Bi-directional LSTMs (BiLSTM) with mean pooling Bi-directional LSTMs (BiLSTM) with max poolingè¿™ä¸¤ç§æ–¹æ³•ä½¿ç”¨äº†åŒå‘LSTMç»“åˆä¸€ä¸ªpoolingå±‚çš„æ–¹æ³•æ¥èŽ·å–å¥å­è¡¨ç¤ºï¼Œå…·ä½“å…¬å¼å¦‚ä¸‹ï¼š self-attentive networkè¿™ä¸ªç½‘ç»œåœ¨åŒå‘LSTMçš„åŸºç¡€ä¸ŠåŠ å…¥äº†attentionæœºåˆ¶ï¼Œå…·ä½“ç½‘ç»œç»“æž„å¦‚ä¸‹ï¼š hierarchical convolutional networks Now that we have discussed the various sentence encoding architectures used in the paper, letâ€™s go through the part of the network which takes these sentence embeddings and predicts the output label. After the sentence vectors are fed as input to this model, 3 matching methods are applied to extract relations between the text, u and hypothesis, v â€“ concatenation of the two representations (u, v) element-wise product u * v and, absolute element-wise difference |u â€“ v | The resulting vector captures information from both the text, u and the hypothesis, v, and is fed into a 3-class classifier consisting of multiple fully connected layers followed by a softmax layer. Universal Sentence Encoderè¿™ç¯‡æ–‡ç« åŸºäºŽInferSentï¼Œ ä¹Ÿæ˜¯æƒ³æ‰¾åˆ°ä¸€ä¸ªuniversal encoderã€‚ä¸åŒä¹‹å¤„åœ¨äºŽæ–‡ç« æŠŠInferSentçš„bi-lstmæ¢æˆäº†DANï¼ˆæˆ–è€…Transformer)ï¼Œè€Œä½¿ç”¨DANè¿™æ ·â€œç®€å•â€çš„encoderçš„æ•ˆæžœç«Ÿç„¶ç›¸å½“å¥½ï¼ˆå°¤å…¶æ˜¯æ—¶é—´å’Œå†…å­˜æ¶ˆè€—å’Œå…¶ä»–ç®—æ³•æ¯”å°å¾ˆå¤šã€‚ï¼‰ The Google Sentence Encoder is Googleâ€™s answer to Facebookâ€™s InferSent. It comes in two forms: an advanced model that takes the element-wise sum of the context-aware word representations produced by the encoding subgraph of a Transformer model. a simpler Deep Averaging Network (DAN) where input embeddings for words and bigrams are averaged together and passed through a feed-forward deep neural network.The Transformer-based model tends to give better results, but at the time of writing, only the DAN-based encoder was available. In contrast to InferSent, the Google Sentence Encoder was trained on a combination of unsupervised data (in a skip-thought-like task) and supervised data (the SNLI corpus). DANå…¶å®žDAN(Deep Averaging Networks)åº”è¯¥å±žäºŽBag of Wordsç±»çš„ç®—æ³•ã€‚å› ä¸ºæ¯”è¾ƒç‰¹æ®Šï¼Œå•ç‹¬åˆ—å‡ºæ¥ã€‚ å®ƒæ˜¯åœ¨å¯¹æ‰€æœ‰è¯è¯­å–å¹³å‡åŽï¼Œåœ¨ä¸Šé¢åŠ ä¸Šå‡ å±‚ç¥žç»ç½‘ç»œã€‚ç‰¹æ®Šçš„åœ°æ–¹åœ¨äºŽå®ƒåœ¨sentiment analysisä¸­è¡¨çŽ°ä¹Ÿä¸é”™ï¼Œè¿™åœ¨BOWç±»æ–¹æ³•ä¸­æ¯”è¾ƒç½•è§ã€‚ æ–°æ–¹æ³• ç±»åž‹ åŸºäºŽçš„æ—§ç®—æ³• è´¡çŒ® SIF æ— ç›‘ç£ BOW ä¸€ä¸ªç®€å•è€Œæœ‰æ•ˆçš„baselineç®—æ³• InferSent ç›‘ç£ NA æ‰¾åˆ°äº†NLPé¢†åŸŸçš„ImageNet â€“ SNLIï¼Œ å¹¶ç»™å‡ºäº†ä¸€ä¸ªstate-of-art ç®—æ³• P-mean æ— ç›‘ç£ BOW æ¯”SIFæ›´ç®€å•ä¸”æœ‰æ•ˆçš„ä¸€ä¸ªç®—æ³•ä¸”é€‚ç”¨äºŽcross-lingual Universal-sentence-encoder ç›‘ç£ InferSent æ›´åŠ ç®€å•çš„encoder æ–‡ç« å…±æå‡ºä¸¤ç§åŸºäºŽä¸åŒç½‘ç»œæž¶æž„çš„Universal Sentence Encoderï¼šTransformer and Deep Averaging Network (DAN).Our two encoders have different design goals. One based on the transformer architecture targets high accuracy at the cost of greater model complexity and resource consumption. The other targets efficient inference with slightly reduced accuracy. deep contextualized word representationsintroduction: è¿™ç§embedding -context å¿…è¦æ€§çš„ä»‹ç»ï¼Œæ„Ÿè§‰æ˜¯æœ‰æ›´å¥½ï¼Œæ²¡æœ‰ä¹Ÿæ˜¯èƒ½å¤Ÿç†è§£çš„ã€‚Why do we need contextualized representations?As an illustrative example, take the following two sentences: â€œThe bank on the other end of the street was robbedâ€â€œWe had a picnic on the bank of the riverâ€ Both sentences use the word â€œbankâ€, but the meaning of the word differs completely between them. This phenomenon where two identical words change meaning depending on the context is known as â€œpolysemyâ€œ, and has been an issue in the NLP deep learning community ever since word embeddings really took off. Most current neural networks are bad at handling polysemy because they use a single vector to represent the meaning of the word â€œbankâ€, regardless of the context. In reality, the vector representing any word should change depending on the words around it. ä»€ä¹ˆæ˜¯ä¸€ä¸ªå¥½çš„è¯å‘é‡ï¼šELMoèƒ½å¤Ÿå­¦ä¹ åˆ°è¯æ±‡ç”¨æ³•çš„å¤æ‚æ€§ï¼Œæ¯”å¦‚è¯­æ³•ã€è¯­ä¹‰ã€‚ELMoèƒ½å¤Ÿå­¦ä¹ ä¸åŒä¸Šä¸‹æ–‡æƒ…å†µä¸‹çš„è¯æ±‡å¤šä¹‰æ€§ã€‚ ä¹‹å‰çš„åšæ³•çš„ç¼ºç‚¹æ˜¯å¯¹äºŽæ¯ä¸€ä¸ªå•è¯éƒ½æœ‰å”¯ä¸€çš„ä¸€ä¸ªembeddingè¡¨ç¤º, è€Œå¯¹äºŽå¤šä¹‰è¯æ˜¾ç„¶è¿™ç§åšæ³•ä¸ç¬¦åˆç›´è§‰, è€Œå•è¯çš„æ„æ€åˆå’Œä¸Šä¸‹æ–‡ç›¸å…³, ELMoçš„åšæ³•æ˜¯æˆ‘ä»¬åªé¢„è®­ç»ƒlanguage model, è€Œword embeddingæ˜¯é€šè¿‡è¾“å…¥çš„å¥å­å®žæ—¶è¾“å‡ºçš„, è¿™æ ·å•è¯çš„æ„æ€å°±æ˜¯ä¸Šä¸‹æ–‡ç›¸å…³çš„äº†, è¿™æ ·å°±å¾ˆå¤§ç¨‹åº¦ä¸Šç¼“è§£äº†æ­§ä¹‰çš„å‘ç”Ÿ. è¿™ç§ç®—æ³•çš„ç‰¹ç‚¹æ˜¯ï¼šæ¯ä¸€ä¸ªword representationéƒ½æ˜¯æ•´ä¸ªè¾“å…¥è¯­å¥çš„å‡½æ•°ã€‚å…·ä½“åšæ³•å°±æ˜¯å…ˆåœ¨å¤§è¯­æ–™ä¸Šä»¥language modelä¸ºç›®æ ‡è®­ç»ƒå‡ºbidirectional LSTMæ¨¡åž‹ï¼Œç„¶åŽåˆ©ç”¨LSTMäº§ç”Ÿè¯è¯­çš„è¡¨å¾ã€‚ELMoæ•…è€Œå¾—å(Embeddings from Language Models)ã€‚ä¸ºäº†åº”ç”¨åœ¨ä¸‹æ¸¸çš„NLPä»»åŠ¡ä¸­ï¼Œä¸€èˆ¬å…ˆåˆ©ç”¨ä¸‹æ¸¸ä»»åŠ¡çš„è¯­æ–™åº“(æ³¨æ„è¿™é‡Œå¿½ç•¥æŽ‰label)è¿›è¡Œlanguage modelçš„å¾®è°ƒ,è¿™ç§å¾®è°ƒç›¸å½“äºŽä¸€ç§domain transfer; ç„¶åŽæ‰åˆ©ç”¨labelçš„ä¿¡æ¯è¿›è¡Œsupervised learningã€‚ ï¼ˆè¿™ä¸ªæè¿°è·Ÿ cv æ˜¯æƒŠäººçš„ç›¸ä¼¼ï¼‰ELMoè¡¨å¾æ˜¯â€œæ·±â€çš„ï¼Œå°±æ˜¯è¯´å®ƒä»¬æ˜¯biLMçš„æ‰€æœ‰å±‚çš„å†…éƒ¨è¡¨å¾çš„å‡½æ•°ã€‚è¿™æ ·åšçš„å¥½å¤„æ˜¯èƒ½å¤Ÿäº§ç”Ÿä¸°å¯Œçš„è¯è¯­è¡¨å¾ã€‚é«˜å±‚çš„LSTMçš„çŠ¶æ€å¯ä»¥æ•æ‰è¯è¯­æ„ä¹‰ä¸­å’Œè¯­å¢ƒç›¸å…³çš„é‚£æ–¹é¢çš„ç‰¹å¾(æ¯”å¦‚å¯ä»¥ç”¨æ¥åšè¯­ä¹‰çš„æ¶ˆæ­§)ï¼Œè€Œä½Žå±‚çš„LSTMå¯ä»¥æ‰¾åˆ°è¯­æ³•æ–¹é¢çš„ç‰¹å¾(æ¯”å¦‚å¯ä»¥åšè¯æ€§æ ‡æ³¨)ã€‚å¦‚æžœæŠŠå®ƒä»¬ç»“åˆåœ¨ä¸€èµ·ï¼Œåœ¨ä¸‹æ¸¸çš„NLPä»»åŠ¡ä¸­ä¼šä½“çŽ°ä¼˜åŠ¿ã€‚ Salient featuresELMo representations are: Contextual: The representation for each word depends on the entire context in which it is used. Deep: The word representations combine all layers of a deep pre-trained neural network. Character based: ELMo representations are purely character based, allowing the network to use morphological clues to form robust representations for out-of-vocabulary tokens unseen in training. related work: é’ˆå¯¹ä¼ ç»Ÿè¯å‘é‡æ˜¯å›ºå®šçš„ï¼Œä¸Žä¸Šä¸‹æ–‡è¯­å¢ƒæ— å…³çš„ç¼ºç‚¹ï¼Œå…ˆå‰çš„å·¥ä½œå¤šé€šè¿‡ä¸¤ç§æ–¹å¼æ¥è§£å†³ï¼š (1) é€šè¿‡å¼•å…¥å­—ç¬¦çº§(subword)ä¿¡æ¯ä¸°å¯Œè¯å‘é‡è¡¨è¾¾ï¼› (2) å­¦ä¹ æ¯ä¸ªå•è¯ä¸åŒå«ä¹‰çš„ç‹¬ç«‹å‘é‡ï¼› ELMoä¹Ÿåˆ©ç”¨äº†å­—ç¬¦å·ç§¯ï¼ˆCharacter-Convolutionsï¼‰å¼•å…¥å­—ç¬¦çº§ä¿¡æ¯ï¼Œå¹¶åŒæ—¶ç»“åˆäº†æ·±åº¦åŒå‘è¯­è¨€æ¨¡åž‹çš„å„å±‚éšçŠ¶æ€æ¥ä¸°å¯Œè¯å‘é‡è¡¨è¾¾ã€‚ P.s.ï¼šåŸºäºŽå­—ç¬¦çš„æ¨¡åž‹ä¸ä»…èƒ½å¤Ÿé€šè¿‡å¼•å…¥å­—ç¬¦çº§ä¿¡æ¯ä¸°å¯Œè¯å‘é‡è¡¨è¾¾ï¼Œä¹Ÿèƒ½å¤Ÿåœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šè§£å†³NLPé¢†åŸŸçš„OOVï¼ˆOut-Of-Vocabularyï¼‰é—®é¢˜ã€‚ ELMoç”¨åˆ°ä¸Šæ–‡æåˆ°çš„åŒå‘çš„language model, ç»™å®šNä¸ªtokens (t1, t2,â€¦,tN), language modelé€šè¿‡ç»™å®šå‰é¢çš„k-1ä¸ªä½ç½®çš„tokenåºåˆ—è®¡ç®—ç¬¬kä¸ªtokençš„å‡ºçŽ°çš„æ¦‚çŽ‡:$$p \left( t _ { 1 } , t _ { 2 } , \ldots , t _ { N } \right) = \prod _ { k = 1 } ^ { N } p \left( t _ { k } | t _ { 1 } , t _ { 2 } , \ldots , t _ { k - 1 } \right)$$åŽå‘çš„è®¡ç®—æ–¹æ³•ä¸Žå‰å‘ç›¸ä¼¼: $$p \left( t _ { 1 } , t _ { 2 } , \ldots , t _ { N } \right) = \prod _ { k = 1 } ^ { N } p \left( t _ { k } | t _ { k + 1 } , t _ { k + 2 } , \ldots , t _ { N } \right)$$biLMè®­ç»ƒè¿‡ç¨‹ä¸­çš„ç›®æ ‡å°±æ˜¯æœ€å¤§åŒ–:$$\sum _ { k = 1 } ^ { N } \left( \log p \left( t _ { k } | t _ { 1 } , \ldots , t _ { k - 1 } ; \Theta _ { x } , \vec { \Theta } _ { L S T M } , \Theta _ { s } \right) + \log p \left( t _ { k } | t _ { k + 1 } , \ldots , t _ { N } ; \Theta _ { x } , \overline { \Theta } _ { L S T M } , \Theta _ { s } \right) \right)$$ ELMoå¯¹äºŽæ¯ä¸ªtoken $t_k$, é€šè¿‡ä¸€ä¸ªLå±‚çš„biLMè®¡ç®—å‡º2L+1ä¸ªè¡¨ç¤º:$$R_{ k } = \left{ x _ { k } ^ { L M } , \vec { h } _ { k , j } ^ { L M } , h _ { k , j } ^ { L M } | j = 1 , \ldots , L \right} = \left{ h _ { k , j } ^ { L M } | j = 0 , \ldots , L \right}$$å…¶ä¸­$h _ { k , 0 } ^ { L M }$æ˜¯å¯¹tokenè¿›è¡Œç›´æŽ¥ç¼–ç çš„ç»“æžœ(è¿™é‡Œæ˜¯å­—ç¬¦é€šè¿‡CNNç¼–ç ), $h _ { k , j } ^ { L M } = \left[ \vec { h } _ { k , j } ^ { L M } ; \overline { h } _ { k , j } \right]$ æ˜¯æ¯ä¸ªbiLSTMå±‚è¾“å‡ºçš„ç»“æžœ. åœ¨å®žéªŒä¸­è¿˜å‘çŽ°ä¸åŒå±‚çš„biLMçš„è¾“å‡ºçš„tokenè¡¨ç¤ºå¯¹äºŽä¸åŒçš„ä»»åŠ¡æ•ˆæžœä¸åŒ. åº”ç”¨ä¸­å°†ELMoä¸­æ‰€æœ‰å±‚çš„è¾“å‡ºRåŽ‹ç¼©ä¸ºå•ä¸ªå‘é‡, ELMok=E(Rk;Î˜Ïµ), æœ€ç®€å•çš„åŽ‹ç¼©æ–¹æ³•æ˜¯å–æœ€ä¸Šå±‚çš„ç»“æžœåšä¸ºtokençš„è¡¨ç¤º:$E \left( R _ { k } \right) = h _ { k , L } ^ { L M }$ æ›´é€šç”¨çš„åšæ³•æ˜¯é€šè¿‡ä¸€äº›å‚æ•°æ¥è”åˆæ‰€æœ‰å±‚çš„ä¿¡æ¯:$$E L M o _ { k } ^ { t a s k } = E \left( R _ { k } ; \Theta ^ { t a s k } \right) = \gamma ^ { t a s k } \sum _ { j = 0 } ^ { L } s _ { j } ^ { t a s k } h _ { k , j } ^ { L M }$$ å…¶ä¸­$s_j$æ˜¯ä¸€ä¸ªsoftmaxå‡ºæ¥çš„ç»“æžœ, $Î³$æ˜¯ä¸€ä¸ªä»»åŠ¡ç›¸å…³çš„scaleå‚æ•°, æˆ‘è¯•äº†å¹³å‡æ¯ä¸ªå±‚çš„ä¿¡æ¯å’Œå­¦å‡ºæ¥$s_j$å‘çŽ°å­¦ä¹ å‡ºæ¥çš„æ•ˆæžœä¼šå¥½å¾ˆå¤š. æ–‡ä¸­æåˆ°$Î³$åœ¨ä¸åŒä»»åŠ¡ä¸­å–ä¸åŒçš„å€¼æ•ˆæžœä¼šæœ‰è¾ƒå¤§çš„å·®å¼‚, éœ€è¦æ³¨æ„, åœ¨SQuADä¸­è®¾ç½®ä¸º0.01å–å¾—çš„æ•ˆæžœè¦å¥½äºŽè®¾ç½®ä¸º1æ—¶. ELMo: Context Matters Instead of using a fixed embedding for each word, ELMo looks at the entire sentence before assigning each word in it an embedding. It uses a bi-directional LSTM trained on a specific task to be able to create those embeddings. ELMo provided a significant step towards pre-training in the context of NLP. The ELMo LSTM would be trained on a massive dataset in the language of our dataset, and then we can use it as a component in other models that need to handle language. Whatâ€™s ELMoâ€™s secret? ELMo gained its language understanding from being trained to predict the next word in a sequence of words - a task called Language Modeling. This is convenient because we have vast amounts of text data that such a model can learn from without needing labels. We can see the hidden state of each unrolled-LSTM step peaking out from behind ELMoâ€™s head. Those come in handy in the embedding proecss after this pre-training is done. ELMo actually goes a step further and trains a bi-directional LSTM â€“ so that its language model doesnâ€™t only have a sense of the next word, but also the previous word.ELMo comes up with the contextualized embedding through grouping together the hidden states (and initial embedding) in a certain way (concatenation followed by weighted summation). lstm-based language modelIn case you are unfamiliar with language models, a language model is simply a model that can predict how â€œlikelyâ€ a certain sequence of words is to be a real piece of text. This is generally done by training a model to take a part of sentence (say, the first n words) and predict the next word â€“ or more precisely, output the probability of each word in the vocabulary being the next word (In this blog post, weâ€™ll focus on LSTM-based language models which are the focus of this paper). One trick that this paper uses is to train a language model with reversed sentences that the authors call the â€œbackwardâ€ language model.è¿™ç§æ¨¡åž‹ï¼šä¸Šä¸€ä¸ªæ¨¡åž‹çš„è¾“å‡ºåˆ°ä¸‹ä¸€ä¸ªæ¨¡åž‹è¾“å…¥Furthermore, instead of using a single-layer LSTM, this paper uses a stacked, multi-layer LSTM. Whereas a single-layer LSTM would take the sequence of words as input, a multi-layer LSTM trains multiple LSTMs to take the output sequence of the LSTM in the previous layer as input (of course, the first layer takes the sequence of words as input). This is best illustrated in the following illustration: æœ€åŽçš„embedding æ˜¯æ˜¯å°†ä¸åŒçš„å±‚ combinationèµ·æ¥ï¼Œè¿™ä¸ªç³»æ•°æ˜¯é€šè¿‡å­¦ä¹ å‡ºæ¥çš„ã€‚In ELMo, the part that is task specific is the combination of the task-agnostic representations. The weight is learned for each task and normalized using the softmax function. The parameter $\gamma$ is a task-dependent value that allows for scaling the entire vector, which is important during optimization.]]></content>
      <categories>
        <category>NLP</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Dynamic Programming Examples]]></title>
    <url>%2F2019%2F05%2F27%2FDynamic-Programming-Examples%2F</url>
    <content type="text"><![CDATA[Dynamic Programming is a method for solving a complex problem by breaking it down into a collection of simpler subproblems, solving each of those subproblems just once, and storing their solutions using a memory-based data structure (array, map, etc.). So the next time the same sub-problem occurs, instead of recomputing its solution, one simply looks up the previously computed solution, thereby saving computation time. knapsack problem: https://leetcode.com/problems/partition-equal-subset-sum/ Given a non-empty array containing only positive integers, find if the array can be partitioned into two subsets such that the sum of elements in both subsets is equal. Actually, this is a 0/1 knapsack problem, for each number, we can pick it or not. Let us assume dp[i][j] means whether the specific sum j can be gotten from the first i numbers. If we can pick such a series of numbers from 0-i whose sum is j, dp[i][j] is true, otherwise it is false.Base case: dp[0][0] is true; (zero number consists of sum 0 is true)Transition function: For each number, if we donâ€™t pick it, dp[i][j] = dp[i-1][j], which means if the first i-1 elements has made it to j, dp[i][j] would also make it to j (we can just ignore nums[i]). If we pick nums[i]. dp[i][j] = dp[i-1][j-nums[i]], which represents that j is composed of the current value nums[i] and the remaining composed of other previous numbers. Thus, the transition function is dp[i][j] = dp[i-1][j] || dp[i-1][j-nums[i]] 12345678910111213141516171819202122class Solution(object): def canPartition(self, nums): """ :type nums: List[int] :rtype: bool """ n =len(nums) s = sum(nums) if s &amp;1 ==1: return False dp =[0 for _ in range(s+1)] dp[0] =1 #import ipdb for num in nums: for i in range(s, -1, -1): # ipdb.set_trace() if dp[i]: dp[i+num] =1 if dp[s//2]: return True return False longest commone substring Given two strings â€˜Xâ€™ and â€˜Yâ€™, find the length of the longest common substring. Input : X = â€œGeeksforGeeksâ€, y = â€œGeeksQuizâ€Output : 5 1234567891011121314151617181920212223242526272829"""Given two strings â€˜Xâ€™ and â€˜Yâ€™, find the length of the longest common substring."""class Solution(object): def minDistance(self, word1, word2): m =len(word1) n =len(word2) #dp=[ [None] for _ in range(n+1) for _ in range(m+1)] dp = [[None] *(n +1) for _ in range(m+1) ] for i in range(m+1): for j in range(n+1): if i ==0 or j ==0: dp[i][j] =0 # è¿™ä¸ªæ˜¯python ä¸­è¯­æ³•å†³å®šçš„ word1[len(word1)] æ˜¯è®¿é—®ä¸åˆ°çš„ï¼Œè¿™ä¸ªè®¿é—®æ˜¯ä»Ž0å¼€å§‹çš„ï¼Œæ‰€ä»¥åªèƒ½æ˜¯è¿™æ ·çš„ elif word1[i-1] == word2[j-1]: dp[i][j] =dp[i-1][j-1] +1 else: dp[i][j] =max(dp[i-1][j], dp[i][j-1]) return dp[m][n]solution =Solution()word1 ='abcdaf'word2 ='acbcf'result =solution.minDistance(word1, word2)print(result) matrix chain multiplication Given a sequence of matrices, find the most efficient way to multiply these matrices together. The problem is not actually to perform the multiplications, but merely to decide in which order to perform the multiplications. dp çš„æ€æƒ³å°±æ˜¯å€ŸåŠ©ä¹‹å‰çš„ subquestion çš„ç»“æžœï¼Œç„¶åŽè®¡ç®—æ›´å¤§çš„questionï¼Œè¿™ä¸ªçš„æ ¸å¿ƒåœ¨äºŽå‡å°‘äº†é‡å¤å­å•å…ƒçš„è®¡ç®—ã€‚ 1234567891011121314151617181920212223242526272829303132333435import sys# å°±æ˜¯è¿™ä¸ª l å’Œ1 æ˜¯å¾ˆä¸å®¹æ˜“åˆ†æ¸…æ¥šçš„ï¼Œæ‰€ä»¥è¿™ä¸ªæ˜¯æ…Žé‡ä½¿ç”¨çš„ def MatrixChainOrder(list1, len1): """ :param list1: list of matrix style :param l: len1gth of list :return: """ dp = [[0 for _ in range(len1)] for _ in range(len1)] for i in range(1, len1): dp[i][i] = 0 # l ä¸ªmatrix è¿žæˆçš„æ„æ€ for ll in range(2, len1): for i in range(1, len1 - ll + 1): j = i + ll - 1 dp[i][j] = sys.maxint for k in range(i, j): tmp = dp[i][k] + dp[k + 1][j] + list1[i - 1] * list1[k] * list1[j] if tmp &lt; dp[i][j]: dp[i][j] = tmp return dp[1][len1 - 1]# Driver program to test above functionarr = [1, 2, 3, 4]arr1 = [2, 3, 6, 4, 5]size = len(arr)size1 = len(arr1)print("Minimum number of multiplications is " + str(MatrixChainOrder(arr, size)))print("Minimum number of multiplications is " + str(MatrixChainOrder(arr1, size1)))]]></content>
      <categories>
        <category>ç®—æ³•</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Beyond Word Embedding]]></title>
    <url>%2F2019%2F05%2F22%2Fbeyond-word-embedding%2F</url>
    <content type="text"><![CDATA[Traditional Word VectorsBefore diving directly into Word2Vec itâ€™s worth while to do a brief overview of some of the traditional methods that pre-date neural embeddings. è¿™ä¸ªæ˜¯ç”¨æ¥æè¿°æ–‡ç« çš„ï¼Œæœ‰ä¸€ä¸ªå¤§çš„dictï¼Œç„¶åŽä¸€ç‰‡æ–‡ç« æ˜¯å¦‚ä½•è¿›è¡Œè¡¨ç¤ºã€Bag of Words or BoW vector representations are the most common used traditional vector representation. Each word or n-gram is linked to a vector index and marked as 0 or 1 depending on whether it occurs in a given document. An example of a one hot bag of words representation for documents with one word. å±€é™æ€§: ä¸€æ–¹é¢åªæ˜¯ä¸€ç§counterï¼Œæ²¡æœ‰è€ƒè™‘è¯­ä¹‰ä¿¡æ¯ï¼›å¦ä¸€æ–¹é¢æœ‰äº› words æ˜¯æ˜Žæ˜¾çš„ relevant than others.BoW representations are often used in methods of document classification where the frequency of each word, bi-word or tri-word is a useful feature for training classifiers. One challenge with bag of word representations is that they donâ€™t encode any information with regards to the meaning of a given word.In BoW word occurrences are evenly weighted independently of how frequently or what context they occur. However in most NLP tasks some words are more relevant than others. è¿™ä¸ªæ˜¯å¯ä»¥è®¤è¯†æ˜¯å¯¹äºŽ bag of words â€œrelevantâ€ ä¸Šçš„æ”¹è¿›ï¼šä½¿å¾— é€‰æ‹©çš„words æ›´åŠ çš„ â€œrepresentativeâ€ æ–‡ç« çš„è°ƒæ€§ã€‚TF-IDF, short for term frequencyâ€“inverse document frequency, is a numerical statistic that is intended to reflect how important a word or n-gram is to a document in a collection or corpus. They provide some weighting to a given word based on the context it occurs.The tfâ€“idf value increases proportionally to the number of times a word appears in a document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently than others. ä½†æ˜¯å¯¹äºŽ bag of words ä¸­â€œæ²¡æœ‰ä½“çŽ°è¯­ä¹‰â€ çš„ç¼ºé™·è¿˜æ˜¯æ²¡æœ‰ deal withã€‚However even though tf-idf BoW representations provide weights to different words they are unable to capture the word meaning. è¿™ä¸ªåå­—åªæ˜¯å› ä¸ºæœ‰å®šä¹‰è€Œå­˜åœ¨çš„åå­—ï¼ˆç½‘ç»œæ¨¡åž‹ or æ·±åº¦ç½‘ç»œçš„å‡ºçŽ°å°±æ˜¯ä¸ºäº† handle è¯­ä¹‰ä¿¡æ¯ï¼‰Distributional Embeddings enable word vectors to encapsulate contextual context. Each embedding vector is represented based on the mutual information it has with other words in a given corpus.é‡ç‚¹å°±æ˜¯è¿™ç§æ–¹å¼æ˜¯è¦ predict a target word from context wordsï¼Œä¸€å®šæ˜¯è¦èƒ½å¤Ÿä½“çŽ°è¯­å¢ƒçš„ã€‚Predictive models learn their vectors in order to improve their predictive ability of a loss such as the loss of predicting the vector for a target word from the vectors of the surrounding context words.word2vec æ˜¯ä¸€ç§æ€æƒ³ï¼Œæœ‰ä¸¤ç§CBOW å’Œskip-gram ä¸¤ç§å®žçŽ°ã€‚Word2Vec is a predictive embedding model. There are two main Word2Vec architectures that are used to produce a distributed representation of words: CBOWä¸­çš„ context words æ²¡æœ‰ä½“çŽ° words orderï¼Œä½†æ˜¯è®­ç»ƒé€Ÿåº¦å¿«ã€‚Continuous bag-of-words (CBOW) â€” The order of context words does not influence prediction (bag-of-words assumption). skip-gram weights nearbt context words heavily æ•ˆæžœç›¸å¯¹äºŽ cbow æ›´åŠ ï¼Œ ä½†è®­ç»ƒé€Ÿåº¦ç›¸å¯¹æ¯”è¾ƒæ…¢ã€‚Continuous skip-gram weighs nearby context words more heavily than more distant context words. While order still is not captured each of the context vectors are weighed and compared independently vs CBOW which weighs against the average context. CBOW is faster while skip-gram is slower but does a better job for infrequent words. glove ï¼ˆg lou vï¼‰è¿™ä¸ªä¸å±žäºŽ word2vecï¼Œå…¶åªæ˜¯è€ƒè™‘äº† local contextï¼Œæ²¡æœ‰è€ƒè™‘ global contextBoth CBOW and Skip-Grams are â€œpredictiveâ€ models, in that they only take local contexts into account. word2vec does not take advantage of global context.(ç»†èŠ‚ èƒ½çœ‹æ‡‚å°±çœ‹) GloVe embeddings by contrast leverage the same intuition behind the co-occurrence matrix (å…±ç”ŸçŸ©é˜µ) used distributional embeddings, but uses neural methods to decompose the co-occurrence matrix into more expressive and dense word vectors.æ•ˆæžœï¼šå’Œword2vec ç›¸æ¯”æ²¡æœ‰ definitively better resultsWhile GloVe vectors are faster to train, neither GloVe or Word2Vec has been shown to provide definitively better results rather they should both be evaluated for a given dataset. fasttextè¿™ä¸ªä¸»è¦æ˜¯ each word + n-gram within each wordï¼Œ æœ€åŽçš„æ•ˆæžœæ˜¯å¥½äºŽ word2vec çš„ã€‚FastText, builds on Word2Vec by learning vector representations for each word and the n-grams found within each word. The values of the representations are then averaged into one vector at each training step. While this adds a lot of additional computation to training it enables word embeddings to encode sub-word information. FastText vectors have been shown to be more accurate than Word2Vec vectors by a number of different measures. overview of Neural NLP ArchitecturesDeep Feed Forward Networks 1D CNNs RNNs (LSTM/GRU) encoder- decoder ç»“æž„ attention and copy mechanismsè¿™ä¸ªæ˜¯ attention æœºåˆ¶æå‡ºçš„èƒŒæ™¯ï¼šè§£å†³ å¥å­ä¸­çš„é•¿ä¾èµ–ï¼›contextual impact (specific words may carry more importance at different steps)While in theory they can capture long term dependencies they tend to struggle modeling longer sequences, this is still an open problem. One cause for sub-optimal performance standard RNN encoder-decoder models for sequence to sequence tasks such as NER or translation is that they weight the impact each input vector evenly on each output vector when in reality specific words in the input sequence may carry more importance at different time steps.Attention mechanisms provide a means of weighting the contextual impact of each input vector on each output prediction of the RNN. These mechanisms are responsible for much of the current or near current state of the art in Natural language processing. attention In sum, algorithms can allocate attention, and they can learn how to do so, by adjusting the weights they assign to various inputs. Imagine a heat map over a photo. The heat is attention. è¿™ä¸ªæ˜¯è¦å¼•å‡ºæ¥ context word embeddings.One of the limits of traditional word vectors is that they presume that a wordâ€™s meaning is relatively stable across sentences.å¹¶ä¸æ˜¯ç‰©ç†ä¸Šçš„äºŒç»´å…³ç³»èƒ½å¤Ÿè¡¨ç¤ºè¯è¯­ä¹‹é—´çš„ relationshipï¼Œæœ‰æ—¶å€™æ˜¯éœ€è¦é«˜çº¬ç©ºé—´è¿›è¡Œè¡¨ç¤ºçš„ã€‚In fact, the strongest relationships binding a given word to the rest of the sentence may be with words quite distant from it.ä»Ž credit assignmentçš„è§’åº¦é˜è¿°äº† neural networks å°±æ˜¯ allocating importance to input featuresã€‚The fundamental task of all neural networks is credit assignment. Credit assignment is allocating importance to input features through the weights of the neural networkâ€™s model. Learning is the process by which neural networks figure out which input features correlate highly with the outcomes the net tries to predict, and their learnings are embodied in the adjusted quantities of the weights that result in accurate decisions about the data theyâ€™re exposed to.è¿™ä¸ªæ˜¯ä¼ ç»Ÿçš„ LSTM ï¼ˆencoder -decoderï¼‰ æ¨¡åž‹ï¼Œé—®é¢˜åœ¨äºŽå½“å¥å­è¿‡é•¿ï¼ˆæ¯”å¦‚è¯´å¤§äºŽ20 wordsï¼‰ä¹‹åŽï¼Œencoder æ˜¯æ— æ³• memory ä¹‹å‰çš„æ‰€æœ‰ wordsï¼Œæ‰€ä»¥æ•ˆæžœå°±ä¼šå˜å¾—å·®ä¸€äº›ã€‚ ä½†æ˜¯ attention å°±æ˜¯æ¨¡ä»¿äº†äººç¿»è¯‘è¿‡ç¨‹ï¼Œä¸€æ®µä½œä¸ºä¸€ä¸ªå•ä½ï¼Œç„¶åŽè¿›è¡Œç¿»è¯‘ã€‚è¿™æ ·å°±å¯ä»¥æŒç»­ä¿è¯è¾ƒé«˜ä¸­ç¡®çŽ‡çš„è¾“å‡ºã€‚In neural networks, attention primarily serves as a memory-access mechanism. æ¯æ¬¡çš„è¾“å‡ºéƒ½æ˜¯å…³æ³¨ä¸åŒçš„åœ°æ–¹ï¼Œä½†æ˜¯è‡³äºŽå“ªé‡Œæ›´åŠ é‡è¦ï¼Œè¿™ä¸ªäº¤ç»™äº† feedback mechanism åå‘ä¼ æ’­ã€‚ä¸‹é¢çš„å›¾ç‰‡ååˆ†æ¸…æ™°çš„å±•ç¤ºäº† åœ¨ç¿»è¯‘çš„è¿‡ç¨‹ä¸­ â€œfocusâ€ æ˜¯ä¸æ–­åœ°å˜åŒ–çš„ã€‚Above, a model highlights which pixels it is focusing on as it predicts the underlined word in the respective captions. Below, a language model highlights the words from one language, French, that were relevant as it produced the English words in the translation. As you can see, attention provides us with a route to interpretability. We can render attention as a heat map over input data such as words and pixels, and thus communicate to human operators how a neural network made a decision. (This could be the basis of a feedback mechanism whereby those humans tell the network to pay attention to certain features and not others.) (This could be the basis of a feedback mechanism whereby those humans tell the network to pay attention to certain features and not others.) Additionally in Machine Reading Comprehension and Summarization systems RNNs often tend to generate results, that while on first glance look structurally correct are in reality hallucinated or incorrect. One mechanism that helps mitigate some of these issues is the Copy Mechanism.copy mechanism ç®€å•è¯´æ¥å°±æ˜¯ word embedding or raw text.The copy mechanism is an additional layer applied during decoding that decides whether it is better to generate the next word from the source sentence or from the general embedding vocabulary. reading comprehension and summary ä¸Šé¢æ˜¯è¯´çš„åœ¨ machine translationï¼Œä¸‹é¢è¯´çš„æ˜¯ é˜…è¯»ç†è§£ å’Œ summaryé¢†åŸŸã€‚Additionally in Machine Reading Comprehension and Summarization systems RNNs often tend to generate results, that while on first glance look structurally correct are in reality hallucinated or incorrect. One mechanism that helps mitigate some of these issues is the Copy Mechanism.copy mechanism ç®€å•è¯´æ¥å°±æ˜¯ decide word embedding from model or raw text.The copy mechanism is an additional layer applied during decoding that decides whether it is better to generate the next word from the source sentence or from the general embedding vocabulary. Taming Recurrent Neural Networks for Better Summarizationæœ‰ä¸¤ç§ä¸åŒçš„ summarization:Two types of summarizationï¼šExtractive ï¼ˆYou might think of these approaches as like a highlighter.ï¼‰ Abstractiveï¼ˆBy the same analogy, these approaches are like a pen.ï¼‰The great majority of existing approaches to automatic summarization are extractive â€“ mostly because it is much easier to select text than it is to generate text from scratch.ä½†æ˜¯ä¸€ä¸ªé—®é¢˜åœ¨äºŽï¼Œåªæ˜¯ä½¿ç”¨ extrative way å¯èƒ½å¾—åˆ°ç›¸åŒçš„wordsï¼ŒProblem 1: The summaries sometimes reproduce factual details inaccurately (e.g. Germany beat Argentina 3-2). This is especially common for rare or out-of-vocabulary words such as 2-0.Problem 2: The summaries sometimes repeat themselves (e.g. Germany beat Germany beat Germany beatâ€¦)Easier Copying with Pointer-Generator Networksã€‚è¿™ä¸ªè·Ÿ attention ä¸æ˜¯å¾ˆç›¸å…³ï¼Œç®€å•è¯´å°±æ˜¯In this way, the pointer-generator network is a best of both worlds, combining both extraction (pointing) and abstraction (generating). To tackle Problem 2 (repetitive summaries), we use a technique called coverage. The idea is that we use the attention distribution to keep track of whatâ€™s been covered so far, and penalize the network for attending to same parts again. elmo (e l mo)elmo äº§ç”Ÿä¸€ä¸ª embedding æ˜¯æ ¹æ® context äº§ç”Ÿçš„ã€‚ELMo is a model generates embeddings for a word based on the context it appears thus generating slightly different embeddings for each of its occurrence.ï¼ˆæ„Ÿè§‰ç†è§£ä¸€ä¸ªæ¦‚å¿µéƒ½æ˜¯ æ ¹æ®å…¶ for example è¿›è¡Œç†è§£çš„ï¼‰For example, the word â€œplayâ€ in the sentence above using standard word embeddings encodes multiple meanings such as the verb to play or in the case of the sentence a theatre production. In standard word embeddings such as Glove, Fast Text or Word2Vec each instance of the word play would have the same representation. å‚è€ƒblog:https://towardsdatascience.com/beyond-word-embeddings-part-2-word-vectors-nlp-modeling-from-bow-to-bert-4ebd4711d0ec http://www.abigailsee.com/2017/04/16/taming-rnns-for-better-summarization.html https://skymind.ai/wiki/word2vec]]></content>
      <categories>
        <category>NLP</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Hyper-parameter Optimization for Machine Learning]]></title>
    <url>%2F2019%2F05%2F22%2FHyperparameter-optimization-for-machine-learning%2F</url>
    <content type="text"><![CDATA[Following are four common methods of hyperparameter optimization for machine learning in order of increasing efficiency: Manual Grid search Random search Bayesian model-based optimization Random SearchFirst we will implement a common technique for hyperparameter optimization: random search. Each iteration, we choose a random set of model hyperparameters from a search space. Empirically, random search is very effective, returning nearly as good results as grid search with a significant reduction in time spent searching. However, it is still an uninformed method in the sense that it does not use past evaluations of the objective function to inform the choices it makes for the next evaluation. Random search uses the following four parts, which also are used in Bayesian hyperparameter optimization: Domain: values over which to search Optimization algorithm: pick the next values at random! (yes this qualifies as an algorithm) Objective function to minimize: in this case our metric is cross validation ROC AUC Results history that tracks the hyperparameters tried and the cross validation metric Random search can be implemented in the Scikit-Learn library using RandomizedSearchCV, however, because we are using Early Stopping (to determine the optimal number of estimators), we will have to implement the method ourselves (more practice!). This is pretty straightforward, and many of the ideas in random search will transfer over to Bayesian hyperparameter optimization. 123456789101112131415161718192021222324252627282930313233343536373839# Load librariesfrom scipy.stats import uniformfrom sklearn import linear_model, datasetsfrom sklearn.model_selection import RandomizedSearchCV# data and model# Load datairis = datasets.load_iris()X = iris.datay = iris.target# Create logistic regressionlogistic = linear_model.LogisticRegression()# Create Hyperparameter Search Space# Create regularization penalty space# å¦‚æžœæ¯”è¾ƒå°‘ï¼Œé‚£ä¹ˆä¹…æžšä¸¾å‡ºæ¥penalty = ['l1', 'l2']# å¦‚æžœæ˜¯æœ‰è§„å¾‹çš„è¿žç»­çš„ï¼Œå°±ä½¿ç”¨è¿™ç§æ–¹å¼åˆ—ä¸¾å‡ºæ¥# Create regularization hyperparameter distribution using uniform distributionC = uniform(loc=0, scale=4)# Create hyperparameter optionshyperparameters = dict(C=C, penalty=penalty)# cv: cross validation, This approach involves randomly dividing the set of observations into k groups, or folds, of approximately equal size. The first fold is treated as a validation set, and the method is fit on the remaining k âˆ’ 1 folds.# Create randomized search 5-fold cross validation and 100 iterationsclf = RandomizedSearchCV(logistic, hyperparameters, random_state=1, n_iter=100, cv=5, verbose=0, n_jobs=-1)# Fit randomized searchbest_model = clf.fit(X, y)# View best hyperparameters# æ³¨æ„è¿™ç§èŽ·å–best params çš„æ–¹å¼print('Best Penalty:', best_model.best_estimator_.get_params()['penalty'])print('Best C:', best_model.best_estimator_.get_params()['C'])# Predict target vectorbest_model.predict(X) grid search and random search 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364import numpy as npfrom time import timefrom scipy.stats import randint as sp_randintfrom sklearn.model_selection import GridSearchCVfrom sklearn.model_selection import RandomizedSearchCVfrom sklearn.datasets import load_digitsfrom sklearn.ensemble import RandomForestClassifier# get some datadigits = load_digits()X, y = digits.data, digits.target# build a classifierclf = RandomForestClassifier(n_estimators=20)# Utility function to report best scoresdef report(results, n_top=3): for i in range(1, n_top + 1): candidates = np.flatnonzero(results['rank_test_score'] == i) for candidate in candidates: print("Model with rank: &#123;0&#125;".format(i)) print("Mean validation score: &#123;0:.3f&#125; (std: &#123;1:.3f&#125;)".format( results['mean_test_score'][candidate], results['std_test_score'][candidate])) print("Parameters: &#123;0&#125;".format(results['params'][candidate])) print("")# specify parameters and distributions to sample fromparam_dist = &#123;"max_depth": [3, None], "max_features": sp_randint(1, 11), "min_samples_split": sp_randint(2, 11), "bootstrap": [True, False], "criterion": ["gini", "entropy"]&#125;# run randomized searchn_iter_search = 20random_search = RandomizedSearchCV(clf, param_distributions=param_dist, n_iter=n_iter_search, cv=5, iid=False)start = time()random_search.fit(X, y)print("RandomizedSearchCV took %.2f seconds for %d candidates" " parameter settings." % ((time() - start), n_iter_search))report(random_search.cv_results_)# use a full grid over all parametersparam_grid = &#123;"max_depth": [3, None], "max_features": [1, 3, 10], "min_samples_split": [2, 3, 10], "bootstrap": [True, False], "criterion": ["gini", "entropy"]&#125;# run grid searchgrid_search = GridSearchCV(clf, param_grid=param_grid, cv=5, iid=False)start = time()grid_search.fit(X, y)print("GridSearchCV took %.2f seconds for %d candidate parameter settings." % (time() - start, len(grid_search.cv_results_['params'])))report(grid_search.cv_results_) Random search without in-built function:write it yourself, more information.https://github.com/WillKoehrsen/hyperparameter-optimization/blob/master/Bayesian%20Hyperparameter%20Optimization%20of%20Gradient%20Boosting%20Machine.ipynb Bayesian Hyperparameter OptimizationThe one-sentence summary of Bayesian hyperparameter optimization is: build a probability model of the objective function and use it to select the most promising hyperparameters to evaluate in the true objective function. The basic idea is: spend a little more time selecting the next hyperparameters in order to make fewer calls to the objective function. In the case of hyperparameter optimization, the objective function is the validation error of a machine learning model using a set of hyperparameters. The aim is to find the hyperparameters that yield the lowest error on the validation set in the hope that these results generalize to the testing set. Evaluating the objective function is expensive because it requires training the machine learning model with a specific set of hyperparameters. Ideally, we want a method that can explore the search space while also limiting evaluations of poor hyperparameter choices. Bayesian hyperparameter tuning uses a continually updated probability model to â€œconcentrateâ€ on promising hyperparameters by reasoning from past results. æœ‰å¾ˆå¤šåŸºäºŽè¿™ç§æ€æƒ³çš„å®žçŽ°ï¼Œhyperopt åªæ˜¯å…¶ä¸­ä¸€ç§There are several Bayesian optimization libraries in Python which differ in the algorithm for the surrogate of the objective function. In this article, we will work with Hyperopt, which uses the Tree Parzen Estimator (TPE) Other Python libraries include Spearmint (Gaussian Process surrogate) and SMAC (Random Forest Regression). There are four parts to a Bayesian Optimization problem: Objective Function: what we want to minimize, in this case the validation error of a machine learning model with respect to the hyperparameters ï¼ˆåŽŸæ¥model ä¸­çš„ objective functionï¼‰ Domain Space: hyperparameter values to search over ï¼ˆè°ƒå‚ç©ºé—´ï¼‰ Optimization algorithm: method for constructing the surrogate model and choosing the next hyperparameter values to evaluate ï¼ˆloss å’Œè°ƒå‚ç©ºé—´çš„ æ–°çš„å…³ç³»ï¼‰ Result history: stored outcomes from evaluations of the objective function consisting of the hyperparameters and validation loss ï¼ˆresult æ²¡æœ‰ä»€ä¹ˆå¥½è¯´çš„ï¼‰ å…¶ä¸­çš„ Bayesian Hyperparameter Optimization using Hyperoptæ˜¯å¯ä»¥å¥½å¥½å­¦ä¹ çš„ã€‚ data scientists è¿™ç§ä¸œè¥¿æ›´åŠ è´´è¿‘äºŽ data scientist çœŸçš„ã€‚å‚è€ƒä¸€ï¼šhttps://github.com/WillKoehrsen/hyperparameter-optimization/blob/master/Bayesian%20Hyperparameter%20Optimization%20of%20Gradient%20Boosting%20Machine.ipynb å‚è€ƒäºŒï¼šhttps://github.com/WillKoehrsen/hyperparameter-optimization/blob/master/Introduction%20to%20Bayesian%20Optimization%20with%20Hyperopt.ipynb]]></content>
      <categories>
        <category>æœºå™¨å­¦ä¹ </category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Introduction to Natural Language Processing for Text]]></title>
    <url>%2F2019%2F05%2F21%2FIntroduction-to-Natural-Language-Processing-for-Text%2F</url>
    <content type="text"><![CDATA[Natural Language Processing is used to apply machine learning algorithms to text and speech. For example, we can use it to create systems like speech recognition, document summarization, machine translation, spam detection, named entity recognition, question answering, autocomplete, predictive typing and so on. NLTK (Natural Language Toolkit) is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to many corpora and lexical resources. Also, it contains a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning. Best of all, NLTK is a free, open source, community-driven project. In this article, weâ€™ll cover the following topics.è¿™äº›åŠŸèƒ½éƒ½æ˜¯å¯ä»¥ä½¿ç”¨nltk è¿›è¡Œå®žçŽ°çš„ã€‚text Lemmatization æ¯”å¦‚ï¼Œå•è¯â€œcarsâ€è¯å½¢è¿˜åŽŸåŽçš„å•è¯ä¸ºâ€œcarâ€ï¼Œå•è¯â€œateâ€è¯å½¢è¿˜åŽŸåŽçš„å•è¯ä¸ºâ€œeatâ€ã€‚ Sentence Tokenizationæ®µè½æˆå¥ã€‚Sentence tokenization (also called sentence segmentation) is the problem of dividing a string of written language into its component sentences. The idea here looks very simple. In English and some other languages, we can split apart the sentences whenever we see a punctuation mark.ï¼ˆæ ‡ç‚¹ç¬¦å·ï¼‰ Word Tokenizationå¥å­æˆè¯ï¼Œé¢—ç²’åº¦å˜å¾—æ›´å°ã€‚Word tokenization (also called word segmentation) is the problem of dividing a string of written language into its component words. In English and many other languages using some form of Latin alphabet, space is a good approximation of a word divider. Text Lemmatization and Stemmingè¿™ç§æ“ä½œå¦‚æžœè¢«è®¤ä¸ºæ˜¯ä¸€ç§ normalizationï¼Œé‚£ä¹ˆä¸€ä¸ªä¼˜ç‚¹å°±æ˜¯åŠ å¿«äº†è¿è¡Œçš„é€Ÿåº¦ã€‚ä»Žä¸åŒçš„å½¢å¼åˆ°ç»Ÿä¸€çš„å½¢å¼ï¼Œè¿™å¯ä»¥è®¤ä¸ºå‡å°‘äº†å˜é‡ã€‚æ„Ÿè§‰è¿™ä¸ªæ›´åŠ æ¶‰åŠè¯­æ³•ï¼Œè¯­æ³•æ ‘ä¹‹ç±»çš„ä¸œè¥¿ã€‚For grammatical reasons, documents can contain different forms of a word such as drive, drives, driving. Also, sometimes we have related words with a similar meaning, such as nation, national, nationality. Stemming and lemmatization are special cases of normalization. However, they are different from each other. Stemming usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes.Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma. Stop Wordså› ä¸º stop wordså¾€å¾€æ˜¯å¸¦äº† noise rather than useful informationï¼Œæ‰€ä»¥è¿™ä¸ªæ˜¯è¦åŽ»æŽ‰çš„ã€‚Stop words are words which are filtered out before or after processing of text. When applying machine learning to text, these words can add a lot of noise. Thatâ€™s why we want to remove these irrelevant words. stop words dictionary å¯ä»¥ç†è§£æˆä¸€ç§è¿‡æ»¤è¯è¡¨ï¼Œæ˜¯å¯ä»¥æ ¹æ®åº”ç”¨çš„ä¸åŒï¼Œç„¶åŽ changeçš„ã€‚Stop words usually refer to the most common words such as â€œandâ€, â€œtheâ€, â€œaâ€ in a language, but there is no single universal list of stopwords. The list of the stop words can change depending on your application. åœ¨å­˜å‚¨ stopword çš„æ—¶å€™ä½¿ç”¨ set rather than list ä¸»è¦åŽŸå› æ˜¯ much faster than search operations in a set.You might wonder why we convert our list into a set. Set is an abstract data type that can store unique values, without any particular order. The search operation in a set is much faster than the search operation in a list. For a small number of words, there is no big difference, but if you have a large number of words itâ€™s highly recommended to use the set type. RegexA kind of search pattern. A regular expression, regex, or regexp is a sequence of characters that define a search pattern. Letâ€™s see some basics. 12345678910. - match any character except newline\w - match word\d - match digit\s - match whitespace\W - match not word\D - match not digit\S - match not whitespace[abc] - match any of a, b, or c[^abc] - not match a, b, or c[a-g] - match a character between a &amp; g è¿™ä¸ªè§£é‡Šè¯´æ˜Žäº†ä¸ºä»€ä¹ˆåœ¨æ­£åˆ™è¡¨è¾¾å¼ ä¸­ä½¿ç”¨ râ€â€ ä½œä¸ºä¸€ç§å‰ç¼€ã€‚å› ä¸ºæ­£åˆ™è¡¨è¾¾æ˜¯ä¸­ â€\â€œ çš„ä½¿ç”¨å’Œ python ä¸­çš„â€\â€ ä½¿ç”¨æœ‰å†²çªã€‚ç®€è€Œè¨€ä¹‹ï¼Œå¦‚æžœåŠ ä¸Šäº† râ€â€ é‚£ä¹ˆè¿™ä¸ªå°±æ˜¯ä¸€ç§å®Œå…¨çš„ æ­£åˆ™è¡¨è¾¾å¼çš„è¯­æ³•äº†ã€‚ Regular expressions use the backslash character (â€˜\â€™) to indicate special forms or to allow special characters to be used without invoking their special meaning. This collides with Pythonâ€™s usage of the same character for the same purpose in string literals; for example, to match a literal backslash, one might have to write â€˜\\â€˜ as the pattern string, because the regular expression must be \, and each backslash must be expressed as \ inside a regular Python string literal.The solution is to use Pythonâ€™s raw string notation for regular expression patterns; backslashes are not handled in any special way in a string literal prefixed with â€˜râ€™. So râ€\nâ€ is a two-character string containing â€˜\â€™ and â€˜nâ€™, while â€œ\nâ€ is a one-character string containing a newline. Usually, patterns will be expressed in Python code using this raw string notation. An example, 1234import resentence = "The development of snowboarding was inspired by skateboarding, sledding, surfing and skiing."pattern = r"[^\w]"print(re.sub(pattern, " ", sentence)) Bag of words Machine learning algorithms cannot work with raw text directly, we need to convert the text into vectors of numbers. This is called feature extraction.The bag-of-words model is a popular and simple feature extraction technique used when we work with text. It describes the occurrence of each word within a document. è¿™ä¸ªæ˜¯ bag of wordsçš„â€ç‰¹ç‚¹â€œï¼š order or structure of words æ²¡æœ‰ä½“çŽ°å‡ºæ¥ã€‚Any information about the order or structure of words is discarded. Thatâ€™s why itâ€™s called a bag of words. This model is trying to understand whether a known word occurs in a document, but donâ€™t know where is that word in the document. The intuition is that similar documents have similar contents. Also, from a content, we can learn something about the meaning of the document. To use this model, we need to: Design a vocabulary of known words (also called tokens) Choose a measure of the presence of known words 1) æœ€ç®€å•çš„æ–¹å¼æ˜¯ â€œoccurrenceâ€ ï¼Œå¦‚æžœå‡ºçŽ°äº† æ ‡ä¸º1 å¦åˆ™æ ‡ä¸º0ï¼›è¿™ç§æ˜¯æœ€ä¸ºç®€å•çš„ bag of words æœ€çš„æ–¹å¼ï¼Œè¿™å››ä¸ªæ˜¯ä¸€ä¸€å¯¹åº”çš„ã€‚æ³¨æ„ä½“ä¼šã€‚ The complexity of the bag-of-words model comes in deciding how to design the vocabulary of known words (tokens) and how to score the presence of known words. bag of words ä¸­ä½¿ç”¨ â€œoccurrenceâ€ çš„æ–¹å¼çš„ç¼ºç‚¹ï¼šç¨€ç–çŸ©é˜µï¼ˆå½“dict å¾ˆå¤§çš„æ—¶å€™ï¼Œæ–‡ç« çš„ representationä¸­æœ‰ç›¸å½“æˆåˆ†çš„0ï¼‰ã€‚ In some cases, we can have a huge amount of data and in this cases, the length of the vector that represents a document might be thousands or millions of elements. Furthermore, each document may contain only a few of the known words in the vocabulary.Therefore the vector representations will have a lot of zeros. These vectors which have a lot of zeros are called sparse vectors. They require more memory and computational resources.We can decrease the number of the known words when using a bag-of-words model to decrease the required memory and computational resources. We can use the text cleaning techniques weâ€™ve already seen in this article before we create our bag-of-words model: å‡å°‘ dictionary size çš„æ–¹å¼ã€‚ Ignoring punctuationRemoving the stop words from our documentsReducing the words to their base form (Text Lemmatization and Stemming)Fixing misspelled words n-gram çš„æ€æƒ³æ˜¯å¾ˆå¹¿æ³›ï¼šé€šè¿‡ sequence of wordsï¼Œè¿™ä¸ªæ˜¯å¯ä»¥å¢žåŠ æ–‡æœ¬çš„è¡¨è¾¾åŠ›çš„ã€‚An n-gram is a sequence of a number of items (words, letter, numbers, digits, etc.). In the context of text corpora, n-grams typically refer to a sequence of words. A unigram is one word, a bigram is a sequence of two words, a trigram is a sequence of three words etc. å…³äºŽå¦‚ä½•åŽ» score the presence of wordï¼š è¿™é‡Œæ˜¯æœ‰ä¸‰ç§æ–¹å¼çš„ã€‚We saw one very simple approach - the binary approach (1 for presence, 0 for absence).Some additional scoring methods are:2) Counts. Count the number of times each word appears in a document.3) Frequencies. Calculate the frequency that each word appears in document out of all the words in the document. TF-IDF è¿™ä¸ªè¯­å¢ƒ æ˜¯ç›¸å¯¹äºŽ frequency è€Œè¨€çš„ï¼Œå…³é”®è¯æ˜¯ä¸ä¸€å®šæœ‰ é¢‘çŽ‡æ‰€å†³å®šï¼Œè€Œä¸€äº› rarer or domain-specific words å¯èƒ½æ˜¯æ›´åŠ å¸¸è§çš„ã€‚One problem with scoring word frequency is that the most frequent words in the document start to have the highest scores. These frequent words may not contain as much â€œinformational gainâ€ to the model compared with some rarer and domain-specific words. One approach to fix that problem is to penalize words that are frequent across all the documents. This approach is called TF-IDF. TF-IDF çš„å…³é”®åœ¨äºŽä½“çŽ°äº†â€œè¯­æ–™åº“â€ã€‚TF-IDF, short for term frequency-inverse document frequency is a statistical measure used to evaluate the importance of a word to a document in a collection or corpus. å‚è€ƒèµ„æ–™https://towardsdatascience.com/introduction-to-natural-language-processing-for-text-df845750fb63]]></content>
      <categories>
        <category>NLP</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[å‰‘æŒ‡offer-å…¶ä»–]]></title>
    <url>%2F2019%2F05%2F17%2F%E5%89%91%E6%8C%87offer-%E5%85%B6%E4%BB%96%2F</url>
    <content type="text"><![CDATA[è¿™æ˜¯å‰‘æŒ‡offer ç³»åˆ—å››éƒ¨æ›²ä¸­çš„æœ€åŽä¸€éƒ¨ï¼Œå› ä¸ºæœ‰äº›ç®—æ³•é¢˜ç›®ç±»åˆ«æ•°é‡å¤ªå°‘å°±æ±‡æ€»åˆ°äº†â€å…¶ä»–â€œ, æ¯”å¦‚ä½è¿ç®—ã€æ­£åˆ™åŒ¹é…ç­‰ã€‚ç¬¬ä¸€éƒ¨å…³äºŽå­—ç¬¦ä¸²å’Œæ•°ç»„ï¼Œç¬¬äºŒéƒ¨æ˜¯æ ˆã€é˜Ÿåˆ—ã€é“¾è¡¨å’Œæ ‘ï¼Œ ç¬¬ä¸‰éƒ¨é€’å½’ã€å›žæº¯å’ŒåŠ¨æ€è§„åˆ’ã€‚ äºŒè¿›åˆ¶ä¸­1çš„ä¸ªæ•° è¾“å…¥ä¸€ä¸ªæ•´æ•°ï¼Œè¾“å‡ºè¯¥æ•°äºŒè¿›åˆ¶è¡¨ç¤ºä¸­1çš„ä¸ªæ•°ã€‚å…¶ä¸­è´Ÿæ•°ç”¨è¡¥ç è¡¨ç¤ºã€‚ Tipsï¼šé¦–å…ˆå¤„ç†æ­£æ•°, bitwise and operationï¼Œ å¾ˆç®€å•ã€‚å¯¹äºŽè´Ÿæ•°ï¼Œéœ€è¦è½¬æ¢æˆ æ­£æ•°ç„¶åŽè¿›è¡Œå¤„ç†ï¼Œmathã€‚ 123456789101112131415161718192021class Solution: def NumberOf1(self, n): # write code here if n == 0: return 0 if n &gt; 0: counts = self.number_of_positive(n) else: n = abs(n) - 1 counts = 32 - self.number_of_positive(n) return counts def number_of_positive(self, n): if n == 0: return 0 counts = 0 while n: counts += (n &amp; 1) n = n &gt;&gt; 1 return counts æ•°å€¼çš„æ•´æ•°æ¬¡æ–¹ ç»™å®šä¸€ä¸ªdoubleç±»åž‹çš„æµ®ç‚¹æ•°baseå’Œintç±»åž‹çš„æ•´æ•°exponentã€‚æ±‚baseçš„exponentæ¬¡æ–¹ã€‚ Tips: æ¬¡æ–¹ä½¿ç”¨ä¹˜æ³•æ¥è¿›è¡Œç´¯ä¹˜ 1234567891011121314151617181920212223242526272829303132333435class Solution: """ è¿™ä¸ªå°±æ˜¯è¾¹ç•Œæ¡ä»¶æ¯”è¾ƒå¤šè€Œå·²ï¼Œéœ€è¦åˆ†åˆ«åˆ¤æ–­ base å’Œ exponent çš„æ­£è´Ÿ """ def Power(self, base, exponent): # write code here if base == 0 and exponent != 0: return 0 if base != 0 and exponent == 0: return 1 flag = 1 if base &lt;= 0 and (exponent % 2 == 1): flag = -1 base = abs(base) result = 1 if exponent &gt; 0: reverse = 0 else: reverse = 1 exponent = abs(exponent) if exponent % 2 == 0: result = base * base for i in range(exponent // 2 - 1): result = result * result else: result = base * base for i in range(exponent // 2 - 1): result = result * result result = result * base if reverse: result = 1.0 / result return result * flag æœ€å°çš„Kä¸ªæ•° è¾“å…¥nä¸ªæ•´æ•°ï¼Œæ‰¾å‡ºå…¶ä¸­æœ€å°çš„Kä¸ªæ•°ã€‚ä¾‹å¦‚è¾“å…¥4,5,1,6,2,7,3,8è¿™8ä¸ªæ•°å­—ï¼Œåˆ™æœ€å°çš„4ä¸ªæ•°å­—æ˜¯1,2,3,4,ã€‚ Tipsï¼š è¿™ä¸ªæœ‰ç‚¹æŠ•æœºå–å·§äº†ï¼Œä½¿ç”¨äº† â€œheapqâ€ çš„åº“å‡½æ•°ã€‚è¿™ä¸ªé¢˜ç›®è·Ÿ ç¬¬ Kä¸ª smallest æ˜¯æœ‰å·®åˆ«çš„ï¼Œå¿«æŽ’ä¸­çš„ partition æ˜¯æ‰¾åˆ°äº† ä¸€ä¸ªæ•°å­—åœ¨æœ€åŽæŽ’åºç»“æžœä¸­çš„ä½ç½®ã€‚å¯¹äºŽæœ‰â€ç´¯åŠ â€œå‰ Kä¸ªæ•°å­—è¿˜æ˜¯è¦ä½¿ç”¨å¸¸è§„çš„æŽ’åºã€‚æ¯”è¾ƒå¥½çš„å°±æ˜¯å †æŽ’åºã€‚ 123456789class Solution: # æƒ³è¯´çš„æ˜¯æ—¢ç„¶æ˜¯ä½¿ç”¨è¿™ç§å¼€æºçš„åº“å‡½æ•° é‚£ä¹ˆå°±è®°ä½è¿™ç§å‡½æ•°åå­— def GetLeastNumbers_Solution(self, tinput, k): # write code here if len(tinput) &lt; k: return [] import heapq res = heapq.nsmallest(k, tinput) return res The function partition puts the numbers smaller than nums[left] to its left and then returns the new index of nums[left]. The returned index is actually telling us how small nums[left] ranks in nums. 123456789101112131415161718192021222324252627282930313233343536class Solution(object): def findKthLargest(self, nums, k): """ :type nums: List[int] :type k: int :rtype: int """ left, right = 0, len(nums) - 1 while True: pos = self.partition(nums, left, right) # è¿™ä¸ªåœ¨æŽ’åºçš„æ—¶å€™ï¼Œæ˜¯æŠŠå¤§çš„æ•°å­—æ”¾åˆ°å‰é¢ï¼Œè€Œå‰é¢æ˜¯pos æ˜¯ä»Ž0 å¼€å§‹çš„ï¼Œ # æ‰€ä»¥è¿™é‡Œæ˜¯ k-1 if pos == k - 1: return nums[pos] # å·¦è¾¹çš„å¹¶ä¸è¶³ä»¥æž„æˆk ä¸ªï¼Œ é‚£ä¹ˆåœ¨å³è¾¹ elif pos &lt; k - 1: left = pos + 1 else: right = pos - 1 def partition(self, nums, left, right): # choose nums[left] as pivot pivot = nums[left] # p1, p2å°±ç±»ä¼¼ working ä¸­çš„left right p1, p2 = left + 1, right while p1 &lt;= p2: if nums[p1] &lt; pivot and nums[p2] &gt; pivot: nums[p1], nums[p2] = nums[p2], nums[p1] p1, p2 = p1 + 1, p2 - 1 elif nums[p1] &gt;= pivot: p1 += 1 else: #nums[p2] &lt;= pivot: p2 -=1 nums[left], nums[p2] = nums[p2], nums[left] return p2 æ•´æ•°ä¸­1å‡ºçŽ°çš„æ¬¡æ•°ï¼ˆä»Ž1åˆ°næ•´æ•°ä¸­1å‡ºçŽ°çš„æ¬¡æ•°ï¼‰ æ±‚å‡º1~13çš„æ•´æ•°ä¸­1å‡ºçŽ°çš„æ¬¡æ•°,å¹¶ç®—å‡º100~1300çš„æ•´æ•°ä¸­1å‡ºçŽ°çš„æ¬¡æ•°ï¼Ÿä¸ºæ­¤ä»–ç‰¹åˆ«æ•°äº†ä¸€ä¸‹1~13ä¸­åŒ…å«1çš„æ•°å­—æœ‰1ã€10ã€11ã€12ã€13å› æ­¤å…±å‡ºçŽ°6æ¬¡,ä½†æ˜¯å¯¹äºŽåŽé¢é—®é¢˜ä»–å°±æ²¡è¾™äº†ã€‚ACMerå¸Œæœ›ä½ ä»¬å¸®å¸®ä»–,å¹¶æŠŠé—®é¢˜æ›´åŠ æ™®éåŒ–,å¯ä»¥å¾ˆå¿«çš„æ±‚å‡ºä»»æ„éžè´Ÿæ•´æ•°åŒºé—´ä¸­1å‡ºçŽ°çš„æ¬¡æ•°ï¼ˆä»Ž1 åˆ° n ä¸­1å‡ºçŽ°çš„æ¬¡æ•°ï¼‰ã€‚ Tipsï¼šmath, è®¡æ•°åŽŸç†ï¼ŒæŒ‰ä½ç»Ÿè®¡è¯¥ä½ä¸º1æ—¶å¯èƒ½åŒ…å«çš„æ•°å­—æ€»æ•°.ç”±ä½Žä½å‘é«˜ä½ä¾æ¬¡éåŽ†æ•°å­—nçš„æ¯ä¸€ä½curnã€‚è®°å½“å‰ä½æ•°ä¸ºcï¼Œcurnå·¦è¾¹ï¼ˆé«˜ä½ï¼‰çš„æ•°å­—ç‰‡æ®µä¸ºhighnï¼Œcurå³è¾¹ï¼ˆä½Žä½ï¼‰çš„æ•°å­—ç‰‡æ®µä¸ºlownï¼Œlowc = 10 ^ c è‹¥curn = 0ï¼Œåˆ™é«˜ä½èŒƒå›´ä¸º0 ~ highn - 1ï¼Œä½Žä½0 ~ lowc - 1 è‹¥curn = 1ï¼Œåˆ™é«˜ä½èŒƒå›´ä¸º0 ~ highn - 1ï¼Œä½Žä½0 ~ lowc - 1ï¼›æˆ–è€… é«˜ä½ä¸ºhighnï¼Œ ä½Žä½0 ~ lown è‹¥curn ï¼ž 1ï¼Œåˆ™é«˜ä½èŒƒå›´ä¸º0 ~ highnï¼Œ ä½Žä½ä¸º0 ~ lowc - 1 123456789101112131415161718192021222324252627282930313233343536class Solution: # æ•°å­—çš„åŸºæœ¬ç»“æž„åˆ†æˆ weights +working_num + n%base è¿™ä¸‰ä¸ªéƒ¨åˆ† # ç„¶åŽä¸€ä¸ªwhile å¾ªçŽ¯æ˜¯å¤„ç†ä¸€ä¸ªæ•°å­— def NumberOf1Between1AndN_Solution(self, n): # write code here if n &lt; 1: return 0 num = n counts = 0 base = 1 while num: weights = num % 10 num = num // 10 counts += base * num if weights == 1: counts += (n % base) + 1 elif weights &gt; 1: counts += base base *= 10 return counts ``` - æŠŠæ•°ç»„æŽ’æˆæœ€å°çš„æ•°&gt; è¾“å…¥ä¸€ä¸ªæ­£æ•´æ•°æ•°ç»„ï¼ŒæŠŠæ•°ç»„é‡Œæ‰€æœ‰æ•°å­—æ‹¼æŽ¥èµ·æ¥æŽ’æˆä¸€ä¸ªæ•°ï¼Œæ‰“å°èƒ½æ‹¼æŽ¥å‡ºçš„æ‰€æœ‰æ•°å­—ä¸­æœ€å°çš„ä¸€ä¸ªã€‚ä¾‹å¦‚è¾“å…¥æ•°ç»„&#123;3ï¼Œ32ï¼Œ321&#125;ï¼Œåˆ™æ‰“å°å‡ºè¿™ä¸‰ä¸ªæ•°å­—èƒ½æŽ’æˆçš„æœ€å°æ•°å­—ä¸º321323ã€‚Tipsï¼šä½¿ç”¨sorted() å‡½æ•°ï¼Œ string ç±»åž‹çš„æŽ’åº å’Œ int ç±»åž‹çš„æŽ’åºæ˜¯ä¸€æ ·çš„ï¼Œåœ¨python é‡Œé¢æ¥è¯´ã€‚```pythonclass Solution: def PrintMinNumber(self, numbers): # write code here sorted_list = sorted(numbers, cmp=lambda a, b: cmp(str(a) + str(b), str(b) + str(a))) # è¿™ä¸ªæ—¶å€™å·²ç»æŽ’å¥½åºï¼Œç„¶åŽåªè¦ä¸€ä¸ªä¸ªè¿žæŽ¥èµ·æ¥å°±è¡Œäº† return ''.join(map(str, sorted_list)) ä¸‘æ•° æŠŠåªåŒ…å«è´¨å› å­2ã€3å’Œ5çš„æ•°ç§°ä½œä¸‘æ•°ï¼ˆUgly Numberï¼‰ã€‚ä¾‹å¦‚6ã€8éƒ½æ˜¯ä¸‘æ•°ï¼Œä½†14ä¸æ˜¯ï¼Œå› ä¸ºå®ƒåŒ…å«è´¨å› å­7ã€‚ ä¹ æƒ¯ä¸Šæˆ‘ä»¬æŠŠ1å½“åšæ˜¯ç¬¬ä¸€ä¸ªä¸‘æ•°ã€‚æ±‚æŒ‰ä»Žå°åˆ°å¤§çš„é¡ºåºçš„ç¬¬Nä¸ªä¸‘æ•°ã€‚ Tipsï¼šä¹‹åŽçš„ä¸‘æ•°è‚¯å®šæ˜¯2ï¼Œ3æˆ–5 çš„å€æ•°ï¼Œåˆ†åˆ«å•ç‹¬è®¡æ•°ï¼Œç„¶åŽé€‰æ‹©æœ€å°çš„ã€‚ 123456789101112131415161718192021class Solution: # åœ¨è¿›è¡Œ append æ“ä½œçš„æ—¶å€™åŽ»é‡ï¼Œ def GetUglyNumber_Solution(self, index): # write code here if index &lt;1: return 0 list1 = [1] # æ„å‘³ç€åªèƒ½æ˜¯ append() æ“ä½œäº† i, j, k = 0, 0, 0 while len(list1) &lt; index: num = min(list1[i] * 2, list1[j] * 3, list1[k] * 5) if num &gt; list1[-1]: list1.append(num) if num == list1[i] * 2: i += 1 elif num == list1[j] * 3: j += 1 else: k += 1 return list1[-1] æ­£åˆ™è¡¨è¾¾å¼åŒ¹é… è¯·å®žçŽ°ä¸€ä¸ªå‡½æ•°ç”¨æ¥åŒ¹é…åŒ…æ‹¬â€™.â€™å’Œâ€™â€˜çš„æ­£åˆ™è¡¨è¾¾å¼ã€‚æ¨¡å¼ä¸­çš„å­—ç¬¦â€™.â€™è¡¨ç¤ºä»»æ„ä¸€ä¸ªå­—ç¬¦ï¼Œè€Œâ€™â€˜è¡¨ç¤ºå®ƒå‰é¢çš„å­—ç¬¦å¯ä»¥å‡ºçŽ°ä»»æ„æ¬¡ï¼ˆåŒ…å«0æ¬¡ï¼‰ã€‚ åœ¨æœ¬é¢˜ä¸­ï¼ŒåŒ¹é…æ˜¯æŒ‡å­—ç¬¦ä¸²çš„æ‰€æœ‰å­—ç¬¦åŒ¹é…æ•´ä¸ªæ¨¡å¼ã€‚ä¾‹å¦‚ï¼Œå­—ç¬¦ä¸²â€aaaâ€ä¸Žæ¨¡å¼â€a.aâ€å’Œâ€abacaâ€åŒ¹é…ï¼Œä½†æ˜¯ä¸Žâ€aa.aâ€å’Œâ€ab*aâ€å‡ä¸åŒ¹é… Tips: dp é—®é¢˜ã€‚è½¬æ¢æ–¹ç¨‹ dp[i][j] i è¡¨ç¤º string çš„index jè¡¨ç¤º pattern çš„indexï¼Œ dp[i][j] ==dp[i-1][j-1] or dp[i][j] =dp[i][j-2] or dp[i][j-1] ã€‚ 123456789101112131415161718192021222324class Solution: # s, patternéƒ½æ˜¯å­—ç¬¦ä¸² # https://www.youtube.com/watch?v=l3hda49XcDE å¿ƒä¸­ä¸€å®šè¦æœ‰è¿™ä¸ªè¡¨æ ¼, a[i][j] è¿™ä¸ªæ›´åƒæ˜¯ä¸€ç§æŒ‡é’ˆ def match(self, s, pattern): if len(s) == 0 and len(pattern) == 0: return True dp = [[False for _ in range(len(pattern) + 1)] for _ in range(len(s) + 1)] dp[0][0] = True for j in range(1, len(pattern) + 1): if pattern[j - 1] == "*": dp[0][j] = dp[0][j - 2] for i in range(1, len(s) + 1): for j in range(1, len(pattern) + 1): if pattern[j - 1] == s[i - 1] or pattern[j - 1] == ".": dp[i][j] = dp[i - 1][j - 1] elif pattern[j - 1] == "*": dp[i][j] = dp[i][j - 2] if s[i - 1] == pattern[j - 2] or pattern[j - 2] == ".": dp[i][j] = dp[i][j] or dp[i - 1][j] else: dp[i][j] = False return dp[len(s)][len(pattern)] æ•°æ®æµä¸­çš„ä¸­ä½æ•° å¦‚ä½•å¾—åˆ°ä¸€ä¸ªæ•°æ®æµä¸­çš„ä¸­ä½æ•°ï¼Ÿå¦‚æžœä»Žæ•°æ®æµä¸­è¯»å‡ºå¥‡æ•°ä¸ªæ•°å€¼ï¼Œé‚£ä¹ˆä¸­ä½æ•°å°±æ˜¯æ‰€æœ‰æ•°å€¼æŽ’åºä¹‹åŽä½äºŽä¸­é—´çš„æ•°å€¼ã€‚å¦‚æžœä»Žæ•°æ®æµä¸­è¯»å‡ºå¶æ•°ä¸ªæ•°å€¼ï¼Œé‚£ä¹ˆä¸­ä½æ•°å°±æ˜¯æ‰€æœ‰æ•°å€¼æŽ’åºä¹‹åŽä¸­é—´ä¸¤ä¸ªæ•°çš„å¹³å‡å€¼ã€‚æˆ‘ä»¬ä½¿ç”¨Insert()æ–¹æ³•è¯»å–æ•°æ®æµï¼Œä½¿ç”¨GetMedian()æ–¹æ³•èŽ·å–å½“å‰è¯»å–æ•°æ®çš„ä¸­ä½æ•°ã€‚ Tipsï¼šä¸»è¦æ˜¯ä½“çŽ°äº†æ•°æ®æµï¼Œè¦æ±‚èƒ½å¤Ÿ insert å…ƒç´ ï¼Œç„¶åŽåŸºäºŽå½“å‰çš„çŠ¶æ€åŽ» getmedian() ï¼Œæ˜¯åŠ¨æ€çš„ï¼Œè€Œä¸æ˜¯é™æ€çš„ã€‚ 1234567891011121314151617181920class Solution: """ å¯¹äºŽæ•°æ®æµ è¿™ä¸ªåº”è¯¥æ˜¯ç¬¬äºŒæ¬¡æŽ¥è§¦äº†ï¼Œéœ€è¦ä½¿ç”¨ä¸€ä¸ªå…¨å±€å˜é‡ """ # è™½ç„¶çŸ¥é“è¿™ä¸ªä½¿ç”¨ å †çš„æ€æƒ³æ˜¯æ›´ä¼˜çš„ï¼Œæœç´¢æ—¶é—´å¯ä»¥Oï¼ˆ1ï¼‰ï¼Œ å †çš„è°ƒæ•´æ˜¯ O(log n) # ä½†æ˜¯æ²¡æœ‰ä»€ä¹ˆå¾ˆå¥½çš„æ•™ç¨‹ï¼Œæ‰€ä»¥æˆ‘ä¹Ÿæ²¡æœ‰å­¦ä¼šå•Š def __init__(self): self.list1 = [] def Insert(self, num): self.list1.append(num) def GetMedian(self, ch): length = len(self.list1) # æˆ‘è®°å¾—æœ‰ä¸€ä¸ªæ›´åŠ å¿«ä¸€äº› self.list1 = sorted(self.list1) if length % 2 == 0: return (self.list1[length // 2] + self.list1[length // 2 - 1]) / 2.0 else: return self.list1[length // 2] æ»‘åŠ¨çª—å£çš„æœ€å¤§å€¼ ç»™å®šä¸€ä¸ªæ•°ç»„å’Œæ»‘åŠ¨çª—å£çš„å¤§å°ï¼Œæ‰¾å‡ºæ‰€æœ‰æ»‘åŠ¨çª—å£é‡Œæ•°å€¼çš„æœ€å¤§å€¼ã€‚ä¾‹å¦‚ï¼Œå¦‚æžœè¾“å…¥æ•°ç»„{2,3,4,2,6,2,5,1}åŠæ»‘åŠ¨çª—å£çš„å¤§å°3ï¼Œé‚£ä¹ˆä¸€å…±å­˜åœ¨6ä¸ªæ»‘åŠ¨çª—å£ï¼Œä»–ä»¬çš„æœ€å¤§å€¼åˆ†åˆ«ä¸º{4,4,6,6,6,5}ï¼› é’ˆå¯¹æ•°ç»„{2,3,4,2,6,2,5,1}çš„æ»‘åŠ¨çª—å£æœ‰ä»¥ä¸‹6ä¸ªï¼š {[2,3,4],2,6,2,5,1}ï¼Œ {2,[3,4,2],6,2,5,1}ï¼Œ {2,3,[4,2,6],2,5,1}ï¼Œ {2,3,4,[2,6,2],5,1}ï¼Œ {2,3,4,2,[6,2,5],1}ï¼Œ {2,3,4,2,6,[2,5,1]}ã€‚ Tipsï¼šä½¿ç”¨max(list1) è¿™æ ·çš„æ“ä½œæ˜¯å¯è¡Œçš„ã€‚ 123456789class Solution: # æœ€ç®€å•çš„æ¨¡æ‹Ÿæ»‘åŠ¨çª—å£ çš„è¿‡ç¨‹ def maxInWindows(self, num, size): slip = [] if not num or len(num) &lt; size or size == 0: return [] for i in range(len(num) - size + 1): slip.append(max(num[i:i + size])) return slip]]></content>
      <categories>
        <category>ç®—æ³•</category>
      </categories>
      <tags>
        <tag>å‰‘æŒ‡offer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[å‰‘æŒ‡offer-é€’å½’ã€å›žæº¯å’ŒåŠ¨æ€è§„åˆ’]]></title>
    <url>%2F2019%2F05%2F17%2F%E5%89%91%E6%8C%87offer-%E9%80%92%E5%BD%92-%E5%9B%9E%E6%BA%AF%E5%92%8C%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%2F</url>
    <content type="text"><![CDATA[è¿™æ˜¯å‰‘æŒ‡offer ç³»åˆ—å››éƒ¨æ›²ä¸­çš„ç¬¬ä¸‰éƒ¨ï¼šé€’å½’ã€å›žæº¯å’ŒåŠ¨æ€è§„åˆ’ã€‚ç¬¬ä¸€éƒ¨å…³äºŽå­—ç¬¦ä¸²å’Œæ•°ç»„ï¼Œç¬¬äºŒéƒ¨æ˜¯æ ˆã€é˜Ÿåˆ—ã€é“¾è¡¨å’Œæ ‘ï¼Œ æœ€åŽä¸€éƒ¨åˆ†åœ¨è¿™é‡Œã€‚ æ–æ³¢é‚£å¥‘æ•°åˆ— å¤§å®¶éƒ½çŸ¥é“æ–æ³¢é‚£å¥‘æ•°åˆ—ï¼ŒçŽ°åœ¨è¦æ±‚è¾“å…¥ä¸€ä¸ªæ•´æ•°nï¼Œè¯·ä½ è¾“å‡ºæ–æ³¢é‚£å¥‘æ•°åˆ—çš„ç¬¬né¡¹ï¼ˆä»Ž0å¼€å§‹ï¼Œç¬¬0é¡¹ä¸º0ï¼‰ã€‚n&lt;=39 Tips: ç®€å•çš„é€’å½’ï¼Œå¯ä»¥è½¬æ¢æˆå¾ªçŽ¯ã€‚ 1234567891011121314class Solution: # python ä¸­list çš„åˆå§‹åŒ–ï¼Œæœ€å¼€å§‹çš„æ˜¯ä»Ž0 å¼€å§‹ï¼Œæ‰€ä»¥æ˜¯éœ€è¦å¤šè¿›è¡Œä¸€ä¸ªåˆå§‹åŒ–çš„ def Fibonacci(self, n): # write code here if n == 0: return 0 if n == 1: return 1 arr = [0] * (n + 1) arr[0] = 0 arr[1] = 1 for i in range(2, n + 1): arr[i] = arr[i - 1] + arr[i - 2] return arr[n] è·³å°é˜¶ ä¸€åªé’è›™ä¸€æ¬¡å¯ä»¥è·³ä¸Š1çº§å°é˜¶ï¼Œä¹Ÿå¯ä»¥è·³ä¸Š2çº§ã€‚æ±‚è¯¥é’è›™è·³ä¸Šä¸€ä¸ªnçº§çš„å°é˜¶æ€»å…±æœ‰å¤šå°‘ç§è·³æ³•ï¼ˆå…ˆåŽæ¬¡åºä¸åŒç®—ä¸åŒçš„ç»“æžœï¼‰ã€‚ Tipsï¼š åŒä¸Šã€‚ 123456789101112131415class Solution: def jumpFloor(self, number): # write code here if number == 1: return 1 if number == 2: return 2 arr = [0] * (number + 1) arr[1] = 1 arr[2] = 2 for i in range(3, number + 1): arr[i] = arr[i - 1] + arr[i - 2] return arr[number] è·³å°é˜¶2 ä¸€åªé’è›™ä¸€æ¬¡å¯ä»¥è·³ä¸Š1çº§å°é˜¶ï¼Œä¹Ÿå¯ä»¥è·³ä¸Š2çº§â€¦â€¦å®ƒä¹Ÿå¯ä»¥è·³ä¸Šnçº§ã€‚æ±‚è¯¥é’è›™è·³ä¸Šä¸€ä¸ªnçº§çš„å°é˜¶æ€»å…±æœ‰å¤šå°‘ç§è·³æ³•ã€‚ Tipsï¼šåŒä¸Šã€‚ 12345678910111213class Solution: """ åœ¨ä½¿ç”¨ forå¾ªçŽ¯çš„æ—¶å€™ï¼Œæ³¨æ„ range() è¿™ç§å–å€¼ï¼Œç©¶ç«Ÿæ˜¯ä½¿ç”¨ range() ä½œä¸ºæ¬¡æ•°çš„è®¡é‡ï¼› è¿˜æ˜¯è¦ä½¿ç”¨range ä¸­çš„index ã€‚ä¸¤è€…æ˜¯ä¸ç›¸åŒçš„æ“ä½œï¼Œå°¤å…¶æ˜¯å¯¹äºŽå‰åŽçš„å–å€¼ã€‚ """ def jumpFloorII(self, number): # write code here if number == 1: return 1 nums = 1 for i in range(number - 1): nums = nums * 2 return nums çŸ©å½¢è¦†ç›– æˆ‘ä»¬å¯ä»¥ç”¨2*1çš„å°çŸ©å½¢æ¨ªç€æˆ–è€…ç«–ç€åŽ»è¦†ç›–æ›´å¤§çš„çŸ©å½¢ã€‚è¯·é—®ç”¨nä¸ª2*1çš„å°çŸ©å½¢æ— é‡å åœ°è¦†ç›–ä¸€ä¸ª2*nçš„å¤§çŸ©å½¢ï¼Œæ€»å…±æœ‰å¤šå°‘ç§æ–¹æ³•ï¼Ÿ Tips: math, æ‰¾å‡ºé€’å½’æ–¹ç¨‹ã€‚ 1234567891011121314151617"""æ—¢ç„¶ç»“æžœåªæ˜¯æœ€åŽä¸€ä¸ªè§£ï¼Œé‚£ä¹ˆå°±æ²¡æœ‰å¿…è¦ä¿å­˜ä¸­é—´å˜é‡ï¼Œæ‰€ä»¥åªæ˜¯ï¼Œæ‰€ä»¥ç©ºé—´å¤æ‚åº¦ä»ŽOï¼ˆnï¼‰ -&gt; O(1) ï¼Œè¿™ä¸ªæ˜¯è¶…çº§niceçš„"""class Solution: def rectCover(self, number): # write code here if number &lt;= 0: return 0 if number &lt;= 2: return number # åªæ˜¯ä¸¤ä¸ªå˜é‡ç½¢äº† a, b = 1, 2 while number &gt; 2: a, b = b, a + b number -= 1 return b æœºå™¨äººçš„è¿åŠ¨èŒƒå›´ åœ°ä¸Šæœ‰ä¸€ä¸ªmè¡Œå’Œnåˆ—çš„æ–¹æ ¼ã€‚ä¸€ä¸ªæœºå™¨äººä»Žåæ ‡0,0çš„æ ¼å­å¼€å§‹ç§»åŠ¨ï¼Œæ¯ä¸€æ¬¡åªèƒ½å‘å·¦ï¼Œå³ï¼Œä¸Šï¼Œä¸‹å››ä¸ªæ–¹å‘ç§»åŠ¨ä¸€æ ¼ï¼Œä½†æ˜¯ä¸èƒ½è¿›å…¥è¡Œåæ ‡å’Œåˆ—åæ ‡çš„æ•°ä½ä¹‹å’Œå¤§äºŽkçš„æ ¼å­ã€‚ ä¾‹å¦‚ï¼Œå½“kä¸º18æ—¶ï¼Œæœºå™¨äººèƒ½å¤Ÿè¿›å…¥æ–¹æ ¼ï¼ˆ35,37ï¼‰ï¼Œå› ä¸º3+5+3+7 = 18ã€‚ä½†æ˜¯ï¼Œå®ƒä¸èƒ½è¿›å…¥æ–¹æ ¼ï¼ˆ35,38ï¼‰ï¼Œå› ä¸º3+5+3+8 = 19ã€‚è¯·é—®è¯¥æœºå™¨äººèƒ½å¤Ÿè¾¾åˆ°å¤šå°‘ä¸ªæ ¼å­ï¼Ÿ Tipsï¼šé€’å½’ï¼Œè½¬ç§»æ–¹ç¨‹ä¸éš¾ï¼Œåœ¨ä¸Šä¸‹å·¦å³å››ä¸ªæ–¹å‘è¿›è¡Œå°è¯•ï¼Œéœ€è¦åˆ¤æ–­çš„æ¡ä»¶æ¯”è¾ƒå¤šï¼Œæ¯”å¦‚æ˜¯å¦è®¿é—®è¿‡ï¼Œæ•°ä½ä¹‹å’Œç­‰ä¸€äº›æ¡ä»¶ã€‚ 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465class Solution: def movingCount(self, threshold, rows, cols): # visited ä¸ä¸€å®šæ˜¯äºŒç»´çš„ï¼Œåªè¦æ˜¯èƒ½å¤Ÿ"è‡ªåœ†å…¶è¯´" å°±è¡Œã€‚ visited = [False] * (rows * cols) count = self.movingCountCore(threshold, rows, cols, 0, 0, visited) return count def movingCountCore(self, threshold, rows, cols, row, col, visited): count = 0 # å°±æ˜¯è¿™ä¸ªè®¿é—®è®°å½•æ˜¯éœ€è¦è¿›è¡Œå˜åŒ–çš„ï¼Œ å¦‚æžœæ˜¯false ï¼Œç„¶åŽè®¿é—®ä¹‹åŽ æ˜¯éœ€è¦è®¾ç½®ä¸º trueçš„ if self.check(threshold, rows, cols, row, col, visited): visited[row * cols + col] = True count = 1 + self.movingCountCore(threshold, rows, cols, row, col - 1, visited) + \ self.movingCountCore(threshold, rows, cols, row, col + 1, visited) + \ self.movingCountCore(threshold, rows, cols, row + 1, col, visited) + \ self.movingCountCore(threshold, rows, cols, row - 1, col, visited) return count def check(self, threshold, rows, cols, row, col, visited): if row &gt;= 0 and row &lt; rows and col &gt;= 0 and col &lt; cols and self.judge(threshold, row, col) and not visited[row * cols + col]: return True else: return False def judge(self, threshold, i, j): if sum(map(int, str(i) + str(j))) &lt;= threshold: return True else: return False``` - çŸ©é˜µä¸­çš„è·¯å¾„&gt; è¯·è®¾è®¡ä¸€ä¸ªå‡½æ•°ï¼Œç”¨æ¥åˆ¤æ–­åœ¨ä¸€ä¸ªçŸ©é˜µä¸­æ˜¯å¦å­˜åœ¨ä¸€æ¡åŒ…å«æŸå­—ç¬¦ä¸²æ‰€æœ‰å­—ç¬¦çš„è·¯å¾„ã€‚è·¯å¾„å¯ä»¥ä»ŽçŸ©é˜µä¸­çš„ä»»æ„ä¸€ä¸ªæ ¼å­å¼€å§‹ï¼Œæ¯ä¸€æ­¥å¯ä»¥åœ¨çŸ©é˜µä¸­å‘å·¦ï¼Œå‘å³ï¼Œå‘ä¸Šï¼Œå‘ä¸‹ç§»åŠ¨ä¸€ä¸ªæ ¼å­ã€‚å¦‚æžœä¸€æ¡è·¯å¾„ç»è¿‡äº†çŸ©é˜µä¸­çš„æŸä¸€ä¸ªæ ¼å­ï¼Œåˆ™ä¹‹åŽä¸èƒ½å†æ¬¡è¿›å…¥è¿™ä¸ªæ ¼å­ã€‚ ä¾‹å¦‚ a b c e s f c s a d e e è¿™æ ·çš„3 X 4 çŸ©é˜µä¸­åŒ…å«ä¸€æ¡å­—ç¬¦ä¸²"bcced"çš„è·¯å¾„ï¼Œä½†æ˜¯çŸ©é˜µä¸­ä¸åŒ…å«"abcb"è·¯å¾„ï¼Œå› ä¸ºå­—ç¬¦ä¸²çš„ç¬¬ä¸€ä¸ªå­—ç¬¦bå æ®äº†çŸ©é˜µä¸­çš„ç¬¬ä¸€è¡Œç¬¬äºŒä¸ªæ ¼å­ä¹‹åŽï¼Œè·¯å¾„ä¸èƒ½å†æ¬¡è¿›å…¥è¯¥æ ¼å­ã€‚Tipsï¼š åœ¨äºŒç»´æ•°ç»„ä¸­æ¯ä¸ªç‚¹ä¸Šéƒ½è¿›è¡Œå°è¯•ï¼Œæ¯ä¸ªç‚¹ä¸ŠåŒæ ·æ˜¯ä¸Šä¸‹å·¦å³è¿›è¡Œå°è¯•ï¼Œè¿”å›žç¬¦åˆæ¡ä»¶çš„ã€‚```pythonclass Solution: # é€’å½’ è¿™ä¸ªæ˜¯ true or false åˆ¤æ–­ç±»åž‹çš„ã€‚ # æ€è·¯ï¼šå…ˆæ˜¯ rows* cols è¿™æ ·çš„å…¨éƒ¨éåŽ† def hasPath(self, matrix, rows, cols, path): # å¦‚æžœä½¿ç”¨ [ for _in range(rows) ] for _ in range(cols) ï¼Œ è¿™ä¸ªæ˜¯æœ‰ç»“æž„çš„ rows* cols assist = [True] * rows * cols for i in range(rows): for j in range(cols): if self.rightPath(matrix, rows, cols, i, j, path, assist): return True return False def rightPath(self, matrix, rows, cols, i, j, path, assist): if not path: return True index = i * cols + j if i &lt; 0 or i &gt;= rows or j &lt; 0 or j &gt;= cols or matrix[index] != path[0] or assist[index] == False: return False assist[index] = False if (self.rightPath(matrix, rows, cols, i + 1, j, path[1:], assist) or self.rightPath(matrix, rows, cols, i - 1, j, path[1:], assist) or self.rightPath(matrix, rows, cols, i, j - 1, path[1:], assist) or self.rightPath(matrix, rows, cols, i, j + 1, path[1:], assist)): return True assist[index] = True return False]]></content>
      <categories>
        <category>ç®—æ³•</category>
      </categories>
      <tags>
        <tag>å‰‘æŒ‡offer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[å‰‘æŒ‡Offer-æ ˆã€é˜Ÿåˆ—ã€é“¾è¡¨å’Œæ ‘]]></title>
    <url>%2F2019%2F05%2F17%2F%E5%89%91%E6%8C%87Offer-%E6%A0%88-%E9%98%9F%E5%88%97-%E9%93%BE%E8%A1%A8%E5%92%8C%E6%A0%91%2F</url>
    <content type="text"><![CDATA[è¿™æ˜¯å‰‘æŒ‡offer ç³»åˆ—å››éƒ¨æ›²ä¸­çš„ç¬¬äºŒéƒ¨ï¼šæ ˆã€é˜Ÿåˆ—ã€é“¾è¡¨å’Œæ ‘ã€‚ç¬¬ä¸€éƒ¨å…³äºŽå­—ç¬¦ä¸²å’Œæ•°ç»„ï¼Œç¬¬ä¸‰éƒ¨æ˜¯é€’å½’ã€å›žæº¯å’ŒåŠ¨æ€è§„åˆ’ï¼Œ æœ€åŽä¸€éƒ¨åˆ†åœ¨è¿™é‡Œã€‚ ä»Žå°¾åˆ°å¤´æ‰“å°é“¾è¡¨ è¾“å…¥ä¸€ä¸ªé“¾è¡¨ï¼ŒæŒ‰é“¾è¡¨å€¼ä»Žå°¾åˆ°å¤´çš„é¡ºåºè¿”å›žä¸€ä¸ªArrayListã€‚ Tips: ä»Žå°¾åˆ°å¤´ï¼Œè€ƒå¯Ÿæ˜¯æ ˆçš„æ•°æ®ç»“æž„ï¼Œåœ¨python ä¸­ä½¿ç”¨list æ¥å®žçŽ°æ ˆã€‚ 123456789101112131415161718# -*- coding:utf-8 -*-# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: # è¿”å›žä»Žå°¾éƒ¨åˆ°å¤´éƒ¨çš„åˆ—è¡¨å€¼åºåˆ—ï¼Œä¾‹å¦‚[1,2,3] # += , -= è¿™ä¸ªéƒ½æ˜¯åŒä¸€ç§ç±»åž‹çš„ def printListFromTailToHead(self, listNode): # write code here arraylist =[] head = listNode while head != None: arraylist += [head.val] # è¿™ä¸ªåœ¨è¿™é‡Œç­‰æ•ˆäºŽ arraylist.append(head.val) head = head.next return arraylist[::-1] % é‡å»ºäºŒå‰æ ‘ è¾“å…¥æŸäºŒå‰æ ‘çš„å‰åºéåŽ†å’Œä¸­åºéåŽ†çš„ç»“æžœï¼Œè¯·é‡å»ºå‡ºè¯¥äºŒå‰æ ‘ã€‚å‡è®¾è¾“å…¥çš„å‰åºéåŽ†å’Œä¸­åºéåŽ†çš„ç»“æžœä¸­éƒ½ä¸å«é‡å¤çš„æ•°å­—ã€‚ä¾‹å¦‚è¾“å…¥å‰åºéåŽ†åºåˆ—{1,2,4,7,3,5,6,8}å’Œä¸­åºéåŽ†åºåˆ—{4,7,2,1,5,3,8,6}ï¼Œåˆ™é‡å»ºäºŒå‰æ ‘å¹¶è¿”å›žã€‚ Tips: é€’å½’ï¼ŒäºŒå‰æ ‘çš„é¢˜ç›®å¤§å¤šæ•°éƒ½æ˜¯å¯ä»¥ä½¿ç”¨é€’å½’çš„æ€æƒ³è¿›è¡Œè§£å†³ï¼Œå› ä¸ºäºŒå‰æ ‘æœ¬èº«ç»“æž„å°±æ˜¯é€’å½’å®šä¹‰çš„ã€‚é€’å½’ä¼˜ç‚¹åœ¨äºŽä»£ç é‡æ¯”è¾ƒå°‘ã€‚ä»Žå…ˆåºéåŽ†ä¸­æ‰¾å‡ºæ ¹èŠ‚ç‚¹ï¼Œä»Žä¸­åºéåŽ†ä¸­æ‰¾å‡ºå·¦å³å­æ ‘ã€‚ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152# -*- coding:utf-8 -*-# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: # è¿”å›žæž„é€ çš„TreeNodeæ ¹èŠ‚ç‚¹ # éœ€è¦ç†è§£åœ¨å‰åºéåŽ†ä¸­æ˜¯å…ˆéåŽ†å·¦å­æ ‘çš„ï¼Œå¹¶ä¸”ä¸­åºå’Œå‰åºä¸­å·¦å­æ ‘çš„ä¸ªæ•°æ˜¯ä¸ä¼šå˜çš„ def reConstructBinaryTree(self, pre, tin): # write code here if len(pre) == 0: return None root = TreeNode(pre[0]) # è¿™ä¸ªindex å‡½æ•°æ˜¯éœ€è¦è®°ä½çš„ index = tin.index(pre[0]) # è¿™é‡Œä¹Ÿæ˜¯éœ€è¦ä¿®æ”¹çš„ # pre å’Œ tinéƒ½æ˜¯éœ€è¦ç©ºå‡ºä¸€ä¸ª root.value çš„ä½ç½®ï¼Œåªä¸è¿‡é€‰æ‹©ç©ºçš„ä½ç½®æ˜¯ä¸ä¸€æ ·çš„ root.left = self.reConstructBinaryTree(pre[1:index + 1], tin[:index]) root.right = self.reConstructBinaryTree(pre[index + 1:], tin[index + 1:]) return root``` %- ç”¨ä¸¤ä¸ªæ ˆå®žçŽ°é˜Ÿåˆ—&gt; ç”¨ä¸¤ä¸ªæ ˆæ¥å®žçŽ°ä¸€ä¸ªé˜Ÿåˆ—ï¼Œå®Œæˆé˜Ÿåˆ—çš„Pushå’ŒPopæ“ä½œã€‚ é˜Ÿåˆ—ä¸­çš„å…ƒç´ ä¸ºintç±»åž‹ã€‚Tipsï¼š åœ¨python ä¸­æ ˆç­‰åŒäºŽä½¿ç”¨list å®žçŽ°ã€‚ä½¿ç”¨ä¸¤ä¸ªæ ˆï¼Œæ„å‘³ç€ä¸€ä¸ªæ˜¯push_stack ä¸€ä¸ªæ˜¯pop_stackï¼Œä½¿ç”¨ä¸¤ä¸ªæ ˆçš„â€œåŽè¿›å…ˆå‡ºâ€è¡¨ç¤ºé˜Ÿåˆ—çš„å…ˆè¿›å…ˆå‡ºï¼ˆpush and popï¼‰ä»Žè¯­æ³•ä¸Šè®² ï¼Œif list1 ==[], é‚£ä¹ˆ list1 ==None, è¿™ä¸¤ä¸ªæ¡ä»¶æ˜¯å¯ä»¥äº¤æ¢åˆ¤æ–­çš„ã€‚ï¼ˆåœ¨list ä¸­ï¼‰```pythonclass Solution: def __init__(self): self.list1 =[] self.list2 =[] def push(self, node): # write code here self.list1.append(node) def pop(self): # return if not self.list1 and not self.list2 : return None if self.list2 : return self.list2.pop() else: while self.list1: self.list2.append(self.list1.pop()) return self.list2.pop() é“¾è¡¨ä¸­å€’æ•°ç¬¬kä¸ªç»“ç‚¹ è¾“å…¥ä¸€ä¸ªé“¾è¡¨ï¼Œè¾“å‡ºè¯¥é“¾è¡¨ä¸­å€’æ•°ç¬¬kä¸ªç»“ç‚¹ã€‚ Tipsï¼š ä¸¤ç§è§£æ³•ã€‚ä¸€ç§æ˜¯éåŽ†å­˜å‚¨åˆ°list ä¸­ï¼Œç©ºé—´å¤æ‚åº¦æ˜¯O(N), å¦å¤–ä¸€ç§æ˜¯ä¸¤ä¸ªæŒ‡é’ˆp1ï¼Œp2ï¼Œè·ç¦»ç›¸å·®kï¼Œå½“p2 åˆ°è¾¾é“¾è¡¨å°¾éƒ¨ï¼Œp1 å°±åœ¨å¯¼æ•°ç¬¬k ä¸ªä½ç½®ã€‚ 1234567891011121314151617181920212223242526272829303132# -*- coding:utf-8 -*-# class ListNode:# def __init__(self, x):# self.val = x# self.next = None"""å°è¯•ä½¿ç”¨ä¸¤ä¸ªæŒ‡é’ˆç‰ˆæœ¬p1 p2 å¹¶ä¸”è¿™ç§ length åœ¨å‘½åä¸Šæ˜¯éœ€è¦è§„èŒƒçš„, å¹¶ä¸”è¿™ç§æŒ‡é’ˆæ“ä½œï¼Œæœ€å¥½æ˜¯æ‹·è´å‡ºæ¥è¿›è¡Œæ“ä½œä¸ç®¡æ€Žä¹ˆè¯´ï¼Œè¿˜æ˜¯åº”è¯¥æ±‚è§£å‡ºæ¥ length of listNodeï¼Œè¿™ç§æ‰æ˜¯æ­£é€”å¯ä»¥ä½¿ç”¨ä¸¤ä¸ªæŒ‡é’ˆï¼Œ"""class Solution: def FindKthToTail(self, head, k): # write code here if head == None or k &lt;= 0: return None p1 = head p2 = head len1 = 0 while p1: len1 += 1 p1 = p1.next if k &gt; len1: return None p1 = head while k: p1 = p1.next k -= 1 while p1: p1 = p1.next p2 = p2.next return p2 åè½¬é“¾è¡¨ è¾“å…¥ä¸€ä¸ªé“¾è¡¨ï¼Œåè½¬é“¾è¡¨åŽï¼Œè¾“å‡ºæ–°é“¾è¡¨çš„è¡¨å¤´ã€‚ Tipsï¼š éœ€è¦ä¸‰ä¸ªæŒ‡é’ˆï¼Œcurï¼Œnext_node, preã€‚ 1234567891011121314151617181920212223242526# -*- coding:utf-8 -*-# class ListNode:# def __init__(self, x):# self.val = x# self.next = None"""ä¿®æ”¹é“¾è¡¨æ˜¯éœ€è¦ä¸‰ä¸ªæŒ‡é’ˆçš„ pre, cur, next_node å¦‚æžœå¯¹ä¸‰ä¸ªæŒ‡é’ˆåè¿›è¡Œå‘½åå¥½äº†ï¼Œé‚£ä¹ˆè¿™ä¸ªå°±æ˜¯æˆåŠŸçš„ä¸€èˆ¬äº†ï¼Œ è¿™ä¸ªä¸å®¹æ˜“æƒ³åˆ°çš„æ˜¯è®¾ç½®pre =None ï¼Œè¿™ä¸ªæ˜¯ä¸€ä¸ªç»†èŠ‚ç»éªŒæ€§çš„é—®é¢˜"""class Solution: # è¿”å›žListNode def ReverseList(self, pHead): # write code here if pHead ==None: return None pre =None cur =pHead while cur: next_node =cur.next cur.next =pre pre, cur =cur, next_node return pre åˆå¹¶ä¸¤ä¸ªæŽ’åºçš„é“¾è¡¨ Tipsï¼š å½’å¹¶æŽ’åºä¸­çš„â€œå¹¶â€ æ“ä½œï¼Œåªä¸è¿‡ç”±åŽŸæ¥çš„list æ“ä½œåˆ°çŽ°åœ¨çš„ linkedlist æ“ä½œã€‚ è¾“å…¥ä¸¤ä¸ªå•è°ƒé€’å¢žçš„é“¾è¡¨ï¼Œè¾“å‡ºä¸¤ä¸ªé“¾è¡¨åˆæˆåŽçš„é“¾è¡¨ï¼Œå½“ç„¶æˆ‘ä»¬éœ€è¦åˆæˆåŽçš„é“¾è¡¨æ»¡è¶³å•è°ƒä¸å‡è§„åˆ™ã€‚ 1234567891011121314151617181920212223242526272829303132# def __init__(self, x):# self.val = x# self.next = None"""å°±æ˜¯åœ¨ä½¿ç”¨ä¸¤ä¸ªæˆ–è€…å¤šä¸ª index (p1 or p2) éåŽ†çš„æ—¶å€™ï¼Œä¸€ä¸ªå¸¸è§çš„é”™è¯¯å°±æ˜¯å¿˜è®°äº†ä¸æ–­æ›´æ–°index"""class Solution: # è¿”å›žåˆå¹¶åŽåˆ—è¡¨ def Merge(self, pHead1, pHead2): # write code here if pHead1 == None: return pHead2 if pHead2 == None: return pHead1 head = ListNode(-1) head1 = head p1 = pHead1 p2 = pHead2 while p1 and p2: if p1.val &lt; p2.val: head.next = p1 p1 = p1.next else: head.next = p2 p2 = p2.next head = head.next if p1 == None: head.next = p2 if p2 == None: head.next = p1 return head1.next æ ‘çš„å­ç»“æž„ è¾“å…¥ä¸¤æ£µäºŒå‰æ ‘Aï¼ŒBï¼Œåˆ¤æ–­Bæ˜¯ä¸æ˜¯Açš„å­ç»“æž„ã€‚ï¼ˆpsï¼šæˆ‘ä»¬çº¦å®šç©ºæ ‘ä¸æ˜¯ä»»æ„ä¸€ä¸ªæ ‘çš„å­ç»“æž„ï¼‰ Tipsï¼šæ ¹èŠ‚ç‚¹ç›¸åŒï¼Œå·¦å³å­æ ‘ç›¸åŒã€‚ï¼ˆæ•°å€¼å’Œç»“æž„ï¼‰ã€‚å®žçŽ°çš„ä½¿ç”¨æœ‰ä¸¤ä¸ªé€’å½’ç¨‹åºï¼Œæ„å‘³ç€æœ‰ä¸¤ä¸ªè·³å‡ºçš„æ¡ä»¶ã€‚ä¸€ä¸ªæ˜¯ä»ŽAæ ‘ä¸­æ‰¾ç»“ç‚¹å’ŒB æ ‘çš„æ ¹èŠ‚ç‚¹ï¼Œä¸€ä¸ªæ˜¯æ ¹èŠ‚ç‚¹ç›¸åŒä¹‹åŽï¼Œåˆ¤æ–­å·¦å³å­æ ‘æ˜¯å¦ç›¸åŒã€‚ 12345678910111213141516171819202122232425262728293031323334# -*- coding:utf-8 -*-# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = None"""åˆ†æˆä¸¤éƒ¨ï¼šé¦–å…ˆå¯»æ‰¾ä¸¤ä¸ªæ ¹èŠ‚ç‚¹çš„å€¼æ˜¯å¦ç›¸åŒï¼›ç„¶åŽåˆ¤æ–­å­æ ‘æ˜¯å¦å®Œå…¨ç›¸åŒsubTree è¿™ä¸ªå‡½æ•°å°±æ˜¯åˆ¤æ–­å­æ ‘æ˜¯å¦å®Œå…¨ç›¸åŒçš„ï¼Œæ‰€ä»¥å‡½æ•°çš„åŠŸèƒ½ä¸€å®šè¦æžå¥½"""class Solution: def HasSubtree(self, pRoot1, pRoot2): if not pRoot1: return False if not pRoot2: return False result =False if pRoot1.val ==pRoot2.val: result =self.subTree(pRoot1, pRoot2) if result ==False: result = self.HasSubtree(pRoot1.left, pRoot2) or self.HasSubtree(pRoot1.right, pRoot2) return result def subTree(self, root1, root2): if not root2: return True if not root1: return False if root1.val ==root2.val: return self.subTree(root1.left, root2.left) and self.subTree(root1.right, root2.right) return False äºŒå‰æ ‘çš„é•œåƒ æ“ä½œç»™å®šçš„äºŒå‰æ ‘ï¼Œå°†å…¶å˜æ¢ä¸ºæºäºŒå‰æ ‘çš„é•œåƒã€‚ Tipsï¼šæ±‚è§£äºŒå‰æ ‘é•œåƒï¼ŒA çš„å·¦å³å­æ ‘å¯¹åº”ç€B çš„å³å·¦å­æ ‘ã€‚ 123456789101112131415161718192021# -*- coding:utf-8 -*-# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = None"""å°±æ˜¯åœ¨æŸä¸ªå·¦ï¼ˆå³ï¼‰å­æ ‘æ˜¯None çš„æƒ…å†µä¸‹ï¼Œè¿™ä¸ªä¹Ÿæ˜¯å¯ä»¥è¿›è¡Œäº¤æ¢çš„ï¼Œç»“æŸçš„æ ‡å¿—åº”è¯¥æ˜¯æ ¹èŠ‚ç‚¹æ˜¯å¦ä¸ºç©º"""class Solution: # è¿”å›žé•œåƒæ ‘çš„æ ¹èŠ‚ç‚¹ def Mirror(self, root): # write code here if not root: return None root.left , root.right =root.right, root.left if root.left: self.Mirror(root.left) if root.right: self.Mirror(root.right) return root åŒ…å«minå‡½æ•°çš„æ ˆ å®šä¹‰æ ˆçš„æ•°æ®ç»“æž„ï¼Œè¯·åœ¨è¯¥ç±»åž‹ä¸­å®žçŽ°ä¸€ä¸ªèƒ½å¤Ÿå¾—åˆ°æ ˆä¸­æ‰€å«æœ€å°å…ƒç´ çš„minå‡½æ•°ï¼ˆæ—¶é—´å¤æ‚åº¦åº”ä¸ºOï¼ˆ1ï¼‰ï¼‰ã€‚ Tips: è¿™ä¸ªè·Ÿâ€œä½¿ç”¨ä¸¤ä¸ªæ ˆè¡¨ç¤ºé˜Ÿåˆ—â€ æ˜¯å·®ä¸å¤šçš„ï¼Œå°±æ˜¯å•ç‹¬ä½¿ç”¨ä¸€ä¸ªlist å­˜å‚¨min å‡½æ•°è°ƒç”¨çš„ä¸€ä¸ªåˆ—è¡¨ï¼Œè¿™æ ·çš„è¯èƒ½è¾¾åˆ°æ—¶é—´å¤æ‚åº¦æ˜¯ O(1). 1234567891011121314151617181920212223242526272829303132# -*- coding:utf-8 -*-"""è¿™ä¸ªæ ˆä¸­æœ€å°çš„å…ƒç´ æ˜¯å˜åŒ–çš„ï¼Œå¥½å¥½ç†è§£ä¸€ä¸‹ï¼Œå¦‚æžœå¼¹å‡ºäº†ä¸€ä¸ªæ¯”è¾ƒå¤§çš„å…ƒç´ ï¼Œé‚£ä¹ˆæ ˆä¸­æœ€å°çš„å…ƒç´ æ˜¯ä¸å˜çš„æ‰€å«å…ƒç´ çš„æœ€å°å…ƒç´ top() and min() æ“ä½œæ˜¯ä¸éœ€è¦åˆ é™¤å…ƒç´ çš„ï¼Œ pop æ˜¯åˆ é™¤äº†å…ƒç´ """class Solution: def __init__(self): self.all_list = [] self.min_list = [] def push(self, node): # write code here if not self.min_list: self.min_list.append(node) else: self.min_list.append(min(node, self.min())) self.all_list.append(node) def pop(self): self.all_list.pop() self.min_list.pop() # write code here def top(self): return self.all_list[-1] # write code here def min(self): return self.min_list[-1] æ ˆçš„åŽ‹å…¥ã€å¼¹å‡ºåºåˆ— è¾“å…¥ä¸¤ä¸ªæ•´æ•°åºåˆ—ï¼Œç¬¬ä¸€ä¸ªåºåˆ—è¡¨ç¤ºæ ˆçš„åŽ‹å…¥é¡ºåºï¼Œè¯·åˆ¤æ–­ç¬¬äºŒä¸ªåºåˆ—æ˜¯å¦å¯èƒ½ä¸ºè¯¥æ ˆçš„å¼¹å‡ºé¡ºåºã€‚å‡è®¾åŽ‹å…¥æ ˆçš„æ‰€æœ‰æ•°å­—å‡ä¸ç›¸ç­‰ã€‚ä¾‹å¦‚åºåˆ—1,2,3,4,5æ˜¯æŸæ ˆçš„åŽ‹å…¥é¡ºåºï¼Œåºåˆ—4,5,3,2,1æ˜¯è¯¥åŽ‹æ ˆåºåˆ—å¯¹åº”çš„ä¸€ä¸ªå¼¹å‡ºåºåˆ—ï¼Œä½†4,3,5,1,2å°±ä¸å¯èƒ½æ˜¯è¯¥åŽ‹æ ˆåºåˆ—çš„å¼¹å‡ºåºåˆ—ã€‚ï¼ˆæ³¨æ„ï¼šè¿™ä¸¤ä¸ªåºåˆ—çš„é•¿åº¦æ˜¯ç›¸ç­‰çš„ï¼‰ Tips: ä½¿ç”¨ä¸€ä¸ªlist æ¥æ¨¡æ‹ŸåŽ‹å…¥å’Œå¼¹å‡ºè¿‡ç¨‹ï¼ŒéåŽ†å¼¹å‡ºåºåˆ—popVï¼Œå¦‚æžœç»“æŸï¼Œé‚£ä¹ˆreturn Trueã€‚ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849class Solution:def IsPopOrder(self, pushV, popV): if not pushV: return False tmp =[] while popV: if tmp and popV[0] == tmp[-1]: popV.pop(0) tmp.pop() elif pushV: tmp.append(pushV.pop(0)) else: return False return True ``` - ä»Žä¸Šå¾€ä¸‹æ‰“å°äºŒå‰æ ‘&gt; ä»Žä¸Šå¾€ä¸‹æ‰“å°å‡ºäºŒå‰æ ‘çš„æ¯ä¸ªèŠ‚ç‚¹ï¼ŒåŒå±‚èŠ‚ç‚¹ä»Žå·¦è‡³å³æ‰“å°ã€‚Tipsï¼š å±‚æ¬¡éåŽ†ï¼ŒéåŽ†æ ¹èŠ‚ç‚¹ä¹‹åŽåŠ å…¥å·¦å³ç»“ç‚¹ã€‚```python# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: # è¿”å›žä»Žä¸Šåˆ°ä¸‹æ¯ä¸ªèŠ‚ç‚¹å€¼åˆ—è¡¨ï¼Œä¾‹ï¼š[1,2,3] # å±‚åºéåŽ†äºŒå‰æ ‘ï¼Œ è¿™ä¸ªè·Ÿæ•°æ®ç»“æž„ é˜Ÿåˆ—æœ‰ç±»ä¼¼çš„ # nodes è£…ä¸Šç»“ç‚¹ï¼Œç„¶åŽvlaues è£…ä¸Šæ•°å€¼ def PrintFromTopToBottom(self, root): # write code here if not root: return [] nodes =[] values = [] nodes.append(root) while nodes: node = nodes.pop(0) values.append(node.val) if node.left: nodes.append(node.left) if node.right: nodes.append(node.right) return values äºŒå‰æœç´¢æ ‘çš„åŽåºéåŽ†åºåˆ— è¾“å…¥ä¸€ä¸ªæ•´æ•°æ•°ç»„ï¼Œåˆ¤æ–­è¯¥æ•°ç»„æ˜¯ä¸æ˜¯æŸäºŒå‰æœç´¢æ ‘çš„åŽåºéåŽ†çš„ç»“æžœã€‚å¦‚æžœæ˜¯åˆ™è¾“å‡ºYes,å¦åˆ™è¾“å‡ºNoã€‚å‡è®¾è¾“å…¥çš„æ•°ç»„çš„ä»»æ„ä¸¤ä¸ªæ•°å­—éƒ½äº’ä¸ç›¸åŒã€‚ Tipsï¼šäºŒå‰æœç´¢æ ‘ï¼ŒæŒ‰ç…§ä¸­åºéåŽ†çš„è¯ï¼Œå°±æ˜¯ä¸€ä¸ªæŽ’åºçš„äºŒå‰æ ‘ï¼Œæ ¹èŠ‚ç‚¹å¤§äºŽå·¦å­æ ‘ï¼Œå³å­æ ‘å¤§äºŽæ ¹èŠ‚ç‚¹ã€‚åŽåºéåŽ†åºåˆ—ä¸­æœ€åŽä¸€ä¸ªæ˜¯æ ¹èŠ‚ç‚¹ï¼Œå°äºŽæ ¹èŠ‚ç‚¹æ˜¯å·¦å­æ ‘ï¼Œå¤§äºŽæ ¹èŠ‚ç‚¹çš„æ˜¯å³å­æ ‘ï¼Œè¿™æ ·è¿›è¡Œåˆ¤æ–­ã€‚ 123456789101112131415class Solution: # åŽåºéåŽ†ç»“æžœï¼Œ æœ€åŽä¸€ä¸ªæ˜¯æ ¹èŠ‚ç‚¹ï¼Œè¿™ä¸ªæ˜¯é€’å½’çš„æ€æƒ³ # äºŒå‰æœç´¢æ ‘ï¼Œ å·¦å­æ ‘å°äºŽæ ¹èŠ‚ç‚¹ï¼Œå³å­æ ‘å¤§äºŽæ ¹èŠ‚ç‚¹ def VerifySquenceOfBST(self, sequence): # write code here if not sequence: return False root = sequence[-1] for i in range(len(sequence)): if sequence[i] &gt; root: break for j in range(i, len(sequence)): if sequence[j] &lt; root: return False return True äºŒå‰æ ‘ä¸­å’Œä¸ºæŸä¸€å€¼çš„è·¯å¾„ è¾“å…¥ä¸€é¢—äºŒå‰æ ‘çš„è·ŸèŠ‚ç‚¹å’Œä¸€ä¸ªæ•´æ•°ï¼Œæ‰“å°å‡ºäºŒå‰æ ‘ä¸­ç»“ç‚¹å€¼çš„å’Œä¸ºè¾“å…¥æ•´æ•°çš„æ‰€æœ‰è·¯å¾„ã€‚è·¯å¾„å®šä¹‰ä¸ºä»Žæ ‘çš„æ ¹ç»“ç‚¹å¼€å§‹å¾€ä¸‹ä¸€ç›´åˆ°å¶ç»“ç‚¹æ‰€ç»è¿‡çš„ç»“ç‚¹å½¢æˆä¸€æ¡è·¯å¾„ã€‚(æ³¨æ„: åœ¨è¿”å›žå€¼çš„listä¸­ï¼Œæ•°ç»„é•¿åº¦å¤§çš„æ•°ç»„é å‰) Tipsï¼š æ ‘çš„éåŽ†ï¼Œæ·±åº¦ä¼˜å…ˆç®—æ³•ï¼ˆdfsï¼‰ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: # è¿”å›žäºŒç»´åˆ—è¡¨ï¼Œå†…éƒ¨æ¯ä¸ªåˆ—è¡¨è¡¨ç¤ºæ‰¾åˆ°çš„è·¯å¾„ # æ·±åº¦ä¼˜å…ˆ dfs() è¿™æ ·çš„ä¸€ä¸ªç®—æ³• def FindPath(self, root, expectNumber): # write code here if not root: return [] self.target = expectNumber paths = [] self.dfs(root, [root.val], paths) return pathsdef dfs(self, root, path, paths): if not root.left and not root.right and sum(path) == self.target: paths.append(path) if root.left: self.dfs(root.left, path + [root.left.val], paths) if root.right: self.dfs(root.right, path + [root.right.val], paths) ``` - å¤æ‚é“¾è¡¨çš„å¤åˆ¶&gt; è¾“å…¥ä¸€ä¸ªå¤æ‚é“¾è¡¨ï¼ˆæ¯ä¸ªèŠ‚ç‚¹ä¸­æœ‰èŠ‚ç‚¹å€¼ï¼Œä»¥åŠä¸¤ä¸ªæŒ‡é’ˆï¼Œä¸€ä¸ªæŒ‡å‘ä¸‹ä¸€ä¸ªèŠ‚ç‚¹ï¼Œå¦ä¸€ä¸ªç‰¹æ®ŠæŒ‡é’ˆæŒ‡å‘ä»»æ„ä¸€ä¸ªèŠ‚ç‚¹ï¼‰ï¼Œè¿”å›žç»“æžœä¸ºå¤åˆ¶åŽå¤æ‚é“¾è¡¨çš„headã€‚ï¼ˆæ³¨æ„ï¼Œè¾“å‡ºç»“æžœä¸­è¯·ä¸è¦è¿”å›žå‚æ•°ä¸­çš„èŠ‚ç‚¹å¼•ç”¨ï¼Œå¦åˆ™åˆ¤é¢˜ç¨‹åºä¼šç›´æŽ¥è¿”å›žç©ºï¼‰Tips: å…ˆæ˜¯åœ¨åŽŸæ¥çš„é“¾è¡¨ä¸Šè¿›è¡Œäº†ç›¸åŒç»“ç‚¹çš„copyå’Œnext æŒ‡é’ˆçš„æŒ‡å‘ï¼Œç„¶åŽæ˜¯random æŒ‡é’ˆçš„æŒ‡å‘ï¼Œæœ€åŽæ˜¯å°†åŽŸå§‹é“¾è¡¨å’Œcopy çš„é“¾è¡¨è¿›è¡Œåˆ†ç¦»ã€‚```python # -*- coding:utf-8 -*- # class RandomListNode: # def __init__(self, x): # self.label = x # self.next = None # self.random = None class Solution: # è¿”å›ž RandomListNode # é¦–å…ˆæ˜¯ç»“ç‚¹çš„å¤åˆ¶å’Œ next æŒ‡é’ˆçš„è¿žæŽ¥ï¼Œ ç„¶åŽæ˜¯random æŒ‡é’ˆçš„è¿žæŽ¥ï¼Œæœ€åŽæ˜¯é€‰æ‹©å‡ºå¤åˆ¶çš„ç»“ç‚¹ def Clone(self, pHead): # write code here if not pHead: return None self.clone_nodes(pHead) self.connect_nodes(pHead) return self.select_nodes(pHead) def clone_nodes(self, head): if not head: return None while head: cloned = RandomListNode(head.label) cloned.next = head.next head.next = cloned head = cloned.next def connect_nodes(self, head): if not head: return None while head: cloned = head.next if head.random: cloned.random = head.random.next head = cloned.next def select_nodes(self, head): if not head: return None cloned =cloned_head =None # è¿™ä¸ªif çš„ä½œç”¨æ˜¯ä¸ºäº†ä¿å­˜ä¸€ä¸ª cloned_headçš„ç»“ç‚¹ï¼Œ # ä¸€å®šè¦ä»Žè¿™ä¸ªåŠŸèƒ½å‡ºå‘ if head: cloned =cloned_head =head.next head.next =cloned.next head =head.next while head: cloned.next =head.next cloned =cloned.next head.next =cloned.next head =head.next return cloned_head äºŒå‰æœç´¢æ ‘ä¸ŽåŒå‘é“¾è¡¨ è¾“å…¥ä¸€æ£µäºŒå‰æœç´¢æ ‘ï¼Œå°†è¯¥äºŒå‰æœç´¢æ ‘è½¬æ¢æˆä¸€ä¸ªæŽ’åºçš„åŒå‘é“¾è¡¨ã€‚è¦æ±‚ä¸èƒ½åˆ›å»ºä»»ä½•æ–°çš„ç»“ç‚¹ï¼Œåªèƒ½è°ƒæ•´æ ‘ä¸­ç»“ç‚¹æŒ‡é’ˆçš„æŒ‡å‘ã€‚ Tipsï¼šä¸­åºéåŽ†äºŒå‰æœç´¢æ ‘å°±æ˜¯ä¸€ç§æŽ’åºçš„ä¹¦çš„ç»“ç‚¹ï¼Œç„¶åŽæ ‘çš„å·¦å³æŒ‡é’ˆå¯ä»¥ä½œä¸ºé“¾è¡¨ä¸­çš„æŒ‡å‘ä½¿ç”¨ã€‚ 12345678910111213141516171819202122232425262728293031323334# -*- coding:utf-8 -*-# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: # ä½¿ç”¨çš„æ ‘çš„ç»“æž„ è¡¨ç¤ºä¸€ç§åŒå‘é“¾è¡¨ # äºŒå‰æœç´¢æ ‘ ï¼Œå·¦å­æ ‘å°äºŽæ ¹èŠ‚ç‚¹ï¼Œå³å­æ ‘å¤§äºŽæ ¹èŠ‚ç‚¹ # ä¸­åºéåŽ†å¾—åˆ°å°±æ˜¯ä¸€ç§æŽ’å¥½åºçš„ç»“æž„ # åªèƒ½è°ƒæ•´æ ‘ä¸­ç»“ç‚¹æŒ‡é’ˆçš„æŒ‡å‘ def Convert(self, pRootOfTree): # write code here if not pRootOfTree: return None tree = pRootOfTree res = [] self.helper(tree, res) for i in range(len(res) - 1): res[i].right = res[i + 1] res[i + 1].left = res[i] # è¿™ä¸ªè¿”å›žå€¼ä¹Ÿæ˜¯æ¯”è¾ƒé¬¼ç•œå‘€ï¼Œ å°±æ˜¯éœ€è¦è¿™æ ·è¿”å›ž return res[0]def helper(self, root, res): if not root: return None if root.left: self.helper(root.left, res) res.append(root) if root.right: self.helper(root.right, res) ä¸¤ä¸ªé“¾è¡¨çš„ç¬¬ä¸€ä¸ªå…¬å…±ç»“ç‚¹ è¾“å…¥ä¸¤ä¸ªé“¾è¡¨ï¼Œæ‰¾å‡ºå®ƒä»¬çš„ç¬¬ä¸€ä¸ªå…¬å…±ç»“ç‚¹ã€‚ Tipsï¼šå°±æ˜¯ä¸€ä¸ª m*n çš„é—®é¢˜ï¼ˆmï¼Œn åˆ†åˆ«ä»£è¡¨ä¸¤ä¸ªé“¾è¡¨çš„é•¿åº¦ï¼‰ 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647# -*- coding:utf-8 -*-# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: # ä¸¤ä¸ªæŒ‡é’ˆæŒ‡å‘çš„æ˜¯ ä¸€ä¸ªç»“ç‚¹ï¼Œä¸€ä¸ªå†…å­˜çš„ä¸¤ä¸ªæŒ‡å‘ # å°†å¯èƒ½ä¸åŒé•¿åº¦çš„ä¸¤ä¸ªé“¾è¡¨è½¬æ¢æˆç›¸åŒé•¿åº¦çš„ä¸¤ä¸ªé“¾è¡¨çš„æ¯”è¾ƒï¼Œä½¿ç”¨ def FindFirstCommonNode(self, pHead1, pHead2): # write code here if not pHead1 or not pHead2: return None p1 = pHead1 p2 = pHead2 while p1 != p2: # è¿™ä¸ªp1 åªèƒ½æŒ‡å‘äº†æœ€åŽä¸€ä¸ªç»“ç‚¹ï¼Œä½†æœ€åŽä¸€ä¸ªèŠ‚ç‚¹ä¸ä¸€å®šç›¸åŒ p1 = pHead2 if not p1 else p1.next p2 = pHead1 if not p2 else p2.next return p1 ``` - äºŒå‰æ ‘çš„æ·±åº¦&gt; è¾“å…¥ä¸€æ£µäºŒå‰æ ‘ï¼Œæ±‚è¯¥æ ‘çš„æ·±åº¦ã€‚ä»Žæ ¹ç»“ç‚¹åˆ°å¶ç»“ç‚¹ä¾æ¬¡ç»è¿‡çš„ç»“ç‚¹ï¼ˆå«æ ¹ã€å¶ç»“ç‚¹ï¼‰å½¢æˆæ ‘çš„ä¸€æ¡è·¯å¾„ï¼Œæœ€é•¿è·¯å¾„çš„é•¿åº¦ä¸ºæ ‘çš„æ·±åº¦ã€‚Tipsï¼šé€’å½’ï¼Œç›¸æ¯”äºŽäºŒå‰æ ‘çš„è·¯å¾„ï¼Œè¿™ä¸ªåªæ˜¯è¿”å›žä¸€ä¸ªæ•°å€¼å°±è¡Œã€‚```python# -*- coding:utf-8 -*-# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: """ åˆ†åˆ«æ±‚è§£ å·¦å³å­æ ‘çš„æ·±åº¦ï¼Œç„¶åŽmax(left, right) è¿™æ ·çš„æ“ä½œ """ def TreeDepth(self, pRoot): if not pRoot: return 0 left = self.TreeDepth(pRoot.left) + 1 right = self.TreeDepth(pRoot.right) + 1 # è¿™ä¸ªreturn æ˜¯æœ€åŽæ‰§è¡Œä¸€æ¬¡çš„ï¼Œç„¶åŽä¸Šé¢é‚£ä¸ªéƒ½æ˜¯ä¸æ–­çš„åœ¨è¿›è¡Œé€’å½’åŠ æ·± # è¿™ä¸ª left right å·²ç»å®Œæˆäº†ï¼Œæœ€åŽçš„æ•ˆæžœåªæ˜¯ è¿”å›ž max(left, right) è¿™æ ·å­ return max(left, right) å¹³è¡¡äºŒå‰æ ‘ è¾“å…¥ä¸€æ£µäºŒå‰æ ‘ï¼Œåˆ¤æ–­è¯¥äºŒå‰æ ‘æ˜¯å¦æ˜¯å¹³è¡¡äºŒå‰æ ‘ã€‚ Tipsï¼š å·¦å³å­æ ‘çš„æ·±åº¦å·®æœ€å¤§ä¸è¶…è¿‡1ã€‚ä¸¤ä¸ªé€’å½’ï¼Œä¸€ä¸ªæ˜¯è®¡ç®—æ ‘çš„æ·±åº¦çš„é€’å½’ï¼Œä¸€ä¸ªæ˜¯åˆ¤æ–­å·¦å³å­æ ‘æ˜¯å¦æ˜¯å¹³è¡¡äºŒå‰æ ‘çš„é€’å½’ã€‚ 123456789101112131415161718192021222324252627# -*- coding:utf-8 -*-# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: # é€’å½’å¸¸è§çš„éƒ½ä¼šæœ‰ä¸¤ä¸ªreturn è·³å‡ºæ¡ä»¶ï¼Œä¸€ä¸ªæ˜¯å¼‚å¸¸çš„æ¡ä»¶ï¼Œä¸€ä¸ªæ˜¯æ­£ç¡®çš„è¿”å›ž def get_depth(self, root): if not root: return 0 left =self.get_depth(root.left) right =self.get_depth(root.right) return max(left, right) +1 def IsBalanced_Solution(self, pRoot): if not pRoot: return True left =self.get_depth(pRoot.left) right =self.get_depth(pRoot.right) if abs(left-right) &gt;1: return False return self.IsBalanced_Solution(pRoot.left) and self.IsBalanced_Solution(pRoot.right) é“¾è¡¨ä¸­çŽ¯çš„å…¥å£ç»“ç‚¹ ç»™ä¸€ä¸ªé“¾è¡¨ï¼Œè‹¥å…¶ä¸­åŒ…å«çŽ¯ï¼Œè¯·æ‰¾å‡ºè¯¥é“¾è¡¨çš„çŽ¯çš„å…¥å£ç»“ç‚¹ï¼Œå¦åˆ™ï¼Œè¾“å‡ºnullã€‚ Tipsï¼š ä¸¤ä¸ªå¿«æ…¢æŒ‡é’ˆï¼Œå¼€æŒ‡é’ˆåœ¨çŽ¯å†…ç›¸é‡æ…¢æŒ‡é’ˆã€‚ï¼ˆä¸¤ä¸ªæŒ‡é’ˆä¸€ä¸ªéœ€è¦å†çŽ¯å¤–ï¼Œä¸€ä¸ªåœ¨çŽ¯å†…ï¼Œç„¶åŽåŒæ ·çš„é€Ÿåº¦èµ°ï¼Œæœ€åŽæ‰èƒ½ç›¸é‡ï¼‰é‡ç½®å¿«æŒ‡é’ˆåˆ°å¤´ç»“ç‚¹ï¼Œä¸¤ä¸ªæŒ‡é’ˆç›¸åŒé€Ÿåº¦ï¼Œå½“å†æ¬¡ç›¸é‡æ—¶å€™ï¼Œé‚£å°±æ˜¯å…¥å£ç»“ç‚¹ã€‚ 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556 # -*- coding:utf-8 -*- # class ListNode: # def __init__(self, x): # self.val = x # self.next = None class Solution: # çŽ°åœ¨é•¿ä¸ªè®°æ€§å§ï¼Œåœ¨ä½¿ç”¨next è¿™æ ·çš„æ—¶å€™ è¦å…ˆåˆ¤æ–­è¿™ä¸ªæ˜¯ä¸æ˜¯å­˜åœ¨çš„ def EntryNodeOfLoop(self, pHead): # write code here if not pHead or not pHead.next or not pHead.next.next: return None twoTimes =pHead.next.next oneTime =pHead.next while twoTimes != oneTime: twoTimes =twoTimes.next.next oneTime =oneTime.next twoTimes =pHead while twoTimes != oneTime: twoTimes =twoTimes.next oneTime =oneTime.next return twoTimes ``` - åˆ é™¤é“¾è¡¨ä¸­é‡å¤çš„ç»“ç‚¹&gt; åœ¨ä¸€ä¸ªæŽ’åºçš„é“¾è¡¨ä¸­ï¼Œå­˜åœ¨é‡å¤çš„ç»“ç‚¹ï¼Œè¯·åˆ é™¤è¯¥é“¾è¡¨ä¸­é‡å¤çš„ç»“ç‚¹ï¼Œé‡å¤çš„ç»“ç‚¹ä¸ä¿ç•™ï¼Œè¿”å›žé“¾è¡¨å¤´æŒ‡é’ˆã€‚ ä¾‹å¦‚ï¼Œé“¾è¡¨1-&gt;2-&gt;3-&gt;3-&gt;4-&gt;4-&gt;5 å¤„ç†åŽä¸º 1-&gt;2-&gt;5```python# -*- coding:utf-8 -*-# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: def deleteDuplication(self, pHead): # write code here head = ListNode(-1) head.next = pHead curr = pHead last = head while curr and curr.next: # val =curr.val # è¿™ä¸ªæ¡ä»¶æ¯”è¾ƒç®€å•ï¼Œæ‰€ä»¥å¯ä»¥æ”¾åˆ°å‰é¢ if curr.val != curr.next.val: curr = curr.next last = last.next else: # è¿™ä¸ªæ¡ä»¶ curr è¿˜æ˜¯éœ€è¦æ³¨æ„ä¸€ä¸‹çš„ val = curr.val # python ä¸­ condition1 and condition2 è¿™ç§æ˜¯æœ‰å…ˆåŽé¡ºåºçš„ # å¯èƒ½æ˜¯å­˜åœ¨çŸ­è·¯çŽ°è±¡çš„ï¼Œ å¦‚æžœ curr ä¸æˆç«‹ï¼Œé‚£ä¹ˆåŽé¢çš„æ˜¯ä¸ä¼šæ‰§è¡Œçš„ # è‰æ‹Ÿ while curr and val == curr.val: curr = curr.next last.next = curr return head.next äºŒå‰æ ‘çš„ä¸‹ä¸€ä¸ªç»“ç‚¹ ç»™å®šä¸€ä¸ªäºŒå‰æ ‘å’Œå…¶ä¸­çš„ä¸€ä¸ªç»“ç‚¹ï¼Œè¯·æ‰¾å‡ºä¸­åºéåŽ†é¡ºåºçš„ä¸‹ä¸€ä¸ªç»“ç‚¹å¹¶ä¸”è¿”å›žã€‚æ³¨æ„ï¼Œæ ‘ä¸­çš„ç»“ç‚¹ä¸ä»…åŒ…å«å·¦å³å­ç»“ç‚¹ï¼ŒåŒæ—¶åŒ…å«æŒ‡å‘çˆ¶ç»“ç‚¹çš„æŒ‡é’ˆã€‚ Tipsï¼šä¸­åºéåŽ†çš„ä¸‹ä¸€ä¸ªç»“ç‚¹ï¼Œå¦‚æžœå­˜åœ¨å³èŠ‚ç‚¹ï¼Œé‚£ä¹ˆä¸‹ä¸€ä¸ªç»“ç‚¹æ˜¯å³èŠ‚ç‚¹æœ€å·¦è¾¹çš„ä¸€ä¸ªç‚¹ï¼›å¦‚æžœè¯¥ç»“ç‚¹æ˜¯å…¶çˆ¶èŠ‚ç‚¹çš„å·¦ç»“ç‚¹ï¼Œé‚£ä¹ˆä¸‹ä¸€èŠ‚ç‚¹æ˜¯å…¶çˆ¶èŠ‚ç‚¹ï¼Œå¦åˆ™ä¸€ç›´å›žæº¯ã€‚ 123456789101112131415161718192021222324252627282930# -*- coding:utf-8 -*-# class TreeLinkNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = None# self.next = Noneclass Solution: # https://blog.csdn.net/fuxuemingzhu/article/details/79723819 # è¿™ä¸ªæ˜¯æ±‚è§£ä¸­åºéåŽ†ä¸­æŸä¸ªç»“ç‚¹çš„ä¸‹ä¸€ä¸ªç»“ç‚¹ # è¿™pNode å°±æ˜¯ä¸€ä¸ªæ™®é€šçš„ç»“ç‚¹ def GetNext(self, pNode): # write code here if not pNode: return None # å¦‚æžœå­˜åœ¨å³ç»“ç‚¹ if pNode.right: pNode = pNode.right while pNode.left: pNode = pNode.left return pNode # å¦‚æžœæ˜¯çˆ¶èŠ‚ç‚¹çš„å·¦å­æ ‘ else: # è¿™é‡Œä½¿ç”¨ pNode.next è¡¨ç¤ºçˆ¶èŠ‚ç‚¹ while pNode.next: if pNode == pNode.next.left: return pNode.next # è¿™ä¸ªæ˜¯å³ç»“ç‚¹ pNode = pNode.next return None å¯¹ç§°çš„äºŒå‰æ ‘ è¯·å®žçŽ°ä¸€ä¸ªå‡½æ•°ï¼Œç”¨æ¥åˆ¤æ–­ä¸€é¢—äºŒå‰æ ‘æ˜¯ä¸æ˜¯å¯¹ç§°çš„ã€‚æ³¨æ„ï¼Œå¦‚æžœä¸€ä¸ªäºŒå‰æ ‘åŒæ­¤äºŒå‰æ ‘çš„é•œåƒæ˜¯åŒæ ·çš„ï¼Œå®šä¹‰å…¶ä¸ºå¯¹ç§°çš„ã€‚ Tips: åˆ¤æ–­é•œåƒå’Œé€’å½’ç”Ÿæˆè¿›è¡Œè¿˜æ˜¯ä¸å¤ªä¸€æ ·çš„å“ˆã€‚é€’å½’åˆ¤æ–­ï¼Œæ ¹èŠ‚ç‚¹ç›¸åŒï¼Œç„¶åŽå·¦å³å­æ ‘æ˜¯å¦æ˜¯å¯¹ç§°ã€‚ 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677# -*- coding:utf-8 -*-# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: # é•œåƒçš„æ¦‚å¿µ å’Œé€’å½’ # isSame() è¿™ä¸ªå°±æ˜¯åˆ¤æ–­ä¸¤ä¸ªå­æ ‘æ˜¯å¦é•œåƒçš„æ“ä½œ def isSame(self, p, q): if not p and not q: return True # å¥½å¥½æ€è€ƒ ä¸‹é¢è¿™ä¸¤ä¸ªè·³å‡ºæ¡ä»¶ä¸ºä»€ä¹ˆæ˜¯ä¸åˆé€‚çš„ if p and q and p.val == q.val: return self.isSame(p.left, q.right) and self.isSame(p.right, q.left) def isSymmetrical(self, pRoot): # write code here # æœ€å¼€å§‹çš„æ¡ä»¶ å¦‚æžœéƒ½æ˜¯ none é‚£ä¹ˆè¿™ä¸ªæ˜¯å¯¹ç§°çš„ if not pRoot: return True if pRoot.left and not pRoot.right: return False if not pRoot.left and pRoot.right: return False return self.isSame(pRoot.left, pRoot.right) ``` - æŒ‰ä¹‹å­—å½¢é¡ºåºæ‰“å°äºŒå‰æ ‘&gt; è¯·å®žçŽ°ä¸€ä¸ªå‡½æ•°æŒ‰ç…§ä¹‹å­—å½¢æ‰“å°äºŒå‰æ ‘ï¼Œå³ç¬¬ä¸€è¡ŒæŒ‰ç…§ä»Žå·¦åˆ°å³çš„é¡ºåºæ‰“å°ï¼Œç¬¬äºŒå±‚æŒ‰ç…§ä»Žå³è‡³å·¦çš„é¡ºåºæ‰“å°ï¼Œç¬¬ä¸‰è¡ŒæŒ‰ç…§ä»Žå·¦åˆ°å³çš„é¡ºåºæ‰“å°ï¼Œå…¶ä»–è¡Œä»¥æ­¤ç±»æŽ¨ã€‚Tipsï¼šå±‚åºéåŽ†çš„å‡çº§ç‰ˆï¼Œæœ‰ä¸¤ç§æ€è·¯ï¼Œä¸€ç§æ˜¯ä½¿ç”¨å•ç‹¬ stack (list) çš„æ€æƒ³å­˜å‚¨å¶æ•°å±‚æ•°ï¼Œä¸€ç§æ˜¯å…ˆæŒ‰ç…§åŽŸå…ˆå±‚åºéåŽ†çš„æ€æƒ³ï¼Œæœ€åŽå¯¹äºŽå¶æ•°çš„ç»“æžœè¿›è¡Œâ€œç¿»è½¬â€ å¤„ç†ã€‚é€‰æ‹©åŽè€…ï¼Œå› ä¸ºä»£ç ä¸Šæ¯”è¾ƒç®€å•ã€‚```python# -*- coding:utf-8 -*-# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: # å±‚åºéåŽ† + å¶æ•°ç¿»è½¬ # https://blog.csdn.net/fuxuemingzhu/article/details/79724959 def level(self, root, level, res): """ root: the root of tree level: res: result """ if not root: return if len(res) == level: res.append([]) res[level].append(root.val) if root.left: self.level(root.left, level + 1, res) if root.right: self.level(root.right, level + 1, res) def Print(self, pRoot): # write code here if not pRoot: return [] res = [] self.level(pRoot, 0, res) for level in range(1, len(res), 2): res[level] = res[level][::-1] return res æŠŠäºŒå‰æ ‘æ‰“å°æˆå¤šè¡Œ ä»Žä¸Šåˆ°ä¸‹æŒ‰å±‚æ‰“å°äºŒå‰æ ‘ï¼ŒåŒä¸€å±‚ç»“ç‚¹ä»Žå·¦è‡³å³è¾“å‡ºã€‚æ¯ä¸€å±‚è¾“å‡ºä¸€è¡Œã€‚ Tips: å’Œä¸Šä¸€ä¸ªé¢˜ç›®ç±»ä¼¼ï¼Œåœ¨éåŽ†äºŒå‰æ ‘çš„æ—¶å€™ï¼Œå…³é”®æ˜¯åŠ å…¥äº† [level] å±‚æ•°è¿™ç§ä¿¡æ¯ã€‚ 123456789101112131415161718192021222324252627282930# -*- coding:utf-8 -*-# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: # è¿”å›žäºŒç»´åˆ—è¡¨[[1,2],[4,5]] def level(self, root, level, res): # ä½ è¿™é‡Œä¹Ÿæ²¡æœ‰è¯´è¦è¿”å›žå€¼çš„æ„æ€å‘€ï¼Œè¿™ä¸ªç›´æŽ¥æ˜¯ return if not root: return if level == len(res): res.append([]) res[level].append(root.val) if root.left: self.level(root.left, level + 1, res) if root.right: # res[level] =self.level(root.right, level+1, res) # å› ä¸ºè¿™ä¸ªæ˜¯ ä¼ çš„å€¼ï¼Œæ‰€ä»¥ä¸éœ€è¦ä½¿ç”¨è¿”å›žå€¼çš„ self.level(root.right, level + 1, res) def Print(self, pRoot): if not pRoot: return [] res = [] self.level(pRoot, 0, res) return res åºåˆ—åŒ–äºŒå‰æ ‘ è¯·å®žçŽ°ä¸¤ä¸ªå‡½æ•°ï¼Œåˆ†åˆ«ç”¨æ¥åºåˆ—åŒ–å’Œååºåˆ—åŒ–äºŒå‰æ ‘ Tipsï¼šåºåˆ—å·å’Œååºåˆ—åŒ–åªæ˜¯ä¸€ç§çº¦å®šçš„å­˜å‚¨çš„å½¢å¼ã€‚ # -*- coding:utf-8 -*- # class TreeNode: # def __init__(self, x): # self.val = x # self.left = None # self.right = None class Solution: """ åºåˆ—åŒ–å°±æ˜¯ä»Žæ ‘ç»“æž„ è½¬æˆå­—ç¬¦ä¸²çš„ç»“æž„ï¼›åä¹‹ï¼Œä¹Ÿæ˜¯æˆç«‹çš„ã€‚ ä½¿ç”¨å…ˆåºéåŽ†çš„æ–¹æ³•ã€‚ https://suixinblog.cn/2019/03/target-offer-serialize-binary-tree.html#%E4%BB%A3%E7%A0%81 """ def __init__(self): self.flag = -1 def Serialize(self, root): # write code here if not root: return "#" return str(root.val) + "," + self.Serialize(root.left) + "," + self.Serialize(root.right) def Deserialize(self, s): # write code here self.flag += 1 string = s.split(',') if self.flag &gt; len(string): return None root = None if string[self.flag] != '#': root = TreeNode(int(string[self.flag])) root.left = self.Deserialize(s) root.right = self.Deserialize(s) return root äºŒå‰æœç´¢æ ‘çš„ç¬¬kä¸ªç»“ç‚¹ ç»™å®šä¸€æ£µäºŒå‰æœç´¢æ ‘ï¼Œè¯·æ‰¾å‡ºå…¶ä¸­çš„ç¬¬kå°çš„ç»“ç‚¹ã€‚ä¾‹å¦‚ï¼Œ ï¼ˆ5ï¼Œ3ï¼Œ7ï¼Œ2ï¼Œ4ï¼Œ6ï¼Œ8ï¼‰ ä¸­ï¼ŒæŒ‰ç»“ç‚¹æ•°å€¼å¤§å°é¡ºåºç¬¬ä¸‰å°ç»“ç‚¹çš„å€¼ä¸º4ã€‚ Tips: äºŒå‰æœç´¢æ ‘ï¼Œä¸­åºéåŽ†ä¹‹åŽæœ‰åºï¼Œç„¶åŽå–ç¬¬ k ä¸ªç»“ç‚¹ã€‚ # class TreeNode: # def __init__(self, x): # self.val = x # self.left = None # self.right = None class Solution: def middle(self, root, result): if not root: return if root.left: self.middle(root.left, result) result.append(root) if root.right: self.middle(root.right, result) def KthNode(self, pRoot, k): # write code here if not pRoot: return result = [] self.middle(pRoot, result) if len(result) &lt; k or k &lt; 1: return return result[k - 1]]]></content>
      <categories>
        <category>ç®—æ³•</category>
      </categories>
      <tags>
        <tag>å‰‘æŒ‡offer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[å‰‘æŒ‡Offer-å­—ç¬¦ä¸²å’Œæ•°ç»„]]></title>
    <url>%2F2019%2F05%2F17%2F%E5%89%91%E6%8C%87Offer-%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%92%8C%E6%95%B0%E7%BB%84%2F</url>
    <content type="text"><![CDATA[é¢˜ç›®éƒ½æ˜¯æ¥è‡ªç‰›å®¢ç½‘åœ¨çº¿åˆ·é¢˜ä¸­çš„å‰‘æŒ‡offerã€‚æœ€è¿‘æ‰¾å®žä¹ å·¥ä½œï¼Œä½œä¸ºåˆ·é¢˜è®°å½•ï¼Œé¡ºä¾¿ä»Žè€ƒå¯ŸçŸ¥è¯†ç‚¹çš„è§’åº¦åˆ†ç±»æ•´ç†ã€‚ä¸»è¦åˆ†æˆä»¥ä¸‹å››å¤§ç±»ï¼š å­—ç¬¦ä¸²ã€æ•°ç»„ é“¾è¡¨ã€æ ‘ é€’å½’ã€å›žæº¯ã€åŠ¨æ€è§„åˆ’ å…¶ä»–, æ¯”å¦‚ä½è¿ç®—ã€æ­£åˆ™åŒ¹é…ç­‰ ä¸åŒç±»åˆ«ä»¥ä¸€ç« ä»‹ç»ï¼Œä¹‹åŽå¯èƒ½ä¼šéšæ—¶updateã€‚è¿™æ˜¯å‰‘æŒ‡offer ç³»åˆ—å››éƒ¨æ›²ä¸­çš„ç¬¬ä¸€éƒ¨ã€‚ç¬¬ä¸€éƒ¨å…³äºŽå­—ç¬¦ä¸²å’Œæ•°ç»„ï¼Œç¬¬äºŒéƒ¨æ˜¯æ ˆã€é˜Ÿåˆ—ã€é“¾è¡¨å’Œæ ‘ï¼Œç¬¬ä¸‰éƒ¨é€’å½’ã€å›žæº¯å’ŒåŠ¨æ€è§„åˆ’ï¼Œ æœ€åŽä¸€éƒ¨åˆ†åœ¨è¿™é‡Œã€‚ äºŒç»´æ•°ç»„ä¸­çš„æŸ¥æ‰¾ åœ¨ä¸€ä¸ªäºŒç»´æ•°ç»„ä¸­ï¼ˆæ¯ä¸ªä¸€ç»´æ•°ç»„çš„é•¿åº¦ç›¸åŒï¼‰ï¼Œæ¯ä¸€è¡Œéƒ½æŒ‰ç…§ä»Žå·¦åˆ°å³é€’å¢žçš„é¡ºåºæŽ’åºï¼Œæ¯ä¸€åˆ—éƒ½æŒ‰ç…§ä»Žä¸Šåˆ°ä¸‹é€’å¢žçš„é¡ºåºæŽ’åºã€‚è¯·å®Œæˆä¸€ä¸ªå‡½æ•°ï¼Œè¾“å…¥è¿™æ ·çš„ä¸€ä¸ªäºŒç»´æ•°ç»„å’Œä¸€ä¸ªæ•´æ•°ï¼Œåˆ¤æ–­æ•°ç»„ä¸­æ˜¯å¦å«æœ‰è¯¥æ•´æ•°ã€‚ Tips: æ•°ç»„æ˜¯ä»Žå·¦ä¸Šæ–¹åˆ°å³ä¸‹æ–¹è¿™æ ·çš„é€’å¢žï¼ŒæŒ‡é’ˆï¼ˆä¸¤ä¸ªï¼‰æœ‰ä¸¤ä¸ªè¿åŠ¨æ–¹å‘ï¼Œä¸€ä¸ªæ˜¯å‘å·¦ä¸€ä¸ªæ˜¯å‘ä¸‹ã€‚ 12345678910111213141516171819class Solution:# array äºŒç»´åˆ—è¡¨sdef Find(self, target, array): # write code here # å¯ä»¥å°è¯•ä¸€ä¸‹ åæ ‡ç§»åŠ¨çš„æ€æƒ³, æ‰€ä»¥è¿™ä¸ªå°±æ˜¯ä¸€ç§åæ ‡ç§»åŠ¨çš„æ€æƒ³ # å°±æ˜¯ä¸€ä¸ªæ¡ä»¶æœ‰äº†ä¹‹åŽ é‚£ä¹ˆæŽ¥ä¸‹æ¥çš„else ä¹Ÿå¯ä»¥é¡ºç€å°±å†™ä¸Šæ¥çš„ rows =len(array) -1 cols =len(array[0]) -1 row =0 col =cols while row &lt;= rows and col &gt;=0: if array[row][col] == target: return True elif array[row][col] &gt; target: col -=1 else: row +=1 return False æ›¿æ¢ç©ºæ ¼ è¯·å®žçŽ°ä¸€ä¸ªå‡½æ•°ï¼Œå°†ä¸€ä¸ªå­—ç¬¦ä¸²ä¸­çš„æ¯ä¸ªç©ºæ ¼æ›¿æ¢æˆâ€œ\%20â€ã€‚ä¾‹å¦‚ï¼Œå½“å­—ç¬¦ä¸²ä¸ºWe Are Happy.åˆ™ç»è¿‡æ›¿æ¢ä¹‹åŽçš„å­—ç¬¦ä¸²ä¸ºWe\%20Are\%20Happyã€‚ Tipsï¼š å­—ç¬¦ä¸²çš„éåŽ†å¯¹äºŽ python è€Œè¨€æ˜¯æ¯”è¾ƒç®€å•çš„ã€‚ 123456789101112class Solution:# s æºå­—ç¬¦ä¸²def replaceSpace(self, s): # write code here # python ä¸­çš„ str å°±æ˜¯ array of char converted ="" for ch in s: if ch ==" ": converted += "%20" else: converted += ch return converted æ—‹è½¬æ•°ç»„çš„æœ€å°æ•°å­— æŠŠä¸€ä¸ªæ•°ç»„æœ€å¼€å§‹çš„è‹¥å¹²ä¸ªå…ƒç´ æ¬åˆ°æ•°ç»„çš„æœ«å°¾ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºæ•°ç»„çš„æ—‹è½¬ã€‚ è¾“å…¥ä¸€ä¸ªéžå‡æŽ’åºçš„æ•°ç»„çš„ä¸€ä¸ªæ—‹è½¬ï¼Œè¾“å‡ºæ—‹è½¬æ•°ç»„çš„æœ€å°å…ƒç´ ã€‚ ä¾‹å¦‚æ•°ç»„{3,4,5,1,2}ä¸º{1,2,3,4,5}çš„ä¸€ä¸ªæ—‹è½¬ï¼Œè¯¥æ•°ç»„çš„æœ€å°å€¼ä¸º1ã€‚ NOTEï¼šç»™å‡ºçš„æ‰€æœ‰å…ƒç´ éƒ½å¤§äºŽ0ï¼Œè‹¥æ•°ç»„å¤§å°ä¸º0ï¼Œè¯·è¿”å›ž0ã€‚ 123456789101112131415161718192021222324class Solution:'''é¢˜ç›®æ˜¯ä¸å‡ï¼Œæ‰€ä»¥ä¸‹é¢åˆ¤æ–­çš„æ—¶å€™ä¹Ÿæ˜¯ä¸å‡ã€‚è§£é¢˜çš„å…³é”®ç‚¹ï¼š è®¾ç½®ç¬¬ä¸€ä¸ªå…ƒç´ ä¸ºå‡å®šçš„min_value,ç„¶åŽæ³¨æ„è¿™ä¸ªæ˜¯éžå‡çš„æ•°ç»„ï¼ˆæ³¨æ„å¤„ç†ç­‰å·çš„æƒ…å†µï¼‰'''def minNumberInRotateArray(self, rotateArray): # write code here arr =rotateArray if len(arr) ==0: return 0 min_value =arr[0] left, right =0, len(arr)-1 while right-left &gt;1: mid =(left+right) //2 if arr[left] &lt;=arr[mid]: left =mid elif arr[mid] &lt;= arr[right]: right =mid min_value =arr[right] return min_value è°ƒæ•´æ•°ç»„é¡ºåºä½¿å¥‡æ•°ä½äºŽå¶æ•°å‰é¢ è¾“å…¥ä¸€ä¸ªæ•´æ•°æ•°ç»„ï¼Œå®žçŽ°ä¸€ä¸ªå‡½æ•°æ¥è°ƒæ•´è¯¥æ•°ç»„ä¸­æ•°å­—çš„é¡ºåºï¼Œä½¿å¾—æ‰€æœ‰çš„å¥‡æ•°ä½äºŽæ•°ç»„çš„å‰åŠéƒ¨åˆ†ï¼Œæ‰€æœ‰çš„å¶æ•°ä½äºŽæ•°ç»„çš„åŽåŠéƒ¨åˆ†ï¼Œå¹¶ä¿è¯å¥‡æ•°å’Œå¥‡æ•°ï¼Œå¶æ•°å’Œå¶æ•°ä¹‹é—´çš„ç›¸å¯¹ä½ç½®ä¸å˜ã€‚ Tipsï¼š è¿™ä¸ªè·Ÿå¿«é€ŸæŽ’åºæ˜¯æœ‰ç‚¹åƒçš„ï¼Œä»Žå³å·¦å„æ‰¾åˆ°ä¸€ä¸ªä¸ç¬¦åˆæ¡ä»¶ï¼Œç„¶åŽäº¤æ¢ä½ç½®ã€‚å¿«æŽ’ä¸­è¿™ä¸ªè¿‡ç¨‹æ˜¯çº¿æ€§çš„ï¼Œéžå¹¶è¡Œã€‚ä¸‹é¢çš„å®žçŽ°ä»Žç®—æ³•è§’åº¦å¹¶ä¸æ˜¯niceçš„ï¼Œä½¿ç”¨äº†python ä¸­çš„listï¼Œæœ€å¥½çš„åº”è¯¥æ˜¯ inplace çš„é‚£ç§ï¼Œç«‹åœ°äº¤æ¢ã€‚ 1234567891011121314151617"""list å’Œlist ä¹‹é—´çš„è¿žæŽ¥ï¼Œä½¿ç”¨ list1+ list2 å°±æ˜¯å¯ä»¥çš„"""class Solution: def reOrderArray(self, array): # write code here if array ==[]: return [] odd_list =[] even_list =[] for item in array: if item% 2 ==1: odd_list.append(item) else: even_list.append(item) return odd_list+even_list ä¸Šé¢ç‰ˆæœ¬ä¿ç•™äº†åŽŸå§‹æ•°å­—ç›¸å¯¹çš„é¡ºåºï¼Œä¸‹é¢è¿™ä¸ªæ²¡æœ‰ä¿ç•™ç›¸å¯¹çš„é¡ºåºã€‚å‰è€…çš„ç©ºé—´å¤æ‚åº¦æ˜¯O(N), åŽè€…çš„ç©ºé—´å¤æ‚åº¦æ˜¯O(1). 123456789101112131415161718class Solution: def reOrderArray(self, array): if len(array) ==0: return [] left =0 right =len(array)-1 while left &lt; right: # å¦‚æžœæ˜¯å¥‡æ•° key =array[left] while left &lt; right and array[right] &amp; 1 == 0: right -= 1 array[left] = array[right] # å¦‚æžœæ˜¯å¶æ•° while left &lt; right and array[left] &amp; 1 == 1: left += 1 array[right] = key return array é¡ºæ—¶é’ˆæ‰“å°çŸ©é˜µ è¾“å…¥ä¸€ä¸ªçŸ©é˜µï¼ŒæŒ‰ç…§ä»Žå¤–å‘é‡Œä»¥é¡ºæ—¶é’ˆçš„é¡ºåºä¾æ¬¡æ‰“å°å‡ºæ¯ä¸€ä¸ªæ•°å­—ï¼Œä¾‹å¦‚ï¼Œå¦‚æžœè¾“å…¥å¦‚ä¸‹4 X 4çŸ©é˜µï¼š 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 åˆ™ä¾æ¬¡æ‰“å°å‡ºæ•°å­—1,2,3,4,8,12,16,15,14,13,9,5,6,7,11,10. Tips: ç»™å®šä¸€ä¸ªèµ·å§‹ç‚¹ï¼Œç„¶åŽæŒ‰ç…§é¡ºæ—¶é’ˆæ—‹è½¬åŽ»éåŽ†ã€‚æœ€åŽå¤„ç†å•è¡Œæˆ–è€…å•åˆ—çš„æƒ…å†µã€‚ 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768class Solution: """ æœ€åŽçš„ å¿…é¡»æ˜¯ä¸‰ç§ç‰¹æ®Šæƒ…å†µï¼Œå¦‚æžœä¿®æ”¹æˆä¸¤ç§ï¼Œé‚£ä¹ˆâ€åªå‰©ä¸‹ä¸€ä¸ªâ€œ è¿™ç§ç‰¹æ®Šæƒ…å†µå°±è¢«è®¡ç®—äº†ä¸¤æ¬¡ """# matrixç±»åž‹ä¸ºäºŒç»´åˆ—è¡¨ï¼Œéœ€è¦è¿”å›žåˆ—è¡¨def printMatrix(self, matrix): rows = len(matrix) cols = len(matrix[0]) top = 0 left = 0 down = rows - 1 right = cols - 1 result = [] while top &lt; down and left &lt; right: for j in range(left, right + 1): result.append(matrix[top][j]) top += 1 for i in range(top, down + 1): result.append(matrix[i][right]) right -= 1 for j in range(right, left - 1, -1): result.append(matrix[down][j]) down -= 1 for i in range(down, top - 1, -1): result.append(matrix[i][left]) left += 1 if top == down and left &lt; right: for j in range(left, right + 1): result.append(matrix[top][j]) if top &lt; down and left == right: for i in range(top, down + 1): result.append(matrix[i][left]) if top == down and left == right: result.append(matrix[top][left]) return result ``` - å­—ç¬¦ä¸²çš„æŽ’åˆ—&gt; è¾“å…¥ä¸€ä¸ªå­—ç¬¦ä¸²,æŒ‰å­—å…¸åºæ‰“å°å‡ºè¯¥å­—ç¬¦ä¸²ä¸­å­—ç¬¦çš„æ‰€æœ‰æŽ’åˆ—ã€‚ä¾‹å¦‚è¾“å…¥å­—ç¬¦ä¸²abc,åˆ™æ‰“å°å‡ºç”±å­—ç¬¦a,b,cæ‰€èƒ½æŽ’åˆ—å‡ºæ¥çš„æ‰€æœ‰å­—ç¬¦ä¸²abc,acb,bac,bca,cabå’Œcbaã€‚Tips: é€’å½’```pythonclass Solution:# é€’å½’ï¼š å˜æ¢æ–¹ç¨‹ï¼š ç¬¬ä¸€å­—æ¯å’Œ å‰©ä¸‹çš„æ‰€æœ‰çš„å­—æ¯def Permutation(self, ss): # write code here if not ss: return [] res = [] self.helper(ss, '', res) return sorted(list(set(res)))def helper(self, ss, path, res): if not ss: res.append(path) for i in range(len(ss)): self.helper(ss[:i] + ss[i + 1:], path + ss[i], res) æ•°ç»„ä¸­å‡ºçŽ°æ¬¡æ•°è¶…è¿‡ä¸€åŠçš„æ•°å­— æ•°ç»„ä¸­æœ‰ä¸€ä¸ªæ•°å­—å‡ºçŽ°çš„æ¬¡æ•°è¶…è¿‡æ•°ç»„é•¿åº¦çš„ä¸€åŠï¼Œè¯·æ‰¾å‡ºè¿™ä¸ªæ•°å­—ã€‚ä¾‹å¦‚è¾“å…¥ä¸€ä¸ªé•¿åº¦ä¸º9çš„æ•°ç»„{1,2,3,2,2,2,5,4,2}ã€‚ç”±äºŽæ•°å­—2åœ¨æ•°ç»„ä¸­å‡ºçŽ°äº†5æ¬¡ï¼Œè¶…è¿‡æ•°ç»„é•¿åº¦çš„ä¸€åŠï¼Œå› æ­¤è¾“å‡º2ã€‚å¦‚æžœä¸å­˜åœ¨åˆ™è¾“å‡º0ã€‚ Tipsï¼š å¦‚æžœæŸä¸ªæ•°å­—å‡ºçŽ°çš„æ¬¡æ•°å¤šäºŽä¸€åŠï¼Œé‚£ä¹ˆå…¶ä»–æ‰€æœ‰éžè¯¥æ•°å­—çš„å‡ºçŽ°çš„é¢‘æ•°æ˜¯å°äºŽè¯¥æ•°å­—çš„ï¼Œæ‰€ä»¥å½¢æˆä¸€ä¸ªäºŒåˆ†ç±»ã€‚ 1234567891011121314151617181920class Solution:# å¦‚æžœå­˜åœ¨è¿™æ ·çš„æ•°å­—ï¼Œé‚£ä¹ˆè¿™ä¸ªæ•°å­—çš„é¢‘æ•°ä¸€å®šæ˜¯å¤§äºŽå…¶ä»–æ‰€æœ‰çš„é¢‘æ•°# æ‰€ä»¥å¯ä»¥ç»Ÿè®¡ä¸€ä¸‹è¿™ä¸ªdef MoreThanHalfNum_Solution(self, numbers): # write code here if not numbers: return 0 target = numbers[0] nums = 0 # ç»Ÿè®¡å‡ºçŽ°æ¬¡æ•°æœ€å¤šçš„æ•°å­— for i in numbers: if target == i: nums += 1 elif nums == 0: target = i nums = 1 else: nums -= 1 res = target if numbers.count(target) &gt; len(numbers) // 2 else 0 return res è¿žç»­å­æ•°ç»„çš„æœ€å¤§å’Œ HZå¶å°”ä¼šæ‹¿äº›ä¸“ä¸šé—®é¢˜æ¥å¿½æ‚ é‚£äº›éžè®¡ç®—æœºä¸“ä¸šçš„åŒå­¦ã€‚ä»Šå¤©æµ‹è¯•ç»„å¼€å®Œä¼šåŽ,ä»–åˆå‘è¯äº†:åœ¨å¤è€çš„ä¸€ç»´æ¨¡å¼è¯†åˆ«ä¸­,å¸¸å¸¸éœ€è¦è®¡ç®—è¿žç»­å­å‘é‡çš„æœ€å¤§å’Œ,å½“å‘é‡å…¨ä¸ºæ­£æ•°çš„æ—¶å€™,é—®é¢˜å¾ˆå¥½è§£å†³ã€‚ä½†æ˜¯,å¦‚æžœå‘é‡ä¸­åŒ…å«è´Ÿæ•°,æ˜¯å¦åº”è¯¥åŒ…å«æŸä¸ªè´Ÿæ•°,å¹¶æœŸæœ›æ—è¾¹çš„æ­£æ•°ä¼šå¼¥è¡¥å®ƒå‘¢ï¼Ÿä¾‹å¦‚:{6,-3,-2,7,-15,1,2,2},è¿žç»­å­å‘é‡çš„æœ€å¤§å’Œä¸º8(ä»Žç¬¬0ä¸ªå¼€å§‹,åˆ°ç¬¬3ä¸ªä¸ºæ­¢)ã€‚ç»™ä¸€ä¸ªæ•°ç»„ï¼Œè¿”å›žå®ƒçš„æœ€å¤§è¿žç»­å­åºåˆ—çš„å’Œï¼Œä½ ä¼šä¸ä¼šè¢«ä»–å¿½æ‚ ä½ï¼Ÿ(å­å‘é‡çš„é•¿åº¦è‡³å°‘æ˜¯1) Tips: ä¸€ç»´æ•°ç»„ï¼ŒéåŽ†ä¸€éï¼Œç„¶åŽæœ€å¤§å­æ•°ç»„å’Œçš„è¿‡ç¨‹ã€‚ 12345678910111213141516class Solution:# largest , sum è¿™æ˜¯ä¸¤ä¸ªä¸åŒçš„çŠ¶æ€# æ³¨æ„åˆå§‹åŒ–def FindGreatestSumOfSubArray(self, array): # write code here if not array: return [] largest =array[0] sum_of_array =0 for i in array: sum_of_array += i if sum_of_array &gt; largest: largest =sum_of_array elif sum_of_array &lt;0: sum_of_array =0 return largest ç¬¬ä¸€ä¸ªåªå‡ºçŽ°ä¸€æ¬¡çš„å­—ç¬¦ åœ¨ä¸€ä¸ªå­—ç¬¦ä¸²(0&lt;=å­—ç¬¦ä¸²é•¿åº¦&lt;=10000ï¼Œå…¨éƒ¨ç”±å­—æ¯ç»„æˆ)ä¸­æ‰¾åˆ°ç¬¬ä¸€ä¸ªåªå‡ºçŽ°ä¸€æ¬¡çš„å­—ç¬¦,å¹¶è¿”å›žå®ƒçš„ä½ç½®, å¦‚æžœæ²¡æœ‰åˆ™è¿”å›ž -1ï¼ˆéœ€è¦åŒºåˆ†å¤§å°å†™ï¼‰. 1234567891011121314151617181920212223242526272829303132class Solution: """ Three ways to get dictionary of string s """ def FirstNotRepeatingChar(self, s): if not s: return -1 # get dictionary from collections import defaultdict dict1 =defaultdict(int) for string in s: dict1[string] += 1 # or this way # from collections import Counter # dict1 =Counter(s) # or do it yourself #dict1 =self.Counter_self(s) for index, val in enumerate(s): if dict1[val] == 1: return -1 def Counter_self(self, s): dict1 = &#123;&#125; for val in s: if val not in dict1: dict1[val] = 1 else: dict1[val] = dict1[val] + 1 return dict1 æ•°ç»„ä¸­çš„é€†åºå¯¹ åœ¨æ•°ç»„ä¸­çš„ä¸¤ä¸ªæ•°å­—ï¼Œå¦‚æžœå‰é¢ä¸€ä¸ªæ•°å­—å¤§äºŽåŽé¢çš„æ•°å­—ï¼Œåˆ™è¿™ä¸¤ä¸ªæ•°å­—ç»„æˆä¸€ä¸ªé€†åºå¯¹ã€‚è¾“å…¥ä¸€ä¸ªæ•°ç»„,æ±‚å‡ºè¿™ä¸ªæ•°ç»„ä¸­çš„é€†åºå¯¹çš„æ€»æ•°Pã€‚å¹¶å°†På¯¹1000000007å–æ¨¡çš„ç»“æžœè¾“å‡ºã€‚ å³è¾“å‡ºP%1000000007 123456789101112131415161718192021222324252627282930313233343536373839404142# -*- coding:utf-8 -*-# ä¹‹å‰åœ¨ç‰›å®¢ç½‘ä¸Šæ˜¯å¯ä»¥é€šè¿‡çš„ï¼ŒçŽ°åœ¨å› ä¸ºæ—¶é—´å¤æ‚åº¦åˆæ²¡åŠžæ³•é€šè¿‡äº†class Solution: # è¿™ä¸ªä¸€æ–¤éš¾å‡ºå¤©æœºäº† å…ˆä¸çœ‹äº† # è¿™ä¸ªåˆ°åŽæ¥å°±å·²ç»èƒŒä¸‹æ¥äº† def InversePairs(self, data): # write code here if not data: return 0 temp = [i for i in data] return self.mergeSort(temp, data, 0, len(data ) -1) % 1000000007 def mergeSort(self, temp, data, low, high): if low &gt;= high: temp[low] = data[low] return 0 mid = (low + high) / 2 # ä¸æ‡‚ data å’Œ temp ä¸ºä»€ä¹ˆæ˜¯é¢ å€’é¡ºåº left = self.mergeSort(data, temp, low, mid) right = self.mergeSort(data, temp, mid +1, high) count = 0 i = low j = mid +1 index = low while i &lt;= mid and j &lt;= high: if data[i] &lt;= data[j]: temp[index] = data[i] i += 1 else: temp[index] = data[j] count += mid - i +1 j += 1 index += 1 while i &lt;= mid: temp[index] = data[i] i += 1 index += 1 while j &lt;= high: temp[index] = data[j] j += 1 index += 1 return count + left + right æ•°å­—åœ¨æŽ’åºæ•°ç»„ä¸­å‡ºçŽ°çš„æ¬¡æ•° ç»Ÿè®¡ä¸€ä¸ªæ•°å­—åœ¨æŽ’åºæ•°ç»„ä¸­å‡ºçŽ°çš„æ¬¡æ•°ã€‚ 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859class Solution:# äºŒåˆ†æŸ¥æ‰¾ï¼Œå½“ data[mid] ==key çš„æ—¶å€™æ˜¯é¡ºåºæŸ¥æ‰¾ï¼Œæ˜¯é€’å½’è·³å‡ºçš„æ¡ä»¶def GetNumberOfK(self, data, k): # write code here # è¿™ä¸ªæ˜¯æœ‰ä¸¤ä¸ªè·³å‡ºæ¡ä»¶çš„ï¼Œä¸€ä¸ªæ˜¯æ­£ç¡®çš„è·³å‡ºï¼Œä¸€ä¸ªæ˜¯ä¸æ­£ç¡®çš„è·³å‡º if not data: return 0 mid =len(data) // 2 if data[mid] == k: left = right = mid for i in range(mid - 1, -1, -1): if data[i] == k: left -= 1 for i in range(mid + 1, len(data)): if data[i] == k: right += 1 return right - left + 1 # ä¸€åŠä¸€åŠçš„èˆåŽ»æ•°æ® elif data[mid] &lt; k: return self.GetNumberOfK(data[mid + 1:], k) else: return self.GetNumberOfK(data[:mid - 1], k) ``` - æ•°ç»„ä¸­åªå‡ºçŽ°ä¸€æ¬¡çš„æ•°å­—&gt; ä¸€ä¸ªæ•´åž‹æ•°ç»„é‡Œé™¤äº†ä¸¤ä¸ªæ•°å­—ä¹‹å¤–ï¼Œå…¶ä»–çš„æ•°å­—éƒ½å‡ºçŽ°äº†ä¸¤æ¬¡ã€‚è¯·å†™ç¨‹åºæ‰¾å‡ºè¿™ä¸¤ä¸ªåªå‡ºçŽ°ä¸€æ¬¡çš„æ•°å­—ã€‚Tips: å¼‚æˆ–æ“ä½œï¼Œä¸¤ä¸ªåè¿›åˆ¶æ•°å­—ç»è¿‡å¼‚æˆ–ï¼ˆäºŒçº§åˆ¶è®¡ç®—è¿‡ç¨‹ï¼‰ï¼Œæœ€åŽçš„ç»“æžœæ˜¯10è¿›åˆ¶çš„å½¢å¼ã€‚å¦‚æžœä¸¤ä¸ªç›¸åŒçš„æ•°å­—å¼‚æˆ–ï¼Œé‚£ä¹ˆæœ€åŽçš„ç»“æžœæ˜¯0ï¼Œå¦‚æžœæ˜¯ä¸åŒçš„æ•°å­—ï¼Œæœ€åŽçš„ç»“æžœæ˜¯éž0.æ¯”å¦‚&gt; 2^4 # 4&gt; 3^4 # 7&gt; 40^42 # 2ç»“æžœçš„äºŒè¿›åˆ¶å½¢å¼ä¸€å®šè‡³å°‘æœ‰ä¸€ä¸ª "1". ä½¿ç”¨index å¾—åˆ°ä¸¤ä¸ªä¸åŒçš„æ•°å­—äºŒè¿›åˆ¶å½¢å¼ä¸‹çš„ä½ç½®ï¼Œç„¶åŽä»Žè¯¥ä½ç½®å°†åŽŸæ¥çš„æ•°ç»„åˆ†æˆä¸¤ç±»ï¼Œé‚£ä¹ˆæ¯ç±»ä¸­åªå«æœ‰ä¸€ä¸ªå‡ºçŽ°ä¸€æ¬¡çš„æ•°å­—ï¼ŒæŽ¥ç€ä½¿ç”¨å¼‚æˆ–æ“ä½œã€‚```python class Solution: # è¿”å›ž[a,b] å…¶ä¸­abæ˜¯å‡ºçŽ°ä¸€æ¬¡çš„ä¸¤ä¸ªæ•°å­— # ä½¿ç”¨å¼‚æˆ–çš„æ€§è´¨ï¼Œå¦‚æžœåªæœ‰ä¸€ä¸ªä¸åŒï¼Œå…¶ä»–çš„å¶æ¬¡å‡ºçŽ°ï¼Œé‚£ä¹ˆå…¨éƒ¨å¼‚æˆ–çš„ç»“æžœ # å°±æ˜¯é‚£ä¸ªå•ä¸€çš„æ•°å­— def FindNumsAppearOnce(self, array): # write code here remain, index =0, 1 for num in array: remain = remain ^ num # æ‰¾å‡ºç¬¬ä¸€ä¸ªæ˜¯1 çš„ä½ç½® # index éƒ½æ˜¯ while (remain &amp; index) ==0: index = index &lt;&lt;1 res1, res2 =0,0 for num in array: # è¿™ä¸ªæ¡ä»¶å¿…é¡»æ˜¯0, è¡¨ç¤ºä¸¤ä¸ªåœ¨è¿™ä¸ªä½æ•°æ˜¯ç›¸åŒçš„ï¼Œ if num &amp; index ==0: res1 =res1 ^ num else: res2 =res2 ^ num return [res1, res2] å’Œä¸ºSçš„è¿žç»­æ­£æ•°åºåˆ— å°æ˜Žå¾ˆå–œæ¬¢æ•°å­¦,æœ‰ä¸€å¤©ä»–åœ¨åšæ•°å­¦ä½œä¸šæ—¶,è¦æ±‚è®¡ç®—å‡º9~16çš„å’Œ,ä»–é©¬ä¸Šå°±å†™å‡ºäº†æ­£ç¡®ç­”æ¡ˆæ˜¯100ã€‚ä½†æ˜¯ä»–å¹¶ä¸æ»¡è¶³äºŽæ­¤,ä»–åœ¨æƒ³ç©¶ç«Ÿæœ‰å¤šå°‘ç§è¿žç»­çš„æ­£æ•°åºåˆ—çš„å’Œä¸º100(è‡³å°‘åŒ…æ‹¬ä¸¤ä¸ªæ•°)ã€‚æ²¡å¤šä¹…,ä»–å°±å¾—åˆ°å¦ä¸€ç»„è¿žç»­æ­£æ•°å’Œä¸º100çš„åºåˆ—:18,19,20,21,22ã€‚çŽ°åœ¨æŠŠé—®é¢˜äº¤ç»™ä½ ,ä½ èƒ½ä¸èƒ½ä¹Ÿå¾ˆå¿«çš„æ‰¾å‡ºæ‰€æœ‰å’Œä¸ºSçš„è¿žç»­æ­£æ•°åºåˆ—? Tips: æ»‘åŠ¨çª—å£ï¼ˆä¸¤ä¸ªæŒ‡é’ˆï¼‰ 12345678910111213141516def FindContinuousSequence(self, tsum): # write code here if tsum &lt; 2: return [] left = 1 right = left + 1 res = [] while left &lt; tsum // 2 + 1: if sum(range(left, right)) == tsum: res.append(range(left, right)) left += 1 elif sum(range(left, right)) &lt; tsum: right += 1 else: left += 1 return res å’Œä¸ºSçš„ä¸¤ä¸ªæ•°å­— è¾“å…¥ä¸€ä¸ªé€’å¢žæŽ’åºçš„æ•°ç»„å’Œä¸€ä¸ªæ•°å­—Sï¼Œåœ¨æ•°ç»„ä¸­æŸ¥æ‰¾ä¸¤ä¸ªæ•°ï¼Œä½¿å¾—ä»–ä»¬çš„å’Œæ­£å¥½æ˜¯Sï¼Œå¦‚æžœæœ‰å¤šå¯¹æ•°å­—çš„å’Œç­‰äºŽSï¼Œè¾“å‡ºä¸¤ä¸ªæ•°çš„ä¹˜ç§¯æœ€å°çš„ã€‚ å’Œä¸Šä¸€ä¸ªé¢˜ç›®çš„ä¸åŒç‚¹åœ¨äºŽï¼Œè¯¥é¢˜ç›®æ˜¯ç»™å®šäº†æŸä¸ªé€’å¢žçš„æ•°ç»„ã€‚ä¸Šä¸€ä¸ªé¢˜ç›®é»˜è®¤çš„æ˜¯ (0, tsum//2+1) è¿™æ ·çš„åºåˆ—ã€‚ 12345678910111213141516class Solution: def FindNumbersWithSum(self, array, tsum): # write code here if len(array) &lt; 2: return [] left = 0 right = len(array) - 1 while left &lt; right: if array[left] + array[right] == tsum: return [array[left], array[right]] elif array[left] + array[right] &lt; tsum: left += 1 else: right -= 1 return [] å·¦æ—‹è½¬å­—ç¬¦ä¸² æ±‡ç¼–è¯­è¨€ä¸­æœ‰ä¸€ç§ç§»ä½æŒ‡ä»¤å«åšå¾ªçŽ¯å·¦ç§»ï¼ˆROLï¼‰ï¼ŒçŽ°åœ¨æœ‰ä¸ªç®€å•çš„ä»»åŠ¡ï¼Œå°±æ˜¯ç”¨å­—ç¬¦ä¸²æ¨¡æ‹Ÿè¿™ä¸ªæŒ‡ä»¤çš„è¿ç®—ç»“æžœã€‚å¯¹äºŽä¸€ä¸ªç»™å®šçš„å­—ç¬¦åºåˆ—Sï¼Œè¯·ä½ æŠŠå…¶å¾ªçŽ¯å·¦ç§»Kä½åŽçš„åºåˆ—è¾“å‡ºã€‚ä¾‹å¦‚ï¼Œå­—ç¬¦åºåˆ—S=â€abcXYZdefâ€,è¦æ±‚è¾“å‡ºå¾ªçŽ¯å·¦ç§»3ä½åŽçš„ç»“æžœï¼Œå³â€œXYZdefabcâ€ã€‚æ˜¯ä¸æ˜¯å¾ˆç®€å•ï¼ŸOKï¼Œæžå®šå®ƒï¼ Tips: pythonä¸­å­—ç¬¦ä¸²çš„å¤„ç†æ˜¯æ²¡æœ‰åŽ‹åŠ›çš„ 1234567891011121314151617181920212223242526272829303132333435363738394041424344class Solution: def LeftRotateString(self, s, n): # write code here if len(s) &lt; n: return '' return s[n:] + s[:n]``` - ç¿»è½¬å•è¯é¡ºåºåˆ—&gt; ç‰›å®¢æœ€è¿‘æ¥äº†ä¸€ä¸ªæ–°å‘˜å·¥Fishï¼Œæ¯å¤©æ—©æ™¨æ€»æ˜¯ä¼šæ‹¿ç€ä¸€æœ¬è‹±æ–‡æ‚å¿—ï¼Œå†™äº›å¥å­åœ¨æœ¬å­ä¸Šã€‚åŒäº‹Catå¯¹Fishå†™çš„å†…å®¹é¢‡æ„Ÿå…´è¶£ï¼Œæœ‰ä¸€å¤©ä»–å‘Fishå€Ÿæ¥ç¿»çœ‹ï¼Œä½†å´è¯»ä¸æ‡‚å®ƒçš„æ„æ€ã€‚ä¾‹å¦‚ï¼Œâ€œstudent. a am Iâ€ã€‚åŽæ¥æ‰æ„è¯†åˆ°ï¼Œè¿™å®¶ä¼™åŽŸæ¥æŠŠå¥å­å•è¯çš„é¡ºåºç¿»è½¬äº†ï¼Œæ­£ç¡®çš„å¥å­åº”è¯¥æ˜¯â€œI am a student.â€ã€‚Catå¯¹ä¸€ä¸€çš„ç¿»è½¬è¿™äº›å•è¯é¡ºåºå¯ä¸åœ¨è¡Œï¼Œä½ èƒ½å¸®åŠ©ä»–ä¹ˆï¼ŸTipsï¼šé¦–å…ˆå®šä¹‰ä¸€ä¸ªhelper() åŠŸèƒ½æ˜¯ç¿»è½¬å­—ç¬¦ä¸²ï¼Œç„¶åŽä¸¤æ¬¡ç¿»è½¬ã€‚ç¬¬ä¸€æ¬¡æ˜¯å…¨éƒ¨ç¿»è½¬ï¼Œç¬¬äºŒæ¬¡æ˜¯å•è¯ç¿»è½¬ã€‚```pythonclass Solution: def Reverse(self, s, left, right): while left &lt;right: s[left], s[right] = s[right], s[left] left +=1 right -=1 def ReverseSentence(self, s): # write code here if not s: return s # from immutable string to mutable list s =list(s) self.Reverse(s, 0, len(s) - 1) start, end = 0, 0 # è¿™ä¸ªå°äºŽå· æ˜¯python ä¸­ç‰¹æœ‰çš„å‘ï¼ŒçœŸæ­£èƒ½å¤Ÿè®¿é—®çš„åŒºé—´æ˜¯ [0, len(s)-1] è¿™æ ·çš„åŒºé—´ while start &lt; len(s): if s[start] == " ": start += 1 end += 1 elif end == len(s) or s[end] == " ": self.Reverse(s, start, end - 1) # update æ“ä½œ end += 1 start = end else: end += 1 return "".join(s) æ‰‘å…‹ç‰Œé¡ºå­ LLä»Šå¤©å¿ƒæƒ…ç‰¹åˆ«å¥½,å› ä¸ºä»–åŽ»ä¹°äº†ä¸€å‰¯æ‰‘å…‹ç‰Œ,å‘çŽ°é‡Œé¢å±…ç„¶æœ‰2ä¸ªå¤§çŽ‹,2ä¸ªå°çŽ‹(ä¸€å‰¯ç‰ŒåŽŸæœ¬æ˜¯54å¼ ^_^)â€¦ä»–éšæœºä»Žä¸­æŠ½å‡ºäº†5å¼ ç‰Œ,æƒ³æµ‹æµ‹è‡ªå·±çš„æ‰‹æ°”,çœ‹çœ‹èƒ½ä¸èƒ½æŠ½åˆ°é¡ºå­,å¦‚æžœæŠ½åˆ°çš„è¯,ä»–å†³å®šåŽ»ä¹°ä½“è‚²å½©ç¥¨,å˜¿å˜¿ï¼ï¼â€œçº¢å¿ƒA,é»‘æ¡ƒ3,å°çŽ‹,å¤§çŽ‹,æ–¹ç‰‡5â€,â€œOh My God!â€ä¸æ˜¯é¡ºå­â€¦..LLä¸é«˜å…´äº†,ä»–æƒ³äº†æƒ³,å†³å®šå¤§/å° çŽ‹å¯ä»¥çœ‹æˆä»»ä½•æ•°å­—,å¹¶ä¸”Açœ‹ä½œ1,Jä¸º11,Qä¸º12,Kä¸º13ã€‚ä¸Šé¢çš„5å¼ ç‰Œå°±å¯ä»¥å˜æˆâ€œ1,2,3,4,5â€(å¤§å°çŽ‹åˆ†åˆ«çœ‹ä½œ2å’Œ4),â€œSo Lucky!â€ã€‚LLå†³å®šåŽ»ä¹°ä½“è‚²å½©ç¥¨å•¦ã€‚ çŽ°åœ¨,è¦æ±‚ä½ ä½¿ç”¨è¿™å¹…ç‰Œæ¨¡æ‹Ÿä¸Šé¢çš„è¿‡ç¨‹,ç„¶åŽå‘Šè¯‰æˆ‘ä»¬LLçš„è¿æ°”å¦‚ä½•ï¼Œ å¦‚æžœç‰Œèƒ½ç»„æˆé¡ºå­å°±è¾“å‡ºtrueï¼Œå¦åˆ™å°±è¾“å‡ºfalseã€‚ä¸ºäº†æ–¹ä¾¿èµ·è§,ä½ å¯ä»¥è®¤ä¸ºå¤§å°çŽ‹æ˜¯0ã€‚ Tipsï¼šlist ä¸­çš„ç©ºç¼ºæ•°é‡ éœ€è¦ä¸å¤§äºŽ å¤§å°çŽ‹æ€»æ•°ï¼Œè¿™æ ·æ‰èƒ½æž„æˆé¡ºå­ã€‚ 12345678910111213141516171819202122232425class Solution: """ ç©ºç¼ºæ˜¯1 æ„å‘³ç€è¿™ä¸¤ä¸ªæ•°å­—æ˜¯è¿žç»­çš„ æ¯”å¦‚è¯´ 1 2ï¼Œ è¿™ä¸ªbig -small ==1, æ‰€ä»¥è¿™ä¸ªç©ºç¼ºæ˜¯0ï¼Œä¸ç”¨è¿›è¡Œå¡«å……ã€‚ """ def IsContinuous(self, numbers): # write code here if not numbers: return False numbers.sort() # sort() sorted() è¿™ç§æ€Žä¹ˆä½¿ç”¨ï¼Œè¿”å›žå€¼æ˜¯ä»€ä¹ˆï¼Œè¿™äº›åŸºæœ¬çš„ä¸œè¥¿ zeros =numbers.count(0) gaps = 0 left = zeros # å› ä¸ºè¿™ä¸ªæ˜¯æŽ’åºä¹‹åŽçš„ç»“æžœï¼Œæ‰€ä»¥å¯ä»¥è¿™æ ·è¿›è¡Œæ“ä½œ right = left + 1 # å®žé™…ä¸Šè¿˜æ˜¯ä¸¤ä¸ªæŒ‡é’ˆï¼Œ æ‰€ä»¥å¯ä»¥ä½¿ç”¨ä¸¤ä¸ªæŒ‡é’ˆè¿›è¡Œæ“ä½œ # æœ¬è´¨ä¸Šæ˜¯ä¸¤ä¸ª ç›¸é‚»æŒ‡é’ˆåœ¨è¿›è¡Œç§»åŠ¨ï¼Œå› ä¸ºæ˜¯æŽ’åºä¹‹åŽï¼Œæ‰€ä»¥æ²¡æœ‰é—®é¢˜ while right &lt; len(numbers): if numbers[left] == numbers[right]: return False gaps += numbers[right] - numbers[left] - 1 left = right right += 1 # è¿™ç§æ˜¯çœŸçš„ å¾ˆç®€æ´ï¼Œ gaps &lt;= zeros å°‘åŽ»äº†å¾ˆå¤šif elseçš„åˆ¤æ–­ return gaps &lt;= zeros å­©å­ä»¬çš„æ¸¸æˆ(åœ†åœˆä¸­æœ€åŽå‰©ä¸‹çš„æ•°) æ¯å¹´å…­ä¸€å„¿ç«¥èŠ‚,ç‰›å®¢éƒ½ä¼šå‡†å¤‡ä¸€äº›å°ç¤¼ç‰©åŽ»çœ‹æœ›å­¤å„¿é™¢çš„å°æœ‹å‹,ä»Šå¹´äº¦æ˜¯å¦‚æ­¤ã€‚HFä½œä¸ºç‰›å®¢çš„èµ„æ·±å…ƒè€,è‡ªç„¶ä¹Ÿå‡†å¤‡äº†ä¸€äº›å°æ¸¸æˆã€‚å…¶ä¸­,æœ‰ä¸ªæ¸¸æˆæ˜¯è¿™æ ·çš„:é¦–å…ˆ,è®©å°æœ‹å‹ä»¬å›´æˆä¸€ä¸ªå¤§åœˆã€‚ç„¶åŽ,ä»–éšæœºæŒ‡å®šä¸€ä¸ªæ•°m,è®©ç¼–å·ä¸º0çš„å°æœ‹å‹å¼€å§‹æŠ¥æ•°ã€‚æ¯æ¬¡å–Šåˆ°m-1çš„é‚£ä¸ªå°æœ‹å‹è¦å‡ºåˆ—å”±é¦–æ­Œ,ç„¶åŽå¯ä»¥åœ¨ç¤¼å“ç®±ä¸­ä»»æ„çš„æŒ‘é€‰ç¤¼ç‰©,å¹¶ä¸”ä¸å†å›žåˆ°åœˆä¸­,ä»Žä»–çš„ä¸‹ä¸€ä¸ªå°æœ‹å‹å¼€å§‹,ç»§ç»­0â€¦m-1æŠ¥æ•°â€¦.è¿™æ ·ä¸‹åŽ»â€¦.ç›´åˆ°å‰©ä¸‹æœ€åŽä¸€ä¸ªå°æœ‹å‹,å¯ä»¥ä¸ç”¨è¡¨æ¼”,å¹¶ä¸”æ‹¿åˆ°ç‰›å®¢åè´µçš„â€œåä¾¦æŽ¢æŸ¯å—â€å…¸è—ç‰ˆ(åé¢æœ‰é™å“¦!!^_^)ã€‚è¯·ä½ è¯•ç€æƒ³ä¸‹,å“ªä¸ªå°æœ‹å‹ä¼šå¾—åˆ°è¿™ä»½ç¤¼å“å‘¢ï¼Ÿ(æ³¨ï¼šå°æœ‹å‹çš„ç¼–å·æ˜¯ä»Ž0åˆ°n-1) Tips: çº¦ç‘Ÿå¤«çŽ¯çš„é—®é¢˜ï¼Œ è¦æ±‚æ±‚è§£çš„æ˜¯æœ€åŽèƒœåˆ©è€…çš„ç¼–å·ï¼Œæ‰€ä»¥åº”ç”¨æ•°å­¦æŠ€å·§å°±å¯ä»¥äº†ã€‚ $ f(x) = (f( x-1) + m ) % (x) $ , å…±æœ‰ m ä¸ªç¼–å·ï¼Œn ä¸ªäºº 12345678910111213class Solution:# mod æ±‚ä½™ çš„æ“ä½œï¼Œ a mod b ==c ,è¯´æ˜Ž aé™¤ä»¥b ä¹‹åŽä½™æ•°æ˜¯c# https://blog.csdn.net/gatieme/article/details/51435055ï¼Œ ä»Žåšé¢˜æ€è·¯ä¸Šè®²è§£çš„æ¯”è¾ƒå¥½# n ä¸ªå°æœ‹å‹ï¼Œç„¶åŽæ˜¯m ä¸ªç¼–å·def LastRemaining_Solution(self, n, m): # write code here if n&lt; 1 or m&lt;1: return -1 last =0 for i in range(2, n+1): # è¿™ä¸ªç›¸å½“äºŽ æ˜¯ä¸€ä¸ª â€œæŒ‘é€‰äººâ€ çš„é€†è¿‡ç¨‹ï¼Œ å› ä¸ºä½¿ç”¨çš„ mod æ“ä½œå°±æ˜¯å–ä½™çš„æ“ä½œ last =(last +m) %i return last æ±‚1+2+3+â€¦+n æ±‚1+2+3+â€¦+nï¼Œè¦æ±‚ä¸èƒ½ä½¿ç”¨ä¹˜é™¤æ³•ã€forã€whileã€ifã€elseã€switchã€caseç­‰å…³é”®å­—åŠæ¡ä»¶åˆ¤æ–­è¯­å¥ï¼ˆA?B:Cï¼‰ã€‚ Tipsï¼š ä¸ä½¿ç”¨æ¡ä»¶åˆ¤æ–­ï¼Œæ¥æŽ§åˆ¶è·³å‡ºï¼›è¿™é‡Œä½¿ç”¨çš„æ˜¯ â€œçŸ­è·¯æ¡ä»¶â€ æ¥ æŽ§åˆ¶ é€’å½’çš„è·³å‡ºã€‚ 12345678910111213141516class Solution: # å¦‚æžœä½ æƒ³ä½¿ç”¨å…¨å±€å˜é‡ï¼Œé‚£ä¹ˆæ”¾åœ¨ __init__ ä¸­å°±æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„æ–¹å¼ def __init__(self): self.ans =0 def Sum_Solution(self, n): # write code here self.recur(n) return self.ans # n&gt;0 å°±æ˜¯ä¸€ä¸ªçŸ­è·¯æ¡ä»¶ï¼Œè¿™ä¸ªç›´æŽ¥å†³å®šäº†åŽé¢é€’å½’ä¼šä¸ä¼šç»§ç»­æ‰§è¡Œä¸‹åŽ»ï¼Œä¹Ÿå°±æ˜¯è·³å‡ºçš„æ¡ä»¶ # è‡³äºŽä¼šä¸ä¼šå›žåˆ°åŽŸæ¥æœ€åˆçš„çŠ¶æ€ï¼Œè¿™ä¸ªæ˜¯ä¸é‡è¦çš„ï¼Œæœ€åŽçš„ç»“æžœæ˜¯ self.ans ï¼Œfalseä¹‹åŽç›´æŽ¥ä½¿ç”¨è¿™ä¸ªå°±è¡Œäº† def recur(self, n): self.ans += n n -= 1 return n &gt; 0 and self.Sum_Solution(n) ä¸ç”¨åŠ å‡ä¹˜é™¤åšåŠ æ³• å†™ä¸€ä¸ªå‡½æ•°ï¼Œæ±‚ä¸¤ä¸ªæ•´æ•°ä¹‹å’Œï¼Œè¦æ±‚åœ¨å‡½æ•°ä½“å†…ä¸å¾—ä½¿ç”¨+ã€-ã€*ã€/å››åˆ™è¿ç®—ç¬¦å·ã€‚ Tips: ä½¿ç”¨ å¼‚æˆ–å’Œä¸Ž æ¥è¿›è¡Œ â€œåŠ â€ã€â€œå‡â€çš„æ“ä½œã€‚åŠ æ³•æ˜¯åˆ†æˆå½“å‰ä½ç›¸åŠ  å’Œè¿›ä½ä¸¤ä¸ªéƒ¨åˆ†çš„ã€‚psï¼Œpython ä¸­ å‡½æ•°åæ˜¯åˆ†å¤§å°å†™çš„ã€‚ 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253class Solution: """ ä¸èƒ½ä½¿ç”¨å››åˆ™è¿ç®—ç¬¦ï¼Œæˆ‘ä»¬å°±å¯ä»¥ä½¿ç”¨ä½è¿ç®—ç¬¦ã€‚å¯¹è¿™ä¸¤ä¸ªæ•°åœ¨æ›´åº•å±‚çš„è§’åº¦ä¸Šè¿›è¡Œè¿ç®—ã€‚ä¹Ÿå°±æ˜¯ä»Ž 01 è¿™ç§å­ä¸²çš„è§’åº¦è¿›è¡Œè€ƒè™‘ https://blog.csdn.net/derrantcm/article/details/46798763 è¿™ä¸ªåšå®¢å¯¹äºŽæ•°çš„è¿ç®—è¿‡ç¨‹å’Œ ä½è¿ç®—æ˜¯å¦‚ä½•ä¸€ä¸€å¯¹åº”çš„ã€‚åˆ†ä¸ºä¸è¿›ä½ç›¸åŠ å’Œè¿›ä½ç›¸åŠ ã€‚ """ # è¿™ä¸ªåªèƒ½æ˜¯æ­£æ•´æ•°çš„æ“ä½œï¼ŒçœŸæ˜¯çƒ­åŠ›ç‹— # è¿™ä¸ªå¼„å‡ºæ¥æ˜¯çœŸå¿ƒä¸å®¹æ˜“å“ˆ def add(self, num1, num2): while num2 != 0: carry = num1 &amp; num2 num1 = num1 ^ num2 # è¿™ä¸ªåº”è¯¥ç†è§£ä¸ºåˆ°é«˜ä½ è€Œä¸æ˜¯*2 è¿™æ ·çš„æ“ä½œ num2 = carry &lt;&lt; 1 return num1 def sub(self, num1, num2): while num2 != 0: carry = (~num1) &amp; num2 num1 = num1 ^ num2 num2 = carry &lt;&lt; 1 return num1 def Add(self, num1, num2): if num1 &gt;= 0 and num2 &gt;= 0: result = self.add(num1, num2) elif num1 &gt; 0 and num2 &lt; 0: flag = 1 if num1 &gt; abs(num2) else -1 # num2 =abs(num2) # keep num1 bigger than num2 if num1 &lt; abs(num2): num1, num2 = abs(num2), num1 result = self.sub(num1, abs(num2)) result = result * flag elif num1 &lt; 0 and num2 &gt; 0: flag = 1 if abs(num1) &lt; num2 else -1 if abs(num1) &lt; num2: num1, num2 = num2, abs(num1) result = self.sub(abs(num1), num2) result = result * flag else: flag = -1 num1 = abs(num1) num2 = abs(num2) result = self.add(num1, num2) result = result * flag return result æŠŠå­—ç¬¦ä¸²è½¬æ¢æˆæ•´æ•° å°†ä¸€ä¸ªå­—ç¬¦ä¸²è½¬æ¢æˆä¸€ä¸ªæ•´æ•°(å®žçŽ°Integer.valueOf(string)çš„åŠŸèƒ½ï¼Œä½†æ˜¯stringä¸ç¬¦åˆæ•°å­—è¦æ±‚æ—¶è¿”å›ž0)ï¼Œè¦æ±‚ä¸èƒ½ä½¿ç”¨å­—ç¬¦ä¸²è½¬æ¢æ•´æ•°çš„åº“å‡½æ•°ã€‚ æ•°å€¼ä¸º0æˆ–è€…å­—ç¬¦ä¸²ä¸æ˜¯ä¸€ä¸ªåˆæ³•çš„æ•°å€¼åˆ™è¿”å›ž0ã€‚ Tipsï¼šè¿˜æ˜¯python ä¸­å¤„ç† stringï¼Œ ä½¿ç”¨ dictionary å¤„ç†å­—ç¬¦ä¸²å’Œ æ•°å­—çš„åŒ¹é…ã€‚ 12345678910111213141516171819202122class Solution:# æœ‰å¾ˆå¤šä¸åˆæ³•çš„è¾“å…¥ï¼Œæ¯”å¦‚ç©ºå­—ç¬¦ä¸²ï¼Œæ­£è´Ÿå·ï¼Œéžæ•°å­—å­—ç¬¦ æ•°æ®æº¢å‡ºï¼Œæ‰€ä»¥ä»Žåé¢è€ƒè™‘æ›´åŠ ç®€å•ä¸€äº›# åˆæ³•çš„è¾“å…¥åªæœ‰æ•°å­—å’Œç¬¦å·ä½ + å’Œ-def StrToInt(self, s): # write code here int_list=['0', '1', '2', '3', '4', '5', '6','7', '8', '9', '+', '-'] if s ==" ": return 0 sum1 =0 flag =1 # æ­£è´Ÿå· for string in s: if string not in int_list: return 0 if string =="+": flag =1 continue elif string =="-": flag = -1 continue else: sum1 =sum1 *10 +int_list.index(string) return sum1*flag æ•°ç»„ä¸­é‡å¤çš„æ•°å­— åœ¨ä¸€ä¸ªé•¿åº¦ä¸ºnçš„æ•°ç»„é‡Œçš„æ‰€æœ‰æ•°å­—éƒ½åœ¨0åˆ°n-1çš„èŒƒå›´å†…ã€‚ æ•°ç»„ä¸­æŸäº›æ•°å­—æ˜¯é‡å¤çš„ï¼Œä½†ä¸çŸ¥é“æœ‰å‡ ä¸ªæ•°å­—æ˜¯é‡å¤çš„ã€‚ä¹Ÿä¸çŸ¥é“æ¯ä¸ªæ•°å­—é‡å¤å‡ æ¬¡ã€‚è¯·æ‰¾å‡ºæ•°ç»„ä¸­ä»»æ„ä¸€ä¸ªé‡å¤çš„æ•°å­—ã€‚ ä¾‹å¦‚ï¼Œå¦‚æžœè¾“å…¥é•¿åº¦ä¸º7çš„æ•°ç»„{2,3,1,0,2,5,3}ï¼Œé‚£ä¹ˆå¯¹åº”çš„è¾“å‡ºæ˜¯ç¬¬ä¸€ä¸ªé‡å¤çš„æ•°å­—2ã€‚ Tipsï¼š å½“ç„¶è¿™ä¸ªæ˜¯å¯ä»¥ä½¿ç”¨dictionaryï¼Œéœ€è¦O(N) è¿™æ ·çš„ç©ºé—´ã€‚è¿˜æœ‰ä¸€ç§æ€è·¯ï¼Œè¿™ä¸ªæ‰€æœ‰çš„æ•°å­—éƒ½æ˜¯ (0, n-1) è¿™æ ·çš„åŒºé—´ï¼Œæ‰€ä»¥æ˜¯å¯ä»¥å’Œindex è¿›è¡Œè”ç³»ä¸€ä¸‹çš„ã€‚è¿™ä¸ª â€œäº¤æ¢â€ æ˜¯ä¼šè¿›è¡ŒæŽ’åºçš„ï¼ˆindex å¯¹åº”ç€ numberï¼‰ï¼Œä½†æŽ’åºä¸æ˜¯æœ€ç»ˆçš„ç›®çš„ï¼Œæœ€ç»ˆç›®çš„æ˜¯å¾—åˆ°é¦–ä¸ªé‡å¤çš„æ•°å­—ã€‚ 12345678910111213141516171819202122232425262728293031323334353637383940414243class Solution: # è¿™é‡Œè¦ç‰¹åˆ«æ³¨æ„~æ‰¾åˆ°ä»»æ„é‡å¤çš„ä¸€ä¸ªå€¼å¹¶èµ‹å€¼åˆ°duplication[0] # å‡½æ•°è¿”å›žTrue/False # ç¬¬äºŒç§æ–¹å¼ï¼Œå¦‚æžœè¿™ä¸ªæ˜¯æœ‰åºçš„ é‚£ä¹ˆ numbers[i] ==i è¿™ä¸ªæ˜¯æˆç«‹çš„ def duplicate(self, numbers, duplication): # write code here length =len(numbers) for i in range(length): while i != numbers[i]: if numbers[numbers[i]] == numbers[i]: duplication[0] = numbers[i] return True else: numbers[numbers[i]], numbers[i] = numbers[i], numbers[numbers[i]] return False``` - æž„å»ºä¹˜ç§¯æ•°ç»„&gt; ç»™å®šä¸€ä¸ªæ•°ç»„A[0,1,...,n-1],è¯·æž„å»ºä¸€ä¸ªæ•°ç»„B[0,1,...,n-1],å…¶ä¸­Bä¸­çš„å…ƒç´ B[i]=A[0]*A[1]*...*A[i-1]*A[i+1]*...*A[n-1]ã€‚ä¸èƒ½ä½¿ç”¨é™¤æ³•ã€‚Tips: åˆ†æˆä¸Šä¸‹ä¸‰è§’å½¢è¿›è¡Œè®¡ç®—ï¼Œä¸èƒ½æ¯ä¸ªB[i] éƒ½è¿›è¡Œå•ç‹¬é‡å¤è®¡ç®—ã€‚ä¸‹ä¸‰è§’å½¢æ˜¯ä»Žä¸Šå¾€ä¸‹éåŽ†ï¼Œä¸Šä¸‰è§’å½¢æ˜¯ä»Žä¸‹å¾€ä¸ŠéåŽ†ã€‚ans å­˜å‚¨å„ä¸ªä¸åŒçš„ç»“æžœã€‚```python class Solution: # æ€è·¯ï¼š è½¬æ¢æˆå›¾å½¢çš„å°±å®¹æ˜“æƒ³ä¸€äº›ï¼Œ https://blog.csdn.net/u010005281/article/details/80200398 # ä»£ç ï¼šhttps://blog.csdn.net/fuxuemingzhu/article/details/79718543 # A æ˜¯ä¸€ä¸ªlist ï¼Œåªæ˜¯è‡ªå·±æž„å»ºçš„æ˜¯ä¸€ä¸ªçŸ©é˜µ def multiply(self, A): # write code here ans =[] tmp =1 length =len(A) # å€¼å¾—æ˜¯ rows # é¦–å…ˆæ˜¯ä¸‹ä¸‰è§’å½¢ å„ä¸ªéƒ¨åˆ†çš„æ•°å€¼çš„ç›¸ä¹˜ï¼Œ ä»Žä¸Šå¾€ä¸‹éåŽ† for i in range(length): ans.append(tmp) tmp *= A[i] tmp =1 # ä¸Šä¸‰è§’å½¢ ä»Žä¸‹å¾€ä¸Šè¿›è¡ŒéåŽ† for i in range(length-1, -1, -1): ans[i] *= tmp tmp *= A[i] return ans è¡¨ç¤ºæ•°å€¼çš„å­—ç¬¦ä¸² è¯·å®žçŽ°ä¸€ä¸ªå‡½æ•°ç”¨æ¥åˆ¤æ–­å­—ç¬¦ä¸²æ˜¯å¦è¡¨ç¤ºæ•°å€¼ï¼ˆåŒ…æ‹¬æ•´æ•°å’Œå°æ•°ï¼‰ã€‚ä¾‹å¦‚ï¼Œå­—ç¬¦ä¸²â€+100â€,â€5e2â€,â€-123â€,â€3.1416â€å’Œâ€-1E-16â€éƒ½è¡¨ç¤ºæ•°å€¼ã€‚ ä½†æ˜¯â€12eâ€,â€1a3.14â€,â€1.2.3â€,â€+-5â€å’Œâ€12e+4.3â€éƒ½ä¸æ˜¯ã€‚ 123456789101112131415161718192021222324252627282930313233343536373839class Solution: # så­—ç¬¦ä¸² # ç¬¬ä¸€ç§æ–¹æ³•æ˜¯ float()å¼ºè½¬ï¼Œä¸€ç§æ˜¯ re æ­£åˆ™è¡¨è¾¾å¼åŒ¹é… æœ€åŽä¸€ç§é€»è¾‘åˆ¤æ–­ä¹‹ç±»çš„ # ä»¥ e ä¸ºåˆ†å‰²ç¬¦ï¼Œåˆ†æˆfront and behind ä¸¤éƒ¨åˆ†ï¼Œbehind é•¿åº¦ä¸èƒ½ä¸º0 æˆ–è€…å‡ºçŽ° . # digitçš„åˆ¤æ–­ï¼Œ+- åªèƒ½å‡ºçŽ°åœ¨é¦–ä½ï¼Œ . åªèƒ½å‡ºçŽ°ä¸€æ¬¡ """ https://github.com/leeguandong/Interview-code-practice-python/blob/master/%E5%89%91%E6%8C%87offer/%E8%A1%A8%E7%A4%BA%E6%95%B0%E5%80%BC%E7%9A%84%E5%AD%97%E7%AC%A6%E4%B8%B2.py """ def isNumeric(self, s): if not s or len(s) == 0: return s s = [i.lower() for i in s] if 'e' in s: index = s.index('e') front = s[:index] behind = s[index + 1:] if len(behind) == 0 or '.' in behind: return False f = self.Digit(front) b = self.Digit(behind) return f and b else: isNum = self.Digit(s) return isNum def Digit(self, s): dotNum = 0 allowNum = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '.', '+', '-'] for i in range(len(s)): if s[i] not in allowNum: return False if s[i] == '.': dotNum += 1 if s[i] in '+-' and i != 0: return False if dotNum &gt; 1: return False return True å­—ç¬¦æµä¸­ç¬¬ä¸€ä¸ªä¸é‡å¤çš„å­—ç¬¦ è¯·å®žçŽ°ä¸€ä¸ªå‡½æ•°ç”¨æ¥æ‰¾å‡ºå­—ç¬¦æµä¸­ç¬¬ä¸€ä¸ªåªå‡ºçŽ°ä¸€æ¬¡çš„å­—ç¬¦ã€‚ä¾‹å¦‚ï¼Œå½“ä»Žå­—ç¬¦æµä¸­åªè¯»å‡ºå‰ä¸¤ä¸ªå­—ç¬¦â€goâ€æ—¶ï¼Œç¬¬ä¸€ä¸ªåªå‡ºçŽ°ä¸€æ¬¡çš„å­—ç¬¦æ˜¯â€gâ€ã€‚å½“ä»Žè¯¥å­—ç¬¦æµä¸­è¯»å‡ºå‰å…­ä¸ªå­—ç¬¦â€œgoogleâ€æ—¶ï¼Œç¬¬ä¸€ä¸ªåªå‡ºçŽ°ä¸€æ¬¡çš„å­—ç¬¦æ˜¯â€lâ€ã€‚ 12345678910111213141516171819class Solution: # è¿”å›žå¯¹åº”char # è¿™ä¸ªæ²¡æœ‰äº†dict é‚£ä¹ˆä¾èµ–äºŽ count å‡½æ•° # ä¸»è¦å·®åˆ«åœ¨äºŽæœ‰äº†ä¸€ä¸ª å­—ç¬¦æµï¼Œæ˜¯åŠ¨æ€çš„ï¼Œæ‰€ä»¥éœ€è¦æœ‰ä¸€ä¸ªå¤§çš„å­˜å‚¨çš„list def __init__(self): self.list1 =[] def FirstAppearingOnce(self): # write code here for string in self.list1: if self.list1.count(string) == 1: return string return "#" def Insert(self, char): # write code here self.list1.append(char)]]></content>
      <categories>
        <category>ç®—æ³•</category>
      </categories>
      <tags>
        <tag>å‰‘æŒ‡offer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pygen]]></title>
    <url>%2F2019%2F04%2F24%2Fpygen%2F</url>
    <content type="text"><![CDATA[pygenåŠŸèƒ½ï¼šæœ‰å…³è”çš„éšæœºç”Ÿæˆäººåï¼Œé‚®ç®±ï¼ŒID Card (ssn)ï¼Œç”µè¯ï¼Œåœ°å€ç­‰ä¿¡æ¯ï¼Œå¹¶ä¸”å¯ä»¥é€‰æ‹©ä¿å­˜ä¸º pandas dataframeæ ¼å¼, æ•°æ®åº“â€.dbâ€ æ–‡ä»¶, Excel æ–‡ä»¶å’Œcsv æ–‡ä»¶æ ¼å¼ï¼Œç”¨äºŽæœºå™¨å­¦ä¹ è®­ç»ƒã€‚é¡¹ç›®åœ°å€githubã€‚ éšæœºç”Ÿæˆè™šå‡ä¸ªäººä¿¡æ¯å…·æœ‰å¾ˆå¤§çš„åº”ç”¨ç©ºé—´ã€‚é¦–å…ˆï¼Œè™šå‡çš„ç”Ÿæˆæ•°æ®å¯ä»¥ç”¨äºŽæœºå™¨å­¦ä¹ æ¨¡åž‹çš„â€œå‡†å¤‡æ•°æ®â€ï¼Œå½“çœŸå®žçš„æ•°æ®æ¯”è¾ƒå°‘æˆ–è€…éš¾ä»¥èŽ·å¾—çš„æ—¶å€™ï¼Œå¯ä»¥ä½¿ç”¨ç”Ÿæˆæ•°æ®è¿›è¡Œè®­ç»ƒæ¨¡åž‹ï¼Œå¾…æ¨¡åž‹è°ƒé€šä¹‹åŽï¼Œç„¶åŽä½¿ç”¨çœŸå®žçš„æ¨¡åž‹ã€‚å¹¶ä¸”ï¼Œå½“çœŸå®žçš„æ•°æ®é›†ä¸­ç¼ºå°‘æŸäº›ç‰¹å¾æ—¶å€™ï¼Œå¯ä»¥ä½¿ç”¨è¿™ç§æ–¹æ³•è¿›è¡Œç‰¹å¾çš„å¡«å……ã€‚æ¯”å¦‚å¤§çš„æ•°æ®é›†ä¸­ç¼ºå°‘çŽ°å±…åŸŽå¸‚åœ°å€çš„æ—¶å€™ï¼Œå¯ä»¥è°ƒç”¨è¯¥åº“ä¸­çš„ â€œcity_realâ€ è¿›è¡Œå¡«å……ã€‚ å½“å‰ä½¿ç”¨æœ€ä¸ºå¹¿æ³›çš„æ˜¯ Faker å¼€æºåº“ç”¨äºŽæ•°æ®çš„ç”Ÿæˆã€‚è™½ç„¶è¯¥åº“æ”¯æŒä¸­æ–‡ï¼Œä½†æ˜¯å¯¹äºŽä¸­æ–‡çš„æ”¯æŒåŠ›åº¦æœ‰é™ï¼Œæ‰€ä»¥æœ‰æ—¶å€™å¹¶ä¸èƒ½æ»¡è¶³æˆ‘çš„éœ€æ±‚ï¼Œæ¯”å¦‚è¯´ç”Ÿæˆçš„èº«ä»½è¯ (ssn) å’Œå§“åæ‰€èƒ½ä½“çŽ°çš„æ€§åˆ«æ˜¯ä¸åŒ¹é…(äº†è§£æ›´å¤šå¯ä»¥å‚è€ƒè¿™é‡Œ)ã€ç”Ÿæˆçš„å§“åä¸­ç¼ºå°‘å¤å§“å’Œç”µè¯é‚®ç®±ç­‰ä¿¡æ¯ä¸ç¬¦åˆæˆ‘ä»¬çš„ä½¿ç”¨ä¹ æƒ¯ç­‰ç­‰ã€‚æ‰€ä»¥æˆ‘å°†ä»Žä»¥ä¸‹å‡ ç‚¹æ”¹è¿›ï¼š å¢žå¼ºæ•°æ®ä¹‹é—´ç›¸å…³æ€§ ç”Ÿæˆåå­—çš„å¤šæ ·æ€§ ç¬¦åˆå›½äººä½¿ç”¨ä¹ æƒ¯çš„é‚®ç®±ç”µè¯ æä¾›ä¿å­˜å¤šç§ä¿å­˜æ–‡ä»¶æ ¼å¼ï¼Œæ›´åŠ é€‚åˆæœºå™¨å­¦ä¹ çš„è®­ç»ƒ ä¸­æ–‡åå­—æœ‰å¾ˆå¼ºçš„æ€§åˆ«å±žæ€§ã€‚ä¾‹å¦‚åå­—ä¸­å¸¦æœ‰â€œæ°â€â€œå¿—â€â€œå®â€ç­‰å­—çš„ä¸€èˆ¬ä¸ºç”·æ€§ï¼Œå¸¦æœ‰â€œç¬â€â€œä½©â€â€œæ¢…â€ç­‰å­—çš„ä¸€èˆ¬ä¸ºå¥³æ€§ã€‚å½“ç„¶ä¹Ÿæœ‰ä¸€äº›æ¯”è¾ƒä¸­æ€§çš„å­—ï¼Œä¾‹å¦‚â€œæ–‡â€â€œå®‰â€â€œæ¸…â€ç­‰ï¼Œæ¯”è¾ƒéš¾çŒœæµ‹æ€§åˆ«ï¼Œå…³äºŽè¿™ç‚¹ä¼šåœ¨å¦ä¸€ä¸ªåšå®¢ä¸­å±•å¼€ï¼Œè¯·æœŸå¾…ã€‚ faker å¯¹ä¸­æ–‡çš„æ”¯æŒæœ‰é™ï¼Œæ¯”å¦‚ä¸‹é¢è¿™ç§æƒ…å†µã€‚ 1234from faker import Fakerfake = Faker('zh_CN')for _ in range(10): print(fake.name(),fake.ssn(),fake.phone_number()) ä»Žå›¾ä¸­å¯ä»¥æ˜Žæ˜¾çš„çœ‹å‡º â€œçŽ‹çŽ‰æ¢…â€å’Œ â€œæŽæ¡‚èŠ±â€éƒ½æ˜¯ä¸¤ä¸ªå¥³æ€§ï¼Œä½†æ˜¯è¿™ç§èº«ä»½è¯ä¿¡æ¯ï¼ˆssnï¼‰éƒ½æ²¡æœ‰ä½“çŽ°è¿™ç‚¹ã€‚å…³äºŽèº«ä»½è¯çš„ç§‘æ™®ä¿¡æ¯å¯ä»¥ä»Žè¿™é‡ŒèŽ·å¾—ã€‚ç®€å•æ¥è¯´å€’æ•°ç¬¬äºŒä½è¡¨ç¤ºæ€§åˆ«ä¿¡æ¯ï¼Œå¦‚æžœæ˜¯ç”·æ€§å°±æ˜¯å¥‡æ•°å¦‚æžœæ˜¯å¥³æ€§å°±æ˜¯å¶æ•°ã€‚faker ç”Ÿæˆçš„æ•°æ®æ˜¯ä¸å…·æœ‰æ•°æ®ä¹‹é—´çš„ç›¸å…³æ€§çš„ã€‚ åŸºäºŽæ­¤ï¼Œæˆ‘ä»¬è¿›è¡Œäº†æ”¹è¿›ã€‚é¦–å…ˆæ˜¯å§“åçš„ç”Ÿæˆï¼Œç„¶åŽæ˜¯æ€§åˆ«çš„åˆ¤æ–­ï¼Œæœ€åŽå†ç”Ÿæˆç›¸åº”æ€§åˆ«çš„èº«ä»½è¯å·ç ã€‚ 123from pygen import pygendb =pygen()db.gen_dataframe(fields =['name', 'ssn', 'phone', 'email']) æ•ˆæžœå¦‚ä¸‹ï¼š çº¢è‰²çº¿æ¡è¡¨ç¤ºå§“åå’Œæ€§åˆ«å¯¹åº”ä¸€è‡´ï¼Œè“è‰²çº¿æ¡è¡¨ç¤ºç»“æžœä¸ç¡®å®šï¼ˆâ€œé•œé˜³ç‚Žâ€ åƒæ˜¯ä¸€ä¸ªä¸­æ€§çš„åå­—ï¼‰ï¼Œç»¿è‰²è¡¨ç¤ºç”Ÿæˆäº†å«æœ‰å¤å§“çš„åå­—ï¼Œå¢žå¼ºäº†æ•°æ®çš„å¤šæ ·æ€§ã€‚ ä»Žä¸Šå›¾çš„ â€œmailâ€ ä¸€åˆ—å¯ä»¥çœ‹å‡ºé‚®ç®±å‰ç¼€çš„å‘½ååŸºæœ¬ä¸Šæ˜¯ä¸­æ–‡åå­—ä¸­â€œå§“â€ å’Œâ€œæ°‘â€çš„æ‹¼éŸ³ç»„åˆï¼ŒåŠ å¼ºäº†æ•°æ®ä¹‹é—´çš„ç›¸å…³æ€§å’ŒçœŸå®žåº¦ã€‚ å¦å¤–ï¼Œç”µè¯å·ç æŒ‰ç…§è¿è¥å•†åˆ†ä¸ºä¸‰ç±»ï¼š0 è¡¨ç¤ºç§»åŠ¨ï¼Œ1è¡¨ç¤ºè”é€šï¼Œ2è¡¨ç¤ºç”µä¿¡ã€‚ print(&apos;ç§»åŠ¨å­—æ®µ:&apos;) for _ in range(5): print(db.simple_ph_num(types =0)) print(&apos;è”é€šå­—æ®µ:&apos;) for _ in range(5): print(db.simple_ph_num(types =1)) print(&apos;ç”µä¿¡å­—æ®µï¼š&apos;) for _ in range(5): print(db.simple_ph_num(types =2)) è¾“å‡ºï¼š ç§»åŠ¨å­—æ®µ: 15023689929 16771753917 16790223946 15950129353 15271129554è”é€šå­—æ®µ: 13869739303 13786227031 13950354445 15137578545 15240836142ç”µä¿¡å­—æ®µï¼š 17172983067 15658567011 18562313243 17073127396 15543448286 æœ€åŽæä¾›äº†å¤šç§æ–‡ä»¶ä¿å­˜æ ¼å¼ï¼ŒåŒ…æ‹¬â€.csvâ€, â€œ.dbâ€ å’Œâ€.xlsxâ€ç­‰æ ¼å¼ã€‚å¯ä»¥ä½¿ç”¨å¦‚ä¸‹ï¼š from pygen import pygen db =pygen() db.gen_table(filename =filename, fields =[&apos;name&apos;, &apos;ssn&apos;, &apos;phone&apos;, &apos;email&apos;]) db.gen_excel(filename =filename, fields =[&apos;name&apos;, &apos;ssn&apos;, &apos;phone&apos;, &apos;email&apos;]) db.gen_csv(filename =filename, fields =[&apos;name&apos;, &apos;ssn&apos;, &apos;phone&apos;, &apos;email&apos;])]]></content>
      <categories>
        <category>æœºå™¨å­¦ä¹ </category>
      </categories>
      <tags>
        <tag>Data Enhancement</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mode Collapse in GANs]]></title>
    <url>%2F2019%2F04%2F18%2Fmode-collapse-in-gan%2F</url>
    <content type="text"><![CDATA[Mode collapse, a failure case for GANs where the generator generate a limited diversity of samples, regardless of the input. But what causes the mode collapse? There are four reasons for that. The objective of GANs The generator, generates new data, while the discriminator evaluates it for authenticity but not for the diversity of generated instances. the generator can win by producing a polynomial number of training examples. And a low capacity discriminator cannot detect this process, thus, it cannot guide the generator to approximate the target distribution. Even if a high discriminator identifies and assigns the collapse part a low probability, then the generator will simply move from its collapsed output to focus on another fixed output point. Generator No matter the objective function is, if it only considers individual samples (without looking forward or backward) then the generator is not directly incentivised to produce diverse examples. From [1], standard GAN training corresponds exactly to updating the generator parameters using only the first term in this gradient because of a fixed discriminator during GAN training. Therefore, in standard GAN training, each generator update step is a partial collapse towards a delta function. $$\frac { \mathrm { d } f _ { K } \left( \theta _ { G } , \theta _ { D } \right) } { \mathrm { d } \theta _ { G } } = \frac { \partial f \left( \theta _ { G } , \theta _ { D } ^ { K } \left( \theta _ { G } , \theta _ { D } \right) \right) } { \partial \theta _ { G } } + \frac { \partial f \left( \theta _ { G } , \theta _ { D } ^ { K } \left( \theta _ { G } , \theta _ { D } \right) \right) } { \partial \theta _ { D } ^ { K } \left( \theta _ { G } , \theta _ { D } \right) } \frac { \mathrm { d } \theta _ { D } ^ { K } \left( \theta _ { G } , \theta _ { D } \right) } { \mathrm { d } \theta _ { G } }$$ Some methods have been proposed. Multiple generators and weight-sharing generators are developed to capture more modes of the distribution. Discriminator The mode collapse is often explained as gradient exploding of discriminator, which comes from the imbalance between the discriminator and the generator. For example, the technique of TTUR could help discriminator to keep its optimality. But some researchers believe that this is a desirable goal since a good discriminator can give good feedback and ignore the fact. In addition, the discriminator process each example independently, the generator depends on discriminator, thus no mechanism to tell the outputs of the generator to become more similar to each other. The idea from [2], that we could use mini-batch discrimination to help generator give better feedback A straightforward approach to handle multimodality is to take random noise vectors along with the conditional contexts as inputs, where the contexts determine the main content and noise vectors are responsible for variations.The noise vectors are ignored or of minor impacts, since cGANs pay more attention to learn from the high-dimensional and structured conditional contexts. Another question Mode collapse may happen only partially?since training is stochastic progress, the input of generator network will vary and the sample drawn from the real distribution will also vary But sometimes mode collapse is not all bad news. In style transfer using GAN, we are happy to convert one image to just a good one, rather than finding all variants. Indeed, the specialization in the partial mode collapse sometimes creates higher quality images. referrences:[1]. Section 2.4 of Unrolled Generative Adversarial Networks[2]. Section 3.2 of Improved Techniques for Training GANs[3]. Mode Seeking Generative Adversarial Networks for Diverse Image Synthesis[4]. Improving Generalization and Stability of Generative Adversarial Networks]]></content>
      <categories>
        <category>Deep learning</category>
      </categories>
      <tags>
        <tag>GANs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[A Not-So-Gentle Introduction to Hyper-parameters Tuning]]></title>
    <url>%2F2019%2F04%2F17%2Fa-not-so-gentle-introduction-to-hyperparameters-tuning%2F</url>
    <content type="text"><![CDATA[Setting the hyper-parameters seems like a black art that requires years of experience to acquire. Currently, there are no simple and easy ways to set hyper-parameters, especifically, batch size, learning rate, momentum, and weight decay. A grid search or random search maybe sounds like a good idea. In this blog, Iâ€™d like to share you my idea from reading papers and my projects. Hyper-parametersBatch SizeLearning rate is maybe the most important hyper-parameters, but we choose batch size firstly because large batch size needs a large learning rate in most circumstances. A general principle is: use as a large batch size as possible to fit your CPU memory or/both GPU memory. There are several reasons: larger batch sizes permit the use of larger learning rates A constant number of iterations favors larger batch sizes However, small batch sizes add regularization while large batch sizes add less. So utilize it while balancing the proper amount of regularization. Learning RateWe will introduce the idea from [Cyclical Learning Rates for Training Neural Networks][1]: Cyclical Learning Rates. Instead of monotonically decreasing the learning rate, this method lets the learning rate cyclically vary between reasonable boundary values. The essence of this learning rate policy comes from the observation that increasing the learning rate might have a short term negative effect and yet achieve a longer term beneficial effect. This observation leads to the idea of letting the learning rate vary within a range of values rather than adopting a stepwise fixed or exponentially decreasing value. That is, one sets minimum and maximum boundaries and the learning rate cyclically varies between these bounds.%From Cyclical Learning Rates for Training Neural Networks An intuitive understanding of why CLR methods work comes from considering the loss function topology. Dauphin et al. argue that the difficulty in minimizing the loss arises from saddle points rather than poor local minima. Saddle points have small gradients that slow the learning process. However, increasing the learning rate allows for more rapid traversal of saddle point plateaus. But the question is that how can we find the Minimum bound and Maximum bound. There is a simple way to estimate the reasonable minimum and maximum boundary values with one training run of the network for a few epochs. It is a â€œLR range testâ€; run your model for several epochs while letting the learning rate increase linearly between low and high LR values. For example, set both the step size and maxiter to the same number of iterations. In this case, the learning rate will increase linearly from the minimum value to the maximum value during this short run. Next, plot the accuracy versus learning rate. Note the learning rate value when the accuracy starts to increase and when the accuracy slows, becomes ragged, or starts to fall. These two learning rates are good choices for bounds; that is, set $base-{lr}$ to the first value and set $max-{lr}$ to the latter value. MomentumSince learning rate is regarded as the most important hyper-parameter to tune then momentum is also important. Like learning rates, it is valuable to set momentum as large as possible without causing instabilities during training. The large learning rate can deal with local minimum but works fail when it comes to saddle point where momentum comes to rescue. The local minimum is like the following picture.In mathematics, a saddle point or minimax point is a point on the surface of the graph of a function where the slopes (derivatives) in orthogonal directions are all zero (a critical point), but which is not a local extremum of the function. Your first step from the very top would likely take you down, but then youâ€™d be on a flat rice terrace. The gradient would be zero, and youâ€™d have nowhere to go. To remedy this, we employ momentum - the algorithm remembers its last step and adds some psroportion of it to the current step. This way, even if the algorithm is stuck in a flat region, or a small local minimum, it can get out and continue towards the true minimum. In summary: when performing gradient descent, learning rate measures how much the current situation affects the next step, while momentum measures how much past steps affect the next step. Weights DecayWhen training neural networks, it is common to use â€œweight decay,â€ where after each update, the weights are multiplied by a factor slightly less than 1. This prevents the weights from growing too large and can be seen as gradient descent on a quadratic regularization term. But why? Large weights might correlate with certain patterns in the input data (x), this means that the model almost hard codes certain values. This then makes our training data fit well but our test data fit less well. The idea of weight decay is simple: to prevent overfitting, every time we update a weight $w$ with the gradient $âˆ‡J$ in respect to $w$, we also subtract from it $Î»âˆ™w$. This gives the weights a tendency to decay towards zero, hence the name. L2 is a type of weights decay.$$J ( W ; X , y ) + \frac { 1 } { 2 } \lambda \cdot | W | ^ { 2 }$$ But weights decay is not necessarily true for all gradient-base algorithms and was recently shown to not be the case for adaptive gradient algorithms, such as Adam. In addition, weight decay is not the only regularization technique. In the past few years, some other approaches have been introduced such as Dropout, Bagging, Early Stop, and Parameter Sharing which work very well in NNs. Takeoff Batch Size Use as a large batch size as possible to fit your memory Learning Rate Perform a learning rate range test to identify a â€œlargeâ€ learning rate. Momentum Test with short runs of momentum values 0.99, 0.97, 0.95, and 0.9 to get the best value for momentum. If using the 1-cycle learning rate schedule, it is better to use a cyclical momentum (CM) that starts at this maximum momentum value and decreases with increasing learning rate to a value of 0.8 or 0.85. Weights Decay A grid search to determine the proper magnitude but usually does not require more than one significant figure accuracy.A more complex dataset requires less regularization so test smaller weight decay values, such as $10^{âˆ’4} $, $10^{âˆ’5} $, $10^{âˆ’6} $, 0.A shallow architecture requires more regularization so test larger weight decay values, such as $10^{âˆ’2} $, $10^{âˆ’3} $, $10^{âˆ’4} $. References[1]. Cyclical Learning Rates for Training Neural Networks[2]. A disciplined approach to neural network hyper-parameters]]></content>
      <categories>
        <category>Deep learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Weights Initialization]]></title>
    <url>%2F2019%2F04%2F17%2Fweights-initialization%2F</url>
    <content type="text"><![CDATA[Training a neural network consists of four steps: initialize weights and biases, forward propagation, compute the loss function and backward propagation. This blog mainly focuses on the first part: weights initialization. After completing this tutorial, you will know: Four main types of weights initialization How to choose between Xavier /zivier/ initialization and He initialization Types of Weights Initialization Initializing weights with zero When you set all weights in a neural network to zero, the derivative with respect to loss function is the same for every $ w$ in the same layer, thus all the weights have the same values in the subsequent iteration, which makes your model equivalent to a linear model. Initializing weights randomly You can get weights like this (Python): w =np.random.randn(layer_size[l],layer_size[l-1]) The weighs follows standard normal distribution while it can potentially lead to two issues: vanishing gradients and exploding gradients.ä¸‹é¢çš„æƒ…å†µæ˜¯å¾ˆå®¹æ˜“å‘ç”Ÿï¼Œå› ä¸ºç½‘ç»œä¸­ç‰¹å¾è¶³å¤Ÿå¤šï¼ˆç½‘ç»œç»“æž„è¶³å¤Ÿå®½ï¼‰ï¼Œæ‰€ä»¥ random å¾—åˆ°æ•°å€¼æœ‰è¶³å¤Ÿçš„ coverageï¼Œæ‰€ä»¥å°±ä¼šå‡ºçŽ° weights too small or too large è¿™ç§æƒ…å†µã€‚ If the weights start too small, then the signal shrinks as it passes through each layer until itâ€™s too small to be useful. If the weights start too large, then the signal grows as it passes through each layer until itâ€™s too massive to be useful (big value in sigmoid function). Thus there are two necessary conditions to consider: The values of each activation layer wonâ€™t be zero The values of each activation layer wonâ€™t go into the area of saturation Xavier/Glorot Initialization For deep networks, we can use a heuristic to initialize the weights depending on the non-linear activation function. This applies to Xavier and He initialization. Xavier/Glorot Initialization initializes the weights in your network by drawing them from a distribution with zero mean and a specific variance.$$ { var } ( w _ { i } ) = \frac { 1 } { layer_{l-1}}$$ w=np.random.randn(layer_size[l],layer_size[l-1])*np.sqrt(1/layer_size[l-1]) In practice, it works better for layers with sigmoid or tanh function. He Initialization Using RELU or Leaky RELU is relatively robust to the vanishing/ exploding gradient issues compared with sigmoid function especially for networks that are not too deep. And it the case of Leaky RELU, it never has zero gradients. For RELU, we multiply the randomly generated values of $w$ by: $$\sqrt { \frac { 2 } { layer _ { [ l - 1 ] } } }$$ w=np.random.randn(layer_size[l],layer_size[l-1])*np.sqrt(2/layer_size[l-1]) Sometimes, we combine the idea of Xavier initialization and He initializaiton so the variance becomes the following: $$\sqrt { \frac { 2 } { layer _ { [ l - 1 ] } + \operatorname { layer } _ { [ l ] } } }$$ w=np.random.randn(layer_size[l],layer_size[l-1])*np.sqrt(2/(layer_size[l-1]+layer_size[l])) The idea behind this is that we set the weights neither too much bigger than 1 nor too much less than 1 so the gradients do not vanish or explode too quickly. TakeoffIn summary, the main difference in machine learning is the following: He initialization works better for layers with ReLu(s) activation. Xavier initialization works better for layers with sigmoid activation. Referrence:He initialization Xavier initialization]]></content>
      <categories>
        <category>Deep learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[CycleGAN & StyleGAN]]></title>
    <url>%2F2019%2F04%2F09%2Fcyclegan-stylegan%2F</url>
    <content type="text"><![CDATA[In the past few yeas, GANs have been used in lots of different applications such as generating synthetic data, style transfer, super-resolution and text2image generation. But we donnâ€™t aim to give a overview of what GANs are made for. There are many great and detailed blogs for your understanding. What this post is about Main ideas of CycleGAN Keypoints in StyleGAN A Gentle Introduction of GANsWe assume the reader has some prior experience with neural networks. In addition, if you are familiar with GANs you can skip this section. The famous minimax objective function can be formulated as following:$$\min _ { \theta _ { g } } \max _ { \theta _ { d } } \left[ \mathbb { E } _ { x \sim p _ { d a t a } } \log D _ { \theta _ { d } } ( x ) + \mathbb { E } _ { z \sim p ( z ) } \log \left( 1 - D _ { \theta _ { d } } \left( G _ { \theta _ { g } } ( z ) \right) \right) \right)$$But in practical, the loss function cannot work very well. So we have alternative objective function: Gradient ascent on discriminator $$\max _ { \theta _ { d } } \left[ \mathbb { E } _ { x \sim p _ { d a t a } } \log D _ { \theta _ { d } } ( x ) + \mathbb { E } _ { z \sim p ( z ) } \log \left( 1 - D _ { \theta _ { d } } \left( G _ { \theta _ { g } } ( z ) \right) \right) \right]$$ Gradient ascent on generator$$\max _ { \theta _ { g } } \mathbb { E } _ { z \sim p ( z ) } \log \left( D _ { \theta _ { d } } \left( G _ { \theta _ { g } } ( z ) \right) \right)$$ The reasoning behind this can be found in original paper. Simplily speaking, we can get higher gradient signal for bad samples, which works much better in practice.bad case çš„æ—¶å€™ï¼Œä½¿ç”¨åŽŸæ¥çš„min-max functionä¼šä½¿å¾—å­¦ä¹ çŽ‡ä¸å¤Ÿï¼Œä½¿ç”¨ gradient ascent å°±ä¼šå¥½ä¸€äº›ã€‚ From Stanford CS231 Lecture 13 â€” Generative Models Main ideas of CycleGANCycleGAN was introduced in 2017 out of Berkeley, Unpaired Image-to-Image Translation Using Cycle-Coonsistent Adversarial Networks. This task is performed on unpaired data. Recent methods such as Pix2Pix depend on the availability of training examples where the samee data is availabel in both domains. However, CycleGAN is able to learning such pair information without one-to-one mapping between training data in source and target domains. Network ArchitectureWe build three networks. A generator $F$ to convert image $y$ to image $ \hat{x}$ A generator $G$ to convert image $\hat{x}$ to image $ \hat{y}$ A discriminator $D$ to idenfify real image or generated pictureSimplified version of CycleGAN architecture can be showed in the following.The function $F$ and $G$ are generator network, which consists of encoder, transformer and decoder. Encoder is extracting the features from an image which is done by convolution networks. Each convolution layer leads to extraction of progressively higher level features. We would like to transform the feature emebdding of an image from domain $X$ to that of domain $Y$. So for this, authors have used 6 layers of resnet blocks. Resnet block is a neural network layer which consists of two convolutiona layers when a residue of input ia added to the output. This is done to ensure peoperties of input of previous layers are available for later layers as well.Resnet block can be summarized in following imageThe decoder transfer embedding from $y$ back to original embedding $x$. Loss functionThere are two types of losses in CycleGAN. Besides adversarial loss, we have another loss named reconstruction cost.Adversarial loss is similary to original GAN.$$\operatorname { Loss } _ { a d v } \left( F , D _ { x } , Y \right) = \frac { 1 } { m } \sum _ { i = 1 } ^ { m } \left( 1 - D _ { x } \left( F \left( y _ { i } \right) \right) \right) ^ { 2 }$$$$\operatorname { Loss } _ { a d v } \left( G , D _ { y } , X \right) = \frac { 1 } { m } \sum _ { i = 1 } ^ { m } \left( 1 - D _ { y } \left( G \left( x _ { i } \right) \right) \right) ^ { 2 }$$However, the adversarial loss alone is not sufficient to produce good looking images, which can not enfore that the input and output are recognizably the same. The cycle consistency loss addresses this issue. It relies on the expectation that if you convert an image to the other domain and back again, and then you should get back something similar to what you put in. It enforces that $F ( G ( x ) ) \approx x$ and $G ( F ( y ) ) \approx y$.$$\operatorname { Loss } _ { c y c } ( G , F , X , Y ) = \frac { 1 } { m } \sum _ { i = 1 } ^ { m } \left[ F \left( G \left( x _ { i } \right) \right) - x _ { i } \right] + \left[ G \left( F \left( y _ { i } \right) \right) - y _ { i } \right]$$We can get the full objective function by putting these two together.$$\mathcal { L } \left( G , F , D _ { x} , D _ { y } \right) = \mathcal { L } _ { \text { GAN } } \left( G , D _ { y } , X , Y \right) + \mathcal { L } _ { \text { GAN } } \left( F , D _ { x } , Y , X \right) + \lambda \mathcal { L } _ { \text { cyc } } ( G , F )$$ Keypoints of StyleGANThe StyleGAN offeras an upgrade version of ProGANâ€™s image generator, with a focus on the generator. ProGAN generates high-quality images but, in most models, its ability to control specific features of the generated image is very limited. In other word, the features are entangled and therefore attempting to tweak the input, even a bit, usually affects multiple features at the same time. A good illustrations would be following pictures.Compared with first version (ProGAN), the new generator includes several additions to ProGANâ€™s generators. Mapping NetworkThe mapping networkâ€™s goal in to encode the input vector into an intermediate vector whose different elements control different visual features, which consists of 8 fully connected layers and its output $w$ is of the same size as the input. Style Modules (AdaIN)The AdaIn (Adaptive Instance Normalization) module transfers the encoded information $w$, created by the mapping network, into the generated image. Removing traditional inputSince the encoded information $w$ from mapping network was used into generator image, the traditional random input can be omitted and replaced by constant values.]]></content>
      <categories>
        <category>Deep learning</category>
      </categories>
      <tags>
        <tag>GANs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python from Beginner to Master]]></title>
    <url>%2F2019%2F04%2F09%2Fpython-for-beginners%2F</url>
    <content type="text"><![CDATA[Basic Skillsmodulepython æ–‡ä»¶å¯ä»¥å½“åšä¸»æ–‡ä»¶è¿›è¡Œè¿è¡Œæˆ–è€…å½“åšå‡½æ•°çš„é›†åˆè¿›è¡Œè°ƒç”¨ã€‚å¦‚æžœæ˜¯å‰è€…ä¸€èˆ¬æ˜¯éœ€è¦åŒ…å«â€__name__ ==â€__main__â€ã€‚å¯¹äºŽåŽè€…å°±æ˜¯åœ¨å…¶ä»–çš„pythonæ–‡ä»¶ä¸­è¿›è¡Œè°ƒç”¨ã€‚12import my_module # pythonæ–‡ä»¶from my_module import my_object packagesfrom packageroot.packagefolder.mod import my_object Note: Ensure each directory within your package import contains a file __init__.py python-pathpython2 å’Œpython3 ä½¿ç”¨ä¸åŒçš„è§£é‡Šå™¨ï¼Œå¯¼è‡´åœ¨ä¸€äº›å‡½æ•°å‘½åå’Œè®¡ç®—ä¸Šæœ‰ä¸€äº›å·®åˆ«ï¼Œæœ€å¥½åœ¨æ–‡ä»¶çš„å¼€å¤´æ ‡æ˜Žä½¿ç”¨çš„è§£é‡Šå™¨ã€‚ while or forwhile : provide a condition and run the loop until the condition is not met. for: loop for a number of specific times; loop over items or characters of a string. examples:1234[Variable] AggregateFunction([Value] for [item] in [collection])x =[1, 2,3, 4, 5]y =[ 2*a for a in x if a%2 ==0]y &gt;&gt; [4, 8] æˆ–è€…å¯ä»¥ä½¿ç”¨è¿™æ ·æ›´åŠ ç®€æ´çš„è¯­å¥ï¼š12345678910111213 lambda arguments : expression fun1 = lambda a,b,c : a+b+c print(fun1(5,6,2))``` æ¥ä¸ªæ¯”è¾ƒå¤æ‚çš„ä¾‹å­:```python nums =[1,2,3,4,5] letters =['a', 'b', 'c','d','e'] # ä¸¤ä¸ªfor å¾ªçŽ¯ä¹Ÿæ˜¯è¦ç†Ÿç»ƒ nums_letters =[[n, l] for n in nums for l in letters ] nums_letters break, continue, or passThe break, continue, and pass statements in Python will allow you to use for loops and while loops more effectively in your code.12345678910number = 0for number in range(10): number = number + 1 if number == 5: pass # pass here print('Number is ' + str(number))print('Out of loop') The pass statement occurring after the if conditional statement is telling the program to continue to run the loop and ignore the fact that the variable number evaluates as equivalent to 5 during one of its iterations. yield å¯ä»¥ç”¨ç”¨ä½œæ–°çš„ ifçš„æµ‹è¯•, return results without termination The pass statement can create minimal classes, or act as a placeholder when working on new code and thinking on an algorithmic level before hammering out details.pass çš„å­˜åœ¨å°±æ˜¯å å‘ï¼Œå¦åˆ™è¿™ä¸ªåœ°æ–¹å°±æ˜¯æŠ¥é”™ï¼ˆIndentationErrorï¼‰ã€‚ç”¨äºŽæƒ³è¦æ‰©å±•çš„åœ°æ–¹ï¼Œä½†æ˜¯çŽ°åœ¨è¿˜æ²¡æœ‰æ‰©å±•ã€‚æ¯”å¦‚åœ¨æŸä¸ªmethod ä¸‹é¢æˆ–è€…æŸä¸ª if æ¡ä»¶ä¸‹ã€‚ yield or returnç»å¸¸è¢«ç”¨æ¥ä½œä¸ºç”Ÿæˆå™¨ã€‚ when you call a normal function with a return statement the function is terminated whenever it encounters a return statement. In a function with a yield statement the state of the function is â€˜savedâ€™ from the last call and can be picked up the next time you call a generator function. for examples12345678910111213141516171819gen_exp =(x **2 for x in range(10) if x %2 ==0)for x in gen_exp: print(x)def my_gen(): for x in range(5): yield xgen1 =my_gen()next(gen1)def my_generator1(): yield 1 yield 2 yield 3 my_gen =my_generator1()# ä½¿ç”¨ next() è¿›è¡Œè°ƒç”¨ä¸‹ä¸€ä¸ªnext(my_gen) recursionA function calling itself is known as recursion. list, tuples, or dictionaryåœ¨python ä¸­æ˜¯ä½¿ç”¨é¢‘ç¹çš„data structureï¼Œè¿™ä¸ªæ˜¯å±žäºŽ collection ç±»åˆ«ï¼Œé‡Œé¢æ”¾çš„æ˜¯element. list: to add/update/ delete an item of a collection 123456my_list.append('C') #adds at the endmy_list[1] = 'D' #updatemy_list.pop(1) # removesmylist.pop() # é»˜è®¤å°±æ˜¯ç±»ä¼¼ æ ˆçš„ç»“æž„ï¼Œå°±æ˜¯pop å‡ºæ¥æœ€åŽä¸€ä¸ªmylist.pop(0) # å½“ç„¶ä¹Ÿå¯ä»¥æ ¹æ®index æŒ‡å®šç‰¹å®šçš„ pop(delete) çš„element or 12del mylist[1:2] # é€šè¿‡æŒ‡å®š index range ç„¶åŽè¿›è¡Œdelmylist.sort() # æ”¯æŒ sorting ç„¶åŽæ˜¯ä»Žå°åˆ°å¤§, è¿™ä¸ªsortæ˜¯ä¸€ç§æ“ä½œï¼Œinplace çš„æ“ä½œ tuples: tuples store a sequence of objects, the object can be of any type. Tuples are faster than lists. dictionary: It stores key/value pair objects. 12345678910111213141516171819202122232425262728293031 my_dict =dict() my_dict['key'] ='value' or my_dict =&#123;'key': 'value', ...&#125; for key in my_dict: # do something if 'some key' in my_dict: # do something``` ### Iterators###```pythonclass yrange: def __init__(self, n): self.i = 0 self.n = n # è¿™ä¸ªè¡¨æ˜Žæ˜¯ä¸€ä¸ª iteratorï¼Œmake an object iterable def __iter__(self): return self # è¿™ä¸ªnext å‡½æ•°å°±è¢«å½“åšæ˜¯ classçš„å±žæ€§ï¼Œå¯ä»¥è¢«å¤–éƒ¨è°ƒç”¨çš„ï¼Œ def next(self): if self.i &lt; self.n: i = self.i self.i += 1 return i else: raise StopIteration() shallow vs deep copypython3 ä¸­ï¼šå¯¹äºŽç®€å•çš„æ•°æ®ç±»åž‹ï¼Œåƒint ï¼Œstringï¼Œè¿™ç§ copy() å’Œcopy.deepcopy() è¿™ä¸¤è€…éƒ½æ˜¯ç›¸åŒçš„ï¼Œcopy éƒ½æ˜¯ä¸€ç§æ˜ å°„ï¼Œéƒ½æ˜¯ç›¸å½“äºŽâ€å€¼â€œ ä¸Šçš„å¼•ç”¨ï¼›12345aa =2bb =aaprint(id(aa), id(bb)) # ç›¸åŒbb =3print(id(aa), id(bb)) # ä¸åŒï¼Œå› ä¸ºæŠŠ3 è¿™ä¸ªå€¼é‡æ–°å¤åˆ¶ç»™äº†å˜é‡bb å¯¹äºŽå¤æ‚çš„æ•°æ®ç±»åž‹ï¼Œä½¿ç”¨deepcopy() çš„æ—¶å€™ï¼Œæœ¬æ¥å°±æ˜¯ä¼šé‡æ–°æ‹·è´ä¸€ä»½åˆ°å†…å­˜ä¸­ã€‚åœ¨python3 ä¸­copy() å’Œdeepcopy() è¿™ä¸ªæ˜¯æ²¡æœ‰ä»€ä¹ˆåŒºåˆ«çš„ã€‚12345list1 =['a', 'b']list2 =list1 # è¿™ä¸ªæ˜¯å¼•ç”¨ï¼Œæ‰€ä»¥å’Œlist1 æ˜¯ç›¸åŒçš„list3 =copy.copy(list1) # è¿™ä¸ªid å’Œlist1 ä¸åŒlist4 =copy.deepcopy(list1)# è¿™ä¸ªid å’Œlist1 ä¸åŒ print(id(list1), id(list2), id(list3), id(list4)) object oriented design1234567891011121314151617181920212223242526272829303132333435363738394041424344454647 class ParentClass: def my_function(self): print 'I am here' class SubClass1(ParentClass): class SubClass2(ParentClass): ``` å¯¹äºŽå¤šç»§æ‰¿çš„æ”¯æŒ ï¼ˆæŽ¥å£ï¼‰```python class A(B,C): #A implments B and C``` å¦‚æžœæƒ³è¦call parent class function then you can dp:```python super(A, self).funcion_name()``` ### garbage collection###all the objects in python are stored in a heap space. Python has an in-built garbage collection mechanism. Memory management in Python involves a private heap containing all Python objects and data structures. The management of this private heap is ensured internally by the Python memory manager.In computer science, a heap is a specialized tree-based data structure which is essentially an almost complete[1] tree that satisfies the heap property: if P is a parent node of C, then the key(the value) of P is either greater than or equal to (in a max heap) or less than or equal to (in a min heap) the key of C.### try...catch###```python # raise exceptions try: raise TyeError except: print('exception') # catching exceptions try: do_something() except: print('exception') # try/ catch /finally try: do_something() except TypeError: print('exception') finally: close_connections() Advanced FeaturesLetâ€™s move on to advanced features. Lambda functionsA Lambda Function is a small, anonymous function â€” anonymous in the sense that it doesnâ€™t actually have a name. A lambda function can take any number of arguments, but must always have only one expression: 1234x = lambda a, b : a * b print(x(5, 6)) # prints '30' # åŒ¿åå‡½æ•°ä¹Ÿæ˜¯å‡½æ•°ï¼Œè°ƒç”¨çš„æ—¶å€™ä½¿ç”¨è¿™æ ·çš„æ–¹å¼ x = lambda a : a*3 + 3 print(x(3)) # prints '12' MapsMap() is a built-in Python function used to apply a function to a sequence of elements like a list or dictionary. Itâ€™s a very clean and most importantly readable way to perform such an operation.ç›¸å¯¹äºŽ lambda, map ä½¿ç”¨çš„é¢‘çŽ‡æ›´å°‘äº†ã€‚ æœ€åŽè¿”å›žçš„æ˜¯ä¸€ä¸ªlistã€‚ 1234567891011def square_it_func(a): return a * ax = map(square_it_func, [1, 4, 7])print(x) # prints '[1, 16, 49]'def multiplier_func(a, b): return a * bx = map(multiplier_func, [1, 4, 7], [2, 5, 8])print(x) # prints '[2, 20, 56]' FilteringThe Filter built-in function is quite similar to the Map function in that it applies a function to a sequence (list, tuple, dictionary). The key difference is that filter() will only return the elements which the applied function returned as True. 123456789101112 # Our numbersnumbers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]# Function that filters out all numbers which are odddef filter_odd_numbers(num): if num % 2 == 0: return True else: return Falsefiltered_numbers = filter(filter_odd_numbers, numbers)print(filtered_numbers)# filtered_numbers = [2, 4, 6, 8, 10, 12, 14] 123456from itertools import *def check_for_drop(x): print ('Checking: ', x) return (x &gt; 5)for i in dropwhile(check_for_drop, [2, 4, 6, 8, 10, 12]): print ('Result: ', i) Itertools123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263from itertools import *# zip å°±æ˜¯ä¸€å—è®¿é—®çš„é‚£ç§å½¢å¼ï¼Œè¿”å›žçš„æ˜¯ä¸€ä¸ªtuple æ•°æ®ç±»åž‹# zip ,joing two lists into a list of tuples# Easy joining of two lists into a list of tuplesfor i in zip([1, 2, 3], ['a', 'b', 'c']): print (i)# ('a', 1)# ('b', 2)# ('c', 3)# å°±æ˜¯ä¸€ä¸ªcount() è®¡æ•°åŠŸèƒ½# The count() function returns an interator that # produces consecutive integers, forever. This # one is great for adding indices next to your list # elements for readability and convenience# in python3, no need to import izip, use zip directly# è¿™ä¸ª count() åªæœ‰åœ¨è¿™é‡Œæ‰æœ‰æ„ä¹‰ï¼Œå¦‚æžœåªæ˜¯å•ç‹¬è°ƒç”¨ï¼Œæ²¡æœ‰æ„Ÿè§‰æœ‰å¤šå¤§çš„æ„ä¹‰for i in zip(count(1), ['Bob', 'Emily', 'Joe']): print (i)# (1, 'Bob')# (2, 'Emily')# (3, 'Joe') # check ï¼Œ becomes false for the first time è¿™ä¸ªæ¡ä»¶å¾ˆå…³é”®, å¯ä»¥ç†è§£æˆåªæ˜¯æ‰¾åˆ°ç¬¬ä¸€ä¸ªfalse çš„æ¡ä»¶ï¼Œç„¶åŽå°±ä¸å†æ‰§è¡Œè¯¥å‡½æ•°# The dropwhile() function returns an iterator that returns # all the elements of the input which come after a certain # condition becomes false for the first time. def check_for_drop(x): print 'Checking: ', x return (x &gt; 5)for i in dropwhile(check_for_drop, [2, 4, 6, 8, 10, 12]): print 'Result: ', i# æ³¨æ„ç†è§£è¿™ä¸ªè¾“å‡º# Checking: 2# Result: 2# Result: 4# Result: 6# Result: 8# Result: 10# Result: 12# æˆ‘çš„ç†è§£è¿™ä¸ª groupby å°±å’Œæ•°æ®åº“ä¸­çš„groupby æ˜¯ç›¸åŒçš„æ•ˆæžœ# The groupby() function is great for retrieving bunches# of iterator elements which are the same or have similar # propertiesfrom itertools import groupbythings = [("animal", "bear"), ("animal", "duck"), ("plant", "cactus"), ("vehicle", "speed boat"), ("vehicle", "school bus")]for key, group in groupby(things, lambda x: x[0]): for thing in group: print ("A %s is a %s." % (thing[1], key))#A bear is a animal.#A duck is a animal.#A cactus is a plant.#A speed boat is a vehicle.#A school bus is a vehicle. GeneratorGenerator functions allow you to declare a function that behaves like an iterator, i.e. it can be used in a for loop. This greatly simplifies your code and is much more memory efficient than a simple for loop. 12345678910111213141516numbers = list()# range()for i in range(1000): numbers.append(i + 1)total = sum(numbers)# (2) Using a generatordef generate_numbers(n): num = 0 while num &lt; n: yield num # è¿™ä¸ªyield ä¹‹åŽï¼Œå‡½æ•°å¹¶æ²¡æœ‰ç»“æŸï¼Œä¸åƒ return é‚£ç§å‡½æ•° num += 1total = sum(generate_numbers(1000))print(total)total = sum(range(1000 + 1))print(total) List Comprehensionå¸¸è§çš„å‡ ç§å½¢å¼ï¼š(An iterable is something you can loop over) list comprehensions vs loops: list comprehensions are more efficient both computationally and coding space Every list comprehension can be rewritten as a for loop, but not every for loop can be rewritten as a list comprehension. ä»Žä¼˜åŒ–çš„è§’åº¦ list comprehensionsæ˜¯ä¼˜äºŽ for loop ä¸­çš„if else æ“ä½œçš„ã€‚å› ä¸ºå‰è€…æ˜¯ predicatable pattern æ˜¯å¯ä»¥é¢„æµ‹çš„ã€‚However, keep in mind that list comprehensions are faster because they are optimized for the Python interpreter to spot a predictable pattern during looping. a small code demo:åœ¨äºŽä½¿ç”¨åŠŸèƒ½ timeit libary è¿›è¡Œå‡½æ•°çš„è®¡æ—¶æ¯”è¾ƒã€‚12345678910111213import timeitdef squares(size): result = [] for number in range(size): result.append(number * number) return resultdef squares_comprehension(size): return [number * number for number in range(size)] print(timeit.timeit("squares(50)", "from __main__ import squares", number=1_000_000))print(timeit.timeit("squares_comprehension(50)", "from __main__ import squares_comprehension", number=1_000_000)) more complex list comprehensions: è¿™ç§if çš„å†™æ³• æ˜¯ä¸¤ä¸ªè¿›è¡Œå¹¶åˆ—çš„ã€‚å…¶å®žå¯ä»¥å†™æˆ 123456numbers = [1, 2, 3, 4, 5, 6, 18, 20]squares = [number for number in numbers if number % 2 == 0 if number % 3 == 0]# orsquares = [number for number in numbers if number % 2 == 0 and number % 3 == 0]print(squares)# output: [6, 18] åœ¨ output expression ä¸­ï¼Œä¹Ÿæ˜¯å¯ä»¥ä½¿ç”¨ if else è¿›è¡Œè¿›ä¸€æ­¥è¾“å‡ºç­›é€‰ã€‚12345numbers = [1, 2, 3, 4, 5, 6, 18, 20] squares = ["small" if number &lt; 10 else "big" for number in numbers if number % 2 == 0 if number % 3 == 0] print(squares)ouput: ['small', 'big'] converting nested loops into list comprehensionä»£ç åŠŸèƒ½ï¼š éƒ½æ˜¯æŠŠäºŒç»´çš„ matrix è½¬æˆäº†ä¸€ä¸ª list ï¼ˆflattenedï¼‰123456matrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]flattened = []for row in matrix: for item in row: flattened.append(item)print(flattened) æ³¨æ„è¿™ä¸ªé¡ºåºï¼Œå…ˆæ˜¯row in matrix ç„¶åŽæ˜¯ item in row.123 matrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]flattened = [item for row in matrix for item in row] print(flattened) ouput matric from nested list comprehensions:12matrix = [[item for item in range(5)] for row in range(3)]print(matrix) å¯¹äºŽ dictionary çš„æ”¯æŒï¼š ä¸»è¦æ˜¯ dict1.items() å’Œ key, value çš„ä½¿ç”¨123prices = &#123;"beer": 2, "fish": 5, "apple": 1&#125;float_prices = &#123;key:float(value) for key, value in prices.items()&#125;print(float_prices) ä»Žä»£ç çš„è§’åº¦ï¼Œå¯ä»¥çœ‹å‡ºï¼Œæ“ä½œå’Œæœ€åŽçš„è¿”å›žçš„å½¢å¼æ˜¯æ²¡æœ‰å¾ˆå¤§çš„å…³ç³»ï¼Œä¸Šé¢æ˜¯ [], è¿™ä¸ªæ˜¯ {}, åˆ†åˆ«å¯¹åº”çš„æ˜¯ list å’Œ set ä¸¤ç§ä¸åŒçš„æ ¼å¼ã€‚123numbers = [10, 10, 20, 30, 12, -20, 0, 1]unique_squares = &#123;number**2 for number in numbers&#125;print(unique_squares) Working with CSV, Json and XMLOver the years, the list of possible formats that you can store your data in has grown significantly. But, there are 3 that dominate in their everyday usage: CSV, JSON, and XML. In this article, Iâ€™m going to share with you the easiest ways to work with these 3 popular data formats in Python! æœ‰ä¸¤ç§æ–¹å¼åŽ»è¯»å†™ csv fileï¼šä¸€ç§æ˜¯ pd.read_csv() ï¼Œä¸€ç§æ˜¯built-in çš„library ä¸­çš„åº“å‡½æ•°ä¹‹å‰ä¸€ç›´ä½¿ç”¨çš„pd.read_csv(), çŽ°åœ¨æ‰å‘çŽ°python æœ‰built-in çš„libraryã€‚We can do both read and write of a CSV using the built-in Python csv library. Usually, weâ€™ll read the data into a list of lists. python in-built function.12345678910111213141516171819import csv filename = "my_data.csv"fields = [] rows = [] with open(filename, 'r') as csvfile: csvreader = csv.reader(csvfile) # å¦‚æžœå•å•æ˜¯è¿™ä¸ªforï¼Œé‚£ä¹ˆå†…å­˜æ˜¯æ¶ˆè€—æ¯”è¾ƒå¤§çš„ # fields = csvreader.next() for row in csvreader: rows.append(row)for row in rows[:5]: print(row)# Writing to csv file with open(filename, 'w+') as csvfile: csvwriter = csv.writer(csvfile) csvwriter.writerow(fields) csvwriter.writerows(rows) 12345678910111213141516171819202122import pandas as pdfrom dicttoxml import dicttoxmlimport json# Building our dataframedata = &#123;'Name': ['Emily', 'Katie', 'John', 'Mike'], 'Goals': [12, 8, 16, 3], 'Assists': [18, 24, 9, 14], 'Shots': [112, 96, 101, 82] &#125;df = pd.DataFrame(data, columns=data.keys())# Converting the dataframe to a dictionary# Then save it to filedata_dict = df.to_dict(orient="records")with open('output.json', "w+") as f: json.dump(data_dict, f, indent=4)# Converting the dataframe to XML# Then save it to filexml_data = dicttoxml(data_dict).decode()with open("output.xml", "w+") as f: f.write(xml_data) 12345678910111213141516171819import jsonimport pandas as pd# Read the data from file# We now have a Python dictionarywith open('data.json') as f: data_listofdict = json.load(f) # We can do the same thing with pandasdata_df = pd.read_json('data.json', orient='records')# We can write a dictionary to JSON like so# Use 'indent' and 'sort_keys' to make the JSON# file look nicewith open('new_data.json', 'w+') as json_file: json.dump(data_listofdict, json_file, indent=4, sort_keys=True)# And again the same thing with pandasexport = data_df.to_json('new_data.json', orient='records') å‚è€ƒèµ„æ–™ï¼šhttps://towardsdatascience.com/the-easy-way-to-work-with-csv-json-and-xml-in-python-5056f9325ca9]]></content>
      <categories>
        <category>Python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[The Evaluation of Sentence Similarity]]></title>
    <url>%2F2019%2F04%2F06%2FThe-evaluation-of-sentence-similarity%2F</url>
    <content type="text"><![CDATA[I am trying to write my first english blog based on two reasons: First, the data set used in this blog is english; Second, Iâ€™d like to expand my reach and attract more audiences, although I should admit that nobody cares. DataInitially I want to use chinese corpus, but I cannot find a proper one. The data should sound like this one: word1 word2 similarity scoreé˜¿æ‹‰ä¼¯äºº é˜¿æ‹‰ä¼¯ 7.2ç•œäº§ å†œä¸š 5.6åž‚æ¶Ž å´‡æ•¬ 3.4æ¬¡åº ç§©åº 4.7å®šå¿ƒä¸¸ è¯å“ 4.3æˆ¿ç§Ÿ ä»·æ ¼ 5.2ç¿¡ç¿  å®çŸ³ 6.7é«˜ç§‘æŠ€ æŠ€æœ¯ 7.5è´­å…¥ è´­ä¹° 8.5è§‚éŸ³ è©è¨ 8.2å½’å¹¶ åˆå¹¶ 7.7 not like this: ä¸ºä½•æˆ‘æ— æ³•ç”³è¯·å¼€é€šèŠ±å‘—ä¿¡ç”¨å¡æ”¶æ¬¾ æ”¯ä»˜å®å¼€é€šä¿¡ç”¨å¡èŠ±å‘—æ”¶æ¬¾ä¸ç¬¦åˆæ¡ä»¶æ€Žä¹ˆå›žäº‹ 1èŠ±å‘—åˆ†æœŸä»˜æ¬¾ä¼šå½±å“ä½¿ç”¨å— èŠ±å‘—åˆ†æœŸæœ‰ä»€ä¹ˆå½±å“å— 0ä¸ºä»€ä¹ˆæˆ‘èŠ±å‘—æ²¡æœ‰ä¸´æ—¶é¢åº¦ èŠ±å‘—æ²¡æœ‰ä¸´æ—¶é¢åº¦æ€Žä¹ˆå¯ä»¥è´Ÿ 0èƒ½ä¸èƒ½å¼€èŠ±å‘—è€å…„ èŠ±å‘—é€¾æœŸäº†è¿˜èƒ½å¼€é€š 0æˆ‘çš„æ€Žä¹ˆå¼€é€šèŠ±å‘—æ”¶é’± è¿™ä¸ªèŠ±å‘—æ˜¯ä¸ªä»€ä¹ˆå•¥ï¼Ÿæˆ‘æ²¡å¼€é€š æˆ‘æ€Žä¹ˆæœ‰è´¦å• 0èš‚èšå€Ÿå‘—å¯ä»¥åœæŽ‰ä¹ˆ èš‚èšå€Ÿå‘—ä¸ºä»€ä¹ˆç»™æˆ‘å…³æŽ‰äº† 0æˆ‘æƒ³æŠŠèŠ±å‘—åŠŸèƒ½å…³äº† æˆ‘åŽ»é¥­åº—åƒé¥­ï¼Œèƒ½ç”¨èŠ±å‘—æ”¯ä»˜å— 0ä¸ºä»€ä¹ˆæˆ‘å€Ÿå‘—å¼€é€šäº†åˆå…³é—­äº† ä¸ºä»€ä¹ˆå€Ÿå‘—å­˜åœ¨é£Žé™© 0æ”¯ä»˜å®è¢«å†»äº†èŠ±å‘—è¦æ€Žä¹ˆè¿˜ æ”¯ä»˜åŠŸèƒ½å†»ç»“äº†ï¼ŒèŠ±å‘—è¿˜ä¸äº†æ€Žä¹ˆåŠž 1 If you can find the dataset where â€˜similarity scoreâ€™ is double, please donot hesitate to email me. So, the choice has to be enlgish corpus. The dataset used in this experiment are STSbenchmark and SICK data. The SICK data contains 10,000 sentence paris labeled with semantic relatedness and entailment relation. Similarity MethodsBaselineAs the baseline, we just take the embedding of the words in sentence, and compute the average, weighted by frequency of each word. 1234567891011121314151617181920212223242526272829303132def run_avg_benchmark(sentences1, sentences2, model=None, use_stoplist=False, doc_freqs=None): if doc_freqs is not None: N = doc_freqs["NUM_DOCS"] sims = [] for (sent1, sent2) in zip(sentences1, sentences2): tokens1 = sent1.tokens_without_stop if use_stoplist else sent1.tokens tokens2 = sent2.tokens_without_stop if use_stoplist else sent2.tokens tokens1 = [token for token in tokens1 if token in model] tokens2 = [token for token in tokens2 if token in model] if len(tokens1) == 0 or len(tokens2) == 0: sims.append(0) continue tokfreqs1 = Counter(tokens1) tokfreqs2 = Counter(tokens2) weights1 = [tokfreqs1[token] * math.log(N / (doc_freqs.get(token, 0) + 1)) for token in tokfreqs1] if doc_freqs else None weights2 = [tokfreqs2[token] * math.log(N / (doc_freqs.get(token, 0) + 1)) for token in tokfreqs2] if doc_freqs else None embedding1 = np.average([model[token] for token in tokfreqs1], axis=0, weights=weights1).reshape(1, -1) embedding2 = np.average([model[token] for token in tokfreqs2], axis=0, weights=weights2).reshape(1, -1) sim = cosine_similarity(embedding1, embedding2)[0][0] sims.append(sim) return sims Smooth Inverse FrequencyThe baseline, like we did before, is very simple and crude of computing sentence embedding. Word frequency cannot reliably reflect its importance to sentence, semantically speaking. Smooth Inverse Frequency (SIF) tries to solve this problem. SIF is very similar to the weighted average we used before, with the difference that itâ€™s weighted by this formular. $$\operatorname { SIF } ( w ) = \frac { a } { ( a + p ( w ) )}$$where $a$ is a hyper-parameter (set to 0.001 by default) and $ p(w)$ is the estimated word frequency in the corpus. we need to perform common component removal: subtract from the sentence embedding obtained above the first principal component of the matrix. This corrects for the influence of high-frequency words that have syntactic or dicourse function, such as â€˜butâ€™, â€˜andâ€™, etc. You can find more information from this paper. 12345678910111213141516171819202122232425262728293031323334353637def remove_first_principal_component(X): svd = TruncatedSVD(n_components=1, n_iter=7, random_state=0) svd.fit(X) pc = svd.components_ XX = X - X.dot(pc.transpose()) * pc return XXdef run_sif_benchmark(sentences1, sentences2, model, freqs=&#123;&#125;, use_stoplist=False, a=0.001): total_freq = sum(freqs.values()) embeddings = [] # SIF requires us to first collect all sentence embeddings and then perform # common component analysis. for (sent1, sent2) in zip(sentences1, sentences2): tokens1 = sent1.tokens_without_stop if use_stoplist else sent1.tokens tokens2 = sent2.tokens_without_stop if use_stoplist else sent2.tokens tokens1 = [token for token in tokens1 if token in model] tokens2 = [token for token in tokens2 if token in model] weights1 = [a / (a + freqs.get(token, 0) / total_freq) for token in tokens1] weights2 = [a / (a + freqs.get(token, 0) / total_freq) for token in tokens2] embedding1 = np.average([model[token] for token in tokens1], axis=0, weights=weights1) embedding2 = np.average([model[token] for token in tokens2], axis=0, weights=weights2) embeddings.append(embedding1) embeddings.append(embedding2) embeddings = remove_first_principal_component(np.array(embeddings)) sims = [cosine_similarity(embeddings[idx * 2].reshape(1, -1), embeddings[idx * 2 + 1].reshape(1, -1))[0][0] for idx in range(int(len(embeddings) / 2))] return sims Google Sentence EncoderInferSent is a pre-trained encoder that produces sentence embedding, which opensourced by Facebook. The Google Sentence Encoder is Googleâ€™s answer to Facebookâ€™s InferSent. In contrast to InferSent, the Google Sentence Encoder was trained on a combination of unsupervised data and supervised data (SNLI corpus), which tends to give better results. The codes can be used in Google Jupyter Notebook 12345678910111213141516171819202122232425import tensorflow_hub as hubtf.logging.set_verbosity(tf.logging.ERROR)embed = hub.Module("https://tfhub.dev/google/universal-sentence-encoder/1")def run_gse_benchmark(sentences1, sentences2): sts_input1 = tf.placeholder(tf.string, shape=(None)) sts_input2 = tf.placeholder(tf.string, shape=(None)) sts_encode1 = tf.nn.l2_normalize(embed(sts_input1)) sts_encode2 = tf.nn.l2_normalize(embed(sts_input2)) sim_scores = tf.reduce_sum(tf.multiply(sts_encode1, sts_encode2), axis=1) with tf.Session() as session: session.run(tf.global_variables_initializer()) session.run(tf.tables_initializer()) [gse_sims] = session.run( [sim_scores], feed_dict=&#123; sts_input1: [sent1.raw for sent1 in sentences1], sts_input2: [sent2.raw for sent2 in sentences2] &#125;) return gse_sims Experiments1234567891011121314def run_experiment(df, benchmarks): sentences1 = [Sentence(s) for s in df['sent_1']] sentences2 = [Sentence(s) for s in df['sent_2']] pearson_cors, spearman_cors = [], [] for label, method in benchmarks: sims = method(sentences1, sentences2) pearson_correlation = scipy.stats.pearsonr(sims, df['sim'])[0] print(label, pearson_correlation) pearson_cors.append(pearson_correlation) spearman_correlation = scipy.stats.spearmanr(sims, df['sim'])[0] spearman_cors.append(spearman_correlation) return pearson_cors, spearman_cors Helper function: 1234567891011import functools as ftbenchmarks = [ ("AVG-GLOVE", ft.partial(run_avg_benchmark, model=glove, use_stoplist=False)), ("AVG-GLOVE-STOP", ft.partial(run_avg_benchmark, model=glove, use_stoplist=True)), ("AVG-GLOVE-TFIDF", ft.partial(run_avg_benchmark, model=glove, use_stoplist=False, doc_freqs=doc_frequencies)), ("AVG-GLOVE-TFIDF-STOP", ft.partial(run_avg_benchmark, model=glove, use_stoplist=True, doc_freqs=doc_frequencies)), ("SIF-W2V", ft.partial(run_sif_benchmark, freqs=frequencies, model=word2vec, use_stoplist=False)), ("SIF-GLOVE", ft.partial(run_sif_benchmark, freqs=frequencies, model=glove, use_stoplist=False)),] Results123import matplotlib.pyplot as pltplt.rcParams['figure.figsize'] = (20,13)spearman[['AVG-GLOVE', 'AVG-GLOVE-STOP','AVG-GLOVE-TFIDF', 'AVG-GLOVE-TFIDF-STOP','GSE']].plot(kind="bar").legend(loc="lower left") Take Off Smooth Inverse Frequency methods are better than baseline, no matter with word2vec or Glove embeddings. Google Sentence Encoder has the similar performance as Smooth Inverse Frequency. Using tf-idf weights does not help and using a stoplist looks like a reasonable choice. Pearson CorrelationSpearman Correlation Full codes can be found in here.]]></content>
      <categories>
        <category>NLP</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Weights Initialization & Activation Function & Optimizer]]></title>
    <url>%2F2019%2F03%2F26%2F%E6%B7%B1%E5%BA%A6%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E7%A2%8E%E7%A2%8E%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[ä¸»è¦ä»‹ç»äº†å¸¸è§çš„ç½‘ç»œä¸­æƒé‡çš„åˆå§‹åŒ–ï¼Œæ¿€æ´»å‡½æ•°å’Œä¼˜åŒ–å™¨ã€‚ Weights Initializationweights çš„åˆå§‹åŒ–åœ¨ç½‘ç»œçš„è®­ç»ƒèµ·åˆ°é‡è¦çš„ä½œç”¨ï¼Œåˆå§‹åŒ–çš„å¥½åèƒ½å¤Ÿç›´æŽ¥å½±å“åˆ°ç½‘ç»œæ˜¯å¦å¯ä»¥æ­£å¸¸æ”¶æ•›ã€‚è¿™é‡Œçš„åˆå§‹åŒ–éƒ½æ˜¯æŒ‡çš„æ˜¯weightsåˆå§‹åŒ–ã€‚bias è¿™ä¸ªå˜é‡å°±æ˜¯åœ¨ä¼å›¾åŽ»æè¿°çœŸå®žçš„åˆ†å¸ƒï¼Œé€šè¿‡å¼•å…¥éšæœºæ€§æ¥è¡¨ç¤ºè¿™ä¸ªæ˜¯å…·æœ‰ æŽ¨å¹¿æ€§çš„ã€‚ Hereâ€™s another trick â€” before squishing our scalar value (called an activation) into the sigmoid function, we can add a little number called the bias, b. There will be two biases, one for each user and one for each movie. Random Initializationæœ€åŸºç¡€çš„å³ä½¿ bias ä½¿ç”¨ zero initialization ï¼Œç„¶åŽ weights ä½¿ç”¨ random initialzationã€‚è¿™ç§æ–¹æ³•çš„ç¼ºé™·åœ¨äºŽæ¢¯åº¦æ¶ˆå¤±ã€‚å°±æ˜¯ä½ çš„weights å¦‚æžœå¾ˆå¤§æˆ–è€…å¾ˆå°çš„æ—¶å€™ï¼Œå†åŠ ä¸Šå¦‚æžœä½¿ç”¨äº†sigmoid é‚£ä¹ˆå¾ˆå®¹æ˜“å‡ºçŽ°ä¸Šè¿°çš„çŽ°è±¡ã€‚ a) If weights are initialized with very high values the term np.dot(W,X)+bbecomes significantly higher and if an activation function like sigmoid() is applied, the function maps its value near to 1 where slope of gradient changes slowly and learning takes a lot of time.b) If weights are initialized with low values it gets mapped to 0, where the case is same as above. He Initialization$$W^{[l]} =np.random.randn(size_l, size_l -1)* np.sqrt(2/size_l -1) $$è¿™ä¸ªæ˜¯ä½¿ç”¨ relu æˆ–è€…è¯´ leaky relu é…åˆä½¿ç”¨çš„ã€‚ æ‰€ä»¥ä¸Šè¿°çš„åˆå§‹åŒ–ä»Žæ•°å­¦çš„è§’åº¦åŽ»ç†è§£ï¼š ä»Ž(0,1) æ ‡å‡†æ­£å¤ªåˆ†å¸ƒ è½¬æ¢æˆäº† (0, np.sqrt(2/ size_l -1)) è¿™æ ·çš„åˆ†å¸ƒï¼Œå°±æ˜¯ä½ ç½‘ç»œç»“æž„æ˜¯è¶Šå®½ï¼Œé‚£ä¹ˆè¿™ä¸ªæ–¹å·®å°±æ˜¯è¶Šå°çš„ï¼Œæœ€åŽçš„ç»“æžœæ˜¯è¶Šé›†ä¸­çš„ï¼Œå°±è¶Šé›†ä¸­çš„ å‡å€¼ u å·¦å³ã€‚ä»Žå›¾åƒçš„è§’åº¦çœ‹ï¼Œæ–¹å·®è¶Šå¤§ï¼Œå›¾åƒè¶ŠçŸ®èƒ–ï¼›æ–¹å·®è¶Šå°ï¼Œå›¾åƒè¶Šç˜¦é«˜ã€‚ Xavier initialization /ËˆzeÉªvjÉ™r/sè¿™ä¸ªæ˜¯ä½¿ç”¨tanh() ä½œä¸º activation functionçš„ã€‚$$W^{[l]} =np.random.randn(size_l, size_l -1)* np.sqrt( 1/size_l -1) $$æ€»çš„æ€æƒ³åŽŸåˆ™ï¼šThey set the weights neither too much bigger that 1, nor too much less than 1.å°±æ˜¯æœ¬æ¥å°±æ˜¯åœ¨ (0,1) æ ‡å‡†æ­£å¤ªåˆ†å¸ƒå‡ºæ¥ï¼Œç„¶åŽè¿›è¡Œäº†è¿›ä¸€æ­¥çš„çº¦æŸæ¡ä»¶ã€‚ Activation functionæ€»çš„è¯´å¯ä»¥åˆ†ä¸ºçº¿æ€§å’Œéžçº¿æ€§çš„æ¿€æ´»å‡½æ•°ï¼Œ activation function çš„ä½œç”¨ å°±æ˜¯å¯¹äºŽç½‘ç»œçš„è¾“å‡º è¯´yes or no. It maps the resulting values in between 0 to 1 or -1 to 1 etc. Sigmoid function (Logistic Activation)the only reason why we use sigmoid is because it exists between 0 to 1. è¿™ä¸ªéžå¸¸æœ‰åˆ©äºŽ predict probability. å› ä¸ºè‡ªç„¶æ˜ å°„åˆ° 0 æ˜¯ä¸å­˜åœ¨ ç„¶åŽ1 æ˜¯å­˜åœ¨ã€‚è€Œå½“å¤šåˆ†ç±»çš„æ—¶å€™ï¼Œä½¿ç”¨softmaxã€‚ Tanh function The advantage is that the negative inputs will be mapped strongly negative and the zero inputs will be mapped near zero in the tanh graph. Relu (Rectified Linear Unit) Activationæœ¬è´¨ä¸Šæ˜¯åˆ†æ®µå‡½æ•°ã€‚ range: [0, infinity]. The function and its derivative both are monotonic. Leaky Reluæ¯å½“ä¸€ä¸ªä¼Ÿå¤§çš„ä¸œè¥¿äº§å‡ºï¼Œæ€»ä¼šä¼´éšç€æ¯”è¾ƒæ˜Žæ˜¾çš„é”™è¯¯ï¼Œç„¶åŽç´§è·Ÿç€å°±æ˜¯æœ‰ä¸€ä¸ª rectified(improved) è¿™ç§ç‰ˆæœ¬ã€‚è¿™ä¸ªç›¸æ¯”ä¹‹å‰å°±æ˜¯ä¿®æ­£äº† å½“è¾“å…¥æ˜¯è´Ÿæ•°çš„æ—¶å€™ï¼Œæ€Žä¹ˆåŠžçš„é—®é¢˜ã€‚ Softmaxè¿™ä¸ªéœ€è¦ä»Žæ•°å­¦çš„è§’åº¦åŽ»ç†è§£ï¼Œæ•°å­¦å…¬å¼å°±æ˜¯ä¸‹é¢ã€‚æœ‰ä¸€ä¸ªçœŸå®žçš„ä¾‹å­ï¼š å¦‚æžœä¸€å¼ â€4â€ çš„å›¾ç‰‡ï¼Œè¾“å‡ºä¸€ä¸ªç½‘ç»œï¼Œæœ€åŽæ˜¯softmax æ¿€æ´»å‡½æ•°ï¼Œé‚£ä¹ˆæœ€åŽå¾—åˆ°çš„ â€œ4â€ çš„æ¦‚çŽ‡æ˜¯æœ€å¤§çš„ï¼Œä½†æ˜¯ä¹Ÿæ˜¯æœ‰å…¶ä»–çš„å¯èƒ½æ€§å­˜åœ¨çš„ï¼Œè¿™ä¸ªå°±æ˜¯softmax çš„æ•ˆæžœã€‚ æœ€ä¸»è¦çš„åŠŸèƒ½æ˜¯ convert number into probabilities. è¿™ç§æ•ˆæžœï¼Œä¸åƒsigmoid é‚£æ ·æœ‰å¾ˆæ˜Žç¡®çš„æ•°å­¦å…¬å¼ã€‚ $$\sigma ( z ) _ { j } = \frac { e ^ { z _ { j } } } { \sum _ { k = 1 } ^ { K } e ^ { z _ { k } } }$$ Softmax function, a wonderfulactivation function that turns numbers aka logits into probabilities that sum to one.Remember the takeaway is: the essential goal of softmax is to turn numbers into probabilities. OptimizerGradient Descentæœ€åŸºæœ¬çš„åº”è¯¥æ˜¯ gradient descentï¼Œè¿™ä¸ªç®—æ˜¯å¼€å±±é¼»ç¥–ã€‚ç»™å‡ºäº†æ¯æ¬¡æ¢¯åº¦å˜åŒ–çš„æ–¹å‘ Momentumä¸ªäººæ›´å–œæ¬¢å°†å…¶ç¿»è¯‘æˆåŠ¨é‡ï¼Œè¿™ä¸ªgradient è€ƒè™‘ä¹‹å‰çš„gradient çš„æ–¹å‘å’Œå¤§å°ï¼Œæœ‰ç‚¹åƒåŠ¨é‡çš„çš„å«ä¹‰ã€‚ä¸ä»…è€ƒè™‘äº†current step, è€Œä¸”accumulates gradient of the past steps. A very popular technique that is used along with SGD is called Momentum. Instead of using only the gradient of the current step to guide the search, momentum also accumulates the gradient of the past steps to determine the direction to go. å¥½å¤„åœ¨äºŽ: most recent is weighted than the less recent onesthe weightage of the most recent previous gradients is more than the less recent ones.for example: RMSpropRMSprop, or Root Mean Square Propogation has an interesting history. RMSProp also tries to dampen the oscillations, but in a different way than momentum. Adamè¿™ç§å¥—è·¯æ€»æ˜¯ä¸€æ ·çš„ï¼Œæ€»æ˜¯æœ‰ä¸€ä¸ªä¸­å’Œè€…ï¼Œå…³é”®æ˜¯å¦‚ä½•æ•´åˆåˆ°ä¸€èµ·ã€‚ Adam or Adaptive Moment Optimization algorithms combines the heuristics of both Momentum and RMSProp. The hyperparameter beta1 is generally kept around 0.9 while beta_2 is kept at 0.99. Epsilon is chosen to be 1e-10 generally. å¯¹äºŽå…¬å¼çš„è§£é‡Šï¼ŒEq 1 and Eq 2æ˜¯come from RMSprop, Eq 3 comes from Adamã€‚æœ€åŽä¸€æ­¥æ˜¯ update æ“ä½œï¼Œæ¡ç†æ¸…æ¥šï¼Œæ²¡æœ‰é—®é¢˜ã€‚(å…¬å¼ä¸­çš„v è¡¨ç¤ºå¯¼æ•°ï¼Œg è¡¨ç¤ºå¯¼æ•°e ä¸å¤ªæ¸…æ¥š)Here, we compute the exponential average of the gradient as well as the squares of the gradient for each parameters (Eq 1, and Eq 2). To decide our learning step, we multiply our learning rate by average of the gradient (as was the case with momentum) and divide it by the root mean square of the exponential average of square of gradients (as was the case with momentum) in equation 3. Then, we add the update.)]]></content>
      <categories>
        <category>Deep learning</category>
      </categories>
      <tags>
        <tag>Loss Function</tag>
        <tag>Activation Function</tag>
        <tag>Optimizer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[siamese network]]></title>
    <url>%2F2019%2F03%2F26%2Fsiamese-network%2F</url>
    <content type="text"><![CDATA[ä¸»è¦æ˜¯ä»‹ç»è‡ªå·±è®ºæ–‡ä¸­çš„ç½‘ç»œç»“æž„ï¼šsiamese networkã€‚ ä½†å‡¡ä¼˜åŒ–ï¼Œæ— éžä¸¤æ¡è·¯ï¼šåœ¨å¥½çš„åŸºç¡€ä¸Šæ›´å¿«ï¼Œåœ¨å¿«çš„åŸºç¡€ä¸Šæ•ˆæžœæ›´å¥½ã€‚ Siamese networkè®­ç»ƒé€Ÿåº¦å¿«ï¼Œæ‰€ä»¥åªæ˜¯éœ€è¦å…¶åœ¨è®­ç»ƒæ•ˆæžœä¸Šæ›´å¥½ã€‚ å…ˆæ¥åˆ†æžä¸€ä¸‹ä¸ºä»€ä¹ˆè®­ç»ƒé€Ÿåº¦å¿«ã€‚é‚£ä¹ˆä¸å¾—ä¸åˆ†æžè¯¥ç½‘ç»œç»“æž„ã€‚æ•´ä¸ªç½‘ç»œçš„è¾“å…¥æ˜¯ (img1, img2, y) è¿™æ ·çš„ä¸‰å…ƒç»„ï¼Œimg è¡¨ç¤ºå›¾ç‰‡ï¼Œyè¡¨ç¤ºlabelã€‚å›¾ç‰‡å¯ä»¥æ˜¯åŒä¸€ç±»åˆ«çš„å›¾ç‰‡ï¼Œä¹Ÿå¯ä»¥æ˜¯ä¸åŒç±»åˆ«çš„ï¼Œyè¡¨ç¤ºä¸¤å¼ å›¾ç‰‡ä¹‹é—´çš„ç›¸ä¼¼ç¨‹åº¦ï¼Œyçš„å–å€¼ (0,1)ï¼Œ0è¡¨ç¤ºç›¸ä¼¼ï¼ˆåŒä¸€ç±»åˆ«ï¼‰ï¼Œ1 è¡¨ç¤ºä¸ç›¸ä¼¼ï¼ˆä¸åŒç±»åˆ«ï¼‰ã€‚å¹¶ä¸”y æ˜¯double ç±»åž‹ï¼Œå±žäºŽ0-1 åŒºé—´ä»»æ„çš„æ•°å­—ã€‚ç½‘è·¯ç»“æž„æœ€åŽçš„è¾“å‡ºæ˜¯ 0-1 åŒºé—´çš„ä»»æ„æ•°å­—ã€‚é€šå¸¸æ˜¯ä»¥0.5 ä½œä¸ºåˆ†ç•Œçº¿ï¼Œå¦‚æžœå°äºŽ0.5 é‚£ä¹ˆè®¤ä¸ºä¸¤ç§å›¾ç‰‡æ˜¯å±žäºŽåŒä¸€ç±»åˆ«ï¼Œæˆ–è€…è¯´æ›´ç›¸ä¼¼ï¼›åä¹‹ä¹Ÿæˆç«‹ã€‚é‡è¦çš„ä¸€ç‚¹æ˜¯ä¸­é—´çš„weights å®žçŽ°äº†æƒå€¼å…±äº«ï¼Œè¿™æ ·å¯ä»¥åŠ å¿«ç½‘ç»œçš„è®­ç»ƒé€Ÿåº¦ã€‚ loss functionè¿™ä¸ªæ˜¯å±žäºŽç»å…¸çš„ contrastive loss functionã€‚å½“y æŽ¥è¿‘äºŽ0çš„æ—¶å€™ï¼Œå³åŠéƒ¨åˆ†æ¶ˆå¤±ï¼Œè¿™ä¸ªæ˜¯è¡¨ç¤ºä¸¤å¼ å›¾ç‰‡å¾ˆæ˜¯ç›¸ä¼¼ï¼Œç„¶åŽå°±ä¸æ–­ä½¿å¾— æ¬§æ°è·ç¦»å‡å°‘ï¼›å½“y æŽ¥è¿‘äºŽ1çš„æ—¶å€™ï¼Œå·¦åŠéƒ¨åˆ†æ¶ˆå¤±ï¼Œè¿™ä¸ªæ—¶å€™ä¸¤å¼ å›¾ç‰‡å¾ˆä¸ç›¸ä¼¼ï¼Œç„¶åŽå³è¾¹å°±æ˜¯ hinge lossã€‚å‚æ•°m ä½œä¸ºä¸€ç§margin æ˜¯æ˜¯å¯ä»¥è°ƒèŠ‚ï¼Œæˆ‘çš„å®žéªŒä¸­ m å–1.æ€»çš„æ€æƒ³ï¼šå°±æ˜¯ä½¿å¾—ç›¸è¿‘çš„å›¾åƒè·ç¦»ç›¸è¿‘ï¼Œä¸æƒ³è¿‘çš„å›¾åƒè·ç¦»å˜è¿œã€‚ $L ( W , ( Y , X _ { 1 } , X _ { 2 } ) ) = ( 1 - Y ) \frac { 1 } { 2 } ( D _ { W } ) ^ { 2 } + ( Y ) \frac { 1 } { 2 } { \max ( 0 , m - D _ { W } ) } ^ { 2 }$ Spectral Normalizationå›¾åƒè¾“å…¥åˆ°ç½‘ç»œä¹‹å‰ä½¿ç”¨æ­£åˆ™åŒ–ï¼Œç„¶åŽè¾“å…¥åˆ°æ¿€æ´»å‡½æ•°ä¹‹å‰ä¹Ÿæ˜¯ä½¿ç”¨æ­£åˆ™åŒ–ï¼Œæ‰€ä»¥è¿™ç§æ•ˆæžœä¹Ÿæ˜¯æ‰©å±•åˆ° weightsï¼Œç›´æŽ¥å¯¹ weights è¿›è¡Œæ­£åˆ™åŒ–ä½¿å…¶ç¬¦åˆ Lipschitz çº¦æŸï¼Œé¿å…ä½¿ç”¨å¤§çš„gradientsã€‚åœ¨GAN ç½‘ç»œä¸­çš„ discriminator æˆ–è€… generator éƒ½å‘çŽ°äº†å…¶å¯ä»¥ç¨³å®šè®­ç»ƒçš„è¿‡ç¨‹ã€‚åœ¨å®žéªŒä¸­ï¼Œæˆ‘ä»¬æ‰©å¤§äº†äº†è¿™ç§ä½¿ç”¨èŒƒå›´ï¼ŒæŠŠå…¶åº”ç”¨åˆ°æ‰€æœ‰çš„ç½‘ç»œçš„layerä¸Šã€‚ self-attention mechanismAttention æœºåˆ¶è‡ªä»Ž â€œAttention Is All You Needâ€ å¼€å§‹ç«çˆ†ï¼Œå¹¶ä¸”å®žéªŒçš„æ•ˆæžœä¹Ÿæ˜¯å¾ˆå¥½çš„ï¼Œç„¶åŽåœ¨å›¾åƒé¢†åŸŸä¹Ÿå¼€å§‹å°è¯•ä½¿ç”¨ attention æœºåˆ¶æ¥è§£å†³é•¿ä¾èµ–çš„é—®é¢˜ã€‚åº”ç”¨åˆ°å›¾åƒé¢†åŸŸä¸»è¦æ˜¯ explore spatial locality information, è¯´ç™½äº†å°±æ˜¯ç»†èŠ‚çš„ä¿¡æ¯ã€‚ If we look at the DCGAN model, we see that regular GANs are heavily based on convolution operations, which use a local receptive field (convolutional kernel) to learn representations. Simple features like edges and corners are learned in the first few layers. Also, ConvNets are able to use these simple representations to learn more complex ones. However, long-range dependency might be hard to learn. Long-range dependency (long-term dependency) is from RNN, which we can say anything larger than trigram as a long term dependency. Thus, most of the image content does not exhibit elaborated shape such as sky or the ocean looks fine. The task of creating geometrically complex forms, such as four-legged animals, is far more challenging. This is where attention comes into play. è€Œ self-attention ä¸­QKV ä¸‰ä¸ªéƒ¨åˆ†æ˜¯ç›¸åŒçš„ï¼Œå¯¹äºŽè¿™ç§å¤„ç†æ–¹æ³•å’ŒRes_block è¿˜æ˜¯æœ‰ç‚¹ç›¸ä¼¼çš„ã€‚ ç»“æžœè®­ç»ƒæ•°æ®é›†ä½¿ç”¨æ˜¯ Cifar-10ï¼Œè®°å½•äº†è®­ç»ƒè¿‡ç¨‹ä¸­ acc å’Œloss çš„å˜åŒ–æƒ…å†µã€‚é™¤äº†è®­ç»ƒçš„æ•ˆæžœæ¯”è¾ƒå¥½å¤–ï¼Œè®­ç»ƒé€Ÿåº¦ä¹Ÿæ˜¯éžå¸¸å¿«çš„ï¼Œå¯ä»¥æ¸…æ¥šçš„çœ‹åˆ°model acc åœ¨æŽ¥è¿‘25 epochesçš„æ—¶å€™å°±å¼€å§‹æ”¶æ•›ã€‚]]></content>
      <categories>
        <category>Deep learning</category>
      </categories>
      <tags>
        <tag>siamese network</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[fastText & faiss]]></title>
    <url>%2F2019%2F03%2F25%2FfastText-faiss%2F</url>
    <content type="text"><![CDATA[fastTextfastTextç»“åˆäº†è‡ªç„¶è¯­è¨€å¤„ç†å’Œæœºå™¨å­¦ä¹ ä¸­æœ€æˆåŠŸçš„ç†å¿µã€‚è¿™äº›åŒ…æ‹¬äº†ä½¿ç”¨è¯è¢‹ä»¥åŠn-gramè¢‹è¡¨å¾è¯­å¥ï¼Œè¿˜æœ‰ä½¿ç”¨å­å­—(subword)ä¿¡æ¯ï¼Œå¹¶é€šè¿‡éšè—è¡¨å¾åœ¨ç±»åˆ«é—´å…±äº«ä¿¡æ¯ã€‚å¦å¤–é‡‡ç”¨äº†ä¸€ä¸ªsoftmaxå±‚çº§(åˆ©ç”¨äº†ç±»åˆ«ä¸å‡è¡¡åˆ†å¸ƒçš„ä¼˜åŠ¿)æ¥åŠ é€Ÿè¿ç®—è¿‡ç¨‹ã€‚ fastText ä¸»è¦æ˜¯ç”¨æ¥è§£å†³ word representationså’Œ sentence classification. æœ‰è¶£çš„æ˜¯å‰è€…æ˜¯æ— ç›‘ç£çš„å­¦ä¹ æ–¹å¼ï¼ŒåŽè€…æ˜¯æœ‰ç›‘ç£çš„å­¦ä¹ æ–¹å¼ã€‚åˆ†åˆ«ä¸»è¦æ¥è‡ª â€Enriching Word Vectors with Subword Informationâ€œ å’Œ â€œBag of Tricks for Efficient Text Classificationâ€ ä¸¤ç¯‡è®ºæ–‡ã€‚å¹¶ä¸”ä½¿ç”¨çš„æ˜¯ shallow neural network è€Œä¸æ˜¯æ·±åº¦ç½‘ç»œã€‚ FastText is a library created by the Facebook Research Team for efficient learning of word representations and sentence classification. Take off:fastText æ–¹æ³•åŒ…å«ä¸‰éƒ¨åˆ†ï¼šæ¨¡åž‹æž¶æž„ã€å±‚æ¬¡ Softmax å’Œ N-gram ç‰¹å¾ã€‚fasttext æœ‰ä¸¤ä¸ªç”¨å¤„ï¼š text classification å’Œ word embedding ã€‚ä½¿ç”¨åœºæ™¯ï¼šå¤§åž‹æ•°æ®ï¼Œé«˜æ•ˆè®¡ç®— ä¸‹é¢è¿›è¡Œç»†è¯´ï¼š æ¨¡åž‹æž¶æž„è¿™ä¸ªæ˜¯æ€»çš„æ¡†æž¶å›¾ã€‚ æŠ±æ­‰å“ˆ è¿™ä¸ªå¼•ç”¨æ‰¾ä¸è§äº†ï¼Œå¦‚æžœæœ‰ä¾µæƒï¼Œplease email me..åˆ†ä¸ºä¸¤ä¸ªéƒ¨åˆ†ä»‹ç»è¿™ä¸ªç½‘ç»œç»“æž„ï¼šä»Žinput -&gt; hidden:è¾“å…¥å±‚è¾“å…¥çš„æ˜¯ä¸€ä¸ªå·²ç»åˆ†è¯åŽçŸ­æ–‡æœ¬ã€‚çŸ­æ–‡æœ¬ä¸­æ¯ä¸ªè¯çš„è¯å‘é‡æ˜¯ç”±è¯¥çŸ­æ–‡æœ¬çš„one-hotçŸ©é˜µä¹˜ä»¥ä¸€ä¸ªåˆå§‹åŒ–çš„çŸ©é˜µwå¾—åˆ°çš„ã€‚ï¼ˆåŽŸç†å›¾ï¼šä¸‹å›¾æ˜¯fasttext è¿è¡Œçš„æ—¶å€™ï¼Œè¿™ä¸ªåˆ†è¯æ˜¯å†å¤„ç†æˆå•è¯å’Œn-gram ç»„æˆçš„ç‰¹å¾ï¼Œè¿™ä¸ªæ˜¯ä¸éœ€è¦æˆ‘ä»¬è¿›è¡Œæ˜¾æ€§çš„æ“ä½œçš„ï¼‰ä»Ž hidden -&gt; outputï¼šæ’æ’­ä¸€å¥ï¼Œæˆ‘ä»¬ç»å¸¸ä½¿ç”¨çš„é¢„è®­ç»ƒæ¨¡åž‹ä¸­çš„weights æ˜¯ä»Žinput-&gt; hiddenã€‚ Hierarchical Softmaxä»Žåå­—ä¸Šå°±çŸ¥é“è¿™ä¸ªæ˜¯åŸºäºŽsoftmaxçš„æ”¹è¿›ç‰ˆæœ¬ï¼Œä¸»è¦æ˜¯è¿ç®—ä¸Šçš„æ”¹è¿›ã€‚å…·ä½“è¯´æ¥ï¼šåˆ©ç”¨äº†ç±»åˆ«ï¼ˆclassï¼‰ä¸å‡è¡¡è¿™ä¸ªäº‹å®žï¼ˆä¸€äº›ç±»åˆ«å‡ºçŽ°æ¬¡æ•°æ¯”å…¶ä»–çš„æ›´å¤šï¼‰ï¼Œé€šè¿‡ä½¿ç”¨ Huffman ç®—æ³•å»ºç«‹ç”¨äºŽè¡¨å¾ç±»åˆ«çš„æ ‘å½¢ç»“æž„ã€‚fastText æ¨¡åž‹ä½¿ç”¨äº†å±‚æ¬¡ Softmax æŠ€å·§ã€‚å±‚æ¬¡ Softmax æŠ€å·§å»ºç«‹åœ¨Huffmançš„åŸºç¡€ä¸Šï¼Œå¯¹æ ‡ç­¾è¿›è¡Œç¼–ç ï¼Œèƒ½å¤Ÿæžå¤§åœ°ç¼©å°æ¨¡åž‹é¢„æµ‹ç›®æ ‡çš„æ•°é‡ã€‚ è¿™ä¸ªæ˜¯softmax çš„åŽŸå§‹çš„è®¡ç®—å…¬å¼ï¼šé‡‡ç”¨äºŒå‰æ ‘çš„ç»“æž„ä¹‹åŽï¼Œæ—¶é—´ä¸Šä¼˜åŒ–ä¸å°‘ã€‚$O ( N) \rightarrow O \left( \log _ { 2 } N \right)$ã€‚è§ä¸‹å›¾ã€‚ æŠ±æ­‰å“ˆ è¿™ä¸ªå¼•ç”¨æ‰¾ä¸è§äº†ï¼Œå¦‚æžœæœ‰ä¾µæƒï¼Œplease email me..å’Œä¹‹å‰çš„ç¥žç»ç½‘ç»œæ¨¡åž‹ç›¸æ¯”ï¼Œè¿™é‡Œçš„huffmaxæ ‘çš„æ‰€æœ‰å†…éƒ¨èŠ‚ç‚¹å°±ç±»ä¼¼ä¹‹å‰ç¥žç»ç½‘ç»œéšè—å±‚çš„ç¥žç»å…ƒã€‚å…¶ä¸­ï¼Œæ ¹èŠ‚ç‚¹çš„è¯å‘é‡å¯¹åº”æˆ‘ä»¬çš„æŠ•å½±åŽçš„è¯å‘é‡ï¼Œè€Œæ‰€æœ‰çš„å¶å­èŠ‚ç‚¹å°±ç±»ä¼¼äºŽä¹‹å‰ç¥žç»ç½‘ç»œsoftmaxè¾“å‡ºå±‚çš„ç¥žç»å…ƒã€‚å¶å­èŠ‚ç‚¹çš„ä¸ªæ•°å°±æ˜¯è¯æ±‡è¡¨çš„å¤§å°. å’Œä¹‹å‰çš„ç›¸æ¯”ï¼Œä»Žéšè—å±‚åˆ°è¾“å‡ºå±‚çš„softmaxæ˜ å°„ä¸æ˜¯ä¸€ä¸‹å°±å®Œæˆçš„ï¼Œè€Œæ˜¯æ²¿ç€ huffmanæ ‘ä¸€æ­¥æ­¥å®Œæˆçš„ï¼Œå› æ­¤è¿™ç§ softmaxå–åä¸ºâ€Hierarchical softmaxâ€. N-gram ç‰¹å¾N-gramæ˜¯åŸºäºŽè¿™æ ·çš„æ€æƒ³ï¼šæŸä¸ªè¯çš„å‡ºçŽ°ä¾èµ–äºŽå…¶ä»–è‹¥å¹²ä¸ªè¯ï¼›æˆ‘ä»¬èŽ·å¾—çš„ä¿¡æ¯è¶Šå¤šï¼Œé¢„æµ‹è¶Šå‡†ç¡®ã€‚æˆ‘æƒ³è¯´ï¼Œæˆ‘ä»¬æ¯ä¸ªäººçš„å¤§è„‘ä¸­éƒ½æœ‰ä¸€ä¸ªN-gramæ¨¡åž‹ï¼Œè€Œä¸”æ˜¯åœ¨ä¸æ–­å®Œå–„å’Œè®­ç»ƒçš„ã€‚æˆ‘ä»¬çš„è§è¯†ä¸Žç»åŽ†ï¼Œéƒ½åœ¨ä¸°å¯Œç€æˆ‘ä»¬çš„é˜…åŽ†ï¼Œå¢žå¼ºç€æˆ‘ä»¬çš„è”æƒ³èƒ½åŠ›ã€‚ N-gramæ¨¡åž‹æ˜¯ä¸€ç§è¯­è¨€æ¨¡åž‹ï¼ˆLanguage Modelï¼ŒLMï¼‰ï¼Œè¯­è¨€æ¨¡åž‹æ˜¯ä¸€ä¸ªåŸºäºŽæ¦‚çŽ‡çš„åˆ¤åˆ«æ¨¡åž‹ï¼Œå®ƒçš„è¾“å…¥æ˜¯ä¸€å¥è¯ï¼ˆå•è¯çš„é¡ºåºåºåˆ—ï¼‰ï¼Œè¾“å‡ºæ˜¯è¿™å¥è¯çš„æ¦‚çŽ‡ï¼Œå³è¿™äº›å•è¯çš„è”åˆæ¦‚çŽ‡ï¼ˆjoint probabilityï¼‰ã€‚ è¿™æ ·çš„ä½œç”¨ï¼Œä½¿ç”¨N-gramæ¥ç»™æ–‡æœ¬æ·»åŠ é¢å¤–çš„ç‰¹å¾èŽ·å¾—å…³äºŽå±€éƒ¨è¯é¡ºåºçš„éƒ¨åˆ†ä¿¡æ¯ã€‚ä¸¾ä¸ªæ —å­ï¼šå¯¹äºŽå¥å­ï¼šâ€œæˆ‘ å–œæ¬¢ å– å’–å•¡â€, å¦‚æžœä¸è€ƒè™‘é¡ºåºï¼Œé‚£ä¹ˆå°±æ˜¯æ¯ä¸ªè¯ï¼Œâ€œæˆ‘â€ï¼Œâ€œå–œæ¬¢â€ï¼Œâ€œå–â€ï¼Œâ€œå’–å•¡â€è¿™äº”ä¸ªå•è¯çš„word embeddingæ±‚å¹³å‡ã€‚å¦‚æžœè€ƒè™‘2-gram, é‚£ä¹ˆé™¤äº†ä»¥ä¸Šäº”ä¸ªè¯ï¼Œè¿˜æœ‰â€œæˆ‘å–œæ¬¢â€ï¼Œâ€œå–œæ¬¢å–â€ï¼Œâ€œå–å’–å•¡â€ç­‰è¯ã€‚â€œæˆ‘å–œæ¬¢â€ï¼Œâ€œå–œæ¬¢å–â€ï¼Œâ€œå–å’–å•¡â€è¿™ä¸‰ä¸ªè¯å°±ä½œä¸ºè¿™ä¸ªå¥å­çš„æ–‡æœ¬ç‰¹å¾ã€‚æˆ‘ä»¬ç»å¸¸è§åˆ°çš„åœºæ™¯ï¼šè¾“å…¥æ³•çš„é¢„é€‰è¯æ±‡ã€‚å°±æ˜¯å¯ä»¥é€šè¿‡è¿™ç§æ–¹å¼å®žçŽ°çš„ã€‚ å½“ç„¶ä½¿ç”¨äº†æ›´å¤šçš„ç‰¹å¾æ„å‘³ç€é€ æˆäº†æ•ˆçŽ‡ä¸‹é™ï¼ŒäºŽæ˜¯è¯¥ä½œè€…æå‡ºäº†ä¸¤ç§è§£å†³æ–¹æ³•ï¼šè¿‡æ»¤æŽ‰ä½Žè¯é¢‘ï¼›ä½¿ç”¨è¯ç²’åº¦ä»£æ›¿å­—ç²’åº¦ã€‚æ¯”å¦‚è¯´æµ·æ…§å¯ºä½¿ç”¨ä¸Šé¢é‚£ä¸ªå¥å­â€æˆ‘å–œæ¬¢å–å’–å•¡â€œï¼Œå¦‚æžœä½¿ç”¨å­ç²’åº¦çš„2-gramï¼Œé‚£ä¹ˆäº§ç”Ÿçš„ç‰¹å¾æ˜¯â€œæˆ‘å–œâ€ï¼Œâ€œå–œæ¬¢â€ï¼Œâ€œæ¬¢å–â€ï¼Œâ€œå–å’–â€ï¼Œâ€œå’–å•¡â€ã€‚å¦‚æžœä½¿ç”¨è¯ç²’åº¦ä¸º2-gramï¼Œé‚£ä¹ˆäº§ç”Ÿçš„ç‰¹å¾æ˜¯â€œæˆ‘å–œæ¬¢â€ï¼Œâ€œå–œæ¬¢å–â€ï¼Œâ€œå–å’–å•¡â€ã€‚ è¡¥å……ä¸€å¥ï¼Œsubwordså°±æ˜¯ä¸€ä¸ªè¯çš„character-levelçš„n-gramã€‚æ¯”å¦‚å•è¯â€helloâ€ï¼Œé•¿åº¦è‡³å°‘ä¸º3çš„char-levelçš„ngramæœ‰â€helâ€,â€ellâ€,â€lloâ€,â€hellâ€,â€elloâ€ä»¥åŠæœ¬èº«â€helloâ€ã€‚n-gram æ˜¯ä¸€ç§æ€æƒ³ï¼Œå¯ä»¥æ˜¯é’ˆå¯¹words ä¹‹é—´çš„ï¼Œä¹Ÿæ˜¯å¯ä»¥é’ˆå¯¹ä¸€ä¸ªword å†…éƒ¨çš„ã€‚å‰è€…å°±æ˜¯å€™é€‰è¯ï¼Œå“ªäº›è¯è¯­å®¹æ˜“ç»„åˆå‡ºçŽ°ï¼ŒåŽè€…ä¸»è¦æ˜¯å¯¹äºŽå•è¯æœ¬èº«çš„ä¼¸å±•ï¼Œå¯ä»¥æŠŠæ²¡æœ‰åœ¨dict ä¸­çš„å•è¯ï¼Œä½¿ç”¨å­—è¯ï¼ˆsubwordï¼‰ è¿›è¡Œè¡¨ç¤ºï¼Œæ‰©å……äº†æ¨¡åž‹çš„è¡¨è¾¾èƒ½åŠ›ã€‚ Negative Samplingä¸»è¦æ˜¯å‡è½»è®¡ç®—é‡çš„è§’åº¦è€ƒè™‘çš„ï¼Œæ¯æ¬¡è®©ä¸€ä¸ªè®­ç»ƒæ ·æœ¬ä»…ä»…æ›´æ–°ä¸€éƒ¨åˆ†çš„æƒé‡å‚æ•°ï¼Œè¿™ä¸ªæŠ€æœ¯ä¸æ˜¯ fastText é¦–åˆ›çš„ï¼Œä½†æ˜¯æœ¬ç€æ€»ç»“çŸ¥è¯†ç‚¹çš„ï¼Œä¹Ÿå°±æ”¾åœ¨è¿™é‡Œäº†ã€‚CBOW / Skip-gramæ¨¡åž‹ ï¼ˆè¿™ä¸ªè®ºæ–‡ä¸­ï¼‰æå‡ºäº†ä¸¤ç§æ–¹æ³•ï¼Œä¸€ç§æ˜¯Hierarchical Softmaxï¼Œå¦ä¸€ç§æ˜¯Negative Samplingã€‚è®ºæ–‡ä¸­æå‡ºçš„ä¸¤ç§æ–¹æ³•éƒ½æ˜¯ç”¨æ¥æé«˜è®¡ç®—æ•ˆçŽ‡çš„ï¼Œä¸‹é¢è¯´ä¸€ä¸‹è´Ÿé‡‡æ ·ã€‚åœ¨è®­ç»ƒç¥žç»ç½‘ç»œæ—¶ï¼Œæ¯å½“æŽ¥å—ä¸€ä¸ªè®­ç»ƒæ ·æœ¬ï¼Œç„¶åŽè°ƒæ•´æ‰€æœ‰ç¥žç»å•å…ƒæƒé‡å‚æ•°ï¼Œæ¥ä½¿ç¥žç»ç½‘ç»œé¢„æµ‹æ›´åŠ å‡†ç¡®ã€‚æ¢å¥è¯è¯´ï¼Œæ¯ä¸ªè®­ç»ƒæ ·æœ¬éƒ½å°†ä¼šè°ƒæ•´æ‰€æœ‰ç¥žç»ç½‘ç»œä¸­çš„å‚æ•°ã€‚è€Œ Negative Sampling æ¯æ¬¡è®©ä¸€ä¸ªè®­ç»ƒæ ·æœ¬ä»…ä»…æ›´æ–°ä¸€å°éƒ¨åˆ†çš„æƒé‡å‚æ•°ï¼Œä»Žè€Œé™ä½Žæ¢¯åº¦ä¸‹é™è¿‡ç¨‹ä¸­çš„è®¡ç®—é‡ã€‚å¦‚æžœ vocabulary å¤§å°ä¸º1ä¸‡æ—¶ï¼Œ å½“è¾“å…¥æ ·æœ¬ ( â€œfoxâ€, â€œquickâ€) åˆ°ç¥žç»ç½‘ç»œæ—¶ï¼Œ â€œ foxâ€ ç»è¿‡ one-hot ç¼–ç ï¼Œåœ¨è¾“å‡ºå±‚æˆ‘ä»¬æœŸæœ›å¯¹åº” â€œquickâ€ å•è¯çš„é‚£ä¸ªç¥žç»å…ƒç»“ç‚¹è¾“å‡º 1ï¼Œå…¶ä½™ 9999 ä¸ªéƒ½åº”è¯¥è¾“å‡º0ã€‚åœ¨è¿™é‡Œï¼Œè¿™9999ä¸ªæˆ‘ä»¬æœŸæœ›è¾“å‡ºä¸º0çš„ç¥žç»å…ƒç»“ç‚¹æ‰€å¯¹åº”çš„å•è¯æˆ‘ä»¬ç§°ä¸º negative wordï¼Œéšæœºé€‰æ‹©ä¸€å°éƒ¨åˆ†çš„ negative wordsï¼Œæ¯”å¦‚é€‰ 10ä¸ª negative words æ¥æ›´æ–°å¯¹åº”çš„æƒé‡å‚æ•°ã€‚è§£å†³çš„é—®é¢˜ï¼Œåœ¨æœ€åŽä¸€å±‚ softmax çš„è®¡ç®—é‡å¤ªå¤§ï¼Œç›¸å½“äºŽæ¯ä¸€æ¬¡word éƒ½æ˜¯éœ€è¦æ•´ä¸ªdict é‡çš„çº§åˆ«çš„æ›´æ–°ã€‚ç„¶åŽé€‰æ‹© k ä¸ªnegative wordsï¼Œåªæ˜¯è®¡ç®—è¿™äº›softmax çš„å€¼ã€‚ Training a neural network means taking a training example and adjusting all of the neuron weights slightly so that it predicts that training sample more accurately. In other words, each training sample will tweak all of the weights in the neural network.As we discussed above, the size of our word vocabulary means that our skip-gram neural network has a tremendous number of weights, all of which would be updated slightly by every one of our billions of training samples!Negative sampling addresses this by having each training sample only modify a small percentage of the weights, rather than all of them. Hereâ€™s how it works.When training the network on the word pair (â€œfoxâ€, â€œquickâ€), recall that the â€œlabelâ€ or â€œcorrect outputâ€ of the network is a one-hot vector. That is, for the output neuron corresponding to â€œquickâ€ to output a 1, and for all of the other thousands of output neurons to output a 0.With negative sampling, we are instead going to randomly select just a small number of â€œnegativeâ€ words (letâ€™s say 5) to update the weights for. (In this context, a â€œnegativeâ€ word is one for which we want the network to output a 0 for). We will also still update the weights for our â€œpositiveâ€ word (which is the word â€œquickâ€ in our current example).The paper says that selecting 5-20 words works well for smaller datasets, and you can get away with only 2-5 words for large datasets.Recall that the output layer of our model has a weight matrix thatâ€™s 300 x 10,000. So we will just be updating the weights for our positive word (â€œquickâ€), plus the weights for 5 other words that we want to output 0. Thatâ€™s a total of 6 output neurons, and 1,800 weight values total. Thatâ€™s only 0.06% of the 3M weights in the output layer!In the hidden layer, only the weights for the input word are updated (this is true whether youâ€™re using Negative Sampling or not). Positive samples and Negative samplesOne little detail thatâ€™s missing from the description above is how do we select the negative samples.The negative samples are chosen using the unigram distribution. Essentially, the probability of selecting a word as a negative sample is related to its frequency, with more frequent words being more likely to be selected as negative samples. Instead of using the raw frequency in the original word2vec paper, each word is given a weight thatâ€™s equal to itâ€™s frequency (word count) raised to the 3/4 power. The probability for selecting a word is just itâ€™s weight divided by the sum of weights for all words.$$P \left( w _ { i } \right) = \frac { f \left( w _ { i } \right) ^ { 3 / 4 } } { \sum _ { i = 0 } ^ { n } \left( f \left( w _ { j } \right) ^ { 3 / 4 } \right) }$$This decision to raise the frequency to the 3/4 power appears to be empirical; as the author claims it outperformed other functions (e.g. just using unigram distribution).Side note: The way this selection is implemented in the original word2vec C code is interesting. They have a large array with 100M elements (which they refer to as the unigram table). They fill this table with the index of each word in the vocabulary multiple times, and the number of times a wordâ€™s index appears in the table is given by Then, to actually select a negative sample, we just generate a random integer between 0 and 100M, and use the word at that index in the table. Since the higher probability words occur more times in the table, weâ€™re more likely to pick those. è¿™ä¸ªä¹Ÿæ˜¯æœ‰è®² ä»»ä½•è¿›è¡Œnegative sampleçš„é€‰æ‹©http://jalammar.github.io/illustrated-word2vec/ä¸€èˆ¬æ¥è¯´åœ¨ word2vec ä¸­context æ˜¯ä¼šé€‰æ‹©åˆ° 5ï¼Œç„¶åŽè¿™ä¸ª positive / negative sample ä¼šæ˜¯(1/6), ç„¶åŽ nagative sample æ˜¯éšæœºåœ¨ dictionaryé‡Œé¢é€‰çš„ï¼ˆæ‰€ä»¥æœ‰å¯èƒ½é€‰åˆ° positive sampleï¼‰ï¼Œ è¿™ä¸ªæ˜¯è¿™ä¸ªdictionary æ˜¯æ ¹æ®é¢‘çŽ‡ï¼Œå‡ºçŽ°æ¬¡æ•°è¶Šå¤šçš„ï¼Œè¢«é€‰ä¸­çš„å¯èƒ½æ€§ä¹Ÿè¶Šå¤§ã€‚The number of negative samples is another factor of the training process. The original paper prescribes 5-20 as being a good number of negative samples. It also states that 2-5 seems to be enough when you have a large enough dataset. The Gensim default is 5 negative samples. To address this, we need to introduce negative samples to our dataset â€“ samples of words that are not neighbors. Our model needs to return 0 for those samples. Now thatâ€™s a challenge that the model has to work hard to solve â€“ but still at blazing fast speed.This idea is inspired by Noise-contrastive estimation [pdf]. We are contrasting the actual signal (positive examples of neighboring words) with noise (randomly selected words that are not neighbors). This leads to a great tradeoff of computational and statistical efficiency. Essentially, the probability for selecting a word as a negative sample is related to its frequency, with more frequent words being more likely to be selected as negative samples. They fill this table with the index of each word in the vocabulary multiple times, and the number of times a wordâ€™s index appears in the table is given byP(wi)P(wi)* table_size. Then, to actually select a negative sample, you just generate a random integer between 0 and 100M, and use the word at that index in the table. Since the higher probability words occur more times in the table, youâ€™re more likely to pick those. Itâ€™s now time to build out our skip-gram generator which will give us pair of words and their relevance (word, word in the same window), with label 1 (positive samples). (word, random word from the vocabulary), with label 0 (negative samples). ä½¿ç”¨ç¬¬ä¸€ä¸ªåº”ç”¨åœºæ™¯ï¼šè¯å‘é‡ã€‚fastTextä½œä¸ºè®­ç»ƒè¯å‘é‡è®¤ä¸ºå¯ä»¥æœ‰ä¸¤ç§æ¨¡å¼ï¼Œä¸€ç§æ˜¯æ ¹æ®å‘¨å›´è¯è¯­é¢„æµ‹ä¸­å¿ƒè¯æ±‡çš„CBOW ï¼ˆcontinuous bag-of-wordsï¼‰æ¨¡åž‹ï¼Œå¦ä¸€ç§æ˜¯æ ¹æ®ä¸­å¿ƒè¯æ±‡é¢„æµ‹ä¸Šä¸‹æ–‡çš„ skip-gram æ¨¡åž‹ã€‚ ./fasttext â€“ It is used to invoke the FastText library. skipgram/cbow â€“ It is where you specify whether skipgram or cbow is to be used to create the word representations. -input â€“ This is the name of the parameter which specifies the following word to be used as the name of the file used for training. This argument should be used as is. data.txt â€“ a sample text file over which we wish to train the skipgram or cbow model. Change this name to the name of the text file you have. -output â€“ This is the name of the parameter which specifies the following word to be used as the name of the model being created. This argument is to be used as is. model â€“ This is the name of the model created.Running the above command will create two files named model.bin and model.vec. model.bin contains the model parameters, dictionary and the hyperparameters and can be used to compute word vectors. model.vec is a text file that contains the word vectors for one word per line. æœ€åŽç”Ÿæˆæœ‰ä¸¤ä¸ªæ–‡ä»¶ï¼Œä¸€ä¸ª xxx.bin æ–‡ä»¶ï¼Œä¸€ä¸ªæ˜¯ xxx.vec æ–‡ä»¶ï¼Œå‰è€…æ˜¯é¢„è®­ç»ƒæ¨¡åž‹ï¼ŒåŽè€…æ˜¯è¯å‘é‡ã€‚è¿™ä¸¤ä¸ªå¯èƒ½æ˜¯æœ€é‡è¦çš„æ ¼å¼äº†ã€‚ The most important parameters of the model are its dimension and the range of size for the subwords. å¸¸è§çš„ä»£ç æ ¼å¼ï¼š ./fasttext skipgram -input data/fil9 -output result/fil9 -minn 2 -maxn 5 -dim 300 è·‘åä¸€ä¸‹è¯´ä¸€ä¸‹shellçš„å°æŠ€å·§ã€‚ä½¿ç”¨echo æˆ–è€… &lt; è¿™æ ·è¿›è¡Œå•ä¸ªè¯æˆ–è€…å¤šä¸ªè¯çš„è¯å‘é‡çš„æŸ¥è¯¢ã€‚ ./fasttext print-word-vectors model.bin &lt; queries.txtecho â€œwordâ€ | ./fasttext print-word-vectors model.bin Finding simialr words: ./fasttext nn model.bin ç¬¬äºŒä¸ªåº”ç”¨åœºæ™¯ï¼šæ–‡æœ¬åˆ†ç±»ã€‚ Sentiment analysis and email classification are classic examples of text classification åœ¨è®­ç»ƒæ•°æ®é›†ä¸­label é»˜è®¤æ˜¯ä½¿ç”¨ â€œ__label__â€ è¿›è¡Œè¡¨ç¤ºçš„ï¼Œå½“ç„¶ä¹Ÿæ˜¯å¯ä»¥è¿›è¡Œè‡ªå®šä¹‰çš„ã€‚ ./fasttext supervised -input train.ft.txt -output model_kaggle -label __label__ -lr 0.5 å°±æ˜¯è¿›è¡Œpredictçš„æ—¶å€™ï¼Œæœ‰æ—¶å€™å¹¶ä¸æ˜¯å¾ˆèƒ½æƒ³èµ·æ¥åªæ˜¯predict top 3 è¿™æ ·çš„ä¸œè¥¿ã€‚ # Predicting on the test dataset ./fasttext predict model_kaggle.bin test.ft.txt # Predicting the top 3 labels ./fasttext predict model_kaggle.bin test.ft.txt 3 faissç”¨é€”ï¼šç›¸ä¼¼åº¦æ£€æµ‹å’Œç¨ å¯†å‘é‡çš„èšç±»ã€‚ Faiss is a library for efficient similarity search and clustering of dense vectors. ä¹‹å‰çš„å®žä¹ ç»åŽ†ä¸»è¦æ˜¯ç”¨faiss å¤„ç†æ–‡æœ¬çš„representationï¼Œä½†æ˜¯è¿™ä¸ªæ˜¯æœ‰åå·®çš„ï¼Œå‡¡æ˜¯èƒ½å¤Ÿæ‰“æˆè¯å‘é‡ï¼Œéƒ½æ˜¯å¯ä»¥ä½¿ç”¨faiss è¿›è¡Œè®¡ç®—çš„ï¼Œå½“ç„¶è¿™è¯å‘é‡éœ€è¦æ»¡è¶³ï¼šç›¸è¿‘å†…å®¹åœ¨ç›¸è¿‘çš„ç©ºé—´ã€‚ Once the vectors are extracted by learning machinery (from images, videos, text documents, and elsewhere), theyâ€™re ready to feed into the similarity search library. faissçš„å®žçŽ°è¿‡ç¨‹é¦–å…ˆä½¿ç”¨ indexå¯¹äºŽå‘é‡è¿›è¡Œé¢„å¤„ç†ï¼Œç„¶åŽé€‰æ‹©ä¸åŒçš„æ¨¡å¼ã€‚ ç‰ºç‰²äº†ä¸€äº›ç²¾ç¡®æ€§æ¥ä½¿å¾—è¿è¡Œé€Ÿåº¦æ›´å¿«ã€‚ Similarity search can be made orders of magnitude faster if weâ€™re willing to trade some accuracy; that is, deviate a bit from the reference result. For example, it may not matter much if the first and second results of an image similarity search are swapped, since theyâ€™re probably both correct results for a given query. Accelerating the search involves some pre-processing of the data set, an operation that we call indexing. å‘é‡çš„æ¯”è¾ƒæœ‰ä¸¤ç§metricï¼šä¸€ç§æ˜¯L2 ä¸€ç§æ˜¯åŸºäºŽconsine (ç‚¹ä¹˜)è¿›è¡Œæ£€ç´¢ã€‚å‰è€…æ˜¯æ±‚è§£æœ€å°çš„å€¼ï¼ŒåŽè€…æ˜¯é€šè¿‡innerâ€”â€”product æ±‚è§£maximum. å¹¶ä¸”æ˜¯æ”¯æŒGPUçš„ï¼Œåœ¨åŽŸæ¥CPUä¸Šå»ºç«‹çš„indexï¼Œç„¶åŽå¾ˆå¥½çš„è¿ç§»åˆ° GPUä¸Šã€‚ åœ¨æ¶‰åŠindexä½¿ç”¨è€ƒè™‘é€Ÿåº¦ï¼Œaccå’Œå†…å­˜å¤§å°ä¸‰ä¸ªä¸åŒçš„ç»´åº¦ã€‚ç„¶åŽä¸åŒçš„index æ˜¯æœ‰ä¸åŒçš„ä¾§é‡çš„ã€‚]]></content>
      <categories>
        <category>NLP</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[NLPä¸­çš„ç¢Žç¢Žå¿µ]]></title>
    <url>%2F2019%2F03%2F25%2FNLP%E4%B8%AD%E7%9A%84%E7%A2%8E%E7%A2%8E%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[ä¸»è¦ä»‹ç»å…³é”®è¯æå–å¹¶æ•´ç†ä¸€ä¸‹NLPç›¸å…³çš„åŸºç¡€çŸ¥è¯†ç‚¹ã€‚ å…³é”®è¯æå–TF-IDFè¿™ä¸ªæ˜¯å¯ä»¥å‚çœ‹ä¹‹å‰è‡ªå·±å†™çš„ä¸€ä¸ªåšå®¢ å¡æ–¹åˆ†å¸ƒå¡æ–¹æ£€éªŒæ˜¯ä»¥Ï‡2åˆ†å¸ƒä¸ºåŸºç¡€çš„ä¸€ç§å¸¸ç”¨å‡è®¾æ£€éªŒæ–¹æ³•ï¼Œå®ƒçš„æ— æ•ˆå‡è®¾H0æ˜¯ï¼šè§‚å¯Ÿé¢‘æ•°ä¸ŽæœŸæœ›é¢‘æ•°æ²¡æœ‰å·®åˆ«ã€‚è¯¥æ£€éªŒçš„åŸºæœ¬æ€æƒ³æ˜¯ï¼šé¦–å…ˆå‡è®¾H0æˆç«‹ï¼ŒåŸºäºŽæ­¤å‰æè®¡ç®—å‡ºÏ‡2å€¼ï¼Œå®ƒè¡¨ç¤ºè§‚å¯Ÿå€¼ä¸Žç†è®ºå€¼ä¹‹é—´çš„åç¦»ç¨‹åº¦ã€‚è€Œå®žé™…åº”ç”¨åˆ°ç‰¹å¾é€‰æ‹©ä¸­çš„æ—¶å€™ï¼Œæˆ‘ä»¬ä¸éœ€è¦çŸ¥é“è‡ªç”±åº¦ï¼Œä¸è¦çŸ¥é“å¡æ–¹åˆ†å¸ƒï¼Œæˆ‘ä»¬åªéœ€è¦æ ¹æ®ç®—å‡ºæ¥çš„Ï‡2 è¿›è¡ŒæŽ’åºå°±å¥½äº†ï¼Œè¶Šå¤§æˆ‘ä»¬å°±è¶Šå–œæ¬¢ï¼æŒ‘é€‰æœ€å¤§çš„ä¸€å †ï¼ŒäºŽæ˜¯å°±å®Œæˆäº†åˆ©ç”¨å¡æ–¹æ£€éªŒæ¥è¿›è¡Œç‰¹å¾æå–ã€‚å¡æ–¹åˆ†å¸ƒçš„ç¼ºç‚¹ï¼šæ²¡æœ‰è€ƒè™‘è¯é¢‘ Textrankæœ‰ä¸€ä¸ªä¸Žä¹‹å¾ˆåƒçš„æ¦‚å¿µ pageRankingï¼Œæœ€å¼€å§‹æ˜¯ç”¨æ¥è®¡ç®—ç½‘é¡µçš„é‡è¦æ€§ã€‚Textrank ä¸»è¦ç”¨æ¥æå–æ–‡ç« çš„å…³é”®è¯ï¼Œç„¶åŽæ¯”è¾ƒé€‚åˆé•¿æ–‡æœ¬ã€‚ CBOWå’Œskip-gramä¸¾ä¸€ä¸ªç®€å•çš„å°ä¾‹å­è¯´æ˜Ž CBOWå’Œskip-gramçš„åŒºåˆ«ï¼šskip-gram æ˜¯æ ¹æ®ä¸­å¿ƒè¯æ±‡ç„¶åŽé¢„æµ‹ä¸Šä¸‹æ–‡è¯æ±‡ï¼Œè¿™ä¸ªä¸æ˜¯ä¸€ä¸‹å­è¾“å…¥ä¸Šä¸‹æ–‡è¯æ±‡çš„ï¼Œè€Œæ˜¯ä¸€ä¸ªè¿‡ç¨‹ï¼Œä¸­å¿ƒè¯æ±‡å’Œä¸Šä¸‹æ–‡è¯æ±‡1 ï¼Œä¸­å¿ƒè¯æ±‡å’Œä¸Šä¸‹æ–‡è¯æ±‡2ï¼Œè¿™æ ·çš„è¿›è¡Œè¾“å…¥ã€‚ cbow å’Œå…¶çš„åŒºåˆ«ï¼Œåœ¨äºŽç®€å•ç›¸åŠ äº†ä¸Šä¸‹æ–‡è¯æ±‡ä½œä¸ºä¸€ä¸ªæ•´ä½“ï¼Œç„¶åŽå’Œä¸­å¿ƒè¯æ±‡è¿›è¡Œè¾“å…¥ï¼Œæ‰€ä»¥æœ€åŽæ˜¯è¿™æ ·çš„ç»“æžœã€‚ ä½¿ç”¨skip gramè®­ç»ƒçš„æ—¶é—´æ›´é•¿ï¼Œä½†æ˜¯å¯¹äºŽå‡ºçŽ°é¢‘çŽ‡ä¸é«˜çš„è¯æ±‡ï¼Œæ•ˆæžœæ¯”è¾ƒå¥½ã€‚ä½†CBOWçš„è®­ç»ƒé€Ÿåº¦æ˜¯ç›¸å¯¹æ¥è¯´æ¯”è¾ƒå¿«ä¸€äº›ã€‚]]></content>
      <categories>
        <category>NLP</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Pythonä¸­çš„å¤šçº¿ç¨‹å’Œå¤šè¿›ç¨‹]]></title>
    <url>%2F2019%2F03%2F25%2FPython%E4%B8%AD%E7%9A%84%E5%A4%9A%E7%BA%BF%E7%A8%8B%E5%92%8C%E5%A4%9A%E8%BF%9B%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[å¤šçº¿ç¨‹å’Œå¤šè¿›ç¨‹é—®é¢˜æ˜¯å¯ä»¥å¯¹åº”åˆ° å¹¶å‘ ï¼ˆcncurrencyï¼‰å’Œå¹¶è¡Œ(parallelism)ä¸Šçš„ã€‚ å¹¶å‘ï¼Œå°±æ˜¯ä¸€ä¸ªå•æ ¸cpuåŒæ—¶å¼€å§‹äº†å¤šä¸ªä»»åŠ¡ï¼Œä½†æ˜¯è¿™ä¸ªä»»åŠ¡å¹¶ä¸æ˜¯åŒæ—¶ç‹¬ç«‹è¿›è¡Œçš„ï¼Œè€Œæ˜¯é€šè¿‡cpuçš„ä¸æ–­åˆ‡æ¢ï¼Œä¿å­˜çŽ°åœºï¼Œç„¶åŽé‡å¯è¿™æ ·çš„å¿«é€Ÿçš„åˆ‡æ¢ï¼Œç»™ç”¨æˆ·çš„æ„Ÿè§‰æ˜¯å¹¶å‘ï¼Œä½†æ˜¯å®žé™…ä¸Šæ˜¯cpuçš„è®¡ç®—èƒ½åŠ›å—åˆ°äº†é™åˆ¶ï¼Œç”¨æˆ·ä½“éªŒæ¯”è¾ƒå¥½ä¸€äº›ã€‚å¦‚æžœåœ¨å¤šæ ¸cpu ï¼ˆæ¯”å¦‚æˆ‘çš„mac æ˜¯ä¸€cpu 6æ ¸ï¼‰è¿™æ ·çš„è¯å®Œå…¨æ˜¯å¯ä»¥è¾¾åˆ°å¹¶è¡Œçš„ï¼Œè¿™ä¸ªæ˜¯çœŸæ­£çš„ç‹¬ç«‹æ“ä½œ(parallelism)ï¼Œå¯¹åº”çš„æ˜¯å¤šè¿›ç¨‹çš„ã€‚å¯¹åº”python ä¸­çš„å®žçŽ°å¤šçº¿ç¨‹æ˜¯ä½¿ç”¨threadingï¼Œå¤„ç†çš„æ˜¯io å“åº”ï¼›å¤šè¿›ç¨‹æ˜¯Concurrencyï¼Œä½¿ç”¨multiprocessingåŒ…ï¼Œå¤„ç†çš„æ˜¯å¤šæ ¸cpuçš„æ“ä½œã€‚ Take off: å¦‚æžœå¤„ç†io å“åº”ï¼Œé‚£ä¹ˆä½¿ç”¨å¤šçº¿ç¨‹ï¼›å¦‚æžœæ˜¯è®¡ç®—ï¼Œé‚£ä¹ˆä½¿ç”¨å¤šè¿›ç¨‹ã€‚ So, before we go deeper into the multiprocessing module, itâ€™s worthwhile ensuring you know the advantages of using multiprocessing over multithreading. The general rule of thumb is that, if you are trying to improve the performance of CPU-bound tasks, multiprocessing is what you want to use. However, if your particular task is Input/Output bound, then youâ€™ll generally want to use multithreading to improve the performance of your applications. ä¸‹é¢æ˜¯å¤šçº¿ç¨‹çš„demoã€‚ import multiprocessing as mp def my_func(x): print(mp.current_process()) return x ** x def main(): pool = mp.Pool(mp.cpu_count()) # è¿™ä¸ªè¿˜æ˜¯å¾ˆå¥½çš„ pool è¿™ä¸ªçš„ä¸ªæ•°å’Œä½ çš„cpu count æ˜¯ä¿æŒä¸€è‡´çš„ result = pool.map(my_func, [4, 2, 3, 5, 3, 2, 1, 2]) result_set_2 = pool.map(my_func, [4, 6, 5, 4, 6, 3, 23, 4, 6]) print(result) print(result_set_2) if __name__ == &quot;__main__&quot;: main() è¿™ä¸ªæ˜¯å¤šè¿›ç¨‹çš„demoã€‚ import threading class Worker(threading.Thread): # Our workers constructor, note the super() method which is vital if we want this # to function properly def __init__(self): super(Worker, self).__init__() def run(self): for i in range(10): print(i) def main(): thread1 = Worker() thread1.start() thread2 = Worker() thread2.start() thread3 = Worker() thread3.start() thread4 = Worker() thread4.start() if __name__ == &quot;__main__&quot;: main()]]></content>
      <categories>
        <category>Python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Data Pre-processing å­¦ä¹ ç¬”è®°]]></title>
    <url>%2F2019%2F03%2F25%2FData-Pre-processing%2F</url>
    <content type="text"><![CDATA[Data Cleaning:è¿™ä¸ªæ­¥éª¤ä¸»è¦å¤„ç† missing values å’Œ noisy data (outlier).å¯¹äºŽmissing values ï¼Œå¯ä»¥åˆ†æˆä¸¤ä¸ªé—®é¢˜ï¼Œè¦ä¸è¦å¤„ç† å’Œå¦‚ä½•å¤„ç†ï¼Œå…·ä½“è¯´æ¥æœ‰ä¸€ä¸‹å¤„ç†æ‰‹æ®µï¼š ignore the tuple; fill in the missing value manually use a global constant to fill in the missing value use the attribute mean to fill in the missing value (å‡å€¼) use the most probable value to fill in the missing value (mode ä¼—æ•°) æœ‰æ—¶å€™å°±æ˜¯æ ¹æ®æŸå‡ ä¸ªç‰¹å¾ç„¶åŽå¼„ä¸€ä¸ªç®€å•çš„å›žå½’æ¨¡åž‹ï¼Œæ ¹æ®æ¨¡åž‹è¿›è¡Œpredict å…³äºŽè¿™å‡ ç§æ–¹æ³•å¦‚ä½•åŽ»é€‰æ‹©ï¼Œæˆ‘å¦‚æžœè¯´ â€œit dependsâ€ï¼Œé‚£ä¹ˆå…¶ä»–äººä¸è®¤ä¸ºè¿™æ˜¯ä¸€ä¸ªå…·æœ‰è¯´æœåŠ›çš„ç­”æ¡ˆï¼Œä»–ä»¬æ›´åƒçŸ¥é“ it depends what, and when and why to use specific method? æˆ‘è®¤ä¸ºåº”è¯¥æ˜¯æ ¹æ®ç¼ºçœå€¼ç¨‹åº¦å’Œé‡è¦æ€§è¿›è¡Œç»éªŒæ€§çš„é€‰æ‹©ï¼Œè¿™ä¹ŸåŽ»å°±æ˜¯ empirical studyå§ã€‚ æŽ¥ç€æ˜¯ noisy data (outlier)ï¼Œæˆ‘çš„è§‚ç‚¹æ˜¯é¦–å…ˆå¾—è®¤è¯†åˆ°è¿™ä¸ªæ˜¯é”™è¯¯çš„æ•°æ®ï¼Œä¸æ˜¯çœŸå®žçš„æ•°æ®æ¥æºï¼Œå¯èƒ½æ˜¯æ¥è‡ªäººä¸ºçš„ç¬”è¯¯ æˆ–è€…ä»ªå™¨è®°å½•çš„é—®é¢˜ï¼Œè¿™ä¸ªæ˜¯éœ€è¦ä¿®æ”¹çš„ã€‚å¯ä»¥ä½¿ç”¨èšç±» (clustering) è¿›è¡Œnoisy data çš„æ£€æµ‹ï¼Œæ‰¾åˆ°ä¹‹åŽè¿™ä¸ªå°±ç±»ä¼¼ missing valueäº†ï¼Œå¯ä»¥é‡‡å–ä»¥ä¸Šçš„æ‰‹æ®µè¿›è¡Œæ“ä½œï¼Œåº”è¯¥æ³¨æ„åˆ°çš„è¿™ä¸ª noisy data æ‰€å æ¯”ä¾‹ä¸ä¼šå¾ˆé«˜ï¼Œå¦åˆ™å°±æˆäº†ä¸»è¦çš„æ•°æ®åˆ†å¸ƒäº†ã€‚ Data Integration:å¤„ç†æ•°æ®åº“æ•°æ®ï¼Œç»å¸¸æ˜¯éœ€è¦å¤„ç†å­è¡¨ä¿¡æ¯çš„ï¼Œé‚£ä¹ˆå¿…ç„¶å­˜åœ¨ç€ä¸»è¡¨ï¼Œè€Œå­è¡¨ç³»ä¿¡æ¯å¾€å¾€æ˜¯ä¸»è¡¨ä¿¡æ¯çš„æŸä¸€æ–¹é¢çš„ç»†åŒ–ã€‚æ‰€ä»¥æœ‰å¿…è¦å°†ä¸¤è€…è¿žæŽ¥èµ·æ¥ã€‚ Data Transformation:In data transformation, the data are transformed or consolidated into forms appropriate for mining.è¿™é‡Œæƒ³è¦æ¾„æ¸…çš„æ˜¯å¾ˆå¤šç›¸åŒçš„å†…å®¹éƒ½å¯ä»¥ç”¨ä¸åŒçš„æ–¹å¼è¡¨è¾¾ï¼Œå¹¶ä¸”å¯ä»¥æ”¾åœ¨æ•°æ®å¤„ç†çš„ä¸åŒé˜¶æ®µï¼Œå¹¶ä¸”è¿™ç§å·¥ä½œä¸æ˜¯ä¸€æ¬¡æ€§å®Œæˆçš„ï¼Œè€Œæ˜¯è¿­ä»£çš„ until you run out your patience and time.é¦–å…ˆæˆ‘æŽ¥è§¦çš„æœ€å¸¸è§çš„å°±æ˜¯ discrete variables -&gt; continuous variables. å½“ç„¶å¯¹äºŽ discrete variablesï¼ŒåŸºäºŽæ ‘ç»“æž„çš„æœºå™¨å­¦ä¹ æ¨¡åž‹æ˜¯å¯ä»¥å¤„ç†çš„ï¼Œè¿™é‡Œæƒ³è¯´çš„æ˜¯æœ‰è¿™ç§æ–¹å¼ã€‚è¿™ç§ transformation å¸¸è§çš„å¤„ç†æ–¹å¼: one-hot æˆ–è€… label encoding. å¦‚æžœæŒ‰ç…§ data transformationçš„é¢„è®¾ï¼Œé‚£ä¹ˆ normalization å°±ä¹Ÿå±žäºŽè¯¥æ¨¡å—çš„å†…å®¹ã€‚ ä¸è®ºæ˜¯åœ¨ machine learning è¿˜æ˜¯åœ¨ å›¾åƒå¤„ç†çš„æ—¶å€™ï¼Œå¯¹äºŽåŽŸå§‹çš„æ•°æ®ç»å¸¸é‡‡å– normalization. ä¸€æ–¹é¢è¿™ä¸ªæ˜¯å¯ä»¥é¢„é˜²æ¢¯åº¦æ¶ˆå¤± æˆ–è€… gradient exploding, å¦‚æžœä½ é‡‡ç”¨äº† Sigmoidçš„æ¿€æ´»å‡½æ•°çš„è¯ã€‚å¦ä¸€æ–¹é¢æˆ‘è®¤ä¸ºæ›´åŠ é‡è¦çš„åŽŸå› æ˜¯å°† ä¸åŒçš„æ•°æ®æ”¾åœ¨äº†åŒä¸€ä¸ªå°ºåº¦ä¸‹ï¼Œå¦‚æžœä½ é‡‡å–äº† normalizationä¹‹åŽã€‚å®žçŽ°è¿™ç§normalization ç»å¸¸é‡‡ç”¨çš„æ˜¯ä»¥ä¸‹ä¸‰ç§æ–¹å¼ï¼š min-max normalization: $x ^ { \prime } = \frac { x - \min ( x ) } { \max ( x ) - \min ( x ) }$mean normalization:$x ^ { \prime } = \frac { x - \text { average } ( x ) } { \max ( x ) - \min ( x ) }$standardization: $x ^ { \prime } = \frac { x - \overline { x } } { \sigma }$ Data Reduction:ä¸€èˆ¬æ¥è¯´å¾ˆå°‘æåŠåˆ°åˆ° data reductionçš„å¿…è¦æ€§ï¼Œå¦‚æžœéžè¦ç»™å‡ºåŽŸå› ï¼Œé‚£ä¹ˆå¯ä»¥ä»Žæ—¶é—´å’Œç©ºé—´çš„è§’åº¦è¿›è¡Œè€ƒè™‘ã€‚æ›´åŠ éœ€è¦å…³æ³¨çš„æ˜¯å¦‚ä½•åšçš„é—®é¢˜ã€‚ æˆ‘çš„ç†è§£reduction å¯ä»¥ä»Žä¸¤ä¸ªç»´åº¦è¿›è¡Œè€ƒè™‘ï¼Œå‡è®¾ä¸€ä¸ª matrics A æ˜¯ m*nï¼Œè¿™ä¸ªæ˜¯ä¸€ä¸ªäºŒç»´çš„çŸ©é˜µï¼Œé‚£ä¹ˆå¯ä»¥ä»Ž è¡Œåˆ—ä¸¤æ–¹é¢å…¥æ‰‹ã€‚æ˜ å°„åˆ°æœºå™¨å­¦ä¹ ä¸­ä¸€èˆ¬è¿™æ ·æè¿° ä»Ždimension å’Œ dataä¸¤ä¸ªè§’åº¦åŽ»æè¿°ï¼Œåˆ†åˆ«ç§°ä¹‹ä¸º dimension reduction å’Œ data compression. å‰è€…æŒ‡çš„æ˜¯ç‰¹å¾çš„é€‰å–ï¼ŒåŽè€…æ˜¯æ•°æ®sizeçš„å‡å°‘ã€‚dimension reduction: where irrelevant, weakly relevant, or redundant attributes or dimensions may be detected and removed.data compression: PCA çº¿æ€§é™ç»´ to reduce the data set size. è¿™ä¸ªæ˜¯é’ˆå¯¹æŸä¸€ä¸ªç‰¹å¾å±•å¼€çš„ã€‚ æœºå™¨å­¦ä¹ ä¸­çš„ç‰¹å¾å·¥ç¨‹æ˜¯æœ‰ä¸€å®šæŠ€å·§å¯è¨€ï¼Œå…¶ä¸­æˆ‘è§‰å¾—æœ€ä¸ºæœ‰è¶£çš„æ˜¯: generation or you can call it abstraction. å¯¹äºŽç‰¹å¾çš„æ³›çš„æå–æ‰æ˜¯å¯¹äºŽé—®é¢˜æœ¬èº«æˆ–è€…ç‰¹å¾çš„ç†è§£ï¼Œè¿™ä¸ä»…éœ€è¦ç§¯ç´¯ï¼Œæ›´éœ€è¦å¯¹äºŽè¯¥é—®é¢˜é¢†åŸŸçš„ä¸“ä¸šçŸ¥è¯†ï¼Œ thatâ€™s all.ä¸¾ä¸ªæ —å­ï¼Œåœ¨ â€œHome Credit Default Riskâ€ (kaggle ç«žèµ›)ä¸­ï¼ŒåŽŸå§‹çš„è®­ç»ƒæ•°æ®æœ‰ä¿¡è´·é‡‘é¢å’Œå®¢æˆ·çš„å¹´æ”¶å…¥ï¼Œè¿™ä¸ªæ—¶å€™ â€œcredit_income_percentâ€ å°±æ˜¯ç±»ä¼¼è¿™ç§æ€§è´¨çš„æå–ç‰¹å¾ã€‚]]></content>
      <categories>
        <category>æœºå™¨å­¦ä¹ </category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Dimension Reduction]]></title>
    <url>%2F2019%2F03%2F25%2FDimension-Reduction%2F</url>
    <content type="text"><![CDATA[å¯¹äºŽdimension reductionæœ€è¿‘æœ‰äº†æ–°çš„ç†è§£:å¹¿ä¹‰ä¸Šå°†é™ç»´å°±æ˜¯ä½¿ç”¨æ›´å°‘çš„æ•°æ® (bits) å´ä¿å­˜äº†å°½å¯èƒ½å¤šçš„ä¿¡æ¯ã€‚You can use this concept to reduce the number of features in your dataset without having to lose much information and keep (or improve) the modelâ€™s performance. ä¸å¿…çº ç»“äºŽé‡‡ç”¨é™ç»´çš„å¿…è¦æ€§ï¼Œç›´æŽ¥è¿›å…¥ techniques of dimension reduction. low variancesè¿™ä¸ªæ˜¯é’ˆå¯¹ä¸€ä¸ªç‰¹å¾å†…éƒ¨çš„ï¼Œå¦‚æžœä¸€ä¸ªç‰¹å¾çš„æ•°æ®æœ¬èº«æ²¡æœ‰ä»€ä¹ˆå˜åŒ–ï¼Œé‚£ä¹ˆè¿™ä¸ªç±»ä¼¼å°±æ˜¯ä¸€ç§â€œæ­»â€æ•°æ®ã€‚ high correlation filterç”¨æ¥åˆ¤åˆ«ç‰¹å¾ x å’Œæœ€åŽçš„ targetä¹‹é—´çš„ç›¸å…³æ€§ principal component analysis (PCA)our old good friend. å¦‚æžœä½ æé™ç»´ï¼Œä½†æ˜¯ä½ ä¸çŸ¥é“PCAï¼Œé‚£ä¹ˆå°±è¯´ä¸è¿‡åŽ»ã€‚è¯¥æ–¹æ³•çš„åŸºæœ¬æ€è·¯ï¼šä¸€ä¸ªåŸºï¼ˆå‘é‡ç©ºé—´ï¼‰çš„å˜æ¢ï¼Œä½¿å¾—å˜æ¢åŽçš„æ•°æ®æœ‰ç€æœ€å¤§çš„æ–¹å·®ã€‚It works on a condition that while the data in a higher dimensional space is mapped to data in a lower dimension space, the variance of the data in the lower dimensional space should be maximum. ä¸‹é¢æ˜¯PCAçš„ä¸€äº›ç‰¹ç‚¹ï¼š A principal component is a linear combination of the original variablesPrincipal components are extracted in such a way that the first principal component explains maximum variance in the datasetSecond principal component tries to explain the remaining variance in the dataset and is uncorrelated to the first principal componentThird principal component tries to explain the variance which is not explained by the first two principal components and so on ä¸»æˆåˆ†æ˜¯ä¸æ–­ç”Ÿæˆçš„ï¼Œåœ¨å‰è€…åŸºç¡€ä¹‹ä¸Šç”Ÿæˆçš„ã€‚ The first component is the most important one, followed by the second, then the third, and so on. Singular Value Decomposition (SVD) ç¿»è¯‘æˆä¸­æ–‡æ„Ÿè§‰è¿˜æ˜¯æŒºåˆ«æ‰­çš„ï¼Œå¥‡å¼‚å€¼åˆ†è§£ã€‚å…³äºŽå¥‡å¼‚å€¼ï¼Œç‰¹å¾å€¼è¿™äº›æ•°å­¦æ¦‚å¿µæ‰“ç®—å¦å¤–å†™ä¸€ä¸ªä¸»é¢˜ï¼Œwait a moment. ç®€å•ç†è§£PCA æ˜¯é’ˆå¯¹æ–¹é˜µ (mm), SVDæ˜¯é’ˆå¯¹çŸ©é˜µ(m n)ï¼Œæ‰€ä»¥åŽè€…æ˜¯å…·æœ‰æ›´å¤§çš„é€‚ç”¨èŒƒå›´ã€‚ Independent Component Analysis (ICA) è¿™ä¸ªæ˜¯åœ¨é¢è¯•çš„æ—¶å€™è¢«é—®é“çš„ä¸€ç§é™ç»´æ–¹æ³•ã€‚æŠ“ä½ç‹¬ç«‹å‘é‡åº”è¯¥å°±æ²¡æœ‰é—®é¢˜ã€‚ Independent Component Analysis (ICA) is based on information-theory and is also one of the most widely used dimensionality reduction techniques. The major difference between PCA and ICA is that PCA looks for uncorrelated factors while ICA looks for independent factors. pca å’Œ ica çš„å·®åˆ«åœ¨äºŽï¼Œç›¸å…³æ€§å’Œç‹¬ç«‹æ€§çš„å·®åˆ«ã€‚ åŸºæœ¬å‡è®¾ï¼š This algorithm assumes that the given variables are linear mixtures of some unknown latent variables. It also assumes that these latent variables are mutually independent, i.e., they are not dependent on other variables and hence they are called the independent components of the observed data. ICA å’ŒPCAçš„å¼‚åŒï¼šä»Žçº¿æ€§ä»£æ•°çš„è§’åº¦åŽ»ç†è§£ï¼ŒPCAå’ŒICAéƒ½æ˜¯è¦æ‰¾åˆ°ä¸€ç»„åŸºï¼Œè¿™ç»„åŸºå¼ æˆä¸€ä¸ªç‰¹å¾ç©ºé—´ï¼Œæ•°æ®çš„å¤„ç†å°±éƒ½éœ€è¦æ˜ å°„åˆ°æ–°ç©ºé—´ä¸­åŽ»ã€‚ICAç›¸æ¯”ä¸ŽPCAæ›´èƒ½åˆ»ç”»å˜é‡çš„éšæœºç»Ÿè®¡ç‰¹æ€§ï¼Œä¸”èƒ½æŠ‘åˆ¶é«˜æ–¯å™ªå£°ã€‚ T-SNE å°±æ˜¯æŒ‡å‡º t-SNE è¿™ä¸ªå¯ä»¥éžçº¿æ€§é™ç»´ã€‚æœ‰local approaches å’Œ global approaches ä¸¤ç§æ–¹å¼ï¼Œæˆ‘å¤§æ¦‚çš„ç†è§£å°±æ˜¯ï¼šç±»ä¹‹é—´è¿˜èƒ½å°½å¯èƒ½çš„è¿œç¦»ï¼Œç±»å†…ä¿æŒå·®å¼‚æ€§ã€‚è¿™ä¸ªä½¿ç”¨åœºæ™¯æ˜¯åœ¨å¯è§†åŒ–ä¸­ï¼Œç»å¸¸ä¼šçœ‹è§å°†æ•°æ®æˆ–è€… So far we have learned that PCA is a good choice for dimensionality reduction and visualization for datasets with a large number of variables. But what if we could use something more advanced? What if we can easily search for patterns in a non-linear way? t-SNE is one such technique. There are mainly two types of approaches we can use to map the data points:Local approaches : They maps nearby points on the manifold to nearby points in the low dimensional representation.Global approaches : They attempt to preserve geometry at all scales, i.e. mapping nearby points on manifold to nearby points in low dimensional representation as well as far away points to far away points. ä¸‹é¢ä»‹ç»ä¸¤ç§ä¸æ˜¯é‚£ä¹ˆâ€œå¸¸è§„â€ï¼Œä½†æ˜¯ä¹Ÿç¬¦åˆâ€dimension reductionâ€ å®šä¹‰çš„æ–¹å¼ã€‚ projection By projecting one vector onto the other, dimensionality can be reduced. autoencoder ç½‘ç»œç»“æž„é€šå¸¸æœ‰ encoderå’Œdecoderä¸¤éƒ¨åˆ†ç»„æˆï¼Œé‚£ä¹ˆencoder å°±ä½œä¸º information abstraction,è€Œ decoderä½œä¸ºä¸€ç§é‡æ–°æ˜ å°„ã€‚ä»Žè¿™ä¸ªè§’åº¦NLPä¸­çš„è¯å‘é‡ä¹Ÿæ˜¯å¯ä»¥æ˜¯ä¸€ç§é™ç»´æ‰‹æ®µã€‚]]></content>
      <categories>
        <category>æœºå™¨å­¦ä¹ </category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Linear Algebra in ML]]></title>
    <url>%2F2019%2F03%2F25%2FLinear-Algebra-in-ML%2F</url>
    <content type="text"><![CDATA[æˆ‘è§‰å¾—åˆ° ML ä¸­çš„ä¸€ä¸ªéš¾ç‚¹ï¼šå°±æ˜¯ç”±åŽŸæ¥ç®€å•çš„ linear equations ç›´æŽ¥è¿‡æ¸¡åˆ°äº† matrics and vectorsã€‚è¿™ä¸ªè¿‡ç¨‹æ˜¯æ²¡æœ‰äººè·Ÿä½ è¯´çš„ã€‚ç½‘ç»œç»“æž„å¯ä»¥å½“ä½œæ˜¯ä¸€ä¸ªcomplicated å¹¶ä¸”æ˜¯æ— æ³•è¡¨ç¤ºçš„å‡½æ•°ï¼ŒäºŽæ˜¯å¾ˆå¤šä½¿ç”¨è€…æŠŠå®ƒå½“ä½œé»‘åŒ£å­ï¼Œå…³å¿ƒäºŽè¾“å…¥å’Œè¾“å‡ºï¼Œä¸­é—´è¿‡ç¨‹ donâ€™t care. å˜é‡ï¼ˆç‰¹å¾ä¸ªæ•°ï¼‰å’Œè§£çš„å…³ç³»å¤šå˜é‡å’Œæœ€åŽtargetçš„å…³ç³»æ˜¯å¯ä»¥ä½¿ç”¨ matrices è¿›è¡Œè¡¨ç¤ºçš„ï¼Œè¿™å°±æ˜¯ä¸€ç§æ•°å­¦å…¬å¼åŒ–ã€‚ Broadly speaking, in linear algebra data is represented in the form of linear equations. These linear equations are in turn represented in the form of matrices and vectors. å…ˆç›´è§‚çš„æ„Ÿå—ä¸€ä¸‹å˜é‡å’Œå›¾å½¢ï¼ˆå¯è§†åŒ–ï¼‰çš„å…³ç³»ã€‚ä¸¤ä¸ªå˜é‡ç»„æˆçš„equations æ˜¯ä¸¤æ¡çº¿çš„ç›¸äº¤æƒ…å†µã€‚è€Œä¸‰ä¸ªå˜é‡åœ¨ç©ºé—´ä¸­æœ‰ä¸‰ç§æƒ…å†µï¼š ç›¸äº¤ï¼Œå¹³è¡Œï¼Œä¸åœ¨ä¸€ä¸ªå¹³é¢ä¸Šã€‚ä¸‰ä¸ªå˜é‡ç»„æˆçš„equations æ˜¯ä¸‰ä¸ªé¢çš„ç›¸äº¤æƒ…å†µã€‚æœ‰å››ç§æƒ…å†µ (try hard to figure it out)ï¼šNo intersection at all.Planes intersect in a line.They can intersect in a plane.All the three planes intersect at a point. å½“åˆ°è¾¾4 dims çš„æ—¶å€™ï¼Œitâ€™s impossible to visulize it. terms in related to matrixè¿™äº›è¯æ±‡ (terms) ç»å¸¸åœ¨æ–‡çŒ®ä¸­å‡ºçŽ°ï¼Œéœ€è¦å¯¹äºŽå…¶å«ä¹‰æœ‰ä¸ªæ¯”è¾ƒå¥½çš„è®¤è¯†ã€‚Order of matrix â€“ If a matrix has 3 rows and 4 columns, order of the matrix is 34 i.e. rowcolumn. (ç¿»è¯‘æˆ çŸ©é˜µçš„é˜¶)Square matrix â€“ The matrix in which the number of rows is equal to the number of columns.Diagonal matrix â€“ A matrix with all the non-diagonal elements equal to 0 is called a diagonal matrix.Upper triangular matrix â€“ Square matrix with all the elements below diagonal equal to 0.Lower triangular matrix â€“ Square matrix with all the elements above the diagonal equal to 0.Scalar matrix â€“ Square matrix with all the diagonal elements equal to some constant k.Identity matrix â€“ Square matrix with all the diagonal elements equal to 1 and all the non-diagonal elements equal to 0.Column matrix â€“ The matrix which consists of only 1 column. Sometimes, it is used to represent a vector.Row matrix â€“ A matrix consisting only of row.Trace â€“ It is the sum of all the diagonal elements of a square matrix.Rank of a matrix â€“ Rank of a matrix is equal to the maximum number of linearly independent row vectors in a matrix.Determinant of a matrix - çŸ©é˜µçš„è¡Œåˆ—å¼è½¬ç½® -åœ¨å›¾å½¢ matrixä¸­è¿˜æ˜¯å¾ˆå¸¸è§çš„ã€‚$$\mathrm { A } _ { \mathrm { ij } } ^ { \mathrm{T}} = \mathrm { A } _ { \mathrm { ji } }$$ è¿™ä¸ªçŸ©é˜µä¹˜æ³•å’Œå…ƒç´ ç›¸ç§°çš„åŒºåˆ«ï¼ŒåŽè€…æ˜¯element-wise è¿›è¡Œçš„ã€‚å¯ä»¥ä»Žå¦å¤–ä¸€ä¸ªè§’åº¦åŽ»åˆ—åŠçŸ©é˜µç›¸ç§°ï¼š This operation on a vector is called linear transformation. å°±æ˜¯åŽé¢çš„vector æ˜ å°„åˆ°äº†å‰é¢çš„çŸ©é˜µç©ºé—´ã€‚ ç‰¹å¾å€¼å’Œå¥‡å¼‚å€¼ç€ä¸¤ä¸ªæ˜¯åˆ†åˆ«å¯¹åº”ç€PCA å’ŒSVDã€‚Eigenvalues and Eigenvectorså¦‚å…¬å¼æ‰€ç¤ºï¼Œç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡çš„ä¹˜ç§¯å°±æ˜¯æ–¹é˜µå’Œç‰¹å¾å‘é‡çš„ä¹˜ç§¯ï¼ŒåŽŸå…ˆçš„æ–¹é˜µæ˜¯å¯ä»¥é™ç»´è¡¨ç¤ºæˆç‰¹å¾å‘é‡å’Œç‰¹å¾å€¼çš„ã€‚$ A x = \lambda x $ å¯¹äºŽå¥‡å¼‚å€¼åˆ†è§£ï¼Œæœ€å¸¸è§çš„å°±æ˜¯è¿™ç§è¡¨è¾¾ï¼š$A = U \Sigma V ^ { T }$ç‰¹å¾å€¼åˆ†è§£å’Œå¥‡å¼‚å€¼åˆ†è§£éƒ½æ˜¯ç»™ä¸€ä¸ªçŸ©é˜µ(çº¿æ€§å˜æ¢)æ‰¾ä¸€ç»„ç‰¹æ®Šçš„åŸºï¼Œç‰¹å¾å€¼åˆ†è§£æ‰¾åˆ°äº†ç‰¹å¾å‘é‡è¿™ç»„åŸºï¼Œåœ¨è¿™ç»„åŸºä¸‹è¯¥çº¿æ€§å˜æ¢åªæœ‰ç¼©æ”¾æ•ˆæžœã€‚è€Œå¥‡å¼‚å€¼åˆ†è§£åˆ™æ˜¯æ‰¾åˆ°å¦ä¸€ç»„åŸºï¼Œè¿™ç»„åŸºä¸‹çº¿æ€§å˜æ¢çš„æ—‹è½¬ã€ç¼©æ”¾ã€æŠ•å½±ä¸‰ç§åŠŸèƒ½ç‹¬ç«‹åœ°å±•ç¤ºå‡ºæ¥äº†ã€‚]]></content>
      <categories>
        <category>æœºå™¨å­¦ä¹ </category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Back to my blog]]></title>
    <url>%2F2019%2F03%2F07%2FBack-to-my-blog%2F</url>
    <content type="text"><![CDATA[ç›´åˆ°æŸä¸€å¤©å‘çŽ°ä¸ªäººç½‘ç«™ä¸­çš„å›¾ç‰‡éƒ½æ˜¾ç¤ºä¸å‡ºæ¥äº†ï¼ŒæŸ¥äº†ä¸€ä¸‹æ‰å‘çŽ°ä¹‹å‰çš„å›¾åºŠä¸èƒ½ç”¨äº†ï¼ˆç‚¹åæ‰¹è¯„ä¸ƒç‰›ï¼‰ï¼Œæžœæ–­å¼ƒä¹‹ï¼Œæ¢äº†ä¸ªå¤§åŽ‚å­äº§å“ã€‚è¯æ˜Žä¸€ä¸‹å›¾ç‰‡æ˜¯èƒ½å‡ºæ¥çš„ã€‚psï¼šä¹‹å‰çš„å›¾ç‰‡æœ‰æ—¶é—´å†æ•´ç†åˆ°æ–°çš„å¹³å°ä¸Šã€‚]]></content>
      <categories>
        <category>äººé—´ä¸å€¼å¾—</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[åŸºäºŽsimhashçš„æ–‡æœ¬ç›¸ä¼¼åº¦æ¯”è¾ƒ]]></title>
    <url>%2F2018%2F08%2F23%2F%E5%9F%BA%E4%BA%8Esimhash%E7%9A%84%E6%96%87%E6%9C%AC%E7%9B%B8%E4%BC%BC%E5%BA%A6%E6%AF%94%E8%BE%83%2F</url>
    <content type="text"><![CDATA[æˆ‘åªæ˜¯æƒ³è¯´æˆ‘è¿˜æ²¡æœ‰æ”¾å¼ƒè¿™ä¸ªç½‘ç«™â€¦ æœ¬æ–‡ä¸»è¦è®°å½•ä½¿ç”¨simhashæ¯”è¾ƒä¸­æ–‡æ–‡æœ¬çš„ç›¸ä¼¼åº¦é—®é¢˜ã€‚å…ˆè¯´ä¸€ä¸‹æ–‡æœ¬ç‰¹å¾ï¼Œæ•°æ®å±žäºŽä¸­æ–‡æ–‡æœ¬ï¼Œæ¯ç¯‡æ–‡ç« çš„å­—æ•°å¤§äºŽ500,å°äºŽ2000,åŸºæœ¬ä¸Šå±žäºŽå¤§æ–‡æœ¬ã€‚æ­¥éª¤å¦‚ä¸‹ï¼š åŸºäºŽtf-idfæå–æ–‡æœ¬çš„å…³é”®è¯ã€‚å¦‚æžœè¿™äº›å…³é”®è¯åœ¨ä¹‹åŽçš„æ¯”è¾ƒä¸­æ˜¯ç›¸åŒçš„ï¼Œé‚£ä¹ˆè®¤ä¸ºå¯¹åº”çš„æ–‡ç« ä¹Ÿæ˜¯ç›¸åŒã€‚ç®€è€Œè¨€ä¹‹ï¼Œè¿™äº›æå–çš„å…³é”®è¯å¯ä»¥çœ‹åšåŽŸæ–‡ç« çš„â€ä»£è¡¨â€. æ ¹æ®å…³é”®å­—è®¡ç®—simhashç¼–ç ï¼Œç„¶åŽä½¿ç”¨hamming distanceè¿›è¡Œæ¯”è¾ƒä¸¤è€…çš„ä¸åŒã€‚å¦‚æžœå¯¹äºŽä¸Šè¿°æ¦‚å¿µæ¯”è¾ƒæ¨¡ç³Šï¼Œå»ºè®®é¦–å…ˆé˜…è¯»è¯¥ç¯‡åšå®¢ã€‚ é¡ºæ»‘è¿‡æ¸¡åˆ°ä»£ç å®žçŽ°ï¼š123456789101112131415# å¸¸è§„å¯¼åŒ…import sys,codecsimport pandas as pdimport numpy as npimport jieba.possegimport jieba.analysefrom sklearn import feature_extractionfrom sklearn.feature_extraction.text import TfidfTransformerfrom sklearn.feature_extraction.text import CountVectorizer# æ•°æ®é›†çš„è·¯å¾„path =&quot;../tianmao2.csv&quot;names =[&apos;where&apos;, &apos;time&apos;, &apos;title&apos;, &apos;url&apos;, &apos;contents&apos;]data =pd.read_csv(path, delimiter=&apos;\t&apos;, names= names, nrows=200)data[&apos;id&apos;] =data.index+1data.head() æˆ‘ä»¬ä½¿ç”¨titleå’Œcontents ç»„åˆä½œä¸ºåŽŸå§‹å¤„ç†çš„æ•°æ®ï¼Œæˆ‘ä»¬è®¤ä¸ºè¯¥æ•°æ®èƒ½å¤Ÿå°±æ˜¯æ–‡ç« çš„å†…å®¹ã€‚1stopkey = [w.strip() for w in codecs.open(&apos;../keyword_extraction/data/stopWord.txt&apos;, &apos;r&apos;).readlines()] è¯¥stop wordsæ˜¯ä¸­æ–‡åœç”¨è¯ï¼Œå°±æ˜¯å¸¸è§çš„â€çš„ äº†â€ã€‚å¸¸è§çš„æœ‰ç™¾åº¦åœç”¨è¯è¡¨ã€å“ˆå°”æ»¨å·¥ä¸šå¤§å­¦åœç”¨è¯è¡¨ä»¥åŠä¸­ç§‘é™¢çš„åœç”¨è¯è¡¨ã€‚è¿™é‡Œä½¿ç”¨çš„æ˜¯ä¸­ç§‘é™¢çš„åœç”¨è¯ã€‚å¯¹äºŽåœç”¨è¯çš„å­˜å‚¨ï¼Œå¯ä»¥ä½¿ç”¨set ï¼Œå› ä¸ºset è¦æ¯” listçš„æ£€ç´¢è¦å¿«ã€‚12345678def dataPrepos(text, stopkey): l = [] pos = [&apos;n&apos;, &apos;nz&apos;, &apos;v&apos;, &apos;vd&apos;, &apos;vn&apos;, &apos;l&apos;, &apos;a&apos;, &apos;d&apos;] # å®šä¹‰é€‰å–çš„è¯æ€§ seg = jieba.posseg.cut(text) # åˆ†è¯ for i in seg: if i.word not in stopkey and i.flag in pos: # åŽ»åœç”¨è¯ + è¯æ€§ç­›é€‰ l.append(i.word) return l æˆ‘ä»¬é€‰æ‹©åè¯ä½œä¸ºä¸»è¦çš„åˆ†æžå¯¹è±¡ã€‚12345678idList, titleList, abstractList = data[&apos;id&apos;], data[&apos;title&apos;], data[&apos;contents&apos;]corpus = [] # å°†æ‰€æœ‰æ–‡æ¡£è¾“å‡ºåˆ°ä¸€ä¸ªlistä¸­ï¼Œä¸€è¡Œå°±æ˜¯ä¸€ä¸ªæ–‡æ¡£# è¿™ä¸ª è™½ç„¶ä½¿ç”¨ &quot; &quot; è¿›è¡Œåˆ†å‰² ä½†æ˜¯å®žé™…ä¸Šè¿˜æ˜¯ä¸€ä¸ªæ‰“çš„listfor index in range(len(idList)): text = &apos;%sã€‚%s&apos; % (titleList[index], abstractList[index]) # æ‹¼æŽ¥æ ‡é¢˜å’Œæ‘˜è¦ text = dataPrepos(text, stopkey) # æ–‡æœ¬é¢„å¤„ç† text = &quot; &quot;.join(text) # è¿žæŽ¥æˆå­—ç¬¦ä¸²ï¼Œç©ºæ ¼åˆ†éš” corpus.append(text) è¿™é‡Œçš„corus æ˜¯å°†æ‰€æœ‰çš„ç»è¿‡é¢„å¤„ç†æ–‡æ¡£ä½œä¸ºå½“å‰è®¡ç®— idf çš„è¯­æ–™åº“ã€‚123456789vectorizer = CountVectorizer()X = vectorizer.fit_transform(corpus) # è¯é¢‘çŸ©é˜µ,a[i][j]:è¡¨ç¤ºjè¯åœ¨ç¬¬iä¸ªæ–‡æœ¬ä¸­çš„è¯é¢‘# 2ã€ç»Ÿè®¡æ¯ä¸ªè¯çš„tf-idfæƒå€¼transformer = TfidfTransformer()tfidf = transformer.fit_transform(X)# 3ã€èŽ·å–è¯è¢‹æ¨¡åž‹ä¸­çš„å…³é”®è¯word = vectorizer.get_feature_names()# 4ã€èŽ·å–tf-idfçŸ©é˜µï¼Œa[i][j]è¡¨ç¤ºjè¯åœ¨iç¯‡æ–‡æœ¬ä¸­çš„tf-idfæƒé‡weight = tfidf.toarray() ä½¿ç”¨sklearn å†…ç½®çš„å‡½æ•°è®¡ç®—tf-idfã€‚1234567891011121314151617181920212223242526272829topK = 10ids, titles, keys, weights = [], [], [], []for i in range(len(weight)): print(&quot;-------è¿™é‡Œè¾“å‡ºç¬¬&quot;, i + 1, &quot;ç¯‡æ–‡æœ¬çš„è¯è¯­tf-idf------&quot;) ids.append(idList[i]) titles.append(titleList[i]) df_word, df_weight = [], [] # å½“å‰æ–‡ç« çš„æ‰€æœ‰è¯æ±‡åˆ—è¡¨ã€è¯æ±‡å¯¹åº”æƒé‡åˆ—è¡¨ for j in range(len(word)): # print(word[j],weight[i][j]) df_word.append(word[j]) df_weight.append(weight[i][j]) df_word = pd.DataFrame(df_word, columns=[&apos;word&apos;]) df_weight = pd.DataFrame(df_weight, columns=[&apos;weight&apos;]) word_weight = pd.concat([df_word, df_weight], axis=1) # æ‹¼æŽ¥è¯æ±‡åˆ—è¡¨å’Œæƒé‡åˆ—è¡¨ word_weight = word_weight.sort_values(by=&quot;weight&quot;, ascending=False) # æŒ‰ç…§æƒé‡å€¼é™åºæŽ’åˆ— # åœ¨è¿™é‡Œå¯ä»¥æŸ¥çœ‹ kçš„é€‰å–çš„æ•°å€¼åº”è¯¥æ˜¯å¤šå¤§ï¼Œ # from ipdb import set_trace # set_trace() keyword = np.array(word_weight[&apos;word&apos;]) # é€‰æ‹©è¯æ±‡åˆ—å¹¶è½¬æˆæ•°ç»„æ ¼å¼ word_split = [keyword[x] for x in range(0, topK)] # æŠ½å–å‰topKä¸ªè¯æ±‡ä½œä¸ºå…³é”®è¯ word_split = &quot; &quot;.join(word_split) keys.append(word_split) wei = np.array(word_weight[&apos;weight&apos;]) wei_split = [str(wei[x]) for x in range(0, topK)] wei_split = &quot; &quot;.join(wei_split) weights.append(wei_split) # è¿™é‡Œçš„å‘½å å®¹æ˜“æ··æ·†result = pd.DataFrame(&#123;&quot;id&quot;: ids, &quot;title&quot;: titles, &quot;key&quot;: keys, &apos;weight&apos;: weights&#125;, columns=[&apos;id&apos;, &apos;title&apos;, &apos;key&apos;, &apos;weight&apos;]) é€‰æ‹©å‰10ä¸ªé¢‘çŽ‡æœ€é«˜çš„è¯è¯­ä½œä¸ºè¯¥ç¯‡æ–‡ç« çš„ä»£è¡¨ï¼Œå½“ç„¶è¿™ä¸ªå‚æ•°æ˜¯å¯ä»¥è°ƒæ•´ï¼Œéœ€è¦æ ¹æ®å…·ä½“çš„é—®é¢˜å’Œç»“æžœè¿›è¡Œè°ƒæ•´ã€‚1result.head() æœ€åŽçš„æ•ˆæžœå¦‚ä¸Šã€‚è‡³æ­¤æˆ‘ä»¬ç¬¬ä¸€æ­¥çš„æå–æ–‡ç« çš„å…³é”®è¯å°±å·²ç»åšå®Œã€‚ä¸‹é¢è¿›è¡Œç›¸ä¼¼åº¦çš„æ¯”è¾ƒã€‚ 1234import jiebaimport jieba.analyseimport pandas as pd#æ—¥å¸¸å¯¼åŒ… æ•°æ®å’Œä¸Šè¿°çš„ä¸€æ ·ï¼Œæ‰€ä»¥å°±ä¸æˆªå›¾äº†ã€‚123datasets =pd.read_csv(&quot;../tianmao2-tf-idf.csv&quot;)tokens =datasets[&apos;key&apos;]weights =datasets[&apos;weight&apos;] æå–å…³é”®è¯å’Œå¯¹åº”çš„æƒé‡ã€‚123456789101112print(tokens[0], len(tokens[0]))print(weights[0], len(weights[0]))tokens0 =tokens[0].split()weights0 =weights[0].split()len(tokens0)len(weights0)tokens1 =tokens[1].split()weights1 =weights[1].split()import astweights0 =[ ast.literal_eval(i) for i in weights0]weights1 =[ ast.literal_eval(i) for i in weights1] æž„é€ æµ‹è¯•ç”¨ä¾‹ã€‚å› ä¸ºæƒé‡æ˜¯å­—ç¬¦ä¸²ï¼Œæ‰€ä»¥ç®€å•å¤„ç†è½¬æˆæ•´æ•°ã€‚ 12dict0 =dict(zip(tokens0, weights0))dict1 =dict(zip(tokens1, weights1)) å®šä¹‰ä¸€ä¸ªSimhashï¼Œæä¾›å¯¹æ–‡æ¡£çš„æ•°å€¼æ˜ å°„å’Œæ–‡æ¡£é—´ç›¸ä¼¼åº¦è®¡ç®—çš„åŠŸèƒ½.123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081class Simhash(object): # åˆå§‹åŒ–å‡½æ•° def __init__(self, weights_dict, tokens=&apos;&apos;, hashbits=64): self.hashbits = hashbits self.hash = self.simhash_function(tokens, weights_dict) # toStringå‡½æ•° # ä¸æ‡‚è¿™ä¸ª self.hash ï¼Œå‡¡æ˜¯å¸¦æœ‰self çš„å‡½æ•°éƒ½æ˜¯å¯ä»¥ç±»å˜é‡ï¼Œæ‰€ä»¥è¿™ä¸ªå°±æ˜¯è¿”å›žçš„ self.hashè¿™ä¸ªå˜é‡ #å‡¡æ˜¯ä½¿ç”¨__str__ è¿™ç§ç±»åž‹çš„å‡½æ•° éƒ½æ˜¯é‡å†™ åŽŸæ¥çš„å‡½æ•° def __str__(self): return str(self.hash) &quot;&quot;&quot; ord() å‡½æ•°æ˜¯ chr() å‡½æ•°ï¼ˆå¯¹äºŽ8ä½çš„ASCIIå­—ç¬¦ä¸²ï¼‰æˆ– unichr() å‡½æ•°ï¼ˆå¯¹äºŽUnicodeå¯¹è±¡ï¼‰çš„é…å¯¹å‡½æ•°ï¼Œå®ƒä»¥ä¸€ä¸ªå­—ç¬¦ï¼ˆé•¿åº¦ä¸º1çš„å­—ç¬¦ä¸²ï¼‰ä½œä¸ºå‚æ•°ï¼Œè¿”å›žå¯¹åº”çš„ ASCII æ•°å€¼ï¼Œæˆ–è€… Unicode æ•°å€¼ &quot;&quot;&quot; # ç»™æ¯ä¸€ä¸ªå•è¯ç”Ÿæˆå¯¹åº”çš„hashå€¼ # è¿™ä¸ªæ“ä½œæžæ‡‚ä¹‹åŽä¸€å®šå¾ˆç®€æ´ï¼Œ ä½†æ˜¯çŽ°åœ¨å¾ˆéš¾ç†è§£ï¼Œå› ä¸ºä¸æ˜¯å­—ç¬¦ä¸²ï¼Œè€Œæ˜¯ä½æ“ä½œ def _string_hash(self, source): if source == &apos;&apos;: return 0 else: x = ord(source[0]) &lt;&lt; 7 # &lt;&lt; è¡¨ç¤º ä¹˜ä»¥2^7 ; &gt;&gt; è¡¨ç¤ºé™¤ä»¥ ; ** è¡¨ç¤ºæ¬¡æ–¹çš„æ„æ€ # ^ : æŒ‰ä½å¼‚æˆ– (äºŒè¿›åˆ¶è¿›è¡Œå¼‚æˆ–)ï¼› &amp; æŒ‰ä½è¿›è¡Œä¸Ž æ“ä½œ # å·¦ç§»ä½æ“ä½œä¹Ÿæ˜¯å¯ä»¥ç†è§£ä¸º 2^x çš„æ“ä½œï¼Œå› ä¸ºå­˜å‚¨æ˜¯äºŒè¿›åˆ¶ï¼Œè¿™æ ·å·¦ç§»ä¸€ä½ è¡¨ç¤ºÃ—2 ä¸€æ¬¡ m = 1000003 mask = 2 ** self.hashbits - 1 for c in source: x = ((x * m) ^ ord(c)) &amp; mask x ^= len(source) if x == -1: x = -2 return x # ç”Ÿæˆsimhashå€¼ def simhash_function(self, tokens, weights_dict): v = [0] * self.hashbits # è¿™ç§ä½¿ç”¨ &#123;&#125; dictionary ç„¶åŽå¼ºè¡Œå¾—åˆ°item å†è¿›è¡ŒéåŽ†ä¹Ÿæ˜¯ç‰›é€¼ for key, t in &#123;x: self._string_hash(x) for x in tokens&#125;.items(): for i in range(self.hashbits): bitmask = 1 &lt;&lt; i if t &amp; bitmask: v[i] += weights_dict[key] else: v[i] -= weights_dict[key] fingerprint = 0 for i in range(self.hashbits): if v[i] &gt;= 0: fingerprint += 1 &lt;&lt; i return fingerprint # æ±‚æ–‡æ¡£é—´çš„æµ·æ˜Žè·ç¦» def hamming_distance(self, other): x = (self.hash ^ other.hash) &amp; ((1 &lt;&lt; self.hashbits) - 1 ) tot = 0 while x : tot += 1 x &amp;= x - 1 return tot #æ±‚ç›¸ä¼¼åº¦ # è¿™ä¸ªç›¸ä¼¼åº¦çš„è®¡ç®—ï¼Œååˆ†ç®€å•ï¼Œå¦‚æžœä¸¤ä¸ªæ•°æŽ¥è¿‘ï¼Œé‚£ä¹ˆå°±æ˜¯è®¤ä¸ºç›¸ä¼¼ã€‚è¶Šæ˜¯æŽ¥è¿‘1 è¶Šæ˜¯ç›¸ä¼¼ï¼Œ # ä¸æ˜¯åŽŸå…ˆé‚£ç§ä»¥æŸä¸€ä¸ªå‚æ•°æ•´æ•° å¦‚3 ä¸ºè·ç¦»çš„ç›¸ä¼¼åº¦ def similarity(self, other): a = float(self.hash) b = float(other.hash) if a &gt; b: return b / a else: return a / b if __name__ == &apos;__main__&apos;: hash0 = Simhash(weights_dict=dict0, tokens=tokens0) print(hash0) hash1 = Simhash(weights_dict=dict1, tokens=tokens1) print(hash1) print(hash0.hamming_distance(hash1)) print(hash0.similarity(hash1)) ç»“æžœå¦‚ä¸Šã€‚å¯ä»¥çœ‹å‡ºè¯¥ä¾‹å­ä¸­ä½¿ç”¨çš„ä¸¤ä¸¤æ¯”è¾ƒçš„æ–¹å¼ï¼Œå¯¹äºŽå¤§æ•°æ®æ¥è¯´ï¼Œä¸€èˆ¬å¯èƒ½ä¼šç”¨åˆ°å€’æŽ’ç´¢å¼•å’Œcpuå¹¶è¡ŒæŠ€æœ¯ã€‚]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>æ–‡æœ¬ç›¸ä¼¼åº¦</tag>
        <tag>Simhash</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ–‡æœ¬ç›¸ä¼¼åº¦æ¯”è¾ƒåŸºæœ¬çŸ¥è¯†]]></title>
    <url>%2F2018%2F08%2F23%2F%E6%96%87%E6%9C%AC%E7%9B%B8%E4%BC%BC%E5%BA%A6%E6%AF%94%E8%BE%83%E5%9F%BA%E6%9C%AC%E7%9F%A5%E8%AF%86%2F</url>
    <content type="text"><![CDATA[æœ¬æ–‡æœåŠ¡äºŽè¯¥ç¯‡åšå®¢,ä¸»è¦è¿›è¡Œåè¯è§£é‡Šã€‚ simhashåŸºæœ¬æ¦‚å¿µsimhash ä¹Ÿæ˜¯ä¸€ç§hashï¼Œä¸€èˆ¬çš„hash å‡½æ•°æ˜ å°„è§„åˆ™åªéœ€è¦æ»¡è¶³ä»¥ä¸‹ä¸¤ä¸ªæ¡ä»¶ï¼š å¯¹å¾ˆå¤šä¸åŒçš„ç‰¹å¾æ¥è¯´ï¼Œå®ƒä»¬å¯¹æ‰€å¯¹åº”çš„å‘é‡æ˜¯å‡åŒ€éšæœºåˆ†å¸ƒçš„ ç›¸åŒçš„ç‰¹å¾æ¥è¯´å¯¹åº”çš„å‘é‡æ˜¯å”¯ä¸€ç®€å•æ¥è¯´æ™®é€šçš„hashæ˜ å°„éœ€è¦æ»¡è¶³éšæœºåˆ†å¸ƒå’Œå”¯ä¸€æ€§ä¸¤ä¸ªæ¡ä»¶ã€‚simhashæƒ³è¦å®žçŽ°çš„æ˜¯ï¼Œå¦‚æžœåŽŸæ¥çš„æ–‡æœ¬çš„ç‰¹å¾æ˜¯ç›¸ä¼¼ï¼Œé‚£ä¹ˆæ˜ å°„ä¹‹åŽçš„ç¼–ç ä¹Ÿæ˜¯ç›¸ä¼¼ã€‚è¿™é‡Œä½¿ç”¨ hamming distance è¿›è¡Œæ¯”è¾ƒsimhashæ˜ å°„ä¹‹åŽçš„è·ç¦»ã€‚æ ¹æ®ç»éªŒå€¼ï¼Œå¯¹64ä½çš„ SimHashå€¼ï¼Œæµ·æ˜Žè·ç¦»åœ¨3ä»¥å†…çš„å¯è®¤ä¸ºç›¸ä¼¼åº¦æ¯”è¾ƒé«˜ã€‚ç¼–ç ä¹‹åŽçš„è¡¨ç¤ºåœ¨è‹±æ–‡ä¸­æ˜¯ fingerprint(æŒ‡çº¹)ã€‚simhashæœ€åˆè¢«google ç”¨äºŽç½‘é¡µåŽ»é‡ï¼Œå½“æ—¶ä½¿ç”¨çš„fingerprint æ˜¯64,æ‰€ä»¥è¿™é‡Œæ²¿ç”¨äº†è¿™ä¸ªä¼ ç»Ÿã€‚64ä½çš„ç­¾åå¯ä»¥è¡¨ç¤ºå¤šè¾¾264ä¸ªè±¡é™ï¼Œå› æ­¤åªä¿å­˜æ‰€åœ¨è±¡é™çš„ä¿¡æ¯ä¹Ÿè¶³å¤Ÿè¡¨å¾ä¸€ä¸ªæ–‡æ¡£äº†ã€‚æ›´è¿›ä¸€æ­¥ï¼Œè¡¨ç¤ºçš„æ–‡æ¡£çš„æ•°å­—æœ€å¤šæ˜¯å¤šå°‘ï¼Ÿè¿™ä¸ªåº”è¯¥å¯ä»¥å‡†ç¡®è®¡ç®—ç‰¹å¾çš„ä¸ªæ•°åº”ä¸ºå¦‚æžœç”¨ä¸‰ä½(01) è¡¨ç¤ºï¼Œé‚£ä¹ˆæœ‰8ç§ï¼Œé‚£ä¹ˆ2^64 è¿™ä¹ˆå¤šç§ç‰¹å¾ï¼Œæ‰€ä»¥16*10^18 è¿™ä¹ˆå¤šã€‚ç®—æ³•æ­¥éª¤ç¬¬ä¸€æ­¥ï¼šæ–‡æœ¬é¢„å¤„ç†å¾—åˆ°åˆ†è¯ï¼ˆåŽ»é‡ï¼ŒåŽ»é™¤çš„äº†stop wordsï¼‰,ç„¶åŽweightæƒé‡å¯ä»¥ä½¿ç”¨åˆ†è¯çš„frequency æˆ–è€…tfidf è¿›è¡Œå¾—åˆ°ç¬¬äºŒæ­¥ï¼šè¿›è¡Œhash æ˜ å°„ï¼ˆå¯ä»¥ä½¿ç”¨md5è¿™ç§ä¼ ç»Ÿçš„æ˜ å°„æ–¹å¼ï¼ŒåŸºæœ¬çš„è¦æ±‚å°±æ˜¯å‡åŒ€æ˜ å°„åˆ°ä¸€ä¸ªç©ºé—´ï¼Œè¿™ç§æ˜ å°„å¹¶ä¸èƒ½åæ˜ åŽŸå§‹æ ·æœ¬çš„ç›¸å…³æ€§ï¼‰ç¬¬ä¸‰æ­¥ï¼šhash æ˜ å°„å€¼å’Œweight è¿›è¡Œç›¸ä¹˜ï¼Œå¦‚æžœåŽŸæ¥æ˜¯1 åˆ™ä¹˜ä»¥1ï¼Œå¦‚æžœæ˜¯0 åˆ™ä¹˜ä»¥-1ï¼Œç¬¬å››æ­¥ï¼š åˆ—å‘ç›¸åŠ ï¼Œå¾—åˆ°summing weights,è¿›è¡Œé™ç»´å¦‚æžœæ˜¯æ­£æ•°é‚£ä¹ˆä¸º1ï¼Œå¦‚æžœæ˜¯è´Ÿæ•°é‚£ä¹ˆä¸º-1ç„¶åŽè¿™ä¸ªsimhashå°±å‡ºæ¥äº†.æœ‰å›¾æœ‰çœŸç›¸ simhashçš„å±€é™æ€§ï¼šåªè€ƒè™‘åˆ°æ–‡ç« å­˜åœ¨å“ªäº›è¯ï¼Œæ²¡æœ‰è€ƒè™‘åˆ°è¯çš„é¡ºåºã€‚ä¸è¿‡ç›¸åº”çš„ä¼˜ç‚¹æ˜¯ï¼Œå¯ä»¥å®žçŽ°æµ·é‡æ–‡ç« ç›¸ä¼¼åº¦è®¡ç®—ã€‚æ–‡ç« ç›¸ä¼¼åº¦è®¡ç®—å¿½ç•¥è¯çš„é¡ºåºä¹‹åŽæ•ˆæžœæ›´å¥½ã€‚æ‰€ä»¥åœ¨å¤„ç†å¤§æ–‡æœ¬æ—¶å€™ï¼Œsimhashæ˜¯æœ‰æ•ˆçš„ï¼Œä½†æ˜¯åœ¨å¤„ç†å°æ–‡æœ¬ï¼Œè¿™ç§æ•ˆæžœå¾€å¾€ä¸èƒ½è¢«ä¿è¯ã€‚ç›´è§‚ä¸Šç†è§£ï¼Œåœ¨ä¸€ç‰‡æ®µæ–‡ç« æˆ–è€…æ®µè½ä¸­ï¼Œè¯è¯­å‡ºçŽ°çš„é¡ºåºè¿˜æ˜¯æ¯”è¾ƒé‡è¦çš„ã€‚ minhashå¯ä»¥å‚è€ƒè¯¥è§†é¢‘å’Œè¿™ç¯‡æ–‡ç« ã€‚ Locality Sensitive HashingLocality Sensitive Hashing(å±€éƒ¨æ•æ„Ÿå“ˆå¸Œ)ä½œç”¨å°±æ˜¯ä»Žæµ·é‡çš„æ•°æ®ä¸­æŒ–æŽ˜å‡ºç›¸ä¼¼çš„æ•°æ®ï¼Œå¯ä»¥å…·ä½“åº”ç”¨åˆ°æ–‡æœ¬ç›¸ä¼¼åº¦æ£€æµ‹ã€ç½‘é¡µæœç´¢ç­‰é¢†åŸŸã€‚ä¸Šé¢çš„simhahå’Œminhash å°±æ˜¯è¯¥æ€æƒ³çš„å®žçŽ°ã€‚ è·ç¦»å‡½æ•°è¿™é‡Œçš„è·ç¦»å‡½æ•°éƒ½æ˜¯ç”¨æ¥æ–‡æœ¬ç›¸ä¼¼åº¦ã€‚ Jaccardç›¸ä¼¼åº¦ç®€å•æ¥è¯´äº¤é›†é™¤ä»¥å¹¶é›†ã€‚è¿™ä¸ªé›†åˆä¸­å­˜æ”¾çš„æ˜¯æ–‡ç« æˆ–è€…æ®µè½çš„å…³é”®è¯ã€‚1234567891011def JaccardSim(str_a, str_b): &apos;&apos;&apos; Jaccardç›¸ä¼¼æ€§ç³»æ•° è®¡ç®—saå’Œsbçš„ç›¸ä¼¼åº¦ lenï¼ˆsa &amp; sbï¼‰/ lenï¼ˆsa | sbï¼‰ &apos;&apos;&apos; seta = splitWords(str_a)[1] setb = splitWords(str_b)[1] sa_sb = 1.0 * len(seta &amp; setb) / len(seta | setb) return sa_sb å¯ä»¥çœ‹åˆ°æ ¸å¿ƒä»£ç å¾ˆç®€å•ï¼Œç»è¿‡åˆ†è¯ä¹‹åŽï¼Œå°±æ˜¯seta å’Œsetb è¿›è¡Œçš„æ“ä½œã€‚ cosine12345def cos_sim(a, b): a = np.array(a) b = np.array(b) # return &#123;&quot;æ–‡æœ¬çš„ä½™å¼¦ç›¸ä¼¼åº¦:&quot;:np.sum(a*b) / (np.sqrt(np.sum(a ** 2)) * np.sqrt(np.sum(b ** 2)))&#125; return np.sum(a * b) / (np.sqrt(np.sum(a ** 2)) * np.sqrt(np.sum(b ** 2))) å°†æ–‡æœ¬çš„å…³é”®è¯æ˜ å°„æˆæŸç§é«˜ç»´å‡½æ•°ï¼Œç„¶åŽåœ¨é«˜ç»´ç©ºé—´ä¸­è®¡ç®—ä¸¤è€…çš„ç›¸ä¼¼åº¦ã€‚ tf-idfåœ¨simhash ä¸­ä½¿ç”¨ tf-idfä½œä¸ºæˆ‘ä»¬çš„æ¯”è¾ƒå‡½æ•°ã€‚TF-IDFçš„ä¸»è¦æ€æƒ³å°±æ˜¯ï¼šå¦‚æžœæŸä¸ªè¯åœ¨ä¸€ç¯‡æ–‡æ¡£ä¸­å‡ºçŽ°çš„é¢‘çŽ‡é«˜ï¼Œä¹Ÿå³TFé«˜ï¼›å¹¶ä¸”åœ¨è¯­æ–™åº“ä¸­å…¶ä»–æ–‡æ¡£ä¸­å¾ˆå°‘å‡ºçŽ°ï¼Œå³DFçš„ä½Žï¼Œä¹Ÿå³IDFé«˜ï¼Œåˆ™è®¤ä¸ºè¿™ä¸ªè¯å…·æœ‰å¾ˆå¥½çš„ç±»åˆ«åŒºåˆ†èƒ½åŠ›ã€‚è¯é¢‘(term frequency)æœ‰ä¸¤ç§è®¡ç®—æ–¹å¼,åŽè€…è€ƒè™‘äº†ç›¸å¯¹çš„æƒ…å†µã€‚è®¡ç®—idf(inverse document frequency):TF-IDF ä¼˜ç‚¹æ˜¯ç®€å•å¿«é€Ÿï¼Œæ¯”è¾ƒç¬¦åˆå®žé™…ã€‚ç¼ºç‚¹ï¼Œæ— æ³•ä½“çŽ°è¯çš„ä½ç½®ä¿¡æ¯ï¼Œæ‰€æœ‰çš„ä½ç½®éƒ½æ˜¯è¢«è®¤ä¸ºé‡è¦æ€§ç›¸åŒï¼Œä½†æ˜¯å¼€å¤´ç»“å°¾ï¼Œæ®µè½çš„å¼€å¤´å’Œæ®µè½çš„ç»“å°¾ï¼Œthereforeï¼Œsoï¼Œbutè¿™äº›è¯è¯­éƒ½æ˜¯æ²¡æœ‰ä½“çŽ°çš„ã€‚ Hamming distancehamming distanceå°±æ˜¯æ¯”è¾ƒ01ä¸²çš„ä¸åŒï¼ŒæŒ‰ç…§ä½è¿›è¡Œæ¯”è¾ƒã€‚ç®—æ³•ï¼šå¼‚æˆ–æ—¶ï¼Œåªæœ‰åœ¨ä¸¤ä¸ªæ¯”è¾ƒçš„ä½ä¸åŒæ—¶å…¶ç»“æžœæ˜¯1 ï¼Œå¦åˆ™ç»“æžœä¸º0ï¼Œä¸¤ä¸ªäºŒè¿›åˆ¶â€œå¼‚æˆ–â€åŽå¾—åˆ°1çš„ä¸ªæ•°å³ä¸ºæµ·æ˜Žè·ç¦»çš„å¤§å°ã€‚123456789101112131415161718hashbits =64 # ä½¿ç”¨64ä½è¿›è¡Œç¼–ç def simhash_function(tokens, weights_dict): v = [0] * hashbits # è¿™ç§ &#123;key: value&#125;.item() çš„æ“ä½œä¹Ÿæ˜¯æ²¡æœ‰äº†è°äº† for key, t in &#123;x: _string_hash(x) for x in tokens&#125;.items(): for i in range(hashbits): bitmask = 1 &lt;&lt; i if t &amp; bitmask: v[i] += weights_dict[key] else: v[i] -= weights_dict[key] fingerprint = 0 for i in range(hashbits): if v[i] &gt;= 0: fingerprint += 1 &lt;&lt; i return fingerprintfingerprint = simhash_function(tokens, weights) min edit distance123456789101112131415161718192021222324# æœ€å°ç¼–è¾‘è·ç¦»def min_edit_distance(str1, str2): rows =len(str2) +1 cols =len(str1) +1 arr =[[0 for _ in range(cols)] for _ in range(rows)] # è¿™ç§ç®€æ´çš„ä»£ç ä¹Ÿæ˜¯ç‰›é€¼ for j in range(cols): arr[0][j] =j for i in range(rows): arr[i][0] =i for i in range(1, rows): for j in range(1, cols): # å› ä¸ºstring æ˜¯ä»Ž0 ï¼Œlen(str) -1çš„ if str2[i-1] ==str1[j-1]: arr[i][j] =arr[i-1][j-1] else: # ä»¥åŽè§åˆ°è¿™æ ·çš„å¼å­ï¼Œå°±è¦æƒ³åˆ°è¿™ä¸ªäºŒç»´çš„æ•°ç»„ï¼Œå› ä¸ºè¿™ä¸ªæ˜¯å¯ä»¥å¸®åŠ©è®°å¿†çš„ arr[i][j] =1 +min(arr[i-1][j-1], arr[i-1][j], arr[i][j-1]) # å³ä¸‹è§’å°±æ˜¯è·ç¦» return arr[rows-1][cols-1]str_a =&quot;abcdef&quot;str_b =&quot;azced&quot;result =min_edit_distance(str_a, str_b)result å…·ä½“å¯ä»¥å‚çœ‹è¯¥è§†é¢‘è®²è§£ã€‚(ps. å¦‚æžœåˆ·leetcode,ä¹Ÿå¯ä»¥å‚çœ‹è¯¥è§†é¢‘) åˆ†è¯åœ¨è‹±æ–‡ä¸­å­˜åœ¨å¤©ç„¶çš„ç©ºæ ¼å¯ä»¥è¿›è¡Œåˆ†è¯æ“ä½œï¼Œä½†æ˜¯ä¸­æ–‡çš„åˆ†è¯å°±æ¯”è¾ƒå¤æ‚äº†ã€‚å¸¸ç”¨çš„ä¸­æ–‡åˆ†è¯å¼€æºå·¥å…·æœ‰ jiebaå’ŒHanLPå‰è€…ç®€å•æ˜“è¡Œï¼Œå®¹æ˜“ä¸Šæ‰‹ï¼›åŽè€…åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä½œä¸ºæ±‰è¯­è¨€å¤„ç†åŒ…ï¼Œå¯ä»¥ç”¨äºŽè¯æ€§æ ‡æ³¨ï¼Œå‘½åå®žä½“è¯†åˆ«ç­‰ä¸€ç³»åˆ—åŠŸèƒ½ã€‚å¸¸ç”¨çš„è‹±æ–‡åˆ†è¯ corenlp å€’æŽ’ç´¢å¼•å€’æŽ’ç´¢å¼•ä½¿ç”¨pythonåœ¨å®žçŽ°ä¸Šå°±æ˜¯ä¸€ä¸ªdictionary åµŒå¥—ä¸€ä¸ª set(). ä¸€èˆ¬çš„ç´¢å¼•éƒ½æ˜¯æ•°å­—æˆ–è€…è‹±æ–‡å­—æ¯æ˜ å°„å†…å®¹ï¼Œå…·ä½“åœ¨æ”¾åˆ°simhashçš„æƒ…æ™¯ä¸‹å°±æ˜¯ä½¿ç”¨æ–‡ç« çš„åºåˆ—å·å¯¹åº”æå–å‡ºæ¥çš„å…³é”®è¯ã€‚ä½†æ˜¯å€’æŽ’ç´¢å¼•å°±æ˜¯å…³é”®è¯å¯¹åº”æ–‡ç« çš„åºåˆ—å·ï¼Œç±»ä¼¼ä¸ŽåŽŸæ¥çš„â€å€¼â€å¯¹åº”è¿™â€é”®â€ï¼Œæ‰€ä»¥ç§°ä¹‹ä¸ºå€’æŽ’ç´¢å¼•ã€‚ä¸€èˆ¬ä½¿ç”¨åœ¨å¬å›žçš„åœºæ™¯ä¸‹ï¼Œä½¿ç”¨å…³é”®è¯ç„¶åŽå‡ºçŽ°äº†è¯¥å…³é”®è¯ä¸‹çš„index çš„é›†åˆã€‚å¯ä»¥å‚è€ƒè¿™ç¯‡æ–‡ç« ã€‚]]></content>
      <tags>
        <tag>æ–‡æœ¬ç›¸ä¼¼åº¦</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Differences Between l1 and l2 as Loss Function and Regularization]]></title>
    <url>%2F2018%2F07%2F21%2Fdifferences-between-l1-and-l2-as-loss-function-and-regularization%2F</url>
    <content type="text"><![CDATA[å¦‚æžœè¡¥äº¤ä½œä¸šä¹Ÿç®—ä½œä¸šçš„è¯ï¼Œé‚£ä¹ˆè¿™ç¯‡åšæ–‡å°±ç®—åšä½œä¸šã€‚L1 å’ŒL2 ä½œä¸ºLoss functionå’Œ regularizationï¼Œä¸ªäººæ„Ÿè§‰æ˜¯ä¸€ä¸ªç»å¸¸å®¹æ˜“æ··æ·†çš„æ¦‚å¿µã€‚ä½†æ˜¯å¦‚æžœè¯»è€…è§‰å¾—å¾ˆæ¸…æ¥šï¼Œé‚£ä¹ˆå°±å¯ä»¥è·³è¿‡äº†ã€‚æœ¬æ–‡å¤§é‡å€Ÿé‰´äºŽè¯¥åšå®¢ï¼ŒåŽŸæ–‡æ˜¯è‹±æ–‡ï¼Œå¦‚æžœè¯»è€…è‹±æ–‡èƒ½å¤Ÿhandleï¼Œå»ºè®®è¯»åŽŸæ–‡ã€‚ As loss functionloss function or error function æ˜¯ç”¨æ¥è¡¡é‡çœŸå®žy å’Œç”Ÿæˆçš„f(x) ä¹‹é—´å·®è·çš„å‡½æ•°ã€‚åœ¨æ¨¡åž‹è®­ç»ƒä¸­æˆ‘ä»¬ä¸€èˆ¬æƒ…å†µä¸‹ä¸æ–­è®­ç»ƒæ¨¡åž‹ä½¿å¾—loss functionä¸æ–­ä¸‹é™ï¼ˆå¦‚æžœtaskè¦æ±‚loss functionæ˜¯å¢žå¤§ï¼Œè¿™æ—¶å€™ä¸€èˆ¬åŠ ä¸Šç¬¦å·æˆ–è€…è½¬æ¢æˆ 1- loss fucntionï¼Œæœ€åŽå®žçŽ°çš„è¿˜æ˜¯loss functionä¸‹é™ï¼‰ã€‚å¥½çš„å›žåˆ°L1 loss functionå’ŒL2 loss function. L1-norm loss function is also known as least absolute deviations(LAD), least absolute errors. It is basically minimizing the sum of the absolute differences between the target value Y and the estimated values (f(x)).L2-norm loss function is also known as least squares error(LSE). It is basically minimizing the sum of the square of the differences between the target value Y and the estimated values (f(x)).ä¸»è¦ä½ ä»Žä¸€ä¸‹ä¸‰ä¸ªæŒ‡æ ‡åŽ»è¡¡é‡ä¸¤è€…çš„ä¸åŒï¼š robustnessï¼Œstabilityå’Œæ˜¯å¦å…·æœ‰å”¯ä¸€è§£ã€‚wiki ä¸­å…³äºŽrobust ä¸­æ˜¯è¿™æ ·å®šä¹‰ï¼š A learning algorithm that can reduce the chance of fitting noise is called robustã€‚å…·æœ‰æ›´å¥½çš„æ³›åŒ–æ€§èƒ½ï¼Œä¸åŽ»è¿‡åº¦æ‹Ÿåˆnoise. å…³äºŽstability) wiki æ˜¯è¿™æ ·å®šä¹‰ï¼šStability, also known as algorithmic stability, is a notion in computational learning theory of how a machine learning algorithm is perturbed by small changes to its inputs. æˆ‘å¯¹äºŽå‰ä¸¤ä¸ªæŒ‡æ ‡çš„ç†è§£ï¼šrobustness æ˜¯å¯¹äºŽåŽŸå§‹çš„train æ ·æœ¬ä¸­ç¦»ç¾¤ç‚¹çš„æ€åº¦ï¼Œå¦‚æžœæŸä¸ªæ¨¡åž‹æ˜¯robustnessçš„ï¼Œé‚£ä¹ˆå¯¹äºŽè¯¥æ•°æ®é›†ä¸­çš„ç¦»ç¾¤ç‚¹æ˜¯èƒ½å¤ŸæŠ—å¹²æ‰°çš„ã€‚åä¹‹åˆ™æ˜¯ä¸å…·æœ‰robustnessçš„ã€‚stabilityæ˜¯å¯¹äºŽåŽŸå§‹train æ•°æ®çš„è½»å¾®çš„å¹³ç§»çš„ååº”ï¼Œå¦‚æžœå¯¹äºŽæŸä¸ªåŽŸå§‹æ•°æ®çš„è½»å¾®å¹³ç§»ï¼Œæœ€åŽçš„ç»“æžœæ²¡æœ‰äº§ç”Ÿå¾ˆå¤§çš„æ³¢åŠ¨ï¼Œé‚£ä¹ˆè¯¥æ¨¡åž‹å°±æ˜¯å…·æœ‰stability, åä¹‹ï¼Œåˆ™ä¸å…·æœ‰stability. æ¯”è¾ƒL1-norm å’ŒL2-normåœ¨å‰ä¸¤ä¸ªè¯„ä»·æŒ‡æ ‡ä¸­çš„è¡¨çŽ°ï¼šå¯¹äºŽç¬¬ä¸‰ç‚¹ï¼Œæˆ‘æƒ³åœ¨ä¸‹é¢è¿›è¡Œä»‹ç»ã€‚å› ä¸ºè¿™ç‚¹å’ŒåŽé¢å’Œä¸‹é¢çš„solution uniquenessæ˜¯ç›¸åŒçš„ã€‚ As regularizationä»ŽXGBoostè°ƒå‚æŒ‡å—ä¸­æˆ‘ä»¬çŸ¥é“objective function = loss funcion + regularization. è€Œæˆ‘ä»¬å¤§å¤šæ•°æƒ…å†µä¸‹æåŠçš„éƒ½æ˜¯loss function,å¸¸å¸¸å¿½ç•¥äº†regularization çš„ä½œç”¨ã€‚æ‰€ä»¥æ­£ç¡®çš„å§¿æ€åº”è¯¥æ˜¯è¿™æ ·çš„ï¼šThe regularization term controls the complexity of the model, which helps us to avoid overfitting.å¯¹äºŽæ¨¡åž‹è®­ç»ƒï¼Œä¸€å¼€å§‹çš„æƒ³æ³•æ˜¯å°½é‡çš„overfitting, å› ä¸ºå°±çŽ°åœ¨ä¸æˆç†Ÿçš„ç»éªŒè€Œè¨€ï¼Œå¯¹äºŽoverfittingè¿™ä¸ªé—®é¢˜æœ‰å¾ˆå¤šå¤„ç†æ–¹æ³•ï¼Œæ¯”å¦‚å·ç§¯æ·±åº¦ç¥žç»ç½‘ç»œä¸­çš„dropout, LightGBMä¸­çš„early stop å’Œéšæœºé‡‡æ ·çš„æ€æƒ³ã€‚ è¿™äº›æ–¹æ³•éƒ½æ˜¯å¯ä»¥ç¼“è§£overfittingï¼Œæ‰€ä»¥å¯ä»¥å‡ºçŽ°overfittingã€‚ç›¸åï¼Œå¦‚æžœä½ çš„æ¨¡åž‹æ˜¯underfittingï¼Œé‚£ä¹ˆä½ å°±å¾®æ˜¾å°´å°¬äº†ã€‚å¥½ï¼Œæ”¶å›žåˆ°L1 and L2ã€‚ å…ˆä¸Šå…¬å¼L1 regularization on least squares:L2 regularization on least squares:The difference between their properties can be promptly summarized as follows: å¯¹äºŽç¬¬ä¸€ç‚¹computational efficientçš„ç†è§£ï¼šå¹³æ–¹æ¯”ç»å¯¹å€¼æ›´å®¹æ˜“è®¡ç®—ï¼Œå¹³æ–¹å¯ä»¥æ±‚å¯¼ç›´æŽ¥æ±‚æœ€å€¼ï¼Œä½†æ˜¯ç»å¯¹å€¼å°±æ— æ³•æ±‚å¯¼ã€‚å¹¶ä¸”L1 regularizationåœ¨ non-sparse casesä¸­æ˜¯ computational inefficientï¼Œä½†æ˜¯åœ¨ sparse(0æ¯”è¾ƒå¤š) casesä¸­æ˜¯æœ‰ç›¸åº”çš„ç¨€ç–ç®—æ³•æ¥è¿›è¡Œä¼˜åŒ–çš„ï¼Œæ‰€ä»¥æ˜¯computational efficient.å¯¹äºŽç¬¬äºŒç‚¹æ˜¯å¦å…·æœ‰sparse solutionå¯ä»¥ä»Žå‡ ä½•æ„ä¹‰çš„è§’åº¦è§£è¯»ï¼šThe green line(L2-norm) is the unique shortest path, while the red, blue yellow(L1-norm) are all same length for the same route. Built-in feature selection is frequently mentioned as a useful property of the L1-norm, which the L2-norm does not. This is actually a result of the L1-norm, which tends to produces sparse coefficients (explained below). Suppose the model have 100 coefficients but only 10 of them have non-zero coefficients, this is effectively saying that â€œthe other 90 predictors are useless in predicting the target valuesâ€. L2-norm produces non-sparse coefficients, so does not have this property. æ‰€ä»¥è¡¨æ ¼ä¸­ç¬¬ä¸‰ç‚¹ä¹Ÿæ˜¯é¡ºç†æˆç« çš„äº†ã€‚è‡³æ­¤ï¼Œæˆ‘ä»¬åŒºåˆ†äº†L1-norm vs L2-norm loss function å’ŒL1-regularization vs L2-regularizationã€‚]]></content>
      <categories>
        <category>æœºå™¨å­¦ä¹ </category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[LightGBMå’ŒXGBooståŠå…¶è°ƒå‚]]></title>
    <url>%2F2018%2F07%2F21%2FLightGBM%E5%92%8CXGBoost%E5%8F%8A%E5%85%B6%E8%B0%83%E5%8F%82%2F</url>
    <content type="text"><![CDATA[lightGBMè°ƒå‚(å¸¸ç”¨å‚æ•°)Since lightGBM is based on decision tree algorithms, it splits the tree with the best fit whereas boosting algorithms split the tree depth wise or level wise rather than leaf-wise. So when growing on the same leaf in Light GBM, the leaf-wise algorithm can reduce more loss than the level-wise algorithm and hence results in much better accuracy which can rarely be achieved by any of the existing boosting algorithms. Also, it is surprisingly very fast, hence the word â€˜Lightâ€™.Leaf wise splits lead to increase in complexity and may lead to overfitting and it can be overcome by specifying another parameter max_depth parameter. Advantages of LightGBM faster training speed and higher efficiencyLight GBM use histogram based algorithm i.e it buckets continuous feature values into discrete bins which fasten the training procedure. lower memory usageReplaces continuous values to discrete bins which result in lower memory usage. better accuracy than any other boosting algorithmIt produces much more complex trees by following leaf wise split approach rather than a level-wise approach which is the main factor in achieving higher accuracy. compatibility with large datasetsIt is capable of performing equally good with large datasets with a significant reduction in training time as compared to XGBOOST. parallel learning supported lightGBMè°ƒå‚(å¸¸ç”¨å‚æ•°) taskdefault= train, option: train, prediction applicationdefault= regression, option: regression, binary, multiclass, lambdarank(lambdarank application) datatraining data, è¿™ä¸ªæ¯”è¾ƒè¯¡å¼‚ï¼Œä½ éœ€è¦åˆ›å»ºä¸€ä¸ªlightGBMç±»åž‹çš„data num_iterationsdefault =100, å¯ä»¥è®¾ç½®ä¸ºçš„å¤§ä¸€äº›ï¼Œç„¶åŽä½¿ç”¨early_stoppingè¿›è¡Œè°ƒèŠ‚ã€‚ early_stopping_rounddefault =0, will stop training if one metric of one validation data doesnâ€™t improve in last early_stopping_round rounds. num_leavesdefault =31, number of leaves in a tree devicedefault =cpu, options: gpu, cpu, choose gpu for faster training. max_depthspecify the max depth to which tree will grow, which is very important. feature_fractiondefault =1, specifies the fraction of features to be taken for each iteration. bagging_fractiondefault =1, spefifies the fraction of data to be used for each iteration and it generally used to speed up the training and avoid overfitting. max_binmax number of bins to bucket the feature values.å› ä¸ºæ¨¡åž‹æ˜¯åŸºäºŽbinè®­ç»ƒçš„ï¼Œå¦‚æžœbin æ•°é‡è¶Šå¤šï¼Œå¾—åˆ°better accuracy,åŒæ—¶æ›´åŠ å®¹æ˜“ overfitting. num_threads labelspecify the label columns. categorical_featurespecify the categorical features num_classdefault =1, used only for multi-class classification referrencewhich-algorithm-takes-the-crown-light-gbm-vs-xgboostLightGBM å¦‚ä½•è°ƒå‚å®˜æ–¹æ–‡æ¡£param_tuningå®˜æ–¹æ–‡æ¡£parameter XGBoostè°ƒå‚Advantage of XGBoost regularizationstandard GBM implementation has no regularization, in fact, XGBoost is also known as â€˜regularized boostingâ€™ technique. parallel processingwe know that boosting is sequential process so how can it be parallelized? this link to explore further. high flexibilityXGBoost allow users to define custom optimization objectives and evaluation criteria handling missing valuesvery useful property. XGBoost has an in-built routine to handle missing values. User is required to supply a different value than other observations and pass that as a parameter. XGBoost tries different things as it encounter a missing value on each node and learns which path to take for missing values in future. Tree pruningA GBM would stop splitting a node when it encounters a negative loss in the split. Thus it is more of a greedy algorithm. XGBoost on the other hand make splits upon the max_depth specified and then start pruning the tree backwards and remove splits beyond which there is no positive gain. built-in cross-validationThis is unlike GBM where we have to run a grid-search and only a limited values can be tested. continue on existing model XGBoost Parametersgeneral parametersGeneral Parameters: Guide the overall functioning booster:default =gbtree, can be gbtree, gblinear or dart. ä¸€èˆ¬ä½¿ç”¨gbtree. silent:default =0, silent mode is activated if set to 1(no running messages will be printed) nthread:default to maximum of threads. booster parametersBooster Parameters: Guide the individual booster (tree/regression) at each step eta(learning rate):default=0.3, typical final values to be used: 0.01-0.2, using CV to tune min_child_weight:minimum sum of instance weight (hessian) needed in a child. If the tree partition step results in a leaf node with the sum of instance weight less than min_child_weight, then the building process will give up further partitioning. In linear regression mode, this simply corresponds to minimum number of instances needed to be in each node. The larger, the more conservative the algorithm will be.default =1,too high values can lead to under-fitting, it should be tuned using CV. æ•°å€¼è¶Šå°è¶Šå®¹æ˜“è¿‡æ‹Ÿåˆï¼Œè¶Šå¤§è¶Šå®¹æ˜“ under-fitting. max_depth:default =6, typical values: 3-10, should be tuned using CV. gamma:default =0, Gamma specifies the minimum loss reduction required to make a split.å¦‚æžœåœ¨åˆ†è£‚è¿‡ç¨‹ä¸­å°äºŽè¯¥å€¼ï¼Œé‚£ä¹ˆå°±ä¸ä¼šç»§ç»­åˆ†è£‚ã€‚ subsample:default =1, typical values: 0.5-1. Denotes the fraction of observations to be randomly samples for each tree. colsample_bytree:default =1, typical values: 0.5-1. colsample_bytreeå’Œsubsampleä¸åŒç‚¹ï¼šcolsample_byæ˜¯ç‰¹å¾çš„éšæœºfraction, subsampleæ˜¯rowsçš„éšæœºfractionã€‚ lambda:default =1, L2 regularization term on weights(analogous to Ridge regression). Though many data scientists donâ€™t use it often, it should be explored to reduce overfitting. alpha:default =0, L1 regularization term on weight (analogous to Lasso regression) scale_pos_weight:default =1, a value greater than 0 should be used in case of high class imbalance as it helps in faster convergence. learning task parametersLearning Task Parameters: Guide the optimization performed objectivebinary: logistic- returns predicated probability(not class)multi: softmax- returns predicated class(not probabilities)multi: softprob- returns predicated probability of each data point belonging to each class. eval_metircdefault according to objective(rmse for regression and error for classification), used for validation data.typical values: rmse(root mean square error), mse(mean absolute error), logloss(negative log-likelihood), error(binary classification error rate, 0.5 threshold), auc(area under the curve) seeddefault =0, used for reproducible results and also for parameter tuning. Control OverfittingThere are in general two ways that you can control overfitting in xgboost. The first way is to directly control model complexity. The second way is to add regularization parameters Referrencecomplete guide parameter tuning xgboost with codes pythonå®˜æ–¹æ–‡æ¡£ param_tuningå®˜æ–¹æ–‡æ¡£ parameter]]></content>
      <categories>
        <category>æœºå™¨å­¦ä¹ </category>
      </categories>
      <tags>
        <tag>LightGBM</tag>
        <tag>XGBoost</tag>
        <tag>è°ƒå‚</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[é‚£äº›å¹´çš„ç®—æ³•é¢˜ç›®ï¼ˆäºŒï¼‰]]></title>
    <url>%2F2018%2F07%2F21%2F%E9%82%A3%E4%BA%9B%E5%B9%B4%E7%9A%84%E7%AE%97%E6%B3%95%E9%A2%98%E7%9B%AE%EF%BC%88%E4%BA%8C%EF%BC%89%2F</url>
    <content type="text"><![CDATA[æœ€é•¿01ç›¸åŒå­ä¸²å·²çŸ¥ä¸€ä¸ªé•¿åº¦ä¸ºNçš„å­—ç¬¦ä¸²ï¼Œåªç”±0å’Œ1ç»„æˆï¼Œ æ±‚ä¸€ä¸ªæœ€é•¿çš„å­ä¸²ï¼Œè¦æ±‚è¯¥å­ä¸²å‡ºçŽ°0å’Œ1çš„æ¬¡æ•°ç›¸ç­‰ã€‚æ€è·¯ï¼šæœ€ç®€å•çš„æ–¹å¼æ˜¯å…ˆç”Ÿæˆå­—ä¸²ï¼Œç„¶åŽåˆ¤æ–­æ¯ä¸ªå­—ä¸²æ˜¯å¦æ»¡è¶³0çš„ä¸ªæ•°å’Œ1çš„ä¸ªæ•°ç›¸åŒã€‚è¿™ç§æš´åŠ›æ±‚è§£æ—¶é—´å¤æ‚åº¦O(n^3),æ˜Žæ˜¾æ˜¯ä¸åˆç†çš„ã€‚ä¸‹é¢è¯´ä¸€ä¸‹ç®€å•çš„åšæ³•ï¼šå®šä¹‰ä¸€ä¸ªæ•°ç»„B[N]ï¼ŒB[i]è¡¨ç¤ºä»ŽA[0â€¦i]ä¸­ num_of_0 - num_of_1ï¼Œ0çš„ä¸ªæ•°ä¸Ž1çš„ä¸ªæ•°çš„å·® ã€‚é‚£ä¹ˆå¦‚æžœA[i] ~ A[j]æ˜¯ç¬¦åˆæ¡ä»¶çš„å­ä¸²ï¼Œä¸€å®šæœ‰ B[i] == B[j]ï¼Œå› ä¸ºä¸­é—´çš„éƒ¨åˆ†0ã€1ä¸ªæ•°ç›¸ç­‰ï¼Œç›¸å‡ç­‰äºŽ0ã€‚ æ—¶é—´å¤æ‚åº¦ï¼šO(n)ï¼Œç©ºé—´å¤æ‚åº¦ï¼šO(n) ç®—æ³•çš„æ€è·¯æ˜¯ä¸€æ ·çš„ã€‚ Following is a solution that uses O(n) extra space and solves the problem in O(n) time complexity.Let input array be arr[] of size n and maxsize be the size of output subarray.1) Consider all 0 values as -1. The problem now reduces to find out the maximum length subarray with sum = 0.2) Create a temporary array sumleft[] of size n. Store the sum of all elements from arr[0] to arr[i] in sumleft[i]. This can be done in O(n) time.3) There are two cases, the output subarray may start from 0th index or may start from some other index. We will return the max of the values obtained by two cases.4) To find the maximum length subarray starting from 0th index, scan the sumleft[] and find the maximum i where sumleft[i] = 0.5) Now, we need to find the subarray where subarray sum is 0 and start index is not 0. This problem is equivalent to finding two indexes i &amp; j in sumleft[] such that sumleft[i] = sumleft[j] and j-i is maximum. To solve this, we can create a hash table with size = max-min+1 where min is the minimum value in the sumleft[] and max is the maximum value in the sumleft[]. The idea is to hash the leftmost occurrences of all different values in sumleft[]. The size of hash is chosen as max-min+1 because there can be these many different possible values in sumleft[]. Initialize all values in hash as -16) To fill and use hash[], traverse sumleft[] from 0 to n-1. If a value is not present in hash[], then store its index in hash. If the value is present, then calculate the difference of current index of sumleft[] and previously stored value in hash[]. If this difference is more than maxsize, then update the maxsize.7) To handle corner cases (all 1s and all 0s), we initialize maxsize as -1. If the maxsize remains -1, then print there is no such subarray. ä»£ç å®žçŽ°12345678910111213141516171819202122232425262728def lengest01SubStr(s): &apos;&apos;&apos; æœ€é•¿0,1 ç›¸ç­‰çš„å­ä¸²é•¿åº¦ &apos;&apos;&apos; count =[0, 0] B =[0]*len(s) dic =&#123;&#125; # ä¿å­˜ 0 1 çš„å·®å€¼ lengest =0 for i in range(len(s)): count[int(s[i])] +=1 B[i] =count[0] - count[1] # start from 0th index if B[i] ==0: lengest +=1 continue if B[i] in dic: # i -dic[B[i]] , not from 0th index lengest =max(lengest, i- dic[B[i]]) else: dic[B[i]] =i return lengesta =&apos;1011010&apos;b =&apos;10110100&apos;print(lengest01SubStr(a)) # 6 # &apos;011010&apos;print(lengest01SubStr(b)) # 8 # &apos;10110100&apos; é¡ºæ—¶é’ˆæ‰“å°çŸ©é˜µè¾“å…¥ä¸€ä¸ªçŸ©é˜µï¼ŒæŒ‰ç…§ä»Žå¤–å‘é‡Œä»¥é¡ºæ—¶é’ˆçš„é¡ºåºä¾æ¬¡æ‰«å°å‡ºæ¯ä¸€ä¸ªæ•°å­—ã€‚æ€è·¯ï¼šå‘çŽ°ç½‘ä¸Šæœ‰å¾ˆå¤šä½¿ç”¨é€’å½’çš„ï¼Œä½†æ˜¯ä½¿ç”¨å››ä¸ªå¾ªçŽ¯å°±å¯ä»¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚æ‰¾åˆ°æ¯æ¬¡å¼€å§‹çš„èµ·ç‚¹ï¼Œç„¶åŽæŒ‰ç…§æœ€ä¸Šé¢ä¸€è¡Œï¼Œæœ€å³é¢ä¸€åˆ—ï¼Œæœ€å°é¢ä¸€è¡Œå’Œæœ€å·¦é¢ä¸€è¡Œè¿™æ ·çš„é¡ºåºè¿›è¡Œæ‰“å°å³å¯ã€‚ æ€è·¯ï¼šæ‰¾åˆ°å·¦ä¸Šè§’çš„ï¼Œä¸€ä¸ªstart_pointï¼Œ ç„¶åŽæ ¹æ®è¿™ä¸ªç‚¹è¿›è¡Œä¸Šä¸‹å·¦å³çš„å¾ªçŽ¯ã€‚ 123456789101112131415161718192021222324252627282930313233343536373839404142def printMatrix(matrix): if matrix ==[[]]: return # ç¬¬ä¸€æ¬¡è§è¿™æ ·åˆ¤æ–­ç©ºçš„matrix row =len(matrix) column =len(matrix[0]) # è¿™é‡Œçš„left, right, up, down éƒ½æ˜¯çœŸå®žèƒ½å¤Ÿaccessåˆ°æ•°æ®çš„ left =0 right =column -1 up =0 down =row -1 res =[] while right &gt;left and up &lt;down: # from left to right for i in range(left, right+1): res.append(matrix[up][i]) # from up to down for i in range(up+1, down+1): res.append(matrix[i][right]) # from right to left for i in range(right-1, left-1, -1): res.append(matrix[down][i]) for i in range(down-1, up, -1): res.append(matrix[i][left]) left +=1 right -=1 up +=1 down -=1 # æœ€åŽå¯¹äºŽè¿™ç§ç‰¹æ®Šæƒ…å†µçš„å¤„ç†æ˜¯å®¹æ˜“å¿˜è®°çš„ # left one row è¿™ç§æƒ…å†µå¾ˆç‰¹æ®Šï¼Œåªæ˜¯ä»Žå·¦å¾€å³éåŽ† if up ==down and left &lt;right: for i in range(left, right+1): res.append(matrix[up][i]) # left one column åªæœ‰å¯èƒ½æ˜¯ä»Žä¸Šå¾€ä¸‹éåŽ† if left ==right and up &lt;down: for i in range(up, down+1): res.append(matrix[i][left]) if up ==down and left ==right: res.append(matrix[left][up]) return resprint(printMatrix(matrix)) ä¸‹é¢è¿™ä¸ªç‰ˆæœ¬å¹¶æ²¡æœ‰è¿è¡ŒæˆåŠŸï¼Œä½†æ˜¯ä¸­é—´æœ‰ä¸ªè¯­æ³•ç‚¹æ˜¯å¯ä»¥å­¦ä¹ çš„ã€‚12345678910111213141516171819202122232425262728293031323334353637def printMatrix(matrix): res =[] # ç¬¬ä¸€ä¸ªåæ ‡è¡¨ç¤ºè¡Œæ•°ï¼Œç¬¬äºŒä¸ªåæ ‡è¡¨ç¤ºåˆ—æ•° # m è¡¨ç¤ºè¡Œæ•°ï¼Œn è¡¨ç¤ºåˆ—æ•° m =len(matrix) n = len(matrix[0]) if m ==1 and n ==1: res =[matrix[0][0]] return res else: for o in range(int((min(m,n)+1)/2)): # ä¸åŠ è¿™ä¸ª[] ä¼šæœ‰è¯­æ³•é”™è¯¯ [res.append(matrix[o][i]) for i in range(o, n-o)] [res.append(matrix[j][n-o-1]) for j in range(o, m-o) if matrix[j][n-o-1] not in res ] # æ„Ÿè§‰è¿™ä¸ªæœ€åŽçš„not in æ˜¯ä¸åˆç†çš„ï¼Œä¸‡ä¸€è¿™ä¸¤ä¸ªæ•°å­—å°±æ˜¯ç›¸åŒå‘¢ï¼Œè¯•ä¸€ä¸‹ [res.append(matrix[m-o-1][k]) for k in range(n-1, o-1, -1) if matrix[m-o-1][k] not in res] [res.append(matrix[l][o]) for l in range(m-1-o, o-1,-1) if matrix[l][o] not in res] # è¿™ç§å†™æ³•è¿˜æ˜¯å°‘ç”¨ï¼Œå¯ä»¥ä½¿ç”¨append,å°±å°‘ç”¨è¿™ç§ï¼Œå¯è¯»æ€§ä¸å¼ºï¼Œå®¹æ˜“é”™ return resdef printMatrix(matrix): res =[] # ç¬¬ä¸€ä¸ªåæ ‡è¡¨ç¤ºè¡Œæ•°ï¼Œç¬¬äºŒä¸ªåæ ‡è¡¨ç¤ºåˆ—æ•° # m è¡¨ç¤ºè¡Œæ•°ï¼Œn è¡¨ç¤ºåˆ—æ•° m =len(matrix) n = len(matrix[0]) if m ==1 and n ==1: res =[matrix[0][0]] return res else: for o in range(int((min(m,n)+1)/2)): # ä¸åŠ è¿™ä¸ª[] ä¼šæœ‰è¯­æ³•é”™è¯¯ [res.append(matrix[o][i]) for i in range(o, n-o)] [res.append(matrix[j][n-o-1]) for j in range(o, m-o) if matrix[j][n-o-1] not in res ] # æ„Ÿè§‰è¿™ä¸ªæœ€åŽçš„not in æ˜¯ä¸åˆç†çš„ï¼Œä¸‡ä¸€è¿™ä¸¤ä¸ªæ•°å­—å°±æ˜¯ç›¸åŒå‘¢ï¼Œè¯•ä¸€ä¸‹ [res.append(matrix[m-o-1][k]) for k in range(n-1, o-1, -1) if matrix[m-o-1][k] not in res] [res.append(matrix[l][o]) for l in range(m-1-o, o-1,-1) if matrix[l][o] not in res] # è¿™ç§å†™æ³•è¿˜æ˜¯å°‘ç”¨ï¼Œå¯ä»¥ä½¿ç”¨append,å°±å°‘ç”¨è¿™ç§ï¼Œå¯è¯»æ€§ä¸å¼ºï¼Œå®¹æ˜“é”™ return res]]></content>
      <categories>
        <category>ç®—æ³•</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Loss Activation and Optimisation Function]]></title>
    <url>%2F2018%2F07%2F07%2FLoss-Activation-and-Optimisation-Function%2F</url>
    <content type="text"><![CDATA[ç”¨å›¾è¯´è¯â€¦ Loss Function(Error Function)For accurate predictions, one needs to minimize the calculated error. In a neural network, this is done using back propagation.æŸå¤±å‡½æ•°æ˜¯ç”¨æ¥ä¼°é‡ä½ æ¨¡åž‹çš„é¢„æµ‹å€¼f(x)ä¸ŽçœŸå®žå€¼Yçš„ä¸ä¸€è‡´ç¨‹åº¦ï¼Œå®ƒæ˜¯ä¸€ä¸ªéžè´Ÿå®žå€¼å‡½æ•°,é€šå¸¸ä½¿ç”¨L(Y,f(x))æ¥è¡¨ç¤ºï¼ŒæŸå¤±å‡½æ•°è¶Šå°ï¼Œæ¨¡åž‹çš„é²æ£’æ€§å°±è¶Šå¥½ã€‚æŒ‰ç…§å‡½æ•°ç§ç±»å¯ä»¥åˆ’åˆ†ä¸€ä¸‹ä¸»è¦çš„å‡ ä¸ªç±»åˆ«ã€‚ logæŸå¤±å‡½æ•°å¸¸ç”¨äºŽé€»è¾‘å›žå½’ä¸­ã€‚ å¹³æ–¹æŸå¤±å‡½æ•°åˆç§°ä¸ºæœ€å°äºŒä¹˜æ³•ï¼Œæœ€ä¼˜æ‹Ÿåˆç›´çº¿åº”è¯¥æ˜¯ä½¿å„ç‚¹åˆ°å›žå½’ç›´çº¿çš„è·ç¦»å’Œæœ€å°çš„ç›´çº¿ï¼Œå³å¹³æ–¹å’Œæœ€å°ã€‚Y-f(X) è¡¨ç¤ºæ®‹å·®ï¼Œæ®‹å·®åœ¨æ•°ç†ç»Ÿè®¡ä¸­æ˜¯æŒ‡å®žé™…è§‚å¯Ÿå€¼ä¸Žä¼°è®¡å€¼ï¼ˆæ‹Ÿåˆå€¼ï¼‰ä¹‹é—´çš„å·®ã€‚æ•´ä¸ªå¼å­è¡¨ç¤ºçš„æ˜¯æ®‹å·®çš„å¹³æ–¹å’Œã€‚ æŒ‡æ•°æŸå¤±å‡½æ•°Adaboostçš„ç›®æ ‡å¼å­å°±æ˜¯æŒ‡æ•°æŸå¤±ï¼Œåœ¨ç»™å®šnä¸ªæ ·æœ¬çš„æƒ…å†µä¸‹ï¼ŒAdaboostçš„æŸå¤±å‡½æ•°ä¸ºï¼š HingeæŸå¤±å‡½æ•°å¬ç€åå­—æ€ªæ€ªçš„ï¼Œä½†æ˜¯SVMï¼ˆsupport vector machineï¼‰çš„æŸå¤±å‡½æ•°å°±æ˜¯è¿™ä¸ªã€‚ä»Žç›®æ ‡å‡½æ•°çœ‹æ¥ï¼Œlr é‡‡ç”¨çš„logistic loss å’Œ svm é‡‡ç”¨çš„ hinge loss function æ€æƒ³éƒ½æ˜¯å¢žåŠ å¯¹åˆ†ç±»å½±å“è¾ƒå¤§ç‚¹çš„æƒé‡ï¼Œå‡å°‘é‚£äº›ä¸Žåˆ†ç±»ç›¸å…³ä¸å¤§ç‚¹çš„æƒé‡ã€‚ä½†æ˜¯ä¸¤ä¸ªæ–¹æ³•å¤„ç†çš„æ–¹æ³•ä¸åŒ: LRé‡‡ç”¨ä¸€ä¸ªsigmodçš„æ˜ å°„å‡½æ•°ï¼Œé€šè¿‡è¿™æ ·çš„éžçº¿æ€§æ˜ å°„ï¼Œå¤§å¤§é™ä½Žäº†ç¦»åˆ†ç±»å¹³é¢ç‚¹è¿œçš„æƒé‡ ; SVM é‡‡ç”¨çš„æ˜¯ä¸€ä¸ªhinge loss functionï¼Œé€šè¿‡ä¸Šå›¾å¯ä»¥çœ‹åˆ°ï¼Œå¯¹äºŽé‚£äº›ç¦»è¶…å¹³é¢æ¯”è¾ƒè¿œçš„ç‚¹ï¼Œç›´æŽ¥è®¾ä¸º0äº†ï¼Œä¹Ÿå°±æ˜¯è¯´ç›´æŽ¥å¿½è§†ï¼Œåªè€ƒè™‘é‚£äº›å¯¹åˆ†ç±»å¹³é¢æœ‰å½±å“çš„ç‚¹ï¼Œè¿™äº›ç‚¹å°±æ˜¯æˆ‘ä»¬ç»å¸¸å¬åˆ°çš„æ”¯æŒå‘é‡ã€‚å¦ä¸€æ–¹é¢ï¼Œæ”¯æŒå‘é‡æœºåªè€ƒè™‘å±€éƒ¨çš„è¾¹ç•Œçº¿é™„è¿‘çš„ç‚¹ï¼Œè€Œé€»è¾‘å›žå½’è€ƒè™‘å…¨å±€ï¼ˆè¿œç¦»çš„ç‚¹å¯¹è¾¹ç•Œçº¿çš„ç¡®å®šä¹Ÿèµ·ä½œç”¨ï¼Œè™½ç„¶ä½œç”¨ä¼šç›¸å¯¹å°ä¸€äº›ï¼‰ã€‚ æŒ‰ç…§åº”ç”¨é¢†åŸŸå¯ä»¥åˆ’åˆ†ä¸º:Regressive loss functions:They are used in case of regressive problems, that is when the target variable is continuous. Most widely used regressive loss function is Mean Square Error.Other loss functions are:Absolute error â€” measures the mean absolute value of the element-wise difference between input.Classification loss functions:The output variable in classification problem is usually a probability value f(x), called the score for the input x. Generally, the magnitude of the score represents the confidence of our prediction. The target variable y, is a binary variable, 1 for true and -1 for false.On an example (x,y), the margin is defined as yf(x). The margin is a measure of how correct we are. Most classification losses mainly aim to maximize the margin. Some classification algorithms are: Binary Cross Entropy,Negative Log Likelihood,Margin Classifier and Soft Margin Classifier.Embedding loss functions:It deals with problems where we have to measure whether two inputs are similar or dissimilar. Some examples are: L1 Hinge Error- Calculates the L1 distance between two inputs. Cosine Error- Cosine distance between two inputs. Visualising Loss Functions:We performed the task to reconstruct an image using a type of neural network called Autoencoders. Different results were obtained for the same task by using different Loss Functions, while everything else in the neural network architecture remained constant. Thus, the difference in result represents the properties of the different loss functions employed. A very simple data set, MNIST data set was used for this purpose. Three loss functions were used to reconstruct images. Activation FunctionWhat?Itâ€™s just a thing (node) that you add to the output end of any neural network. It is also known as Transfer Function. It can also be attached in between two Neural Networks. Sigmoid or Logistic Activation Functionå¯¼æ•°æ¯”è¾ƒæœ‰ç‰¹ç‚¹ï¼š Softmax FunctionThe softmax function is a more generalized logistic activation function which is used for multiclass classification.ä½¿ç”¨softmaxå’Œå¤šä¸ªlogisticçš„å¤šåˆ†ç±»çš„åŒºåˆ«ï¼šsoftmaxå›žå½’è¿›è¡Œçš„å¤šåˆ†ç±»ï¼Œç±»ä¸Žç±»ä¹‹é—´æ˜¯äº’æ–¥çš„ï¼Œå³ä¸€ä¸ªè¾“å…¥åªèƒ½è¢«å½’ä¸ºä¸€ç±»ï¼›å¤šä¸ªlogisticå›žå½’è¿›è¡Œå¤šåˆ†ç±»ï¼Œè¾“å‡ºçš„ç±»åˆ«å¹¶ä¸æ˜¯äº’æ–¥çš„ï¼Œå³â€è‹¹æžœâ€è¿™ä¸ªè¯è¯­æ—¢å±žäºŽâ€æ°´æžœâ€ç±»ä¹Ÿå±žäºŽâ€3Câ€ç±»åˆ«ã€‚ Tanh ReLUReLU (Rectified Linear Unit) Activation Functionit is used in almost all the convolutional neural networks or deep learning.åœ¨å·ç§¯ç½‘ç»œå’Œæ·±åº¦ç½‘ç»œä¸­ç»å¸¸çœ‹åˆ°ã€‚ Leaky ReLUæ˜Žæ˜¾çš„å‘çŽ°ï¼ŒLeaky ReLUæ˜¯å¯¹äºŽåœ¨ X&lt;0æ—¶å€™çš„æ”¹è¿›ã€‚Both Leaky and Randomized ReLU functions are monotonic in nature. Also, their derivatives also monotonic in nature. Optimisation AlgorithmsOptimisation Algoritms are used to update weights and biases i.e. the internal parameters of a model to reduce the error. They can be divided into two categories:Back Propogation and Optimisation Function: Error J(w) is a function of internal parameters of model i.e weights and bias. For accurate predictions, one needs to minimize the calculated error. In a neural network, this is done using back propagation. The current error is typically propagated backwards to a previous layer, where it is used to modify the weights and bias in such a way that the error is minimized. The weights are modified using a function called Optimization Function.Optimisation functions usually calculate the gradient i.e. the partial derivative of loss function with respect to weights, and the weights are modified in the opposite direction of the calculated gradient. This cycle is repeated until we reach the minima of loss function. Constant Learning Rate AlgorithmsHere Î· is called as learning rate which is a hyperparameter that has to be tuned. Choosing a proper learning rate can be difficult.é€‰çš„å°è®­ç»ƒé€Ÿåº¦æ…¢ï¼Œ While a learning rate that is too large can hinder convergence and cause the loss function to fluctuate around the minimum or even to divergeA similar hyperparameter is momentum(åŠ¨é‡), which determines the velocity with which learning rate has to be increased as we approach the minima. Adaptive Learning Algorithms(è‡ªé€‚åº”)Adaptive gradient descent algorithms such as Adagrad, Adadelta, RMSprop, Adam, provide an alternative to classical SGD. They have per-paramter learning rate methods, which provide heuristic approach without requiring expensive work in tuning hyperparameters for the learning rate schedule manually. The challenge of using gradient descent is that their hyper parameters have to be defined in advance and they depend heavily on the type of model and problem. Another problem is that the same learning rate is applied to all parameter updates. If we have sparse data, we may want to update the parameters in different extent instead.(å‡è£…ç¿»è¯‘:gradient descent çš„è‰°éš¾ä¹‹å¤„åœ¨äºŽéœ€è¦æå‰ defineè¿™ç§hyper parameters, å¹¶ä¸”ä¸åŒæ•°æ®çš„learning rate åº”è¯¥æ˜¯ä¸åŒï¼ˆå­¦ä¹ çŽ‡åº”è¯¥æ ¹æ®æ•°æ®çš„ç¨€ç–ä¸Žå¦å˜åŒ–)) è¯´åˆ°è¿™æˆ‘ä»¬å°±å¤šè¯´ä¸€äº›å…³äºŽè‡ªé€‚åº”ç®—æ³•çš„å†…å®¹ã€‚We used three first order optimisation functions and studied their effect-Stochastic Gradient Decent, Adagrad and Adam. Gradient Descent calcultes gradient for the whole dataset and updates values in direction opposite to the gradients until we find a local minima. Stochastic Gradient Descent performs a parameter update for each training example unlike normal Gradient Descent which performs only one update. Thus it is much faster. Gradient Decent algorithms can further be improved by tuning important parametes like momentum, learning rate etc. Adagrad is more preferrable for a sparse data set as it makes big updates for infrequent parameters and small updates for frequent parameters. It uses a different learning Rate for every parameter Î¸ at a time step based on the past gradients which were computed for that parameter. Thus we do not need to manually tune the learning rate. Adam stands for Adaptive Moment Estimation. It also calculates different learning rate. Adam works well in practice, is faster, and outperforms other techniques. Stochastic Gradient Decent was much faster than the other algorithms but the results produced were far from optimum. Both, Adagrad and Adam produced better results that SGD, but they were computationally extensive. Adam was slightly faster than Adagrad. Thus, while using a particular optimization function, one has to make a trade off between more computation power and more optimum results. SGD å’Œ GDDçš„åŒºåˆ«ï¼šæ­£å¦‚ä¸Šæ‰€è¯´ï¼Œåœ¨âˆ‚E/âˆ‚wi=âˆ‘ï¼ˆh(x)-yï¼‰(xi) çš„æ—¶å€™âˆ‘è€—è´¹äº†å¤§é‡çš„æ—¶é—´ï¼Œç‰¹åˆ«æ˜¯åœ¨è®­ç»ƒé›†åºžå¤§çš„æ—¶å€™ã€‚æ‰€ä»¥è‚¯å®šæœ‰äººä¼šçŒœæƒ³ï¼Œå¦‚æžœæŠŠæ±‚å’ŒåŽ»æŽ‰å¦‚ä½•ï¼Œå³å˜ä¸ºâˆ‚E/âˆ‚wi=ï¼ˆh(x)-yï¼‰(xi)ã€‚ï¼ˆåªæ˜¯ä¸“æ³¨äºŽå½“å‰è®­ç»ƒè®­ç»ƒé›†åˆï¼Œå½“å‰çš„æ•°æ®ï¼‰å¯¹äºŽæ­¥é•¿Î·çš„å–å€¼ï¼Œæ ‡å‡†æ¢¯åº¦ä¸‹é™çš„Î·æ¯”éšæœºæ¢¯åº¦ä¸‹é™çš„å¤§ã€‚å› ä¸ºæ ‡å‡†æ¢¯åº¦ä¸‹é™çš„æ˜¯ä½¿ç”¨å‡†ç¡®çš„æ¢¯åº¦ï¼Œç†ç›´æ°”å£®åœ°èµ°ï¼Œéšæœºæ¢¯åº¦ä¸‹é™ä½¿ç”¨çš„æ˜¯è¿‘ä¼¼çš„æ¢¯åº¦ï¼Œå°±å¾—å°å¿ƒç¿¼ç¿¼åœ°èµ°ï¼Œæ€•ä¸€ä¸å°å¿ƒè¯¯å…¥æ­§é€”å—è¾•åŒ—è¾™äº†ã€‚ äº‹å®žè¯æ˜Žï¼šä¸­æ–‡å’Œè‹±æ–‡æ··åœ¨ä¸€èµ·ï¼ŒæŽ’ç‰ˆæ˜¯éš¾çœ‹çš„ã€‚]]></content>
      <categories>
        <category>æœºå™¨å­¦ä¹ </category>
      </categories>
      <tags>
        <tag>ä¼˜åŒ–</tag>
        <tag>Loss Function</tag>
        <tag>Activation Function</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ç‰¹å¾å·¥ç¨‹ç›¸å…³æ¦‚å¿µ]]></title>
    <url>%2F2018%2F06%2F29%2F%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[ç‰¹å¾ç¦»æ•£åŒ–ï¼Ÿè¿žç»­åŒ–ç‰¹å¾å°±æ˜¯ä¸€äº›ä¸å¯æžšä¸¾çš„æœ‰ç†æ•°ã€‚é‚£ä¹ˆä»€ä¹ˆæ˜¯ç¦»æ•£åŒ–ç‰¹å¾å‘¢ï¼Ÿ ç¦»æ•£åŒ–ç‰¹å¾å°±æ˜¯å¯æžšä¸¾çš„ç‰¹å¾ã€‚ç¦»æ•£åŒ–çš„ä½œç”¨æ˜¯æŠŠæ•°æ®å˜æˆå¯è®¡ç®—çŠ¶æ€ã€‚è€Œç‰¹å¾å·¥ç¨‹å°±æ˜¯ä»ŽåŽŸå§‹å­—æ®µä¸­æ ¹æ®ä¸šåŠ¡æå–å‡ºå¯¹æ¨¡åž‹æœ‰æ•ˆçš„ç‰¹å¾å‡ºæ¥ã€‚ åœ¨çº¿æ€§æ¨¡åž‹ä¸‹(w.x)ï¼Œwå·²ç»ç¡®å®šçš„æƒ…å†µä¸‹ï¼Œxçš„æŸä¸ªç‰¹å¾çš„å€¼æ˜¯20ï¼Œæˆ–è€…30ï¼Œw.xçš„å€¼ç›¸å·®æ˜¯å¾ˆå¤§çš„ï¼Œä½†æ˜¯å¾€å¾€20å²çš„äººè·Ÿ30å²çš„äººå¯¹åŒä¸€ä¸ªå¹¿å‘Šçš„å…´è¶£å·®è·ä¸ä¼šé‚£ä¹ˆå¤§ã€‚è¿žç»­ç‰¹å¾ç¦»æ•£åŒ–çš„åŸºæœ¬å‡è®¾ï¼Œæ˜¯é»˜è®¤è¿žç»­ç‰¹å¾ä¸åŒåŒºé—´çš„å–å€¼å¯¹ç»“æžœçš„è´¡çŒ®æ˜¯ä¸ä¸€æ ·çš„ã€‚ç¦»æ•£åŒ–å’Œè¿žç»­åŒ–æœ€å¤§çš„åŒºåˆ«æ˜¯ï¼Œå¯¹ä¸€ä¸ªå­—æ®µåšè¿žç»­åŒ–åŽçš„ç»“æžœå°±è¿˜åªæ˜¯ä¸€ä¸ªç‰¹å¾ï¼Œè€Œç¦»æ•£åŒ–åŽçš„è¿™ä¸€åˆ—æœ‰å¤šå°‘ä¸ªkey(å­—æ®µå¯èƒ½çš„å€¼)å°±ä¼šæŠ½å–å‡ºå¤šå°‘ä¸ªç‰¹å¾ã€‚å½“ç»è¿‡ç¦»æ•£åŒ–ä¹‹åŽï¼Œç‰¹å¾å„æœ‰å„çš„æƒé‡ï¼Œå½¼æ­¤ä¹‹é—´å°±æ²¡æœ‰å…³ç³»äº†ã€‚ æ¨¡åž‹æ˜¯ä½¿ç”¨ç¦»æ•£ç‰¹å¾è¿˜æ˜¯è¿žç»­ç‰¹å¾, å…¶å®žæ˜¯ä¸€ä¸ªâ€œæµ·é‡ç¦»æ•£ç‰¹å¾+ç®€å•æ¨¡åž‹â€ åŒ â€œå°‘é‡è¿žç»­ç‰¹å¾+å¤æ‚æ¨¡åž‹â€çš„æƒè¡¡ã€‚æ—¢å¯ä»¥ç¦»æ•£åŒ–ç”¨çº¿æ€§æ¨¡åž‹ï¼Œä¹Ÿå¯ä»¥ç”¨è¿žç»­ç‰¹å¾åŠ æ·±åº¦å­¦ä¹ ã€‚ å¸¸ç”¨çš„é€‰å–ç¦»æ•£ç‚¹çš„æ–¹æ³•ï¼šç­‰è·ç¦»ç¦»æ•£ï¼Œç­‰æ ·æœ¬ç¦»æ•£ã€ç”»å›¾è§‚å¯Ÿè¶‹åŠ¿å’Œå†³ç­–æ ‘æ¨¡åž‹(å¤©ç”Ÿå°±å¯ä»¥å¯¹è¿žç»­ç‰¹å¾åˆ†æ®µ)ã€‚ åœ¨å·¥ä¸šç•Œï¼Œå¾ˆå°‘ç›´æŽ¥å°†è¿žç»­å€¼ä½œä¸ºç‰¹å¾å–‚ç»™é€»è¾‘å›žå½’æ¨¡åž‹ï¼Œè€Œæ˜¯å°†è¿žç»­ç‰¹å¾ç¦»æ•£åŒ–ä¸ºä¸€ç³»åˆ—0ã€1ç‰¹å¾äº¤ç»™é€»è¾‘å›žå½’æ¨¡åž‹ï¼Œè¿™æ ·åšçš„ä¼˜åŠ¿æœ‰ä»¥ä¸‹å‡ ç‚¹ï¼š å•å˜é‡ç¦»æ•£åŒ–ä¸ºNä¸ªåŽï¼Œæ¯ä¸ªå˜é‡æœ‰å•ç‹¬çš„æƒé‡ï¼Œåœ¨æ¿€æ´»å‡½æ•°çš„ä½œç”¨ä¸‹ç›¸å½“äºŽä¸ºæ¨¡åž‹å¢žåŠ äº†éžçº¿æ€§ï¼Œèƒ½å¤Ÿæå‡æ¨¡åž‹è¡¨è¾¾èƒ½åŠ›ï¼ŒåŠ å¤§æ‹Ÿåˆã€‚ ç¦»æ•£åŒ–åŽçš„ç‰¹å¾å¯¹å¼‚å¸¸æ•°æ®æœ‰å¾ˆå¼ºçš„é²æ£’æ€§ï¼šæ¯”å¦‚ä¸€ä¸ªç‰¹å¾æ˜¯å¹´é¾„&gt;30æ˜¯1ï¼Œå¦åˆ™0ã€‚å¦‚æžœç‰¹å¾æ²¡æœ‰ç¦»æ•£åŒ–ï¼Œä¸€ä¸ªå¼‚å¸¸æ•°æ®â€œå¹´é¾„300å²â€ä¼šç»™æ¨¡åž‹é€ æˆå¾ˆå¤§çš„å¹²æ‰°ï¼Œå› ä¸ºç‰¹å¾å€¼çš„å¼‚å¸¸ä¼šå¯¼è‡´æƒé‡ä¹Ÿå°±æ˜¯wçš„å€¼ä¹Ÿä¼šå¼‚å¸¸ã€‚ ä¸€å®šæœ‰åŒå­¦æ‹…å¿ƒç‰¹å¾è¿‡å¤šä¼šå¯¼è‡´è¿ç®—ç¼“æ…¢ï¼Œä½†æ˜¯LRæ˜¯çº¿æ€§æ¨¡åž‹ï¼Œæˆ‘ä»¬åœ¨å†…éƒ¨è®¡ç®—çš„æ—¶å€™æ˜¯å‘é‡åŒ–è®¡ç®—ï¼Œè€Œä¸æ˜¯å¾ªçŽ¯è¿­ä»£ã€‚ç¨€ç–å‘é‡å†…ç§¯ä¹˜æ³•è¿ç®—é€Ÿåº¦å¿«ï¼Œè®¡ç®—ç»“æžœæ–¹ä¾¿å­˜å‚¨ï¼Œå®¹æ˜“æ‰©å±•ã€‚ æ‰€ä»¥ä¸ç”¨æ‹…å¿ƒåƒGBDTç®—æ³•é‚£æ ·ï¼Œç‰¹å¾å¤šäº†å°±è·‘ä¸åŠ¨äº†(æˆ‘ä»¬éƒ½è¯´GBDTä¸èƒ½ç”¨ç¦»æ•£ç‰¹å¾ä¸æ˜¯å› ä¸ºå®ƒå¤„ç†ä¸äº†ç¦»æ•£ç‰¹å¾ï¼Œè€Œæ˜¯å› ä¸ºç¦»æ•£åŒ–ç‰¹å¾åŽä¼šäº§ç”Ÿç‰¹åˆ«å¤šçš„ç‰¹å¾ï¼Œå†³ç­–æ ‘çš„å¶å­èŠ‚ç‚¹è¿‡å¤šï¼ŒéåŽ†çš„æ—¶å€™å¤ªæ…¢äº†)ã€‚ æ‰€ä»¥æµ·é‡ç¦»æ•£ç‰¹å¾ï¼‹LRæ˜¯ä¸šå†…å¸¸è§çš„ä¸€ä¸ªåšæ³•ã€‚è€Œå°‘é‡è¿žç»­ç‰¹å¾+å¤æ‚æ¨¡åž‹æ˜¯å¦å¤–ä¸€ç§åšæ³•ï¼Œä¾‹å¦‚GBDTã€‚ å½“ç„¶ä¹Ÿå¯ä»¥é€šè¿‡æ·±åº¦å­¦ä¹ æ¥è¿›è¡Œç‰¹å¾é€‰æ‹©ï¼šç›®å‰è¿™ç§æ‰‹æ®µæ­£åœ¨éšç€æ·±åº¦å­¦ä¹ çš„æµè¡Œè€Œæˆä¸ºä¸€ç§æ‰‹æ®µï¼Œå°¤å…¶æ˜¯åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸï¼ŒåŽŸå› æ˜¯æ·±åº¦å­¦ä¹ å…·æœ‰è‡ªåŠ¨å­¦ä¹ ç‰¹å¾çš„èƒ½åŠ›ï¼Œè¿™ä¹Ÿæ˜¯æ·±åº¦å­¦ä¹ åˆå«unsupervised feature learningçš„åŽŸå› ã€‚ä»Žæ·±åº¦å­¦ä¹ æ¨¡åž‹ä¸­é€‰æ‹©æŸä¸€ç¥žç»å±‚çš„ç‰¹å¾åŽå°±å¯ä»¥ç”¨æ¥è¿›è¡Œæœ€ç»ˆç›®æ ‡æ¨¡åž‹çš„è®­ç»ƒäº†ã€‚ å‚è€ƒæ–‡çŒ®:https://blog.csdn.net/lujiandong1/article/details/52412123 ç»„åˆç‰¹å¾å…ˆæ˜¯ç¦»æ•£åŒ–ï¼Œç„¶åŽæ˜¯ç‰¹å¾ç»„åˆã€‚äº¤å‰ä»Žç†è®ºä¸Šè€Œè¨€æ˜¯ä¸ºäº†å¼•å…¥ç‰¹å¾ä¹‹é—´çš„äº¤äº’ï¼Œä¹Ÿå³ä¸ºäº†å¼•å…¥éžçº¿æ€§ã€‚LR(é€»è¾‘å›žå½’ï¼‰åˆ†ç±»ç®—æ³•:å› ä¸ºçº¿æ€§å‡½æ•°çš„è¡¨è¾¾èƒ½åŠ›æœ‰é™ï¼Œæ‰€ä»¥æˆ‘ä»¬å¼•å…¥æ¿€æ´»å‡½æ•°å°±æ˜¯ç»™LRå¢žåŠ éžçº¿æ€§å…³ç³»ã€‚èƒ½è®©ä¸€æ¡ç›´çº¿å˜æˆæ›²çº¿ã€‚è¿™æ ·å¯ä»¥æ‹Ÿåˆå‡ºæ›´å¥½çš„æ•ˆæžœã€‚ï¼ˆä¹Ÿç”±æ­¤æ‰åŽäº†åŽæ¥è¯´çš„è¿‡æ‹Ÿåˆé—®é¢˜è€Œå¼•å…¥äº†æ­£åˆ™åŒ–è¶…å‚æ•°ï¼‰LRæ¨¡åž‹ä¹‹æ‰€ä»¥å¾ˆå—æ¬¢è¿Žï¼Œä¸»è¦æ˜¯å› ä¸ºLRæ¨¡åž‹æœ¬è´¨æ˜¯å¯¹æ•°çº¿æ€§æ¨¡åž‹ï¼Œå®žçŽ°ç®€å•ï¼Œæ˜“äºŽå¹¶è¡Œï¼Œå¤§è§„æ¨¡æ‰©å±•æ–¹ä¾¿ï¼Œè¿­ä»£é€Ÿåº¦å¿«ï¼ŒåŒæ—¶ä½¿ç”¨çš„ç‰¹å¾æ¯”è¾ƒå¥½è§£é‡Šï¼Œé¢„æµ‹è¾“å‡ºåœ¨0ä¸Ž1ä¹‹é—´å¥‘åˆæ¦‚çŽ‡æ¨¡åž‹ã€‚ï¼ˆæ¨¡åž‹çš„å¯è§£é‡Šæ€§ä¸¾ä¾‹ï¼Œæ¯”å¦‚A-Bçš„æƒé‡æ¯”è¾ƒå¤§ï¼ŒAä»£è¡¨ç”¨æˆ·ï¼ŒBä»£è¡¨ç‰©å“ï¼Œé‚£ä¹ˆå¯ä»¥è®¤ä¸ºAæ˜¯å¯¹Bæ¯”è¾ƒæ„Ÿå…´è¶£çš„ï¼‰ä½†æ˜¯ï¼Œçº¿æ€§æ¨¡åž‹å¯¹äºŽéžçº¿æ€§å…³ç³»ç¼ºä¹å‡†ç¡®åˆ»ç”»ï¼Œç‰¹å¾ç»„åˆæ­£å¥½å¯ä»¥åŠ å…¥éžçº¿æ€§è¡¨è¾¾ï¼Œå¢žå¼ºæ¨¡åž‹çš„è¡¨è¾¾èƒ½åŠ›ã€‚å¦å¤–ï¼Œå¹¿å‘ŠLRä¸­ï¼ŒåŸºæœ¬ç‰¹å¾å¯ä»¥è®¤ä¸ºæ˜¯ç”¨äºŽå…¨å±€å»ºæ¨¡ï¼Œç»„åˆç‰¹å¾æ›´åŠ ç²¾ç»†ï¼Œæ˜¯ä¸ªæ€§åŒ–å»ºæ¨¡ï¼Œå› ä¸ºåœ¨è¿™ç§å¤§è§„æ¨¡ç¦»æ•£LRä¸­ï¼Œå•å¯¹å…¨å±€å»ºæ¨¡ä¼šå¯¹éƒ¨åˆ†ç”¨æˆ·æœ‰åï¼Œå¯¹æ¯ä¸€ç”¨æˆ·å»ºæ¨¡åˆæ•°æ®ä¸è¶³æ˜“è¿‡æ‹ŸåˆåŒæ—¶å¸¦æ¥æ¨¡åž‹æ•°é‡çˆ†ç‚¸ï¼Œæ‰€ä»¥åŸºæœ¬ç‰¹å¾+ç»„åˆç‰¹å¾å…¼é¡¾äº†å…¨å±€å’Œä¸ªæ€§åŒ–ã€‚ ä»Žç»Ÿè®¡çš„è§’åº¦è§£é‡Šï¼ŒåŸºæœ¬ç‰¹å¾ä»…ä»…æ˜¯çœŸå®žç‰¹å¾åˆ†å¸ƒåœ¨ä½Žç»´çš„æ˜ å°„ï¼Œä¸è¶³ä»¥æè¿°çœŸå®žçš„åˆ†å¸ƒï¼ŒåŠ å…¥ç‰¹å¾åœ¨é«˜ç»´ç©ºé—´æ‹ŸåˆçœŸå®žåˆ†å¸ƒï¼Œä½¿å¾—é¢„æµ‹æ›´åŠ å‡†ç¡®ã€‚ æ­£åˆ™åŒ–çœŸæ­£æµ‹è¯•ä¸€ä¸ªæ¨¡åž‹çš„ä¸æ˜¯ç®€å•ä¸Žå¦ï¼Œæ›´é‡è¦åœ¨äºŽå®ƒåœ¨é¢„æµ‹æ–°çš„æƒ…å†µæ—¶è¡¨çŽ°å¦‚ä½•ã€‚å°æƒé‡æ„å‘³ç€ç½‘ç»œçš„è¡Œä¸ºä¸ä¼šå› ä¸ºæˆ‘ä»¬éšæ„æ›´æ”¹äº†ä¸€äº›è¾“å…¥è€Œæ”¹å˜å¤ªå¤šã€‚è¿™æ˜¯æˆ‘ä»¬åŠ äº†æ­£åˆ™åŒ–ä¹‹åŽçš„æˆæœ¬å‡½æ•°ï¼Œå¯ä»¥çœ‹æˆ‘ä»¬åŽé¢åŠ å…¥äº†æ­£åˆ™åŒ– Î» çš„è¡¨è¾¾å¼æ¥å®Œå–„æˆæœ¬å‡½æ•°ã€‚ä¸ºä»€ä¹ˆåŠ å…¥Î»èƒ½å¤Ÿå‡è½»è¿‡æ‹Ÿåˆå‘¢ï¼Ÿç›´è§‚ä¸€ç‚¹çš„è§£é‡Šæ˜¯è®¾ç½®çš„Î»å€¼è¶Šå¤§ï¼Œé‚£ä¹ˆå‚æ•°wçš„å€¼å°±ä¼šè¢«åŽ‹ç¼©çš„è¶Šå°(åœ¨æ¢¯åº¦ä¸‹é™ä¸­, æ¯æ¬¡è¿­ä»£çš„æ­¥é•¿ï¼Œä¹Ÿå°±æ˜¯è¿™ä¸ªå…¬å¼w=w - å­¦ä¹ çŽ‡*æˆæœ¬å‡½æ•°å¯¹wçš„å¯¼æ•°ï¼Œ çŽ°åœ¨ç”±äºŽæˆæœ¬å‡½æ•°å¢žåŠ äº†æ­£åˆ™é¡¹ï¼Œä½¿å¾—Jå’Œwå˜å¾—æ•°å€¼ç›¸å…³äº†)ã€‚ å‡è®¾Î»è®¾ç½®çš„è¶³å¤Ÿå¤§ï¼Œé‚£ä¹ˆwä¼šæ— é™çš„è¶‹è¿‘äºŽ0. æŠŠå¤šéšè—å±‚çš„å•å…ƒçš„æƒé‡è®¾ç½®ä¸º0ä»¥åŽï¼Œé‚£ä¹ˆåŸºæœ¬ä¸Šå°±æ˜¯æ¶ˆé™¤æŽ‰äº†è¿™äº›å•å…ƒçš„ä½œç”¨ï¼Œè€Œä½¿å¾—ç½‘ç»œæ¨¡åž‹å¾—åˆ°ç®€åŒ–ï¼Œå°±åƒä¸‹é¢çš„å›¾ä¸€æ ·ã€‚ç”±äºŽæ­£åˆ™åŒ–çš„è®¾ç½®ï¼Œæ¶ˆé™¤äº†ä¸€äº›éšè—å•å…ƒçš„ä½œç”¨ã€‚è€Œä½¿å¾—æ•´ä¸ªæ¨¡åž‹è¶Šæ¥è¶ŠæŽ¥è¿‘äºŽçº¿æ€§åŒ–ï¼Œä¹Ÿå°±æ˜¯ä»Žä¸‹å›¾ä¸­çš„è¿‡æ‹Ÿåˆå¾€æ¬ æ‹Ÿåˆåè½¬ã€‚å½“ç„¶æˆ‘ä»¬æœ‰ä¸€ä¸ªé€‚åˆçš„Î»çš„å€¼ï¼Œèƒ½è®©æˆ‘ä»¬çš„æ‹ŸåˆçŠ¶æ€è¾¾åˆ°æœ€ä½³ã€‚æ‰€ä»¥æˆ‘ä»¬åœ¨è®­ç»ƒæ¨¡åž‹çš„æ—¶å€™ï¼Œå¾€å¾€éƒ½ä¼šæœ‰ä¸€ä¸ªï¼¬ï¼’æ­£åˆ™é¡¹çš„è¶…å‚æ•°éœ€è¦æˆ‘ä»¬è®¾ç½®ã€‚è¿™æ˜¯æˆ‘ä»¬çš„tanhæ¿€æ´»å‡½æ•°ï¼Œ å¯ä»¥çœ‹åˆ°å½“zçš„å€¼è¶Šå¤§æ—¶ï¼Œæ•´ä¸ªå‡½æ•°çš„éžçº¿æ€§å°±è¶Šå¤§ï¼Œè€Œzçš„å€¼è¶Šå°(å›¾ä¸­çº¢è‰²åŠ ç²—éƒ¨åˆ†),å‡½æ•°å°±è¶Šæ˜¯å‘ˆçŽ°å‡ºçº¿æ€§åˆ†å¸ƒã€‚ æ‰€ä»¥å½“æˆ‘ä»¬å¢žåŠ Î»çš„å€¼ï¼Œ wå¾—å€¼å°±è¶Šå°ï¼Œç›¸åº”çš„zçš„å€¼ä¹Ÿå°±è¶Šå°ã€‚å› ä¸ºz = wx + bã€‚ è€Œæˆ‘ä»¬ç¬¬ä¸€æ¬¡è¯´æ¿€æ´»å‡½æ•°çš„æ—¶å€™å°±è¯´è¿‡ç¥žç»ç½‘ç»œä¸­åŸºæœ¬ä¸Šæ˜¯ä¸ä½¿ç”¨çº¿æ€§å‡½æ•°ä½œä¸ºæ¿€æ´»å‡½æ•°çš„ï¼Œå› ä¸ºä¸è®ºæœ‰å¤šå°‘å±‚ï¼Œå¤šå°‘ä¸ªå•å…ƒï¼Œçº¿æ€§æ¿€æ´»å‡½æ•°ä¼šä½¿å¾—æ‰€æœ‰å•å…ƒæ‰€è®¡ç®—çš„éƒ½å‘ˆçŽ°çº¿æ€§çŠ¶æ€ã€‚ æ‚è´§é“ºç‰¹å¾å·¥ç¨‹å¯ä»¥åˆ†ä¸ºç‰¹å¾å¤„ç†ã€Feature Selectionï¼ˆç‰¹å¾é€‰æ‹©ï¼‰ã€Feature Extractionï¼ˆç‰¹å¾æå–ï¼‰å’ŒFeature constructionï¼ˆç‰¹å¾æž„é€ ï¼‰ç­‰é˜¶æ®µã€‚å½’ä¸€åŒ–ï¼ˆåŽ»ä¸­å¿ƒï¼Œæ–¹å·®å½’ä¸€ï¼‰æ˜¯å±žäºŽç‰¹å¾(é¢„)å¤„ç†:æŠŠç‰¹å¾å€¼åŽ‹ç¼©æˆ0~1çš„åŒºé—´ã€‚ One-hotï¼ˆä¹Ÿå«One-of-kï¼‰çš„æ–¹æ³•æŠŠæ¯ä¸ªæ— åºç‰¹å¾è½¬åŒ–ä¸ºä¸€ä¸ªæ•°å€¼å‘é‡ã€‚æ¯”å¦‚ä¸€ä¸ªæ— åºç‰¹å¾coloræœ‰ä¸‰ç§å–å€¼ï¼šredï¼Œgreenï¼Œblueã€‚é‚£ä¹ˆå¯ä»¥ç”¨ä¸€ä¸ªé•¿åº¦ä¸º3çš„å‘é‡æ¥è¡¨ç¤ºå®ƒï¼Œå‘é‡ä¸­çš„å„ä¸ªå€¼åˆ†åˆ«å¯¹åº”äºŽredï¼Œgreenï¼Œblueã€‚å¦‚ï¼šè¿™ç§æ–¹æ³•åœ¨NLPé‡Œç”¨çš„å¾ˆå¤šï¼Œå°±æ˜¯æ‰€è°“çš„è¯å‘é‡æ¨¡åž‹ã€‚å˜æ¢åŽçš„å‘é‡é•¿åº¦å¯¹äºŽè¯å…¸é•¿åº¦ï¼Œæ¯ä¸ªè¯å¯¹åº”äºŽå‘é‡ä¸­çš„ä¸€ä¸ªå…ƒç´ ã€‚ å¯¹äºŽæ¬ æ‹Ÿåˆ: å¢žåŠ ç¥žç»ç½‘ç»œå¤æ‚åº¦ï¼Œå‡ºçŽ°æ¬ æ‹Ÿåˆçš„åŽŸå› ä¹‹ä¸€æ˜¯ç”±äºŽå‡½æ•°çš„éžçº¿æ€§ä¸è¶³ï¼Œæ‰€ä»¥ç”¨æ›´å¤æ‚çš„ç½‘ç»œæ¨¡åž‹è¿›è¡Œè®­ç»ƒæ¥åŠ æ·±æ‹Ÿåˆã€‚å¯¹äºŽè¿‡æ‹Ÿåˆï¼šå¢žåŠ æ•°æ®è§„æ¨¡ï¼Œ å‡ºçŽ°è¿‡æ‹Ÿåˆçš„åŽŸå› ä¹‹ä¸€æ˜¯æ•°æ®è§„æ¨¡ä¸è¶³è€Œé€ æˆçš„æ•°æ®åˆ†å¸ƒä¸å‡ï¼Œæ‰©å±•æ•°æ®è§„æ¨¡èƒ½æ¯”è¾ƒå¥½çš„è§£å†³è¿™ä¸ªé—®é¢˜ã€‚å½“ç„¶å¦ä¸€ä¸ªåšæ³•æ˜¯æ­£åˆ™åŒ–ï¼Œæˆ‘ä»¬é‡‡å–ä½¿ç”¨æ­£åˆ™åŒ–æ¥è§£å†³è¿‡æ‹Ÿåˆé—®é¢˜ï¼Œå¸¸ç”¨çš„æ˜¯L2æ­£åˆ™ï¼Œå…¶ä»–çš„è¿˜æœ‰L1å’Œ Dropoutæ­£åˆ™ã€‚ å¾ˆå¤šäººä¼šæƒ³ç€æ—¢ç„¶çº¿æ€§åˆ†ç±»å™¨æžä¸å®šï¼Œé‚£å°±ç›´æŽ¥æ‰¾ä¸ªéžçº¿æ€§çš„å¥½äº†ï¼Œæ¯”å¦‚é«˜æ–¯æ ¸çš„SVMã€‚æˆ‘ä»¬ç¡®å®žå¯ä»¥é€šè¿‡è¿™ç§ç®€å•æ¢ç®—æ³•çš„æ–¹å¼è§£å†³è¿™ä¸ªç®€å•çš„é—®é¢˜ã€‚ä½†å¯¹äºŽå¾ˆå¤šå®žé™…é—®é¢˜ï¼ˆå¦‚å¹¿å‘Šç‚¹å‡»çŽ‡é¢„æµ‹ï¼‰ï¼Œå¾€å¾€ç‰¹å¾éžå¸¸å¤šï¼Œè¿™æ—¶å€™æ—¶é—´çº¦æŸé€šå¸¸ä¸å…è®¸æˆ‘ä»¬ä½¿ç”¨å¾ˆå¤æ‚çš„éžçº¿æ€§åˆ†ç±»å™¨ã€‚è¿™ä¹Ÿæ˜¯ä¸ºä»€ä¹ˆç®—æ³•å‘å±•è¿™ä¹ˆå¤šå¹´ï¼Œå¹¿å‘Šç‚¹å‡»çŽ‡é¢„æµ‹æœ€å¸¸ç”¨çš„æ–¹æ³•è¿˜æ˜¯Logistic Regression (LogisticReg)ã€‚]]></content>
      <categories>
        <category>æœºå™¨å­¦ä¹ </category>
      </categories>
      <tags>
        <tag>ç‰¹å¾å·¥ç¨‹</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[å¸¸è§çš„æŽ’åºç®—æ³•æ€»ç»“]]></title>
    <url>%2F2018%2F06%2F29%2F%E5%B8%B8%E8%A7%81%E7%9A%84%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[åˆ†ç±»å’Œæ€»ç»“ æ ¹æ®å¾…æŽ’åºçš„æ•°æ®å¤§å°ä¸åŒï¼Œä½¿å¾—æŽ’åºè¿‡ç¨‹ä¸­æ‰€æ¶‰åŠçš„å­˜å‚¨å™¨ä¸åŒï¼Œå¯åˆ†ä¸ºå†…éƒ¨æŽ’åºå’Œå¤–éƒ¨æŽ’åºã€‚ æŽ’åºå…³é”®å­—å¯èƒ½å‡ºçŽ°é‡å¤ï¼Œæ ¹æ®é‡å¤å…³é”®å­—çš„æŽ’åºæƒ…å†µå¯åˆ†ä¸ºç¨³å®šæŽ’åºå’Œä¸ç¨³å®šæŽ’åºã€‚ å¯¹äºŽå†…éƒ¨æŽ’åºï¼Œä¾æ®ä¸åŒçš„æŽ’åºåŽŸåˆ™ï¼Œå¯åˆ†ä¸ºæ’å…¥æŽ’åºã€äº¤æ¢(å¿«é€Ÿ)æŽ’åºã€é€‰æ‹©æŽ’åºã€å½’å¹¶æŽ’åºå’Œè®¡æ•°æŽ’åºã€‚ é’ˆå¯¹å†…éƒ¨æŽ’åºæ‰€éœ€çš„å·¥ä½œé‡åˆ’åˆ†ï¼Œå¯åˆ†ä¸º:ç®€å•æŽ’åº O(n^2)ã€å…ˆè¿›æŽ’åº O(nlogn)å’ŒåŸºæ•°æŽ’åº O(d*n)ã€‚å¸¸è§ç®—æ³•çš„æ€§è´¨æ€»ç»“ï¼š æŽ’åºç®—æ³•å®žçŽ°é»˜è®¤éƒ½æ˜¯å‡åºâ€¦ æ’å…¥æŽ’åº(Insert Sort)æ€æƒ³ï¼š æœ‰åºæ•°ç»„+insert one every one time æ’å…¥æŽ’åºçš„å·¥ä½œæ–¹å¼éžå¸¸åƒäººä»¬æŽ’åºä¸€æ‰‹æ‰‘å…‹ç‰Œä¸€æ ·ã€‚å¼€å§‹æ—¶ï¼Œæˆ‘ä»¬çš„å·¦æ‰‹ä¸ºç©ºå¹¶ä¸”æ¡Œå­ä¸Šçš„ç‰Œé¢æœä¸‹ã€‚ç„¶åŽï¼Œæˆ‘ä»¬æ¯æ¬¡ä»Žæ¡Œå­ä¸Šæ‹¿èµ°ä¸€å¼ ç‰Œå¹¶å°†å®ƒæ’å…¥å·¦æ‰‹ä¸­æ­£ç¡®çš„ä½ç½®ã€‚ä¸ºäº†æ‰¾åˆ°ä¸€å¼ ç‰Œçš„æ­£ç¡®ä½ç½®ï¼Œæˆ‘ä»¬ä»Žå³åˆ°å·¦å°†å®ƒä¸Žå·²åœ¨æ‰‹ä¸­çš„æ¯å¼ ç‰Œè¿›è¡Œæ¯”è¾ƒï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼šæ­¥éª¤ï¼š ä»Žç¬¬ä¸€ä¸ªå…ƒç´ å¼€å§‹ï¼Œè¯¥å…ƒç´ å¯ä»¥è®¤ä¸ºå·²ç»è¢«æŽ’åº å–å‡ºä¸‹ä¸€ä¸ªå…ƒç´ ï¼Œåœ¨å·²ç»æŽ’åºçš„å…ƒç´ åºåˆ—ä¸­ä»ŽåŽå‘å‰æ‰«æ å¦‚æžœè¯¥å…ƒç´ ï¼ˆå·²æŽ’åºï¼‰å¤§äºŽæ–°å…ƒç´ ï¼Œå°†è¯¥å…ƒç´ ç§»åˆ°ä¸‹ä¸€ä½ç½® é‡å¤æ­¥éª¤3ï¼Œç›´åˆ°æ‰¾åˆ°å·²æŽ’åºçš„å…ƒç´ å°äºŽæˆ–è€…ç­‰äºŽæ–°å…ƒç´ çš„ä½ç½® å°†æ–°å…ƒç´ æ’å…¥åˆ°è¯¥ä½ç½®åŽ é‡å¤æ­¥éª¤2~5 123456789101112131415def insert_sort(lists): count = len(lists) for i in range(1, count): key =lists[i] j =i-1 while j &gt;= 0: if lists[j] &gt;key: lists[j+1] =lists[j] lists[j] =key j -= 1 return lists# testlists =[1,2,-3, 90,34]print(insert_sort(lists)) é€‰æ‹©æŽ’åº(Select Sort)æ€æƒ³å’Œæ­¥éª¤ï¼š select one every time ç®€å•é€‰æ‹©æŽ’åºæ˜¯æœ€ç®€å•ç›´è§‚çš„ä¸€ç§ç®—æ³•ï¼ŒåŸºæœ¬æ€æƒ³ä¸ºæ¯ä¸€è¶Ÿä»Žå¾…æŽ’åºçš„æ•°æ®å…ƒç´ ä¸­é€‰æ‹©æœ€å°ï¼ˆæˆ–æœ€å¤§ï¼‰çš„ä¸€ä¸ªå…ƒç´ ä½œä¸ºé¦–å…ƒç´ ï¼Œç›´åˆ°æ‰€æœ‰å…ƒç´ æŽ’å®Œä¸ºæ­¢ï¼Œç®€å•é€‰æ‹©æŽ’åºæ˜¯ä¸ç¨³å®šæŽ’åºã€‚ åˆ†æžï¼š æ— è®ºä»€ä¹ˆæ•°æ®è¿›åŽ»éƒ½æ˜¯ O(nÂ²) çš„æ—¶é—´å¤æ‚åº¦ã€‚æ‰€ä»¥ç”¨åˆ°å®ƒçš„æ—¶å€™ï¼Œæ•°æ®è§„æ¨¡è¶Šå°è¶Šå¥½ã€‚å”¯ä¸€çš„å¥½å¤„å¯èƒ½å°±æ˜¯ä¸å ç”¨é¢å¤–çš„å†…å­˜ç©ºé—´äº†å§ã€‚ ä»£ç å®žçŽ°: 12345678910111213def select_sort(lists): count =len(lists) for i in range(0, count): min =i for j in range(i+1, count): if lists[min] &gt;lists[j]: min =j lists[min], lists[i] =lists[i], lists[min] return lists# testlists =[1, 23,45, 0,-1]print(select_sort(lists)) å†’æ³¡æŽ’åº(Bubble Sort)æ€æƒ³ï¼š move smallest one time (å‡è®¾ä¸å‡) æœ‰ä¸¤ç§è¯´æ³• ä»Žå³å¾€å·¦ï¼š æœ€å°å€¼è¢«ç§»åˆ°äº†æœ€å·¦è¾¹ã€‚ ã€å†’æ³¡æ³•ã€‘çš„æœ¬æ„ ä»Žå·¦å¾€å³ï¼š æœ€å¤§å€¼è¢«ç§»åˆ°äº†æœ€å³è¾¹ã€‚ æ­¤æ—¶å…¶å®žåº”è¯¥å« ã€æ²‰çŸ³æ³•ã€‘ ä»Žå·¦å‘å³ æ²‰çŸ³æ³• å›¾è§£ï¼š åˆ†æžï¼šå¹³å‡æ—¶é—´å¤æ‚åº¦ï¼šO(n^2)æœ€åç©ºé—´å¤æ‚åº¦ï¼š æ€»å…± O(n)ï¼Œéœ€è¦è¾…åŠ©ç©ºé—´ O(1) 1234567891011def bubble_sort(lists): count =len(lists) for i in range(0, count): for j in range(i+1, count): if lists[i]&gt; lists[j]: lists[i], lists[j] =lists[j], lists[i] return lists# testlists =[1,34,45,0,89]print(bubble_sort(lists)) å½’å¹¶æŽ’åº(Merge Sort)æ€æƒ³ï¼š divide-and-conquer +é€’å½’ å½’å¹¶æŽ’åºï¼ˆMERGE-SORTï¼‰æ˜¯åˆ©ç”¨å½’å¹¶çš„æ€æƒ³å®žçŽ°çš„æŽ’åºæ–¹æ³•ï¼Œè¯¥ç®—æ³•é‡‡ç”¨ç»å…¸çš„åˆ†æ²»ï¼ˆdivide-and-conquerï¼‰ç­–ç•¥ï¼ˆåˆ†æ²»æ³•å°†é—®é¢˜åˆ†(divide)æˆä¸€äº›å°çš„é—®é¢˜ç„¶åŽé€’å½’æ±‚è§£ï¼Œè€Œæ²»(conquer)çš„é˜¶æ®µåˆ™å°†åˆ†çš„é˜¶æ®µå¾—åˆ°çš„å„ç­”æ¡ˆâ€ä¿®è¡¥â€åœ¨ä¸€èµ·ï¼Œå³åˆ†è€Œæ²»ä¹‹)ã€‚ åˆ†æžï¼šæ—¶é—´å¤æ‚åº¦ï¼šO(nlogn)ç©ºé—´å¤æ‚åº¦ï¼šO(N)ï¼Œå½’å¹¶æŽ’åºéœ€è¦ä¸€ä¸ªä¸ŽåŽŸæ•°ç»„ç›¸åŒé•¿åº¦çš„æ•°ç»„åšè¾…åŠ©æ¥æŽ’åº 1234567891011121314151617181920212223242526def merge(left, right): i, j =0,0 result =[] while i&lt;len(left) and j &lt;len(right): if left[i] &lt;= right[j]: result.append(left[i]) i +=1 else: result.append(right[j]) j +=1 result += left[i:] result += right[j:] return resultdef merge_sort(lists): if len(lists) &lt;=1: return lists num =int(len(lists)/2) left =merge_sort(lists[:num]) right = merge_sort(lists[num:]) return merge(left, right)# merge_sort æ˜¯å…ˆåˆ‡åˆ†ï¼Œç„¶åŽå†æ•´åˆï¼Œquick sort æ˜¯ä¸¤ä¸ªæŒ‡é’ˆ# testlists =[1, 34, 23,45,0,9]print(merge_sort(lists)) å¿«é€ŸæŽ’åº(Quick Sort)æ€æƒ³ï¼šä»»æ„é€‰æ‹©ä¸€ä¸ªkey(é€šå¸¸é€‰æ‹©a[0])ï¼Œå°†æ¯”ä»–å°çš„æ•°æ®æ”¾åœ¨å®ƒçš„å‰é¢ï¼Œæ¯”ä»–å¤§çš„æ•°å­—æ”¾åœ¨å®ƒçš„åŽé¢ã€‚é€’å½’è¿›è¡Œã€‚ æ­¥éª¤ï¼š ä»Žæ•°åˆ—ä¸­æŒ‘å‡ºä¸€ä¸ªåŸºå‡†å€¼ã€‚ å°†æ‰€æœ‰æ¯”åŸºå‡†å€¼å°çš„æ‘†æ”¾åœ¨åŸºå‡†å‰é¢ï¼Œæ‰€æœ‰æ¯”åŸºå‡†å€¼å¤§çš„æ‘†åœ¨åŸºå‡†çš„åŽé¢(ç›¸åŒçš„æ•°å¯ä»¥åˆ°ä»»ä¸€è¾¹)ï¼›åœ¨è¿™ä¸ªåˆ†åŒºé€€å‡ºä¹‹åŽï¼Œè¯¥åŸºå‡†å°±å¤„äºŽæ•°åˆ—çš„ä¸­é—´ä½ç½®ã€‚ é€’å½’åœ°æŠŠâ€åŸºå‡†å€¼å‰é¢çš„å­æ•°åˆ—â€å’Œâ€åŸºå‡†å€¼åŽé¢çš„å­æ•°åˆ—â€è¿›è¡ŒæŽ’åºã€‚ åˆ†æžï¼šå¿«é€ŸæŽ’åºçš„æ—¶é—´å¤æ‚åº¦åœ¨æœ€åæƒ…å†µä¸‹æ˜¯O($N^2$)ï¼Œå¹³å‡çš„æ—¶é—´å¤æ‚åº¦æ˜¯O(N*logN)ï¼Œé‡‡ç”¨çš„æ˜¯åˆ†æ²»çš„æ€æƒ³ï¼ŒäºŒå‰æ ‘çš„ç»“æž„ã€‚ ä¸‹é¢ä»¥æ•°åˆ— a={30,40,60,10,20,50} ä¸ºä¾‹ï¼Œæ¼”ç¤ºå®ƒçš„å¿«é€ŸæŽ’åºè¿‡ç¨‹(å¦‚ä¸‹å›¾)ã€‚ è¿™ä¸ªæ˜¯ç»è¿‡ä¸€ä¸ªè¿­ä»£çš„ç»“æžœï¼Œæ¯ä¸€ä¸ªè¿­ä»£ï¼Œéƒ½æŽ’å¥½äº†åŸºå‡†æ•°å­—çš„ä½ç½®ã€‚æŒ‰ç…§åŒæ ·çš„æ–¹æ³•ï¼Œå¯¹å­æ•°åˆ—è¿›è¡Œé€’å½’éåŽ†ã€‚æœ€åŽå¾—åˆ°æœ‰åºæ•°ç»„ï¼ 12345678910111213141516171819202122232425def quick_sort(lists, left, right): if left &gt;= right: return lists key =lists[left] low =left high =right while left &lt; right: # å› ä¸ºä½ æœ€åˆkey å–å¾—æ˜¯leftï¼Œç„¶åŽä»Žå³è¾¹æ‰¾åˆ°ä¸€ä¸ªæ¯”keyå°çš„ï¼Œç„¶åŽæ›¿æ¢left çš„ä½ç½® while left &lt;right and lists[right]&gt;=key: right -= 1 lists[left] =lists[right] while left &lt; right and lists[left] &lt;= key: left +=1 lists[right] =lists[left] lists[left] =key # è¿™é‡Œçš„left å’Œright éƒ½æ˜¯å¯ä»¥çš„ï¼Œå› ä¸ºä»Žwhile ä¸­å‡ºæ¥ä¹‹åŽä¸¤è€…æ˜¯ç›¸åŒçš„ # è¿™ä¸ªæ­¥ä¼æ˜¯1,æ‰€ä»¥åªèƒ½æ˜¯ä¸€ä¸ªä¸ªå˜åŒ– quick_sort(lists, low, left-1) quick_sort(lists, left+1, high) # å› ä¸ºleftçš„ä½ç½®å·²ç»è¢«å äº†ï¼Œæ‰€ä»¥åªæ˜¯åˆ’åˆ†å·¦è¾¹ä¸€å—ï¼Œå³è¾¹ä¸€å—å°±æ˜¯å¯ä»¥çš„ return lists# testlists =[3,2,45, 100,1,56,56]#lists =[1,2,3,2,2,2,5,4,2]print(quick_sort(lists, 0, len(lists)-1)) Kth Largest Element in an Array Find the kth largest element in an unsorted array. Note that it is the kth largest element in the sorted order, not the kth distinct element. Tips: ä½¿ç”¨çš„æ˜¯ å¿«æŽ’çš„æ€æƒ³ï¼Œæœ€åŽçš„å¹³å‡æ—¶é—´å¤æ‚åº¦æ˜¯O(N) ï¼Œæ‰€ä»¥æ˜¯ä¸€ä¸ªä¸é”™çš„ç®—æ³•ã€‚ kth å’Œ â€å¿«æŽ’â€œ å®žçŽ°çš„æ—¶å€™ç¨å¾®æœ‰ä¸€äº›åŒºåˆ«ï¼Œå‰è€…åªæœ‰å‘çŽ° ä¸€ç»„æ•°æ®ï¼ˆä¸€ä¸ªæ¯”pivot å¤§ï¼Œä¸€ä¸ªæ¯” pivot å°ï¼‰æ‰è¿›è¡Œäº¤æ¢ï¼ŒåŽè€…æ˜¯æ˜¯ ä¸¤ä¸ªwhileï¼Œ åªè¦å‘çŽ°ä¸€ä¸ªå°±è¿›è¡Œäº¤æ¢ã€‚è¿™ä¸ªæ˜¯ç»†å¾®çš„å·®åˆ«ã€‚ 123456789101112131415161718192021222324252627282930313233343536class Solution(object): def findKthLargest(self, nums, k): """ :type nums: List[int] :type k: int :rtype: int """ left, right = 0, len(nums) - 1 while True: pos = self.partition(nums, left, right) # è¿™ä¸ªåœ¨æŽ’åºçš„æ—¶å€™ï¼Œæ˜¯æŠŠå¤§çš„æ•°å­—æ”¾åˆ°å‰é¢ï¼Œè€Œå‰é¢æ˜¯pos æ˜¯ä»Ž0 å¼€å§‹çš„ï¼Œ # æ‰€ä»¥è¿™é‡Œæ˜¯ k-1 if pos == k - 1: return nums[pos] # å·¦è¾¹çš„å¹¶ä¸è¶³ä»¥æž„æˆk ä¸ªï¼Œ é‚£ä¹ˆåœ¨å³è¾¹ elif pos &lt; k - 1: left = pos + 1 else: right = pos - 1 def partition(self, nums, left, right): # choose nums[left] as pivot pivot = nums[left] # p1, p2å°±ç±»ä¼¼ working ä¸­çš„left right p1, p2 = left + 1, right while p1 &lt;= p2: if nums[p1] &lt; pivot and nums[p2] &gt; pivot: nums[p1], nums[p2] = nums[p2], nums[p1] p1, p2 = p1 + 1, p2 - 1 elif nums[p1] &gt;= pivot: p1 += 1 else: #nums[p2] &lt;= pivot: p2 -=1 nums[left], nums[p2] = nums[p2], nums[left] return p2 å †æŽ’åºå‚çœ‹å¦å¤–ä¸€ç¯‡åšå®¢ å‚è€ƒæ–‡çŒ®ç®—æ³•åŠ¨å›¾æ•ˆæžœæŽ’åºç®—æ³•åˆ†ç±»]]></content>
      <categories>
        <category>ç®—æ³•</category>
      </categories>
      <tags>
        <tag>æŽ’åºç®—æ³•</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[é‚£äº›å¹´çš„ç®—æ³•é¢˜ç›®ï¼ˆä¸€ï¼‰]]></title>
    <url>%2F2018%2F06%2F22%2F%E9%82%A3%E4%BA%9B%E5%B9%B4%E7%9A%84%E7%AE%97%E6%B3%95%E9%A2%98%E7%9B%AE%EF%BC%88%E4%B8%80%EF%BC%89%2F</url>
    <content type="text"><![CDATA[äº¤ä½œä¸šäº†â€¦ å®Œå…¨äºŒå‰æ ‘æ’å…¥é—®é¢˜æè¿°å·²çŸ¥ä¸€ä¸ªå®Œå…¨äºŒå‰æ ‘çš„ç»“æž„ï¼ŒçŽ°åœ¨éœ€è¦å°†ä¸€ä¸ªèŠ‚ç‚¹æ’å…¥åˆ°è¿™é¢—å®Œå…¨äºŒå‰æ ‘çš„æœ€åŽï¼Œä½¿å¾—å®ƒè¿˜æ˜¯ä¸€ä¸ªå®Œå…¨äºŒå‰æ ‘ã€‚ç¬¬ä¸€ç§è§£æ³•ï¼šå¦‚æžœè¯¥æ ‘ä¸ºæ»¡äºŒå‰æ ‘æˆ–è€…å·¦å­æ ‘ä¸ä¸ºæ»¡äºŒå‰æ ‘ï¼Œé‚£ä¹ˆå°±è¿›å…¥å·¦å­æ ‘ï¼Œå¦åˆ™è¿›å…¥å³å­æ ‘ï¼Œé€’å½’è¿›è¡Œã€‚ äºŒå‰æ ‘(Binary Tree)å¼ºè¡Œè¡¥å……ä¸€ä¸‹å…³äºŽäºŒå‰æ ‘æ¦‚å¿µçš„çŸ¥è¯†ã€‚å®Œå…¨äºŒå‰æ ‘(Complete Binary Tree):è‹¥è®¾äºŒå‰æ ‘çš„æ·±åº¦ä¸ºhï¼Œé™¤ç¬¬hå±‚å¤–ï¼Œå…¶å®ƒå„å±‚(1ï½žh-1)çš„ç»“ç‚¹æ•°éƒ½è¾¾åˆ°æœ€å¤§ä¸ªæ•°ï¼Œç¬¬hå±‚æ‰€æœ‰çš„ç»“ç‚¹éƒ½è¿žç»­é›†ä¸­åœ¨æœ€å·¦è¾¹ï¼Œè¿™å°±æ˜¯å®Œå…¨äºŒå‰æ ‘ã€‚æ»¡äºŒå‰æ ‘:æ ‘ä¸­é™¤äº†å¶å­èŠ‚ç‚¹ï¼Œæ¯ä¸ªèŠ‚ç‚¹éƒ½æœ‰ä¸¤ä¸ªå­èŠ‚ç‚¹ã€‚æ»¡äºŒå‰æ ‘æ˜¯ä¸€ç§ç‰¹æ®Šçš„å®Œå…¨äºŒå‰æ ‘ã€‚äºŒå‰æœç´¢æ ‘(binary search tree):æ‰€æœ‰éžå¶å­ç»“ç‚¹è‡³å¤šæ‹¥æœ‰ä¸¤ä¸ªå„¿å­ï¼ˆLeftå’ŒRightï¼‰ï¼›æ‰€æœ‰ç»“ç‚¹å­˜å‚¨ä¸€ä¸ªå…³é”®å­—,éžå¶å­ç»“ç‚¹çš„å·¦æŒ‡é’ˆæŒ‡å‘å°äºŽå…¶å…³é”®å­—çš„å­æ ‘ï¼Œå³æŒ‡é’ˆæŒ‡å‘å¤§äºŽå…¶å…³é”®å­—çš„å­æ ‘ã€‚å¹³è¡¡äºŒå‰æ ‘(AVLæ ‘)ï¼šå®ƒæ˜¯ä¸€é¢—ç©ºæ ‘æˆ–å®ƒçš„å·¦å³ä¸¤ä¸ªå­æ ‘çš„é«˜åº¦å·®çš„ç»å¯¹å€¼ä¸è¶…è¿‡1ã€‚å“ˆå¤«æ›¼æ ‘ï¼šå¸¦æƒè·¯å¾„é•¿åº¦è¾¾åˆ°æœ€å°çš„äºŒå‰æ ‘ï¼Œä¹Ÿå«åšæœ€ä¼˜äºŒå‰æ ‘ã€‚æ ‘çš„æ·±åº¦å’Œé«˜åº¦ï¼šæ·±åº¦æ˜¯ä»Žä¸Šå¾€ä¸‹æ•°ï¼›é«˜åº¦æ˜¯ä»Žä¸‹å¾€ä¸Šæ•° ä»£ç å®žçŽ°å¹³æ»‘è¿‡æ¸¡åˆ°æœ¬é—®é¢˜çš„ä»£ç å®žçŽ°ã€‚12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152#include&lt;iostream&gt;using namespace std;typedef struct Node&#123; int value; struct Node *lchild, *rchild;&#125;Tree;int GetLeftDepth(Tree* root)&#123; Tree* pNode =root-&gt;lchild ; int depth =0; while(pNode != NULL) &#123; depth ++; pNode =pNode-&gt;lchild; &#125; return depth;&#125;int GetRightDepth(Tree* root)&#123; Tree* pNode =root-&gt;rchild; int depth =0 ; while(pNode != NULL) &#123; depth ++ ; pNode =pNode-&gt;rchild ; &#125; return depth;&#125;bool IsFullBinaryTree(Tree* root)&#123; return GetLeftDepth(root) == GetRightDepth(root) ;&#125;void insert(Tree* root, Tree * node)&#123; if (IsFullBinaryTree(root) || !IsFullBinaryTree(root-&gt;lchild))&#123; insert(root-&gt;lchild, node); return ; &#125; if (root-&gt;rchild ==NULL)&#123; root-&gt;rchild =node ; return ; &#125; insert(root-&gt;rchild, node) ;&#125;int main()&#123; Node* a = new Node(); a-&gt;value =1;&#125; ç¬¬äºŒç§æ€è·¯ï¼Œå¦‚æžœå·²çŸ¥ä¹‹å‰æ ‘çš„ä¸ªæ•°ï¼Œé‚£ä¹ˆå¯ä»¥ä½¿ç”¨å‰åºéåŽ†çš„æ–¹å¼ï¼Œå¾—åˆ°å°†è¦æ’å…¥çš„çš„èŠ‚ç‚¹çš„ä½ç½®ï¼Œç„¶åŽæ’å…¥ã€‚123456789101112int insert(Tree *t, int n, struct Node *node);# nè¡¨ç¤ºåŽŸæ¥äºŒå‰æ ‘èŠ‚ç‚¹çš„ä¸ªæ•°# å‰åºéåŽ† void printTree(Tree* root)&#123; if(root ==NULL) &#123; return ; &#125; print(root-&gt;value) printTree(root-&gt;lchild) printTree(root-&gt;rchild)&#125; ç¬¬ä¸‰ç§æ€è·¯ï¼šå› ä¸ºæ˜¯å®Œå…¨äºŒå‰æ ‘ï¼Œé‚£ä¹ˆæ’å…¥çš„ç‚¹åªèƒ½åœ¨æœ€ä¸‹ä¸€å±‚ã€‚äºŽæ˜¯æˆ‘ä»¬å¯ä»¥åŽ»æ‰¾æœ€ä¸‹ä¸€å±‚çš„ä¸­é—´ç‚¹ï¼Œæ‰¾åˆ°æ ¹çš„å·¦å­æ ‘çš„æœ€å³ä¸‹çš„èŠ‚ç‚¹ï¼Œå¦‚æžœè¿™ä¸ªç‚¹å­˜åœ¨ï¼Œé‚£ä¹ˆè¯´æ˜Žï¼Œæœ€ä¸‹ä¸€å±‚çš„å·¦è¾¹å·²ç»å¡«æ»¡ï¼Œé€’å½’å³å­æ ‘ï¼Œå¦åˆ™é€’å½’å·¦å­æ ‘ã€‚ å‚è€ƒæ–‡çŒ®http://www.voidcn.com/article/p-kbxsvnyq-yq.htmlhttps://www.toutiao.com/i6192546626911126017/https://blog.csdn.net/psc0606/article/details/48742239 inplace åŽ»é™¤è¿žç»­çš„ 0ç»™å®šä¸€ä¸ªä¸€ç»´æ•´æ•°æ•°ç»„ï¼Œä¸ä½¿ç”¨é¢å¤–çš„ç©ºé—´ï¼Œæœ¬åœ°åŽ»æŽ‰æ•°ç»„ä¸­è¿žç»­çš„0ã€‚123456789101112131415161718192021222324252627282930#include&lt;iostream&gt;using namespace std;int RemoveDuplicates(int* sortBuffer,int length)&#123; if(sortBuffer == NULL || length == 0) &#123; return false; &#125; int count = 0; for(int i = 1; i &lt; length; i++) &#123; if(sortBuffer[i] ==0 &amp;&amp; 0 == sortBuffer[i-1]) &#123; continue; &#125; else &#123; sortBuffer[count]=sortBuffer[i]; count++; &#125; &#125; return count; &#125;int main()&#123; int length =sizeof(array)/sizeof(int); &#125; æœ€å¤§è¿žç»­å­æ•°ç»„å’Œå·²çŸ¥ä¸€ä¸ªæ•´æ•°äºŒç»´æ•°ç»„ï¼Œæ±‚æœ€å¤§çš„å­æ•°ç»„å’Œ(å­æ•°ç»„çš„å®šä¹‰ä»Žå·¦ä¸Šè§’(x0,y0) åˆ°å³ä¸‹è§’(x1,y1)çš„æ•°ç»„)å…ˆè€ƒè™‘ä¸€ç»´æ•´æ•°æ•°ç»„çš„æƒ…å†µã€‚æœ€å¤§è¿žç»­å­åºåˆ—çš„DPåŠ¨æ€è½¬ç§»æ–¹ç¨‹ä¸º 1234567891011121314151617181920212223242526#include&lt;iostream&gt;using namespace std;int Max(int a, int b)&#123; return a&gt;b ?a:b;&#125;int FindGreatestSubarray(int *arr, int n)&#123; int sum =arr[0]; int max =arr[0]; for(int i =1; i&lt;n; i++)&#123; sum =Max(sum+arr[i], arr[i]); max =Max(sum, max) /** if(sum &gt;=max)&#123; max =sum; &#125; */ &#125; return max;&#125;int main()&#123; return 0;&#125;è®²è§£é“¾æŽ¥ï¼šhttp://kubicode.me/2015/06/23/Algorithm/Max-Sum-in-SubMatrix/ æœ¬é¢˜ç›®çš„è¦æ±‚æ˜¯ä»ŽäºŒä½çš„æ•°ç»„ä¸­æ±‚è§£æœ€å¤§çš„å­çŸ©é˜µã€‚æˆ‘ä»¬å¯ä»¥å°†å…¶è½¬åŒ–æˆä¸€ç»´æ•°ç»„çš„é—®é¢˜ã€‚å¦‚æžœæ˜¯äºŒç»´æ•°ç»„å¯ä»¥åŽ‹ç¼©ä¸ºä¸€ç»´æ•°ç»„ï¼ˆæˆ‘å½“æ—¶ä¹Ÿæ˜¯ä¸æ‡‚è¿™é‡Œï¼‰ã€‚å¦‚æžœæœ€å¤§å­çŸ©é˜µå’ŒåŽŸçŸ©é˜µç­‰é«˜ï¼Œå°±å¯ä»¥è¿™æ ·åŽ‹ç¼©ã€‚12345678910111213141516171819202122232425262728293031323334353637383940414243#include&lt;stdio.h&gt;#include&lt;iostream&gt;#include&lt;vector&gt;using namespace std;#define inf 0x3f3f3f3fint Max(int a, int b)&#123; return a&gt;b? a:b;&#125;// æ±‚è§£ä¸€ç»´æ•°ç»„çš„æœ€å¤§è¿žç»­å­æ•°åˆ—int FindGreatestSubarray(int *arr, int n)&#123; int sum =arr[0]; int max =arr[0]; for(int i =1;i&lt;n;i++)&#123; sum =Max(sum+arr[i], arr[i]) if(sum &gt;=max)&#123; max =sum; &#125; &#125; return max;&#125;int GreatestMatrix(int[][] arr, int rows, int cols)&#123; int maxVal =- inf for(int i =0 ; i &lt;rows; i++)&#123; vector&lt;int&gt; temp(arr[i]); maxVal =Max(maxVal, FindGreatestSubarray(temp)); // å¾—åˆ°ç¬¬ä¸€è¡Œçš„æœ€å¤§å’Œ // å°†è¡Œçš„nä¸ªå…ƒç´ åŠ åˆ°ä¸Šä¸€è¡Œï¼Œç„¶åŽè®¡ç®—æœ€å¤§å’Œ for(int j =i+1; j&lt;rows; j++)&#123; for(int k =0;k&lt;cols ;k++)&#123; temp[k] =arr[j][k]; &#125; // ä¾æ¬¡0~kè¡Œçš„æœ€å¤§å’Œ maxVal =Max(maxVal, FindGreatestSubarray(temp)) &#125; &#125;&#125;int main()&#123;&#125; èšç±»(clustering )èšç±»æ˜¯ä¸€ç§æ— ç›‘ç£å­¦ä¹ (Unsupervised Learning)ï¼Œè¯¥ç®—æ³•åŸºäºŽæ•°æ®å†…éƒ¨çš„ç‰¹å¾å¯»æ‰¾æ ·æœ¬çš„è‡ªç„¶æ—ç¾¤(é›†ç¾¤)ã€‚é€šå¸¸ä½¿ç”¨æ•°æ®å¯è§†åŒ–æ¥è¯„ä»·ç»“æžœã€‚åº”ç”¨åœºæ™¯ï¼šæ–°é—»èšç±»ï¼Œæ–‡ç« æŽ¨èï¼Œç»†åˆ†å®¢æˆ·ã€‚ æœ€å¸¸è§çš„èšç±»ç®—æ³•å°±æ˜¯Kå‡å€¼(K-Means)ï¼šä»¥ç©ºé—´ä¸­kä¸ªç‚¹ä¸ºä¸­å¿ƒè¿›è¡Œèšç±»ï¼Œå¯¹æœ€é è¿‘ä»–ä»¬çš„å¯¹è±¡å½’ç±»ï¼Œé€šè¿‡è¿­ä»£çš„æ–¹æ³•ï¼Œé€æ¬¡æ›´æ–°å„èšç±»ä¸­å¿ƒçš„å€¼ï¼Œç›´åˆ°å¾—åˆ°æœ€å¥½çš„èšç±»ç»“æžœã€‚æœ€åŽå¸Œæœ›è¾¾åˆ°çš„ç›®çš„ï¼šèšç±»ä¸­çš„å¯¹è±¡ç›¸ä¼¼åº¦è¾ƒé«˜ï¼Œèšç±»ä¹‹é—´çš„ç›¸ä¼¼åº¦æ¯”è¾ƒä½Žã€‚ ç›¸ä¼¼åº¦è®¡ç®—ï¼šä¸åŒçš„ç®—æ³•éœ€è¦çš„â€ç›¸ä¼¼åº¦â€æ˜¯ä¸ä¸€æ ·çš„ï¼Œå¯¹äºŽç©ºé—´ä¸­çš„ç‚¹ï¼Œæˆ‘ä»¬ä¸€èˆ¬é€‰å–æ¬§å¼è·ç¦»æ¥è¿›è¡Œè¡¡é‡ï¼Œè®¤ä¸ºè·ç¦»è¶Šè¿‘ï¼Œæ•°æ®ä¹‹é—´è¶Šç›¸ä¼¼ã€‚ å †æŽ’åºå †æ˜¯ä¸€æ£µé¡ºåºå­˜å‚¨çš„å®Œå…¨äºŒå‰æ ‘ï¼Œå †æŽ’åºæ˜¯ä¸€ç§æ ‘å½¢é€‰æ‹©æŽ’åºï¼Œå…¶æ—¶é—´å¤æ‚åº¦ä¸ºO(nlogn)ï¼Œç©ºé—´å¤æ‚åº¦:å¯¹äºŽè®°å½•è¾ƒå°‘çš„æ–‡ä»¶ä¸æŽ¨èä½¿ç”¨ï¼Œå¯¹äºŽè¾ƒå¤§çš„æ–‡ä»¶è¿˜æ˜¯æœ‰æ•ˆçš„.å †åˆ†ä¸ºå¤§æ ¹å †å’Œå°æ ¹å †ã€‚å¤§æ ¹å †çš„è¦æ±‚æ˜¯æ¯ä¸ªèŠ‚ç‚¹çš„å€¼éƒ½ä¸å¤§äºŽå…¶çˆ¶èŠ‚ç‚¹çš„å€¼ï¼Œå³A[PARENT[i]] &gt;= A[i]ã€‚å°æ ¹å †çš„è¦æ±‚æ˜¯æ¯ä¸ªèŠ‚ç‚¹çš„å€¼éƒ½ä¸å°äºŽå…¶çˆ¶èŠ‚ç‚¹çš„å€¼ï¼Œå³A[PARENT[i]] &lt;= A[i]ã€‚ å †çš„æ¯æ¬¡è°ƒæ•´äº¤æ¢å †é¡¶å’Œæœ€åŽä¸€ä¸ªå…ƒç´ ï¼Œç„¶åŽåªæ˜¯è°ƒæ•´å †é¡¶å’Œå †é¡¶çš„å·¦å³å­©å­æ ‘çš„å…³ç³»ã€‚ è¿™ä¸ªæ˜¯è®²è§£è§†é¢‘ 1234567891011121314151617181920212223242526272829303132333435# heap modifydef MAX_Heapify(heap, HeapSize, root): left =2* root+1 right = left +1 larger =root if left &lt;HeapSize and heap[larger] &lt;heap[left]: larger =left if right &lt; HeapSize and heap[larger] &lt;heap[right]: larger =right # if modify the larger then exchange it if larger != root: heap[larger], heap[root] =heap[root], heap[larger] MAX_Heapify(heap, HeapSize, larger)# Build the heapdef Build_MAX_Heap(heap): HeapSize =len(heap) # from the end to the begin for i in range((HeapSize -2)//2, -1,-1): MAX_Heapify(heap, HeapSize, i)# sort after building the heapdef HeapSort(heap): Build_MAX_Heap(heap) for i in range(len(heap)-1, -1, -1): heap[0], heap[i] =heap[i], heap[0] MAX_Heapify(heap, i, 0) return heapif __name__ =="__main__": a =[30, 50, 57, 77, 62, 78, 94, 80, 84] print(a) print("without sort but with build heap") Build_MAX_Heap(a) print(a) è¡¥å……ï¼šstack vs heap vs queue: ä¸­æ–‡ç¿»è¯‘çš„æ—¶å€™æœ‰åå·®ï¼Œæœ€å¥½ä½¿ç”¨å› ä¸ºæ¥è¿›è¡Œç†è§£ã€‚stack å°±æ˜¯å¸¸è§çš„æ ˆï¼Œ we say Last in first Out (LIFO) or First in Last out (FILO); äºŽæ­¤ç›¸å¯¹åº”çš„æ˜¯ queue, the first person in line is the first person to get out of line. This is FIFO. è¿™ä¸ªå’Œæ•°æ®ç»“æž„ä¸­çš„ æ ˆå’Œé˜Ÿåˆ—æ˜¯ä¸€ä¸€å¯¹åº”çš„ã€‚ç„¶åŽè¯´ä¸€ä¸‹ heapï¼Œæ˜¯ç®—æ³•ä¸­çš„ä¸€ç§ç‰¹æ®Šçš„æ ‘å½¢ç»“æž„ã€‚Heap is a tree with some special property. That special property of the heap is, the value of a node must be &gt;= or &lt;= to its children. And one most important property of heap is all leaves must be at level h or at h-1. (where h is the height of the tree). This also called heap must be a complete binary tree. å‚è€ƒæ–‡çŒ®https://blog.csdn.net/minxihou/article/details/51850001https://blog.csdn.net/chibangyuxun/article/details/53018294 KMP(å­—ç¬¦ä¸²é«˜æ•ˆæŸ¥æ‰¾)å‡è®¾ä¸¤ä¸ªå­—ç¬¦ä¸²çš„é•¿åº¦åˆ†åˆ«æ˜¯mï¼Œn(m&gt;n)ï¼Œåœ¨é•¿åº¦ä¸ºmä¸­çš„å­—ç¬¦ä¸²æŸ¥æ‰¾é•¿åº¦ä¸ºnçš„å­—ç¬¦ä¸²ï¼Œæœ€å¸¸è§çš„æ–¹å¼æ˜¯æš´åŠ›æ±‚è§£ï¼Œä½†æ˜¯è¿™ä¸ªå¸¸è§„è§£æ³•çš„æ—¶é—´å¤æ‚åº¦æ˜¯O(nm)ã€‚KMPé€šè¿‡ä¸€ä¸ªO(n)çš„é¢„å¤„ç†ï¼Œå¯ä»¥ä½¿å¾—æ—¶é—´å¤æ‚åº¦é™ä¸ºO(n+m).ä»£ç å®žçŽ°(è§†é¢‘è®²è§£(ç§‘å­¦ä¸Šç½‘))123456789101112131415161718192021222324def kmp_match(s, p): m, n =len(s) ,len(p) cur =0 table = partial_table(p) while cur &lt;= m-n: for i in range(n): if s[i+cur] != p[i]: cur += max(i -table[i-1], 1) break else: return True return Falsedef partial_table(p): prefix =set() postfix =set() ret =[0] for i in range(1, len(p)): prefix.add(p[:i]) postfix =&#123; p[j:i+1] for j in range(1, i+1)&#125; ret.append(len((prefix &amp; postfix or &#123;&apos;&apos;&#125;).pop())) # &amp;ä¸¤ä¸ªsetæ±‚äº¤é›† return retprint(partial_table(&apos;ABCDABD&apos;))print(kmp_match(&quot;BBC ABCDAB ABCDABCDABDE&quot;, &quot;ABCDABD&quot;)) å‚è€ƒæ–‡çŒ®https://www.cnblogs.com/fanguangdexiaoyuer/p/8270332.html äºŒå‰æ ‘çš„éåŽ†åœ¨pythonä¸­äºŒå‰æ ‘çš„ç»“æž„:12345class BinNode(): def __init__(self, val): self.value =val self.lchild =None self.rchild =None å…ˆåºéåŽ†(preOrder)ç¬¬ä¸€ç§æ€è·¯æ˜¯é€’å½’å®žçŽ°ï¼Œç¬¬äºŒç§æ€è·¯å€ŸåŠ©æ ˆçš„ç»“æž„æ¥å®žçŽ°ã€‚æ ˆçš„å¤§å°ç©ºé—´ä¸ºO(h)ï¼Œhä¸ºäºŒå‰æ ‘é«˜åº¦ï¼›æ—¶é—´å¤æ‚åº¦ä¸ºO(n)ï¼Œnæ˜¯æ ‘çš„èŠ‚ç‚¹çš„ä¸ªæ•°ã€‚123456789101112131415161718192021# é€’å½’def preOrder(self, root): if root == None: return print(root.val) self.preOrder(root.lchild) self.preOrder(root.rchild)# å€ŸåŠ©æ ˆç»“æž„def preOrder(self, root): if root == None: return myStack =[] node =root while node or myStack: while node: print(node.val) myStack.append(node) node =node.lchild node =myStack.pop() node =node.rchild ä¸­åºéåŽ†(inOrder)é€’å½’å’Œéžé€’å½’ä¸¤ç§å®žçŽ°æ€è·¯ã€‚å…¥æ ˆçš„é¡ºåºæ˜¯ä¸€æ ·çš„ï¼Œåªæ˜¯æ”¹å˜çš„éåŽ†(print())çš„é¡ºåº.123456789101112131415161718192021# é€’å½’def inOrder(self, root): if root ==None: return self.inOrder(root.lchild) print(root.val) self.inOrder(root.rchild)# å€ŸåŠ©æ ˆç»“æž„def inOrder(self, root): if root ==None: return myStack =[] node =root while node or myStack: while node: myStack.append(node) node =node.lchild node = myStack.pop() print(node.val) node =node.rchild åŽåºéåŽ†(post order)ä»ç„¶æ˜¯é€’å½’å’Œéžé€’å½’ç‰ˆæœ¬ï¼Œéžé€’å½’ä¸­ä½¿ç”¨ä¸¤ä¸ªstack,ä¸¤ä¸ªstackçš„åŽè¿›å…ˆå‡ºç­‰äºŽä¸€ä¸ªå…ˆè¿›å…ˆå‡ºã€‚12345678910111213141516171819202122232425# é€’å½’def postOrder(self, root): if root == None: return self.postOrder(root.lchild) self.postOrder(root.rchild) print(root.val)# å€ŸåŠ©æ ˆç»“æž„def postOrder(self, root): if root ==None: return myStack1 =[] myStack2 =[] node =root myStack1.append(node) while myStack1: node =myStack1.pop() if node.lchild: myStack1.append(node.lchild) if node.rchild: myStack1.append(node.rchild) myStack2.append(node) while myStack2: print(myStack2.pop().val) å±‚åºéåŽ†ä½¿ç”¨åˆ°äº†é˜Ÿåˆ—çš„æ€æƒ³ï¼Œå…ˆè¿›å…ˆå‡ºã€‚å®žé™…ä¸Šï¼Œç”¨çš„æ˜¯Pythonä¸­list.pop(0).æ³¨æ„é»˜è®¤æ˜¯list.pop(-1),ä¹Ÿå°±æ˜¯é»˜è®¤å¼¹å‡ºçš„æ˜¯æœ€åŽä¸€ä¸ªå…ƒç´ ã€‚1234567891011121314def levelOrder(self, root): if root ==None: return myQueue =[] node =root myQueue.append(node) while myQueue: # remove and return item at index (default last) node =myQueue.pop(0) print(node.val) if node.lchild != None: myQueue.append(node.lchild) if node.rchild != None: myQueue.append(node.rchild) å‚è€ƒæ–‡çŒ®https://blog.yangx.site/2016/07/22/Python-binary-tree-traverse/ æ—‹è½¬æ•°ç»„æ‰¾æœ€å°å€¼æŠŠä¸€ä¸ªæ•°ç»„æœ€å¼€å§‹çš„è‹¥å¹²ä¸ªå…ƒç´ æ¬åˆ°æ•°ç»„çš„æœ«å°¾ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºæ•°ç»„çš„æ—‹è½¬ã€‚ è¾“å…¥ä¸€ä¸ªéžé€’å‡æŽ’åºçš„æ•°ç»„çš„ä¸€ä¸ªæ—‹è½¬ï¼Œè¾“å‡ºæ—‹è½¬æ•°ç»„çš„æœ€å°å…ƒç´ ã€‚ ä¾‹å¦‚æ•°ç»„{3,4,5,1,2}ä¸º{1,2,3,4,5}çš„ä¸€ä¸ªæ—‹è½¬ï¼Œè¯¥æ•°ç»„çš„æœ€å°å€¼ä¸º1ã€‚ NOTEï¼šç»™å‡ºçš„æ‰€æœ‰å…ƒç´ éƒ½å¤§äºŽ0ï¼Œè‹¥æ•°ç»„å¤§å°ä¸º0ï¼Œè¯·è¿”å›ž0ã€‚ æš´åŠ›æ±‚è§£å¤§å¤šæ•°çš„é—®é¢˜éƒ½æ˜¯å¯ä»¥æš´åŠ›æ±‚è§£çš„â€“é²è¿…ã€‚(In China å‡¡æ˜¯ä¸çŸ¥é“æ˜¯è°è¯´ï¼Œéƒ½å¯ä»¥è¯´æ˜¯é²è¿…è¯´çš„; In USï¼Œå‡¡æ˜¯ä¸çŸ¥é“è°è¯´çš„ï¼Œas said by Albert Einstein)å› ä¸ºåŽŸæ¥çš„æ•°ç»„å‡è®¾æ˜¯å¢žåºï¼Œæ‰€ä»¥å¦‚æžœå‡ºçŽ°äº†çš„æŸä¸€ä¸ªå…ƒç´ æ¯”ä¸Šä¸€ä¸ªå…ƒç´ å°ï¼Œè¯¥å…ƒç´ å°±æ˜¯è¿™ä¸ªåºåˆ—ä¸­çš„æœ€å°å€¼ã€‚(è¿™ä¸ªæƒ…å†µå…·æœ‰å”¯ä¸€æ€§å§).æ—¶é—´å¤æ‚åº¦O(N)123456789101112def minNumberInRotateArray(rotateArray): arr =rotateArray # just because of laziness if not arr: return 0 if len(arr) ==2: return arr[1] num =arr[0] for i in range(1, len(arr)): if arr[i] &gt;= num: num =arr[i] else: return arr[i] é€’å½’ç‰ˆæœ¬æ—‹è½¬æ•°ç»„ä¹Ÿæ˜¯ä¸€ç§æœ‰åºæ•°ç»„ï¼Œæ—¶é—´å¤æ‚åº¦O(N)â€¦ï¼Œé¢è¯•å®˜è¯´æ”¹è¿›å§â€¦ä½¿ç”¨äºŒåˆ†æ³•é™åˆ°O(logN)ã€‚ In big-O() notation, constant factors are removed. Converting from one logarithm base to another involves multiplying by a constant factor. æ‰€ä»¥è¿™å…³é”®æ˜¯åŽé¢çš„å˜é‡è€Œä¸æ˜¯ä»¥2 æˆ–è€…e ä¸ºåº•ã€‚è¿™ä¸¤ç§å†™æ³•éƒ½æ˜¯æˆç«‹çš„ã€‚ So O(log N) is equivalent to O(log2 N) due to a constant factor.12345678910111213def minNumberInRotateArray(rotateArray): arr = rotateArray if not arr: return 0 if len(arr) ==2: return arr[1] mid =int(len(arr) /2) if arr[mid] &gt; arr[0]: return minNumberInRotateArray(arr[mid:]) elif arr[mid] &lt;arr[0]: return minNumberInRotateArray(arr[:mid+1]) else: return minNumberInRotateArray(arr[1:]) éžé€’å½’ç‰ˆæœ¬é€’å½’ç‰ˆæœ¬å çš„å†…å­˜æ¯”è¾ƒå¤šï¼Œæ”¹è¿›å§..äºŽæ˜¯éžé€’å½’çš„ç‰ˆæœ¬å°±å‡ºæ¥äº†ã€‚éœ€è¦æ³¨æ„çš„æ˜¯è¯¥ç‰ˆæœ¬çš„åˆ¤æ–­æ¯”è¾ƒæ¡ä»¶(å…¶ä¸­ä¸€ç‚¹æ˜¯å’Œ arr[right]è¿›è¡Œæ¯”è¾ƒ)ä¸€å®šè¦å°å¿ƒï¼Œéƒ½æ˜¯å°å‘â€¦è¿™ç§è§£æ³•å…³é”®æ˜¯éœ€è¦æ‰¾åˆ°éžå‡åºåˆ—ï¼ˆå’ŒåŽŸåºåˆ—ç›¸åŒçš„å½¢å¼ï¼‰ï¼Œç„¶åŽå°±å˜å¾—å¯é¢„æµ‹ï¼Œå¯ä»¥æŽ’é™¤è¿™éƒ¨åˆ†å…¶ä»–çš„æ•°å­—ã€‚ ä¸‹é¢è¿™ä¸ªç¡®å®žæ˜¯æ˜¯æ­£ç¡®çš„ä»£ç ï¼Œæ³¨æ„ä½“ä¼šç»†èŠ‚ã€‚éžé€’å‡çš„arrayï¼Œç„¶åŽæ˜¯ä»ŽåŽå¾€å‰æ¯”è¾ƒçš„ã€‚if lese ä¸­çš„æ¡ä»¶æ˜¯å¯ä»¥è°ƒæ¢çš„ï¼Œè™½ç„¶è¿™è¯å¬èµ·æ¥åƒæ˜¯åºŸè¯ã€‚è¿™è¯´æ˜Žä¸¤ä¸ªæ¡ä»¶çš„åˆ¤æ–­çš„é¡ºåºä¸åº”è¯¥äº§ç”Ÿä¸åŒçš„ç»“æžœã€‚1234567891011121314def minNumberInRotateArray(rotateArray): arr =rotateArray left =0 right =len(arr) -1 while left &lt; right: mid = int((left+ right)/2) if arr[mid] &gt;arr[right]: left =mid+1 # ä¸åŒ…å«mid å› ä¸ºmid ç»ä¸å¯èƒ½æ˜¯ æœ€å°å€¼ elif arr[mid] &lt;arr[right]: right =mid # åŒ…å«mid å› ä¸ºmid å¯èƒ½æ˜¯æœ€å°å€¼ else: right -=1 # è¿™ä¸ªä¹Ÿæ˜¯å¯ä»¥æ¢æˆ left +=1 ï¼Œåªè¦æ˜¯èƒ½å¤Ÿæ¸è¿›çš„ return arr[left] æ–‡ç« çš„å°æ ‡é¢˜æ˜¯æ±‚è§£æœ€å°(å¤§)å€¼ï¼Œä¸Šè¿°è®²è¿°çš„éƒ½æ˜¯æœ€å°å€¼ã€‚å¦‚æžœæ±‚è§£æœ€å¤§å€¼ï¼Œç¨å¾®ä¿®æ”¹ä¸€ä¸‹ç‰¹æ®Šæƒ…å†µçš„åˆ¤æ–­æ¡ä»¶ï¼Œå°†è¿”å›žçš„index-1 å³å¯ã€‚å› ä¸ºæœ€å°å€¼çš„ä½ç½®æ˜¯â€æŸä¸€ä¸ªå…ƒç´ æ¯”ä¸Šä¸€ä¸ªå…ƒç´ å°â€ï¼Œé‚£ä¹ˆ index-1 ä¹‹åŽè¿™ä¸ªå…ƒç´ å°±æ˜¯è¯¥æ•°ç»„åºåˆ—ä¸­æœ€å¤§çš„ã€‚ å‚è€ƒæ–‡çŒ®https://blog.csdn.net/u010005281/article/details/79823154 å•é“¾è¡¨åè½¬å•é“¾è¡¨çš„åè½¬æœ‰å¾ªçŽ¯è¿­ä»£å’Œé€’å½’ä¸¤ç§æ–¹æ³•ã€‚å•é“¾è¡¨èŠ‚ç‚¹123456class Node(object): def __init__(self): self.value =None self.next =None def __str__(self): return str(self.value) å¾ªçŽ¯è¿­ä»£å¾ªçŽ¯è¿­ä»£éœ€è¦ç»´æŒä¸‰ä¸ªå˜é‡ï¼špre, head, nextã€‚preæ˜¯headçš„preï¼Œnextæ˜¯headçš„next.(åºŸè¯)1234567891011def reverse_Linkedlist(head): if not head or not head.next : #ç©ºæŒ‡é’ˆæˆ–è€…åªæœ‰ä¸€ä¸ªç»“ç‚¹ return head pre =None # éœ€è¦åˆ›å»ºä¸€ä¸ªNone ä½œä¸ºæœ€åŽçš„æŒ‡å‘ while head: next = head.next head.next =pre pre = head head =next # æœ€åŽä¸€æ¬¡å¾ªçŽ¯è¿­ä»£ Head==Noneï¼Œè€ŒpreæŒ‡å‘äº†å¤´ç»“ç‚¹ return pre é€’å½’ä¸€å¼€å§‹æ­£å¸¸æƒ…å†µä¸‹ä¸ä¼šæ‰§è¡Œifåˆ¤æ–­ï¼Œåˆ©ç”¨é€’å½’èµ°åˆ°é“¾è¡¨çš„æœ«ç«¯ï¼Œnew_headçš„å€¼æ²¡æœ‰å‘ç”Ÿæ”¹å˜ï¼Œä¸ºé“¾è¡¨çš„æœ€åŽä¸€ä¸ªèŠ‚ç‚¹ï¼Œåè½¬ä¹‹åŽå°±æˆä¸ºäº†æ–°é“¾è¡¨çš„headã€‚12345678def reverse_Linkedlist(head): if not head or not head.next: return head new_head = reverse_Linkedlist(head.next) # å°†å½“å‰èŠ‚ç‚¹è®¾ç½®ä¸ºåŽé¢èŠ‚ç‚¹çš„åŽç»­èŠ‚ç‚¹ head.next.next =head head.next =None return new_head æµ‹è¯•12345678910111213141516171819202122232425if __name__ == &quot;__main__&quot;: three = Node() three.value =3 two =Node() two.value =2 two.next =three one =Node() one.value =1 one.next =two head =Node() head.value =0 head.next =one &quot;&quot;&quot; while head: print(head.value, ) head =head.next print(&quot;******&quot;) &quot;&quot;&quot; newhead = reverse_Linkedlist(head) while newhead: print(newhead.value) newhead =newhead.next å‚è€ƒæ–‡çŒ®https://foofish.net/linklist-reverse.html]]></content>
      <categories>
        <category>ç®—æ³•</category>
      </categories>
      <tags>
        <tag>é€’å½’</tag>
        <tag>è¿­ä»£</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Introduction to Ensemble]]></title>
    <url>%2F2018%2F06%2F13%2FIntroduction-to-Ensemble%2F</url>
    <content type="text"><![CDATA[è™½ç„¶åœ¨Titanic Challengeåšå®¢ä¸­ä»‹ç»äº†æ¨¡åž‹èžåˆ(Ensemble)ï¼Œä½†æ˜¯å…¶æ˜¯ä»¥ç‰¹å¾æå–ä¸ºä¸»æ—‹å¾‹ï¼Œæ‰€ä»¥è¿™ç¯‡åšå®¢å°†ç»“åˆå®žæˆ˜ä»Žæœ€åŸºæœ¬çš„Ensembleåˆ°æ¯”è¾ƒå¤æ‚çš„Ensemble,æœ€åŽå®‰åˆ©ä¸€ä¸ªç”¨äºŽæ¨¡åž‹èžåˆçš„package. åœ¨æ–‡ä¸­å¯èƒ½å‡ºçŽ°ä¸€äº›æ¦‚å¿µï¼Œå¦‚æžœä¸å¤ªç†Ÿæ‚‰ï¼Œå¯ä»¥å…ˆåŽ»æ–‡æœ«æ‰¾æ‰¾çœ‹ï¼Œå¦‚æžœæˆ‘æ²¡æœ‰è¡¥å……ï¼Œé‚£ä¹ˆè¯·è‡ªè¡ŒGoogleå§ã€‚ä¸ºäº†ä¸å½±å“é˜…è¯»çš„æµç•…æ€§ï¼Œæˆ‘å¯èƒ½åœ¨å…¶ä¸­ä¸ä¼šæ¶‰åŠå¾ˆå¤šæ¦‚å¿µè§£é‡Šï¼Œæ¯”å¦‚è¯´ROCæ›²çº¿å’ŒAUCå€¼ã€‚å¥½ï¼Œæˆ‘ä»¬è¿›å…¥æ­£æ–‡ã€‚ Datasetæœ¬æ–‡ä¹¦å†™è¿‡ç¨‹ä¸­æ²¿ç”¨å‚è€ƒåšå®¢(Introduction to Python Ensembles)çš„æ•°æ®é›†ã€‚å¯ä»¥åŽ»è¿™é‡Œä¸‹è½½ï¼Œå½“ç„¶æŽ¨èä½¿ç”¨åŽŸä½œè€…å¤„ç†ä¹‹åŽçš„æ•°æ®é›†ï¼Œyou can find hereã€‚ ç®€å•ä»‹ç»ä¸€ä¸‹è¿™ä¸ªæ•°æ®é›†ï¼šFederal Election Commissionè¿™ä¸ªç»„ç»‡æ”¶é›†äº†2007åˆ°2016å¹´çš„donationsè®°å½•ï¼Œæœ€åŽå¾—å‡º When Scientists Donate To Politicians, Itâ€™s Usually To Democratsè¿™æ ·çš„ç»“è®ºã€‚ å¥½äº†ï¼Œæˆ‘ä¸æƒ³åœ¨æ•°æ®é›†è¿™é‡ŒèŠ±å¤ªå¤šæ—¶é—´ï¼Œå³ä½¿ä½ ä¸å¤ªæ˜Žç™½æ•°æ®é›†çš„å…·ä½“å«ä¹‰ï¼Œå®Œå…¨ä¸å½±å“ä¸‹æ–‡çš„é˜…è¯»ï¼Œå› ä¸ºä½ å¾ˆå¿«å°±ä¼šå‘çŽ°ä¸‹æ–‡å¹¶æ²¡æœ‰è¿›è¡Œå¾ˆå¤šå’ŒåŽŸæ•°æ®é›†ç›¸å…³çš„å†…å®¹ï¼Œæ›´å¤šçš„æ˜¯æ¨¡åž‹èžåˆã€‚å½“ç„¶ä½ å¦‚æžœèƒ½å¤Ÿçœ‹æ‡‚ï¼Œå¯ä»¥æ„Ÿå—ä¸€ä¸‹ä¸Šè¿°ç»“è®ºçš„æœ‰è¶£ä¹‹å¤„ã€‚ Give me codes:1234567891011121314151617181920import numpy as npimport pandas as pdimport matplotlib.pyplot as plt%matplotlib inline# import data# Always good to set a seed for reproducibilitySEED = 222np.random.seed(SEED)df = pd.read_csv(&apos;input.csv&apos;)from sklearn.model_selection import train_test_splitfrom sklearn.metrics import roc_auc_scoredef get_train_test(test_size= 0.95): y =1*(df.cand_pty_affiliation ==&apos;REP&apos;) X = df.drop([&quot;cand_pty_affiliation&quot;], axis=1) X = pd.get_dummies(X, sparse=True) X.drop(X.columns[X.std() == 0], axis=1, inplace=True) return train_test_split(X, y, test_size=test_size, random_state=SEED)xtrain, xtest, ytrain, ytest = get_train_test()df.head() ç®€å•çœ‹ä¸€ä¸‹æ•°æ®é•¿ä»€ä¹ˆæ ·å­ï¼Œè™½ç„¶æœ‰äººå¯èƒ½ä¸å¤ªæ‡‚ã€‚ Begin with ensembleä¹‹å‰çš„åšå®¢ä¸»è¦ä»Žensembleåˆ†ç±»çš„è§’åº¦é˜è¿°ï¼ŒçŽ°åœ¨ä»Žæ¦‚å¿µçš„è§’åº¦é˜è¿°ã€‚Ensemble: combining predictions from several models averages out idiosyncratic(æ€ªå¼‚çš„) errors and yield better overall predictions.(æœ‰æ—¶å€™æˆ‘è§‰å¾—è‹±æ–‡è¯´å¾—å¾ˆæ¸…æ¥šï¼Œæ‰€ä»¥å°±ä¸ç¿»è¯‘æˆä¸­æ–‡äº†ï¼Œæ±‚ä¸è¢«æ‰“ã€‚) ç®€å•çš„è¯´ï¼Œå°±æ˜¯å¯ä»¥é˜²æ­¢è¿‡æ‹Ÿåˆã€‚å½“è¿‡æ‹Ÿåˆæ—¶ï¼Œè¾¹ç•Œæ›²çº¿å°±å›žåŽ»è¿‡åˆ†è€ƒè™‘æŸä¸€ä¸ªæˆ–æŸä¸€äº›å°‘æ•°çš„ç‚¹ï¼Œè¿™æ—¶å€™ ensembleé€šè¿‡æŸç§combineæœºåˆ¶ï¼Œç„¶åŽå¾—åˆ°ä¸€ä¸ªæ³›åŒ–æ€§èƒ½æ¯”è¾ƒå¥½çš„è¾¹ç•Œæ›²çº¿ï¼Œä¹Ÿå°±æ˜¯æ¯”è¾ƒå¥½çš„æ¨¡åž‹ã€‚ Decision Treeé¦–å…ˆæˆ‘ä»¬ä»Ž decision treeå¼€å§‹ã€‚A decision tree, which is a tree of if-then rules. The deeper the tree, the more complex the patterns it can capture, but the more prone to overfitting it will be. Because of this, we will need an alternative way of building complex models of decision trees, and an ensemble of different decision trees is one such way.æˆ‘ä»¬å…ˆä½¿ç”¨ depth =112345678from IPython.display import Imagefrom sklearn.metrics import roc_auc_scorefrom sklearn.tree import DecisionTreeClassifiert1 = DecisionTreeClassifier(max_depth=1, random_state=SEED)t1.fit(xtrain, ytrain)p = t1.predict_proba(xtest)[:, 1]print(&quot;Decision tree ROC-AUC score: %.3f&quot; % roc_auc_score(ytest, p)) Decision tree ROC-AUC score: 0.672 å‘çŽ°ç»“æžœä¸å¤ªç†æƒ³ï¼ŒåŠ æ·±depth.1234t2 =DecisionTreeClassifier(max_depth=3, random_state=SEED)t2.fit(xtrain, ytrain)p =t2.predict_proba(xtest)[:, 1]print(&quot;Decision tree ROC-AUC score: %.3f&quot; % roc_auc_score(ytest, p)) Decision tree ROC-AUC score: 0.751 ç”±äºŽæˆ‘ä»¬æœ€åŽè¦ensembleï¼Œè€Œè¿™è¦æ±‚æˆ‘ä»¬è¦æž„é€ æœ‰å·®å¼‚ä½†æ¯ä¸ªä¸æ˜¯é‚£ä¹ˆå·®çš„æ¨¡åž‹ã€‚é¦–å…ˆæˆ‘ä»¬è€ƒè™‘åˆ°ä½¿ç”¨ä¸åŒçš„æ•°æ®é›†ã€‚1234567xtrain_slim =xtrain.drop([&apos;transaction_amt&apos;], axis=1)xtest_slim =xtest.drop([&apos;transaction_amt&apos;], axis=1)t3=DecisionTreeClassifier(max_depth=3, random_state=SEED)t3.fit(xtrain_slim, ytrain)p =t3.predict_proba(xtest_slim)[:,1]print(&apos;Decision tree ROC-AUC score:&#123;&#125;&apos;.format(roc_auc_score(ytest, p))) Decision tree ROC-AUC score:0.7403182587884118é€šè¿‡corr()æ¥æ£€éªŒä¸¤è€…çš„ç›¸å…³æ€§(å·®å¼‚æ€§)123p1 =t2.predict_proba(xtest)[:,1]p2 =t3.predict_proba(xtest_slim)[:,1]pd.DataFrame(&#123;&apos;full_data&apos;:p1, &apos;red_data&apos;:p2&#125;).corr() å‘çŽ°æœ‰ä¸€å®šçš„ç›¸å…³æ€§ï¼Œä½†æ˜¯è¿˜æ˜¯å¯ä»¥å®¹å¿çš„ã€‚äºŽæ˜¯å¼€å§‹èžåˆã€‚1234p1 = t2.predict_proba(xtest)[:, 1]p2 = t3.predict_proba(xtest_slim)[:, 1]p = np.mean([p1, p2], axis=0)print(&quot;Average of decision tree ROC-AUC score: %.3f&quot; % roc_auc_score(ytest, p)) Average of decision tree ROC-AUC score: 0.783æˆ‘ä»¬å‘çŽ°ï¼šä¸¤ä¸ªæ——é¼“ç›¸å½“ï¼ˆ0.74 0.73ï¼‰çš„å¯ä»¥å¾—åˆ°ä¸€ä¸ªæ›´å¥½çš„ç»“æžœï¼Œå¯ä»¥å‡å°‘å†³ç­–å¤±è¯¯ çš„å¹³å‡æ˜¯0.78ã€‚ éœ€è¦æ³¨æ„çš„æ˜¯æˆ‘ä»¬åœ¨æž„é€ ç¬¬äºŒä¸ªæ¨¡åž‹æ—¶å€™é€šè¿‡drop()ä¸¢æŽ‰ä¸€ä¸ªfeatureï¼Œåè€Œå¾—åˆ°ä¸€ä¸ªæ›´å¥½çš„æ¨¡åž‹ã€‚äºŽæ˜¯ä¹Žï¼Œæˆ‘ä»¬æƒ³é€šè¿‡ä½¿ç”¨ä¸åŒçš„å­é›†ï¼ˆä¸åŒçš„ç‰¹å¾ï¼‰æž„é€ ä¸åŒçš„æ¨¡åž‹ï¼Œæ˜¯ä¸æ˜¯èƒ½å¾—åˆ°æ›´å¥½çš„æ¨¡åž‹ï¼Ÿ Random Forest(Bagging)A fast approach that works well in practice is to randomly select a subset of features, fit one decision tree on each draw and average their predictions.This process is known as bootstrapped averaging (often abbreviated bagging), and when applied to decision trees, the resultant model is a Random Forest. (æˆ‘è§‰å¾—åŽŸä½œè€…æ¯”æˆ‘è¯´çš„æ¸…æ¥šï¼Œå€Ÿç”¨äº†)æˆ‘çš„ç†è§£ï¼Œåœ¨ä¸Šé¢å°èŠ‚ä¸­æˆ‘ä»¬ä½¿ç”¨çš„æ˜¯Decision Tree,åœ¨å®žé™…åº”ç”¨ä¸­å‘çŽ°æœ‰å·®å¼‚çš„å¤šä¸ªå­æ ‘çš„æ•ˆæžœè¦æ›´å¥½ä¸€äº›ã€‚è€Œå®žçŽ°è¿™ä¸ªé€”å¾„å¿«é€Ÿçš„æ–¹æ³•å°±æ˜¯ Random Forestã€‚Random åœ¨è¿™é‡Œè¡¨ç¤ºä»»æ„å‡ ä¸ªå­æ ‘(ç‰¹å¾)ï¼Œç„¶åŽè¿™äº›Treeç»„æˆäº†Forestã€‚123456789from sklearn.ensemble import RandomForestClassifierrf =RandomForestClassifier( n_estimators=10, max_features= 3, random_state=SEED)rf.fit(xtrain, ytrain)p =rf.predict_proba(xtest)[:, 1]print(&apos;Average of decision tree ROC-AUC score:&#123;&#125;&apos;.format(roc_auc_score(ytest, p))) Average of decision tree ROC-AUC score:0.844018408542404è¿™å°±æ˜¯å«åšâ€è´¨çš„é£žè·ƒâ€ä»Žé‚£ä¸ª0.783-&gt; 0.844(å°†è¿‘6ä¸ªç™¾åˆ†ç‚¹ï¼Œå¥½å§ï¼Œæœ‰ç‚¹ç¥žç»è´¨äº†â€¦)From nobody to somebody, we are on something.. Ensemble of various modelså¯ä»¥çœ‹å‡ºä¸Šè¿°æ¨¡åž‹ä¸­ï¼Œæœ€åŽçš„æ¨¡åž‹ï¼ˆRandom Forestï¼‰çš„å­æ¨¡åž‹(Decision Tree)ã€‚ä½†æ˜¯å­æ¨¡åž‹å¹¶ä¸æ˜¯å±€é™æ ‘è¿™ä¸€ç§ç»“æž„ï¼Œæˆ‘ä»¬æ›´å¤šçš„é€‰æ‹©ï¼šlinear models, kernel-based models, non-parametric models, neural networks or even other ensembles! ä¸ºäº†é¿å…ä»£ç çš„å†—ä½™æž„é€ äº†ä»¥ä¸‹çš„helper function.123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657# A host of Scikit-learn modelsfrom sklearn.svm import SVC, LinearSVCfrom sklearn.naive_bayes import GaussianNBfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifierfrom sklearn.linear_model import LogisticRegressionfrom sklearn.neighbors import KNeighborsClassifierfrom sklearn.neural_network import MLPClassifierfrom sklearn.kernel_approximation import Nystroemfrom sklearn.kernel_approximation import RBFSamplerfrom sklearn.pipeline import make_pipelinedef get_models(): &quot;&quot;&quot;Generate a library of base learners.&quot;&quot;&quot; nb = GaussianNB() svc = SVC(C=100, probability=True) # Cè¶Šå¤§è¾¹ç•Œè¶Šå¤æ‚ï¼Œä¼šå¯¼è‡´è¿‡æ‹Ÿåˆ knn = KNeighborsClassifier(n_neighbors=3) # KNNç®—æ³•å¯»æ‰¾è®­ç»ƒæ•°æ®ä¸­çš„Kä¸ªæœ€è¿‘çš„æ•°æ®ï¼Œå®ƒä½¿ç”¨æŒ‡å‘æœ€å¤šçš„é‚£ä¸ªç±»åˆ«æ¥ä½œä¸ºé¢„æµ‹çš„è¾“å‡ºã€‚ lr = LogisticRegression(C=100, random_state=SEED) # å¯¹äºŽè¿™ä¸ª c èƒ½çŸ¥é“çš„å°±æ˜¯æ­£åˆ™åŒ–ç³»æ•° smaller values specify stronger regularization. nn = MLPClassifier((80, 10), early_stopping=False, random_state=SEED) gb = GradientBoostingClassifier(n_estimators=100, random_state=SEED) # å­æ¨¡åž‹çš„æ•°é‡ï¼Œé»˜è®¤æ˜¯100, gbcé€šå¸¸ robust to over-fitting, so a large number results in better performance rf = RandomForestClassifier(n_estimators=10, max_features=3, random_state=SEED) models = &#123;&apos;svm&apos;: svc, &apos;knn&apos;: knn, &apos;naive bayes&apos;: nb, &apos;mlp-nn&apos;: nn, &apos;random forest&apos;: rf, &apos;gbm&apos;: gb, &apos;logistic&apos;: lr, &#125; return models def train_predict(model_list): P =np.zeros((ytest.shape[0], len(model_list))) P =pd.DataFrame(P) print(&apos;Fitting models&apos;) cols =list() for i, (name, m) in enumerate(models.items()): print(&quot;%s...&quot; % name, end=&quot; &quot;, flush=False) m.fit(xtrain, ytrain) P.iloc[:, i] =m.predict_proba(xtest)[:, 1] cols.append(name) print(&apos;Done&apos;) P.columns =cols print(&apos;Done.\n&apos;) return P def score_models(P, y): &quot;&quot;&quot;Score model in predication DF&quot;&quot;&quot; print(&apos;Scoring models&apos;) for m in P.columns: score =roc_auc_score(y, P.loc[:, m]) print(&quot;%-26s: %.3f&quot; % (m, score)) print(&apos;Done, \n&apos;) letâ€™s goâ€¦123models =get_models()P =train_predict(models)score_models(P, ytest) This is our base line.Gradient Boosting Machine(GBM) æžœç„¶åä¸è™šä¼ , does bestæˆ‘ä»¬æ¥åˆ†æžä¸€ä¸‹æ¨¡åž‹ä¹‹é—´çš„ç›¸å…³æ€§ï¼ŒåŽŸä½œè€…ä½¿ç”¨çš„mlens package(You can install it with: pip install mlens)ï¼Œæˆ‘è¿™é‡Œç”¨çš„æ˜¯seaborn(install it with: pip install seaborn).åœ¨æ£€æŸ¥ç›¸å…³æ€§(Pearsonç›¸å…³æ€§:è¡¡é‡ä¸¤ä¸ªæ•°æ®é›†åˆçš„çº¿æ€§ç›¸å…³æ€§)æ—¶å€™ï¼Œæˆ‘ä»¬ä½¿ç”¨çš„æ˜¯ç»è¿‡å¤„ç†çš„ç›¸å…³æ€§ã€‚å…·ä½“è¯´æ¥å¯ä»¥ç§°ä¹‹ä¸º error correlation,è¯¦ç»†è§ä»£ç ã€‚ 12345678# look at error correlations is more promising, errors are significantly correlated import seaborn as snsplt.subplots(figsize=(10,8)) corrmat = P.apply(lambda pred: 1*(pred &gt;= 0.5) - ytest.values).corr()sns.set(font_scale=1.5) hm = sns.heatmap(corrmat, cbar=True, annot=True, square=True, fmt=&apos;.2f&apos;, annot_kws=&#123;&apos;size&apos;: 10&#125;) plt.title(&apos;Pearson Correlation of Features&apos;, y=1.05, size=15) plt.show() é¢„æµ‹å€¼å’ŒçœŸå®žå€¼ä¹‹é—´çš„å·®å¼‚ç§°ä¹‹ä¸ºerrorï¼ŒæŸ¥çœ‹errorçš„ Pearson correlationï¼Œæ•ˆæžœæ›´åŠ æ˜Žæ˜¾ã€‚1print(&quot;Ensemble ROC-AUC score: %.3f&quot; % roc_auc_score(ytest, P.mean(axis=1))) Ensemble ROC-AUC score: 0.884ç»“æžœé«˜äºŽæ¯ä¸€ä¸ªå•ç‹¬çš„æ¨¡åž‹ï¼Œä½†æ˜¯ä¸æ˜¯é‚£ä¹ˆæ˜Žæ˜¾ã€‚ Visualize Curve ROC(helper function)æˆ‘ä»¬æ³¨æ„åˆ°ä¹‹å‰ä½¿ç”¨çš„æ‰€æœ‰çš„ç»“æžœçš„è¯„ä»·æ ‡å‡†éƒ½æ˜¯ roc_auc_scoreï¼Œä½†æ˜¯å¹¶æ²¡æœ‰æåŠè¿™æ˜¯ä»€ä¹ˆã€‚å½“ç„¶åœ¨æ­£æ–‡ä¸­ä¹Ÿä¸æ‰“ç®—è§£é‡Šï¼Œå¦‚æžœä¸æ˜¯å¾ˆæ¸…æ¥šï¼Œå¯ä»¥æŸ¥çœ‹æœ¬æ–‡æœ€åŽè¡¥å……æ¦‚å¿µ.ç®€å•æ¥è¯´AUCå¯ä»¥ç”¨æ¥è¡¡é‡â€äºŒåˆ†é—®é¢˜â€çš„æ³›åŒ–èƒ½åŠ›ï¼Œæ˜¯ä¸€ç§è¯„ä»·æŒ‡æ ‡ã€‚æˆ‘ä»¬è¿™é‡Œæƒ³è¯´çš„æ˜¯ visualize Curve ROC,å¯è§†åŒ–ã€‚12345678910111213141516171819202122# a helper function for roc_curvefrom sklearn.metrics import roc_curvedef plot_roc_curve(ytest, P_base_learners, P_ensemble, labels, ens_label): &quot;&quot;&quot;Plot the roc curve for base learners and ensemble.&quot;&quot;&quot; plt.figure(figsize=(10, 8)) plt.plot([0, 1], [0, 1], &apos;k--&apos;) cm = [plt.cm.rainbow(i) for i in np.linspace(0, 1.0, P_base_learners.shape[1] + 1)] for i in range(P_base_learners.shape[1]): p = P_base_learners[:, i] fpr, tpr, _ = roc_curve(ytest, p) plt.plot(fpr, tpr, label=labels[i], c=cm[i + 1]) fpr, tpr, _ = roc_curve(ytest, P_ensemble) plt.plot(fpr, tpr, label=ens_label, c=cm[0]) plt.xlabel(&apos;False positive rate&apos;) plt.ylabel(&apos;True positive rate&apos;) plt.title(&apos;ROC curve&apos;) plt.legend(frameon=False) plt.show()plot_roc_curve(ytest, P.values, P.mean(axis=1), list(P.columns), &quot;ensemble&quot;) æˆ‘ä»¬é¡ºä¾¿æŠŠåˆšæ‰çš„æ¨¡åž‹å¯è§†åŒ–ï¼Œå‘çŽ°ensembleçš„AUCæ˜¯æœ€å¤§çš„ï¼Œæ„å‘³è¿™æ³›åŒ–æ€§èƒ½æ˜¯æœ€å¥½ï¼Œè¿™ä¹Ÿæ˜¯ç¬¦åˆæˆ‘ä»¬çš„è®¤çŸ¥çš„ ã€‚ Beyond ensembles as a simple averageæˆ‘ä»¬å›žåˆ°ä¸»çº¿ä¸Šï¼Œåœ¨ä¸Šä¸€ä¸ªæ¨¡åž‹ä¸­æˆ‘ä»¬æåŠæœ€åŽçš„ ensembleçš„ç»“æžœæ˜¯å¥½äºŽæ¯ä¸ªå•ç‹¬çš„æ¨¡åž‹ï¼Œä½†æ˜¯æ²¡æœ‰é‚£ä¹ˆçªå‡ºã€‚æ ¹æ®ROCæ›²çº¿æˆ‘ä»¬ä¹Ÿå¯ä»¥çœ‹å‡ºï¼Œæœ‰çš„æ¨¡åž‹(KNN)åœ¨è¿™é‡Œè¡¨çŽ°çš„æ²¡æœ‰é‚£ä¹ˆå¥½ï¼Œæˆ‘ä»¬åœ¨æƒ³æ˜¯ä¸æ˜¯ç”±äºŽå› ä¸ºè¿™ä¸ªè€Œæ‹‰åº•äº†æœ€åŽçš„ç»“æžœï¼Œå½“ç„¶è¿™åªæ˜¯çŒœæµ‹ï¼ŒäºŽæ˜¯æˆ‘ä»¬æ‰¾åˆ°äº† tryçš„æ–¹å‘ã€‚å¯èƒ½ç¬¬ä¸€ç›´è§‰åŽ»æŽ‰è¿™ä¸ªæ¨¡åž‹å†è¿›è¡Œèžåˆï¼Œåœ¨è¿™ä¸ªå®žéªŒè¡¨æ˜Žè¯¥ç­–ç•¥æœ€åŽçš„ç»“æžœ0.883ï¼Œç›¸æ¯”ä¸Ž0.884ï¼Œä½ æ‡‚å¾—ï¼Œå¹¶æ²¡æœ‰å˜å¥½ã€‚æˆ‘ä»¬è¿˜æœ‰ä¸€ç§ç­–ç•¥:learn a sensible set of weights to use when averaging predictions.è®©æ¨¡åž‹è‡ªå·±åŽ»å­¦ä¹ å¦‚ä½•è°ƒæ•´å„ä¸ªæ¨¡åž‹ä¹‹é—´çš„æ¯”ä¾‹ã€‚ Learning to combine predicationsä¸ºäº†è®©æ¨¡åž‹è‡ªå­¦ä¹ å„ä¸ªä¹‹é—´çš„é¢„æµ‹æ¯”ä¾‹ï¼Œæˆ‘ä»¬å¼•å…¥äº† meta learner(metaæ˜¯å…ƒï¼Œç†è§£ä¸ºæœ€åŸºç¡€çš„) to learn how to best combine these predictions. é™¤æ­¤ä¹‹å¤–ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä¸åŒçš„æ•°æ®é›†ï¼ŒåƒRandom Forestä½¿ç”¨ä¸åŒçš„æ•°æ®å­é›†(ä¸åŒçš„ç‰¹å¾ç»„æˆçš„æ•°æ®é›†)ã€‚äºŽæ˜¯æˆ‘ä»¬éœ€è¦a method for splitting the training data between the base learners and the meta learner.12345678910base_learners =get_models()meta_learner = GradientBoostingClassifier( n_estimators=1000, loss=&quot;exponential&quot;, max_features=4, max_depth=3, subsample=0.5, learning_rate=0.005, random_state=SEED) ä½¿ç”¨æœ€å¼ºæ¨¡åž‹GBMä½œä¸º meta learnerå¹¶å®šä¹‰å¥½ base_learners.To keep things simple, we split the full training set into a training and prediction set of the base learners. This method is sometimes referred to as Blending. Unfortunately, the terminology differs between communities, so itâ€™s not always easy to know what type of cross-validation the ensemble is using.12345678910# sefine a procedure for generating train and test setsxtrain_base , xpred_base, ytrain_base, ypred_base =train_test_split(xtrain, ytrain, test_size=0.5, random_state=SEED)def train_base_learners(base_learners, inp, out, verbose =True): if verbose: print(&apos;Fitting models&apos;) for i, (name, m) in enumerate(base_learners.items()): if verbose: print(&quot;%s...&quot; % name, end=&quot; &quot;, flush=False) m.fit(inp, out) if verbose: print(&apos;Done.&apos;)train_base_learners(base_learners, xtrain_base, ytrain_base) (æ³¨æ„æˆ‘ä»¬åªæ˜¯ä½¿ç”¨äº†50%çš„dataåŽ»train, test_size =0.5)1234567891011121314def predict_base_learners(pred_base_learners, inp, verbose=True): &quot;&quot;&quot;Generate a prediction matrix.&quot;&quot;&quot; P = np.zeros((inp.shape[0], len(pred_base_learners))) if verbose: print(&quot;Generating base learner predictions.&quot;) for i, (name, m) in enumerate(pred_base_learners.items()): if verbose: print(&quot;%s...&quot; % name, end=&quot; &quot;, flush=False) p = m.predict_proba(inp) # With two classes, need only predictions for one class P[:, i] = p[:, 1] if verbose: print(&quot;done&quot;) return PP_base = predict_base_learners(base_learners, xpred_base) çŽ°åœ¨æˆ‘ä»¬å¾—åˆ°äº†base_learnersçš„predicationsï¼ŒæŽ¥ä¸‹æ¥æˆ‘ä»¬åº”è¯¥ä½¿ç”¨çš„æ˜¯è¿™ä¸ªæµç¨‹ï¼Œåœ¨base learnersçš„åŸºç¡€ä¸Šï¼ˆç±»ä¼¼ä¸¤å±‚ç»“æž„äº† meta learner å­¦ä¹ çš„å¦‚ä½•æ­é…è¿™äº›base learnerä½¿å¾—æœ€åŽçš„ç»“æžœ predicationsæœ€å¤§ï¼‰è¿›è¡Œè®­ç»ƒï¼Œè€Œä¸æ˜¯è®­ç»ƒåŽŸæ¥çš„æ•°æ®é›†ã€‚1meta_learner.fit(P_base, ypred_base) 1234567#meta_learner.fit(P_base, ypred_base)def ensemble_predict(base_learners, meta_learner, inp, verbose=True): &quot;&quot;&quot;Generate predictions from the ensemble.&quot;&quot;&quot; P_pred = predict_base_learners(base_learners, inp, verbose=verbose) return P_pred, meta_learner.predict_proba(P_pred)[:, 1]P_pred, p = ensemble_predict(base_learners, meta_learner, xtest)print(&quot;\nEnsemble ROC-AUC score: %.3f&quot; % roc_auc_score(ytest, p)) æœ€åŽçš„ç»“æžœæ˜¯0.881 ç›¸æ¯”ä¸Žä¹‹å‰æœ€å¥½çš„0.884ï¼ˆä½¿ç”¨ç›¸åŒçš„æ•°æ®é›†ï¼Œæ²¡æœ‰è¿›è¡Œ meta_learnerçš„æ“ä½œï¼‰è¿™æ˜¯å› ä¸ºæˆ‘ä»¬åœ¨åˆ’åˆ†æ•°æ®é›†çš„ä½¿ç”¨åªæ˜¯ä½¿ç”¨äº†0.5çš„æ•°æ®é›†ï¼Œè€Œå‰è€…çš„æ¨¡åž‹ä½¿ç”¨äº†å…¨éƒ¨çš„train setsã€‚æœ‰äººä¸å…ç–‘é—®ï¼šä¸ºä»€ä¹ˆä¸ä½¿ç”¨å…¨éƒ¨çš„dataï¼Ÿæˆ‘çš„ç†è§£æ˜¯åˆ’åˆ†xtrain_base , xpred_base, ytrain_base, ypred_baseä½¿ç”¨çš„æ˜¯ train_test_split()ï¼Œæ€»æ˜¯éœ€è¦è®¾å®šä¸€ä¸ªæ•°å€¼çš„ï¼Œå³ä½¿train_size =0.01ï¼Œä¹Ÿæ˜¯æ²¡æœ‰ç”¨åˆ°å…¨éƒ¨çš„datas. Training with cross-validationæˆ‘ä»¬ä½¿ç”¨cross-validation æ¥ç¼“è§£ä¸Šé¢é‚£ä¸ªé—®é¢˜ã€‚During cross-validated training of the base learners, a copy of each base learner is fitted on Kâˆ’1 folds, and predict the left-out fold. This process is iterated until every fold has been predicted. The more folds we specify, the less data is being left out in each training pass. This makes cross-validated predictions less noisy and a better reflection of performance during test time. The cost is significantly increased training time. Fitting an ensemble with cross-validation is often referred to as stacking, while the ensemble itself is known as the Super Learner. To understand how cross-validation works, we can think of it as an outer loop over our previous ensemble. The outer loop iterates over K distinct test folds, with the remaining data used for training. The inner loop trains the base learners and generate predictions for the held-out data. Hereâ€™s a simple stacking implementation:12345678910111213141516171819202122232425262728293031323334353637383940414243from sklearn.base import clonedef stacking(base_learners, meta_learner, X, y, generator): &quot;&quot;&quot;Simple training routine for stacking.&quot;&quot;&quot; # Train final base learners for test time print(&quot;Fitting final base learners...&quot;, end=&quot;&quot;) train_base_learners(base_learners, X, y, verbose=False) print(&quot;done&quot;) # Generate predictions for training meta learners # Outer loop: print(&quot;Generating cross-validated predictions...&quot;) cv_preds, cv_y = [], [] for i, (train_idx, test_idx) in enumerate(generator.split(X)): fold_xtrain, fold_ytrain = X[train_idx, :], y[train_idx] fold_xtest, fold_ytest = X[test_idx, :], y[test_idx] # Inner loop: step 4 and 5 fold_base_learners = &#123;name: clone(model) for name, model in base_learners.items()&#125; train_base_learners( fold_base_learners, fold_xtrain, fold_ytrain, verbose=False) fold_P_base = predict_base_learners( fold_base_learners, fold_xtest, verbose=False) cv_preds.append(fold_P_base) cv_y.append(fold_ytest) print(&quot;Fold %i done&quot; % (i + 1)) print(&quot;CV-predictions done&quot;) # Be careful to get rows in the right order cv_preds = np.vstack(cv_preds) cv_y = np.hstack(cv_y) # Train meta learner print(&quot;Fitting meta learner...&quot;, end=&quot;&quot;) meta_learner.fit(cv_preds, cv_y) print(&quot;done&quot;) return base_learners, meta_learner å°¤å…¶åœ¨cv_predså’Œcv_yçš„ç»´åº¦é—®é¢˜ä¸Šï¼Œæ³¨æ„å°å¿ƒã€‚The basic difference between blending and stacking is therefore that stacking allows both base learners and the meta learner to train on the full data set. Using 2-fold cross-validation, we can measure the difference this makes in our case:1234567from sklearn.model_selection import KFold# Train with stackingcv_base_learners, cv_meta_learner = stacking( get_models(), clone(meta_learner), xtrain.values, ytrain.values, KFold(2))P_pred, p = ensemble_predict(cv_base_learners, cv_meta_learner, xtest, verbose=False)print(&quot;\nEnsemble ROC-AUC score: %.3f&quot; % roc_auc_score(ytest, p)) Ensemble ROC-AUC score: 0.889è¿™æ˜¯ç›®å‰ä¸ºæ­¢æœ€å¥½çš„ç»“æžœäº†ã€‚Stacking yields a sizeable increase in performance: in fact, it gives us our best score so far. This outcome is typical for small and medium-sized data sets, where the effect of blending can be severe. As the data set size increases, blending and stacking performs similarly. Use packageså¿«è¦æŽ¥è¿‘å°¾å£°äº†ï¼Œåœ¨æ–‡ç« çš„å¼€å§‹ï¼Œæˆ‘ä»¬æåŠè¦å®‰åˆ©ä¸€ä¸ªresembleå¥½ç”¨çš„package. So, itâ€™s now.Here, we use Ensemble and build our previous generalized ensemble, but now using 10-fold cross-validation:123456789101112131415161718192021from mlens.ensemble import SuperLearner# Instantiate the ensemble with 10 foldssl = SuperLearner( folds=10, random_state=SEED, verbose=2, backend=&quot;multiprocessing&quot;)# Add the base learners and the meta learnersl.add(list(base_learners.values()), proba=True) sl.add_meta(meta_learner, proba=True)# Train the ensemblesl.fit(xtrain, ytrain)# Predict the test setp_sl = sl.predict_proba(xtest)print(&quot;\nSuper Learner ROC-AUC score: %.3f&quot; % roc_auc_score(ytest, p_sl[:, 1])) So simple!1plot_roc_curve(ytest, p.reshape(-1, 1), P.mean(axis=1), [&quot;Simple average&quot;], &quot;Super Learner&quot;) å‘çŽ°super learner(meta learner)å’Œ basic learnerçš„meançš„ç»“æžœå·²ç»ä¸ç›¸ä¸Šä¸‹äº†ã€‚super learnerå¾—åˆ°äº†å¾ˆå¥½çš„è®­ç»ƒã€‚ è¡¥å……æ¦‚å¿µ ROCæ›²çº¿å’ŒAUCå€¼ROCå…¨ç§°æ˜¯â€œå—è¯•è€…å·¥ä½œç‰¹å¾â€ï¼ˆReceiver Operating Characteristicï¼‰ã€‚ROCæ›²çº¿çš„é¢ç§¯å°±æ˜¯AUCï¼ˆArea Under the Curveï¼‰ã€‚AUCç”¨äºŽè¡¡é‡â€œäºŒåˆ†ç±»é—®é¢˜â€æœºå™¨å­¦ä¹ ç®—æ³•æ€§èƒ½ï¼ˆæ³›åŒ–èƒ½åŠ›ï¼‰ã€‚è¯´åˆ°è¿™é‡Œä¸å¾—ä¸æåŠå°±æ˜¯ç»å¸¸ä½¿ç”¨çš„ç¬¦å·ï¼ŒTP(True Positive), FP(False, Positive), TN(True Negative),FN(False Negative)ã€‚ä»–ä»¬æ˜¯æ ¹æ®åŽŸæ¥çœŸå®žæ•°æ®å’Œé¢„æµ‹ç±»åˆ«è¿›è¡Œçš„æŽ’åˆ—ç»„åˆï¼ˆå½“ç„¶è¿™æ˜¯é’ˆå¯¹äºŒåˆ†é—®é¢˜ï¼‰ã€‚ ROC æ›²çº¿ROC æ›²çº¿ï¼ˆæŽ¥æ”¶è€…æ“ä½œç‰¹å¾æ›²çº¿ï¼‰æ˜¯ä¸€ç§æ˜¾ç¤ºåˆ†ç±»æ¨¡åž‹åœ¨æ‰€æœ‰åˆ†ç±»é˜ˆå€¼ä¸‹çš„æ•ˆæžœçš„å›¾è¡¨ã€‚è¯¥æ›²çº¿ç»˜åˆ¶äº†ä»¥ä¸‹ä¸¤ä¸ªå‚æ•°ï¼š çœŸæ­£ä¾‹çŽ‡ å‡æ­£ä¾‹çŽ‡çœŸæ­£ä¾‹çŽ‡ (TPR) æ˜¯å¬å›žçŽ‡çš„åŒä¹‰è¯ï¼Œå› æ­¤å®šä¹‰å¦‚ä¸‹ï¼š$$T P R = \frac { T P } { T P + F N }$$å‡æ­£ä¾‹çŽ‡ (FPR) çš„å®šä¹‰å¦‚ä¸‹ï¼š$$F P R = \frac { F P } { F P + T N }$$ ROC ä¸­ TPR =(True positive / ( True positive +False negative)), é‚£ä¸ªfalse negative ä¹Ÿæ˜¯çœŸå®žçš„ç±»åˆ«ï¼Œåªä¸è¿‡æ˜¯é”™è¯¯çš„å½“åšäº† negativeï¼ˆfalse negativeï¼‰ã€‚ï¼ˆå¯ä»¥å‚è§ä¸‹å›¾ï¼‰ ROC æ›²çº¿ç”¨äºŽç»˜åˆ¶é‡‡ç”¨ä¸åŒåˆ†ç±»é˜ˆå€¼æ—¶çš„ TPR ä¸Ž FPRã€‚é™ä½Žåˆ†ç±»é˜ˆå€¼ä¼šå¯¼è‡´å°†æ›´å¤šæ ·æœ¬å½’ä¸ºæ­£ç±»åˆ«ï¼Œä»Žè€Œå¢žåŠ å‡æ­£ä¾‹å’ŒçœŸæ­£ä¾‹çš„ä¸ªæ•°ã€‚ä¸‹å›¾æ˜¾ç¤ºäº†ä¸€ä¸ªå…¸åž‹çš„ ROC æ›²çº¿ã€‚ä¸ºäº†è®¡ç®— ROC æ›²çº¿ä¸Šçš„ç‚¹ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¸åŒçš„åˆ†ç±»é˜ˆå€¼å¤šæ¬¡è¯„ä¼°é€»è¾‘å›žå½’æ¨¡åž‹ï¼Œä½†è¿™æ ·åšæ•ˆçŽ‡éžå¸¸ä½Žã€‚å¹¸è¿çš„æ˜¯ï¼Œæœ‰ä¸€ç§åŸºäºŽæŽ’åºçš„é«˜æ•ˆç®—æ³•å¯ä»¥ä¸ºæˆ‘ä»¬æä¾›æ­¤ç±»ä¿¡æ¯ï¼Œè¿™ç§ç®—æ³•ç§°ä¸ºæ›²çº¿ä¸‹é¢ç§¯ã€‚ ROCæ›²çº¿ï¼Œä¸€èˆ¬é€‚ç”¨äºŽä½ çš„åˆ†ç±»å™¨è¾“å‡ºä¸€ä¸ªâ€œæ¦‚çŽ‡å€¼â€ï¼Œå³è¿™ä¸ªæ ·æœ¬å±žäºŽæŸä¸ªç±»çš„æ¦‚çŽ‡æ˜¯å¤šå°‘ã€‚ å¦‚æ­¤çš„è¯ï¼Œä½ å°±éœ€è¦è®¾å®šä¸€ä¸ªé˜ˆå€¼ï¼Œ å¤§äºŽè¿™ä¸ªé˜ˆå€¼å±žäºŽæ­£ç±»ï¼Œå°äºŽè¿™ä¸ªé˜ˆå€¼å±žäºŽè´Ÿç±»ã€‚ ä»Žè€Œï¼Œå¯¹äºŽè¿™ä¸ªé˜ˆå€¼P0ï¼Œ å°±ä¼šå¾—åˆ°å¯¹åº”çš„TPR, FPR, ä¹Ÿå°±æ˜¯ROCæ›²çº¿ä¸Šçš„ä¸€ä¸ªç‚¹ï¼Œä½ è®¾ç½®ä¸åŒçš„é˜ˆå€¼ï¼Œå°±ä¼šå¾—åˆ°ä¸åŒçš„TPR, FPRï¼Œ ä»Žè€Œæž„æˆROCæ›²çº¿ã€‚ é€šå¸¸æ¥è¯´ é˜ˆå€¼é™ä½Žï¼Œå³è¿›å…¥æ­£ç±»çš„é—¨æ§›å˜ä½Žï¼Œ TPRä¼šå˜å¤§ï¼Œä½†æ˜¯FPRä¹Ÿä¼šå˜å¤§ï¼Œ çœ‹ä»–ä»¬è°å˜çš„å¿«ã€‚ 1234567891011121314gbc = GradientBoostingClassifier()gbc.fit(x_train, y_train)resu = gbc.predict(x_test) #è¿›è¡Œé¢„æµ‹y_pred_gbc = gbc.predict_proba(x_test)[:,1] ###è¿™çŽ©æ„å°±æ˜¯é¢„æµ‹æ¦‚çŽ‡çš„fpr, tpr, threshold = roc_curve(y_test, y_pred_gbc) ###ç”»å›¾çš„æ—¶å€™è¦ç”¨é¢„æµ‹çš„æ¦‚çŽ‡ï¼Œè€Œä¸æ˜¯ä½ çš„é¢„æµ‹çš„å€¼plt.plot(fpr, tpr, 'b', label='AUC = %0.2f' % rocauc)#ç”ŸæˆROCæ›²çº¿plt.legend(loc='lower right')plt.plot([0, 1], [0, 1], 'r--')plt.xlim([0, 1])plt.ylim([0, 1])plt.ylabel('çœŸæ­£çŽ‡')plt.xlabel('å‡æ­£çŽ‡')plt.show() æŽ¥ç€æˆ‘ä»¬è®¡ç®—TRP(True Positive Radio)ï¼ŒFRP(False Positive Ratio)ç”¨äºŽæè¿°ROCæ›²çº¿ï¼Œåˆ†åˆ«è¡¨ç¤ºè¯¥æ›²çº¿çš„Yè½´ï¼ŒXè½´ã€‚TPR=TP/(TP+FN)FPR=FP/(FP+TN)æœ€åŽå°±å½¢æˆäº†ç±»ä¼¼è¿™æ ·çš„å›¾åƒ(æ¥æºäºŽä¸Šè¿°çš„è®­ç»ƒæ¨¡åž‹) æˆ‘ä»¬å¸Œæœ›çš„ç»“æžœæ˜¯TRUè¶Šå¤§ï¼ˆæŽ¥è¿‘1ï¼‰ï¼ŒFRUè¶Šå°ï¼ˆæŽ¥è¿‘0ï¼‰ã€‚AUCçš„å€¼æ˜¯ROCæ‰€è¦†ç›–çš„é¢ç§¯ï¼Œå½“AUCè¶Šå¤§æ—¶å€™ï¼Œåˆ†ç±»å™¨çš„æ•ˆæžœè¶Šå¥½ã€‚ä»Žå›¾ä¸­å¯ä»¥çœ‹å‡ºæ¨¡åž‹(ensemble)çš„é¢ç§¯æ˜¯æœ€å¤§çš„ï¼Œåˆ†ç±»æ•ˆæžœä¹Ÿæ˜¯æœ€å¥½çš„ã€‚å…³äºŽè¯¥å›¾åƒè¿˜æœ‰ä¸€ç‚¹ï¼Œå¦‚æžœä½ çš„æ›²çº¿æ‹Ÿåˆå¯¹è§’çº¿ï¼ˆå›¾ä¸­è™šçº¿ï¼‰ï¼Œé‚£ä¹ˆç›¸å½“äºŽéšæœºçŒœæµ‹ã€‚åœ¨æœºå™¨å­¦ä¹ ä¸­å…³äºŽè¿™æ–¹é¢ç»å¸¸æ¶‰åŠåˆ°çš„è¿˜æœ‰precision(æŸ¥å‡†çŽ‡), recall(æŸ¥å…¨çŽ‡)ä¸¤ä¸ªæ¦‚å¿µï¼Œä¸‹é¢æ˜¯è®¡ç®—å…¬å¼ã€‚å…¶ä¸­n= TP+FPæ€»ç»“ï¼šå›¾ä¸­çš„xï¼Œyè½´çš„è®¡ç®—å’Œ precision, recallä¸æ˜¯ä¸€ä¸ªæ¦‚å¿µï¼Œè™½ç„¶ recall å’Œy è½´åœ¨è®¡ç®—ä¸Šæ˜¯ç›¸åŒçš„ã€‚precision æ˜¯é’ˆå¯¹äºŽé¢„æµ‹æ•°æ®(predicationç»“æžœ)æ¥è¯´çš„ï¼Œè€Œxè½´ï¼Œyè½´(recall)çš„è®¡ç®—æŸç§æ„ä¹‰ä¸Šæ˜¯é’ˆå¯¹åŽŸæ¥çœŸå®žæ•°æ®è€Œè¨€çš„ã€‚æ‰€ä»¥æˆ‘ä»¬åœ¨è®­ç»ƒæ¨¡åž‹è¿‡ç¨‹å¯ä»¥è¿½æ±‚ precision å’Œ recallçš„åŒé«˜(å³å›¾åƒçš„å·¦ä¸Šè§’)ã€‚è¿™æ—¶å€™å¼•å…¥äº†F1-measure(F1 =(2PR)/(R+P)).(P: precision, R: recall) majority/ soft/ hard votingan ensemble that averages classifier predictions is known as a majority voting classifier. When an ensembles averages based on probabilities (as above), we refer to it as soft voting, averaging final class label predictions is known as hard voting. Pearsonç›¸å…³æ€§åæ–¹å·®é™¤ä»¥å„è‡ªçš„æ ‡å‡†å·® GBCå‚æ•°è¿™äº›å‚æ•°ä¸­ï¼Œç±»ä¼¼äºŽAdaboostï¼Œæˆ‘ä»¬æŠŠé‡è¦å‚æ•°åˆ†ä¸ºä¸¤ç±»ï¼Œç¬¬ä¸€ç±»æ˜¯Boostingæ¡†æž¶çš„é‡è¦å‚æ•°ï¼Œç¬¬äºŒç±»æ˜¯å¼±å­¦ä¹ å™¨å³CARTå›žå½’æ ‘çš„é‡è¦å‚æ•°ã€‚n_estimators: ä¹Ÿå°±æ˜¯å¼±å­¦ä¹ å™¨çš„æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼Œæˆ–è€…è¯´æœ€å¤§çš„å¼±å­¦ä¹ å™¨çš„ä¸ªæ•°ã€‚learning_rate: å³æ¯ä¸ªå¼±å­¦ä¹ å™¨çš„æƒé‡ç¼©å‡ç³»æ•°Î½ï¼Œä¹Ÿç§°ä½œæ­¥é•¿å¯¹äºŽåˆ†ç±»æ¨¡åž‹ï¼Œæœ‰å¯¹æ•°ä¼¼ç„¶æŸå¤±å‡½æ•°â€devianceâ€å’ŒæŒ‡æ•°æŸå¤±å‡½æ•°â€exponentialâ€ä¸¤è€…è¾“å…¥é€‰æ‹©ã€‚é»˜è®¤æ˜¯å¯¹æ•°ä¼¼ç„¶æŸå¤±å‡½æ•°â€devianceâ€ã€‚ä¸€èˆ¬æ¥è¯´ï¼ŒæŽ¨èä½¿ç”¨é»˜è®¤çš„â€devianceâ€ã€‚å®ƒå¯¹äºŒå…ƒåˆ†ç¦»å’Œå¤šå…ƒåˆ†ç±»å„è‡ªéƒ½æœ‰æ¯”è¾ƒå¥½çš„ä¼˜åŒ–ã€‚è€ŒæŒ‡æ•°æŸå¤±å‡½æ•°ç­‰äºŽæŠŠæˆ‘ä»¬å¸¦åˆ°äº†Adaboostç®—æ³•ã€‚å¯¹äºŽå›žå½’æ¨¡åž‹ï¼Œæœ‰å‡æ–¹å·®â€lsâ€, ç»å¯¹æŸå¤±â€ladâ€, HuberæŸå¤±â€huberâ€å’Œåˆ†ä½æ•°æŸå¤±â€œquantileâ€ã€‚é»˜è®¤æ˜¯å‡æ–¹å·®â€lsâ€ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œå¦‚æžœæ•°æ®çš„å™ªéŸ³ç‚¹ä¸å¤šï¼Œç”¨é»˜è®¤çš„å‡æ–¹å·®â€lsâ€æ¯”è¾ƒå¥½ã€‚å¦‚æžœæ˜¯å™ªéŸ³ç‚¹è¾ƒå¤šï¼Œåˆ™æŽ¨èç”¨æŠ—å™ªéŸ³çš„æŸå¤±å‡½æ•°â€huberâ€ã€‚è€Œå¦‚æžœæˆ‘ä»¬éœ€è¦å¯¹è®­ç»ƒé›†è¿›è¡Œåˆ†æ®µé¢„æµ‹çš„æ—¶å€™ï¼Œåˆ™é‡‡ç”¨â€œquantileâ€ã€‚max_features:å¯ä»¥ä½¿ç”¨å¾ˆå¤šç§ç±»åž‹çš„å€¼ï¼Œé»˜è®¤æ˜¯â€Noneâ€,æ„å‘³ç€åˆ’åˆ†æ—¶è€ƒè™‘æ‰€æœ‰çš„ç‰¹å¾æ•°.subsample: é€‰æ‹©å°äºŽ1çš„æ¯”ä¾‹å¯ä»¥å‡å°‘æ–¹å·®ï¼Œå³é˜²æ­¢è¿‡æ‹Ÿåˆï¼Œä½†æ˜¯ä¼šå¢žåŠ æ ·æœ¬æ‹Ÿåˆçš„åå·®ï¼Œå› æ­¤å–å€¼ä¸èƒ½å¤ªä½Žã€‚æŽ¨èåœ¨[0.5, 0.8]ä¹‹é—´ï¼Œé»˜è®¤æ˜¯1.0ï¼Œå³ä¸ä½¿ç”¨å­é‡‡æ ·ã€‚ å‚è€ƒæ–‡çŒ®GBCå‚æ•°è®¾ç½®ROCæ›²çº¿å’ŒAUCå€¼Introduction to Python Ensembles]]></content>
      <categories>
        <category>æœºå™¨å­¦ä¹ </category>
      </categories>
      <tags>
        <tag>æ¨¡åž‹èžåˆ(Ensemble)</tag>
        <tag>ROC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[å¯¹æŠ—ç”Ÿæˆç½‘ç»œå®žéªŒå¯¹æ¯”]]></title>
    <url>%2F2018%2F06%2F05%2F%E5%AF%B9%E6%8A%97%E6%80%A7%E7%94%9F%E6%88%90%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[GANæ¨¡åž‹æ˜¯å…¸åž‹çš„éšå¼æ— ç›‘ç£ç”Ÿæˆæ¨¡åž‹ï¼Œå»ºæ¨¡è¿‡ç¨‹ä¸­æ²¡æœ‰åˆ©ç”¨åˆ°æ•°æ®çš„è¯­ä¹‰æ ‡ç­¾ã€‚ä½†åœ¨å®žé™…åº”ç”¨ä¸­ç”Ÿæˆæ¨¡åž‹çš„å¯æŽ§æ€§è‡³å…³é‡è¦ï¼Œæ ¹æ®æ ‡ç­¾ç”Ÿæˆå¯¹å¯æŽ§æœ‰æ ·æœ¬æ›´å…·å®žé™…åº”ç”¨ä»·å€¼ã€‚å›¾åƒç”Ÿæˆæ¨¡åž‹è¢«å¹¿æ³›åº”ç”¨äºŽæ•°æ®å¢žå¼ºã€é£Žæ ¼è½¬æ¢å’Œæ•°æ®è¡¥å…¨é¢†åŸŸï¼Œéœ€è¦å¯æŽ§ä¸”è¯­ä¹‰å®Œå¤‡çš„ç”Ÿæˆæ¨¡åž‹ã€‚æ¡ä»¶GANæ¨¡åž‹åœ¨GANæ¨¡åž‹å»ºæ¨¡æ€è·¯çš„åŸºç¡€ä¸Šï¼Œå°†è¯­ä¹‰æ ‡ç­¾åŠ å…¥äº†å»ºæ¨¡è¿‡ç¨‹ï¼Œå°†æ— ç›‘ç£ç”Ÿæˆæ¨¡åž‹è½¬å˜ä¸ºæœ‰ç›‘ç£çš„æ¡ä»¶ç”Ÿæˆæ¨¡åž‹ã€‚æ¡ä»¶GANæ¨¡åž‹å…·ä½“åŒ…æ‹¬Conditional GANæ¨¡åž‹ã€Semi-GANæ¨¡åž‹å’ŒAC-GANæ¨¡åž‹ã€‚ å®žéªŒæ¨¡åž‹ä»‹ç»ï¼ˆ1ï¼‰Conditional GANæ¨¡åž‹GANæ¨¡åž‹ä¸­åˆ¤åˆ«å™¨Då¯¹è¾“å…¥çš„æ•°æ®æ ·æœ¬çš„æ¥æºè¿›è¡Œåˆ¤åˆ«ï¼Œæ˜¯å…¸åž‹çš„åˆ¤åˆ«æ¨¡åž‹æµç¨‹ã€‚å¦‚æžœåœ¨GANæ¡†æž¶ä¸­åŠ å…¥æœ‰ç›‘ç£ä¿¡æ¯æ¥è¾…åŠ©è®­ç»ƒï¼Œå¦‚å›¾åƒçš„ç±»åˆ«ä¿¡æ¯æ¥è¾…åŠ©åˆ¤åˆ«å™¨Dè¿›è¡Œåˆ¤åˆ«ï¼Œåˆ™ä¼šå¸®åŠ©ç”Ÿæˆæ›´åŠ çœŸå®žçš„æ ·æœ¬ã€‚å…¶ä¸­æœ€åˆçš„å°è¯•æ–¹å¼æ˜¯Conditional GANæ¨¡åž‹ï¼Œç»“æž„å¦‚å›¾æ‰€ç¤ºã€‚Conditional GANæ¨¡åž‹åœ¨ç”Ÿæˆå™¨Gå’Œåˆ¤åˆ«å™¨Dè¾“å…¥ä¸­éƒ½åŠ å…¥äº†æ ‡ç­¾ä¿¡æ¯ï¼Œè¯•å›¾è®©ç”Ÿæˆå™¨Gå­¦ä¹ åˆ°ä»Žæ•°æ®æ ‡ç­¾yåˆ°æ ·æœ¬xçš„æ˜ å°„ï¼Œè®©åˆ¤åˆ«å™¨Då­¦ä¹ å¯¹æ ·æœ¬xå’Œæ ‡ç­¾yçš„ç»„åˆè¿›è¡Œåˆ¤åˆ«ã€‚ä¸ŽGANæ¨¡åž‹ç›¸æ¯”ï¼ŒConditional GANæ¨¡åž‹å¢žåŠ äº†æ ‡ç­¾ä¿¡æ¯çš„è¾“å…¥å°†æ¨¡åž‹è½¬å˜ä¸ºæ¡ä»¶ç”Ÿæˆæ¨¡åž‹ï¼Œåœ¨ä¸€å®šç¨‹åº¦ä¸Šæé«˜äº†æ¨¡åž‹çš„ç¨³å®šæ€§ã€‚Conditional GANæ¨¡åž‹è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œåˆ¤åˆ«å™¨Då¯¹æ ·æœ¬xå’Œæ ‡ç­¾yçš„ç±»åˆ«ç»„åˆè¿›è¡Œè®­ç»ƒï¼Œå¹¶æ²¡æœ‰è¾“å…¥æ ·æœ¬xå’Œæ ‡ç­¾yçš„ç±»åˆ«é”™è¯¯ç»„åˆè¿›è¡Œè®­ç»ƒï¼Œå› æ­¤æ¨¡åž‹å¹¶æ²¡æœ‰å­¦ä¹ æ ·æœ¬xå’Œæ ‡ç­¾yçš„è”åˆåˆ†å¸ƒã€‚ï¼ˆ2ï¼‰Semi-GANæ¨¡åž‹Conditional GANæ¨¡åž‹åˆ©ç”¨äº†æ ‡ç­¾ä¿¡æ¯è¿›è¡Œå»ºæ¨¡ï¼Œä½†æ²¡æœ‰å¯¹æ ‡ç­¾è¯­ä¹‰çš„ä¿¡æ¯è¿›è¡Œè¡¨å¾ï¼Œå¯¼è‡´æ¨¡åž‹èƒ½å¤Ÿå­¦åˆ°çš„ä¿¡æ¯æœ‰é™ã€‚åœ¨ç”Ÿæˆæ¨¡åž‹è¿‡ç¨‹ä¸­ï¼Œå¦‚æžœåˆ¤åˆ«å™¨Dèƒ½å¤Ÿæ˜Žç¡®æŒ‡å‡ºç”Ÿæˆæ ·æœ¬çš„ç±»åˆ«é”™è¯¯ï¼Œåˆ™å¯ä¸ºç”Ÿæˆå™¨Gæä¾›æ›´åŠ ç²¾ç¡®çš„æ¢¯åº¦ä¿¡æ¯ï¼Œæœ€ç»ˆèƒ½ç”Ÿæˆæ›´åŠ çœŸå®žçš„æ ·æœ¬ã€‚Semi-GANåŸºäºŽæ­¤æ€è·¯è¿›è¡Œæ”¹è¿›ï¼Œå…·ä½“ç»“æž„å¦‚å›¾9æ‰€ç¤ºã€‚Semi-GANæ¨¡åž‹åœ¨Conditional GANæ¨¡åž‹çš„åŸºç¡€ä¸Šï¼Œå¯¹åˆ¤åˆ«å™¨Dçš„åˆ†ç±»è¾“å‡ºè¿›è¡Œç»†åŒ–åŠ å…¥äº†åŠç›‘ç£å­¦ä¹ è¿‡ç¨‹ã€‚ï¼ˆ3ï¼‰AC-GANæ¨¡åž‹ä¸ŽConditional GANæ¨¡åž‹ç›¸æ¯”ï¼ŒSemi-GANæ¨¡åž‹ä¸­åˆ¤åˆ«å™¨Dèƒ½å¤Ÿåˆ¤åˆ«çœŸå®žæ ·æœ¬çš„æ¥æºï¼Œå¢žå¼ºäº†åˆ¤åˆ«å™¨Dçš„åˆ¤åˆ«èƒ½åŠ›ã€‚ä½†ç ”ç©¶è¡¨æ˜Žè¿‡å¼ºçš„åˆ¤åˆ«ä¿¡æ¯ä¼šå½±å“ç”Ÿäº§æ ·æœ¬çš„è´¨é‡ï¼Œå…·ä½“åŽŸå› ä¸ºSemi-GANæ¨¡åž‹çš„å»ºæ¨¡è¿‡ç¨‹ä¸ºåŠç›‘ç£åˆ†ç±»è¿‡ç¨‹ï¼Œç›®æ ‡ä¼˜åŒ–å‡½æ•°ä¸ºæ— ç›‘ç£åˆ†ç±»å’Œæœ‰ç›‘ç£åˆ†ç±»ç›®æ ‡å‡½æ•°ä¹‹å’Œã€‚è‹¥åˆ¤åˆ«å™¨Dçš„ç›‘ç£åˆ†ç±»ä¿¡æ¯è¿‡å¼ºï¼Œåˆ™ä¼šå‰Šå¼±åˆ¤åˆ«å™¨Då¯¹æ ·æœ¬æ¥æºçš„åˆ¤åˆ«èƒ½åŠ›ã€‚Conditional GANæ¨¡åž‹èƒ½å¤Ÿç”ŸæˆæŒ‡å®šç±»åˆ«çš„æ ·æœ¬ï¼ŒSemi-GANæ¨¡åž‹èƒ½å¤Ÿåˆ¤åˆ«æ ·æœ¬çš„ç±»åˆ«ä¿¡æ¯ã€‚AC-GANæ¨¡åž‹å°†ä»¥ä¸Šä¸¤ä¸ªæ¨¡åž‹æ€è·¯è¿›è¡Œæ•´åˆï¼Œå¾—åˆ°å¤Ÿè¿›è¡Œæ¡ä»¶ç”Ÿæˆçš„ç”Ÿæˆå™¨Gï¼Œå’Œèƒ½å¤Ÿåˆ¤åˆ«æ ·æœ¬ç±»åˆ«å’Œæ¥æºä¿¡æ¯çš„åˆ¤åˆ«å™¨Dã€‚AC-GANæ¨¡åž‹çš„ç»“æž„å¦‚å›¾10æ‰€ç¤ºã€‚AC-GANæ¨¡åž‹åœ¨Conditional GANçš„åŸºç¡€ä¸Šï¼Œè®©åˆ¤åˆ«å™¨Dåœ¨åˆ¤åˆ«æ ·æœ¬æ¥æºçš„åŒæ—¶ï¼Œè®©æ ·æœ¬è¿›è¡Œåˆ†ç±»ã€‚æ­¤æ—¶çš„åˆ¤åˆ«å™¨Dçš„è¾“å‡ºåˆ†ä¸ºæ ·æœ¬æ¥æºä¿¡æ¯LSå’Œæ ·æœ¬åˆ†ç±»LCä¿¡æ¯ã€‚ ä¸åŒæ¨¡åž‹æ¯”è¾ƒä¸‹é¢ç”¨è¡¨æ ¼çš„æ–¹å¼å¯¹æ¯”åœ¨å®žéªŒä¸­ä½¿ç”¨çš„æ¨¡åž‹çš„ç›®æ ‡å‡½æ•°(ps,å›¾ç”»æ¯”è¾ƒä¸‘ï¼Œä¹‹åŽå†ä¿®æ”¹) Name Paper Link Value Function GAN Arxiv DCGAN Arxiv Semi-GAN Arxiv å’ŒGAN æ¨¡åž‹ç›¸åŒ CGAN Arxiv ACGAN Arxiv our model Arxiv æ•°æ®é›†ä»‹ç»åœ¨å¸¸ç”¨äºŽå›¾åƒç”Ÿæˆçš„å›¾åƒæ•°æ®é›†ä¸­ï¼Œå¤§éƒ¨åˆ†æ•°æ®çš„æ ‡ç­¾ç±»åž‹ä¸ºç¦»æ•£ç±»åž‹ã€‚å…¶ä¸­MNISTå’ŒFashion-MNISTä¸ºå¸¸ç”¨çš„ç°åº¦å›¾åƒæ•°æ®ï¼Œæ¯ç±»æ ·æœ¬åˆ†å¸ƒè¾ƒä¸ºç‹¬ç«‹ï¼Œå¸¸ç”¨äºŽè¿›è¡Œå›¾åƒåˆ†ç±»å’Œæ ·æœ¬ç”Ÿæˆçš„å®žéªŒï¼›SVHNå’ŒCIFAR10ä¸ºå½©è‰²æ•°æ®é›†å›¾åƒåƒç´ åˆ†å¸ƒè¾ƒä¸ºå¤æ‚ï¼Œå…¶ä¸­CIFAR10å¸¸ç”¨æ¥æ£€éªŒåˆ†ç±»ç½‘ç»œæ€§èƒ½çš„è¯„ä»·æ•°æ®é›†ï¼›CelebAä¸ºå¤§è§„æ¨¡çš„äººè„¸è¯†åˆ«å’Œå±žæ€§åˆ†ç±»æ•°æ®é›†ï¼Œæ¯å¹…äººè„¸å›¾åƒåŒ…æ‹¬40ä¸ªå±žæ€§æ ‡ç­¾ï¼›ImageNetä¸ºå›¾åƒåˆ†ç±»å’Œè¯†åˆ«æ•°æ®é›†ï¼Œæ•°æ®é›†ç±»åˆ«åˆ†å¸ƒæ¯”è¾ƒå¤æ‚å…·ä½“åŒ…æ‹¬è‡ªç„¶å›¾åƒå’Œäººä¸ºå›¾åƒã€‚UnityEyesä¸ºäººçœ¼è§†è§‰åˆæˆæ•°æ®é›†ï¼Œæ•°æ®é›†æ ‡ç­¾åŒ…æ‹¬çž³å­”æ ‡ç­¾å’Œè§†è§‰æ–¹å‘æ ‡ç­¾ï¼Œå…¶ä¸­è§†è§‰æ–¹å‘æ ‡ç­¾ä¸ºè¿žç»­çš„è¯­è¨€æ ‡ç­¾ã€‚å¸¸è§çš„ç¦»æ•£æ ‡ç­¾å›¾åƒæ•°æ®é›†çš„æ ·ä¾‹: æˆ‘ä»¬çš„æ¨¡åž‹åœ¨åŽŸå§‹GANæ¨¡åž‹ä¸­ï¼Œç›®æ ‡å‡½æ•°å®šä¹‰ä¸ºç”Ÿæˆå™¨Gå’Œåˆ¤åˆ«å™¨Dçš„åšå¼ˆè¿‡ç¨‹ï¼Œå®šä¹‰V(G;D)ä¸ºæ¨¡åž‹çš„ç›®æ ‡å‡½æ•°ï¼Œç”±ç”Ÿæˆå™¨Gå’Œåˆ¤åˆ«å™¨Dç»„æˆã€‚è¯­ä¹‰åŒ¹é…ç›®æ ‡å‡½æ•°FMlossã€‚åŸºäºŽè¯­ä¹‰åŒ¹é…çš„æ¡ä»¶ç”Ÿæˆç½‘ç»œæ¨¡åž‹çš„ç”Ÿæˆå™¨Gå’Œåˆ¤åˆ«å™¨Dçš„ç›®æ ‡å‡½æ•°åˆ†åˆ«ä¸ºï¼šå¦‚ä¸Šå¼ï¼ŒLSä¸ºæ ·æœ¬æ¥æºï¼ŒLCä¸ºåˆ†ç±»ç»“æžœã€‚åˆ¤åˆ«å™¨Dç›®æ ‡ä¸ºæœ€å¤§åŒ–LC+ LS + FMlossï¼Œå…¶è¯•å›¾å¯¹è¾“å…¥æ ·æœ¬è¿›è¡Œåˆ†ç±»ï¼Œå¹¶é€šè¿‡æ¥æºå’Œè¯­ä¹‰åŒ¹é…åŒºåˆ†ç”Ÿæˆæ ·æœ¬å’ŒåŽŸå§‹æ ·æœ¬ã€‚ç”Ÿæˆå™¨Gçš„ç›®çš„æ˜¯æœ€å¤§åŒ–LC âˆ’ LS âˆ’FMlossï¼Œå…¶è¯•å›¾é€šè¿‡æ ·æœ¬åˆ†ç±»ç»“æžœã€æ ·æœ¬æ¥æºå’Œè¯­ä¹‰åŒ¹é…ç»“æžœæ¥æ¬ºéª—åˆ¤åˆ«å™¨Dã€‚LSæ¥æºæŸå¤±ä¸ŽåŽŸå§‹GANæ¨¡åž‹ç›¸åŒï¼ŒLCä¸ºè¯­ä¹‰æ ‡ç­¾åˆ†ç±»æŸå¤±ï¼Œåœ¨ç±»åˆ«åˆ†ç±»ä¸­ä½¿ç”¨äº¤å‰ä¿¡æ¯ç†µï¼Œåœ¨æ•°å€¼å›žå½’ä¸­åˆ™ä½¿ç”¨å‡æ–¹å·®å›žå½’ã€‚ ç”Ÿæˆç»“æžœå¯¹æ¯”MNISTæ•°æ®é›†ç”Ÿæˆç»“æžœFashion-MNISTæ•°æ®é›†ç”Ÿæˆç»“æžœSVHNæ•°æ®é›†ç”Ÿæˆç»“æžœCIFAR10æ•°æ®é›†ç”Ÿæˆç»“æžœCelebAæ•°æ®é›†ç”Ÿæˆç»“æžœUnityEyesæ•°æ®é›†ç”Ÿæˆç»“æžœ]]></content>
      <categories>
        <category>Deep learning</category>
      </categories>
      <tags>
        <tag>GANs</tag>
        <tag>æ·±åº¦å­¦ä¹ </tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Titanic Challenge]]></title>
    <url>%2F2018%2F06%2F05%2FTitanic-Challenge%2F</url>
    <content type="text"><![CDATA[ç”¨æ­¤åšå®¢è®°å½•è‡ªå·±è§£å†³ Kaggle Titanic challengeè¿‡ç¨‹ä¸­çš„ä¸ªäººæ€»ç»“ã€‚ é—®é¢˜æè¿°è¯´é“Titanic(æ³°å¦å°¼å…‹å·)ï¼Œæœ€ç†Ÿæ‚‰èŽ«è¿‡äºŽTitanic(1997 film),è¿™éƒ¨ç”±è‘—åå¯¼æ¼”è©¹å§†æ–¯Â·å¡æ¢…éš†æ‰§å¯¼ï¼ŒèŽ±æ˜‚çº³å¤šÂ·è¿ªå¡æ™®é‡Œå¥¥ã€å‡¯ç‰¹Â·æ¸©æ–¯èŽ±ç‰¹é¢†è¡”ä¸»æ¼”çš„ç”µå½±ï¼Œç»ä¹…ä¸è¡°â€¦ä½†æ˜¯æˆ‘ä»¬ä»Šå¤©çš„ç”»é£Žä¸æ˜¯è¿™æ ·çš„â€¦ æˆ‘ä»¬ä»Šå¤©è§£å†³çš„é—®é¢˜æ˜¯ä»¥è¯¥äº‹ä»¶é—®èƒŒæ™¯ï¼Œä½†æ˜¯æ²¡æœ‰é‚£ä¹ˆæµªæ¼«ã€‚å…³äºŽè¿™ä¸ªæ¡ˆä¾‹çš„ä»‹ç»ç½‘ä¸Šæœ‰å¾ˆå¤šå†…å®¹ï¼Œä¸ºäº†é¿å…ç´¯èµ˜ï¼Œåœ¨è¿™é‡Œå°±ä¸è¿›è¡Œè¯¦è¿°ã€‚æ€»çš„è¦æ±‚ï¼šé¢„æµ‹åœ¨è¿™ä¸ªäº‹ä»¶ä¸­ä¹˜å®¢æ˜¯å¦æ­»äº¡ã€‚é™„ä¸Šå¯¹äºŽtrain setså’Œ test setsä¸­æ•°æ®çš„ä»‹ç»ã€‚æ›´å¤šè¯¦ç»†çš„å†…å®¹å‚çœ‹ï¼šhttps://www.kaggle.com/c/titanic æ•°æ®åˆ†æžé¡ºæ»‘è¿‡æ¸¡åˆ°ç¬¬äºŒé˜¶æ®µï¼Œæ•°æ®åˆ†æžï¼Œå¯¹äºŽç«žèµ›è€Œè¨€ï¼Œæˆ‘æ„Ÿè§‰å¯¹äºŽæ•°æ®çš„è®¤è¯†çš„é‡è¦æ€§å®Œå…¨ä¸äºšäºŽæ¨¡åž‹çš„é‡è¦æ€§ã€‚ä¹‹åŽæˆ‘ä»¬å°†å†æ¬¡æåˆ°è¿™å¥è¯ã€‚æˆ‘å°†ç»“åˆä»£ç è¿›è¡Œæ•°æ®åˆ†æžã€‚ pandas åŽŸç”Ÿæ•°æ®åˆ†æžå‡½æ•°1234import pandas as pdtrain =pd.read_csv(&apos;data/train.csv&apos;)test =pd.read_csv(&apos;data/test.csv&apos;)train.describe(include=&apos;all&apos;) é™¤äº†ä¸Šé¢ train.describe()ï¼Œä¸‹é¢è¿™ä¸¤ä¸ªä¹Ÿæ˜¯æ¯”è¾ƒå¸¸ç”¨çš„12train.head()train.columns å› ä¸ºæ€»ä½“çš„æ•°æ®é‡æ¯”è¾ƒå°‘ï¼Œæ‰€ä»¥æˆ‘ä»¬é€‰æ‹©æŠŠtrain set å’Œtest setè¿žæŽ¥èµ·æ¥è¿›è¡Œæ•°æ®åˆ†æžå’Œå¤„ç†ã€‚1combined2 = pd.concat([train_data, test_data], axis=0) æ•°æ®è´¨é‡åˆ†æž ç¼ºçœå€¼å¯¹äºŽç¼ºçœå€¼ï¼Œå¸¸ç”¨çš„æ‰‹æ®µå°±æ˜¯å¡«å……ï¼Œä½†æ˜¯é’ˆå¯¹ä¸åŒçš„æ•°æ®æœ‰ä¸åŒçš„å¡«å……æ‰‹æ®µï¼Œæœ‰çš„æ˜¯å‡å€¼å¡«å……ï¼Œæœ‰çš„æ˜¯é»˜è®¤å€¼å¡«å……è¿˜æœ‰çš„æ˜¯æ ¹æ®çŽ°æœ‰æ•°æ®è®­ç»ƒä¸€ä¸ª regressionè¿›è¡Œæ‹Ÿåˆ(è¿™ç§æƒ…å†µå‡ºçŽ°åœ¨ç¼ºçœçš„æ•°æ®æ¯”è¾ƒé‡è¦ï¼Œå¯¹äºŽç»“æžœçš„é¢„æµ‹æœ‰æ¯”è¾ƒå¼ºçš„ç›¸å…³æ€§çš„æ—¶å€™)ã€‚1combined2.Embarked.fillna(&apos;S&apos;, inplace=True) Embardked(ä¸Šèˆ¹æ¸¯å£)ä¸æ˜¯é‚£ä¹ˆèƒ½è¡¨çŽ°å‡ºå’Œç»“æžœ(survival)ç›¸å…³çš„å˜é‡ï¼Œæˆ‘ä¹ˆå¯ä»¥ç›´æŽ¥é‡‡ç”¨æŸä¸ªé»˜è®¤å€¼è¿›è¡Œå¡«å……ã€‚1combined2.Fare.fillna(np.median(combined2.Fare[combined2.Fare.notnull()]), inplace=True) Fare(èˆ¹ç¥¨)æˆ‘ä»¬é€‰æ‹©ä½¿ç”¨å‡å€¼å¡«å…… 12345678classers = [&apos;Fare&apos;,&apos;Parch&apos;,&apos;Pclass&apos;,&apos;SibSp&apos;,&apos;TitleCat&apos;,&apos;CabinCat&apos;,&apos;Sex_female&apos;,&apos;Sex_male&apos;, &apos;EmbarkedCat&apos;, &apos;FamilySize&apos;, &apos;NameLength&apos;, &apos;FamilyId&apos;]age_et = ExtraTreesRegressor(n_estimators=200)X_train = full_data.loc[full_data.Age.notnull(),classers]Y_train = full_data.loc[full_data.Age.notnull(),[&apos;Age&apos;]]X_test = full_data.loc[full_data.Age.isnull(),classers]age_et.fit(X_train,np.ravel(Y_train))age_preds = age_et.predict(X_test)full_data.loc[full_data.Age.isnull(),[&apos;Age&apos;]] = age_preds å› ä¸ºåœ¨ç‰¹å¾æå–çœ‹æ¥ age æ˜¯ä¸€ä¸ªæ¯”è¾ƒé‡è¦çš„å±žæ€§ï¼ˆä¸‹æ–‡ä¸­ä½¿ç”¨ageæ¥è¿›ä¸€æ­¥è®¡ç®—æ€§åˆ«ç‰¹å¾ï¼Œè€Œæ€§åˆ«ç‰¹å¾å¯¹äºŽsurvival æ˜¯é‡è¦çš„å› ç´ ï¼‰ï¼Œæ‰€ä»¥éœ€è¦é€šè¿‡ fitæ¥è¿›è¡Œå¡«å…… null å€¼ã€‚ å¼‚å¸¸å€¼å¼‚å¸¸å€¼çš„æ£€æµ‹12combined2.boxplot()plt.ylim(0, 1000) å¼‚å¸¸å€¼å¤„ç†å¤§å¤šæ•°æƒ…å†µä¸‹æˆ‘ä»¬éƒ½é‡‡å–å¿½è§†ï¼Œä½†æ˜¯æœ‰æ—¶å€™å¼‚å¸¸å€¼ä¸­å´è·Ÿç»“æžœæœ‰æ¯”è¾ƒå¼ºçš„ç›¸å…³æ€§ï¼Œæ¯”å¦‚è¯´è¯¥é¢˜ç›®åˆ†æ•°åœ¨0.9çš„ä¸€ä½å¤§ç¥žåœ¨åšå®¢ä¸­ä½¿ç”¨çš„ç‰¹å¾åŒ…å«åå­—é•¿åº¦ã€‚è¿™ä¸ªåœ¨æˆ‘ä¸€å¼€å§‹çš„ç‰¹å¾æå–ä¸­ç¡®å®žæ²¡æœ‰å¤ªåœ¨æ„åå­—é•¿åº¦ä¹Ÿå¯ä»¥å½“ä½œä¸€ç§å’Œç»“æžœ(servival or dead)ç›¸å…³çš„ç‰¹å¾ã€‚ é‡å¤å€¼é‡å¤å€¼çš„æ£€æµ‹1train[train.duplicated()==True] å¦‚æžœè¿è¡Œç»“æžœä¸ºç©ºï¼Œé‚£ä¹ˆå°±æ˜¯æ²¡æœ‰é‡å¤å€¼,å¦‚æžœæœ‰é‡å¤å€¼ï¼Œä¸€èˆ¬ä½¿ç”¨ä¸‹é¢ç±»ä¼¼çš„ä»£ç éƒ½æ˜¯å¯ä»¥åŽ»é™¤æŽ‰çš„ã€‚1df.drop_duplicates() åˆ†å¸ƒç‰¹å¾åˆ†æž1234567#åˆ†å¸ƒåˆ†æžfig ,ax =plt.subplots(2,2, figsize=(8,6))sns.countplot(&apos;Embarked&apos;, data =train, ax =ax[0,0])sns.countplot(&apos;Pclass&apos;, data =train, ax =ax[0,1])sns.violinplot(&apos;Survived&apos;, &apos;Age&apos;, data= train, ax =ax[1,0]).set(ylim =(-10, 80))sns.countplot(x =&apos;Survived&apos;, data= train, ax =ax[1,1])plt.tight_layout() è¿è¡Œéœ€è¦å¯¼å…¥ seaborn1import seaborn as sns è¿™é‡Œå®‰åˆ©ä¸€ä¸ªæ•°æ®å¯è§†åŒ–å·¥å…·-seabornã€‚å›žæ­£é¢˜ counplot()å¯ä»¥ç›´è§‚çš„çœ‹å‡ºå•ä¸ªæ•°æ®çš„ç‰¹å¾ï¼Œä½†æ˜¯æˆ‘ä»¬æ›´åŠ å…³å¿ƒçš„æ˜¯æ•°æ®å’Œæ•°æ®ä¹‹é—´çš„å…³ç³»ï¼Œæ›´å‡†ç¡®çš„æ˜¯æ•°æ®å’Œé¢„æµ‹æ•°æ®(survival )ä¹‹é—´çš„å…³ç³»ã€‚æ‰€ä»¥,æˆ‘ä»¬è¿›è¡Œç›¸å…³æ€§åˆ†æžã€‚123456plt.subplots(figsize=(10,8)) corrmat = train[train.columns[1:]].corr()sns.set(font_scale=1.5) hm = sns.heatmap(corrmat, cbar=True, annot=True, square=True, fmt=&apos;.2f&apos;, annot_kws=&#123;&apos;size&apos;: 10&#125;) plt.title(&apos;Pearson Correlation of Features&apos;, y=1.05, size=15) plt.show() ä»Žå›¾ä¸­å¯ä»¥çœ‹å‡ºï¼Œåœ¨åŽŸå§‹æ•°æ®é›†ç‰¹å¾ä¸­(ä¸ºä»€ä¹ˆè¿™ä¹ˆè¯´ï¼Œå—¯ï¼Œè¿™æ„å‘³ç€æˆ‘ä»¬ä¸‹æ–‡è¿˜è¦è¿›è¡Œ generate new features),Fareç‰¹å¾æ˜¯å’Œ survivalæœ€ç›¸å…³çš„ã€‚è¿™ä»Žæ•°æ®è§’åº¦è¿™ä½ èˆ¹ç¥¨ä»·é’±è¶Šé«˜ï¼Œä½ ç”Ÿå­˜çš„å‡ çŽ‡å°±è¶Šå¤§(ä¸‰è§‚å°½æ¯)ã€‚å—¯ï¼Œè¿™æ˜¯ç¬¦åˆç»æµŽç¤¾ä¼šçš„è¿è¡Œè§„å¾‹çš„ã€‚æˆ‘ä»¬åœ¨åˆ†æž Pearson Correlationçš„æ—¶å€™ï¼Œå…³æ³¨çš„æ˜¯æ•°å€¼çš„ç»å¯¹å€¼ï¼Œå¦‚æžœæ˜¯æ­£å€¼ï¼Œè¡¨ç¤ºæ­£ç›¸å…³ï¼›å¦‚æžœæ˜¯è´Ÿå€¼ï¼Œè¡¨ç¤ºè´Ÿç›¸å…³ã€‚ å¦‚æžœç»†å¿ƒçš„å°ä¼™ä¼´å‘çŽ°ï¼Œè¿™ä¸ªå¹¶æ²¡æœ‰æŠŠæ‰€æœ‰çš„å˜é‡çš„ç›¸å…³æ€§è¡¨ç¤ºå‡ºæ¥ï¼Œæ˜¯çš„ï¼Œä¸‹æ–‡æˆ‘å°†ç»™å‡ºä¸€ä¸ªåŠ å¼ºç‰ˆçš„ã€‚ ç‰¹å¾å·¥ç¨‹å½“æˆ‘ä»¬å¯¹æ•°æ®æœ‰äº†ä¸€ä¸ªåˆæ­¥çš„è®¤è¯†ï¼Œè¿™æ—¶å€™å°±å¯ä»¥è¿›è¡Œç‰¹å¾å·¥ç¨‹äº†ã€‚ç½‘ä¸Šæµä¼ å¾ˆå¹¿çš„ä¸€å¥è¯â€æ•°æ®ç‰¹å¾å†³å®šäº†æœºå™¨å­¦ä¹ çš„ä¸Šé™ï¼Œè€Œç®—æ³•ä¼˜åŒ–åªæ˜¯å°½å¯èƒ½é€¼è¿‘è¿™ä¸ªä¸Šé™â€ï¼Œæˆ‘æ·±æœ‰ä½“ä¼šã€‚å› ä¸ºä¹‹å‰è¿›è¡Œç‰¹å¾æå–ï¼Œç„¶åŽåœ¨kaggleçš„submission scoreæ˜¯0.73205,ç»è¿‡æ¨¡åž‹èžåˆç„¶åŽè¾¾åˆ°äº†0.78947ï¼Œæé«˜äº†5ä¸ªç™¾åˆ†ç‚¹ã€‚å½“è‡ªå·±åœ¨æ€è€ƒæ•°æ®ç‰¹å¾é‡æ–°è¿›è¡Œç‰¹å¾æå–çš„æ—¶å€™ï¼Œæœ€åŽçš„score æ˜¯0.82296.è¿™éƒ½æ˜¯ä»¥10ä¸ªç™¾åˆ†ç‚¹çš„æé«˜å•Šã€‚æ‰€ä»¥è¿™å¥è¯å¾ˆæœ‰é“ç†ï¼Œæˆ‘è¯•å›¾æ‰¾åˆ°è¿™å¥è¯çš„å‡ºå¤„ï¼Œä»¥è¡¨ç¤ºæˆ‘å¯¹äºŽç‰ˆæƒçš„å°Šé‡ï¼Œä½†æ˜¯ç§‘å­¦ä¸Šç½‘èƒ½åŠ›æœ‰é™ï¼Œæ²¡æœ‰æ‰¾è§ï¼Œä¹Ÿè®¸è¿™å¥è¯æ¥è‡ªç¾¤ä¼—çš„æ™ºæ…§å§ã€‚å›¾ï¼šæˆ‘åœ¨kaggle çš„submission å’Œç›¸åº”çš„score ä½†æ˜¯æˆ‘æƒ³å¼ºè°ƒçš„æ˜¯ç‰¹å¾æå–æ˜¯ä¸ªå¾ˆéš¾æœ‰æ¨¡æ¿åŒ–çš„ä¸œè¥¿ï¼Œè¿™å¾—çœ‹ä¸ªäººå¯¹äºŽè¿™ä¸ªé—®é¢˜çš„ç†è§£å’Œå¯¹äºŽæ•°æ®çš„ç†è§£ï¼Œå¯¹äºŽæ•°æ®å¼‚å¸¸å€¼çš„å¤„ç†ã€‚å¹¶ä¸”è¿˜æƒ³è¯´çš„æ˜¯è¿™ä¸ªä¸€ä¸ªè¿­ä»£çš„è¿‡ç¨‹ï¼Œä¸æ˜¯ä¸€æ­¥åˆ°ä½çš„ã€‚å½“åˆæ­¥æž„é€ å¥½è‡ªå·±ç‰¹å¾ä¹‹åŽå¯ä»¥ä½¿ç”¨å›¾å½¢åŒ–å·¥å…·è¿›è¡Œç®€å•çš„åˆ†æžä¸€ä¸‹ã€‚(ä¸‹å›¾æ˜¯æˆ‘ç¬¬ä¸€æ¬¡æž„é€ çš„ç‰¹å¾å·¥ç¨‹çš„å›¾å½¢åŒ–)12345678910111213141516171819def correlation_heatmap(df): _ , ax = plt.subplots(figsize =(14, 12)) colormap = sns.diverging_palette(220, 10, as_cmap = True) _ = sns.heatmap( df.corr(), cmap = colormap, square=True, cbar_kws=&#123;&apos;shrink&apos;:.9 &#125;, ax=ax, annot=True, linewidths=0.1,vmax=1.0, linecolor=&apos;white&apos;, annot_kws=&#123;&apos;fontsize&apos;:12 &#125; ) plt.title(&apos;Pearson Correlation of Features&apos;, y=1.05, size=15) train2 =train1.drop([&apos;NullCabin&apos;], axis =1)correlation_heatmap(train2) ä¸Šå›¾æ˜¯ç¬¬ä¸€æ¬¡ç‰¹å¾åˆ†æžçš„ç»“æžœ(kaggle score: 0.73205),survivalå’ŒPclasså’ŒFareæ˜¯æœ‰è¾ƒå¼ºçš„æ­£ç›¸å…³æ€§ã€‚ è¯¥å›¾æ˜¯ç¬¬äºŒæ¬¡ç‰¹å¾åˆ†æžåˆ†æžç»“æžœï¼ˆkaggle score:0.82296,survivalä¸Žmale_adult(-0.56)ã€sex_male(-0.54)è´Ÿç›¸å…³,å’Œfemale_adult(0.54)ã€sex_female(0.52)æ­£ç›¸å…³ã€‚ä½ çš„survivalçš„æ¦‚çŽ‡å’Œä½ çš„æ€§åˆ«å’Œå¹´é¾„æœ‰å…³ï¼Œå¦‚æžœä½ æ˜¯æˆå¹´å¥³å­ï¼Œé‚£ä¹ˆä½ å¾ˆå¤§çš„æ¦‚çŽ‡ä¸ä¼šæ­»äº¡(åƒRoseé‚£æ ·)ï¼›å¦‚æžœä½ æ˜¯æˆå¹´ç”·å­ï¼Œé‚£ä¹ˆä½ æœ‰å¾ˆå¤§æ¦‚çŽ‡ä½“çŽ°è‹±ä¼¦çš„ç»…å£«é£Žåº¦ï¼Œä¸»åŠ¨(Jacké‚£æ ·)æˆ–è€…è¢«é€‰æ‹©æ­»äº¡ã€‚çž¬é—´æƒ³èµ·äº†Titanicç”µå½±ä¸­Jackå’ŒRose çš„åœºæ™¯ï¼Œå¥½æ„Ÿäººå•Š!!! æ¨¡åž‹è®­ç»ƒå‘çŽ°å†™äº†è¿™ä¹ˆä¹…ï¼Œè¿˜æ²¡æœ‰å¼€å§‹è®­ç»ƒæ¨¡åž‹ã€‚åŠ å¿«è„šæ­¥â€¦ä¸‹é¢çš„å†…å®¹ä»¥ç¬¬ä¸€æ¬¡è®­ç»ƒæ¨¡åž‹ä¸ºä¾‹ã€‚åœ¨å»ºç«‹åŸºæœ¬æ¨¡åž‹ä¹‹å‰æˆ‘ä»¬éœ€è¦å…ˆå¼•å…¥è¯„ä»·å‡½æ•°ï¼Œä»¥è¯„ä»·ä¸åŒæ¨¡åž‹æ€§èƒ½çš„å¥½åã€‚ é€šè¿‡å‡å€¼å’Œæ–¹å·®æ¥è¯„ä»·æ¨¡åž‹æ€§èƒ½çš„ä¼˜åŠ£1234from sklearn import cross_validation def rmsl(clf): s = cross_validation.cross_val_score(clf, X_train, y_train, cv=5) return (s.mean(),s.std()) å»ºç«‹åŸºæœ¬æ¨¡åž‹1234567891011121314151617181920212223242526272829303132333435363738394041424344from sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis,gaussian_processNLA =[ #ensemble methods ensemble.AdaBoostClassifier(), ensemble.BaggingRegressor(), ensemble.GradientBoostingClassifier(), ensemble.RandomForestClassifier(n_estimators=60), # Gaussian process gaussian_process.GaussianProcessClassifier(), # LM linear_model.LogisticRegression(C=1.0, penalty=&apos;l1&apos;), #Navies Bayes naive_bayes.GaussianNB(), # Nearest Neighbor neighbors.KNeighborsClassifier(n_neighbors=3), # Svm svm.SVC(probability=True), svm.LinearSVC(), #Tree tree.DecisionTreeClassifier(), tree.ExtraTreeClassifier() ]#create table to compare MLAMLA_columns = [&apos;MLA Name&apos;, &apos;MLA Parameters&apos;,&apos;MLA Train Accuracy Mean&apos;, &apos;MLA Test Accuracy Mean&apos;, &apos;MLA Test Accuracy Min&apos; ,&apos;MLA Time&apos;] MLA_compare = pd.DataFrame(columns = MLA_columns) row_index = 0 for alg in MLA: #set name and parameters MLA_compare.loc[row_index, &apos;MLA Name&apos;] = alg.__class__.__name__ MLA_compare.loc[row_index, &apos;MLA Parameters&apos;] = str(alg.get_params()) #score model with cross validation: cv_results = model_selection.cross_validate(alg, X_train, y_train, cv =5,return_train_score=True) MLA_compare.loc[row_index, &apos;MLA Time&apos;] = cv_results[&apos;fit_time&apos;].mean() MLA_compare.loc[row_index, &apos;MLA Train Accuracy Mean&apos;] = cv_results[&apos;train_score&apos;].mean() MLA_compare.loc[row_index, &apos;MLA Test Accuracy Mean&apos;] = cv_results[&apos;test_score&apos;].mean() MLA_compare.loc[row_index, &apos;MLA Test Accuracy Min&apos;] = cv_results[&apos;test_score&apos;].min() #let&apos;s know the worst that can happen! row_index+=1MLA_compare.sort_values(by = [&apos;MLA Test Accuracy Mean&apos;], ascending = False, inplace = True) å½“æˆ‘ä»¬å‘çŽ°æŸä¸ªæ¨¡åž‹æ•ˆæžœæ¯”è¾ƒå¥½çš„æ—¶å€™ï¼Œæˆ‘ä»¬ä»ç„¶å¯ä»¥è¿›ä¸€æ­¥è°ƒå‚ã€‚ä½†æ˜¯è¿™ç§è°ƒå‚å¹¶ä¸æ˜¯æ¯æ¬¡ä¼šå¾—åˆ°better result,æœ‰æ—¶å€™åªæ˜¯ä¸€ä¸ªdecent resultã€‚è°ƒå‚æ˜¯ä¸ªæŠ€æœ¯æ´»ã€‚ä»¥ä¸Šå›¾ä¸­çš„ DecisionTreeClassifierä¸ºä¾‹è¿›è¡Œè°ƒå‚ã€‚12345678param_grid = &#123;&apos;criterion&apos;: [&apos;gini&apos;, &apos;entropy&apos;], &apos;splitter&apos;: [&apos;best&apos;, &apos;random&apos;], &apos;max_depth&apos;: [None, 2,4,6,8,10], &apos;min_samples_split&apos;: [5,10,15,20,25], &apos;max_features&apos;: [None, &apos;auto&apos;, &apos;sqrt&apos;, &apos;log2&apos;] &#125;tune_model = model_selection.GridSearchCV(tree.DecisionTreeClassifier(), param_grid=param_grid, scoring = &apos;accuracy&apos;, cv = 5) cv_results = model_selection.cross_validate(tune_model, X_train, y_train, cv = 5) ä½¿ç”¨çš„æ˜¯sklearn ä¸­çš„model_selection æ¨¡å—ï¼Œè¿›è¡ŒGridSearchï¼Œå…¶å®žåªæ˜¯æŠŠè°ƒå‚è¿‡ç¨‹è‡ªåŠ¨åŒ–ç¨‹åºåŒ–ã€‚å¾—åˆ°çš„ç»“æžœæ˜¯1230.863068608315 #train mean0.79803322024 # test mean0.77094972067 #test min æˆ‘ä»¬é€šè¿‡æ¯”å¯¹å‘çŽ°è¿™ä¸ªç»“æžœå’Œä¸Šå›¾çš„ç»“æžœæ˜¯ç¨å¾®å˜å·®çš„ã€‚å¯è§†åŒ–æ˜¾ç¤ºå„ä¸ªç®—æ³•çš„æ•ˆçŽ‡ï¼š1234sns.barplot(x=&apos;MLA Test Accuracy Mean&apos;, y = &apos;MLA Name&apos;, data = MLA_compare, color = &apos;m&apos;) plt.title(&apos;Machine Learning Algorithm Accuracy Score \n&apos;) plt.xlabel(&apos;Accuracy Score (%)&apos;) plt.ylabel(&apos;Algorithm&apos;) å¯¹æ¯”ä¹‹åŽæˆ‘ä»¬é€‰å–å‡ ä¸ªæ•ˆæžœæ¯”è¾ƒâ€œå¥½â€çš„æ¨¡åž‹ï¼Œç„¶åŽè¿›è¡Œä¸‹ä¸€æ­¥çš„æ¨¡åž‹èžåˆã€‚12345678910111213141516171819MLA_best = [ #Ensemble Methods ensemble.AdaBoostClassifier(), # 0.76076 ensemble.BaggingClassifier(), # 0.72248 ensemble.GradientBoostingClassifier(), # 0.73684 ensemble.RandomForestClassifier(n_estimators = 60), # 0.72727 #GLM linear_model.LogisticRegression(C=1.0, penalty=&apos;l1&apos;, tol=1e-6), # 0.77990 linear_model.RidgeClassifierCV(), # 0.77033 linear_model.LogisticRegressionCV() #0.77033 ] row_index = 0 for alg in MLA_best: algname = alg.__class__.__name__ alg.fit(X_train, y_train) predictions = alg.predict(X_test) result = pd.DataFrame(&#123;&apos;PassengerId&apos;:test[&apos;PassengerId&apos;].as_matrix(), &apos;Survived&apos;:predictions.astype(np.int32)&#125;) result.to_csv(algname+&quot;.csv&quot;, index=False) # save the results row_index+=1 æ¨¡åž‹èžåˆç®€å•çš„è¯´æ¨¡åž‹èžåˆå°±æ˜¯é€šè¿‡å¤šä¸ªdecentæ¨¡åž‹çš„ç»“æžœé€šè¿‡æŸç§æ–¹å¼çš„ç»“åˆï¼Œäº§ç”Ÿäº†æ¯”åŽŸæ¥å•ä¸ªæ¨¡åž‹betterçš„ç»“æžœã€‚å…³äºŽæ¨¡åž‹èžåˆçš„è¯¦ç»†å†…å®¹ï¼Œè¯·ç§»æ­¥å¦ä¸€ç¯‡æ–‡ç« æ¨¡åž‹èžåˆ(Ensemble learning)æˆ‘ä»¬è¿™é‡Œä»¥stacking(äºŒå±‚)ä¸ºä¾‹è¯´æ˜Žæ¨¡åž‹èžåˆã€‚1234567891011121314151617181920212223ntrain = train.shape[0] #891 ntest = test.shape[0] #418 SEED = 0 # for reproducibility NFOLDS = 5 # set folds for out-of-fold prediction kf =model_selection.KFold(n_splits=NFOLDS, random_state=SEED)# å°è£…ç®—æ³•åŸºæœ¬æ“ä½œ class SklearnHelper(object): def __init__(self, clf, seed=0, params=None): params[&apos;random_state&apos;] = seed self.clf = clf(**params) def train(self, x_train, y_train): self.clf.fit(x_train, y_train) def predict(self, x): return self.clf.predict(x) def fit(self,x,y): return self.clf.fit(x,y) def feature_importances(self,x,y): print(self.clf.fit(x,y).feature_importances_) return self.clf.fit(x,y).feature_importances_ ä¸‹é¢æ˜¯å®šä¹‰äº”æŠ˜äº¤å‰éªŒè¯çš„æ–¹æ³•ï¼Œé»˜è®¤æ˜¯ä¸‰æŠ˜ã€‚123456789101112131415161718def get_oof(clf, x_train, y_train, x_test): oof_train = np.zeros((ntrain,)) oof_test = np.zeros((ntest,)) oof_test_skf = np.empty((NFOLDS, ntest)) for i, (train_index, test_index) in enumerate(kf.split(x_train)): x_tr = x_train[train_index] y_tr = y_train[train_index] x_te = x_train[test_index] clf.train(x_tr, y_tr) oof_train[test_index] = clf.predict(x_te) oof_test_skf[i, :] = clf.predict(x_test) oof_test[:] = oof_test_skf.mean(axis=0) return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1) # æƒ³è®©zå˜æˆåªæœ‰ä¸€åˆ—ï¼Œè¡Œæ•°ä¸çŸ¥é“å¤šå°‘ 1234567891011121314151617181920212223242526272829303132from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier,AdaBoostClassifier, GradientBoostingClassifier# å®šä¹‰å››ä¸ªä¸åŒçš„å¼±åˆ†ç±»å™¨çš„å‚æ•°å€¼ # Random Forest parameters rf_params = &#123; &apos;n_jobs&apos;: -1,&apos;n_estimators&apos;: 500,&apos;warm_start&apos;: True, &apos;max_depth&apos;: 6,&apos;min_samples_leaf&apos;: 2, &apos;max_features&apos; : &apos;sqrt&apos;,&apos;verbose&apos;: 0#&apos;max_features&apos;: 0.2, &#125; # Extra Trees Parameters et_params = &#123; &apos;n_jobs&apos;: -1,&apos;n_estimators&apos;:500,&apos;max_depth&apos;: 8,&apos;min_samples_leaf&apos;: 2,&apos;verbose&apos;: 0 #&apos;max_features&apos;: 0.5, &#125; # AdaBoost parameters ada_params = &#123; &apos;n_estimators&apos;: 500,&apos;learning_rate&apos; : 0.75 &#125; # Gradient Boosting parameters gb_params = &#123; &apos;n_estimators&apos;: 500,&apos;max_depth&apos;: 5,&apos;min_samples_leaf&apos;: 2, &apos;verbose&apos;: 0 #&apos;max_features&apos;: 0.2, &#125; # Support Vector Classifier parameters # svc_params = &#123; # &apos;kernel&apos; : &apos;linear&apos;,&apos;C&apos; : 0.025 # &#125; # åˆ›å»ºå››ä¸ªè‹¥åˆ†ç±»å™¨æ¨¡åž‹ rf = SklearnHelper(clf=RandomForestClassifier, seed=SEED, params=rf_params) et = SklearnHelper(clf=ExtraTreesClassifier, seed=SEED, params=et_params) ada = SklearnHelper(clf=AdaBoostClassifier, seed=SEED, params=ada_params) gb = SklearnHelper(clf=GradientBoostingClassifier, seed=SEED, params=gb_params) 1234567891011121314151617181920#X_train =X_train.values#X_test =X_test.values# ä½¿ç”¨äº”æŠ˜äº¤å‰æ–¹æ³•åˆ†åˆ«è®¡ç®—å‡ºä½¿ç”¨ä¸åŒç®—æ³•çš„é¢„æµ‹ç»“æžœï¼Œè¿™äº›ç»“æžœå°†ç”¨äºŽStackingçš„ç¬¬äºŒå±‚é¢„æµ‹ et_oof_train, et_oof_test = get_oof(et, X_train, y_train, X_test) # Extra Trees rf_oof_train, rf_oof_test = get_oof(rf,X_train, y_train, X_test) # Random Forest ada_oof_train, ada_oof_test = get_oof(ada, X_train, y_train, X_test) # AdaBoost gb_oof_train, gb_oof_test = get_oof(gb,X_train, y_train, X_test) # Gradient Boost rf_feature = rf.feature_importances(X_train,y_train) et_feature = et.feature_importances(X_train, y_train) ada_feature = ada.feature_importances(X_train, y_train) gb_feature = gb.feature_importances(X_train,y_train) feature_dataframe = pd.DataFrame( &#123;&apos;features&apos;: cols, &apos;Random Forest feature importances&apos;: rf_feature, &apos;Extra Trees feature importances&apos;: et_feature, &apos;AdaBoost feature importances&apos;: ada_feature, &apos;Gradient Boost feature importances&apos;: gb_feature &#125;) æŽ¥ä¸‹æ¥ä»¥ç¬¬ä¸€å±‚ä¸ºä¸ºåŸºç¡€è®­ç»ƒç¬¬äºŒå±‚12345678base_predictions_train = pd.DataFrame( &#123; &apos;RandomForest&apos;: rf_oof_train.ravel(),# # ravelå‡½æ•°åœ¨é™ç»´æ—¶é»˜è®¤æ˜¯è¡Œåºä¼˜å…ˆ &apos;ExtraTrees&apos;: et_oof_train.ravel(), &apos;AdaBoost&apos;: ada_oof_train.ravel(), &apos;GradientBoost&apos;: gb_oof_train.ravel() &#125;) X_train2 = np.concatenate(( et_oof_train, rf_oof_train, ada_oof_train, gb_oof_train), axis=1) X_test2 = np.concatenate(( et_oof_test, rf_oof_test, ada_oof_test, gb_oof_test), axis=1) ä½¿ç”¨XGBoostè®­ç»ƒç¬¬äºŒå±‚çš„æ•°æ®ã€‚å…³äºŽXGBoostä¸ºä»€ä¹ˆæ˜¯æœ‰æ•ˆçš„å’Œç›¸å…³çš„æ¦‚å¿µï¼Œè¯·ç§»æ­¥XGBoost123456789101112131415# XGboost import xgboost as xgbgbm = xgb.XGBClassifier( #learning_rate = 0.02, n_estimators= 2000, max_depth= 4, min_child_weight= 2, #gamma=1, gamma=0.9, subsample=0.8, colsample_bytree=0.8, objective= &apos;binary:logistic&apos;, nthread= -1, scale_pos_weight=1).fit(X_train2, y_train) predictions = gbm.predict(X_test2) æœ€åŽäº§ç”Ÿç»“æžœæ–‡ä»¶ StackingSubmission.csv12StackingSubmission = pd.DataFrame(&#123;&apos;PassengerId&apos;:test.PassengerId, &apos;Survived&apos;: predictions &#125;) StackingSubmission.to_csv(&quot;StackingSubmission.csv&quot;, index=False) # 0.78947 å‚è€ƒæ–‡çŒ®æœ¬æ–‡åœ¨æ•ˆæžœå¯è§†åŒ–ä¸­å€Ÿé‰´è¯¥åšå®¢ç‰¹å¾æå–å‚çœ‹kaggleå¤šä½å¤§ç¥žï¼Œåœ¨è¿™é‡Œå°±è°¢è¿‡â€¦]]></content>
      <categories>
        <category>ç«žèµ›</category>
      </categories>
      <tags>
        <tag>Kaggle</tag>
        <tag>Titanic</tag>
        <tag>æ³°å¦å°¼å…‹</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¨¡åž‹èžåˆ]]></title>
    <url>%2F2018%2F06%2F05%2F%E6%A8%A1%E5%9E%8B%E8%9E%8D%E5%90%88%2F</url>
    <content type="text"><![CDATA[å¤šä¸ªæœ‰å·®å¼‚æ€§çš„æ¨¡åž‹èžåˆå¯ä»¥æé«˜æ•´ä½“çš„æ€§èƒ½ã€‚å®ƒèƒ½åŒæ—¶é™ä½Žæœ€ç»ˆæ¨¡åž‹çš„bias å’Œvarianceï¼Œä»Žè€Œåœ¨æé«˜ç«žèµ›åˆ†æ•°çš„åŒæ—¶é™ä½Žoverfitting çš„é£Žé™©ã€‚ ä»Žç»“æžœæ–‡ä»¶ä¸­èžåˆè¿™ç§åšæ³•ä¸éœ€è¦é‡æ–°è®­ç»ƒæ¨¡åž‹ï¼Œèžåˆç«žèµ›æäº¤çš„ç»“æžœæ–‡ä»¶å°±å¯ä»¥ï¼Œç®€å•ä¾¿æ·ã€‚ VotingæŠ•ç¥¨åˆ¶ï¼šå°‘æ•°æœä»Žå¤šæ•°ã€‚å¦‚ä¸€ä¸ªåˆ†ç±»é—®é¢˜ï¼Œå¤šä¸ªæ¨¡åž‹çš„æŠ•ç¥¨ï¼ˆå½“ç„¶å¯ä»¥è®¾ç½®æƒé‡ï¼Œè‹¥æ²¡æœ‰å°±æ˜¯å¹³å‡æŠ•ç¥¨ï¼‰ï¼Œæœ€ç»ˆæŠ•ç¥¨æ•°æœ€å¤šçš„ç±»å°±æ˜¯è¢«é¢„æµ‹çš„ç±»ã€‚å¯¹äºŽåŠ æƒè¡¨å†³èžåˆï¼Œæ€§èƒ½è¡¨çŽ°è¾ƒå·®çš„æ¨¡åž‹ï¼ˆæƒå€¼æ¯”è¾ƒä½Žï¼‰åªèƒ½é€šè¿‡å’Œå…¶ä»–æ¨¡åž‹ä¿æŒä¸€è‡´å¢žå¼ºè‡ªå·±çš„è¯´æœåŠ›ã€‚å¯¹äºŽç»“æžœå–å¹³å‡èžåˆï¼Œåœ¨ä¸åŒçš„è¯„ä¼°å‡†åˆ™ä¸Šä¹Ÿèƒ½èŽ·å¾—ä¸é”™çš„æ•ˆæžœåœ¨äºŽï¼šå–å‡å€¼å¸¸å¸¸èƒ½å‡å°‘è¿‡æ‹Ÿåˆçš„çŽ°è±¡ã€‚å¦‚å›¾æ‰€ç¤ºï¼šå¦‚æžœå•ä¸ªæ¨¡åž‹è¿‡æ‹Ÿåˆäº§ç”Ÿäº†ç»¿è‰²çš„è¾¹ç¼˜ï¼Œè¿™æ—¶å€™åŽ»å¹³å‡è¿™ç§ç­–ç•¥ä½¿å¾—å†³ç­–è¾¹ç•Œå˜æˆé»‘è‰²çš„è¾¹ç¼˜ï¼Œè¿™æ ·çš„æ•ˆæžœæ›´å¥½ã€‚æœºå™¨å­¦ä¹ çš„ç›®çš„å¹¶ä¸æ˜¯è®©æ¨¡åž‹è®°ä½è®­ç»ƒæ•°æ®ï¼Œè€Œæ˜¯å…·æœ‰æ›´å¥½çš„æ³›åŒ–æ€§ã€‚ RankingRankçš„æ€æƒ³å…¶å®žå’ŒAveragingä¸€è‡´ï¼Œä½†Rankæ˜¯æŠŠæŽ’ååšå¹³å‡ï¼Œå¯¹äºŽAUCæŒ‡æ ‡æ¯”è¾ƒæœ‰æ•ˆã€‚ è®­ç»ƒæ¨¡åž‹èžåˆ Bagging:ä½¿ç”¨è®­ç»ƒæ•°æ®çš„ä¸åŒéšæœºå­é›†æ¥è®­ç»ƒæ¯ä¸ª Base Modelï¼Œæœ€åŽè¿›è¡Œæ¯ä¸ª Base Model æƒé‡ç›¸åŒçš„ Voteã€‚ä¹Ÿå³ Random Forest çš„åŽŸç†ã€‚ Boosting:Boostingæ˜¯ä¸€ç§è¿­ä»£çš„æ–¹æ³•ï¼Œæ¯ä¸€æ¬¡è®­ç»ƒä¼šæ›´å…³å¿ƒä¸Šä¸€æ¬¡è¢«åˆ†é”™çš„æ ·æœ¬ï¼Œæ¯”å¦‚æ”¹å˜è¢«é”™åˆ†çš„æ ·æœ¬çš„æƒé‡çš„Adaboostæ–¹æ³•ã€‚ä¹Ÿå³ Gradient Boostingï¼ŒAdaboost çš„åŽŸç†ã€‚æ¯” Bagging æ•ˆæžœå¥½ï¼Œä½†æ›´å®¹æ˜“ Overfitã€‚ Blendingç”¨ä¸ç›¸äº¤çš„æ•°æ®è®­ç»ƒä¸åŒçš„ Base Modelï¼Œå°†å®ƒä»¬çš„è¾“å‡ºå–ï¼ˆåŠ æƒï¼‰å¹³å‡ã€‚å®žçŽ°ç®€å•ï¼Œä½†å¯¹è®­ç»ƒæ•°æ®åˆ©ç”¨å°‘äº†ã€‚ Stackinig(ä»¥äºŒå±‚ä¸ºä¾‹)åœ¨ç½‘ä¸Šä¸ºæ•°ä¸å¤šçš„å…³äºŽstackingçš„å†…å®¹ä¸­ï¼Œç›¸ä¿¡ä½ å·²ç»çœ‹è¿‡è¿™å¼ å›¾ç‰‡ï¼šPS:è¿™ä¸æ˜¯åŽŸå›¾ï¼Œæ˜¯åœ¨åŽŸå›¾çš„åŸºç¡€ä¸Šç»è¿‡ä¿®æ”¹ï¼ˆæŠŠæœ€ä¸Šé¢æ ‡é¢˜çš„model 1ï¼Œ2,3,4,5 ä¿®æ”¹ä¸ºmodel 1,1,1,1,1ã€‚å› ä¸ºè¿™ä¸ªä¸€ä¸ªmodel çš„ä¸åŒé˜¶æ®µï¼Œä¸æ˜¯å¤šä¸ªæ¨¡åž‹ï¼‰ æƒ³æ¯”è¾ƒè€Œè¨€æˆ‘æ›´åŠ å–œæ¬¢ä¸‹é¢è¿™å¼ å›¾ç‰‡ï¼Œå› ä¸ºå®ƒæŠŠstacking çš„ä¸åŒé˜¶æ®µè¡¨è¾¾çš„æ›´åŠ æ¸…æ¥šï¼Œå°¤å…¶æ˜¯ç»è¿‡model 1ä¹‹åŽï¼Œmodel 2æ˜¯åœ¨model 1çš„åŸºç¡€ä¸Šè¿›è¡Œè®­ç»ƒçš„ã€‚ å¯¹äºŽç¬¬äºŒé˜¶æ®µä½¿ç”¨çš„æ ·æœ¬é›†åˆï¼Œä¸Šå›¾ä½¿ç”¨çš„æ˜¯ç¬¬ä¸€é˜¶æ®µçš„ç»“æžœæ•°æ®é›†ï¼Œå½“ç„¶è¿˜æœ‰ä¸€ç§æ–¹å¼ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚ åœ¨è¯¥å›¾ä¸­ä¸Šä¸€é˜¶æ®µçš„ç»“æžœ(prob 1-N)åˆ—å’ŒåŽŸå§‹æ•°æ®é›†ç»„æˆæ–°çš„ç‰¹å¾å‘é‡ï¼Œè®­ç»ƒç¬¬äºŒé˜¶æ®µæ¨¡åž‹ã€‚ åè¯è§£é‡Š cross validationäº¤å‰éªŒè¯å½“è¯„ä¼°ä¸åŒçš„å‚æ•°è®¾ç½®ï¼Œå¯¹ç®—æ³•è¡¨çŽ°çš„å½±å“æ—¶ï¼Œä»ç„¶å­˜åœ¨åˆ™è¿‡æ‹Ÿåˆçš„é£Žé™©ã€‚å› ä¸ºåœ¨è°ƒæ•´å‚æ•°ï¼Œä¼˜åŒ–æµ‹è¯•é›†çš„ç®—æ³•è¡¨çŽ°æ—¶ï¼Œæµ‹è¯•é›†çš„ä¿¡æ¯å·²ç»æ³„æ¼è¿›æ¨¡åž‹ä¸­äº†ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œéœ€è¦ä¸€éƒ¨åˆ†æ•°æ®ä½œä¸ºéªŒè¯é›†(validation set)ã€‚ è¿™æ ·ï¼Œç”¨è®­ç»ƒé›†(Train set)çš„æ•°æ®è®­ç»ƒæ¨¡åž‹ï¼›ç”¨éªŒè¯é›†å¯¹æ¨¡åž‹å‚æ•°è°ƒå‰‚ï¼Œå¦‚ä¸Šè¿°ç¨‹åºä¸­çš„Cå€¼ï¼›æœ€åŽï¼Œç®—æ³•çš„è¯„ä»·åœ¨æµ‹è¯•é›†(Test set)ä¸Šå®Œæˆã€‚ä½†æ˜¯å½“æ•°æ®æœ‰åŽŸæ¥çš„ä¸¤ä»½åŒ–æˆä¸‰ä»½ä¹‹åŽï¼Œé™ä½Žäº†å¯»æ•°æ®é‡ï¼›å¦å¤–ç®—æ³•çš„è¡¨çŽ°ä¾èµ–äºŽä¸‰ä¸ªæ•°æ®é›†çš„åˆ’åˆ†ã€‚è§£å†³ä¸Šè¿°ä¸¤ä¸ªé—®é¢˜çš„å¸¸è§æ–¹æ³•: cross validation. æµ‹è¯•é›†ä»ç„¶å•ç‹¬åˆ’åˆ†å‡ºæ¥ï¼Œä½†æ˜¯ validation setä¸ç”¨å•ç‹¬åˆ’åˆ†ã€‚å°†è®­ç»ƒé›†åˆ’åˆ†ä¸ºkä¸ªå°çš„æ•°æ®é›†ï¼Œç§°ä¹‹ä¸ºk-fold CVã€‚å¯¹æ¯ä¸ªfoldè¿›è¡Œä¸‹åˆ—è¿‡ç¨‹ï¼š ç”¨å…¶ä»–k-1 folds ä½œä¸ºtraining setsï¼Œè®­ç»ƒæ¨¡åž‹ æ¨¡åž‹çš„ç»“æžœç”¨å‰©ä¸‹çš„ä¸€ä¸ª foldè¿›è¡Œè¯„ä»·æ¨¡åž‹çš„æ€§èƒ½ç”¨ä¸Šè¿°å¾ªçŽ¯ä¸­çš„ k-fold äº¤å‰éªŒè¯é›†çš„å¹³å‡å€¼è¡¨çŽ°ï¼Œè¿™æ ·çš„åšæ³•å¢žåŠ äº†è®¡ç®—é‡ï¼Œä½†æ˜¯æé«˜æ•°æ®çš„åˆ©ç”¨æ•ˆçŽ‡ã€‚ å‚è€ƒæ–‡çŒ®https://blog.csdn.net/u013395516/article/details/79745063https://blog.csdn.net/u012969412/article/details/76636336https://blog.csdn.net/u012604810/article/details/77579782]]></content>
      <categories>
        <category>ç«žèµ›</category>
      </categories>
      <tags>
        <tag>Kaggle</tag>
        <tag>ä¼˜åŒ–</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[XGBoost]]></title>
    <url>%2F2018%2F06%2F05%2FXGBoost%2F</url>
    <content type="text"><![CDATA[Introduction to XGBoostXGBoost is short for â€œExtreme Gradient Boostingâ€, where the term â€œGradient Boostingâ€ is proposed in the paper Greedy Function Approximation: A Gradient Boosting Machine, by Friedman. XGBoost is based on this original model. And most of the content is based on the websit(http://xgboost.readthedocs.io/en/latest/model.html) element of Supervised learningXGBoost is used for supervised learning problems, where we use the training data (with multiple features) xi to predict a target variable yi. Objective Function: Training Loss + RegularizationA very important fact about objective functions is they must always contain two parts: training loss and regularization.For example, a commomly used training loss is mean squared error.Another commonly used loss function is logistic loss for logistic regression The regularization term is what people usually forget to add. The regularization term controls the complexity of the model, which helps us to avoid overfitting.The general principle is we want both a simple and predictive model. The tradeoff between the two is also referred as bias-variance tradeoff in machine learning. Tree EnsembleThe tree ensemble model is a set of classification and regression trees(CART). Here is a simple example of a CART that classifies whether someone will like computer games.A CART is a bit different from decision trees, where the leaf only contains decision values. In CART, a real score is associated with each of the leaves, which gives us richer interpretations that go beyond classification. This also makes the unified optimization step easier, as we will see in a later part of this tutorial. Usually, a single tree is not strong enough to be used in practice. What is actually used is the so-called tree ensemble model, which sums the prediction of multiple trees together.Here is an example of a tree ensemble of two trees. The prediction scores of each individual tree are summed up to get the final score. If you look at the example, an important fact is that the two trees try to complement each other. Now here comes the question, what is the model for random forests? It is exactly tree ensembles! So random forests and boosted trees are not different in terms of model, the difference is how we train them. This means if you write a predictive service of tree ensembles, you only need to write one of them and they should directly work for both random forests and boosted trees. One example of why elements of supervised learning rock. Tree BoostingAfter introducing the model, let us begin with the real training part. How should we learn the trees? The answer is, as is always for all supervised learning models: define an objective function, and optimize it! Assume we have the following objective function (remember it always needs to contain training loss and regularization) Additive TrainingFirst thing we want to ask is what are the parameters of trees? You can find that what we need to learn are those functions fi, with each containing the structure of the tree and the leaf scores. This is much harder than traditional optimization problem where you can take the gradient and go. It is not easy to train all the trees at once. Instead, we use an additive strategy: fix what we have learned, and add one new tree at a time. It remains to ask, which tree do we want at each step? A natural thing is to add the one that optimizes our objective. Training modelThe XGBoost model for classification is called XGBClassifier(regression is called XGBRegressor). We can create and and fit it to our training dataset. Models are fit using the scikit-learn API and the model.fit() function.]]></content>
      <categories>
        <category>ç«žèµ›</category>
      </categories>
      <tags>
        <tag>XGBoost</tag>
        <tag>æ¨¡åž‹èžåˆ(Ensemble)</tag>
        <tag>Kaggle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ç­‰æ¦‚çŽ‡ç”Ÿæˆå™¨]]></title>
    <url>%2F2018%2F05%2F28%2F%E7%AD%89%E6%A6%82%E7%8E%87%E7%94%9F%E6%88%90%E5%99%A8%2F</url>
    <content type="text"><![CDATA[æ€»ä½“æ¥è¯´æ˜¯å¯ä»¥æœ‰ä¸¤ç§æ€è·¯ï¼šä¸€ç§æ˜¯äºŒçº§åˆ¶çš„ç†è§£ï¼Œä¸€ç§æ˜¯ç”Ÿæˆæ›´å¤§çš„æ•°å­—çš„é›†åˆï¼Œç„¶åŽæ±‚ä½™+å‰ªæžï¼ˆé‡æŠ•ï¼‰ã€‚ ç»™å®šä¸€ä¸ªå¯ä»¥ç­‰æ¦‚çŽ‡ç”Ÿæˆ1-3çš„rand3()å‡½æ•°ç”Ÿæˆå™¨ï¼Œæ±‚è§£å¯ä»¥éšæœºç­‰æ¦‚çŽ‡ç”Ÿæˆ1-7çš„rand7()å‡½æ•°ç”Ÿæˆå™¨ã€‚ è§£é¢˜æ€è·¯ ä½¿ç”¨rand3()å‡½æ•°ç”Ÿæˆ1-9çš„æ•°å­—ï¼Œç„¶åŽä¸¢å¼ƒ8,9ä¸¤ç§å¯èƒ½ã€‚ å®žçŽ°ä»£ç  12345678910class Solution: def random7(self): x =8 while x&gt;7: x =self.random3() + (self.random3()-1)*3 return x def random3(self): return if __name__ =="__main__": print(Solution().random7()) Given a random number generator rand5() generate gen7() 12345def rand7a(): rand7 =22 while rand7&gt;=21: rand7 =rand5()+ rand5()*5 return rand7 %7 1st approach:rand2() in binary is 000 or 001 with 50-50 probability.rand2()2 is 000 or 010 with 50-50 probability.rand2()4 is 000 or 100 with 50-50 probability.So the sum is binary xxx where each x has a 50-50 probability of 0 or 1; so each 000 to 111 has a probablilty of 1/8. 1234567def rand7b(): rand7 =7 while rand7 ==7: rand7 =rand2() +rand2()*2+ rand2()*4 return rand7 2nd approach: going through each of the 8 possibilities of the 3 rand2()s:0+0+0 or 1+0+0 or 0+2+0 or 1+2+0 or 0+0+4 or 1+0+4 or 0+2+4 or 1+2+4all of these with equal probablilty 1/8. ä»Žå¤§æ•°å­—çš„ random åˆ°å°æ•°å­—çš„randomï¼Œéœ€è¦åšçš„æ˜¯ç­‰æ¦‚çŽ‡çš„ç”Ÿæˆï¼Œæ‰€ä»¥å¯¹äºŽä¸ç¬¦åˆè¦æ±‚çš„æ˜¯ å°±æ˜¯ é‡æ–°rand() 12345int rand2() &#123; int x = rand5(); if x == 4 return rand2(); // restart else return x % 2;&#125; Given rand2(), you should get rand5() è¿™ä¸ªæ˜¯å¯ä»¥ä»ŽäºŒè¿›åˆ¶çš„è§’åº¦è¿›è¡Œè§£æžçš„ã€‚ 12345int rand7() &#123; int x = rand2() * 4 + rand2() * 2 + rand2(); if (x == 7) return rand7(); // restart else return x;&#125; å…¶å®žä¸ç®¡ rand7() æ˜¯ä¸æ˜¯äº§ç”Ÿäº†7ï¼Œå³ä½¿äº§ç”Ÿäº†7 é‚£ä¹ˆä¹Ÿæ˜¯å¯ä»¥é€šè¿‡ â€œif conditionâ€ è¿›è¡Œæå‰è¿›è¡Œå¤„ç†çš„ã€‚ è¿™ç§ç®—æ³•éƒ½ä¸æ˜¯å”¯ä¸€çš„ã€‚]]></content>
      <categories>
        <category>ç®—æ³•</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[æœºå™¨å­¦ä¹ ä¼˜åŒ–æ–¹æ³•]]></title>
    <url>%2F2018%2F05%2F28%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[æœºå™¨å­¦ä¹ çš„ä¼˜åŒ–æ–¹æ³•æœ‰å¾ˆå¤šï¼Œè¿™é‡Œä¸»è¦è§£é‡Šçš„æ˜¯ Gradient Descent å’Œ Newtonâ€™s Method. æ¢¯åº¦ä¸‹é™ï¼ˆgradient descentï¼‰ æ¢¯åº¦ä¸‹é™æ³•æ˜¯æœ€æ—©æœ€ç®€å•ï¼Œä¹Ÿæ˜¯æœ€ä¸ºå¸¸ç”¨çš„æœ€ä¼˜åŒ–æ–¹æ³•ã€‚æ¢¯åº¦ä¸‹é™æ³•å®žçŽ°ç®€å•ï¼Œå½“ç›®æ ‡å‡½æ•°æ˜¯å‡¸å‡½æ•°æ—¶ï¼Œæ¢¯åº¦ä¸‹é™æ³•çš„è§£æ˜¯å…¨å±€è§£ã€‚æŽ¥ä¸‹æ¥è¡ç”Ÿçš„çš„å­ç±»:æ‰¹é‡æ¢¯åº¦ä¸‹é™æ³•ï¼ˆBatch Gradient Descentï¼ŒBGDï¼‰éšæœºæ¢¯åº¦ä¸‹é™ï¼ˆStochastic Gradient Descentï¼ŒSGDï¼‰ ç‰›é¡¿æ³•å’Œæ‹Ÿç‰›é¡¿æ³•ï¼ˆNewtonâ€™s method &amp; Quasi-Newton Methodsï¼‰ ç‰›é¡¿æ³•æœ€å¤§çš„ç‰¹ç‚¹å°±åœ¨äºŽå®ƒçš„æ”¶æ•›é€Ÿåº¦å¾ˆå¿«ã€‚ä¸ªäººæ„Ÿè§‰ç‰›é¡¿æ³•åªæ˜¯åœ¨åœ¨æ¯æ¬¡è¿­ä»£çš„æ—¶å€™è¿›è¡Œäº†ä¸¤æ¬¡è¿ç®—ï¼Œå’Œæ¢¯åº¦ä¸‹é™åœ¨æ€»çš„è¿ç®—æ¬¡æ•°ä¸Šå¹¶æ²¡æœ‰å¾ˆå¤§çš„å·®åˆ«ï¼Œä¸ºä»€ä¹ˆä¼šæ”¶æ•›é€Ÿåº¦æ›´å¿«å‘¢ï¼Ÿ åè¯è§£é‡Š å‡¸ä¼˜åŒ–:å¯¹å‡¸ä¼˜åŒ–çš„é—®é¢˜æˆ‘ä»¬åœ¨åŸºç¡€æ•°å­¦ä¸Šé¢å·²ç»æœ‰äº†å¾ˆå¤šè§£å†³æ–¹æ³•ï¼Œä¾‹å¦‚å¯ä»¥å°†å‡¸ä¼˜åŒ–é—®é¢˜Lagerangeåšå¯¹å¶åŒ–ï¼Œç„¶åŽç”¨Newtonã€æ¢¯åº¦ä¸‹é™ç®—æ³•æ±‚è§£ã€‚å‡¸é›†åˆ: å‡¸å‡½æ•°ï¼šJacobiançŸ©é˜µå’ŒHessiançŸ©é˜µï¼šåœ¨å‘é‡åˆ†æžä¸­, é›…å¯æ¯”çŸ©é˜µæ˜¯ä¸€é˜¶åå¯¼æ•°ä»¥ä¸€å®šæ–¹å¼æŽ’åˆ—æˆçš„çŸ©é˜µ, å…¶è¡Œåˆ—å¼ç§°ä¸ºé›…å¯æ¯”è¡Œåˆ—å¼ã€‚HessiançŸ©é˜µï¼š]]></content>
      <categories>
        <category>æœºå™¨å­¦ä¹ </category>
      </categories>
      <tags>
        <tag>ä¼˜åŒ–</tag>
        <tag>Gradient Descent</tag>
        <tag>Newton&#39;s Method</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ±‚è§£å¹³æ–¹æ ¹]]></title>
    <url>%2F2018%2F05%2F28%2F%E6%B1%82%E8%A7%A3%E5%B9%B3%E6%96%B9%E6%A0%B9%2F</url>
    <content type="text"><![CDATA[æ±‚è§£å¹³æ–¹æ ¹çš„ç®—æ³•ä¸»è¦æœ‰ä¸¤ç§ï¼šäºŒåˆ†æ³•(binary search) å’Œç‰›é¡¿è¿­ä»£æ³•(Newtonâ€™s Method) Just give me codesâ€¦1234567891011121314151617181920212223242526272829from math import sqrt# you needn't import sqrt, I do that just for comparing the results of different methodsclass Solution: # in-built function def my_sqrt(self, n): return sqrt(n) def sqrt_binary(self, n): low, high =0,n mid =int((low+high)/2) while abs(mid*mid- n)&gt; 1e-9: if mid*mid&gt;n: high =mid else: low =mid mid =(low+high)/2 return mid # Newton method: math def newton_method(self, n): k =1 while abs(k*k-n) &gt;1e-9: k =(k+n/k)/2 return k if __name__ =="__main__": print(Solution().my_sqrt(2)) print(Solution().sqrt_binary(2)) print(Solution().newton_method(2)) è¿è¡Œç»“æžœ1231.4142135623730951 #built-in function1.4142135623842478 # binary search1.4142135623746899 # Newton method ä»Žç»“æžœä¸­çœ‹ï¼ŒNewton methodæ¯” binary sqrtæ›´åŠ æŽ¥è¿‘ç³»ç»Ÿè‡ªå¸¦çš„sqrt function. å¹¶ä¸”ä»Žæ•°å­¦ä¸Šå¯ä»¥è¯æ˜Ž Newton method æ¯” binary sqrtéœ€è¦æ›´å°‘çš„è¿­ä»£æ¬¡æ•°ã€‚ é™„å½•ï¼šç‰›é¡¿è¿­ä»£æ³•æ˜¯æ±‚æ–¹ç¨‹æ ¹çš„é‡è¦æ–¹æ³•ä¹‹ä¸€ï¼Œå…¶æœ€å¤§ä¼˜ç‚¹æ˜¯åœ¨æ–¹ç¨‹f(x) = 0çš„å•æ ¹é™„è¿‘å…·æœ‰å¹³æ–¹æ”¶æ•›ï¼Œè€Œä¸”è¯¥æ³•è¿˜å¯ä»¥ç”¨æ¥æ±‚æ–¹ç¨‹çš„é‡æ ¹,å¤æ ¹ã€‚ç‰›é¡¿è¿­ä»£æ³•ç»“è®ºå…¶å®žå°±æ˜¯å–æ³°å‹’çº§æ•°å‰ä¸¤é¡¹ç­‰äºŽ0æ±‚å¾—çš„,æ³°å‹’å…¬å¼è¡¨ç¤ºä¸ºæ›´ç®€ç»ƒçš„å†™æ³•ä¸ºï¼šé€šè¿‡ Newton methondé€¼è¿‘æ–¹ç¨‹çš„è§£çš„è¿‡ç¨‹å¯ä»¥è¡¨ç¤ºä¸ºä¸‹å›¾ï¼š referrences:https://blog.csdn.net/ycf74514/article/details/48996383]]></content>
      <categories>
        <category>ç®—æ³•</category>
      </categories>
      <tags>
        <tag>Newton&#39;s Method</tag>
        <tag>æ±‚è§£å¹³æ–¹æ ¹</tag>
        <tag>binary search</tag>
      </tags>
  </entry>
</search>
