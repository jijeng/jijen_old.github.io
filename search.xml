<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[overfit]]></title>
    <url>%2F2019%2F07%2F21%2Foverfit%2F</url>
    <content type="text"><![CDATA[分析机器学习 和深度学习中出现的过拟合现象，从不同的角度简述常用的处理手段。 什么是过拟合?数据集的大小是导致过拟合的原因，网络足够的复杂（有能力）记住了所有的样本，然后再 train sets 表现要远远好于 test sets。 也可以说网络拟合了噪声数据。 如何处理过拟合模型角度简化模型，通过不断降低模型的复杂度（比如随机森林中的估计量，神经网络中的参数），最终达到一个平衡状态：模型足够简单到不产生过拟合，又足够复杂到能从数据中学习。这样操作时一个比较方便的方法是根据模型的复杂程度查看模型在所有数据集上的误差。 图 1 简化模型的另一个好处在于训练速度更加快. 数据角度 获取更多的数据 数据增强 训练过程角度 提前终止 (early stop) 如图 1 ，当 test error 增加的时候，那么模型就应该停止了。 正则化角度使用L1 or L2 在 loss function (error function) 中添加正则项，对损失函数中的weights 进行限制其变大。 BN (Batch Normalization) 也是一种正则化手段，加上BN 之后，学习率可以有很大的提高。 深度学习中的模型Dropout 或者Dropconnect. 其理念就是在训练中随机让神经元无效（即dropout）或让网络中的连接无效（即dropconnect）。这个有点类似集成学习，提高网络模型的泛化性能，减少过拟合的问题。（类似bagging 的思想，使用不同的网络结构在不同的训练集上进行训练）. Dropout 的具体流程 首先随机（临时）删掉网络中一半的隐藏神经元，输入输出神经元保持不变（图3中虚线为部分临时被删除的神经元） 然后把输入x通过修改后的网络前向传播，然后把得到的损失结果通过修改的网络反向传播。一小批训练样本执行完这个过程后，在没有被删除的神经元上按照随机梯度下降法更新对应的参数（w，b）。 然后继续重复这一过程 恢复被删掉的神经元（此时被删除的神经元保持原样，而没有被删除的神经元已经有所更新） 从隐藏层神经元中随机选择一个一半大小的子集临时删除掉（备份被删除神经元的参数）。 对一小批训练样本，先前向传播然后反向传播损失并根据随机梯度下降法更新参数（w，b） （没有被删除的那一部分参数得到更新，删除的神经元参数保持被删除前的结果）。 使用多种模型Bagging： 最典型的就是 随机森林( Random Forest)， 通过不相关的决策树在不同的数据集上进行训练，最后的每个模型使用相同的权重来“融合”。Boosting: 在简单的网络上不断提升。]]></content>
      <categories>
        <category>Deep learning</category>
      </categories>
      <tags>
        <tag>overfit</tag>
        <tag>dropout</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[markdown 和latex 常见的命令]]></title>
    <url>%2F2019%2F07%2F21%2Fmarkdown-latex%2F</url>
    <content type="text"><![CDATA[主要记录总结 latex 在markdown 中的使用，防丢失。 latex 单独使用和 在markdown 中的使用还是不太一样的，所以记下常用的几个。 多行公式对齐 源码： 1234567$$\begin&#123;split&#125;a &amp;= b \\\\c &amp;= d \\\\e &amp;= f \end&#123;split&#125;\tag&#123;1.3&#125;$$ 效果如下：$$\begin{split}a &amp;= b \\c &amp;= d \\e &amp;= f\end{split}\tag{1.1}$$ 分段函数 源码： 1234$$ BP = \begin&#123;cases&#125;1 &amp; c &gt;r \\\\e ^ &#123; ( 1 - r / c ) &#125; &amp; c &lt;= r\end&#123;cases&#125;$$ 效果如下：$$ BP = \begin{cases}1 &amp; c &gt;r \\e ^ { ( 1 - r / c ) } &amp; c &lt;= r\end{cases}\tag{1.2}$$ markdown 页内跳转 源码123&lt;center&gt; &lt;span id=&apos;jump&apos;&gt; 图 1 &lt;/span&gt;&lt;/center&gt; 如[图 1](#jump) ，当 test error 增加的时候，那么模型就应该停止了。]]></content>
      <categories>
        <category>others</category>
      </categories>
      <tags>
        <tag>markdown</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Interview Questions]]></title>
    <url>%2F2019%2F07%2F21%2Finterview-questions%2F</url>
    <content type="text"><![CDATA[主要总结一下非技术类的面试问题。比如说你喜不喜欢加班之类的、期待的薪资呀… 面试中最重要的是和面试官进行互动，互相了解的过程。如果一方说的过多，那么这个面试仍然是不成功的。在互动的过程中，也是可以学习到很多的。 “你有什么问题想要问我吗？” 不要做的事情如下： 千万不要问小白问题（凡是官网上、度娘上能够获取的信息都是小白问题）。但是你基于获得的知识，进一步提出自己的观点和看法，那么这个就是一个比较好的问题。 不要问对方无法给出你答案的问题。比如不要问 HR部门的具体业务，不要问技术面试官 关于薪酬的问题。 大的前提是根据面试官的身份，围绕着应聘职位进行提问。比如说问HR 公司是否有针对技术方面的一些培训制度，团建之类的，想要了解一下公司的氛围和企业文化 “面试中的自我评价” 主要从以下的三个方面入手： 有自己的优点，比如说自己开朗，具有合作精神，那么最好有事例进行说明 和招聘岗位匹配程度 个人的缺点，当然这个缺点不是致命的，是可以进行转折的那种 “为什么选你，而不是别人” 这本来是面试官的职责，那么既然问了，其实是创新性的帅锅给了候选人，面试官带着“证明你配得上这分工作”的心态去提问，要么是懒，要么是暗示这个岗位很热门。 既然是这种情况，我们能做就是专注于我可以怎么样… 因为别人的信息你也是无法回答的。有以下几个反套路出发点 从工作内容出发如果你已经面了一面、二面，对于自己做的事情有了一个比较清晰的认识，那么你可以说… 这个职位的工作内容，正是我比较熟悉和擅长的。需要使用具体的事实案例进行证明。 真诚反问，创造互动如果不是很清楚岗位的需求，那么上面的方式就不行，这个时候需要创造互动。简单自我介绍一下，然后说“我正想和您探讨一下， 您认为做好这个工作，候选人应该具备怎样的条件？” 如果提到的是自己的优势，那么就使用事例证明一下；如果是自己的弱项，那么简单的说一下，然后说自己是如何提高和改进，展示的好学的一面，最好是有例子证明。 最后如果聊得比较来，自己答题不错的话，那么可以在结束的时候补上一句：“我冒昧问一句，您也面试了一些候选人，您觉得我的机会大吗？” “你最大的缺点是什么“ 招聘本身就是用人之长，弥补自身的缺陷。面试官究竟想要什么？ 候选人在回答这种刁钻问题时，很喜欢避重就轻：“我最大的缺点，就是太较真，对细节要求太高”“我最大的缺点是太拼了，不注意身体”额……这恰恰中了面试官的套路：“这位候选人不够真诚。” 对于候选人来说，我推荐两种更加真诚的回答套路（对，套路也可以真诚）。正确的方式- 讲一个真实存在的缺陷，但强调你已经意识到，并已经在改善了。….. （需要有事例进行说明）这种方式，既能让面试官感受到你的真诚，也能让面试官觉得你是对自己有清醒的认识同时已经开始行动，是一个比较踏实的候选人。 说到底，企业招人是招人之长，如果你的长处是企业急需的，那么你的弱点并非致命，面试官还是很希望得到你的。 ”请简单的介绍一下自己“ 当面试官问出这种问题的时候，我的第一反应就是：这个面试官没有提前阅读我的简历。我希望面试官是阅读了的，然而，我也清楚这几率很低。 所以见证套路的时候到了，简单的自我介绍，我来自… 之前去… 做过…工作，然后在… 看到你们招聘… 岗位，觉得这个是一个不错的机会，所以投了简历。我想简单问您一个问题，可以吗？您这个职位是新设置，还是之前的同事离职呢？ ”要和面试官进行互动，面试官说的越多，那么成功的可能性是越大的，不是一昧的听，而是创造和面试官之间的有机的互动” 好的面试如同老友重逢在Central Perk，互相尊重互相理解，即使做不成同事，没准还可以做朋友。我们平时和小伙伴们谈笑风生，可以友好的提问，也可以开善意的玩笑。但到了面试时，为什么要抱着死板的心态生硬尴聊呢？在这一点上我倒是很赞赏部分互联网公司的高管， 他们抱着开放的态度和候选人聊工作甚至聊人生，也愿意给候选人提供自己的建议。 “你能在公司待多久” 对这个公司发展的前景比较感兴趣，我也希望能给公司带来点什么，能够发挥作用；只要双方都觉得有收获，做的事情有挑战有意义，就都是ok的。 “你希望这个职位的薪水是多少” 这个是一个微妙的问题，在条件允许的情况下，尽可能的拖延给出一个精确的数据来回答这个问题。你可以说，我知道这个工作的薪水大概范围是.. 到…。 或者您能否透露一下公司中对相似职位的工作的薪水大概是什么样子的呢？ 如果面试官继续追问，那么你说“我现在的薪水是… 和其他人一样，我希望能够提升这个数字，但我主要的兴趣还是在工作本身” 要记住新的工作本身并不会使得你赚到更多的钱。 在你面试过程中的最后一个阶段之前，少谈薪水的问题。因为到了那个时候你就清楚，如果公司对你有很大的兴趣，那么这个时候薪水待遇就有很大的余地。 “如何看待加班？”如果特殊时期项目需要上线之类，那么偶尔加班是可以接受的，但是如果强制长期加班，那还是算了。当然，如果自己负责的事情出了问题，即使公司没有要求加班，那么自己也是会主动加班把事情弄好，尽自己最大的努力保证项目不延期。 最好的方式是凭着工作能力说话，让别人觉得能力ok 不加班也是把工作做完做好的。 目前团队工作氛围怎么样？ 加班情况如何？之类的是可以提前问一下的。如果自己急需工作，先拿下工作之后再说，等自己缓和一下再谋出路。]]></content>
      <categories>
        <category>others</category>
      </categories>
      <tags>
        <tag>interview</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[All you need to know about seq2seq]]></title>
    <url>%2F2019%2F07%2F12%2Fseq2seq-theory%2F</url>
    <content type="text"><![CDATA[介绍 sequence to sequence 的定义、两种不同的结构、训练过程、注意力机制和常见的一些问题。主要是理论，代码demo 可以参考这里。 定义和结构 定义 Sequence-to-sequence (seq2seq) 模型，顾名思义，其输入是一个序列，输出也是一个序列. 目前来说，对于Seq2Seq生成模型来说，主要的思路是将该问题作为条件语言模型，在已知输入序列和前序生成序列的条件下,最大化下一目标词的概率，而最终希望得到的是整个输出序列的生成出现的概率最大：$$P ( Y | X ) = \sum _ { t = 1 } ^ { T } \log P \left( y _ { t } | y _ { 1 : t - 1 } , X \right)$$ seq2seq 可以用在很多方面：机器翻译、QA 系统、文档摘要生成、Image Captioning (图片描述生成器)。 Figure 1 介绍了一个标准的seq2seq模型。 其中红色的是encoder RNN, 绿色的是decoder RNN. 他们之间 有一个连线， 也就是encoder states 传给decoder RNN，当做initial state. 这个翻译过程可以解释如下。 两种不同的结构 第一种， seq2seq 模型可简单理解为由三部分组成：Encoder、Decoder 和连接两者的 State Vector (中间状态向量) C 。 第二种结构是最简单的结构，和第一种结构相似，只是 Decoder 的第一个时刻只用到了 Encoder 最后输出的中间状态变量 ： 训练过程 encoder 就是一个语言模型 激活函数: softmax() 1234crossent = tf.nn.spase_softmax_cross_entropy_with_logits( labels=decoder_outputs, logits=logits)train_loss = (tf.reduce_sum(crossent * target_weights) / batch_size) loss 计算: 神经网络的Loss计算，把每个翻译出来的单词和目标单词对应， loss 上面写的很清楚 就是 多分类的cross entropy 函数。 decoder 当你训练你的 NMT 模型时（并且一旦你已经训练了模型），可以在给定之前不可见的源语句的情况下获得翻译。这一过程被称作推理。训练与推理之间有一个明确的区分（测试）：在推理时，我们只访问源语句，即 encoder_inputs。解码的方式有很多种，常见的有greedy 解码和束搜索解码（beam-search）。 我们知道在Seq2Seq模型的最终目的是希望生成的序列发生的概率最大，也就是生成序列的联合概率最大。 实际做的时候有两种算法进行 decoder。 greedy decoding：（贪心算法思维，得到是一种局部最优解） Fig 1 描述的是Inference 的状态， decode 输出来的每一个单词都会当做下一个时刻的输入，来进行翻译。 而在训练过程中， 因为知道翻译出来的单词是什么，就会把这个单词当做输入进行训练。 图中 inference的过程中用到了argmax, 这个函数，也就是每次都选择概率最大的那个单词当做翻译。 这个叫做greedy decoding. 这个不是optimal solution。 贪心搜索只选择了概率最大的一个，而集束搜索 (beam search)则选择了概率最大的前k个。这个k值也叫做集束宽度（Beam Width），算法复杂度是O(nKv), v 是字典的大小， n 是输出序列的长度, k 表示保留的解的个数。 Beam search decoding： 这个方法在每个翻译的步骤 都保存k 个最可能的选择， k 就是beam size, 这个方法虽然不能保证最优解， 但是效率高了很多。 k 一般就是5-10. 上图就是一个 beam size =2 的例子。 在T=0 的时候，选择概率最大的两个。 在T =1 的时候，分别将 the 和 a 输入得到两个概率模型，然后选择概率和最大的两个序列。以此类推，最终得到两个序列。然后前者的概率和大于后者，所以就选择的上面的那个，其他的路径就可以丢掉了。 注意力机制Encoder-Decoder 模型的局限性（有两点） 中间语义向量无法完全表达整个输入序列的信息。 随着输入信息长度的增加，由于向量长度固定，先前编码好的信息会被后来的信息覆盖，丢失很多信息。 Attention 模型的特点是 Decoder 不再将整个输入序列编码为固定长度的中间语义向量 Ｃ ，而是根据当前生成的新单词计算新的 $C_{i}$ ，使得每个时刻输入不同的 Ｃ，这样就解决了单词信息丢失的问题。引入了 Attention 的 Encoder-Decoder 模型如下图： 应用在英文翻译中，将英文输入到 Encoder 中，Decoder 输出中文。在图像标注中，将图像特征输入到 Encoder 中，Decoder 输出一段文字对图像的描述。在 QA 系统中，将提出的问题输入 Encoder 中，Decoder 输出对于问题的回答。 常见问题 Exposure Bias： 问题描述：Seq2Seq模型训练的过程中，编码部分的下一个时刻的输出，是需要根据上一个时刻的输出和上一个时刻的隐藏状态和语义变量Ci.此时上一个时刻的输出使用的是真实的token。而在验证Seq2Seq模型的时候，由于不知道上一个时刻的真实token，上一个时刻的输出使用的是上上个时刻的预测的输出token。这将引发Exposure Bias(曝光偏差问题)。 一种解决思路：使用Beam Search的Encoder的方式也能一定程度上降低Exposure Bias问题，因为其考虑了全局解码概率，而不仅仅依赖与前一个词的输出，所以模型前一个预测错误而带来的误差传递的可能性就降低了 OOV 和低频词 问题描述：OOV表示的是词汇表外的未登录词，低频词则是词汇表中的出现次数较低的词。在Decoder阶段时预测的词来自于词汇表，这就造成了未登录词难以生成，低频词也比较小的概率被预测生成。 一种解决思路：如果 focuse 在文本摘录领域，由于其任务的特点，很多OOV 或者不常见的的词其实可以从输入序列中找到，因此一个很自然的想法就是去预测一个开关（switch）的概率 $P(si=1)=f(hi,yi-1,ci) $，如果开关打开了，就是正常地预测词表；如果开关关上了，就需要去原文中指向一个位置作为输出。 连续生成重复词的问题 判断，然后进行惩戒。]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>seq2seq</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[A Gentle Introduction of GBDT]]></title>
    <url>%2F2019%2F07%2F12%2Fgbdt%2F</url>
    <content type="text"><![CDATA[介绍GBDT 的定义、训练过程、优缺点和常见的问题。 定义： Boosting、bagging和stacking是集成学习的三种主要方法。不同于bagging方法，boosting方法通过分步迭代（stage-wise）的方式来构建模型，在迭代的每一步构建的弱学习器都是为了弥补已有模型的不足，通过弱学习器提升为 强学习器的算法。Boosting族算法的著名代表是AdaBoost 和GBDT。 定义由于GBDT的学习过程是通过多轮迭代，每次都在上一轮训练结果的残差（如果是回归问题，使用平方误差作为loss function）的基础上进行学习（基函数的线性组合），来对数据进行分类或者回归。训练的过程是通过不断降低偏差来提高最后分类器的精度。 理解GBDT要分为两步，第一步是理解什么叫做用决策树去拟合当前模型的残差，第二步是理解为什么以及如何用损失函数的负梯度去替代当前模型的残差。 GDBT 处理分类问题GBDT在解决分类问题时有两种办法，一个是选择指数损失函数作为损失函数，此时GBDT退化为AdaBoost算法，另一个是选择类似于逻辑回归的对数似然损失函数。（逻辑回归使用的是对数似然函数） 当损失函数取平方误差损失函数和指数损失函数时，每一步的优化还算简单，可是如果损失函数是其他一般损失函数时，那可就难了。类似逻辑回归中的求解过程，使用梯度下降的方式简化了优化过程，学者使用梯度上升的方式近似求解提升树的优化过程。GBDT这个算法中最关键的一点就是用损失函数在当前模型中的负梯度值，即： 使用平方损失函数时候，GBDT算法的每一步在生成决策树只是拟合前面模型的残差，（y- y^）残差是梯度的一个特例。而当损失函数是其他的形式时候，下一次迭代是使用的负梯度值。 对于一般损失函数，为了使其取得最小值，通过梯度下降算法，每次朝着损失函数的负梯度方向逐步移动，最终使得损失函数极小的方法（此方法要求损失函数可导）。 总的来说，第一颗树是由基尼系数决定? ，之后所有的树的决策全是由残差来决定。 gbdt 的一个通俗的例子：GBDT的思想可以用一个通俗的例子解释，假如有个人30岁，我们首先用20岁去拟合，发现损失有10岁，这时我们用6岁去拟合剩下的损失，发现差距还有4岁，第三轮我们用3岁拟合剩下的差距，差距就只有一岁了。如果我们的迭代轮数还没有完，可以继续迭代下面，每一轮迭代，拟合的岁数误差都会减小。 训练过程How Gradient Boosting WorksGradient boosting involves three elements: A loss function to be optimized. A weak learner to make predictions. An additive model to add weak learners to minimize the loss function. loss functionThe loss function used depends on the type of problem being solved.It must be differentiable, but many standard loss functions are supported and you can define your own.For example, regression may use a squared error and classification may use logarithmic loss.A benefit of the gradient boosting framework is that a new boosting algorithm does not have to be derived for each loss function that may want to be used, instead, it is a generic enough framework that any differentiable loss function can be used. log loss主要用来衡量二分类，cross entroy主要用来衡量多分类，前者是后者在二分类下的特例交叉熵公式 (cross entroy) $- \sum _ { i = 1 } ^ { K } p _ { i } \log q _ { i }$ 其中 $p_i$ 表示真实的分布， $q_i$ 表示预测分布, $K$ 表示分类数; 当 $K =2$ 交叉熵退化成 log loss: $- [ y \log \hat { y } + (1-y)log(1 -\hat{y}) ]$ cross entroy与logloss主要用来衡量分类算法性能，因为cross entroy意义是衡量真实分布p和预测分布q的分布差异程度；mse主要用来衡量回归算法性能； weak learner弱分类器一般选择 CART （分类回归树） Classification And Regression Tree(CART)是决策树的一种，并且是非常重要的决策树， Decision trees are used as the weak learner in gradient boosting.Specifically regression trees are used that output real values for splits and whose output can be added together, allowing subsequent models outputs to be added and “correct” the residuals in the predictions. It is common to constrain the weak learners in specific ways, such as a maximum number of layers, nodes, splits or leaf nodes.This is to ensure that the learners remain weak, but can still be constructed in a greedy manner. additive modelTrees are added one at a time, and existing trees in the model are not changed.A gradient descent procedure is used to minimize the loss when adding trees. Traditionally, gradient descent is used to minimize a set of parameters, such as the coefficients in a regression equation or weights in a neural network. After calculating error or loss, the weights are updated to minimize that error.Instead of parameters, we have weak learner sub-models or more specifically decision trees. After calculating the loss, to perform the gradient descent procedure, we must add a tree to the model that reduces the loss (i.e. follow the gradient). We do this by parameterizing the tree, then modify the parameters of the tree and move in the right direction by (reducing the residual loss.Generally this approach is called functional gradient descent or gradient descent with functions. 优缺点优点（相对于 LR or SVM）： GBDT基于树模型，继承了树模型的优点 [对异常点鲁棒、不相关的特征干扰性低（LR需要加正则）、可以很好地处理缺失值、受噪音的干扰小]数据规模影响不大，因为我们对弱分类器的要求不高，作为弱分类器的决策树的深 度一般设的比较小，即使是大数据量，也可以方便处理。像 SVM 这种数据规模大的时候训练会比较麻烦。 缺点：通常在给定的不带噪音的问题上，他能达到的最佳分类效果还是不如 SVM，逻辑回归之类的。但是，我们实际面对的问题中，往往有很大的噪音，使得 Decision Tree 这个弱势就不那么明显了。 常见的问题 为什么GBDT要把CART回归树树分成m棵二叉树去求（每棵树只有两个叶子节点），而不是求一棵二叉树，这棵树有m+1（最多有2m个叶子节点）层呢？ 这是为了解决过拟合问题，基学习器要具有简单、高偏差和低方差的特点，因此每棵CART回归树的深度不会很深。 为什么第m次学习的目标，是前m-1棵树预测值的累加和的残差？ 一方面通过分步求解，一步步逼近目标值，比一步到位要简单；另一方面每一步的残差计算其实变相地增大了被分错的实例的权重，因为被分错的实例其残差较大，而已经分对的实例的残差趋近于0。这样后面的树就能越来越专注于前面被分错的实例了。提升树是迭代多棵回归树来共同决策。当采用平方误差损失函数时，每一棵回归树学习的是之前所有树的结论和残差，拟合得到一个当前的残差回归树，残差的意义如公式：残差 = 真实值 - 预测值 。提升树即是整个迭代过程生成的回归树的累加。 AdaBoost 算法和 GBDT 算法的区别 Boosting族算法的著名代表是AdaBoost，AdaBoost算法通过给已有模型预测错误的样本更高的权重，使得先前的学习器做错的训练样本在后续受到更多的关注的方式来弥补已有模型的不足。与AdaBoost算法不同，梯度提升方法在迭代的每一步构建一个能够沿着梯度最陡的方向降低损失（steepest-descent）的学习器来弥补已有模型的不足。经典的AdaBoost算法只能处理采用指数损失函数的二分类学习任务2，而梯度提升方法通过设置不同的可微损失函数可以处理各类学习任务（多分类、回归、Ranking等），应用范围大大扩展。另一方面，AdaBoost算法对异常点（outlier）比较敏感，而梯度提升算法通过引入bagging思想、加入正则项等方法能够有效地抵御训练数据中的噪音，具有更好的健壮性。这也是为什么梯度提升算法（尤其是采用决策树作为弱学习器的GBDT算法）如此流行的原因，有种观点认为GBDT是性能最好的机器学习算法，这当然有点过于激进又固步自封的味道，但通常各类机器学习算法比赛的赢家们都非常青睐GBDT算法，由此可见该算法的实力不可小觑。 adaboost 一般是用于分类，gbt 一般用于回归。 为什么是低方差？gbdt通过多轮迭代,每轮迭代产生一个弱分类器，每个分类器在上一轮分类器的残差基础上进行训练。对弱分类器的要求一般是足够简单，并且是低方差和高偏差的。因为训练的过程是通过降低偏差来不断提高最终分类器的精度，（此处是可以证明的）。通俗的理解，方差比较低就是模型表现的稳定与否。 什么是NP 问题?NP问题是指可以在多项式的时间里验证一个解的问题。NP问题的另一个定义是，可以在多项式的时间里猜出一个解的问题。 Bagging 算法? Bagging算法是这样做的：每个分类器都随机从原样本中做有放回的采样，然后分别在这些采样后的样本上训练分类器，然后再把这些分类器组合起来。简单的多数投票一般就可以。其代表算法是随机森林 AdaBoostAdaBoost，是英文”Adaptive Boosting”（自适应增强）的缩写。它的自适应在于：前一个基本分类器分错的样本会得到加强，加权后的全体样本再次被用来训练下一个基本分类器。同时，在每一轮中加入一个新的弱分类器，直到达到某个预定的足够小的错误率或达到预先指定的最大迭代次数。 具体说来，整个Adaboost 迭代算法就3步： 初始化训练数据的权值分布。如果有N个样本，则每一个训练样本最开始时都被赋予相同的权值：1/N。 训练弱分类器。具体训练过程中，如果某个样本点已经被准确地分类，那么在构造下一个训练集中，它的权值就被降低；相反，如果某个样本点没有被准确地分类，那么它的权值就得到提高。然后，权值更新过的样本集被用于训练下一个分类器，整个训练过程如此迭代地进行下去。 将各个训练得到的弱分类器组合成强分类器。各个弱分类器的训练过程结束后，加大分类误差率小的弱分类器的权重，使其在最终的分类函数中起着较大的决定作用，而降低分类误差率大的弱分类器的权重，使其在最终的分类函数中起着较小的决定作用。换言之，误差率低的弱分类器在最终分类器中占的权重较大，否则较小。 对于决策树，Adaboost分类用了CART分类树，而Adaboost回归用了CART回归树。 AdaBoost 的优点： 作为分类器，分类精度比较高 简单的二元分类器，构造简单，结果可理解 不容易发生过拟合 缺点： 对异常样本敏感，异常样本在迭代过程中可能获得较高的权重，影响最终的强学习器的预测准确率。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>gbdt</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Text Summarization]]></title>
    <url>%2F2019%2F06%2F30%2Ftext-summarization%2F</url>
    <content type="text"><![CDATA[Text Summarization Text summarization is the process of distilling the most important information from a source (or sources) to produce an abridged version for a particular user (or users) and task (or tasks). There are many reasons why Automatic Text Summarization is useful: Summaries reduce reading time. Automatic summarization algorithms are less biased than human summarizers. Personalized summaries are useful in question-answering systems as they provide personalized information. Text summarization methods can be classified into different types. 虽然是可以从不同的角度进行划分，但最常见的分类角度是 based on output type: extractive and abstractive. All extractive summarization algorithms attempt to score the phrases or sentences in a document and return only the most highly informative blocks of text. Abstractive text summarization actually creates new text which doesn’t exist in that form in the document. Abstractive summarization is what you might do when explaining a book you read to your friend, and it is much more difficult for a computer to do than extractive summarization. Computers just aren’t that great at the act of creation. To date, there aren’t any abstractive summarization techniques which work suitably well on long documents. The best performing ones merely create a sentence based upon a single paragraph, or cut the length of a sentence in half while maintaining as much information as possible. Often, grammar suffers horribly. They’re usually based upon neural network models. What is ROUGE?To evaluate the goodness of the generated summary, the common metric in the Text Summarization space is called Rouge score. ROUGE stands for Recall-Oriented Understudy for Gisting Evaluation It works by comparing an automatically produced summary or translation against a set of reference summaries (typically human-produced). It works by matching overlap of n-grams of the generated and reference summary. Extractive Techniques LexRank SummarizerLexRank is an unsupervised approach that gets its inspiration from the same ideas behind Google’s PageRank algorithm. It finds the relative importance of all words in a document and selects the sentences which contain the most of those high-scoring words. NLTK Summarizer Although the technique is basic, we found that it did a good job at creating large summaries. Gensim SummarizerTextRank is based on PageRank algorithm that is used on Google Search Engine. In simple words, it prefers pages which has higher number of pages hitting it. TextRank is a bit more simplistic than LexRank; although both algorithms are very similar, LexRank applies a heuristic post-processing step to remove sentences with highly duplicitous. The gensim algorithm does a good job at creating both long and short summaries. Another cool feature of gensim is that we can get a list of top keywords chosen by the algorithm. This feature can come in handy for other NLP tasks, where we want to use “TextRank” to select words from a document instead of “Bag of Words” or “TF-IDF”. Gensim also has a well-maintained repository and has an active community which is an added asset to using this algorithm. Sentence Embeddings We wanted to evaluate how text summarization works on shorter documents like reviews, emails etc. We used K-means clustering to summarize the types of documents following the aforementioned structure. Then, all of the sentences in a document are clustered in k = sqrt(length of document) clusters. Each cluster of sentence embeddings can be interpreted as a set of semantically similar sentences whose meaning can be expressed by just one candidate sentence in the summary. Candidate sentences corresponding to each cluster are then ordered to form a summary for an email. The order of the candidate sentences in the summary is determined by the positions of the sentences in their corresponding clusters in the original document. Abstraction techniquesDrawbacks of Abstractive summarization Firstly, training the model requires a lot of data and hence time. An inherent problem with abstraction is that the summarizer reproduces factual details incorrectly. For instance, if the article talks about Germany beating Argentina 3–2, the summarizer may replace 3–2 by 2–0 Repetition is another problem faced by the summarizer. As we can see in the second example above, some phrases are repeated in the summary Pointer — Generator Networks Compared to the sequence-to-sequence-with-attention system, the pointer-generator network does a better job at copying words from the source text. Additionally it also is able to copy out-of-vocabulary words allowing the algorithm to handle unseen words even if the corpus has a smaller vocabulary. Hence we can think of pointer generator as a combination approach combining both extraction (pointing) and abstraction (generating). To Summarize..Given the architecture of RNNs and the current computing capabilities, we observed that extractive summarization methods are faster, but equally intuitive as abstractive methods. A few other observations: The network fails to focus on the core of the source text and summarizes a less important, secondary piece of information The attention mechanism, by revealing what the network is “looking at”, shines some precious light into the black box of neural networks, helping us to debug problems like repetition and copying. To make further advances, we need greater insight into what RNNs are learning from text and how that knowledge is represented. Case Study: Text Summarization on EmailsUnsupervised Text Summarization using Sentence Embeddings Step-1: Email CleaningAs salutation and signature lines can vary from email to email and from one language to the other, removing them will require matching against a regular expression. Hi Jane, Thank you for keeping me updated on this issue. I&apos;m happy to hear that the issue got resolved after all and you can now use the app in its full functionality again. Also many thanks for your suggestions. We hope to improve this feature in the future. In case you experience any further problems with the app, please don&apos;t hesitate to contact me again. Best regards, John Doe Customer Support 1600 Amphitheatre Parkway Mountain View, CA United States Step-2: Language Detection As the emails to be summarized can be of any language, the first thing one needs to do is to determine which language an email is in. I used langdetect for my purpose and it supports 55 different languages. Step-3: Sentence TokenizationOnce the languages identification is performed for every email, we can use this information to split each email into its constituent sentences using specific rules for sentence delimiters for each language. Step-4: Skip-Thought EncoderWe need a way to generate fixed length vector representations for each sentence in our emails. Step-5: ClusteringThe number of clusters will be equal to desired number of sentences in the summary. I chose the numbers of sentences in the summary to be equal to the square root of the total number of sentence in the email. Step-6: SummarizationEach cluster of sentence embeddings can be interpreted as a set of semantically similar sentences whose meaning can be expressed by just one candidate sentence in the summary. The candidate sentence is chosen to be the sentence whose vector representation is closest to the cluster center. Candidate sentences corresponding to each cluster are then ordered to form a summary for an email. PageRank 算法和TextRank 介绍 PageRank 算法 PageRank通过互联网中的超链接关系来确定一个网页的排名，其公式是通过一种投票的思想来设计的。整个互联网可以看作是一张有向图图，网页是图中的节点，网页之间的链接就是图中的边。如果网页 A 存在到网页 B 的链接，那么就有一条从网页 A 指向网页 B 的有向边。构造完图后，使用下面的公式来计算网页$ i$的重要性（PR值）： $$S \left( V _ { i } \right) = ( 1 - d ) + d \cdot \sum _ { j \in I n \left( V _ { i } \right) } \frac { 1 } { \left| O u t \left( V _ { j } \right) \right| } S \left( V _ { j } \right)$$ $d$ 是阻尼系数，一般设置为0.85. $\operatorname { In } \left( V _ { i } \right)$ 是指向网页 $i$ 的链接的网页集合。 $\operatorname { Out } \left( V _ { j } \right)$ 是网页 $j$ 中的链接指向的网页的集合。 $\left| O u t \left( V _ { j } \right) \right|$ 是集合中元素的个数。 PageRank 需要多次迭代才能得到最后的结果。 假设我们有4个网页——w1，w2，w3，w4。这些页面包含指向彼此的链接。有些页面可能没有链接，这些页面被称为悬空页面。 Webpage Links w1 [w4, 22] w2 [w3, w1] w3 [] w4 [w1] 在本例中初始化成这样： 最后，这个矩阵中的值将以迭代的方式更新，以获得网页排名。 TextRank 关键词提取 两者的相似之处： In place of web pages, we use sentences Similarity between any two sentences is used as an equivalent to the web page transition probability The similarity scores are stored in a square matrix, similar to the matrix M used for PageRank 不同之处在于后者使用句子之间的相似度作为 weights. $W S \left( V _ { i } \right) = ( 1 - d ) + d \cdot \sum _ { V _ { j } \in \operatorname { In } ( V i ) } \frac { w _ { j i } } { \sum _ { V _ { k } \in O u t \left( V _ { j } \right) } w _ { j k } } W S \left( V _ { j } \right)$ $w_{ij}$ 就是图中节点 $V_i$ 到$V_j$ 的边的权值， 就是两个句子 $S_i$ 和句子 $S_j$ 的相似程度。下面的代码中使用的是 cosine 函数来表示这种相似度。 步骤： The first step would be to concatenate all the text contained in the articles Then split the text into individual sentences In the next step, we will find vector representation (word embeddings) for each and every sentence Similarities between sentence vectors are then calculated and stored in a matrix The similarity matrix is then converted into a graph, with sentences as vertices and similarity scores as edges, for sentence rank calculation Finally, a certain number of top-ranked sentences form the final summary 这里使用的 cosine similarity scores，计算句子之间的相似度，句子embedding 是通过 word embedding 相加而成。1234for i in range(len(sentences)): for j in range(len(sentences)): if i != j: sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))[0,0] Applying PageRank Algorithm123import networkx as nxnx_graph = nx.from_numpy_array(sim_mat)scores = nx.pagerank(nx_graph) TF-IDF 和 TextRank 对比总结： TextRank与TFIDF均严重依赖于分词结果——如果某词在分词时被切分成了两个词，那么在做关键词提取时无法将两个词黏合在一起（TextRank有部分黏合效果，但需要这两个词均为关键词）。因此是否添加标注关键词进自定义词典，将会造成准确率、召回率大相径庭。 TextRank的效果并不优于TFIDF。 TextRank虽然考虑到了词之间的关系，但是仍然倾向于将频繁词作为关键词。 由于TextRank涉及到构建词图及迭代计算，所以提取速度较慢。 自然语言处理有两个切入点，一个是频率一个是语义。上述两种方法本质上还是基于词频的。 Referenceshttps://medium.com/jatana/unsupervised-text-summarization-using-sentence-embeddings-adb15ce83db1 https://towardsdatascience.com/data-scientists-guide-to-summarization-fc0db952e363 https://www.jiqizhixin.com/articles/2018-12-28-18 https://www.analyticsvidhya.com/blog/2018/11/introduction-text-summarization-textrank-python/ https://xiaosheng.me/2017/04/08/article49/]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>text-summarization</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NLP Papers Reading-Machine Translation]]></title>
    <url>%2F2019%2F06%2F29%2Fnlp-papers-reading-machine-translation%2F</url>
    <content type="text"><![CDATA[简单介绍一下 翻译系统的发展过程 然后附上两篇该方面论文的阅读笔记， 即Sequence to Sequence Learning with Neural Network 和 Effective Approaches to Attention-based Neural Machine Translation。 五个主要的发展过程why difficult? Machine translation is challenging given the inherent ambiguity and flexibility of human language. Statistical machine translation replaces classical rule-based systems with models that learn to translate from examples. Neural machine translation models fit a single model rather than a pipeline of fine-tuned models and currently achieve state-of-the-art results. Rule-based Machine Translation Classical machine translation methods often involve rules for converting text in the source language to the target language. The rules are often developed by linguists and may operate at the lexical, syntactic, or semantic level. This focus on rules gives the name to this area of study: Rule-based Machine Translation, or RBMT. The key limitations of the classical machine translation approaches are both the expertise required to develop the rules, and the vast number of rules and exceptions required. Statistical Machine Translation Statistical machine translation, or SMT for short, is the use of statistical models that learn to translate text from a source language to a target language gives a large corpus of examples. Given a sentence T in the target language, we seek the sentence S from which the translator produced T. We know that our chance of error is minimized by choosing that sentence S that is most probable given T. Thus, we wish to choose S so as to maximize $P_r(S|T)$. The approach is data-driven, requiring only a corpus of examples with both source and target language text. This means linguists are not longer required to specify the rules of translation. Although effective, statistical machine translation methods suffered from a narrow focus on the phrases being translated , losing the broader nature of the target text. The hard focus on data-driven approaches also meant that methods may have ignored important syntax distinctions known by linguists. Finally, the statistical approaches required careful tuning of each module in the translation pipeline. Neural Machine Translation The key benefit to the approach is that a single system can be trained directly on source and target text, no longer requiring the pipeline of specialized systems used in statistical machine learning. As such, neural machine translation systems are said to be end-to-end systems as only one model is required for the translation. Encoder-Decoder Model Multilayer Perceptron neural network models can be used for machine translation, although the models are limited by a fixed-length input sequence where the output must be the same length. These early models have been greatly improved upon recently through the use of recurrent neural networks organized into an encoder-decoder architecture that allow for variable length input and output sequences. The key to the encoder-decoder architecture is the ability of the model to encode the source text into an internal fixed-length representation called the context vector. Interestingly, once encoded, different decoding systems could be used, in principle, to translate the context into different languages. The power of this model lies in the fact that it can map sequences of different lengths to each other. Encoder-Decoders with Attention Although effective, the Encoder-Decoder architecture has problems with long sequences of text to be translated. The problem stems from the fixed-length internal representation that must be used to decode each word in the output sequence. The solution is the use of an attention mechanism that allows the model to learn where to place attention on the input sequence as each word of the output sequence is decoded. The encoder-decoder recurrent neural network architecture with attention is currently the state-of-the-art on some benchmark problems for machine translation. And this architecture is used in the heart of the Google Neural Machine Translation system, or GNMT, used in their Google Translate service. Although effective, the neural machine translation systems still suffer some issues, such as scaling to larger vocabularies of words and the slow speed of training the models. There are the current areas of focus for large production neural translation systems, such as the Google system. Attention VS LSTM A limitation of the LSTM architecture is that it encodes the input sequence to a fixed length internal representation. This imposes limits on the length of input sequences that can be reasonably learned and results in worse performance for very long input sequences. After reading this, you will know: This (LSTM) is believed to limit the performance of these networks, especially when considering long input sequences, such as very long sentences in text translation problems. Put another way, each item in the output sequence is conditional on selective items in the input sequence. 而对于 Attention 而言: Each time the proposed model generates a word in a translation, it (soft-) searches for a set of positions in a source sentence where the most relevant information is concentrated. The model then predicts a target word based on the context vectors associated with these source positions and all the previous generated target words.… it encodes the input sentence into a sequence of vectors and chooses a subset of these vectors adaptively while decoding the translation. This frees a neural translation model from having to squash all the information of a source sentence, regardless of its length, into a fixed-length vector. And, This (Attention) increases the computational burden of the model, but results in a more targeted and better-performing model. 同样，除了在 machine translation 中有应用， 在其他领域如image description, CNN 中同样有借鉴的意义。Convolutional neural networks applied to computer vision problems also suffer from similar limitations, where it can be difficult to learn models on very large images. 参考文献 introduction-neural-machine-translationattention-long-short-term-memory-recurrent-neural-networks 两篇machine translation 论文 Sequence to Sequence Learning with Neural Network 研究目的：DNNs需要输入、输出维度是已知和固定的。而语音识别、机器翻译、问答系统等序列到序列问题的序列长度是未知的。DNN有一个明显的缺陷：DNN只能处理输入、输出向量维度是定长的情形。对于输入、输出可变长的情况，使用RNN-Recurrent Neural Network更易求解。 论文贡献之一在于网络结构上：通过学习编码一个可变长度的序列成一个固定长度的向量表示，解码一个给定的固定长度的向量成一个可变长度的序列。实现的时候首先将source sequence通过一个encode LSTM map成一个vector，然后再通过另一个decoder LSTM进行翻译得出output，这也恰恰是image caption里的思想呀（通过CNN将输入图像conv成一个vector或者feature map，然后再输入LSTM），原来大体是这样，接着看。 另外还有一个小的策略：LSTM在长句翻译中的表现也不俗。这归功于对源序列中词序的逆转。虽然LSTM能够基于长期的相关性处理问题，但我们发现在把原句序列逆转的情况下LSMT能学习得更加出色。逆转之后，LSTM测试的复杂度从5.8降至4.7，并且在BLEU上的得分从25.9提升至30.6。 不足之处：其他方面都比较普通，或者说很多论文中都有提到过，比如LSTM可以解决vanishing的问题但没法解决gradient exploding的问题，因此採取gradient crop。模型採用了SGD without momentum。实用的LSTM结构式Grave的《Generating sequence from RNN》中的LSTM结构，等等。 总结：总体来说，这个模型还是採取了贪婪的算法，换句话说，后面的预测对前面的状态有极强的依赖，一旦前面的预测出现问题，后面的预测就不可靠了，这也是一个值得思考和改进的地方。 Effective Approaches to Attention-based Neural Machine Translation 这篇文章的核心在于 attention。 Attention 的作用可以看作是一个对齐模型，传统 SMT 我们用 EM 算法来求解对齐，这里做一个隐式的对齐，将 alignment model 用一个 feedforward neural network 参数化，和其他部分一起训练，神经网络会同时来学习 翻译模型(translation) 和 对齐模型(alignment)。 Attention 可以分成 hard and soft两种模型，简单理解 hard attention 就是从 source sentence 中找到一个能产生单词 $t^{th}$ 对齐的特定单词，把 $s_{t,i}$ 设为1，其他所有单词硬性的认为其概率为0; soft attention 对于source sentence中每个单词都给出一个对齐概率，得到一个概率分布，context vector 就是这些概率分布的一个加权和，整个模型是平滑的且处处可分。 而在该篇论文中提出了一个新的 attention 机制 local attention，在得到 context vector 时，我们不想看所有的 source hidden state，而是每次只看一个 hidden state 的子集(subset)，这样的 attention 其实更集中，也会有更好的结果。Global attention 其实就是 soft attention， local model 实际相当于 hard 和 soft attention 的一个混合或者说折中，主要是用来降低 attention 的花费，简单来说就是每次计算先用预测函数得到 source 相关信息的窗口。 soft or hard attention 还是 global or local attention是从不同的角度进行分类的，前者是在概率分布上，后者是在 context上。 这个是global attention： 这个是 local attention]]></content>
      <categories>
        <category>NLP</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Seq2Seq Translation (English to Chinese)]]></title>
    <url>%2F2019%2F06%2F28%2Fseq2seq-translation-codes%2F</url>
    <content type="text"><![CDATA[基于 tensorflow 的英文文本的预处理和基于 keras 的中英文本预处理。主要是代码，辅助注释。预处理比较详细的步骤： unicode to ascii normalize string tokenization (choose one-hot or not) padding (find proper length of tokenization) 需要选择一个框架 tensorflow or keras 去实现。大量数据建议选择 tensorflow，小模型使用 keras 就行。 热身Text data typically requires some cleanup before it can be embedded in vector space and fed to a machine learning model. Remove tags. For example, “&lt;i>Hello&lt;/i> &lt;b>World&lt;/b>!” is converted to “Hello World!” Remove repeating whitespace characters (spaces, tabs, line breaks). Convert tabs and line breaks to spaces. Remove stopwords. These include the most commonly occurring words in a language, like “the,” “on,” “is,” etc. NLP libraries like gensim provide a default list of stopwords. Convert all text to lowercase. Perform Porter stemming. Porter stemming reduces inflections like “fishing,” “fished,” and “fisher” to the root “fish.” This makes it easier for an ML model to learn how to glean meaning or intent form a sequence of words. 调用 gensim 框架实现预处理： 12345678from gensim.parsing.preprocessing import preprocess_string, strip_tags, strip_multiple_whitespaces, remove_stopwords, stem_textcustom_filters = [strip_tags, strip_multiple_whitespaces, remove_stopwords, stem_text]# 生成器函数def get_tokenized_questions(X): series = pd.Series(pd.concat([X['question1'], X['question2']]),dtype=str) series.dropna() for question in series: yield preprocess_string(question, custom_filters) All by yourself: 将英文 punctuation characters 和字母以空格隔开。 输入： s = &apos;bla. bla? bla.bla! bla...&apos; 输出： bla . bla ? bla . bla ! bla . . . python2 版本，基于库函数 re 实现。 1234567891011s = 'bla. bla? bla.bla! bla...'import re# 这个符号是可以选择的s = re.sub('([.,!?()])', r' \1 ', s) # use a regular expression to match the punctuation characters you are interested and surround them by spaces,s = re.sub('\s&#123;2,&#125;', ' ', s) # use a second step to collapse multiple spaces anywhere in the document:print(s)# replacing everything with space except (a-z, A-Z, ".", "?", "!", ",")w = re.sub(r"[^a-zA-Z?.!,¿]+", " ", w)w = w.rstrip().strip()w = '&lt;start&gt; ' + w + ' &lt;end&gt;' # 这个是可选的，在首尾加上 'start' or 'end' python3 版本，基于 translate实现。 123# 在python3 中可以使用 translate() 这个方法import stringtext = text.translate(str.maketrans(&#123;key: " &#123;0&#125; ".format(key) for key in string.punctuation&#125;)) tensorflow text preprocessing基于 tensorflow 的文本预处理， 适合大量数据，可以使用batch 输入到模型中去。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374import tensorflow as tfimport numpy as npimport unicodedataimport reraw_data = ( ('What a ridiculous concept!', 'Quel concept ridicule !'), ('Your idea is not entirely crazy.', "Votre idée n'est pas complètement folle."), ("A man's worth lies in what he is.", "La valeur d'un homme réside dans ce qu'il est."), ('What he did is very wrong.', "Ce qu'il a fait est très mal."), ("All three of you need to do that.", "Vous avez besoin de faire cela, tous les trois."), ("Are you giving me another chance?", "Me donnez-vous une autre chance ?"), ("Both Tom and Mary work as models.", "Tom et Mary travaillent tous les deux comme mannequins."), ("Can I have a few minutes, please?", "Puis-je avoir quelques minutes, je vous prie ?"), ("Could you close the door, please?", "Pourriez-vous fermer la porte, s'il vous plaît ?"), ("Did you plant pumpkins this year?", "Cette année, avez-vous planté des citrouilles ?"), ("Do you ever study in the library?", "Est-ce que vous étudiez à la bibliothèque des fois ?"), ("Don't be deceived by appearances.", "Ne vous laissez pas abuser par les apparences."), ("Excuse me. Can you speak English?", "Je vous prie de m'excuser ! Savez-vous parler anglais ?"), ("Few people know the true meaning.", "Peu de gens savent ce que cela veut réellement dire."), ("Germany produced many scientists.", "L'Allemagne a produit beaucoup de scientifiques."), ("Guess whose birthday it is today.", "Devine de qui c'est l'anniversaire, aujourd'hui !"), ("He acted like he owned the place.", "Il s'est comporté comme s'il possédait l'endroit."), ("Honesty will pay in the long run.", "L'honnêteté paye à la longue."), ("How do we know this isn't a trap?", "Comment savez-vous qu'il ne s'agit pas d'un piège ?"), ("I can't believe you're giving up.", "Je n'arrive pas à croire que vous abandonniez."),)# convert the unicode file to ascii, 主要是统一编码方式，然后去除 重音符号def unicode_to_ascii(s): return ''.join( c for c in unicodedata.normalize('NFD', s) # UCD是Unicode字符数据库（Unicode Character DataBase）的缩写。 if unicodedata.category(c) != 'Mn') # 去除 重音符号# 类似于热身中的功能def normalize_string(s): s = unicode_to_ascii(s) s = re.sub(r'([!.?])', r' \1', s) # 如果是这三个符号，那么是需要前面加上一个空格 s = re.sub(r'[^a-zA-Z.!?]+', r' ', s) # 除去不是这些符号的字符 s = re.sub(r'\s+', r' ', s) # 出现多个空格，就去除直到1个 return sraw_data_en, raw_data_fr = list(zip(*raw_data)) # 变量名称前加 *，表示传入的是一个元组，两个星号表示是一个dictionary# 从运行的结果看，由原来的 tuple of tuple 变成了两个string of tuple，并没有list 什么事情raw_data_en, raw_data_fr = list(raw_data_en), list(raw_data_fr) # from tuple to list 这个是转换了raw_data_en = [normalize_string(data) for data in raw_data_en]# 这个是decoder的输入，decoder 是有两个输入的，一个是encoder的输出，一个是 其中一个start destination sentence， 最后是一个 end# 是用来计算 loss的raw_data_fr_in = ['&lt;start&gt; ' + normalize_string(data) for data in raw_data_fr]raw_data_fr_out = [normalize_string(data) + ' &lt;end&gt;' for data in raw_data_fr] # 这种操作比较简洁哈en_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')# 默认是会把 ? . or ! 去掉, 因为我们不想让其 filter 掉 上述三个字符，所有自己进行了处理en_tokenizer.fit_on_texts(raw_data_en)data_en = en_tokenizer.texts_to_sequences(raw_data_en)# 这个padding 是为了之后创建 tf.data.Dataset object 使用的，所以还是比较nice的data_en = tf.keras.preprocessing.sequence.pad_sequences(data_en, padding='post')fr_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')# 都是先使用 fit_on_texts() 然后才使用 texts_to_sequence() ，前者相当于训练，后者是输出的结果，我的理解# A mid-way notice though, we can call fit_on_texts multiple times on different corpora and it will update vocabulary automatically.fr_tokenizer.fit_on_texts(raw_data_fr_in)fr_tokenizer.fit_on_texts(raw_data_fr_out)data_fr_in = fr_tokenizer.texts_to_sequences(raw_data_fr_in)data_fr_in = tf.keras.preprocessing.sequence.pad_sequences(data_fr_in, padding='post')data_fr_out = fr_tokenizer.texts_to_sequences(raw_data_fr_out)data_fr_out = tf.keras.preprocessing.sequence.pad_sequences(data_fr_out, padding='post') in addition： 1234567891011121314# 真正在做实验的时候需要注意的事情：# 一种常见的手段就是 limit the size of the dataset to experiment faster (optimal)# 使用tensorflow 中的dataset 的时候，有意识的 shuffle() 数据集 并且使用batch 的思想dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)# 这个和上面的语句是搭配使用的example_input_batch, example_target_batch = next(iter(dataset))example_input_batch.shape, example_target_batch.shape# 就是在真正的实验的过程中， 网络中的shape (batch_size, embedding_size, )# 超级常用的处理的手段， preprocess_sentence() 是一个函数, apply(lambda )data["eng"] = data.eng.apply(lambda w: preprocess_sentence(w))data["es"] = data.es.apply(lambda w: preprocess_sentence(w)) keras text preprocessing这个版本的代码适用于小的数据量，因为当数据量达到百万的时候，应该使用batch 去训练模型，不应一下子读入到内存中，容易爆内存。比较有特点的 filter 中文的字符使用translate 进行处理。一般从经验上讲是不建议 filter 掉 “？。，” 这三个中文字符的，其他的可以filter 掉，对应英文中的 “? , .” 这三个字符。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788import stringimport refrom numpy import array, argmax, random, takeimport pandas as pdfrom keras.models import Sequentialfrom keras.layers import Dense, LSTM, Embedding, Bidirectional, RepeatVector, TimeDistributedfrom keras.preprocessing.text import Tokenizerfrom keras.callbacks import ModelCheckpointfrom keras.preprocessing.sequence import pad_sequencesfrom keras.models import load_modelfrom keras import optimizersimport matplotlib.pyplot as pltfrom sklearn.model_selection import train_test_splitimport numpy as npdef read_text(filename): # open the file file = open(filename, mode='rt', encoding='utf-8') # read all text text = file.read() file.close() return textdef to_lines(text): sents = text.strip().split('\n') sents = [i.split('\t') for i in sents] return sentsdef to_array(path, debug): data =read_text(path) import gc eng_ch =to_lines(data) del data gc.collect() #import ipdb #ipdb.set_trace() eng_ch =np.asarray(eng_ch) # max memory #eng_ch =np.asarray(eng_ch[:5000000]) if debug: #eng_ch =eng_ch[:4, :]# just for chinese dict test eng_ch =eng_ch[:2000, :] return eng_chdef pre_process(eng_ch): import jieba cn_punctuation = "！？｡ ？。? ＃＄％＆ !（）. ＊＋－／：；＜＝＞＠［＼］＾＿｀｛｜｝～｟｠｢｣､、〃》「」『』【】〔〕〖〗〘〙〚〛〜〝〞〟〰〾 〿–—‘ ’ ‛ “ ” „ ‟ …‧﹏" eng_ch[:, 0] = [s.translate(str.maketrans('', '', string.punctuation)) for s in eng_ch[:, 0]] eng_ch[:, 1] = [s.translate(str.maketrans('', '', cn_punctuation)) for s in eng_ch[:, 1]] for i in range(len(eng_ch)): eng_ch[i, 0] =eng_ch[i, 0].lower() for i in range(len(eng_ch)): seg_list =jieba.cut(eng_ch[i, 1]) eng_ch[i, 1] =' '.join(seg_list) return eng_chdef sentence_length(eng_ch): eng_l =[] ch_l =[] # 这里需要看一下 english的数据是否前后有 空格 for i in eng_ch[:, 0]: eng_l.append(len(i.split())) for i in eng_ch[:,1]: ch_l.append(len(i)) length_df =pd.DataFrame(&#123;'eng': eng_l, 'ch':ch_l&#125;) length_df.hist(bins =50) plt.savefig('data-dist-cn.png')def tokenization(lines): tokenizer = Tokenizer() tokenizer.fit_on_texts(lines) return tokenizerdef encode_sequences(tokenizer, length, lines): seq =tokenizer.texts_to_sequences(lines) seq = pad_sequences(seq, maxlen=length, padding='post') return seq]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>seq2seq</tag>
        <tag>translation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Similarity Measures]]></title>
    <url>%2F2019%2F06%2F21%2Fsimilarity-measures%2F</url>
    <content type="text"><![CDATA[The similarity measure is the measure of how much alike two data objects are. Similarity measure in a data mining context is a distance with dimensions representing features of the objects. If this distance is small, it will be the high degree of similarity where large distance will be the low degree of similarity. The similarity is subjective and is highly dependent on the domain and application. For example, two fruits are similar because of color or size or taste. Care should be taken when calculating distance across dimensions/features that are unrelated. The relative values of each element must be normalized, or one feature could end up dominating the distance calculation. Similarity are measured in the range 0 to 1 [0,1]. Euclidean distanceEuclidean distance is the most common use of distance. In most cases when people said about distance, they will refer to Euclidean distance. Euclidean distance is also known as simply distance. When data is dense or continuous, this is the best proximity measure. Applications: Where data is continuous or numerical . Also knows as L2 Norm famously. In an n dimensional space between two vectors x and y the formula is simply the square root of the sum of the square distance: $$d \left( \left[ x _ { 1 } , x _ { 2 } , \ldots , x _ { n } \right] , \left[ y _ { 1 } , y _ { 2 } , \ldots , y _ { n } \right] \right) = \sqrt { \sum _ { i = 1 } ^ { n } \left( x _ { i } - y _ { i } \right) ^ { 2 } }$$ 123456from math import * def euclidean_distance(x,y): return sqrt(sum(pow(a-b,2) for a, b in zip(x, y))) print euclidean_distance([0,3,4,5],[7,6,3,-1]) Manhattan distanceThis Manhattan distance metric is also known as Manhattan length, rectilinear distance, L1 distance or L1 norm, city block distance, Minkowski’s L1 distance, taxi-cab metric, or city block distance. Applications: can be used if the dimensions are continuous or numeric. This distance measure is very similar to Euclidean but it is a sum of the absolute difference of every dimension rather than the sum of squares. $$d = \sum _ { i = 1 } ^ { n } \left| x _ { i } - y _ { i } \right|$$ 123456from math import *def manhattan_distance(x,y): return sum(abs(a-b) for a,b in zip(x,y)) print manhattan_distance([10,20,10],[10,20,20]) Minkowski distanceMinkowski distance is the generalized distance metric. 当p =1 和2 时，恰好是 Euclidean distance 和Manhattan distance. $$\left( \sum _ { i = 1 } ^ { n } \left| x _ { i } - y _ { i } \right| ^ { p } \right) ^ { 1 / p }$$ 1234567891011121314 from math import *from decimal import Decimal def nth_root(value, n_root): root_value = 1/float(n_root) return round (Decimal(value) ** Decimal(root_value),3) def minkowski_distance(x,y,p_value): return nth_root(sum(pow(abs(a-b),p_value) for a,b in zip(x, y)),p_value) print minkowski_distance([0,3,4,5],[7,6,3,-1],3) Cosine similarityCosine similarity is particularly used in positive space, where the outcome is neatly bounded in [0,1]. One of the reasons for the popularity of cosine similarity is that it is very efficient to evaluate, especially for sparse vectors. Applications: Used in identifying document similarity, product recommendations, Information retrieval and works efficiently with high-dimensional sparse data. It is an angle between two data points in the vector space. Given a vector A and B, the cosine distance is the dot product of x and y divided by Euclidean distance. $$\text { similarity } = \cos ( \theta ) = \frac { \mathbf { A } \cdot \mathbf { B } } { | \mathbf { A } | | \mathbf { B } | } = \frac { \sum _ { i = 1 } ^ { n } A _ { i } B _ { i } } { \sqrt { \sum _ { i = 1 } ^ { n } A _ { i } ^ { 2 } } \sqrt { \sum _ { i = 1 } ^ { n } B _ { i } ^ { 2 } } }$$ 12345678910111213from math import * def square_rooted(x): return round(sqrt(sum([a*a for a in x])),3) # round(num, ndigits) 在python2 中是四舍五入 def cosine_similarity(x,y): numerator = sum(a*b for a,b in zip(x,y)) denominator = square_rooted(x)*square_rooted(y) return round(numerator/float(denominator),3) print cosine_similarity([3, 45, 7, 2], [2, 54, 13, 15]) Cosine similarity vs Euclidean distance首先从公式上说， cosine similarity 考虑的是角度 (angle)而不是 magnitude，可以排除文章长度的干扰。 Mathematically, it measures the cosine of the angle between two vectors projected in a multi-dimensional space. The cosine similarity is advantageous because even if the two similar documents are far apart by the Euclidean distance (due to the size of the document). 如果涉及到（使用 tf or tf-idf）生成了向量表示，使用 cosine similarity 比 euclidean 更好。 如果使用 word2vec 生成的向量，那么 euclidean 是不是更好的选择。Cosine is mostly used on very sparse, discrete domains such as text. Here, most dimensions are 0 and do not matter at all. 如果你想要 magnitude，那么使用 ED。Euclidean is commonly used on dense, continuous variables. There every dimension matters。 这里有一个重要的观点， Cosine is essentially the same as Euclidean on normalized data. 在很高的维度，两者都是不行的，这就是 Curse of Dimensionality. Jaccard similarityJaccard Distance measures how close two sets are. It is simply a ratio of the intersection of the sets to the Union. Can be used when the datatypes are categorical . Example: Products purchased/viewed by customers. Typically used in Product recommendation, Clustering customers based on purchase/engagement patterns. Please note Jaccard distance is a dissimilarity metric and Jaccard coefficient, J(A,B) is a similarity metric. $$d _ { J } ( A , B ) = 1 - J ( A , B ) = \frac { | A \cup B | - | A \cap B | } { | A \cup B | }$$ 123456789from math import * def jaccard_similarity(x,y): intersection_cardinality = len(set.intersection(*[set(x), set(y)])) # 参数前一个 *表示传入的是一个元祖 tuple union_cardinality = len(set.union(*[set(x), set(y)])) return intersection_cardinality/float(union_cardinality) print jaccard_similarity([0,1,2,5,6],[0,2,3,5,7,9]) Edit Distance Edit distance is used when the comparing strings. Ideal use cases would be auto spell check, meta data correction etc. The distance between two strings are smaller if the number of corrections ( insertions or deletions ) needed to perfectly match are smaller. 常见的dp 解法。 123456789101112131415161718192021222324class Solution(object): def minDistance(self, word1, word2): """ :type word1: str :type word2: str :rtype: int """ m = len(word1) n = len(word2) table = [[0] * (n + 1) for _ in range(m + 1)] for i in range(m + 1): table[i][0] = i for j in range(n + 1): table[0][j] = j for i in range(1, m + 1): for j in range(1, n + 1): if word1[i - 1] == word2[j - 1]: table[i][j] = table[i - 1][j - 1] else: table[i][j] = 1 + min(table[i - 1][j], table[i][j - 1], table[i - 1][j - 1]) return table[-1][-1]]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>similarity-measures</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ResNet Understanding]]></title>
    <url>%2F2019%2F06%2F21%2Fresnet-understanding%2F</url>
    <content type="text"><![CDATA[本文介绍 ResNet 的背景，想要解决的问题，基本思路和框架 和对于其的一种解读方式。 背景网络的深度是容易出现梯度爆炸和梯度消失，造成网络的不收敛。一些方法已经在很大程度上可以缓解这个问题，比如使用 ReLU激活函数、 良好的权值初始化方法 、还有 intermediate normalization layers(即网络中间的batch normalization)。并且对于网络过程中的过拟合问题，也提出了一些办法如，使用 regularization、权值衰减和dropout方法。 但解决了深度网络收敛问题之后，又出现了另外一个问题。 残差网络要解决的问题一般来说在没有过拟合的情况下，可以逐步增加网络的深度。但在实验中发现了这样的问题。网络退化： 网络越深，训练误差越大。（accuracy开始饱和，原文中这样说的）这种退化并不是由于过拟合造成的，并且在适当深度模型中增加更多的层会导致更多的训练误差 基本思路和结构作者基于增加层如果为恒等映射那么更深层网络不应该比浅层网络产生更高错误率的思想 如图所示左边的是传统的plain networks的结构，右边的是修改为ResNet的结构。改变前目标： 训练 F(x) 逼近 H(x)改变后目标：训练 F(x) 逼近 H(x) -x 即增加一个identity mapping（恒等映射），将原始所需要学的函数H(x)转换成F(x)+x，而作者认为这两种表达的效果相同，但是优化的难度却并不相同，作者假设F(x)的优化 会比H(x)简单的多。这一想法也是源于图像处理中的残差向量编码，通过一个reformulation，将一个问题分解成多个尺度直接的残差问题，能够很好的起到优化训练的效果 在论文中尝试了 skip 2层或者 3层 一种解读方式 残差网络单元其中可以分解成右图的形式，从图中可以看出，残差网络其实是由多种路径组合的一个网络，直白了说，残差网络其实是很多并行子网络的组合，整个残差网络其实相当于一个多人投票系统（Ensembling） 从这可以看出其实ResNet是由大多数中度网络和一小部分浅度网络和深度网络组成的， 说明虽然表面上ResNet网络很深，但是其实起实际作用的网络层数并没有很深我们可以看出大多数的梯度其实都集中在中间的路径上，论文里称为effective path。 (网络越深，也是容易出现梯度消失或者梯度爆炸，这个是没有问题的)ResNet其实就是一个多人投票系统。 ps, 现在深度网络基本上分成两个方向，一个像 resnet 向着”深度“发展，一个向着”宽度“的inception network]]></content>
      <categories>
        <category>Deep learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[LeetCode-Others]]></title>
    <url>%2F2019%2F06%2F16%2Fleetcode-others%2F</url>
    <content type="text"><![CDATA[LeetCode 刷题总结（四），使用Python 实现。该篇题目类型主要包含无法归类到上述三篇的题目。 Reverse Integer Given a 32-bit signed integer, reverse digits of an integer. Input: 120 Output: 21 Input: 123 Output: 321 https://leetcode.com/problems/reverse-integer/ Tips： 这个只是reverse 操作，注意一些细节比如负号和数字 0的处理。对于最大数的表示 pow(2, 31)。 通过 % 求余获得最小位。 12345678910111213141516class Solution(object): def reverse(self, x): """ :type x: int :rtype: int """ result = 0 if x &lt; 0: symbol = -1 x = -x else: symbol = 1 while x: result = result * 10 + x % 10 x /= 10 return 0 if result &gt; pow(2, 31) else result * symbol String to Integer (atoi) Implement atoi which converts a string to an integer. Tips: 涉及到bit 级别数字处理的一般都会用到 res =res 10 + something 这样的东西。对于能够表示的数字的判断 max(-pow(2, 31), min(ressign, pow(2, 31) -1)) 这个还是挺经典的代码的。 https://leetcode.com/problems/string-to-integer-atoi/ 1234567891011121314151617181920212223class Solution(object): def myAtoi(self, str): """ :type str: str :rtype: int """ ls =list(str.strip()) if len(ls) == 0: return 0 sign = -1 if ls[0] == '-' else 1 index=0 # 有一个index 是贯穿始终的 res =0 if ls[index] in ['-', '+']: index +=1 for i in range(index, len(ls)): if ls[i].isdigit(): res =res *10 + ord(ls[i]) -ord('0') else: # case "words and 987" 是不能有 字母的 break return max(-pow(2, 31), min(sign*res, pow(2,31) -1)) Palindrome Number Determine whether an integer is a palindrome. An integer is a palindrome when it reads the same backward as forward. Tips： 回文数。代码写的很巧妙，整体上说是通过 / 和 % 获得数字的头和尾，在实现的时候有若干细节。 1234567891011121314151617181920class Solution(object): def isPalindrome(self, x): """ :type x: int :rtype: bool """ if x &lt; 0: return False ranger = 1 while x / ranger &gt;= 10: ranger *= 10 while x: left = x / ranger right = x % 10 if left != right: return False x = (x % ranger) / 10 ranger /= 100 return True Integer to Roman Roman numerals are represented by seven different symbols: I, V, X, L, C, D and M. Tips： 基于roman 数字规则的转换。代码实现角度 tuple都是要好于 list （内存和速度方面都是），如果你想要存储的是静态的可以遍历的数据，不需要每次进行修改的话，why not 123456789101112131415161718class Solution(object): def intToRoman(self, num): if num &lt;= 0: return "" digits = &#123; "I":1, "V":5, "X":10, "L":50, "C":100, "D":500, "M":1000 &#125; nums = (1000, 900, 500, 400, 100, 90, 50, 40, 10, 9, 5, 4, 1) chs = ("M", "CM", "D", "CD", "C", "XC", "L", "XL", "X", "IX", "V", "IV", "I") len = 13 s = "" while num &gt; 0: for i in range(0, len): if num &gt;= nums[i]: num -= nums[i] s += chs[i] break return s Roman to Integer Roman numerals are represented by seven different symbols: I, V, X, L, C, D and M. Tips： 和上一道题目相似。 1234567891011121314151617181920class Solution(object): def romanToInt(self, s): """ 这个是输入时 string，所以是 index 遍历的在 dict 中进行访问，但是上一个题目是总的 number，是没有办法的 """ digits = &#123; "I":1, "V":5, "X":10, "L":50, "C":100, "D":500, "M":1000 &#125; len_s = len(s) num = 0 # 这个少遍历了一个 ，因为其中有 i+1 的存在 for i in range(0, len_s - 1): cur = digits[s[i]] next_s = digits[s[i + 1]] if cur &gt;= next_s: num += cur else: num -= cur # 处理的是最后一个 num += digits[s[len_s - 1]] return num Letter Combinations of a Phone Number Given a string containing digits from 2-9 inclusive, return all possible letter combinations that the number could represent.A mapping of digit to letters (just like on the telephone buttons) is given below. Note that 1 does not map to any letters. Input: &quot;23&quot; Output: [&quot;ad&quot;, &quot;ae&quot;, &quot;af&quot;, &quot;bd&quot;, &quot;be&quot;, &quot;bf&quot;, &quot;cd&quot;, &quot;ce&quot;, &quot;cf&quot;]. Tips：使用循环的方式表示层级的关系（使用二层循环，第一层表示外围a， 第二层表示 def 等）。 1234567891011121314151617181920212223242526272829303132333435363738class Solution: def letterCombinations(self, digits): """ :type digits: str :rtype: List[str] """ if not digits or digits == "": return [] # 命名很到位 maps =&#123; '1': (), '0': (), '2': ('a', 'b', 'c'), '3': ('d', 'e', 'f'), '4': ('g', 'h', 'i'), '5': ('j', 'k', 'l'), '6': ('m', 'n', 'o'), '7': ('p', 'q', 'r', 's'), '8': ('t', 'u', 'v'), '9': ('w', 'x', 'y', 'z') &#125; results = [""] for digit in digits: tuple1 = maps[digit] tmp =[] if len(tuple1) == 0: continue # 二重循环 for prefix in results: for suffix in tuple1: tmp.append(prefix + suffix) results = tmp return results Divide Two Integers Given two integers dividend and divisor, divide two integers without using multiplication, division and mod operator.Return the quotient after dividing dividend by divisor.The integer division should truncate toward zero. Tips： 不让使用乘除和 mod 操作，只能使用位运算符了。在python 中0 == False 这个在逻辑判断中是等价的（1 ==True）。计算的时候使用 &lt;&lt; （不断的*2）, ,maybe 是二分 12345678910111213141516171819202122class Solution(object): def divide(self, divident, divisor): sign =-1 if divident* divisor&lt;0 else 1 divident, divisor =abs(divident), abs(divisor) ans =0 while divisor &lt;= divident: div =divisor tmp =1 while (div &lt;&lt;1) &lt;= divident: div &lt;&lt;= 1 tmp &lt;&lt;= 1 divident -= div ans += tmp return max(-pow(2, 31), min(ans*sign, pow(2, 31) -1)) Pow(x, n) Implement pow(x, n), which calculates x raised to the power n (xn) Tips： 简单的递归。 1234567891011121314151617181920class Solution(object): # 递归写起来比较好些，但是有时候比较难理解这个运行的过程 def myPow(self, x,n): """ :type x: float :type n: int :rtype: float """ if n ==0: return 1 # 求解pow() 都是正数，如果n &lt;0,那么需要做的是 取导数 elif n &lt;0: return 1.0/self.myPow(x, -n) else: half =self.myPow(x, n&gt;&gt;1) if n%2 ==0: return half *half else: return x *half*half N-Queens The n-queens puzzle is the problem of placing n queens on an n×n chessboard such that no two queens attack each other. Given an integer n, return all distinct solutions to the n-queens puzzle.Each solution contains a distinct board configuration of the n-queens’ placement, where ‘Q’ and ‘.’ both indicate a queen and an empty space respectively. Input: 4Output: [ [“.Q..”, // Solution 1 “…Q”, “Q…”, “..Q.”], [“..Q.”, // Solution 2 “Q…”, “…Q”, “.Q..”]]Explanation: There exist two distinct solutions to the 4-queens puzzle as shown above. Tips： N-皇后，行列对角线是不能出现重复。中规中矩的递归解法，board 是使用的一维向量， 这样去理解 比如board=[1, 3, 0, 2]，这是4皇后问题的一个解，意思是：在第0行，皇后放在第1列；在第1行，皇后放在第3列。 check函数表示 第k 个 皇后是否能够放在第j 个位置。 123456789101112131415161718192021222324252627class Solution(object): def __init__(self): self.board =[] def check(self, k,j): for i in range(k): # 如果之前的皇后已经放到了这个位置，或者两者在一条直线上，这个abs 用的比较牛逼 if self.board[i] ==j or abs(k -i) ==abs(self.board[i] -j): return False return True def dfs(self, depth, valuelist, n, res): if depth ==n: res.append(valuelist) return for i in range(n): if self.check(depth, i): self.board[depth] =i s ='.'*n self.dfs(depth +1, valuelist+[s[:i] +'Q'+s[i+1:]], n,res) def solveNQueens(self, n): self.board =[-1 for i in range(n)] res =[] self.dfs(0, [], n, res) return res N-Queens II The n-queens puzzle is the problem of placing n queens on an n×n chessboard such that no two queens attack each other. Tips: 这个相对于上一个要简单一些，因为最后的结果要的是 counts 而不是 list of path。按照道理讲是不用记录path 的。 123456789101112131415161718192021222324252627282930313233class Solution(object): def __init__(self): self.board =[] self.count =0 def check(self, k,j): """ check if the kth queen can be put in column j :param k: :param j: :return: """ for i in range(k): if self.board[i] ==j or abs(k -i) ==abs(self.board[i] -j): return False return True def dfs(self, depth, valuelist, n, res): if depth ==n: #res.append(valuelist) self.count +=1 return for i in range(n): if self.check(depth, i): self.board[depth] =i s ='.'*n self.dfs(depth +1, valuelist+[s[:i] +'Q'+s[i+1:]], n,res) def totalNQueens(self, n): self.board =[-1 for i in range(n)] res =[] self.dfs(0, [], n, res) return self.count Add Binary Given two binary strings, return their sum (also a binary string).The input strings are both non-empty and contains only characters 1 or 0. Tips：二级制相加，从后往前走，使用 carry 位置记录进位数。 12345678910111213141516171819202122class Solution(object): def addBinary(self, a, b): """ :type a: str :type b: str :rtype: str """ i, j, carry, res =len(a) -1, len(b) -1, 0, '' while i&gt;=0 or j &gt;=0 or carry: if i &gt;=0: carry += int(a[i]) i -=1 if j&gt;=0: carry += int(b[j]) j -=1 res =str(carry%2) +res carry //=2 return res Max Points on a Line Given n points on a 2D plane, find the maximum number of points that lie on the same straight line. Tips：当使用除法的时候，因为精度问题造成的误差，所以这里使用最大的公约数进行化简。使用以下三种方式处理。 Map from (a,b,c,d) representing y=(a/b)x+(c/d) to set of indices of points that are on that line. a/b and c/d are reduced, i.e. a and b are divided by their GCD and so are c and d. Vertical lines are represented by a tuple with 1 element, the x-axis value Single points are represented by a 2-tuple (x, y). 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647import collectionsimport mathdef fraction(x, y): if x &lt; 0: x, y = -x, -y gcd = math.gcd(x, y) return x // gcd, y // gcd class Solution: def maxPoints(self, points): if not points: return 0 if len(points) == 1: return 1 aligned_points = collections.defaultdict(set) duplicates = collections.defaultdict(set) for i, p in enumerate(points): for j, q in enumerate(points[i + 1:], start=i + 1): # x 是否相同 if q[0] == p[0]: if q[1] == p[1]: duplicates[i].add(j) key = tuple(p) else: key = (q[0],) else: a, b = fraction(q[1] - p[1], q[0] - p[0]) # k斜率 c, d = fraction(p[1] * q[0] - q[1] * p[0], q[0] - p[0]) # b 位移 key = (a, b, c, d) #aligned_points[key] = aligned_points[key] or &#123;i, j&#125; # 因为之前定义的是set aligned_points[key] |= &#123;i, j&#125; for p, dups in duplicates.items(): for key in aligned_points: if p in aligned_points[key]: #aligned_points[key] = aligned_points[key] or dups # 这个or 不是选择的意思，是两者都要的意思 aligned_points[key] |= dups max_points = 0 for aliged in aligned_points.values(): max_points = max(max_points, len(aliged)) return max_points Happy Number Write an algorithm to determine if a number is “happy”.A happy number is a number defined by the following process: Starting with any positive integer, replace the number by the sum of the squares of its digits, and repeat the process until the number equals 1 (where it will stay), or it loops endlessly in a cycle which does not include 1. Those numbers for which this process ends in 1 are happy numbers. Input: 19Output: trueExplanation:12 + 92 = 8282 + 22 = 6862 + 82 = 10012 + 02 + 02 = 1 Tips: 使用set 的思路，没有重复之前就一直遍历；对于数字转成 string 逐个进行处理。 1234567891011121314class Solution(object): def isHappy(self, n): """ :type n: int :rtype: bool """ visited =set() # 这个set 每次add 都是add 到最前面一个，是规律吧 while n not in visited: visited.add(n) n =sum((int(x) **2 for x in str(n)))# () 要比 [] 使用较少的内存 return n ==1 Sum of Two Integers Calculate the sum of two integers a and b, but you are not allowed to use the operator + and -. Input: a = 1, b = 2Output: 3 Tips: 下面的add()函数只是在以下场景中 work: a*b&gt;=0 , or the negative number has a larger absolute value( a &lt; 0 and abs(a) &gt; b &gt; 0 , or b &lt; 0 and abs(b) &gt; a &gt; 0) 123456789101112131415161718192021class Solution(object): def getSum(self, a, b): """ :type a: int :type b: int :rtype: int """ def add(a, b): if not a or not b: return a or b # # ^ get different bits and &amp; gets double 1s, &lt;&lt; moves carry ， 这个可能是加法吧， return add(a^b, (a&amp;b) &lt;&lt; 1) if a*b &lt; 0: # assume a &lt; 0, b &gt; 0 if a &gt; 0: return self.getSum(b, a) if -a == b: return 0 if -a &lt; b: return -add(-a, -b) return add(a, b) Fizz Buzz * Write a program that outputs the string representation of numbers from 1 to n. n = 15,Return:[ “1”, “2”, “Fizz”, “4”, “Buzz”, “Fizz”, “7”, “8”, “Fizz”, “Buzz”, “11”, “Fizz”, “13”, “14”, “FizzBuzz”] Tips: 这是一个 for 循环就可以解决的问题 12345678910111213141516171819class Solution(object): def fizzBuzz(self, n): """ :type n: int :rtype: List[str] """ # 使用一个 result append一下 result = [] for i in xrange(1, n + 1): if i % 3 != 0 and i % 5 != 0: result.append(str(i)) elif i % 3 == 0 and i % 5 != 0: result.append("Fizz") elif i % 3 != 0 and i % 5 == 0: result.append("Buzz") else: result.append("FizzBuzz") return result Find Median from Data Stream Median is the middle value in an ordered integer list. If the size of the list is even, there is no middle value. So the median is the mean of the two middle value. For example,[2,3,4], the median is 3[2,3], the median is (2 + 3) / 2 = 2.5 Tips: 主要难点在于数据流，难在数据结构，使用最小根堆实现。这个是需要寻找 median （中位数），使用大小根堆，分别存储较小的一半 和 较大的一半。那么大根堆的堆顶就对应着较小一半的最大值，小根堆对应着较大部分的最小值。所以中位数就可以 快速的从两个堆顶元素中获得。 https://leetcode.com/problems/find-median-from-data-stream/ 简单说一下在python 中heapq -小根堆的实现。将数值转成负数，那么就可以使用小根堆来mimic 大根堆，因为堆顶是负数最小的。（对应正数最大的） heap.heappush(heap, item), 把一个item 添加到heap中 heap.heappushpop(heap, item), 先把item 放入到堆中，然后再pop() , 这样比 heappush() 然后再heappop() 快一些 push item on the heap, then pop and return the smallest item from the heap. The combined action runs more efficiently than heappush followed by a separate call to heappop() Note that the heapq in python is a min heap, thus we need to invert the values in the smaller half to mimic a &apos;max heap&apos; the add operation is O(nlogn), the find operation is O(1) 123456789101112131415161718from heapq import *class MedianFinder(object): def __init__(self): self.small =[] # max heap (the smaller half of the list), 大根堆存放的是小值，然后根存放的就是最大值, 这个转成-num 当然就是 smaller part self.large =[] # min heap (the larger half of the list), 小根堆存放的是大值，然后根就是存放的最小值 def addNum(self, num): if len(self.small) ==len(self.large): heappush(self.large, -heappushpop(self.small, -num)) else: heappush(self.small, -heappushpop(self.large, num)) def findMedian(self): if len(self.small) ==len(self.large): return float(self.large[0] -self.small[0])/2.0 else: return float(self.large[0]) Flatten Nested List Iterator Given a nested list of integers, implement an iterator to flatten it.Each element is either an integer, or a list – whose elements may also be integers or other lists. Tips: 迭代器和 generator 感觉都是差不多的操作，产生一个数字。这个看懂 API 更重要。只要数字内容，不要嵌套的关系。yield 关键字，调用完之后，程序不结束。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950# """# This is the interface that allows for creating nested lists.# You should not implement it, or speculate about its implementation# """#class NestedInteger(object):# def isInteger(self):# """# @return True if this NestedInteger holds a single integer, rather than a nested list.# :rtype bool# """## def getInteger(self):# """# @return the single integer that this NestedInteger holds, if it holds a single integer# Return None if this NestedInteger holds a nested list# :rtype int# """## def getList(self):# """# @return the nested list that this NestedInteger holds, if it holds a nested list# Return None if this NestedInteger holds a single integer# :rtype List[NestedInteger]# """class NestedIterator(object): def __init__(self, nestedList): def gen(nestedList): for x in nestedList: if x.isInteger(): yield x.getInteger() else: for y in gen(x.getList()): yield y self.gen = gen(nestedList) def next(self): return self.value def hasNext(self): try: self.value = next(self.gen) return True except StopIteration: return False# Your NestedIterator object will be instantiated and called as such:# i, v = NestedIterator(nestedList), []# while i.hasNext(): v.append(i.next()) Insert Delete GetRandom O(1) Design a data structure that supports all following operations in average O(1) time. insert(val): Inserts an item val to the set if not already present. remove(val): Removes an item val from the set if present. getRandom: Returns a random element from current set of elements. Each element must have the same probability of being returned. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546class RandomizedSet(object): # 就是一个维持list 的东西 def __init__(self): """ Initialize your data structure here. """ self.list =[] def insert(self, val): """ Inserts a value to the set. Returns true if the set did not already contain the specified element. :type val: int :rtype: bool """ if val not in self.list: self.list.append(val) return True return False def remove(self, val): """ Removes a value from the set. Returns true if the set contained the specified element. :type val: int :rtype: bool """ if val in self.list: self.list.remove(val) return True return False def getRandom(self): """ Get a random element from the set. :rtype: int """ length =len(self.list) import random index =random.randint(0,length-1) return self.list[index] # Your RandomizedSet object will be instantiated and called as such:# obj = RandomizedSet()# param_1 = obj.insert(val)# param_2 = obj.remove(val)# param_3 = obj.getRandom() Perfect Squares Given a positive integer n, find the least number of perfect square numbers (for example, 1, 4, 9, 16, …) which sum to n. Tips: 再次理解一下二重循环，如果第二层中的遍历次数和 第一层是有关系的，往往是 O(N*N/2) 的复杂度（这种记法是错误的），用于遍历前 i 个元素。 ···pythonclass Solution(object): # dp[i] 的定义表示 i 这个数字最少使用的 squares 数量 def numSquares(self, n): &quot;&quot;&quot; :type n: int :rtype: int &quot;&quot;&quot; dp = [float(&apos;inf&apos;)]*(n+1) dp[0]=0 count = 2 for i in range(1,n+1): if i&gt;=count*count: count += 1 for j in range(1,count): dp[i] = min(dp[i],dp[i-j*j]+1) return dp[n] &quot;&quot;&quot; # 这样做是不可行的，在于可能重复使用一个，如果没有最大的合适的话 square =[pow(num, 2) for num in range(1, int(math.sqrt(n) +1))] square =square[::-1] count =0 for sq in square: if n-sq &gt;0: n -= sq count +=1 return count &quot;&quot;&quot; 123456789101112131415161718192021222324** Factorial Trailing Zeroes**&gt; Given an integer n, return the number of trailing zeroes in n!.Tips: 数学问题···pythonclass Solution(object): # 这里的 += n/div 已经就表示了 5的个数， 这样是可以加快运算的 def trailingZeroes(self, n): &quot;&quot;&quot; :type n: int :rtype: int &quot;&quot;&quot; div =5 res =0 while div &lt;=n: res += n/div div =div *5 return res Count Primes Count the number of prime numbers less than a non-negative number, n. Tips: 主要是质数倍数的优化，否则是time out 123456789101112131415161718192021class Solution(object): # prime 质数， 1 既不是质数也不是 合数 # 当前数为质数时，排除掉剩下的数中该数的整倍数。遍历过所有的数之后剩下的数全是质数。提升效率的方法是减少遍历的长度。 # 还有一个优化点，可以不必从2~m-1，只需遍历2 ~ √m.因为如果m能被2 ~ m-1之间任一整数整除，其二个因子必定有一个小于或等于√m，另一个大于或等于√m。例如16能被2,4,8整除 # 质数定义为在大于1的自然数中，除了1和它本身以外不再有其他因数。 # 最后 return 的是 counter，个数 而不是具体的数字 def countPrimes(self, n): """ :type n: int :rtype: int """ if n &lt;= 2: return 0 prime = [True] * n prime[:2] = [False, False] for base in range(2, int((n ) ** 0.5) + 1): # 时间上是 [2, sqrt(m)] ，但是在python 中实现是这样的 if prime[base]: prime[base ** 2::base] = [False] * len(prime[base ** 2::base]) return sum(prime) Missing Number Given an array containing n distinct numbers taken from 0, 1, 2, …, n, find the one that is missing from the array. Tips: 使用公式 index 和前 n数的问题。 1234567891011class Solution(object): # 如果没有限制内存，那么是可以使用dict，然后根据index 和value 进行判断的 # 凡是和对应的index 发生关系，那么这个就有优化的可能，就变得比较有意思 def missingNumber(self, nums): """ :type nums: List[int] :rtype: int """ n =len(nums) return n*(n+1)/2 -sum(nums) Power of Three Given an integer, write a function to determine if it is a power of three. Tips: 求余 and /= 相结合的常见手法 12345678910111213141516171819class Solution(object): # 这个是在考察除法的运算过程 def isPowerOfThree(self, n): """ :type n: int :rtype: bool """ if n &lt;=0: return False if n ==1: return True while n &gt;1: if n %3 !=0: return False n /= 3 return True Implement Trie (Prefix Tree) Implement a trie with insert, search, and startsWith methods. Tips: 这个数据结构很有用，字典树。 123456789101112131415161718192021222324252627282930313233343536373839404142434445class TrieNode: # Initialize your data structure here. # https://www.cnblogs.com/beiyeqingteng/p/5625540.html，这里面有个图 # 还是挺形象的 def __init__(self): # 这个是什么数据结构呀 self.children = collections.defaultdict(TrieNode) # 这种设置还是理解不够深刻 self.is_word = False# 考察的是字典树，这种数据结构# 保存字母的话，是 26叉树，保存数字的话10 叉树class Trie(object): def __init__(self): self.root =TrieNode() def insert(self, word): current =self.root for letter in word: current =current.children[letter] current.is_word =True def search(self, word): current =self.root for letter in word: current =current.children.get(letter) if not current: return False return current.is_word def startsWith(self, prefix): current =self.root for letter in prefix: current =current.children.get(letter) # dictionary 操作，得到的是值 if not current: return False return True# Your Trie object will be instantiated and called as such:# obj = Trie()# obj.insert(word)# param_2 = obj.search(word)# param_3 = obj.startsWith(prefix)]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode-Recursion]]></title>
    <url>%2F2019%2F06%2F16%2Fleetcode-recursion%2F</url>
    <content type="text"><![CDATA[LeetCode 刷题总结（三）， 使用Python 实现。该篇题目类型主要包括：recursion, iteration 和dynamic programming。 Regular Expression Matching Given an input string (s) and a pattern (p), implement regular expression matching with support for ‘.’ and ‘ * ‘ .‘.’ Matches any single character.‘*‘ Matches zero or more of the preceding element. Tips：典型的dp，二维数组是常见的方式。 如果看不懂注释，可以看这里 1234567891011121314151617181920212223242526272829303132333435class Solution(object): """ dp, dp[i][j] means the match status between p[:i] and s[:j] """ def isMatch(self, s, p): dp =[[False]*(len(s)+1) for _ in range(len(p) +1)] dp[0][0]= True # case, of when s is an empty string but p is not, # since each * can eliminate character before it for i in range(2, len(p)+1): dp[i][0] =dp[i-2][0] and p[i-1] =="*" for i in range(1, len(p)+1): for j in range(1, len(s)+1): if p[i-1] =='*': # elimination or propagations dp[i][j] =dp[i-2][j] or dp[i-1][j] # another case, propagations if p[i-2] ==s[j-1] or p[i-2] =='.': # 下面两种写法都是可以 # dp[i][j] = dp[i][j] or dp[i][j-1] dp[i][j] |= dp[i][j-1] else: # 对于and 这个语句就类似 if 语句, 下面两种写法都是可以的 #dp[i][j] =dp[i-1][j-1] and (p[i-1] ==s[j-1] or p[i-1] =='.') if p[i-1] ==s[j-1] or p[i-1] =='.': dp[i][j] =dp[i-1][j-1] return dp[-1][-1] Wildcard Matching Given an input string (s) and a pattern (p), implement wildcard pattern matching with support for ‘?’ and ‘*‘.‘?’ Matches any single character.‘*‘ Matches any sequence of characters (including the empty sequence). Input:s = “aa”p = “a”Output: falseExplanation: “a” does not match the entire string “aa”. Tips: Wildcard 通配符，这个和上一个基本相同啊， 1234567891011121314151617181920212223242526272829303132333435363738394041424344class Solution(object): """ 这个更加简单，while 就是能够搞定的，然后对于 特殊符号特殊判断。使用两个指针进行操作. s[i] ==p[j] 和 p[j] =='?' ，这个是可以放到同一个 if 条件下的。两者是等价的。 关键是 * 的匹配 2，在 p 中出现 * 时，记录 p 中 * 的位置，同时记录此时 s 的位置。 3，从 * 的后面的第一个字符开始匹配。如果匹配失败，返回 s 处，从 s++ 开始重新匹配。 """ def isMatch(self, s, p): """ :type s: str :type p: str :rtype: bool """ j = i = ss = 0; star = -1 # 首先把 string 中的字符比完 while i &lt; len(s): if j &lt; len(p) and (s[i] == p[j] or p[j] == '?'): i += 1; j += 1 continue # star 记录的是 j 的位置，相应的 ss 是记录的i (string) 中的位置 if j &lt; len(p) and p[j] == '*': star = j; j += 1; ss = i; continue # 如果已经有了 star 的出现， 到这里已经说明 star的下一个和 string 中的位置元素不是exact 的匹配 # 所以这里进行了 ss +=1 的操作是为了，相当于把 string 中的char 使用 * 进行了代替 # 好好理解一下 if star != -1: j = star + 1; ss += 1; i = ss continue return False # string 已经比较完了，如果只剩下 * 那么是可以行的，否则是不可行的 while j &lt; len(p) and p[j] == '*': j += 1 if j == len(p): return True return False Valid Parentheses Given a string containing just the characters ‘(‘, ‘)’, ‘{‘, ‘}’, ‘[‘ and ‘]’, determine if the input string is valid. Tips: 必须要使用 len(stack) 进行检测，因为中间的时候也可能 len(stack) 是等于0的，这时候只能是 append() ，不能访问 stack[-1] 123456789101112131415161718192021class Solution(object): def isMatch(self, l, r): return l =='[' and r==']' or l =='(' and r ==')' or l =='&#123;' and r =='&#125;' def isValid(self, s): len_s =len(s) if len_s ==0: return True stack =[] for ch in s: if len(stack) ==0 or not self.isMatch(stack[-1], ch): stack.append(ch) else: stack.pop() return len(stack) ==0 Generate Parentheses Given n pairs of parentheses, write a function to generate all combinations of well-formed parentheses. For example, given n = 3, a solution set is: [ “((()))”, “(()())”, “(())()”, “()(())”, “()()()”] Tips: dfs, left_count 表示是 ‘(‘ 的总数， left_remain 表示 left- right 的差值. 12345678910111213141516171819202122232425class Solution(object): def DogenerateParenthesis(self, n, left_count, left_remain, prefix): if n ==left_count and left_remain ==0: return [prefix] left =[] right =[] if left_count &lt;n: left =self.DogenerateParenthesis(n, left_count+1, left_remain+1, prefix+'(') if left_remain&gt;0: right =self.DogenerateParenthesis(n, left_count, left_remain-1, prefix+')') return left +right def generateParenthesis(self, n): """ :type n: int :rtype: List[str] """ if n ==0: return [] else: list =self.DogenerateParenthesis(n ,0, 0, "") return list Combination Sum Given a set of candidate numbers (candidates) (without duplicates) and a target number (target), find all unique combinations in candidates where the candidate numbers sums to target.The same repeated number may be chosen from candidates unlimited number of times. Tips: 这种找到所有符合题目要求的解，十之八九都是要使用递归。最优解（最值）一般是使用dp，减少子问题的运算。这里给出了 list 和dfs 的结合使用，通过传入 start index来解决是否遍历过的问题。 示意图： 12345678910111213141516171819202122class Solution(object): def dfs(self, candidates, target, start, intermedia, res): # target 这个变量调节了是 继续deeper or return， 每一次都是在变化的。如果 ==0，那么就return 了 if target ==0: res.append(intermedia) return for i in range(start, len(candidates)): if target &lt; candidates[i]: return self.dfs(candidates, target-candidates[i], i, intermedia+[candidates[i]], res) def combinationSum(self, candidates, target): """ :type candidates: List[int] :type target: int :rtype: List[List[int]] """ candidates.sort() res =[] self.dfs(candidates, target, 0, [], res) return res Combination Sum II Given a collection of candidate numbers (candidates) and a target number (target), find all unique combinations in candidates where the candidate numbers sums to target.Each number in candidates may only be used once in the combination. Tips： 这里给出了另外一种遍历list 和dfs 的方法，传入的是部分 list，上面那道题传入了完整的list。 123456789101112131415161718192021222324252627class Solution(object): # @param &#123;integer[]&#125; candidates # @param &#123;integer&#125; target # @return &#123;integer[][]&#125; def combinationSum2(self, candidates, target): candidates.sort() # 排序不影响 时间复杂度的，因为时间复杂度大于排序的时间复杂度 #res=set() res =[] self.findcombination(candidates,target,[],res) #return [list(i) for i in res] return res def findcombination(self,candidates,target,ls,res): if target==0 and ls not in res: # 对于 set() 中使用 add() ，list 中使用 append() #res.add(tuple(ls)) res.append(ls) return # 下面这个判断用和不用 都是相同的效果(时间和空间复杂度上) if target&lt;0: return # not use: c72 ms,11.7M for i in range(len(candidates)): if target&lt;candidates[i]: return self.findcombination(candidates[i+1:],target-candidates[i],ls+[candidates[i]],res) Permutations Given a collection of distinct integers, return all possible permutations. Input: [1,2,3]Output:[ [1,2,3], [1,3,2], [2,1,3], [2,3,1], [3,1,2], [3,2,1]] Tips : extend 是因为list of list ，而不是单独的list，这样能保证最后的结果还是 list of list 123456789101112131415161718192021class Solution(object): def permute(self, nums): return self.doPermute(nums) def doPermute(self, num_list): if len(num_list) ==1: return [num_list] res_list =[] for i in range(len(num_list)): num_list[0], num_list[i] =num_list[i], num_list[0] sub_list =self.doPermute(num_list[1:]) list_head =[num_list[0]] #new_list =list_head+ sub_list new_list = [list_head + list1 for list1 in sub_list] # 可以理解这个是 sub_list 是有一系列的解， 然后再每个解上都加上一个头元素 res_list.extend(new_list) # extend，The list.extend method extends a list by appending elements from an iterable # append 是当做一个整体进行操作 return res_list Permutations II Given a collection of numbers that might contain duplicates, return all possible unique permutations. Tips： 这个 duplicates 是通过 sort 函数，然后在选择 某个index 时候，进行判断一下是否和第一个重合，这样的方式去handle。 12345678910111213141516171819202122232425262728class Solution(object): def doPermuteUnique(self, nums): if len(nums) ==1: return [nums] res_list =[] for i in range(len(nums)): if i&gt;0 and nums[0] ==nums[i]: continue nums[0], nums[i] =nums[i], nums[0] sub_list =self.doPermuteUnique(nums[1:]) list_head =[nums[0]] new_list =[list_head +list1 for list1 in sub_list] res_list.extend(new_list) return res_list def permuteUnique(self, nums): """ :type nums: List[int] :rtype: List[List[int]] """ nums.sort() return self.doPermuteUnique(nums) Climbing Stairs You are climbing a stair case. It takes n steps to reach to the top.Each time you can either climb 1 or 2 steps. In how many distinct ways can you climb to the top?Note: Given n will be a positive integer. Tips： 数学题，斐波那契数列。 解法一： 12345678910111213141516171819202122class Solution(object): # 可以换成数学模型，发现就是 斐波那契数列 # 不使用数字，使用三个变量也是可以的额 def climbStairs(self, n): """ :type n: int :rtype: int """ if n ==1: return 1 elif n ==2: return 2 arr = [0] *(n+1) arr[1] =1 arr[2] =2 for i in range(3, n+1): arr[i] =arr[i-1] +arr[i-2] return arr[n] 解法二123456789101112def climbStairs(self, n): if n ==1: return 1 elif n ==2: return 2 a, b =1,2 c =0 for i in range(3, n+1): c = a+b a, b =b,c return c Combinations Given two integers n and k, return all possible combinations of k numbers out of 1 … n. Tips： 这个是处理的list 和 dfs()的问题，然后使用的传入 index和完整的 list 来控制进度。 12345678910111213141516171819202122class Solution(object): """ 好好理解递归这种逐渐加深的层次 """ def combine(self, n, k): res =[] self.dfs(list(range(1, n+1)), k, 0, [], res) return res def dfs(self, nums, k, index, path, res): # backtracking #if k &lt;0: #return # 这种 return 和result 结合使用的操作是经常常见的 if k ==0: res.append(path) return # 这个index 是很重要的， 在这个index 的基础上选择的 for i in range(index, len(nums)): self.dfs(nums, k-1, i +1, path+ [nums[i]], res) Subsets Given a set of distinct integers, nums, return all possible subsets (the power set). Tips: 使用的是第二种方式，传入部分list，从而由大问题转移成小问题。 12345678910111213141516class Solution(object): # 这种是最简单的深度优先的搜索了， def subsets(self, nums): res =[] self.dfs(nums, [], res) return res def dfs(self, nums, path, res): # 一般来说这个是有跳出条件，回溯的，但是这种情况是没有的，只有最后一个 # [[],[1],[1,2],[1,2,3],[1,3],[2],[2,3],[3]]， 当输出 [1, 2,3] 的时候，return，但是这个return 到了 [1, 3] 这个层次 res.append(path) for i in range(len(nums)): self.dfs(nums[i+1:], path+[nums[i]], res) Subsets II Given a collection of integers that might contain duplicates, nums, return all possible subsets (the power set). Tips： 这个含有duplicates，使用功能sort 然后在 for 循环 的时候进行判断一下。 123456789101112131415161718192021222324class Solution(object): # 递归 def subsetsWithDup(self, nums): """ :type nums: List[int] :rtype: List[List[int]] """ res =[] nums.sort() self.dfs(nums, 0, [], res) return res def dfs(self, nums, index, path, res): if path not in res: res.append(path) #res.append(path) for i in range(index, len(nums)): if i &gt; index and nums[i] ==nums[i-1]: continue self.dfs(nums, i+1, path+[nums[i]], res) # 下面的代码是错误的 memory 但是不知道为什么 Decode Ways A message containing letters from A-Z is being encoded to numbers using the following mapping: &apos;A&apos; -&gt; 1 &apos;B&apos; -&gt; 2 ... &apos;Z&apos; -&gt; 26 Given a non-empty string containing only digits, determine the total number of ways to decode it. Tips： 多少种解码方式。本质是裴波拉契数列, 感觉自己并没有get 到这个本质上是 该数列 1234567891011121314151617181920212223242526class Solution(object): """ DP[i] = DP[i-1] + DP[i-2] \ \___________(if str[i-2] exists and 10&lt;= int(str[i-1] + str[i]))&lt;=26 ) \___________(If str[i-1] exists and str[i] != '0' ) """ def numDecodings(self, s): """ :type s: str :rtype: int """ if not s: return 0 if s =='10': return 1 dp =[0] *(len(s) +1) dp[0] =1 for i in range(1, len(s)+1): if s[i-1] !='0': dp[i] +=dp[i-1] if i &gt;1 and '10' &lt;=s[i-2:i] &lt;='26': dp[i] += dp[i-2] return dp[-1] Binary Tree Inorder Traversal Given a binary tree, return the inorder traversal of its nodes’ values. Tips： 递归。 1234567891011121314151617# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution(object): # inorder 中序遍历, recursive 递归， iterative 迭代 # 这个是递归的 recursively def helper(self, root, res): if root: self.helper(root.left, res) res.append(root.val) self.helper(root.right, res) Tips： 递归的容易写，循环的也好会，使用的stack 先保存左子树，然后不断的node 其右子树。 12345678910111213141516def inorderTraversal(self, root): # 如果使用 迭代，那么就是 stack结构了 res, stack =[], [] while True: while root: stack.append(root) root =root.left if not stack: return res node =stack.pop() res.append(node.val) root =node.right Validate Binary Search Tree Given a binary tree, determine if it is a valid binary search tree (BST).Assume a BST is defined as follows: The left subtree of a node contains only nodes with keys less than the node’s key. The right subtree of a node contains only nodes with keys greater than the node’s key. Both the left and right subtrees must also be binary search trees. Tips： 二叉搜索树的特点，中序遍历，先得到遍历结果，然后判断是否是不减的（只是需要O(N)）. 1234567891011121314151617181920212223242526272829303132# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution(object): def isValidBST(self, root): """ :type root: TreeNode :rtype: bool """ output =[] self.inOrder(root, output) for i in range(1, len(output)): if output[i-1] &gt;= output[i]: return False return True def inOrder(self, root, output): if not root: return self.inOrder(root.left, output) output.append(root.val) self.inOrder(root.right, output) Same Tree Given two binary trees, write a function to check if they are the same or not.Two binary trees are considered the same if they are structurally identical and the nodes have the same value. Tips: 对应的值相同，对应的结构相同。 1234567891011121314151617181920212223242526# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution(object): # 根据上一一个题目的要求，这个是也是可以先进行遍历，然后再比较最后的遍历结果吗 def isSameTree(self, p, q): """ :type p: TreeNode :type q: TreeNode :rtype: bool """ if not p and not q: return True elif not p or not q: return False if p.val ==q.val: return self.isSameTree(p.left, q.left) and self.isSameTree(p.right, q.right) else: return False Symmetric Tree Given a binary tree, check whether it is a mirror of itself (ie, symmetric around its center). Tip： 对称和 same 是在于比较的方式是不一样的。 1234567891011121314151617181920212223242526272829303132# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution(object): # 该题目和 isSameTree 是有点相似的，只是修改部分代码就可以 def isSymmetric(self, root): """ :type root: TreeNode :rtype: bool """ # 这个如果是 [] 或者 None， 是返回true， 因为输入的形式是 list ，所以判断条件是 if root ==[], 这样形式 if not root: return True return self.dfs(root.left, root.right) def dfs(self, p, q): if not p and not q: return True elif not p or not q: return False if p.val == q.val: return self.dfs(p.left, q.right) and self.dfs(p.right, q.left) else: return False Binary Tree Zigzag Level Order Traversal Given a binary tree, return the zigzag level order traversal of its nodes’ values. (ie, from left to right, then right to left for the next level and alternate between). For example:Given binary tree [3,9,20,null,null,15,7], 3 / \ 9 20 / \ 15 7结果是这样的：[ [3], [20,9], [15,7]] Tips： 一行是从左往右，一行是从右往左。层序遍历的变体。从左向右使用 append() ，然后从右向左使用 insert()，这个是没有问题的。 12345678910111213141516171819202122232425262728# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution(object): # 层次遍历 + 奇偶性来决定是否 reverse def zigzagLevelOrder(self, root): """ :type root: TreeNode :rtype: List[List[int]] """ res = [] self.dfs(root, 0, res) return res def dfs(self, root, level, res): if root: if len(res) &lt; level + 1: res.append([]) if level % 2 == 0: res[level].append(root.val) else: res[level].insert(0, root.val) self.dfs(root.left, level+1, res) self.dfs(root.right, level+1, res) Maximum Depth of Binary Tree Given a binary tree, find its maximum depth.The maximum depth is the number of nodes along the longest path from the root node down to the farthest leaf node.Note: A leaf is a node with no children. Tips: 左右子树的max+1，这个是树的深度。 12345678910111213141516171819# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution(object): # 递归， 最简单的方式 def maxDepth(self, root): """ :type root: TreeNode :rtype: int """ if not root: return 0 # 这个是最简单的代码了 return 1 +max(self.maxDepth(root.left), self.maxDepth(root.right)) Binary Tree Level Order Traversal II Given a binary tree, return the bottom-up level order traversal of its nodes’ values. (ie, from left to right, level by level from leaf to root). Tips: 层序遍历，但是 res 需要存储成list of list，这样最后进行reverse，能够表示出 层数的信息。 1234567891011121314151617181920212223242526272829# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution(object): # 需要有一个 level的index， 然后翻转就行了 def levelOrderBottom(self, root): """ :type root: TreeNode :rtype: List[List[int]] """ res =[] self.dfs(root, 0, res) #res 这个是整体的导致，一层 element的倒置，不涉及 element内部的倒置 return res[::-1] def dfs(self, root, level, res): if root: if len(res) &lt; level+1: res.append([]) # 这个是append 一个空的 [] 这种结构，然后下面使用该 list；否则的话 直接进行append res[level].append(root.val) # 这个很重要哦 self.dfs(root.left, level+1, res) self.dfs(root.right, level +1, res) Convert Sorted Array to Binary Search Tree Given an array where elements are sorted in ascending order, convert it to a height balanced BST.For this problem, a height-balanced binary tree is defined as a binary tree in which the depth of the two subtrees of every node never differ by more than 1. Tips： 不减的array 就是 binary search tree 中的中序遍历的结果。递归思想，先要找到 root，然后划分左右子树。递归进行。 1234567891011121314151617181920212223242526# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution(object): # balance tree 这种是递归进行定义的，左右子树相差最多为1 # 主要是不太清楚 如何保证这种 balanced tree def sortedArrayToBST(self, nums): """ :type nums: List[int] :rtype: TreeNode """ if not nums: return None mid =len(nums)//2 root =TreeNode(nums[mid]) root.left =self.sortedArrayToBST(nums[:mid]) root.right =self.sortedArrayToBST(nums[mid+1:]) return root Balanced Binary Tree Given a binary tree, determine if it is height-balanced.For this problem, a height-balanced binary tree is defined as:a binary tree in which the depth of the two subtrees of every node never differ by more than 1. Tip：左右子树的差值不能大于1 为balanced，这个盘别题目，比较容易，重点是 getHeight() 的实现、根节点是 balanced，左右子树也是balanced 1234567891011121314151617181920212223242526# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution(object): # 使用了之前的 求解 树的height 的东西，然后使用定义进行做题 def isBalanced(self, root): """ :type root: TreeNode :rtype: bool """ if not root: return True return abs(self.getHeight(root.left) -self.getHeight(root.right))&lt;2 and self.isBalanced(root.left) and self.isBalanced(root.right) def getHeight(self, root): if not root: return 0 return 1 +max(self.getHeight(root.left), self.getHeight(root.right)) Minimum Depth of Binary Tree Given a binary tree, find its minimum depth.The minimum depth is the number of nodes along the shortest path from the root node down to the nearest leaf node.Note: A leaf is a node with no children. Tip：求 height的变形，如果是叶子节点，那么返回 1+max(left, right) 否则的话，返回左右子树中较小的高度。如果是求高度，那么就不管了什么情况下都是返回 max()+1 1234567891011121314151617181920212223# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution(object): # 求解根节点的左右子树的最小高度 # 如果左右子树都有，那么就是调用该函数， 如果有一个又，那么直接求解高度就行了 def minDepth(self, root): """ :type root: TreeNode :rtype: int """ if not root: return 0 if not root.left or not root.right: # 这个是求解高度的 return 1 +max(self.minDepth(root.left), self.minDepth(root.right)) else: return min(self.minDepth(root.left), self.minDepth(root.right))+1 Path Sum Given a binary tree and a sum, determine if the tree has a root-to-leaf path such that adding up all the values along the path equals the given sum.Note: A leaf is a node with no children. Tips: 有条件的dfs(), 有条件的进行树的路径，树在加深的同时，target 数字也是不断的减少，最后如果相等，那么就是一个合适的解。 12345678910111213141516171819202122232425262728293031323334353637383940# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution(object): # 树的路径 # 总结一下 any all 这种到自己的博客 """ any: Returns true if any of the items is True. It returns False if empty or all are false. Any can be thought of as a sequence of OR operations on the provided iterables. all: Returns true if all of the items are True (or if the iterable is empty). All can be thought of as a sequence of AND operations on the provided iterables. It also short circuit the execution i.e. stop the execution as soon as the result is known. """ def hasPathSum(self, root, sum): """ :type root: TreeNode :type sum: int :rtype: bool """ res =[] self.dfs(root, sum, res) return any(res) def dfs(self, root, target, res): if not root: return False # 对于叶子结点的定义 if not root.left and not root.right: if root.val == target: res.append(True) if root.left: self.dfs(root.left, target-root.val, res) if root.right: self.dfs(root.right, target-root.val, res) Path Sum II Given a binary tree and a sum, find all root-to-leaf paths where each path’s sum equals the given sum.Note: A leaf is a node with no children. Tips： 这个和上面的区别在于，一个是 true or false，一个find all paths，所以需要有一个变量去存储正确的路径。 12345678910111213141516171819202122232425262728293031# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution(object): # 上一道题目是 return true or false，这个是找到所有的路径 def pathSum(self, root, sum): """ :type root: TreeNode :type sum: int :rtype: List[List[int]] """ res =[] self.dfs(root, sum, [], res) return res def dfs(self, root, target, path, res): if not root: return [] if not root.left and not root.right: if root.val == target: res.append(path+[root.val]) if root.left: #这种条件是可以减少迭代的次数 self.dfs(root.left, target-root.val, path+[root.val], res) if root.right: self.dfs(root.right, target-root.val, path+[root.val], res) Flatten Binary Tree to Linked List Given a binary tree, flatten it to a linked list in-place. Tips： 比较有意思，将 tree 的左右子树 flatten 成 linked list的左右结点。其中的 self.pre 就类似一种全局变量，将整个遍历， 1234567891011121314151617181920212223242526272829# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution(object): # 这种 flatten 就是 拉平（先序遍历）， 然后转成linkedlist # 并且这种操作是要求 in-place的 def __init__(self): self.pre =TreeNode('dummy') def flatten(self, root): """ :type root: TreeNode :rtype: None Do not return anything, modify root in-place instead. """ if not root: return tmp =root.right # 这个保存下来，是为了下面的flatten 使用 self.pre.right =root self.pre.left =None self.pre =root self.flatten(root.left) self.flatten(tmp) Populating Next Right Pointers in Each Node You are given a perfect binary tree where all leaves are on the same level, and every parent has two children. The binary tree has the following definition: Tip，属于树的结构的优化，多了一个next 指针指向的是同层的右节点。这个树的操作一般是 in-place，所以在某个递归过程中 return 是不必return value，本生就是在修改。 123456789101112131415161718192021222324252627282930"""# Definition for a Node.class Node(object): def __init__(self, val, left, right, next): self.val = val self.left = left self.right = right self.next = next"""class Solution(object): # perfect binary tree, # 题目的要求， populate each next pointer to its next right node def helper(self, left, right): if not left or not right: return left.next = right # 三种关系，先后顺序是没有关系的 self.helper(left.left, left.right) self.helper(left.right, right.left) self.helper(right.left, right.right) def connect(self, root): if not root: return self.helper(root.left, root.right) return root Populating Next Right Pointers in Each Node II Given a binary tree struct Node { int val; Node left; Node right; Node *next; }Populate each next pointer to point to its next right node. If there is no next right node, the next pointer should be set to NULL. Tips：注意从图片上观察这一题和上一题的区别，这图中表民一个子树的左子树是可以指向另一个子树的右子树，说明这个明显类似层次遍历，而不像上一题那样。 123456789101112131415161718192021222324252627282930313233"""# Definition for a Node.class Node(object): def __init__(self, val, left, right, next): self.val = val self.left = left self.right = right self.next = next"""class Solution(object): # 这个是不太明白的 # 这个相对于上一道题目，只是少了 perfect binary tree, 翻译成中文，满二叉树（完美二叉树），包括最后一层都是满的 def connect(self, root): if root is None: return None queue = [root] while queue: prev,curr = None,None size = len(queue) # 有点类似层次遍历的意思 for i in range(size): curr = queue.pop(0) # 这个 if 只有在for 之内才是有效的，第一次是无效的 if prev : prev.next = curr if curr.left: queue.append(curr.left) if curr.right: queue.append(curr.right) prev = curr curr.next = None return root Binary Tree Maximum Path Sum Given a non-empty binary tree, find the maximum path sum.For this problem, a path is defined as any sequence of nodes from some starting node to any node in the tree along the parent-child connections. The path must contain at least one node and does not need to go through the root. Input: [-10,9,20,null,null,15,7] -10 / \ 9 20 / \ 15 7Output: 42 Tips： 这个的难点在于，可以从任意点开始，然后再任意点结束，并且过不过根节点都是可以的。 分制到底部，在返回的时候传入左右任意一遍最大值加上目前root.val:cur = max(left, right) + root.val 这种情况处理了从Root到左右任意一边的最大值，也就是 root.val + left 和 root.val + right； 还有一种情况就是当最大值 = root.val + left + right， 我们在放入global变量的时候何其比较。 对于最底部叶子节点传上来的值，我们将其设置成0: return cur if cur &gt; 0 else 0 12345678910111213141516171819202122232425262728# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution(object): # 根据以往的经验，树的递归解法一般都是递归到叶节点，然后开始边处理边回溯到根节点。 # 但是这个题目不是， 这个是可以任意 start， 任意 end，然后不一定要经过根节点 def maxPathSum(self, root): """ :type root: TreeNode :rtype: int """ # 使用 self 标志 就意味这个是一种全局的变量， 类似在 init 中进行初始化的 self.res = - float('inf') self.dfs(root) return self.res def dfs(self, root): if not root: return 0 left = self.dfs(root.left) right = self.dfs(root.right) self.res = max(self.res, left + right + root.val) cur = max(left, right) + root.val return cur if cur &gt; 0 else 0 Sum Root to Leaf Numbers Given a binary tree containing digits from 0-9 only, each root-to-leaf path could represent a number.An example is the root-to-leaf path 1-&gt;2-&gt;3 which represents the number 123.Find the total sum of all root-to-leaf numbers.Note: A leaf is a node with no children. Input: [1,2,3] 1 / \ 2 3Output: 25Explanation:The root-to-leaf path 1-&gt;2 represents the number 12.The root-to-leaf path 1-&gt;3 represents the number 13.Therefore, sum = 12 + 13 = 25. Tips: 路径组成的数字代表一个数字，然后所有的路径和相加起来。关键代码只要 cur =pre*10 + root.val， 还是树的路径的遍历吧。 123456789101112131415161718192021222324252627282930313233343536# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution(object): # 对于树的 类型，大概就是这样了， 递归，找出递归的跳出的条件，然后处理保存结果 def sumNumbers(self, root): """ :type root: TreeNode :rtype: int """ self.result =0 self.sumNum(root, 0) return self.result def sumNum(self, root, pre): if not root: return cur = pre *10 +root.val if not root.left and not root.right: self.result += cur return if root.left: self.sumNum(root.left, cur) if root.right: self.sumNum(root.right, cur) Binary Tree Preorder Traversal Given a binary tree, return the preorder traversal of its nodes’ values. Tips：非递归版本（迭代），使用栈（递归的思想就是栈的思想）。 1234567891011121314151617181920212223242526272829# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution(object): # Recursive solution is trivial, could you do it iteratively? def preorderTraversal(self, root): """ :type root: TreeNode :rtype: List[int] """ if not root: return [] res, queue =[], [root] while queue: cur =queue.pop() if cur: res.append(cur.val) queue.append(cur.right) queue.append(cur.left) #queue.append(cur.right) return res LRU Cache Design and implement a data structure for Least Recently Used (LRU) cache. It should support the following operations: get and put.get(key) - Get the value (will always be positive) of the key if the key exists in the cache, otherwise return -1.put(key, value) - Set or insert the value if the key is not already present. When the cache reached its capacity, it should invalidate the least recently used item before inserting a new item.The cache is initialized with a positive capacity. Tips：这个题目纯粹手解，太麻烦了，在python3 中有 collections.OrderedDict() 的实现，这是作弊的写法。特点在于dict +队列（不完全是队列，因为访问之后还会放到队列的最后，而不是弹出）。因为一般的dict 存储的时候是无序（不是按照放入的先后书序），ordereddict 是按照放入的先后顺序进行存储的。题目本身就是先进先出的队列，只不过存储的是 (key, value) 这样的键值对。使用get 的时候，get到一个不能删除，应该放到最后；put的时候在 OrderedDict.popitem()有一个可选参数last（默认为True），当last为True时它从OrderedDict中删除最后一个键值对并返回该键值对，当last为False时它从 OrderedDict中删除第一个键值对并返回该键值对。 123456789101112131415161718192021222324252627282930class LRUCache(object): # python3 environment def __init__(self, capacity): self.size =capacity self.cache = collections.OrderedDict() def get(self, key): if key not in self.cache: return -1 val =self.cache[key] self.cache.move_to_end(key) # Python &gt;= 3.2 return val def put(self, key, val): if key in self.cache: del self.cache[key] self.cache[key] =val if len(self.cache) &gt; self.size: self.cache.popitem(last= False) # Your LRUCache object will be instantiated and called as such:# obj = LRUCache(capacity)# param_1 = obj.get(key)# obj.put(key,value) Insertion Sort List Sort a linked list using insertion sort. Tips： linkedlist 擅长于修改元素（直接修改指向），其中的 if while 是经常搭配使用，发现.. 然后就处理… 1234567891011121314151617181920212223242526272829303132# Definition for singly-linked list.# class ListNode(object):# def __init__(self, x):# self.val = x# self.next = Noneclass Solution(object): # 前插 def insertionSortList(self, head): """ :type head: ListNode :rtype: ListNode """ p =dummy =ListNode(0) cur =dummy.next =head while cur and cur.next: next_val =cur.next.val if cur.val &lt;= next_val: cur =cur.next continue # the sequence is not sorted inorder # find the proper situation # 从头开始找 if p.next.val &gt; next_val: p =dummy while p.next.val &lt;= next_val: p =p.next p.next, cur.next.next, cur.next =cur.next, p.next, cur.next.next return dummy.next Sort List Sort a linked list in O(n log n) time using constant space complexity. Input: 4-&gt;2-&gt;1-&gt;3 Output: 1-&gt;2-&gt;3-&gt;4 Tips: mergesort 的思想，显示把list 分成left and right（分），然后最后merge 算法 1234567891011121314151617181920212223242526272829303132333435# Definition for singly-linked list.# class ListNode(object):# def __init__(self, x):# self.val = x# self.next = Noneclass Solution(object): def merge(self, h1,h2): dummy =tail =ListNode(-1) while h1 and h2: if h1.val &lt; h2.val: tail.next, h1 =h1, h1.next else: tail.next, h2 =h2, h2.next tail =tail.next tail.next =h1 or h2 return dummy.next def sortList(self, head): if not head or not head.next: return head pre, slow, fast =None, head, head # slow fast 直接是两种快慢的不影响的index 遍历方式，slow 是下一个链表的结点 while fast and fast.next: pre, slow, fast =slow, slow.next, fast.next.next pre.next =None # 下面的两种写法是等价的 return self.merge(self.sortList(head), self.sortList(slow)) # return self.merge(*map(self.sortList, (head, slow))) Number of Islands Given a 2d grid map of ‘1’s (land) and ‘0’s (water), count the number of islands. An island is surrounded by water and is formed by connecting adjacent lands horizontally or vertically. You may assume all four edges of the grid are all surrounded by water. Tips: 这个dfs 跟之前的不一样之处在于，需要对于每个点进行 dfs() ，其他的还好。提供了两种解法，第一种比较代码比较少。比较喜欢第一种代码的风格，这样两个函数看起来比较均衡。 https://leetcode.com/problems/number-of-islands/ 解法一：1234567891011121314151617181920212223242526272829class Solution(object): def numIslands(self, grid): """ :type grid: List[List[str]] :rtype: int """ if not grid: return 0 count =0 for i in range(len(grid)): for j in range(len(grid[0])): if grid[i][j] =='1': self.dfs(grid, i,j) # 写法比较巧妙 count +=1 return count def dfs(self, grid, i,j): if i &lt;0 or j&lt;0 or i&gt;=len(grid) or j&gt;= len(grid[0]) or grid[i][j] !='1': return grid[i][j] ='0' self.dfs(grid, i+1, j) self.dfs(grid, i-1, j) self.dfs(grid, i, j+1) self.dfs(grid, i, j-1) 解法二：两种思想一样，写法不一样。 12345678910111213141516171819202122232425262728293031323334353637383940class Solution(object): def numIslands(self, grid): """ :type grid: List[List[str]] :rtype: int """ if not grid: return 0 used =[ [False]* len(grid[0]) for _ in range(len(grid))] count =0 for i in range(len(grid)): for j in range(len(grid[0])): num =self.dfs(grid, used, len(grid)-1, len(grid[0])-1, i, j) if num&gt;0: count +=1 return count def dfs(self, grid, used, row, col, x, y): if grid[x][y] =='0' or used[x][y]: return 0 used[x][y] =True num =1 if x!=0: num += self.dfs(grid, used, row, col, x -1, y) if x !=row: num += self.dfs(grid, used, row, col, x +1,y) if y!=0: num += self.dfs(grid, used, row, col, x, y-1) if y !=col: num += self.dfs(grid, used, row, col, x, y+1) return num Super Egg Drop You are given K eggs, and you have access to a building with N floors from 1 to N.Each egg is identical in function, and if an egg breaks, you cannot drop it again.You know that there exists a floor F with 0 &lt;= F &lt;= N such that any egg dropped at a floor higher than F will break, and any egg dropped at or below floor F will not break. 1234567891011121314151617181920212223class Solution(object): def gameOfLife(self, board): # 纯粹的count，之后的判断是下面决定的 def count(x, y): res =0 # 遍历 点的四周 for r in range(x-1, x+2): for c in range(y-1, y+2): if (r!= x or c!=y) and 0&lt;= r &lt; len(board) and 0&lt;= c &lt; len(board[0]) and board[r][c] &gt;0: res +=1 return res for x in range(len(board)): for y in range(len(board[0])): board[x][y] =count(x, y) +1 if board[x][y] ==1 else -count(x, y) # if board[x][y] == 1, change its value to count(x,y) + 1, the reason I add 1 is to keep it positive for x in range(len(board)): for y in range(len(board[0])): board[x][y] = 1 if board[x][y] in &#123;3, 4, -3&#125; else 0 # &#123;2, 3, -3&#125; Kth Smallest Element in a BST Given a binary search tree, write a function kthSmallest to find the kth smallest element in it. Tips: 二分查找树，中序遍历就是不减的list .有递归，迭代两个版本，共三种实现。倾向于使用第二个版本。迭代，然后使用k 进行及时的跳出。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475# Definition for a binary tree node.class TreeNode(object): def __init__(self, x): self.val = x self.left = None self.right = Noneclass Solution(object): # BST 中序遍历 得到一个不减的list，然后就可得第k 小的元素 # 下面是递归版本 ''' def kthSmallest(self, root, k): """ :type root: TreeNode :type k: int :rtype: int """ if not root: return res =[] self.dfs(root, res) if len(res) +1&lt;k: return return res[k-1] def dfs(self, root, res): if not root: return #res.append(root.val) self.dfs(root.left, res) res.append(root.val) self.dfs(root.right, res) ''' ''' # 相比于第一种方式，时间上是有减少的 def kthSmallest(self, root,k ): stack =[] node =root while True: if node: stack.append(node) node =node.left else: node =stack.pop() # 使用计数的方式进行访问，减少了空间复杂度 k -=1 if not k: break node =node.right return node.val ''' # 这个代码就是有点 抖机灵的那种，如果使用了 try ..catch.. 那么exception 就不会报错 def kthSmallest(self, root, k): def inorder(root, k): if root: inorder(root.left, k) if k ==1: raise Exception(root.val) inorder(root.right, k-1) #return k try: inorder(root, k) except Exception as e: return e.message Lowest Common Ancestor of a Binary Tree Given a binary tree, find the lowest common ancestor (LCA) of two given nodes in the tree.According to the definition of LCA on Wikipedia: “The lowest common ancestor is defined between two nodes p and q as the lowest node in T that has both p and q as descendants (where we allow a node to be a descendant of itself).” Given the following binary tree: root = [3,5,1,6,2,0,8,null,null,7,4] Tips: 这道Follow Up没有BST的特性，所以要对几种case一个一个进行测试。Condition为两种：如果没找到，返回None，找到则返回当前的root(因为找到一个root就不需要继续深入)比对方式： 如果parent的左右孩子都有返回，说明parent就是LCA 如果左边没有返回：则右边返回的就是LCA 如果右边没有返回：则左边返回的就是LCA 讲解 https://leetcode.com/problems/lowest-common-ancestor-of-a-binary-tree/ 1234567891011121314151617181920212223242526272829# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution(object): def lowestCommonAncestor(self, root, p, q): """ :type root: TreeNode :type p: TreeNode :type q: TreeNode :rtype: TreeNode """ if not root: return None if p ==root or q ==root: return root left =self.lowestCommonAncestor(root.left, p, q) right =self.lowestCommonAncestor(root.right, p, q) if left and right: return root if not left: return right if not right: return left Serialize and Deserialize Binary Tree Serialization is the process of converting a data structure or object into a sequence of bits so that it can be stored in a file or memory buffer, or transmitted across a network connection link to be reconstructed later in the same or another computer environment. Tips: 序列化主要是用在 存储和传输上吧. 基于 队列进行实现。队列可以两边进行修改。先序遍历 https://leetcode.com/problems/serialize-and-deserialize-binary-tree/ 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Codec: # 先序遍历 def serialize(self, root): if not root: return "" queue =collections.deque([root]) res =[] # 这个就是一种循环先序遍历二叉树 while queue: # 使用 pop 和 append() 操作右边 node =queue.popleft() # 在队列中使用 popleft 和appendleft() 直接操作队列的左边的增减 if node: queue.append(node.left) queue.append(node.right) res.append(str(node.val) if node else '#') # 使用 # 表示是一种none return ','.join(res) # 使用, 隔开每个node def deserialize(self, data): if not data: return None nodes =data.split(',') root =TreeNode(int(nodes[0])) queue =collections.deque([root]) index =1 # 作为string 的index while queue: node =queue.popleft() if nodes[index] != "#": # nodes[index] is not '#' 这样写也是可以的 node.left =TreeNode(int(nodes[index])) queue.append(node.left) index +=1 if nodes[index] != "#": node.right =TreeNode(int(nodes[index])) queue.append(node.right) index +=1 return root # Your Codec object will be instantiated and called as such:# codec = Codec()# codec.deserialize(codec.serialize(root)) Binary Tree Maximum Path Sum Given a non-empty binary tree, find the maximum path sum.For this problem, a path is defined as any sequence of nodes from some starting node to any node in the tree along the parent-child connections. The path must contain at least one node and does not need to go through the root. Tips: 这个不是树的路径，可以从任意非根节点出发。 1234567891011121314151617181920212223242526272829# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution(object): # 根据以往的经验，树的递归解法一般都是递归到叶节点，然后开始边处理边回溯到根节点。 # 但是这个题目不是， 这个是可以任意 start， 任意 end，然后不一定要经过根节点 def maxPathSum(self, root): """ :type root: TreeNode :rtype: int """ # 这种全局变量的设置确实是必须的，当携带变量的时候就出错了 self.res = - float('inf') self.dfs(root) return self.res def dfs(self, root): if not root: return 0 left = self.dfs(root.left) right = self.dfs(root.right) self.res = max(self.res, left + right + root.val) cur = max(left, right) + root.val return cur if cur &gt; 0 else 0 Number of Islands Given a 2d grid map of ‘1’s (land) and ‘0’s (water), count the number of islands. An island is surrounded by water and is formed by connecting adjacent lands horizontally or vertically. You may assume all four edges of the grid are all surrounded by water. Tips: dfs 12345678910111213141516171819202122232425262728293031class Solution(object): # dfs 是一个中规中矩的算法 # 这种方式更加简洁一点，直接使用 grid[i][j] 是否等于1 进行操作， # 然后如果能返回，在主程序中进行计数，最后的结果比较nice def numIslands(self, grid): """ :type grid: List[List[str]] :rtype: int """ if not grid: return 0 count =0 for i in range(len(grid)): for j in range(len(grid[0])): if grid[i][j] =='1': self.dfs(grid, i,j) count +=1 return count def dfs(self, grid, i, j): if i&lt;0 or j&lt;0 or i&gt;=len(grid) or j &gt;=len(grid[0]) or grid[i][j]!='1': return grid[i][j] ='#' self.dfs(grid, i+1, j) self.dfs(grid, i-1, j) self.dfs(grid, i,j +1) self.dfs(grid, i, j-1) Course Schedule There are a total of n courses you have to take, labeled from 0 to n-1.Some courses may have prerequisites, for example to take course 0 you have to first take course 1, which is expressed as a pair: [0,1]Given the total number of courses and a list of prerequisite pairs, is it possible for you to finish all courses? Tips: dfs 先修课程 12345678910111213141516171819202122232425262728293031323334353637383940414243class Solution(object): """ 这种解释是比较nice的，使用 0 -1 和1 分别表示初始化，正在访问和已经完成 if node v has not been visited, then mark it as 0. if node v is being visited, then mark it as -1. If we find a vertex marked as -1 in DFS, then their is a ring. if node v has been visited, then mark it as 1. If a vertex was marked as 1, then no ring contains v or its successors. """ def canFinish(self, numCourses, prerequisites): """ :type numCourses: int :type prerequisites: List[List[int]] :rtype: bool """ graph = [[] for _ in range(numCourses)] visited = [0 for _ in range(numCourses)] # create graph for pair in prerequisites: x, y = pair graph[x].append(y) # visit each node for i in range(numCourses): if not self.dfs(graph, visited, i): return False return True def dfs(self, graph, visited, i): # if ith node is marked as being visited, then a cycle is found if visited[i] == -1: return False # if it is done visted, then do not visit again if visited[i] == 1: return True # mark as being visited visited[i] = -1 # visit all the neighbours for j in graph[i]: if not self.dfs(graph, visited, j): return False # after visit all the neighbours, mark it as done visited visited[i] = 1 return True Course Schedule II There are a total of n courses you have to take, labeled from 0 to n-1.Some courses may have prerequisites, for example to take course 0 you have to first take course 1, which is expressed as a pair: [0,1]Given the total number of courses and a list of prerequisite pairs, return the ordering of courses you should take to finish all courses.There may be multiple correct orders, you just need to return one of them. If it is impossible to finish all courses, return an empty array. Tips: 和上一题相似，dfs 求解的是路径问题，而不是最值。 1234567891011121314151617181920212223242526272829303132333435363738class Solution(object): # 上一题是true or false 这个题目要求给个能够完成的路径，哎 # 不得不说这个是图的知识点呀 def findOrder(self, numCourses, prerequisites): """ :type numCourses: int :type prerequisites: List[List[int]] :rtype: List[int] """ def dfs(i, visited, graph, res): if visited[i] ==1: return True if visited[i] ==-1: return False visited[i] =-1 for n in graph[i]: if not dfs(n, visited, graph, res): return False res.append(i) visited[i] =1 return True visited =[0] * numCourses graph =&#123;x :[] for x in range(numCourses)&#125; # 注意这个顺序，因为最后要的是路径，所以这样是更加合理的 for p in prerequisites: graph[p[1]].append(p[0]) res =[] for i in range(numCourses): if not dfs(i, visited, graph, res): return [] return res[::-1]]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode-List]]></title>
    <url>%2F2019%2F06%2F16%2Fleetcode-list%2F</url>
    <content type="text"><![CDATA[LeetCode 刷题总结（二）， 使用Python 实现。该篇题目类型主要是： list, linkedList 还有简单的 tree。 Add Two Numbers You are given two non-empty linked lists representing two non-negative integers. The digits are stored in reverse order and each of their nodes contain a single digit. Add the two numbers and return it as a linked list. Input: (2 -&gt; 4 -&gt; 3) + (5 -&gt; 6 -&gt; 4)Output: 7 -&gt; 0 -&gt; 8Explanation: 342 + 465 = 807. Tips: 加法的过程，使用 \% 和 整除进行求解，使用linkedList 进行存储。https://leetcode.com/problems/add-two-numbers/ 123456789101112131415161718192021222324252627# Definition for singly-linked list.# class ListNode(object):# def __init__(self, x):# self.val = x# self.next = Noneclass Solution(object): def addTwoNumbers(self, l1, l2): """ :type l1: ListNode :type l2: ListNode :rtype: ListNode """ dummy = cur = ListNode(0) carry = 0 while l1 or l2 or carry: if l1: carry += l1.val l1 = l1.next if l2: carry += l2.val l2 = l2.next cur.next = ListNode(carry%10) cur = cur.next carry //= 10 return dummy.next Remove Nth Node From End of List Given a linked list, remove the n-th node from the end of list and return its head. Tips: 可以使用”两指针“ 方法进行求解，前后两指针相差 N步数。 12345678910111213141516171819202122232425262728293031323334353637# Definition for singly-linked list.# class ListNode(object):# def __init__(self, x):# self.val = x# self.next = Noneclass Solution(object): def removeNthFromEnd(self, head, n): """ :type head: ListNode :type n: int :rtype: ListNode """ if not head or not head.next: return None new_head =ListNode(-1) new_head.next =head fast =new_head # 先走n 步 for i in range(n): # 这里很好的处理了 n &gt; 链表长度 情况 if fast.next: fast =fast.next else: return head slow =new_head # 一块走 # 因为是 fast 先走到none，所以这个判断条件 while fast.next: fast =fast.next slow =slow.next slow.next =slow.next.next # 这个指向是经验性，还是超级nice的 return new_head.next Merge Two Sorted Lists Merge two sorted linked lists and return it as a new list. The new list should be made by splicing together the nodes of the first two lists. Tips: 归并操作中的”并“ 操作。 https://leetcode.com/problems/merge-two-sorted-lists/ 123456789101112131415161718192021222324252627282930313233343536373839404142# Definition for singly-linked list.# class ListNode(object):# def __init__(self, x):# self.val = x# self.next = Noneclass Solution(object): def mergeTwoLists(self, l1, l2): """ :type l1: ListNode :type l2: ListNode :rtype: ListNode """ if not l1 and not l2: return None elif not l1 or not l2: return l1 or l2 # 这里新建了 node，作为最后返回list 中的head，因为使用哪个子list 都是不确定的 new_node =ListNode(-1) cur =new_node head1 =l1 head2 =l2 while head1 and head2: if head1.val &lt; head2.val: cur.next =head1 head1 =head1.next else: cur.next =head2 head2 =head2.next cur =cur.next # 对于这种 if else 需要是相当的清楚 if head1: cur.next =head1 elif head2: cur.next =head2 return new_node.next Swap Nodes in Pairs Given a linked list, swap every two adjacent nodes and return its head.You may not modify the values in the list’s nodes, only nodes itself may be changed. Given 1-&gt;2-&gt;3-&gt;4, you should return the list as 2-&gt;1-&gt;4-&gt;3. Tips: 常见的类型，使用三个指针修改指向。经常创建 dummy指针，如果head 可能被改变的话。 1234567891011121314151617181920212223242526272829303132# Definition for singly-linked list.# class ListNode(object):# def __init__(self, x):# self.val = x# self.next = Noneclass Solution(object): """ 三个指针的修改，应该是没有问题的 """ def swapPairs(self, head): """ :type head: ListNode :rtype: ListNode """ if not head: return head dummy=pre =ListNode(-1) dummy.next =head while True: cur =pre.next if not cur: break nex =cur.next if not nex: break pre.next, cur.next, nex.next, pre =nex, nex.next, cur, cur return dummy.next Count and Say The count-and-say sequence is the sequence of integers with the first five terms as following: 1. 1 2. 11 3. 21 4. 1211 5. 111221 1 is read off as &quot;one 1&quot; or 11. 11 is read off as &quot;two 1s&quot; or 21. 21 is read off as &quot;one 2, then one 1&quot; or 1211. Tips: 这个是属于循环，字符串处理。 12345678910111213141516171819202122232425262728class Solution(object): def doCountAndSay(self, string): char =string[0] num =0 result ="" for c in string: if char ==c: num +=1 else: result += (str(num)+ char) char =c num =1 result += (str(num) +char) return result def countAndSay(self, n): if 0 ==n: return "" elif 1== n: return "1" result ='1' for i in range(1, n): result =self.doCountAndSay(result) return result Trapping Rain Water Given n non-negative integers representing an elevation map where the width of each bar is 1, compute how much water it is able to trap after raining. Tips: 算法比较巧妙，左右两边进行遍历找出”累计“最高点，在O(N) 时间内完成。能装的水，取决于左右两边( neighbor) 的小值- height[i]。 https://leetcode.com/problems/trapping-rain-water/ 1234567891011121314151617181920212223242526class Solution(object): def trap(self, height): if not height: return 0 len_h =len(height) leftmax=[0]* len_h max_h=0 for i in range(len_h): if height[i] &gt;max_h: max_h =height[i] leftmax[i] =max_h rightmax =[0] *len_h max_h =0 for i in range(len_h-1, -1, -1): if height[i]&gt; max_h: max_h =height[i] rightmax[i] =max_h result =0 for i in range(len_h): result += (min(leftmax[i], rightmax[i])- height[i]) return result Maximum Subarray Given an integer array nums, find the contiguous subarray (containing at least one number) which has the largest sum and return its sum. Input: [-2,1,-3,4,-1,2,1,-5,4], Output: 6 Explanation: [4,-1,2,1] has the largest sum = 6. Tips: 一维数组返回最大的子数组，字符串处理。https://leetcode.com/problems/maximum-subarray/ 1234567891011121314151617181920212223class Solution(object): def maxSubArray(self, nums): """ :type nums: List[int] :rtype: int """ len_n =len(nums) if len_n ==1: return nums[0] max_n =nums[0] # 如果提前初始化，那么之后在更新max_n 时候就不用进行 None 的判断了 sum_n =0 for num in nums: sum_n += num if sum_n&gt; max_n: max_n =sum_n if sum_n &lt;0: sum_n =0 # continue return max_n Insert Interval Given a set of non-overlapping intervals, insert a new interval into the intervals (merge if necessary).You may assume that the intervals were initially sorted according to their start times. Input: intervals = [[1,3],[6,9]], newInterval = [2,5] Output: [[1,5],[6,9]] Tips: 使用( 0, 1) 去区分 start, end 这种点，类似一种数据结构的样子, 排序之后结果的start index 和end index 分别出现在首段和尾段，使用栈进行存储 start index 就行了。https://leetcode.com/problems/insert-interval/ 123456789101112131415161718192021222324252627282930313233343536class Solution(object): def insert(self, intervals, newinterval): """ 这个每个都是有 数据类型的，这个就是 LeetCode 的优点 :type intervals: List[List[int]] :type newInterval: List[int] :rtype: List[List[int]] """ if not intervals: return [newinterval] datas =[] if intervals: datas.append((newinterval[0], 0)) datas.append((newinterval[1], 1)) for interval in intervals: datas.append((interval[0], 0)) datas.append((interval[1], 1)) datas.sort() # sort() 是一个骚操作, 默认的排序显示根据 tuple[0] 进行排序，如果相同，那么根据 tuple[1] 进行排序 # 所以排在前面的一定是 start index。 merged =[] stack =[datas[0]] for i in range(1, len(datas)): data =datas[i] if data[1] ==0: stack.append(data) elif data[1] ==1: if stack: start =stack.pop() if len(stack) ==0: # 这个时候 data 是 end point merged.append((start[0], data[0])) return merged Rotate List Given a linked list, rotate the list to the right by k places, where k is non-negative. Tips： 细节在于k 可能大于 list 的长度。 123456789101112131415161718192021222324252627282930313233343536# Definition for singly-linked list.# class ListNode(object):# def __init__(self, x):# self.val = x# self.next = Noneclass Solution(object): def rotateRight(self, head, k): if k ==0: return head if not head: return head # 头指针 dummy =ListNode(-1) dummy.next =head p =dummy count =0 while p.next: p =p.next count +=1 # 指向了头指针，连成了一个环，下一步开始找头指针 p.next =dummy.next step =count -(k% count) for i in range(step): p =p.next # 找到了头指针，那么下一个就是尾指针 head =p.next p.next =None return head Unique Paths A robot is located at the top-left corner of a m x n grid (marked ‘Start’ in the diagram below).The robot can only move either down or right at any point in time. The robot is trying to reach the bottom-right corner of the grid (marked ‘Finish’ in the diagram below).How many possible unique paths are there? Tips： 最后求解的是unique paths 的数量，而不是具体的路径，所以可以不使用 dp，成了一道模拟排列组合的数学题。 12345678910111213141516171819202122class Solution(object): # 这个总的步数是一定的 m+n -1 ，然后下行和右行也是一定的， # 所以这个是模拟的“排列组合” 的思想 def uniquePaths(self, m, n): """ :type m: int :type n: int :rtype: int """ if m ==0 or n ==0: return 1 up =1 # 这个是分子 for i in range(m+n-2, n -1, -1): up *=i down =1 # 这个是分母 for j in range(1, m): down *= j return up/down Sort Colors Given an array with n objects colored red, white or blue, sort them in-place so that objects of the same color are adjacent, with the colors in the order red, white and blue.Here, we will use the integers 0, 1, and 2 to represent the color red, white, and blue respectively. Tips: 快排思想， 中间数字1 当做key index，左右两边分别是left，right index。 1234567891011121314151617class Solution(object): """ 0, 1, 2 (red, white, blue) """ def sortColors(self, nums): # zero and r record the position of "0" and "2" respectively index, two, zero = 0, len(nums) - 1, 0 while index &lt;= two: if nums[index] == 0: nums[index], nums[zero] = nums[zero], nums[index] index += 1; zero += 1 elif nums[index] == 2: nums[index], nums[two] = nums[two], nums[index] two -= 1 else: index += 1 Remove Duplicates from Sorted List II Given a sorted linked list, delete all nodes that have duplicate numbers, leaving only distinct numbers from the original list. Tips: 常规题，经常出现这样的逻辑， if… while ，如果发现有重复的，那么一直就找到不重复为止。 12345678910111213141516171819202122232425262728# Definition for singly-linked list.# class ListNode(object):# def __init__(self, x):# self.val = x# self.next = Noneclass Solution(object): """ 就是在删除节点的时候，如果head 节点也得删除，这个时候 常常创建一个 dummy 结点 求解的是distinct 的list """ def deleteDuplicates(self, head): dummy =pre =ListNode(0) dummy.next =head while head and head.next: if head.val ==head.next.val: while head and head.next and head.val ==head.next.val: head =head.next head =head.next pre.next =head else: # 这个更新很有意思， head =head.next, pre.next =head pre =pre.next head =head.next return dummy.next Partition List Given a linked list and a value x, partition it such that all nodes less than x come before nodes greater than or equal to x.You should preserve the original relative order of the nodes in each of the two partitions. Tips: 新建了两个结点，分别连接小于 x 和不小于 x 的结点，最后两个结点相连。 list 是直接进行交换位置，但是linkedList 不是这样的。 123456789101112131415161718192021222324252627282930# Definition for singly-linked list.# class ListNode(object):# def __init__(self, x):# self.val = x# self.next = Noneclass Solution(object): # 局部排序，不是全局排序 def partition(self, head, x): """ :type head: ListNode :type x: int :rtype: ListNode """ small =l1 =ListNode(0) great =l2 =ListNode(0) while head: if head.val &lt;x : l1.next =head l1 =l1.next else: l2.next =head l2 =l2.next head =head.next # 这个是一个细节， 最后l2 是需要一个none 进行结束标记 l2.next =None l1.next =great.next return small.next Reverse Linked List II Reverse a linked list from position m to n. Do it in one-pass.Note: 1 ≤ m ≤ n ≤ length of list. Tips: 局部进行reverse，找到该节点，然后迭代进行就可以了。 1234567891011121314151617181920212223242526# Definition for singly-linked list.# class ListNode(object):# def __init__(self, x):# self.val = x# self.next = Noneclass Solution(object): # The idea is simple and intuitive: find linkedlist [m, n], reverse it, then connect m with n+1, connect n with m-1 def reverseBetween(self, head, m, n): pre =dummy =ListNode(0) dummy.next =head #cur, pre =head, dummy for _ in range(m-1): #cur =cur.next pre =pre.next cur =pre.next for _ in range(n-m): tmp =cur.next cur.next =tmp.next tmp.next =pre.next pre.next =tmp return dummy.next Unique Binary Search Trees Given n, how many structurally unique BST’s (binary search trees) that store values 1 … n? Tips: 从处理子问题的角度来看，选取一个结点为根，就把结点切成左右子树，以这个结点为根的可行二叉树数量就是左右子树可行二叉树数量的乘积，所以总的数量是将以所有结点为根的可行结果累加起来。 123456789101112131415161718class Solution(object): # 二叉搜索树，当且仅当中序遍历的时候是单调非减的时候。 # 数学问题 def numTrees(self, n): """ :type n: int :rtype: int """ arr =[0]*(n+1) arr[0] =1 for i in range(1, n+1): for j in range(1, i+1): # 处理的是左右子树的乘积 arr[i] += arr[j-1] *arr[i-j] return arr[-1] Best Time to Buy and Sell Stock Say you have an array for which the ith element is the price of a given stock on day i.If you were only permitted to complete at most one transaction (i.e., buy one and sell one share of the stock), design an algorithm to find the maximum profit.Note that you cannot sell a stock before you buy one. Tips: 虽然这个说是最多一次买入卖出，但是这个价格变化是”连续“的，所以只要是下一个大于上一个就是可以 += profit 中的。 12345678910111213141516class Solution(object): # 从代码上来看，毫无算法可言 def maxProfit(self, prices): """ :type prices: List[int] :rtype: int """ if not prices or len(prices) ==1: return 0 profit =0 for i in range(1, len(prices)): if prices[i] &gt; prices[i-1]: profit += prices[i]-prices[i-1] return profit Best Time to Buy and Sell Stock II Say you have an array for which the ith element is the price of a given stock on day i.Design an algorithm to find the maximum profit. You may complete as many transactions as you like (i.e., buy one and sell one share of the stock multiple times).Note: You may not engage in multiple transactions at the same time (i.e., you must sell the stock before you buy again). Tips: 可以进行多次买卖。和上面的区别在于 下一个只要不小于上一个就是可以累加的。 123456789101112131415class Solution(object): # 可以多次买卖， 买一次然后卖一次。不能多次买入 # 这种就如同寻找的是 增序列。 def maxProfit(self, prices): """ :type prices: List[int] :rtype: int """ total =0 for i in range(1, len(prices)): if prices[i]&gt;= prices[i-1]: total += prices[i]-prices[i-1] return total Longest Consecutive Sequence Given an unsorted array of integers, find the length of the longest consecutive elements sequence.Your algorithm should run in O(n) complexity. Tips： 这种方法很巧妙，如果x-1 not in，那么去 try x+1，然后计数。 123456789101112131415161718class Solution(object): # 第一印象是 先进行排序， 然后选择的过程， 但是限制条件是 O(n) 的复杂度 def longestConsecutive(self, nums): """ :type nums: List[int] :rtype: int """ nums =set(nums) best =0 for x in nums: if x-1 not in nums: y =x+1 while y in nums: y +=1 best =max(best, y-x) return best Candy There are N children standing in a line. Each child is assigned a rating value.You are giving candies to these children subjected to the following requirements: Each child must have at least one candy. Children with a higher rating get more candies than their neighbors.What is the minimum candies you must give? Tips: 因为涉及到 neighbors，所以左右两边进行遍历，因为如果ratings 大的话，那么结果一定得大，所以返回的是较大者。 1234567891011121314151617181920class Solution(object): def candy(self, ratings): """ :type ratings: List[int] :rtype: int """ # 满足第一个条件，at least one candy res =len(ratings) *[1] # left to right, higher then more candies for i in range(1, len(ratings)): if ratings[i] &gt; ratings[i-1]: # 这个是严格的 &gt; res[i] = res[i-1] +1 # right to left, higher then more candy, neighbors for i in range(len(ratings)-1, 0, -1): if ratings[i-1] &gt; ratings[i]: res[i-1] =max(res[i-1], res[i] +1) return sum(res) Single Number Given a non-empty array of integers, every element appears twice except for one. Find that single one. Tips: 异或的性质 1234567891011121314class Solution(object): # 分分钟 异或就出来了 # integers, -2 -1 0 1 2 这样的数字 def singleNumber(self, nums): """ :type nums: List[int] :rtype: int """ standard =0 # 如果正好是其中的 0 出现一次，也是没有关系的， 因为初始化的 0 和 list 中的单数 0 正好匹配，异或操作之后相同为 0 # 结果上是没有什么问题的 for num in nums: standard = num^ standard return standard Single Number II Given a non-empty array of integers, every element appears three times except for one, which appears exactly once. Find that single one. Tips：这个 three times不能使用 异或，从二进制的角度进行考虑，以二进制的形式，将数字存储起来，如果是出现了 3次，那么 %3 结果就是0，最后只是剩下了 那个出现一次的数字 1234567891011121314151617181920212223242526class Solution(object): # 只是出现的一个的single one， 其他的出现三次 # var |= value is short for var = var | value def singleNumber(self, nums): """ :type nums: List[int] :rtype: int """ bit = [0] * 32 for num in nums: for i in range(32): bit[i] += num &gt;&gt; i &amp; 1 # 这个就是从左往右的顺序，先是进行 &gt;&gt; 运算，然后是 &amp; 运算 # 可以想象这个重复计算比较多，因为每次都需要 num &gt;&gt; i 进行位运算 res = 0 for i, val in enumerate(bit): # if the single numble is negative, # this case should be considered separately , 补码 和原码的转换关系 if i == 31 and val % 3: res = -((1 &lt;&lt; 31) - res) else: res |= (val % 3) * (1 &lt;&lt; i) # | 这个是位操作，更加类似不断的取 1 的过程， 然后和该位置的权重相乘 return res Copy List with Random Pointer A linked list is given such that each node contains an additional random pointer which could point to any node in the list or null.Return a deep copy of the list. Tips：在剑指offer 上是通过 指针操作进行做题，但是使用 defaultdict 基本上就不用出，使用dict 来处理这种关系，最后返回的是根节点、 123456789101112131415161718192021222324252627282930"""# Definition for a Node.class Node(object): def __init__(self, val, next, random): self.val = val self.next = next self.random = random"""# 使用dict 有没有感觉在作弊class Solution(object): # 做过这个 def copyRandomList(self, head): """ :type head: Node :rtype: Node """ import ipdb dic = collections.defaultdict(lambda: Node(0, None, None)) # 这个就是给定了一个默认的值, 直接初始化dict 中的value 为这个node # dict 的本身就是存储一种node 的关系，所以dict[n].val , next, random 可以这样进行操作 dic[None] = None n = head while n: dic[n].val = n.val dic[n].next = dic[n.next] dic[n].random = dic[n.random] n = n.next #ipdb.set_trace() return dic[head] Linked List Cycle Given a linked list, determine if it has a cycle in it.To represent a cycle in the given linked list, we use an integer pos which represents the position (0-indexed) in the linked list where tail connects to. If pos is -1, then there is no cycle in the linked list. Tips：快慢两个指针的问题，给了两种方法来实现。 1234567891011121314151617181920212223242526272829303132333435# Definition for singly-linked list.# class ListNode(object):# def __init__(self, x):# self.val = x# self.next = Noneclass Solution(object): # O(1) == constant memory, def hasCycle(self, head): """ :type head: ListNode :rtype: bool """ ''' # 这种不用进行判断 fast 是否可以访问的原因在于 try except 的使用 try: slow = head fast = head.next while slow is not fast: slow = slow.next fast = fast.next.next return True except: return False ''' # 这个是比较中规中矩的写法 slow = fast = head # 注意使用的是 fast进行判断，因为这个走的快 while fast and fast.next: fast = fast.next.next slow = slow.next if slow == fast: return True return False Reorder List Given a singly linked list L: L0→L1→…→Ln-1→Ln,reorder it to: L0→Ln→L1→Ln-1→L2→Ln-2→…You may not modify the values in the list’s nodes, only nodes itself may be changed. Tips： 从中间断开，后半部分翻转，然后和前半部分轮流连接 12345678910111213141516171819202122232425262728293031323334353637383940414243# Definition for singly-linked list.# class ListNode(object):# def __init__(self, x):# self.val = x# self.next = Noneclass Solution(object): def reorderList(self, head): """ :type head: ListNode :rtype: None Do not return anything, modify head in-place instead. """ if not head: return fast, slow = head.next, head # first part has the same or one more node while fast and fast.next: fast = fast.next.next slow = slow.next # reverse the send half p = slow.next slow.next = None node = None # 类似上一个结点， p 是cur 的结点 while p: nex = p.next p.next = node node = p p = nex # combine head part and node part p = head while node: tmp = node.next node.next = p.next # 两个 next 指向操作, 需要next 两次 p.next = node p =p.next.next node = tmp ** Majority Element Given an array of size n, find the majority element. The majority element is the element that appears more than ⌊ n/2 ⌋ times. Output: 2Input: K = 1, N = 2Explanation:Drop the egg from floor 1. If it breaks, we know with certainty that F = 0.Otherwise, drop the egg from floor 2. If it breaks, we know with certainty that F = 1.If it didn’t break, then we know with certainty F = 2.Hence, we needed 2 moves in the worst case to know what F is with certainty. Tips: 可以查看 solution 中的(讲解)[https://leetcode.com/problems/super-egg-drop/] 123456789101112131415161718class Solution(object): def majorityElement(self, nums): """ :type nums: List[int] :rtype: int """ # 初始化 counts =0 for num in nums: if counts ==0: majority =num counts =1 elif majority ==num: counts +=1 else: counts -=1 return majority Maximum Product Subarray Given an integer array nums, find the contiguous subarray within an array (containing at least one number) which has the largest product. Tips: 在于左右两遍遍历，分别得到 prefix和 suffix 的乘积。这个速度上比较快在于存储了之前的结果。实现的时候利用了 1 or prefix[i-1] 这种技巧。 12345678910111213class Solution(object): def maxProduct(self, nums): """ :type nums: List[int] :rtype: int """ prefix =nums suffix =prefix[::-1] for i in range(1, len(prefix)): prefix[i] *= 1 or prefix[i-1] suffix[i] *= 1 or suffix[i-1] return max(prefix+ suffix) Majority Element Given an array of size n, find the majority element. The majority element is the element that appears more than ⌊ n/2 ⌋ times. You may assume that the array is non-empty and the majority element always exist in the array. Tips: majority 的counts 的总数是大于其他所有counts相加之和的 123456789101112131415161718class Solution(object): def majorityElement(self, nums): """ :type nums: List[int] :rtype: int """ majority =None counts =0 for num in nums: if not majority or counts ==0: majority =num counts =1 elif num ==majority: counts +=1 else: counts -=1 return majority Rotate Array Given an array, rotate the array to the right by k steps, where k is non-negative. Tips： 对于python 而言，是不存在切分字符串算法的，一步操作。小的细节是 k %len(array) 更加合理 12345678910111213class Solution(object): def rotate(self, nums, k): """ :type nums: List[int] :type k: int :rtype: None Do not return anything, modify nums in-place instead. """ # 这个in-place 操作是不需要 return k =k%len(nums) # 这个是一个细节吧 #nums[:] =nums[-k:] +nums[:k+1] nums[:] =nums[-k:] +nums[:-k] # 左边有时候是nums 就行，有时候必须nums[:] 表示index的操作，因为环境的问题 # 这种 对称的切分真的是比较好看 Contains Duplicate Given an array of integers, find if the array contains any duplicates.Your function should return true if any value appears at least twice in the array, and it should return false if every element is distinct. Tips: 有很多种方法，比如 dictionary or set，这个简单之处最后返回的是 true or false，不是要找出来。 方法一：使用set，根据length判断。 1234567891011class Solution(object): def containsDuplicate(self, nums): """ :type nums: List[int] :rtype: bool """ # 想法一，排序之后判断， # 想法二：使用dictionary, 在建立的过程中就可以判断，没有必要建立完之后遍历，from collections import Counter # 想法三： 使用set，道理和dictionary 基本上是相同的 return len(nums) !=len(set(nums)) 方法二：使用dictionary，不需要建完之后再判断。 ···pythonclass Solution(object): def containsDuplicate(self, nums): “”” :type nums: List[int] :rtype: bool “”” dic ={} for num in nums: if num in dic: return True else: dic[num] =1 return False 12345678910111213141516171819202122232425** Move Zeroes **&gt; Given an array nums, write a function to move all 0&apos;s to the end of it while maintaining the relative order of the non-zero elements.&gt; Input: [0,1,0,3,12]Output: [1,3,12,0,0]Tips: 双指针问题，pre 指向的是0 ，index是遍历的发现如果不是0，那么进行操作```pythonclass Solution(object): def moveZeroes(self, nums): &quot;&quot;&quot; :type nums: List[int] :rtype: None Do not return anything, modify nums in-place instead. &quot;&quot;&quot; # in-place 操作 pre = 0 for index in range(0, len(nums)): if nums[index]: if not nums[pre]: nums[index], nums[pre] =nums[pre], nums[index] pre +=1 Shuffle an Array Shuffle a set of numbers without duplicates. Tips: 使用库函数randomint， 有 shuffle 和reset 两种操作，前者使用randomint 可以得到一个number，后者使用 list 备份。 12345678910111213141516171819202122232425class Solution(object): def __init__(self, nums): self.original =nums[:] self.nums =nums def reset(self): self.nums =self.original[:] # 这个应该 id() 是不同的 return self.nums def shuffle(self): tmp =self.nums[:] for i in range(len(self.nums)): rand =random.randint(0, len(tmp)-1) self.nums[i] =tmp[rand] del tmp[rand] return self.nums # Your Solution object will be instantiated and called as such:# obj = Solution(nums)# param_1 = obj.reset()# param_2 = obj.shuffle() Intersection of Two Arrays II Given two arrays, write a function to compute their intersection. https://leetcode.com/problems/intersection-of-two-arrays-ii/ Tips: 多看看题意，如果想要映射成dictionary，那么result 就是 min(dict1[i], dict1[j]), 两个dictionary 中values 的最小值。 123456789101112131415161718class Solution(object): def intersect(self, nums1, nums2): """ :type nums1: List[int] :type nums2: List[int] :rtype: List[int] """ # 存储dict 然后如果都存在，那么选择values 较小者 为好 # 说一下几种不同的思路 dic1 =collections.Counter(nums1) res =[] for num in nums2: if dic1[num] &gt;0: res += [num] dic1[num] -=1 return res Increasing Triplet Subsequence Given an unsorted array return whether an increasing subsequence of length 3 exists or not in the array. Input: [1,2,3,4,5]Output: true Tips: 首先学会在python 中表示最大数字(float(‘inf’)), 然后这个技巧相当于选择排序中一次遍历选择最小的那个。代码比较简洁哈 12345678910111213141516class Solution(object): def increasingTriplet(self, nums): """ :type nums: List[int] :rtype: bool """ first = second = float('inf') for n in nums: if n &lt;= first: first = n elif n &lt;= second: second = n else: return True return False Product of Array Except Self Given an array nums of n integers where n &gt; 1, return an array output such that output[i] is equal to the product of all the elements of nums except nums[i]. Tips:左右两遍，这个是一维的还是比较nice，换成 m*n 也是基本的思路吧。 12345678910111213141516171819202122class Solution(object): def productExceptSelf(self, nums): """ :type nums: List[int] :rtype: List[int] """ # 我记得使用一个数组存储起来中间的结果，然后进行操作的 len_n =len(nums) res =[1] *len_n # from left to right for i in range(1, len_n): res[i] =res[i-1] * nums[i-1] tmp =1 # from right to left for i in range(len_n-2, -1, -1): tmp *= nums[i+1] res[i] *= tmp return res Min Stack Design a stack that supports push, pop, top, and retrieving the minimum element in constant time. push(x) – Push element x onto stack. pop() – Removes the element on top of the stack. top() – Get the top element. getMin() – Retrieve the minimum element in the stack. Tips: 使用两个list，在push 和pop 的时候要维护栈，始终保持这stack[-1] 是最小值。getMin() 就直接调用结果就行。 https://leetcode.com/problems/min-stack/ 123456789101112131415161718192021222324252627class MinStack(object): def __init__(self): self.stack =[] self.min =[] def push(self, x): if not self.min: self.min.append(x) else: if x &lt;= self.min[-1]: self.min.append(x) self.stack.append(x) def pop(self): tmp =self.stack.pop() # 这个是合理的，因为只有pop 掉了最小值，然后 min list 才需要改变，之前min list 也只是存储的最小值序列 if tmp ==self.min[-1]: self.min.pop() def top(self): return self.stack[-1] def getMin(self): return self.min[-1] Kth Largest Element in an Array Find the kth largest element in an unsorted array. Note that it is the kth largest element in the sorted order, not the kth distinct element. Tips: 这个很巧妙，是快排的思想，每一次的pivot 是不是第 K大。 12345678910111213141516171819202122232425262728293031323334353637class Solution(object): def findKthLargest(self, nums, k): """ :type nums: List[int] :type k: int :rtype: int """ left, right =0, len(nums)-1 while True: index = self.partition(nums, left, right) if index ==k-1: return nums[index] elif index &lt; k-1: left =index+1 else: right =index -1 def partition(self, nums, left, right): pivot =nums[left] p1, p2 =left+1, right #找出 pivot 这个number 的位置 while p1 &lt;=p2: if nums[p1] &lt; pivot and nums[p2]&gt; pivot: nums[p1], nums[p2] =nums[p2], nums[p1] p1 +=1 p2 -=1 elif nums[p1] &gt;= pivot: p1 +=1 elif nums[p2] &lt;=pivot: p2 -=1 nums[left], nums[p2] =nums[p2] , nums[left] return p2 Min Stack Design a stack that supports push, pop, top, and retrieving the minimum element in constant time. push(x) – Push element x onto stack. pop() – Removes the element on top of the stack. top() – Get the top element. getMin() – Retrieve the minimum element in the stack. Tips: 需要维持两个stack， 一个是日常的，一个是min_stakc, 在push or pop 的过程中需要日常性维护 min_stack . 123456789101112131415161718192021222324252627class MinStack(object): def __init__(self): self.stack =[] self.min =[] def push(self, x): if not self.min: self.min.append(x) else: if x &lt;= self.min[-1]: self.min.append(x) self.stack.append(x) def pop(self): tmp =self.stack.pop() # 这个是合理的，因为只有pop 掉了最小值，然后 min list 才需要改变，之前min list 也只是存储的最小值序列 if tmp ==self.min[-1]: self.min.pop() def top(self): return self.stack[-1] def getMin(self): return self.min[-1] Kth Largest Element in an Array Find the kth largest element in an unsorted array. Note that it is the kth largest element in the sorted order, not the kth distinct element. Tips： 快排中的 pivot 如果恰好是 这个 kth，那么就成了。因为快排以 pivot 为分界点，左边是大于，右边是小于（假设是递减排序的话）；使用二分的方法，去寻找这个 pivot。属于一道比较好的题目。 ···pythonclass Solution(object): # 实际上是在试探 快排中pivot 这个点是否是第k 大值 def findKthLargest(self, nums, k): &quot;&quot;&quot; :type nums: List[int] :type k: int :rtype: int &quot;&quot;&quot; left, right =0, len(nums)-1 while True: index = self.partition(nums, left, right) if index ==k-1: return nums[index] elif index &lt; k-1: left =index+1 else: right =index -1 def partition(self, nums, left, right): pivot =nums[left] p1, p2 =left+1, right #找出 pivot 这个number 的位置 while p1 &lt;=p2: if nums[p1] &lt; pivot and nums[p2]&gt; pivot: nums[p1], nums[p2] =nums[p2], nums[p1] p1 +=1 p2 -=1 elif nums[p1] &gt;= pivot: p1 +=1 elif nums[p2] &lt;=pivot: p2 -=1 nums[left], nums[p2] =nums[p2] , nums[left] return p2 12345678910111213141516171819202122232425262728** Top K Frequent Elements**&gt; Given a non-empty array of integers, return the k most frequent elements. Input: nums = [1,1,1,2,2,3], k = 2 Output: [1,2]Tips: 如果是使用 dictionary 进行计数，那么直接调用 counter 是一个不错的选择； 下面是超级nice的代码。```pythonclass Solution(object): # 一种很简单的方法，就是放到dictionary 中，然后根据values从大到小排序，然后返回相应的keys # python 中的 sort() 函数默认是从小到大进行排序的 def topKFrequent(self, nums, k): &quot;&quot;&quot; :type nums: List[int] :type k: int :rtype: List[int] &quot;&quot;&quot; from collections import Counter # 本质上就是一个dictionary freq =Counter(nums) # 这个dictionary， 然后访问的时候freq 操作的是键，然后freq[x] 是值 #counters =sorted(counters, key =counters[1], reverse =True) uniques=sorted(freq,key=lambda x:freq[x],reverse=True) # 最后返回是一个list，只是按照value 进行排序，返回的是key的列表 return uniques[:k] 4Sum II Given four lists A, B, C, D of integer values, compute how many tuples (i, j, k, l) there are such that A[i] + B[j] + C[k] + D[l] is zero.To make problem a bit easier, all A, B, C, D have same length of N where 0 ≤ N ≤ 500. All integers are in the range of $-2^{28} $ to$ 2^{28} - 1 $ and the result is guaranteed to be at most $2^{31} - 1$. Tips: 思路和 2 sum 是一样，放到dictionary 中去。defaultdict 和dict 的唯一差别在于前者不用记性 key in dict 的判断。 12345678910111213141516171819202122232425class Solution(object): def fourSumCount(self, A, B, C, D): from collections import defaultdict length, dic, res =len(A), defaultdict(int), 0 for a in A: for b in B: dic[a+b] +=1 """ if a+b not in dic: dic[a+b] =1 else: dic[a+b] +=1 """ for c in C: for d in D: if -(c+d) in dic: res += dic[-(c+d)] return res Sliding Window Maximum Given an array nums, there is a sliding window of size k which is moving from the very left of the array to the very right. You can only see the k numbers in the window. Each time the sliding window moves right by one position. Return the max sliding window. Tips: 滑动窗口，然后窗口中的max(). 谁能想得到 python 是擅长处理 list，然后max() 函数就解决了呢 1234567891011121314class Solution(object): def maxSlidingWindow(self, nums, k): """ :type nums: List[int] :type k: int :rtype: List[int] """ if nums ==[]: return () res =[] for i in range(len(nums)-k +1): res.append(max(nums[i:i+k])) return res The Skyline Problem A city’s skyline is the outer contour of the silhouette formed by all the buildings in that city when viewed from a distance. Now suppose you are given the locations and height of all the buildings as shown on a cityscape photo (Figure A), write a program to output the skyline formed by these buildings collectively (Figure B). Tips: 关键点就是记录： 轮廓上升和轮廓下降的点，分别对应着 left 的上升和 right 的下降。评论区讲解，虽然比较难看懂 https://leetcode.com/problems/the-skyline-problem/ 12345678910111213141516171819202122232425262728293031323334from heapq import heappush, heappopclass Solution(object): def getSkyline(self, buildings): """ :type buildings: List[List[int]] :rtype: List[List[int]] """ events =[ (left, -height, right) for left, right, height in buildings] events += list((right, 0, 0) for _, right, _ in buildings) events.sort() # 先是按照left 升序排序，然后是 right 降序排序( 这个就是为什么时候 -right) res =[[0, 0]] # 最小堆，保存当前最高的轮廓 (-Height, right)， 使用-H 转换成最大堆，R 的作用是记录轮廓的有效长度 heap =[(0, float('inf'))] for left, height, right in events: # 如果轮廓上升 if height: heappush(heap, (height, right)) while heap[0][1] &lt;=left: heappop(heap) if res[-1][1] != -heap[0][0]: res += [[left, -heap[0][0]]] return res[1:] Wiggle Sort II Given an unsorted array nums, reorder it such that nums[0] &lt; nums[1] &gt; nums[2] &lt; nums[3]…. Tips: 算法题目被 python 中的lsit 操作给毁了, 可以学习以下 list[::-1], list[::2], 这种是模式化的操作，不是偶然。 12345678910111213141516class Solution(object): def wiggleSort(self, nums): """ :type nums: List[int] :rtype: None Do not return anything, modify nums in-place instead. """ #nums.sort() #half =len(nums)/2 #nums[::2], nums[1::2] =nums[:half][::-1], nums[half:][::-1] nums.sort() half = len(nums[::2]) # 注意这个是 half 必须是这样写的 # 这里面倒过来的原因， 前半个永远不大于后半个，所以这样能保证 波动 nums[::2], nums[1::2] = nums[:half][::-1], nums[half:][::-1] Find Peak Element A peak element is an element that is greater than its neighbors.Given an input array nums, where nums[i] ≠ nums[i+1], find a peak element and return its index. Input: nums = [1,2,3,1]Output: 2Explanation: 3 is a peak element and your function should return the index number 2. Tips: 二分查找， 如果是两个 if 那么就是两个步骤，如果 if else 那么就是一种选择。给定的条件中相邻的元素是不相同的。找到一个解进行了。。 https://leetcode.com/problems/find-peak-element/ 123456789101112131415161718192021class Solution(object): def findPeakElement(self, nums): """ :type nums: List[int] :rtype: int """ if not nums: return -1 if len(nums) ==1: return 0 left, right =0, len(nums)-1 while left &lt; right: mid =(left +right) /2 if nums[mid] &gt; nums[mid+1]: right =mid elif nums[mid] &lt; nums[mid+1]: left =mid +1 return left Find the Duplicate Number Given an array nums containing n + 1 integers where each integer is between 1 and n (inclusive), prove that at least one duplicate number must exist. Assume that there is only one duplicate number, find the duplicate one. Tips: 有两种思路。一种是二分法，一种是两个 pointer 的方法。后者类似linked list 中的操作。好好看看代码， fast =nums[nums[fast]] 这个操作就是 fast =fast.next.next 有木有很神奇的样子。 ···pythonclass Solution(object): # 感觉这个从时间和空间复杂度上限制的好多呀，如果满足这两个维度的，一般是先进行排序，O（nlgn） 时间，然后遍历找出重复的数字 # 基本上有两种思路，一种是 index(faster, slower point)， 一种是二分法 # 根据 indics 是有序的，然后使用二分查找 # The array is not sorted - but the indices of the array are sorted - #Insight &apos;&apos;&apos; def findDuplicate(self, nums): &quot;&quot;&quot; :type nums: List[int] :rtype: int &quot;&quot;&quot; if len(nums) == 0: return 0 low = 0 high = len(nums)-1 # 需要访问两个指针 while low &lt; high: mid = low + int((high-low)&gt;&gt;1) count = 0 for x in nums: if x &lt;= mid: count = count + 1 if count &gt; mid: high = mid else: low = mid+1 return low &apos;&apos;&apos; # 这两种方法的根本依据是 长度为n 包含n+1 个整数，并且只有一个 duplicate def findDuplicate(self, nums): slow = fast = finder = 0 while True: slow = nums[slow] fast = nums[nums[fast]] if slow == fast: while finder != slow: finder = nums[finder] slow = nums[slow] return finder 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960** Count of Smaller Numbers After Self**&gt; You are given an integer array nums and you have to return a new counts array. The counts array has the property where counts[i] is the number of smaller elements to the right of nums[i].&gt; Input: [5,2,6,1]Output: [2,1,1,0] Explanation:To the right of 5 there are 2 smaller elements (2 and 1).To the right of 2 there is only 1 smaller element (1).To the right of 6 there is 1 smaller element (1).To the right of 1 there is 0 smaller element.Tips: 这个本身的应用还是挺有意思的。python 中的库函数bisort (binary sort) 了解一下。逆序遍历，找到合适的位置，插进去，然后index 计数。```pythonclass Solution(object): # The problem is equal to find each number&apos;s inversion count. Actually there are three kinds of solutions: BST, mergeSort, and BITree. While the first two answer&apos;s time complexity is O(nlogn), and BITree time comlexity is O( nlog(maximumNum) ). # 太难了 # 两种解法 &apos;&apos;&apos; def merge(self,left,right,res): i,j=0,0 new_array=[] while i&lt;len(left) and j&lt;len(right): if left[i][1]&gt;right[j][1]: new_array+=[left[i]] res[left[i][0]]+=len(right)-j i+=1 else: new_array+=[right[j]] j+=1 new_array+=left[i:] new_array+=right[j:] return new_array def merge_sort(self,nums,res): if len(nums)&lt;2: return nums mid=len(nums)//2 left=self.merge_sort(nums[:mid],res) right=self.merge_sort(nums[mid:],res) return self.merge(left,right,res) def countSmaller(self, nums): res=[0]*len(nums) self.merge_sort([(i,num) for i,num in enumerate(nums)],res) return res &apos;&apos;&apos; def countSmaller(self, nums): count,sorted=[],[] for num in nums[::-1]: index=bisect.bisect_left(sorted,num) sorted.insert(index,num) count+=[index] return count[::-1] Longest Consecutive Sequence Given an unsorted array of integers, find the length of the longest consecutive elements sequence.Your algorithm should run in O(n) complexity. Tips: 这种解题的方式，是比较新颖的。 12345678910111213141516171819class Solution(object): # 第一印象是 先进行排序， 然后选择的过程， 但是限制条件是 O(n) 的复杂度 def longestConsecutive(self, nums): """ :type nums: List[int] :rtype: int """ nums =set(nums) best =0 # 这种方式真的很简洁 for x in nums: if x-1 not in nums: y =x+1 while y in nums: y +=1 best =max(best, y-x) return best House Robber Tips: dp 的思想运用到极致就是这个样子。使用两个变量句可以搞定。 12345678910111213141516class Solution(object): """ f(0) = nums[0] f(1) = max(num[0], num[1]) f(k) = max( f(k-2) + nums[k], f(k-1) ) """ def rob(self, nums): """ :type nums: List[int] :rtype: int """ # dp 有时候就能这样优化到使用两个变量 last, now =0, 0 for num in nums: last, now =now, max(last +num, now) return now Longest Increasing Subsequence Given an unsorted array of integers, find the length of longest increasing subsequence. Tips: f(i) 表示以 nums[i]为结尾的 longest encreasing subsequence( 第二个for 对比的对象是 f[:i] 使用j 进行遍历 ) 12345678910111213141516171819202122class Solution(object): # 求解最值 唯一解都是可以使用这样的方式的哦 # 不能使用 in 那种骚操作了， 只能踏踏实实的 dp # 这个版本的dp 没有优化好 def lengthOfLIS(self, nums): """ :type nums: List[int] :rtype: int """ if not nums: return 0 dp = [1] * len(nums) """ """ for i in range(len(nums)): for j in range(i): if nums[i] &gt; nums[j]: dp[i] = max(dp[i], dp[j] + 1) return max(dp) Coin Change You are given coins of different denominations and a total amount of money amount. Write a function to compute the fewest number of coins that you need to make up that amount. If that amount of money cannot be made up by any combination of the coins, return -1. Tips: dp问题 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647from sys import maxintclass Solution(object): """ :type coins: List[int] :type amount: int :rtype: int """ ''' # 这种方法不行 if len(coins) ==1: if amount % coins[0] ==0: return amount /coins[0] else: return -1 coins.sort(reverse =True) res =0 for i in range(len(coins)): res += amount /coins[i] amount %= coins[i] return res # table 作为dp， table[i] 表示前i 个数字使用数量最少的硬币能够 表示 # 多看几遍就能理解了 def coinChange(self, coins, amount): table = [0]*(amount + 1) for i in range(1, amount+1): minimum = maxint # 有好几种对于最小值和最大的初始化了 for j in coins: if i &gt;= j and table[i-j] != -1: minimum = min(minimum, table[i-j] + 1) table[i] = -1 if minimum == maxint else minimum return table[amount] ''' # python3 不能使用python2,python2 能使用 python3? def coinChange(self, coins, amount): table = [0 ] *(amount + 1) for i in range(1, amount +1): #minimum =maxint minimum =float('inf') for j in coins: if i &gt;= j and table[ i -j] != -1: minimum = min(minimum, table[ i -j] + 1) table[i] = -1 if minimum == float('inf') else minimum return table[amount]]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode- Array]]></title>
    <url>%2F2019%2F06%2F16%2Fleetcode-array%2F</url>
    <content type="text"><![CDATA[LeetCode 刷题总结（一）， 使用Python 实现。该篇题目类型主要是： array 和string 的相关处理。 Two Sum Given an array of integers, find two numbers such that they add up to a specific target number. Tips： 返回的是 index，所以 dict 中存储的 (num, index) 这样的组合, 是两个不相同的数字的index https://leetcode.com/problems/two-sum/ 12345678910111213class Solution(object): # 不能重复使用一个，return index def twoSum(self, nums, target): """ :type nums: List[int] :type target: int :rtype: List[int] """ cache = &#123;&#125; for i in range(len(nums)): if target - nums[i] in cache and cache[target - nums[i]] != i: # if else 用的是比较简洁的 return [cache[target - nums[i]], i] cache[nums[i]] = i Median of Two Sorted Arrays There are two sorted arrays nums1 and nums2 of size m and n respectively. Find the median of the two sorted arrays. The overall run time complexity should be O(log (m+n)). Tips: 先是 merge，然后选择 median，常规做法，时间0(mn)，不是最优的，还可以达到O(min(m, n) ) 这样的时间复杂度 123456789101112131415161718192021222324252627282930313233343536373839class Solution(object): # 使用 merge() 操作，然后根据，然后取得中位数 def median(self, nums): len_n =len(nums) if len_n &amp;1 ==1: return nums[len_n//2] else: return float(nums[len_n//2]+nums[len_n//2-1])/2 def findMedianSortedArrays(self, nums1, nums2): """ :type nums1: List[int] :type nums2: List[int] :rtype: float """ res =[] if not nums1 or not nums2: res =nums1 or nums2 return self.median(res) else: left, right =0, 0 len_1, len_2 =len(nums1)-1, len(nums2)-1 while left&lt;= len_1 and right &lt;=len_2: if nums1[left] &lt;nums2[right]: res.append(nums1[left]) left +=1 else: res.append(nums2[right]) right +=1 if left &lt;= len_1: res.extend(nums1[left:]) if right &lt;=len_2: res.extend(nums2[right:]) return self.median(res) ZigZag Conversion The string “PAYPALISHIRING” is written in a zigzag pattern on a given number of rows like this: (you may want to display this pattern in a fixed font for better legibility) P A H NA P L S I I GY I R And then read line by line: “PAHNAPLSIIGYIR” Tips: 字符串处理 12345678910111213141516171819202122class Solution(object): def convert(self, s, numRows): """ :type s: str :type numRows: int :rtype: str """ if numRows == 1 or numRows &gt;= len(s): # string of list return s L = [''] * numRows # string of list row, step = 0, 1 for x in s: L[row] += x if row == 0: step = 1 elif row == numRows -1: step = -1 row += step return ''.join(L) # array (list) 转成string 常用的方法 Container With Most Water Given n non-negative integers a1, a2, …, an , where each represents a point at coordinate (i, ai). n vertical lines are drawn such that the two endpoints of line i is at (i, ai) and (i, 0). Find two lines, which together with x-axis forms a container, such that the container contains the most water. The above vertical lines are represented by array [1,8,6,2,5,4,8,3,7]. In this case, the max area of water (blue section) the container can contain is 49. Tips: 左右双指针问题。首先移动高度较小的点，因为两者的距离是肯定变小，如果移动高度大的，那么最后的面积肯定变小；但是如果 移动高度较小的点，那么最后的面积是有可能变大的。所以这个是一个可能性的东西。 1234567891011121314151617181920212223class Solution(object): def maxArea(self, height): len_h = len(height) if len_h == 1: return 0 max_area = 0 left = 0 right = len_h - 1 # left, right =0, len_h -1 while left &lt; right: if height[left] &lt; height[right]: area = (right - left) * height[left] left += 1 else: area = (right - left) * height[right] right -= 1 if area &gt; max_area: max_area = area return max_area Longest Common Prefix Write a function to find the longest common prefix string amongst an array of strings.If there is no common prefix, return an empty string “”. Input: [&quot;flower&quot;,&quot;flow&quot;,&quot;flight&quot;] Output: &quot;fl&quot; Tips: 一个个寻找交集，最朴素的想法、 123456789101112131415161718192021222324252627282930313233class Solution(object): # 时间复杂度是 O(N^2), N=len(strs), 笼统的说 def longestCommonPrefix(self, strs): """ :type strs: List[str] :rtype: str """ if not strs: return "" n =len(strs) if n ==0: return "" elif n ==1: return strs[0] predix =strs[0] for s in strs[1:]: # 这种结构见过了，就是不断迭代，不断地的去寻找 ”交集“ predix =self.findPrefix(predix, s) if "" ==predix: break return predix def findPrefix(self, s1, s2): min_len =min(len(s1), len(s2)) # 这个if 和 return 写的都是很巧妙的 for i in range(0, min_len): if s1[i] != s2[i]: return s1[:i] return s1[:min_len] Remove Duplicates from Sorted Array Given a sorted array nums, remove the duplicates in-place such that each element appear only once and return the new length.Do not allocate extra space for another array, you must do this by modifying the input array in-place with O(1) extra memory. Tips: 前后两个指针覆盖的思想，最后返回的是index，如果发现了后者覆盖前者 12345678910111213141516171819202122class Solution(object): def removeDuplicates(self, nums): """ :type nums: List[int] :rtype: int """ if None == nums: return 0 len_n = len(nums) if len_n &lt;= 1: return len_n m = 0 n = 1 while n &lt; len_n: if nums[m] != nums[n]: m += 1 if m != n: nums[m] = nums[n] n += 1 return m + 1 Remove Element Given an array nums and a value val, remove all instances of that value in-place and return the new length.Do not allocate extra space for another array, you must do this by modifying the input array in-place with O(1) extra memory. Tips: in-place 表示不能创建数组，可以使用（临时）变量。通过双指针进行处理，想想为什么可以使用这么简洁的代码进行处理呢。m n 分别从左到右、从右到左进行遍历，将和 val 相同的元素都放到右边，不相同的放到左边 1234567891011121314151617181920212223242526class Solution(object): def removeElement(self, nums, val): """ :type nums: List[int] :type val: int :rtype: int """ if not nums: return 0 len_n =len(nums) m,n =0, len_n-1 # 注意跳出条件，遍历的方向和跳出条件是相关的, 这个 等号取于不取 一是比较难把握，可以具体带个值 while m &lt;=n: if val ==nums[m]: if val !=nums[n]: # 这个是不能使用 while 找，因为有比较多的case 需要考虑，所以使用 if 进行单步操作 nums[m], nums[n] =nums[n], nums[m] m +=1 n -=1 else: n -=1 else: m +=1 return m # 因为 m 是从0开始的 Search in Rotated Sorted Array Suppose an array sorted in ascending order is rotated at some pivot unknown to you beforehand.(i.e., [0,1,2,4,5,6,7] might become [4,5,6,7,0,1,2]).You are given a target value to search. If found in the array return its index, otherwise return -1. Tips：一定是 二分思想，关键是判断 增序列 和 target 的关系，所以有两层 if 判断条件，一层是增序列， 一层是 target 是否在增序列下面这个观点是要有的： 整个数组由两个有序的子序列构成，且左子序列中的每个元素都&gt;,右子序列中的每个元素。 123456789101112131415161718192021222324class Solution(object): # @param nums, a list of integers # @param target, an integer to be searched # @return an integer def search(self, nums, target): left = 0; right = len(nums) - 1 # 这个 == 是不容易进行取舍的， while left &lt;= right: mid = (left + right) / 2 if target == nums[mid]: return mid # 我当时面试的时候就是这种思路，一定要有条理就是了 # 左边是增序列 if nums[mid] &gt;= nums[left]: if target &lt; nums[mid] and target &gt;= nums[left]: right = mid - 1 else: left = mid + 1 elif nums[mid] &lt; nums[right]: if target &gt; nums[mid] and target &lt;= nums[right]: left = mid + 1 else: right = mid - 1 return -1 Search Insert Position Given a sorted array and a target value, return the index if the target is found. If not, return the index where it would be if it were inserted in order.You may assume no duplicates in the array. Tips： 二分查找，之前是found or not found，现在如果没有找见返回的是 index，没有什么本质区别。 1234567891011121314151617181920212223242526272829class Solution(object): def searchInsert(self, nums, target): """ :type nums: List[int] :type target: int :rtype: int """ if not nums: return None len_n =len(nums) if nums[0] &gt; target: return 0 if nums[-1] &lt; target: return len_n left, right =0, len_n-1 while left&lt;=right: mid =(left +right)/2 if nums[mid] ==target: return mid elif nums[mid] &lt;target: left =mid +1 else: right =mid -1 return left Rotate Image You are given an n x n 2D matrix representing an image.Rotate the image by 90 degrees (clockwise). Given input matrix =[ [1,2,3], [4,5,6], [7,8,9]],rotate the input matrix in-place such that it becomes:[ [7,4,1], [8,5,2], [9,6,3]] Tips: 好好观察四个等式，就是行变列，然后其他的一个坐标是对称的，这个就是旋转 90度；然后有五个变量（有重复的） 1234567891011 class Solution(object): def rotate(self, matrix): n = len(matrix) if 1 == n: return round = int(n / 2) for x in range(0, round): for y in range(x, n - x - 1): matrix[n - y - 1][x], matrix[n - x - 1][n - y - 1], matrix[y][n - x - 1], matrix[x][y] = matrix[n - x - 1][n - y - 1], matrix[y][n - x - 1], matrix[x][y], matrix[n - y - 1][x] Spiral Matrix Given a matrix of m x n elements (m rows, n columns), return all elements of the matrix in spiral order.螺旋形 Input:[ [ 1, 2, 3 ], [ 4, 5, 6 ], [ 7, 8, 9 ]]Output: [1,2,3,6,9,8,7,4,5] Tips: 在处理”行“ 信息的时候，是可以数组切割的。在处理列信息的时候，需要一个个append() 123456789101112131415161718192021222324252627282930313233343536class Solution(object): # 这个不是跟那个 剑指offer 中的顺时针打印输出一样吗， # 这个版本是比较容易理解，所以选择这个版本 def spiralOrder(self, matrix): if matrix ==[]: return [] top, bottom =0, len(matrix)-1 left, right =0, len(matrix[0])-1 res =[] # 当有一个等于的时候就应该跳出来了 while left &lt;right and top&lt; bottom: # 对于行 处理python 是有比较简单的方式的 res += matrix[top][left: right+1] for x in range(top+1, bottom): res.append(matrix[x][right]) res += matrix[bottom][left:right+1][::-1] # 倒叙 for x in range(bottom-1, top, -1): res.append(matrix[x][left]) top, bottom, left, right =top+1, bottom-1, left+1, right-1 if top ==bottom: res += matrix[top][left:right] elif left ==right: for x in range(top, bottom+1): res.append(matrix[x][right]) return res Spiral Matrix II Given a positive integer n, generate a square matrix filled with elements from 1 to $n^2$ in spiral order. Tips : 注意边角的细节。初始化赋值的应该是常见的操作，这里的cur 是比较核心的东西。 123456789101112131415161718192021222324252627282930313233343536class Solution(object): # version 1 是遍历获取， version 2 是填充。这个真是有趣的东西 # 还是设置上下左右四个坐标进行遍历的处理 def generateMatrix(self, n): ans =[ [0] *n for _ in range(n)] top, bottom, left, right =0, n-1, 0, n-1 cur =1 while left &lt;= right and top &lt;= bottom: for i in range(left, right+1): ans[top][i] =cur cur +=1 top +=1 # 根据问题需求，是可以在题目中 设置这种break，不需要等到 while 的判断 if top &gt; bottom: break for i in range(top, bottom+1): ans[i][right] =cur cur +=1 right -=1 if left &gt; right: break # 好好体会这个连接点的处理，左边是能够访问到的，右边为了能够访问到 # 进行了 -1 的操作 for i in range(right, left-1, -1): ans[bottom][i] =cur cur +=1 bottom -=1 if bottom &lt;top: break for i in range(bottom, top-1, -1): ans[i][left] =cur cur +=1 left +=1 return ans Merge Intervals Given a collection of intervals, merge all overlapping intervals. Tips: 需要区分区间的start 和end 点，分别使用 (0 1) 进行区分，然后 sort() ，那么那么start 就出现了最前面，end 就出现了最后面。默认的sort 是先按照 第一个元素排序，然后按照第二个元素排序，所以标识 (0, 1) 这个是没有收到影响的。 12345678910111213141516171819202122232425class Solution(object): def merge(self, intervals): if not intervals: return [] data = [] for interval in intervals: data.append((interval[0], 0)) data.append((interval[1], 1)) data.sort() merged = [] stack = [data[0]] for i in range(1, len(data)): d = data[i] if d[1] == 0: # this is a lower bound, push this onto the stack stack.append(d) elif d[1] == 1: if stack: start = stack.pop() if len(stack) == 0: # we have found our merged interval merged.append( (start[0], d[0])) return merged Length of Last Word Given a string s consists of upper/lower-case alphabets and empty space characters ‘ ‘, return the length of last word in the string.If the last word does not exist, return 0. Tips: 不能使用 split() 因为太多的case需要单独的处理，所以应该使用字母为基本，一个个处理。等不等于 ‘ ‘进行的切分。 1234567891011121314151617class Solution(object): # 这个是需要从 字母角度考虑，而不是从单词角度考虑 def lengthOfLastWord(self, s): len_s =len(s) if 0==len_s: return 0 index =len_s -1 # 找到第一个不是 ' '的字母 while index&gt;=0 and ' ' ==s[index]: index -=1 len_last_word =0 while index &gt;=0 and ' ' != s[index]: index -=1 len_last_word +=1 return len_last_word Valid Number Validate if a given string can be interpreted as a decimal number. Tips :对于小数(decimal ) 各种 case 的熟知程度 123456789101112131415161718192021222324252627class Solution(object): def isNumber(self, s): """ :type s: str :rtype: bool """ s = s.strip() digits = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'] met_dot = met_e = met_digit = False for i, char in enumerate(s): if char in ['+', '-']: if i &gt; 0 and s[i-1] != 'e': return False elif char == '.': if met_dot or met_e: return False met_dot = True elif char == 'e': if met_e or not met_digit: return False met_e, met_digit = True, False #elif char.isdigit(): elif char in digits: met_digit = True else: return False return met_digit Plus One Given a non-empty array of digits representing a non-negative integer, plus one to the integer.The digits are stored such that the most significant digit is at the head of the list, and each element in the array contain a single digit.You may assume the integer does not contain any leading zero, except the number 0 itself. Input: [1,2,3] Output: [1,2,4] Explanation: The array represents the integer 123. Tips: 重点在于加法的处理，一般使用 求余得到digit，然后使用carry 得到进位。 1234567891011121314151617181920class Solution(object): def plusOne(self, digits): """ :type digits: List[int] :rtype: List[int] """ len_s =len(digits) carry =1 for i in range(len_s-1, -1, -1): total =digits[i] +carry digit =int(total %10) carry =int(total //10) digits[i] =digit # 这个是最后一个进位 if carry ==1: digits.insert(0, 1) return digits Simplify Path Given an absolute path for a file (Unix-style), simplify it. Or in other words, convert it to the canonical path. Tips： 12345678910111213141516171819class Solution(object): # 这个从考点上是 stack，但是使用python字符串处理更好 # 按照 '/' 进行split() def simplifyPath(self, path): """ :type path: str :rtype: str """ stack = [] for token in path.split('/'): if token in ('', '.'): pass # continue 这两个是一样的效果， pass 就类似一种占位符，在测试的时候常见 elif token == '..': if stack: stack.pop() else: stack.append(token) return '/' + '/'.join(stack) Edit Distance Given two words word1 and word2, find the minimum number of operations required to convert word1 to word2. Tips: 动态规划就通过存储子问题结果来加快运算，但一个好的动态规划算法会尽量减少空间复杂度。 然后是可以继续优化的，使用 O(n) 的空间的复杂度. 真正的写出来之后，发现代码是比想法更加简单的。 提供了两种解法，第一种比较常规 dp，比较容易理解。123456789101112131415161718192021222324class Solution(object): def minDistance(self, word1, word2): """ :type word1: str :type word2: str :rtype: int """ m = len(word1) n = len(word2) table = [[0] * (n + 1) for _ in range(m + 1)] for i in range(m + 1): table[i][0] = i for j in range(n + 1): table[0][j] = j for i in range(1, m + 1): for j in range(1, n + 1): if word1[i - 1] == word2[j - 1]: table[i][j] = table[i - 1][j - 1] else: table[i][j] = 1 + min(table[i - 1][j], table[i][j - 1], table[i - 1][j - 1]) return table[-1][-1] 第二种就是参考一下吧。 1234567891011121314151617class Solution(object): # 从实现的角度讲，这个是需要把握住有一个 word的index 是不变的 def minDistance(self, word1, word2): l1, l2 = len(word1)+1, len(word2)+1 pre = [0 for _ in range(l2)] for j in range(l2): pre[j] = j for i in range(1, l1): cur = [i]*l2 for j in range(1, l2): cur[j] = min(cur[j-1]+1, pre[j]+1, pre[j-1]+(word1[i-1]!=word2[j-1])) #pre = cur[:] pre =cur return pre[-1] Set Matrix Zeroes Given a m x n matrix, if an element is 0, set its entire row and column to 0. Do it in-place. Tips: 使用第一行和第一列作为标记，使用 0作为标记。 123456789101112131415161718192021class Solution(object): def setZeroes(self, matrix): """ :type matrix: List[List[int]] :rtype: None Do not return anything, modify matrix in-place instead. """ firstRowHasZero = not all(matrix[0]) # all() 只有所有的不为0 返回的才不为0，否则返回0 for i in range(1,len(matrix)): for j in range(len(matrix[0])): if matrix[i][j] == 0: # 这种遍历并标记的方法还是比较优秀的 matrix[0][j] = 0 matrix[i][0] = 0 for i in range(1,len(matrix)): for j in range(len(matrix[0])-1,-1,-1): # 注意是从后往前标记的 if matrix[0][j] == 0 or matrix[i][0] == 0: matrix[i][j] = 0 if firstRowHasZero: matrix[0] = [0]*len(matrix[0]) #最后处理第一行 Remove Duplicates from Sorted List Given a sorted linked list, delete all duplicates such that each element appear only once. Tips: 注意这道题和上面有道题是有差别的，这个是 delete all duplicates 1234567891011121314151617181920212223# Definition for singly-linked list.# class ListNode(object):# def __init__(self, x):# self.val = x# self.next = Noneclass Solution(object): """ 使用两个 while 是因为，逻辑上简单 """ def deleteDuplicates(self, head): """ :type head: ListNode :rtype: ListNode """ cur =head while cur: while cur.next and cur.val == cur.next.val: cur.next =cur.next.next # skip duplicates cur =cur.next return head Merge Sorted Array Given two sorted integer arrays nums1 and nums2, merge nums2 into nums1 as one sorted array. The number of elements initialized in nums1 and nums2 are m and n respectively. You may assume that nums1 has enough space (size that is greater or equal to m + n) to hold additional elements from nums2. Tips: 题目中说了 nums1 是不会出现 index 访问报错的。从后往前遍历，因为这个是要求 merge 2 into 1的。 1234567891011121314151617181920212223242526class Solution(object): def merge(self, nums1, m, nums2, n): """ :type nums1: List[int] :type m: int :type nums2: List[int] :type n: int :rtype: None Do not return anything, modify nums1 in-place instead. """ i, j, k =m-1, n-1, m+n-1 while i &gt;=0 and j&gt;=0: if nums1[i] &gt; nums2[j]: nums1[k] =nums1[i] i -=1 else: nums1[k] =nums2[j] j -=1 k -=1 #import ipdb #ipdb.set_trace() # 如果这是 if 那么使用的就是字符串的切割，如果是while 那么就是一个个操作 if j&gt;=0: nums1[:k+1] =nums2[:j+1] Restore IP Addresses Given a string containing only digits, restore it by returning all possible valid IP address combinations. Tips： 细节比较多，在进行 dfs 的时候 123456789101112131415161718192021222324252627282930class Solution(object): def restoreIpAddresses(self, s): """ :type s: str :rtype: List[str] """ ans = [] self.helper(ans, s, 4, []) # ans 中的item 之间使用 . 进行隔开，这种技术，是非常常见的 # 这个是 list of list， 然后转换成了 list of string return ['.'.join(x) for x in ans] def helper(self, ans, s, k, temp): if len(s) &gt; k * 3: return if k == 0: #ans.append(temp[:]) ans.append(temp) else: for i in range(min(3, len(s) - k + 1)): # s 是一个字符串，当只有一位时，0可以成某一段，如果有两位或三位时，像 00， 01， 001， 011， 000等都是不合法的， # 只能是 0.1.0.0 而不能是00.1.0.0 ，这个是ip语法 # 这个是有连个并列的判断条件 if i == 2 and int(s[:3]) &gt; 255 or i &gt;0 and s[0] =='0': continue self.helper(ans, s[i + 1:], k - 1, temp + [s[:i + 1]]) Interleaving String Given s1, s2, s3, find whether s3 is formed by the interleaving of s1 and s2. Tips： interleaving 插入；s3 是否能够用s1 和s2 组成, len(s1) +len(s2) == len(s3) 这个样子的。行列分别表示s1 和s2 中的字母，然后 (x, y) 值表示当前的能够”走“ 通的路径。 12345678910111213141516171819202122232425262728293031class Solution(object): """ interleaving, 判断s3是否由s1和s2交叉构成， """ def isInterleave(self, s1, s2, s3): """ :type s1: str :type s2: str :type s3: str :rtype: bool """ r, c, l =len(s1), len(s2), len(s3) if r+c !=l: return False # 0 行和 0列的初始化，使用 true or false 来进行表示结果 dp =[ [True] * (c+1) for _ in range(r+1)] for i in range(1, r+1): dp[i][0] =dp[i-1][0] and s1[i-1] == s3[i-1] for j in range(1, c+1): dp[0][j] =dp[0][j-1] and s2[j-1] ==s3[j-1] # 看到代码之后觉得很简单， for i in range(1, r+1): for j in range(1, c+1): dp[i][j] = dp[i-1][j] and s1[i-1] ==s3[i+j-1] or dp[i][j-1] and s2[j-1] ==s3[i+j-1] return dp[-1][-1] 方法二：12345678910111213141516# 从运行的结果来说，内存下降了0.1M， 但是这个时间却商城了def isInterleave(self, s1, s2, s3): r, c, l= len(s1), len(s2), len(s3) if r+c != l: return False dp = [True for _ in range(c+1)] for j in range(1, c+1): dp[j] = dp[j-1] and s2[j-1] == s3[j-1] for i in range(1, r+1): dp[0] = (dp[0] and s1[i-1] == s3[i-1]) for j in range(1, c+1): dp[j] = (dp[j] and s1[i-1] == s3[i-1+j]) or (dp[j-1] and s2[j-1] == s3[i-1+j]) return dp[-1] Pascal’s Triangle Given a non-negative integer numRows, generate the first numRows of Pascal’s triangle. Tips: 这个是小学数学题，变成了编程题、对应好index 进行了。最后 res 可能不是正三角形（直角三角形）但一定是可以这样做的。 123456789101112131415class Solution(object): def generate(self, numRows): """ :type numRows: int :rtype: List[List[int]] """ res = [[1 for _ in range(i+1)] for i in range(numRows)] for i in range(2, numRows): for j in range(1, i): # 这个就是一个数学问题 # 就是上一行(i-1) 的 j-1 和j 元素的相加 res[i][j] =res[i-1][j-1] + res[i-1][j] return res Given a non-negative index k where k ≤ 33, return the kth index row of the Pascal’s triangle.Note that the row index starts from 0. Tips： 相比于上一个，这个只是返回了最后一行。 123456789101112131415class Solution(object): # 相对比上一个，只是输出最后一行的信息， rowIndex def getRow(self, rowIndex): """ :type rowIndex: int :rtype: List[int] """ res =[ [1 for _ in range(i+1) ] for i in range(rowIndex+1)] for i in range(2, rowIndex+1): for j in range(1, i): res[i][j ] = res[i-1][j] + res[i-1][j-1] return res[-1] Valid Palindrome Given a string, determine if it is a palindrome, considering only alphanumeric characters and ignoring cases.Note: For the purpose of this problem, we define empty string as valid palindrome. Input: &quot;A man, a plan, a canal: Panama&quot; Output: true Tips： 建议使用 is.alnum() 这个python 中自带的函数，因为这种判断还是挺常见的。回文数。先是预处理，然后才是 lower() 判断。 1234567891011121314151617181920212223242526class Solution(object): # palindrome 回文数， alphanumeric ， 字母与数字并用的; # 预处理之后，然后比较前后两个字符的异同 # Python isalnum() 方法检测字符串是否由字母和数字组成，这种函数只有在 歪果仁的代码中常见 # s[i] &gt;= 'a' and s[i] &lt;= 'z' or s[i] &gt;= '0' and s[i] &lt;= '9' or s[i] &gt;= 'A' and s[i] &lt;= 'Z':, 这个是国人的写法 # a=''.join([x for x in s if x.isalpha() or x.isdigit()]).lower() 或者这样 # 喜欢写源码 def isPalindrome(self, s): """ :type s: str :rtype: bool """ left, right =0, len(s)-1 while left &lt; right: while left &lt; right and not s[left].isalnum(): left +=1 while left &lt; right and not s[right].isalnum(): right -=1 if s[left].lower() != s[right].lower(): return False left +=1 right -=1 return True Gas Station There are N gas stations along a circular route, where the amount of gas at station i is gas[i].You have a car with an unlimited gas tank and it costs cost[i] of gas to travel from station i to its next station (i+1). You begin the journey with an empty tank at one of the gas stations.Return the starting gas station’s index if you can travel around the circuit once in the clockwise direction, otherwise return -1. Input:gas = [1,2,3,4,5]cost = [3,4,5,1,2]Output: 3Explanation:Start at station 3 (index 3) and fill up with 4 unit of gas. Your tank = 0 + 4 = 4Travel to station 4. Your tank = 4 - 1 + 5 = 8Travel to station 0. Your tank = 8 - 2 + 1 = 7Travel to station 1. Your tank = 7 - 3 + 2 = 6Travel to station 2. Your tank = 6 - 4 + 3 = 5Travel to station 3. The cost is 5. Your gas is just enough to travel back to station 3.Therefore, return 3 as the starting index. Tips: 看着挺吓人的，但是落实到代码上，就是一个循环，当不能出发时， r (rest) 是为0，然后寻求下一个可以出发的点。 123456789101112class Solution(object): def canCompleteCircuit(self, gas, cost): if sum(gas) &lt; sum(cost): return -1 index, rest = 0, 0 for i in range(len(gas)): if gas[i] + rest &lt; cost[i]: #这个是需要遍历整个 gas的，因为有可能开始行但是后来不行，所以开始的index 还是无法完成整个遍历 index = i +1 rest = 0 else: rest += gas[i] - cost[i] return index Evaluate Reverse Polish Notation Evaluate the value of an arithmetic expression in Reverse Polish Notation.Valid operators are +, -, *, /. Each operand may be an integer or another expression. Division between two integers should truncate toward zero. The given RPN expression is always valid. That means the expression would always evaluate to a result and there won&apos;t be any divide by zero operation. Tips： 术语，逆波兰表达式（操作数在前，操作符在后）的一种形式。栈是存储操作数 和运算结果的。对于负数不能整除的，向着 0 靠拢 1234567891011121314151617181920212223242526272829class Solution(object): # 计算逆波兰表达式：把操作数放前面，把操作符后置的一种写法 # 这个明显就是 栈的使用呀，两个栈， def evalRPN(self, tokens): """ :type tokens: List[str] :rtype: int """ stack =[] for t in tokens: if t not in "+-*/": stack.append(int(t)) else: right, left =stack.pop(), stack.pop() if t =="+": stack.append(left +right) elif t =="-": stack.append(left-right) elif t =="*": stack.append(left*right) else: # case like 1/(-2) 负数且不能整除 if left*right &lt;0 and left % right!=0: stack.append(left/right +1) else: stack.append(left/right) return stack.pop() Reverse Words in a String Given an input string, reverse the string word by word. Tips： 正常的思路是第一次全翻转，第二次按照 word 进行翻转。但是python 十分擅长 字符串的处理。 123456789class Solution(object): # 两次翻转。第一次是全翻转，然后第二次是word 翻转 # 字符串类型的算法题目，使用python 是无法get 到算法层面的 hahaha def reverseWords(self, s): """ :type s: str :rtype: str """ return " ".join(s.split()[::-1]) Search a 2D Matrix Write an efficient algorithm that searches for a value in an m x n matrix. This matrix has the following properties: Integers in each row are sorted from left to right. The first integer of each row is greater than the last integer of the previous row. Tips: 注意第二个条件，下一行的开头是大于上一行的末尾，所以如果 dense 一下，是可以看成大的有序，所以思路就是二叉排序。 https://leetcode.com/problems/search-a-2d-matrix/ Input:matrix = [ [1, 3, 5, 7], [10, 11, 16, 20], [23, 30, 34, 50]]target = 3Output: true 123456789101112131415161718192021222324252627class Solution(object): def searchMatrix(self, matrix, target): """ :type matrix: List[List[int]] :type target: int :rtype: bool """ # 如果写成 not target 是有问题，当 target ==0 的时候，这个是不成立的，所以需要看一下数据的范围 # 注意区分 if not matrix or target ==None: return False rows, cols=len(matrix), len(matrix[0]) low, high =0, rows*cols-1 # 总的二叉 while low &lt;=high: mid =(low +high) /2 num =matrix[mid/cols][mid%cols] if num ==target: return True elif num&lt; target: low =mid +1 else: high =mid -1 return False Search a 2D Matrix II Write an efficient algorithm that searches for a value in an m x n matrix. This matrix has the following properties: Integers in each row are sorted in ascending from left to right. Integers in each column are sorted in ascending from top to bottom. [ [1, 4, 7, 11, 15], [2, 5, 8, 12, 19], [3, 6, 9, 16, 22], [10, 13, 14, 17, 24], [18, 21, 23, 26, 30]] Tips: 注意区分和上一道题目的第二点的区别。这个只能是一步步走，下面的程序是从右上方开始走，如果 target 大则向下直走，否则左走。可以有两种初始化方式，一种是右上角，一种是左下角。 https://leetcode.com/problems/search-a-2d-matrix-ii/ 1234567891011121314151617181920212223242526272829303132333435class Solution(object): def searchMatrix(self, matrix, target): """ :type matrix: List[List[int]] :type target: int :rtype: bool """ # 一开始的时候不知道使用什么遍历方式，因为 for 好像不太行，应该使用 while 基于条件遍历 if matrix ==[]: return False rows, cols =len(matrix)-1, len(matrix[0])-1 """ row, col = 0, cols # start points while row &lt;= rows and col &gt;= 0: if matrix[row][col] == target: return True elif matrix[row][col] &lt; target: row +=1 else: col -=1 """ # 还有一种初始化方式 row, col =rows, 0 while row &gt;=0 and col &lt;= cols: if matrix[row][col] ==target: return True elif matrix[row][col] &lt; target: col +=1 else: row -=1 return False Kth Smallest Element in a Sorted Matrix Given a n x n matrix where each of the rows and columns are sorted in ascending order, find the kth smallest element in the matrix. Tips: 主要是看到 example 中的数据，有两种类型，一种是可以把matrix dense 之后依然是有序，另一种不是。这个是属于前者。下面这种解法比较新颖，使用heapq 进行操作，遍历k th 就得到了kth 最小。 ···pythonclass Solution(object): # 有一种方法是初始化为右上角，然后小往左走，大往下走 # 这个默认求解的k smallest，所以python 中heapq 默认也是小根堆，所以 def kthSmallest(self, matrix, k): &quot;&quot;&quot; :type matrix: List[List[int]] :type k: int :rtype: int &quot;&quot;&quot; heap, res, n =[(matrix[0][0], 0, 0)], 0, len(matrix) for k in range(1, k+1): # 这个是次数 res, row, col =heapq.heappop(heap) # 问题在于这里并没有体现了 row col相应的变化 +1 这类东西 # 这个是通过 heapq 不断地push 和pop 来得到相应的 row col 然后进行res 的获取的 if not row and col&lt; n-1: heapq.heappush(heap, (matrix[row][col+1], row, col+1)) if row&lt; n-1: heapq.heappush(heap, (matrix[row+1][col], row+1, col)) return res 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748** Evaluate Reverse Polish Notation**&gt; Evaluate the value of an arithmetic expression in Reverse Polish Notation.Valid operators are +, -, *, /. Each operand may be an integer or another expression.Tips: 逆波兰又称之为后缀表达式，操作符置于操作数后面，这个前后是以“操作符” 进行定义的。解题思路，如果是操作数，那么就压栈，如果是操作符，那么弹出进行运算。```pythonclass Solution(object): def evalRPN(self, tokens): stack =[] operators =[&apos;+&apos;, &apos;-&apos;, &apos;*&apos;, &apos;/&apos;] for token in tokens: if token not in operators: #这种比较nice exact stack.append(int(token)) # 细节 string to int else: if len(stack) &lt;2: return False second =stack.pop() first =stack.pop() if token ==&quot;+&quot;: result =first +second elif token ==&quot;-&quot;: result =first -second elif token ==&quot;*&quot;: result =first *second else: # 除法向来处理就比较麻烦 if second ==0: return False # 这个是操作中的abs 没有改变原来的值，所以比较nice result =abs(first)/abs(second) if first *second &lt;0: result =-result stack.append(result) # 最后只有一个result 值，所以十分简洁 if len(stack) !=1: return False return stack[0] Excel Sheet Column Number Given a column title as appear in an Excel sheet, return its corresponding column number. Tips: 在于 char 和num 的对应关系。ord() 用于 char 转成int 这种库函数还是要有的。 可以看成 26 进制。 123456789101112class Solution(object): def titleToNumber(self, s): """ :type s: str :rtype: int """ res =0 for char in s: res =res*26 +(ord(char)- ord('A') +1) return res Largest Number Given a list of non negative integers, arrange them such that they form the largest number. Tips: string 类型组合成的数字是最大的。 在string 里面 ‘9’ &gt; ‘88888’ 这个是成立，所以这个特性可以处理这个题目，很巧妙。 12345678910class Solution(object): def largestNumber(self, nums): """ :type nums: List[int] :rtype: str """ nums =map(str, nums) nums.sort(cmp =lambda a, b :cmp(a+b, b+a), reverse =True) # 降序 # 可能出现 00 这样的字符串，所以是先 int 然后再string，感觉这个不是算法的味道 return str(int(''.join(nums))) Longest Substring with At Least K Repeating Characters Find the length of the longest substring T of a given string (consists of lowercase letters only) such that every character in T appears no less than k times. Tips: 这个codes 中的else 还是相当的牛逼，第一次见这种写法的。如果 for 循环中的条件不成立，else。 12345678910111213141516171819class Solution: def longestSubstring(self, s, k): """ :type s: str :type k: int :rtype: int """ stack = [] stack.append(s) ans = 0 while stack: s = stack.pop() for c in set(s): if s.count(c) &lt; k: stack.extend([z for z in s.split(c)]) break else: ans = max(ans, len(s)) return ans Longest Increasing Path in a Matrix Given an integer matrix, find the length of the longest increasing path.From each cell, you can either move to four directions: left, right, up or down. You may NOT move diagonally or move outside of the boundary (i.e. wrap-around is not allowed). Tips: dfs, 判断条件是 val &gt; matrix[i][j] 12345678910111213141516171819202122232425262728class Solution(object): # 一看这个就是深度优先搜索 # 这种做法更加普世 def longestIncreasingPath(self, matrix): """ :type matrix: List[List[int]] :rtype: int """ # 表示以这点为终点的 路径是有多长 # 这个逻辑上是比较简单的， 就是dfs()，然哦吼如果从任意一点出发 range() range()， # 使用 dfs() ，如果是value &gt; matrix[][]，就直接返回了 dp[i][j] def dfs(i, j): if not dp[i][j]: val = matrix[i][j] # i-1 的时候要大于0 i+1的时候要i &lt; M 这样的操作 dp[i][j] = 1 + max( dfs(i - 1, j) if i and val &gt; matrix[i - 1][j] else 0, dfs(i + 1, j) if i &lt; M - 1 and val &gt; matrix[i + 1][j] else 0, dfs(i, j - 1) if j and val &gt; matrix[i][j - 1] else 0, dfs(i, j + 1) if j &lt; N - 1 and val &gt; matrix[i][j + 1] else 0) return dp[i][j] if not matrix or not matrix[0]: return 0 M, N = len(matrix), len(matrix[0]) dp = [[0] * N for i in range(M)] # 以该点为终点的 increasing path 有多少个 return max(dfs(x, y) for x in range(M) for y in range(N)) Word Ladder Given two words (beginWord and endWord), and a dictionary’s word list, find the length of shortest transformation sequence from beginWord to endWord, such that:Only one letter can be changed at a time.Each transformed word must exist in the word list. Note that beginWord is not a transformed word. Tips: 讲解-tm) https://leetcode.com/problems/word-ladder/ 123456789101112131415161718192021222324252627class Solution(object): # https://leetcode.com/problems/word-ladder/discuss/157376/Python-(BFS)-tm # 写出来之后就比较好理解，可以好好想想 def ladderLength(self, beginWord, endWord, wordList): """ :type beginWord: str :type endWord: str :type wordList: List[str] :rtype: int """ wordList =set(wordList) queue =collections.deque([(beginWord, 1)]) visited =set() alpha =string.ascii_lowercase # 'abcd..z' while queue: word, length =queue.popleft() if word == endWord: return length for i in range(len(word)): for ch in alpha: new_word =word[:i] +ch+word[i+1:] if new_word in wordList and new_word not in visited: queue.append((new_word, length+1)) visited.add(new_word) return 0 Fraction to Recurring Decimal Given two integers representing the numerator and denominator of a fraction, return the fraction in string format.If the fractional part is repeating, enclose the repeating part in parentheses. Tips: 分数变成小数 123456789101112131415161718192021222324252627282930313233343536373839class Solution(object): # 主要是考察分情况讨论，这样是比较多的 # 就是在拼接呀 def fractionToDecimal(self, numerator, denominator): """ :type numerator: int :type denominator: int :rtype: str """ res ='' if numerator % denominator ==0: return str(numerator/denominator) if numerator* denominator &lt;0: res += '-' numerator, denominator =abs(numerator), abs(denominator) res += str(numerator/denominator) res +='.' numerator %= denominator i =len(res) table =&#123;&#125; # 下面描述的就是辗转相除的过程， 使用 &#123;&#125; 进行存储 while numerator !=0: if numerator not in table: table[numerator] =i else: i =table[numerator] res =res[:i] +'('+res[i:]+')' return res numerator =numerator*10 res += str(numerator/denominator) numerator %= denominator i +=1 return res Reverse Bits Reverse bits of a given 32 bits unsigned integer. Tips: 与操作和左移操作 ( &amp; and &lt;&lt;) 是常见的 bit operation中用到的 ···pythonclass Solution: # @param n, an integer # @return an integer # 没有什么说的， 二级制操作，注意输入和输出都是 integer # One small thing is the plus operator can be replaced by &quot;bitwise or&quot;, aka &quot;|&quot;. # Just generate the answer bit by bit, do not use things like &quot;% 2&quot; or &quot;2 ** k&quot; or &quot;bin&quot;. Bit manipulation is a lot faster. def reverseBits(self, n): ans =0 # 从后往前处理，所以这就reverse 了 for i in range(32): # n&amp;1 是取最后一位 # ans &lt;&lt;1 左移一位，类似乘2 ans += n &amp;1 if i ==31: return ans n &gt;&gt;= 1 ans &lt;&lt;= 1 return ans 12345678910111213141516171819202122232425262728293031323334** Word Break**&gt; Given a non-empty string s and a dictionary wordDict containing a list of non-empty words, determine if s can be segmented into a space-separated sequence of one or more dictionary words.Tips: dp 问题```pythonclass Solution(object): # 字符串的处理，感觉有点难呀 # dp的思路， dp[i] 表示 s[:i] 是否可分 def wordBreak(self, s, wordDict): &quot;&quot;&quot; :type s: str :type wordDict: List[str] :rtype: bool &quot;&quot;&quot; dict =&#123;&#125; for w in wordDict: dict[w] =True dp =[False for x in range(len(s)+1)] dp[0] =True for i in range(1, len(s)+1): # 如果出现了 range(i) 这种是常用的处理字符串的手段，看前i 是否符合某种要求 # 前面的可分，后面的看一下是否可分 for j in range(i): if dp[j] and s[j:i] in dict: dp[i] =True break return dp[-1] Word Break II Given a non-empty string s and a dictionary wordDict containing a list of non-empty words, add spaces in s to construct a sentence where each word is a valid dictionary word. Return all such possible sentences. Tips : dfs 1234567891011121314151617181920212223242526272829303132333435class Solution(object): # 上一题是返回 true or false，这个是要求是路径，那么最直接的就是dfs(), 不能使用dp了 def wordBreak(self, s, wordDict): """ :type s: str :type wordDict: List[str] :rtype: List[str] """ return self.dfs(s, wordDict, &#123;&#125;) def dfs(self, s, wordDict, memo): # 这个memo 就是某长度的字符串，在之前的dfs 中是否存在过 # 'penapple': ['pen apple'], 'applepenapple': ['apple pen apple' if s in memo: return memo[s] if not s: return [] res =[] for word in wordDict: # 这种直接从 dictionary 中寻找要比 从string 中拼凑快一些 if not s.startswith(word): # 这个就是最贪婪的找开头的python 句子 continue if len(word) ==len(s): # 包含且长度相同，那么 res.append() 就是这个操作了 res.append(word) else: rest =self.dfs(s[len(word):], wordDict, memo) # 如果原来的 s 比较长 那么就切分了 # 这种不收 递归影响的思维还是挺牛逼的 哈哈 for item in rest: item =word +' '+item res.append(item) memo[s] =res return res Palindrome Partitioning Given a string s, partition s such that every substring of the partition is a palindrome.Return all possible palindrome partitioning of s. Tips: dfs, 其中 ispa 写的是比较简洁的 123456789101112131415161718192021222324252627class Solution(object): # 感觉这个有点难度呀 # 所有值的一般是 dfs() 这个还是得不断的加强认识的， def partition(self, s): """ :type s: str :rtype: List[List[str]] """ res =[] self.dfs(s, [], res) return res def dfs(self, s, path, res): if not s: res.append(path) return # 关键是这里的理解， path 是不断的增加的，并且 s[I:] 这个是不断的介绍的， # 先是要求 s[:i] 是 palindrome 然后递归 s[i:] 是palindrome ，整体上是比较nice的 for i in range(1, len(s)+1): if self.isPal(s[:i]): self.dfs(s[i:], path+[s[:i]], res) def isPal(self, s): return s ==s[::-1] Word Search II Given a 2D board and a list of words from the dictionary, find all words in the board.Each word must be constructed from letters of sequentially adjacent cell, where “adjacent” cells are those horizontally or vertically neighboring. The same letter cell may not be used more than once in a word. Tips: 使用字典树 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354class TrieNode(): def __init__(self): self.children = collections.defaultdict(TrieNode) self.isWord = Falseclass Trie(): def __init__(self): self.root = TrieNode() def insert(self, word): node = self.root for w in word: node = node.children[w] node.isWord = True def search(self, word): node = self.root for w in word: node = node.children.get(w) if not node: return False return node.isWord# 上面在上一道题目中就应该记住，这个是一道经典的题目class Solution(object): def findWords(self, board, words): res = [] trie = Trie() node = trie.root for w in words: trie.insert(w) # 先是insert，然后在每一个点进行查找，最后看res for i in range(len(board)): for j in range(len(board[0])): self.dfs(board, node, i, j, "", res) return res def dfs(self, board, node, i, j, path, res): if node.isWord: res.append(path) node.isWord = False if i &lt; 0 or i &gt;= len(board) or j &lt; 0 or j &gt;= len(board[0]): return tmp = board[i][j] node = node.children.get(tmp) if not node: return board[i][j] = "#" self.dfs(board, node, i + 1, j, path + tmp, res) self.dfs(board, node, i - 1, j, path + tmp, res) self.dfs(board, node, i, j - 1, path + tmp, res) self.dfs(board, node, i, j + 1, path + tmp, res) board[i][j] = tmp Valid Anagram Given two strings s and t , write a function to determine if t is an anagram of s. Tips: dictionary 的应用 12345678910111213141516171819202122232425262728293031323334353637class Solution(object): # 这个和旋转数组 感觉上是差不多的呀 # 解答的时候，应该从 dictionary 的角度考虑 def isAnagram(self, s, t): """ :type s: str :type t: str :rtype: bool """ dic =&#123;&#125; # dic =collections.defaultdic(int) 和上面的唯一差别就是，直接使用 dic[char] +=1 这样的操作 # 不用判断是否存在 这样的操作 for n in s: if n not in dic: dic[n] =1 else: dic[n] +=1 for n in t: if n not in dic: return False else: dic[n] -=1 """ for n in dic: if dic[n]!=0: return False return True """ for value in dic.values(): if value !=0: return False return True First Unique Character in a String Given a string, find the first non-repeating character in it and return it’s index. If it doesn’t exist, return -1. Tips: 注意是第一个 non-repeating的 1234567891011121314151617181920class Solution(object): # 这个不重复 是整体之后的不重复，而不是左右的不重复，是全局的 # 我的想法使用dict def firstUniqChar(self, s): """ :type s: str :rtype: int """ dic =&#123;&#125; seen =set() for index, ch in enumerate(s): if ch not in seen: dic[ch] =index seen.add(ch) elif ch in dic: del dic[ch] # 这个是通过更新index 达到的 # 因为题目中提到的是 第一个 non-repeating Reverse String Write a function that reverses a string. The input string is given as an array of characters char[].Do not allocate extra space for another array, you must do this by modifying the input array in-place with O(1) extra memory. Tips : in-place 操作 pointer 123456789101112131415class Solution(object): # 我反手一个reverse() 过去，有问题吗 # sting is immutable, cannot reverse in-place def reverseString(self, s): """ :type s: List[str] :rtype: None Do not return anything, modify s in-place instead. """ left, right =0, len(s)-1 while left &lt; right: s[left], s[right] =s[right], s[left] left +=1 right -=1]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[逻辑回归概念]]></title>
    <url>%2F2019%2F06%2F09%2Flr%2F</url>
    <content type="text"><![CDATA[本文主要介绍逻辑回归（logistics regression）和决策树（Decision Tree）。逻辑回归从线性回归出发到逻辑回归，然后手推公式和相关的一些特点；介绍一下决策树的特点。 逻辑回归逻辑回归是线性模型，虽然叫做”回归“，究其原因 逻辑回归从线性回归引申而来，对回归的结果进行 logistic 函数运算，将范围限制在[0,1]区间，并更改损失函数为二值交叉熵损失，使其可用于2分类问题(通过得到的概率值与阈值比较进行分类)。逻辑回归是广义上的线性模型，然后最后的sigmoid 加入了非线性。是处理线性问题的。 公式推导从线性回归问题到逻辑回归过程的推导。 线性二分模型： $$f ( x ) = \theta ^ { T } x$$ 逻辑回归决策函数是将此线性二分类嵌套一个sigmoid函数： $$ f ( x ) = \frac { 1 } { 1 + e ^ { - \theta ^ { T } x } }$$ 损失函数：如果用平方误差（MSE）作为逻辑回归的损失函数,那么函数曲线将是跳跃式的,非凸的(non-convex),原因是logistic函数将数据范围限制在[0,1]区间,而真实标签值非0即1.最小化 MSE 损失容易陷入局部极小点.逻辑回归损失是如下的分情况的凸函数(单个x与y的损失)。 $$P ( y = 1 | x ; \theta ) = h _ { \theta } ( x )$$ $$P ( y = 0 | x ; \theta ) = 1 - h _ { \theta } ( x )$$最初是上述的分段函数，合并成下面的函数，方便计算。$$p ( y | x ; \theta ) = \left( h _ { \theta } ( x ) \right) ^ { y } \left( 1 - h _ { \theta } ( x ) \right) ^ { 1 - y }$$使用最大似然的思想求解。假设我们有n个独立的训练样本{(x1, y1) ,(x2, y2),…, (xn, yn)}，y={0, 1}。那每一个观察到的样本(xi, yi)出现的概率是： 上述似然函数乘法太难算了，然后使用log 将其改为加法，变成了对数似然函数。$$J( \theta ) = \log ( L ( \theta ) ) = \sum _ { i = 1 } ^ { m } y ^ { ( i ) } \log \left( h \left( x ^ { ( i ) } \right) \right) + \left( 1 - y ^ { ( i ) } \right) \log \left( 1 - h \left( x ^ { ( i ) } \right) \right)$$ 求导优化问题sigmoid 函数的特殊性质：$$\sigma ^ { \prime } ( x ) = \sigma ( x ) ( 1 - \sigma ( x ) )$$ 分成三部分求导： 用L(θ)对θ求导，得到：$$\begin{split}\frac { d } { d \theta _ { i } } \operatorname { loss } ( \theta ) &amp;= \left( y \frac { 1 } { \sigma \left( \theta ^ { T } x \right) } - ( 1 - y ) \frac { 1 } { 1 - \sigma \left( \theta ^ { T } x \right) } \right) \frac { d } { d \theta _ { i } } \sigma \left( \theta ^ { T } x \right) \\&amp;= \left( y \frac { 1 } { \sigma \left( \theta ^ { T } x \right) } - ( 1 - y ) \frac { 1 } { 1 - \sigma \left( \theta ^ { T } x \right) } \right) \sigma \left( \theta ^ { T } x \right) \left( 1 - \sigma \left( \theta ^ { T } x \right) \right) \frac { d } { d \theta _ { i } } \theta ^ { T } x \\&amp;= \left( y \left( 1 - \sigma \left( \theta ^ { T } x \right) \right) - ( 1 - y ) \sigma \left( \theta ^ { T } x \right) \right) x _ { i } \\&amp;= \left( y - h _ { \theta } ( x ) \right) x _ { i }\end{split}$$ 注意一会儿有 $\sum$ 一会儿没有的，其实我们更倾向于不用，采用矩阵相乘的方式更加简洁。只是在表达似然函数，使用$ \sum$更加直观$$\theta _ { i } : = \theta _ { j } + \alpha \left( y ^ { ( i ) } - h _ { \theta } \left( x ^ { ( i ) } \right) \right) x _ { j } ^ { ( i ) }$$ 逻辑回归的特点 优点： LR 能以概率的形式输出结果,而非只是 0,1 判定， 可以做 ranking model； LR 的可解释性强,可控度高； 训练快 缺点： 容易欠拟合，一般准确度不太高 只能处理两分类问题. (可以应用多个逻辑回归实现多分类,类似SVM的方式; 另外对于父子类别同时分类的情况,使用逻辑回归要比Softmax等方式效果好) “海量离散特征+简单模型” 同“少量连续特征+复杂模型”的权衡。既可以离散化用线性模型，也可以用连续特征加深度学习 为什么对特征进行离散化 特征从连续变量状态到离散化的初衷在于我们认为不同的区间对于最后的结果的重要性是不同的。同样在工业界，很少直接将连续值作为逻辑回归模型的特征输入，而是将连续特征离散化为一系列0、1特征(one-hot编码)交给逻辑回归模型，这样做的优势有以下几点 离散特征的增加和减少都很容易，易于模型的快速迭代； 稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展； 离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄&gt;30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰； 单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合 离散化后可以进行特征交叉 究其原因，使用 LR+离散模型在于可控可解释。而GBDT 直接使用连续的变量，一方面的原因在于如果特征过多，那么GBDT 是跑不动的。 为什么LR模型的损失函数是交叉熵,而线性回归模型的损失函数却是最小二乘呢？能否随意确定一个损失函数作为目标呢？ 模型的损失函数由各自的响应变量y的概率分布决定，对于线性回归模型，其输出是连续值，所以我们对于该问题假设y服从正态分布；相对的，LR模型一般用来解决二分类问题，所以其输出是0/1，故而我们假设其输出服从伯努利分布；而进一步地，两者的损失函数都是通过极大似然估计推导的来的，所以模型的损失函数并非随意确定。分类模型与回归模型之间有种种联系,比如 SVM 模型可以看作逻辑回归加L2正则项, 并使用了不同的损失函数. 为什么不使用回归模型来做分类?这是一种不好的做法, 因为阈值不好确定, 随着数据集的变动, 阈值也需要有较大变化. 正则项 L2 解决过拟合 L1 解决数据稀疏性 L1和L2正则先验分别服从什么分布从信息论的角度看，向系统加入了正确先验这个信息，肯定会提高系统的性能。两者的差别感性的理解？L1是拉普拉斯分布，L2是高斯分布。拉普拉斯分布：$$f ( x | \mu , b ) = \frac { 1 } { 2 b } e ^ { - \frac { | x - \mu | } { b } }$$高斯分布：$$f \left( x | \mu , \sigma ^ { 2 } \right) = \frac { 1 } { \sqrt { 2 \pi \sigma ^ { 2 } } } e ^ { - \frac { ( x - \mu ) ^ { 2 } } { 2 \sigma ^ { 2 } } }$$ Decision tree主要介绍一下 决策树的特点。 从这次学习中明显的感受到这个 decision tree 是非常容易过拟合的。 We can make our tree more complex by increasing its size , which will result in more and more partitions trying to emulate the circular boundary. 优点在于：可以handle 非线性的变化。decision tree 给人的感觉就是线性或者离散的(category)的都是可以使用，因为decision tree 得到就是一个离散的结果，最大的缺点就是容易过拟合。 This brings us to the biggest problem associated with Decision Trees, that is, they are highly biased class of models. You can make a decision tree model on your training set which might outperform all other algorithms but it’ll prove to be a poor predictor on your test set. You’ll have to rely heavily on pruning and cross validation to get a non-over-fitting model with Decision Trees. 过拟合是可以通过剪枝或者 cross validation 进行缓解 overfit的效果的或者使用 random forest随机性进行”中和“。 This problem of over-fitting is overcome to large extent by using Random Forests, which are nothing but a very clever extension of decision trees. But random forest take away easy to explain business rules because now you have thousands of such trees and their majority votes to make things complex. Also by decision trees have forced interactions between variables , which makes them rather inefficient if most of your variables have no or very weak interactions. 使用范围：1 数值型：数值型目标变量则可以从无限的数值集合中取值，如0.100，42.001等 (数值型目标变量主要用于回归分析)2 标称型：标称型目标变量的结果只在有限目标集中取值，如真与假(标称型目标变量主要用于分类) 决策树的工作原理原始的数据集： 编号 色泽 根蒂 敲声 纹理 脐部 触感 好瓜1 青绿 蜷缩 浊响 清晰 凹陷 硬滑 是2 乌黑 蜷缩 沉闷 清晰 凹陷 硬滑 是3 乌黑 蜷缩 浊响 清晰 凹陷 硬滑 是4 青绿 蜷缩 沉闷 清晰 凹陷 硬滑 是5 浅白 蜷缩 浊响 清晰 凹陷 硬滑 是6 青绿 稍蜷 浊响 清晰 稍凹 软粘 是7 乌黑 稍蜷 浊响 稍糊 稍凹 软粘 是8 乌黑 稍蜷 浊响 清晰 稍凹 硬滑 是9 乌黑 稍蜷 沉闷 稍糊 稍凹 硬滑 否10 青绿 硬挺 清脆 清晰 平坦 软粘 否11 浅白 硬挺 清脆 模糊 平坦 硬滑 否12 浅白 蜷缩 浊响 模糊 平坦 软粘 否13 青绿 稍蜷 浊响 稍糊 凹陷 硬滑 否14 浅白 稍蜷 沉闷 稍糊 凹陷 硬滑 否15 乌黑 稍蜷 浊响 清晰 稍凹 软粘 否16 浅白 蜷缩 浊响 模糊 平坦 硬滑 否17 青绿 蜷缩 沉闷 稍糊 稍凹 硬滑 否 信息熵和信息增益 熵（entropy）： 熵指的是体系的混乱的程度信息增益（information gain）： 在划分数据集前后信息发生的变化称为信息增益，信息增益越大，确定性越强。 决策树的生成过程就是 最优划分属性的选择的过程。 一共有三种策略可以进行选择，大致分为了 ID3 [Quinlan, 1986]、C4.5 [Quinlan, 1993]、CART [Breiman et al., 1984] 三种。一般来说我们希望选择一个属性之后，其分支节点所包含的样本尽可能属于同一个类别，即结点的纯度 (purity)越来越高。 ID3 树中最优划分属性计算举例 信息增益使用上面的数据集进行说明。在决策树学习开始时，根结点包含 𝐷 中所有样例，正例占 $p _ { 1 } = \frac { 8 } { 17 }$，反例占$ p_2=\frac{9}{17}$。根结点的信息熵（下面我们都以比特为单位计算）为： $$H ( D ) = - \sum _ { k = 1 } ^ { 2 } p ( k ) \log _ { 2 } p ( k ) = - \left( \frac { 7 } { 17 } \log _ { 2 } \frac { 7 } { 17 } + \frac { 9 } { 17 } \log _ { 2 } \frac { 9 } { 17 } \right) = 0.998$$ 然后我们要计算出与当前属性集合 {色泽、根蒂、敲声、纹理、脐部、触感}中每个属性的 信息增益，也就是对应的互信息。以属性“色泽”为例，它有 3 个可能取值：{青绿、乌黑、浅白}。以该属性对数据集进行划分，可以得到 3 个子集，分别为：$D_1(色泽=青绿) $、$ D_2(色泽=乌黑)$ 、$D_3(色泽=浅白)$。 对子集$ D_1$ 来说，包含了编号为 {1,4,6,10,13,17} 的 6 个样例，其中正例为 {1,4,6}，占$ p_1=36$ ；反例为 {10,13,17}，占 $p_1=36$。计算其熵为：$$H \left( D _ { 1 } \right) = - \left( \frac { 3 } { 6 } \log _ { 2 } \frac { 3 } { 6 } + \frac { 3 } { 6 } \log _ { 2 } \frac { 3 } { 6 } \right) = 1.000$$ 依次可以计算另外两个子集的信息熵为：$$H \left( D _ { 2 } \right) = 0.918$$$$H \left( D _ { 3 } \right) = 0.722$$最终可以计算数据集 $D$ 的类别信息在属性“色泽”熵的信息增益（也可以理解为类别与“色泽”属性之间的互信息）为： $$\begin{split}Gain(D, 色泽) &amp;= H ( D ) - \sum _ { v = 1 } ^ { 3 } p ( v ) H \left( D _ { v } \right) \\&amp;= 0.998 - \left( \frac { 6 } { 17 } \times 1.000 + \frac { 6 } { 17 } \times 0.918 + \frac { 5 } { 17 } \times 0.722 \right) \\&amp;= 0.109\end{split}$$重复上述的计算步骤，我们可以计算出其他属性的信息增益： $$\begin{split}Gain(𝐷,根蒂) &amp;=0.143 \\Gain(𝐷,敲声) &amp;= 0.141 \\Gain(𝐷,纹理) &amp;= 0.381\\Gain(𝐷,脐部) &amp;= 0.289 \\Gain(𝐷,触感) &amp;=0.006\end{split}$$ 经过比较，发现采用“纹理”进行划分得到的信息增益最大，于是它被选为划分属性。下图给出了根据“纹理”属性划分之后的数据子集： 对每一个数据子集按照上边的步骤继续划分下去就能得到最终的决策树（需要注意的是每次样例子集中的属性不包含父结点中划分所依赖的属性），如下图所示： 信息增益率 采用信息增益来进行划分属性的决策有一个潜在的问题，当某一个属性的取值种类非常多时，对应每一个属性取值的样本子集，其分类的信息熵可能会变得很小。 $$\begin{split}Gain ( D , a ) &amp;= H ( D ) - \sum _ { v = 1 } ^ { V } \frac { \left| D _ { v } \right| } { | D | } H \left( \left| D _ { v } \right| \right) \\&amp;= - \frac { 8 } { 17 } \log _ { 2 } \frac { 8 } { 17 } - \frac { 8 } { 17 } \log _ { 2 } \frac { 8 } { 17 } - \sum _ { v = 1 } ^ { 17 } \frac { 1 } { 17 } \times 0 \\&amp;= 0.9975\end{split}$$最后计算出来的信息增益很大。但是显然，用“编号”属性来作为结点的划分是没有意义的。思考其中的问题在于，对数函数并不是线性的，信息量的减少速度大于类别数量的增加速度。信息增益准则对取值数目较多的属性有所偏好，为了减小这种偏好，C4.5 决策树 采用 信息增益率 (gain ratio) 来选择最优划分属性。其定义如下：$$Gain_ { \mathrm { ratio } } ( D , a ) = \frac { \operatorname { Gain } ( \mathrm { D } , \mathrm { a } ) } { \mathrm { IV } ( \mathrm { a } ) }$$ 其中$$\mathrm { IV } ( a ) = - \sum _ { v = 1 } ^ { V } \frac { \left| D _ { v } \right| } { | D | } \log \frac { \left| D _ { v } \right| } { | D | } = H ( a )$$到这里，我们就可以发现，信息增益率是用属性分类的信息熵对由属性分类引起的互信息熵进行了归一。属性的种类越多其信息熵通常也会越大。 最后一点需要注意的是，增益率准则虽然减少了对取值数目较多的属性依赖，但是增加了对取值数目较少的属性偏好。因此， C4.5 并没有直接选择增益率最大的候选划分属性，而是使用了一个启发式：先从候选划分属性中找出 信息增益 高于 平均水平 的属性，再从中选择 增益率 最高的。 基尼指数 - CART 最后介绍一种选择划分属性的依据是使用 基尼指数 (Gini index)。数据集合 𝐷 的纯度可用基尼指数来度量：$$Gini ( D ) = \sum _ { k = 1 } ^ { | \mathcal { Y } | } \sum _ { k ^ { \prime } \neq k } p _ { k } p _ { k ^ { \prime } } = 1 - \sum _ { k = 1 } ^ { \mathcal { V } } p _ { k } ^ { 2 }$$直观来看，$Gini(𝐷) $ 反映了从数据集 𝐷 中随机抽取两个样本，其类别标记不一致的概率。因此，$Gini(𝐷) $ 越小，则数据集 𝐷 的纯度越高。 对特定属性 𝑎 的基尼指数定义如下：$$Gini_{index}( D , a ) = \sum _ { v = 1 } ^ { V } \frac { \left| D _ { v } \right| } { | D | } \operatorname { Gini } \left( D _ { v } \right) $$我们在候选属性集合 𝐴 中，选择那个使得划分后基尼指数最小的属性作为最优划分属性，即： $$a _ { * } = \arg \min _ { a \in A } Gini_{index } ( D , a )$$ 采用基尼指数作为划分属性的判据的决策树是一种 CART 决策树。 决策树的优缺点优点： 决策树易于理解（能够写出判断的路径，逻辑表达式）和实现， 人们知道该如何去优化。可以处理缺省数据，意味着数据的准备工作是比较简单的。相对比其他的技术，往往需要一般化之类的操作。能够处理数值型和常规性属性。 缺点： 容易过拟合对于各个类别不一致的数据，决策树当中信息增益的结果偏向于那些具有更多数值的特征。 参考文献 决策树及决策树生成与剪枝]]></content>
      <tags>
        <tag>LR</tag>
        <tag>Decision tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SVM All You Need to Know]]></title>
    <url>%2F2019%2F06%2F08%2Fsvm-all-you-need%2F</url>
    <content type="text"><![CDATA[本文主要介绍SVM 相关内容，包括理论原理、在线性可分条件下的公式推导和 SVM的应用特点。最后综合 LR 和 Decision Tree的这篇博客，给出了一些小的建议。 SVM 理论支持向量机分为三个部分，线性可分支持向量机、线性支持向量机、非线性支持向量机。 SVM 原理SVM 是一种二类分类模型。它的基本模型是在特征空间中寻找间隔最大化的分离超平面的线性分类器。 当训练样本线性可分时，通过硬间隔最大化，学习一个线性分类器，即线性可分支持向量机；当训练数据近似线性可分时，引入松弛变量，通过软间隔最大化，学习一个线性分类器，即线性支持向量机；当训练数据线性不可分时，通过使用核技巧及软间隔最大化，学习非线性支持向量机。以上各种情况下的数学推到应当掌握，硬间隔最大化（几何间隔）、学习的对偶问题、软间隔最大化（引入松弛变量）、非线性支持向量机（核技巧）。 SVM 为什么采用间隔最大化当训练数据线性可分时，存在无穷个分离超平面可以将两类数据正确分开。感知机利用误分类最小策略，求得分离超平面，不过此时的解有无穷多个。线性可分支持向量机利用间隔最大化求得最优分离超平面，这时，解是唯一的。另一方面，此时的分隔超平面所产生的分类结果是最鲁棒的，对未知实例的泛化能力最强。可以借此机会阐述一下几何间隔以及函数间隔的关系。 为什么要将求解 SVM 的原始问题转换为其对偶问题一是对偶问题往往更易求解，当我们寻找约束存在时的最优点的时候，约束的存在虽然减小了需要搜寻的范围，但是却使问题变得更加复杂。为了使问题变得易于处理，我们的方法是把目标函数和约束全部融入一个新的函数，即拉格朗日函数，再通过这个函数来寻找最优点。二是可以自然引入核函数，进而推广到非线性分类问题。 为什么 SVM 要引入核函数当样本在原始空间线性不可分时，可将样本从原始空间映射到一个更高维的特征空间，使得样本在这个特征空间内线性可分。而引入这样的映射后，所要求解的对偶问题的求解中，无需求解真正的映射函数，而只需要知道其核函数。核函数的定义：K(x,y)=&lt;ϕ(x),ϕ(y)&gt;，即在特征空间的内积等于它们在原始样本空间中通过核函数 K 计算的结果。一方面数据变成了高维空间中线性可分的数据，另一方面不需要求解具体的映射函数，只需要给定具体的核函数即可，这样使得求解的难度大大降低。 为什么SVM对缺失数据敏感这里说的缺失数据是指缺失某些特征数据，向量数据不完整。SVM 没有处理缺失值的策略。而 SVM 希望样本在特征空间中线性可分，所以特征空间的好坏对SVM的性能很重要。缺失特征数据将影响训练结果的好坏。 SVM 核函数之间的区别一般选择线性核和高斯核，也就是线性核与 RBF 核。 线性核：主要用于线性可分的情形，参数少，速度快，对于一般数据，分类效果已经很理想了。 RBF 核：主要用于线性不可分的情形，参数多，分类结果非常依赖于参数。有很多人是通过训练数据的交叉验证来寻找合适的参数，不过这个过程比较耗时。 如果 Feature 的数量很大，跟样本数量差不多，这时候选用线性核的 SVM。 如果 Feature 的数量比较小，样本数量一般，不算大也不算小，选用高斯核的 SVM。 以上是几个问题在面试中遇到 SVM 算法时，几乎是必问的问题，另外，大家一定要做到自己可以推导集合间隔、函数间隔以及对偶函数，并且理解对偶函数的引入对计算带来的优势。 支持向量 如图所示，上面只有三个点与求解的优化问题有关，它们就叫做支持向量。 SVM公式推导(线性可分条件下)假定样本空间如下$${ ( x _ { 1 } , y _ { 1 } ) , ( x _ { 2 } , y _ { 2 } ) , \ldots , ( x _ { N } , y _ { N } ) }$$ 共有N个向量，其中$x_k$是一个特征向量而不是一个单一数值。 这是一个二分类问题，所以$y=+1 $或者$ y=−1$。那么我们就可以得到 那么，我们可以得到$$y _ { i } \cdot ( w ^ { T } x _ { i } + b ) \geq 1 , i = 1,2 , \ldots , N$$ 因为我们现在只讨论线性可分情况下的支持向量机，那么在这个样本空间中一定存在一个超平面可以将样本集按照y的值分割城两个部分，这个超平面可以表示为$$w ^ { T } x + b = 0$$ 根据这个超平面的表达式以及第一步推到中我们得到的结果，可以得到这个样本集中任意一个样本点距离超平面的距离：$$\gamma = \frac { | w ^ { T } x + b | } { | w | } \geq \frac { 1 } { | w | }$$由此，我们还可以进一步得到整个margin的宽度：$$\gamma = \frac { 2 } { | w | }$$ 由此，根据第一步和第三步的结果，我们可以得到最基本的目标函数： $$\arg \max _ { w , b } \frac { 2 } { | w | } , \text {s.t. } y _ { i } ( w ^ { T } x _ { i } + b ) \geq 1 , i = 1,2 , \ldots , N$$ 我们还可以对这个目标函数进一步做变化： $$\arg \min _ { w , b } \frac { 1 } { 2 } | w | ^ { 2 }, \text {s.t. } y _ { i } ( w ^ { T } x _ { i } + b ) \geq 1 , i = 1,2 , \ldots , N $$ 我们无法继续直接进行计算了，因此引入拉格朗日乘子$$L ( w , b , \alpha ) = \frac { 1 } { 2 } | w | ^ { 2 } + \sum _ { i } \alpha _ { i } [ 1 - y _ { i } ( w ^ { T } x _ { i } + b ) ]$$ 对w和b分别求L的偏导，并令其偏导数等于0：$$\frac { \partial L } { \partial w } = w - \sum _ { i } \alpha _ { i } y _ { i } x _ { i } = 0 \Rightarrow w = \sum _ { i } \alpha _ { i } y _ { i } x _ { i }$$$$\frac { \partial L } { \partial b } = \sum _ { i } \alpha _ { i } y _ { i } = 0$$ 将第七步得到的w和b代入L函数 至此，我们的目标函数已经变成了$$\arg \max _ { \alpha } ( \sum _ { i } \alpha _ { i } - \frac { 1 } { 2 } \sum _ { i } \sum _ { j } \alpha _ { i } \alpha _ { j } y _ { i } y _ { j } x _ { i } ^ { T } x _ { j } )$$$$\text { s.t. } \sum _ { i } \alpha _ { i } y _ { i } = 0$$$$\alpha _ { i } \geq 0 , i = 1,2 , \ldots , N$$ 用数值方法解出α以后，我们带入式子 7就可以得到 $$w^ { * } = \sum _ { i } \alpha _ { i } ^ { * } y _ { i } x _ { i }$$ SVM的特点SVM的优点：就是当大量的特征出现的时候，使用SVM handle large feature spaces; 然而此时 LR 不是一个很好的选择。 SVM can handle large feature spaces which makes them one of the favorite algorithms in text analysis which almost always results in huge number of features where logistic regression is not a very good choice. SVM Pros: Can handle large feature space Can handle non-linear feature interactions Do not rely on entire data SVM Cons: Not very efficient with large number of observations It can be tricky to find appropriate kernel sometimes TakeOff首先使用 LR 进行尝试，不妨试一下 DT，然后 如果特征比较多，但是数据量不是很多，这个时候使用SVM。Always start with logistic regression, if nothing then to use the performance as baselineSee if decision trees (Random Forests) provide significant improvement. Even if you do not end up using the resultant model, you can use random forest results to remove noisy variablesGo for SVM if you have large number of features and number of observations are not a limitation for available resources and time附上链接：https://www.edvancer.in/logistic-regression-vs-decision-trees-vs-svm-part2/]]></content>
      <tags>
        <tag>SVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NLP Papers Reading-Sentence Embedding]]></title>
    <url>%2F2019%2F06%2F01%2Fnlp-papers-reading-sentence-embedding%2F</url>
    <content type="text"><![CDATA[Why Consider Sentence Embedding?One simple way you could do this is by generating a word embedding for each word in a sentence, adding up all the embeddings and divide by the number of words in the sentence to get an “average” embedding for the sentence. 句子embedding的表示 = words emebdding，可能使用 TF-IDF or SIF 作为weights 进行优化。 Alternatively, you could use a more advanced method which attempts to add a weighting function to word embeddings which down-weights common words. This latter approach is known as Smooth Inverse Frequency (SIF). These methods can be used as a successful proxy for sentence embeddings. However, this “success” depends on the dataset being used and the task you want to execute. So for some tasks these methods could be good enough 上述方法的主要缺点：语序；文字在上下文中才有意义；阅读理解，不同的句子是相同的意思，却得到不同的embedding；依赖于前期处理，如分词。 However, there are a number of issues with any of these types of approaches: They ignore word ordering. This is obviously problematic. It’s difficult to capture the semantic meaning of a sentence. The word crash can be used in multiple contexts, e.g. I crashed a party, the stock market crashed, or I crashed my car. It’s difficult to capture this change of context in a word embedding. Sentence length becomes problematic. With sentences we can chain them together to create a long sentence without saying very much, The Philadelphia Eagles won the Super Bowl, The Washington Post reported that the Philadelphia Eagles won the Super Bowl, The politician claimed it was fake news when the Washington Post reported that the Philadelphia Eagles won the Super Bowl, and so on. All these sentences are essentially saying the same thing but if we just use word embeddings, it can be difficult to discover if they are similar. They introduce extra complexity. When using word embeddings as a proxy for sentence embeddings we often need to take extra steps in conjunction with the base model. For example, we need to remove stop words, get averages, measure sentence length and so on. sentence embedding的应用场景：Similar approaches can be used to go beyond representations and semantic search, to document classification and understanding and eventually document summarizing or generation. Words Embed平均词向量与TFIDF加权平均词向量SIF加权平均词向量来自论文 A simple but tough-to-beat baseline for sentence embeddings，更多信息可以参考这里。在大家都从无监督学习走向有监督学习的时候，这个无监督的方法和神经网络的效果是旗鼓相当的。 利用n-grams embeddingfasttext 介绍。简单说 n-gram 是一种概念，可以细化成两部分：character-level 和word-level，前者是可以用来补充词汇，加强对于不常见词的表示能力，后者是对于词序的补充。 DAN（Deep Unordered Composition Rivals Syntactic Methods for Text Classification）其实DAN(Deep Averaging Networks)应该属于Bag of Words类的算法。因为比较特殊，单独列出来。 它是在对所有词语取平均后，在上面加上几层神经网络。特殊的地方在于它在sentiment analysis中表现也不错，这在BOW类方法中比较罕见。 文中提出了DAN(Deep average network)，说白了就是对于一个句子或者一个段落，把每个单词的embedding进行平均求和，得到第一层固定维度的向量，然后在套几层全连接神经网络。本质来讲，这个模型没有考虑单词之间的顺序，not在第一个位置和在最后一个位置对于DAN来讲输入都是一样的，所以自然conver不住这种情况。这是模型本身的问题，没有办法改进，除非换模型，比如textcnn就能很好的解决这种情况对于否定词敏感，比如but,not等，常常判断为negative。训练速度快，且结果较好，和Syntactic Composition性能差不多，但是消耗的计算资源少作为有监督学习任务来讲，可以试一试。但是由于全连接层，无法进行无监督学习。相反，NBOW可以无监督学习，比如文本相似度计算等。当然。对于DAN而言，可以通过迁移学习，预训练好全连接参数，实现无监督学习 总结一下：对简单的任务来说，用简单的网络结构进行处理基本就够了，但是对比较复杂的任务，还是依然需要更复杂的网络结构来学习sentence representation的。 Unsupervised Sentence Embed基于Encoder-decoder的Skip-Thought VectorsContinuing the tour of older papers that started with our ResNet blog post, we now take on Skip-Thought Vectors by Kiros et al. Their goal was to come up with a useful embedding for sentences that was not tuned for a single task and did not require labeled data to train. They took inspiration from Word2Vec skip-gram (you can find my explanation of that algorithm here) and attempt to extend it to sentences. Changing a single word has had almost no effect on the meaning of that sentence. To account for these word level changes, the skip-thought model needs to be able to handle a large variety of words, some of which were not present in the training sentences. The authors solve this by using a pre-trained continuous bag-of-words (CBOW) Word2Vec model and learning a translation from the Word2Vec vectors to the word vectors in their sentences. Below are shown the nearest neighbor words after the vocabulary expansion using query words that do not appear in the training vocabulary: 论文描述了一种通用、分布式句子编码器的无监督学习方法。使用从书籍中提取的连续文本，训练了一个编码器-解码器模型，借鉴了word2vec中skip-gram模型，通过一句话来预测这句话的上一句和下一句。语义和语法属性一致的句子被映射到相似的向量表示。接着引入一个简单的词汇扩展方法来编码不再训练集内的单词，令词汇量扩展到一百万词。本文的模型被称为skip-thoughts，生成的向量称为skip-thought vector。模型采用了当下流行的端到端框架，通过搜集了大量的小说作为训练数据集，将得到的模型中encoder部分作为feature extractor，可以给任意句子生成vector。 skip-thought模型结构借助了skip-gram的思想。在skip-gram中，是以中心词来预测上下文的词；在skip-thought同样是利用中心句子来预测上下文的句子。 论文采用了GRU-RNN作为encoder和decoder，encoder部分的最后一个词的hiddenstate作为decoder的输入来生成词。这里用的是最简单的网络结构，并没有考虑复杂的多层网络、双向网络等提升效果。decoder部分也只是一个考虑了encoder last hidden state的语言模型，并无其他特殊之处，只是有两个decoder，是一个one maps two的情况，但计算方法一样。模型中的目标函数也是两个部分，一个来自于预测下一句，一个来自于预测上一句。我们将构造一个类似于自编码器的序列到序列结构，但是它与自编码器有两个主要的区别。第一，我们有两个 LSTM 输出层：一个用于之前的句子，一个用于下一个句子；第二，我们会在输出 LSTM 中使用教师强迫（teacher forcing）。这意味着我们不仅仅给输出 LSTM 提供了之前的隐藏状态，还提供了实际的前一个单词（可在上图和输出最后一行中查看输入）。 看上去，Skip-thought和Skip-gram挺象。唯一的遗憾是Skip-thought的decoder那部分，它是作为language modeling来处理的. 从这里的讲解知道这个是不存在 ”正负“样本的， 这个的损失函数是 正确的上下句和生成的上下句之间的reconstruction error。 The end product of Skip-Thoughts is the Encoder. The Decoders are thrown away after training. The trained encoder can then be used to generate fixed length representations of sentences which can be used for several downstream tasks such as sentiment classification, semantic similarity, etc. The encoder utilises a word embedding layer that serves as a look up table. This converts each word in the input sentence to its corresponding word embedding, effectively converting the input sentence into a sequence of word embeddings. This embedding layer is also shared with both of the decoders. The model is then trained to minimise the reconstruction error of the previous and next sentences using the resulting embedding h(i) generated from sentence s(i) after it is passed through the encoder. Back propagating the reconstruction error from the decoder allows the encoder to learn the best representation of the input sentence while capturing the relation between itself and the surrounding sentences.Skip-Thoughts is designed to be a sentence encoder and the result is that the decoders are actually discarded after the training process. The encoder along with the word embedding layer is used as a feature extractor able to encode new sentences that are fed through it. Using cosine similarity on the resulting encoded sentence embeddings, provides a powerful semantic similarity mechanism, where you can measure how closely two sentences relate in terms of meaning as well as syntax. Encoder Network: The encoder is typically a GRU-RNN which generates a fixed length vector representation h(i) for each sentence S(i) in the input. The encoded representation h(i) is obtained by passing final hidden state of the GRU cell (i.e. after it has seen the entire sentence) to multiple dense layers. Decoder Network: The decoder network takes this vector representation h(i) as input and tries to generate two sentences — S(i-1) and S(i+1), which could occur before and after the input sentence respectively. Separate decoders are implemented for generation of previous and next sentences, both being GRU-RNNs. The vector representation h(i) acts as the initial hidden state for the GRUs of the decoder networks. 词汇扩展 作者在训练完过后用在Google News dataset上预训练的模型对Vocabulary进行了词汇扩展主要是为了弥补我们的 Decoder 模型中词汇不足的问题。具体的做法就是：(from https://www.cnblogs.com/jiangxinyang/p/9638991.html) 该思路借鉴于Tomas Mikolov的一篇文章Exploiting Similarities among Languages for Machine Translation中解决机器翻译missing words问题的思路，对训练集产生的词汇表V(RNN)进行了扩展，具体的思路可参考Mikolov的文章，达到的效果是建立了大数据集下V(Word2Vec)和本文V(RNN)之间的映射，V(Word2Vec)的规模远远大于V(RNN)，论文中V(RNN)包括了20000个词，V(Word2Vec)包括了930000多个词，成功地解决了这一问题，使得本文提出的无监督模型有大的应用价值。 评价观点这个方法只是适用于长文本，要求是至少有两个衔接的句子，思想和skip-gram 比较相近。 源码google 的实现作者的实现论文 Quick-Thought vectors2018年发表的论文An efficient framework for learning sentence representations提出了一种简单且有效的框架用于学习句子表示。和常规的编码解码类模型（如skip-thoughts和SDAE）不同的是，本文采用一种分类器的方式学习句子表示。具体地，模型的输入为一个句子$s$以及一个候选句子集合$S_{cand}$，其中$S_{cand}$包含一个句子$s_{ctxt}$是$s$的上下文句子（也就是$s $)的前一个句子或后一个句子）以及其他不是$s$上下文的句子。模型通过对$s$以及$S_{cand}$中的每个句子进行编码，然后输入到一个分类器中，让分类器选出$S_{cand}$中的哪个句子是$s_{ctxt}$。实验设置候选句子集合大小为3，即$S_{cand}​$包含1个上下文句子和两个无关句子。模型结构如下： 模型有如下两个细节需要注意：模型使用的分类器（得分函数）$c$非常简单，是两个向量内积，即$c(u, v)=u^Tv$，计算$s$的embedding与所有$S_{cand}$中的句子向量内积得分后，输入到softmax层进行分类。使用简单分类器是为了引导模型着重训练句子编码器，因为我们的目的是为了得到好的句子向量表示而不是好的分类器。虽然某些监督任务模型如文本蕴含模型是参数共享的，$s$的编码器参数和候选句子编码器参数是不同的（不共享），因为句子表示学习往往是在大规模语料上进行训练，不必担心参数学习不充分的问题。测试时，给定待编码句子$s$，通过该模型得到的句子表示是两种编码器的连结 $[ f ( s ) ;g ( s ) ]$。 看上去，Skip-thought和Skip-gram挺象。唯一的遗憾是Skip-thought的decoder那部分，它是作为language modeling来处理的。而Skip-gram则是利用一个classifier预测周围的词(通过hierarchical softmax 或者negative sampling）。QT针对这个问题，对decoder部分做了大的调整，它直接把decoder拿掉，取而代之的是一个classifier。这个classifier负责预测哪些句子才是context sentences。 QT的classifier取代了Skip-thought的Decoder。这样做的好处是运行的速度大大提升了，用判别问题取代了生成式问题。有趣的是，虽然QT出现的比Skip-thought更晚，但是方法更简单，也更加接近Word2Vec算法。QT是一种新的state-of-art的算法。它不光效果好，而且训练时间要远小于其他算法。在算法方法上和效果上，都可称为是句子表征界的Word2Vec一般的存在。和前面几篇介绍的不同算法放在一起比较，同样都是为了找到好的句子表征，它们采取了不同的路径：InferSent在寻找NLP领域的ImageNet, 它的成功更像是在寻找数据集和任务上的成功，当然它成功的找到了SNLI; Concatenated p-means在寻找NLP领域的convolutional filter, 即怎样才能更好的提炼出句子级别的特征，它找到了p-means操作，以及利用了不同的embeddings; QT则是直接在算法层面上，寻找句子级别的Word2Vec, 算法上的改进让它受益。我们看到不同的方法在不同的方向上都作出了努力和取得了成效，很难讲哪种努力会更有效或者更有潜力。唯一唯一可以肯定的是，从应用层面上来讲，合适的才是最好的。 Supervised Sentence EmbedInferSent来自论文Supervised Learning of Universal Sentence Representations from Natural Language Inference Data，更多信息参考 这里 Multi-task learning Sentence EmbedUniversal Sentence Encoder来自论文 Universal Sentence Encoder，更多信息参考 Universal Sentence Encoder]]></content>
      <categories>
        <category>NLP</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[NLP Papers Reading- BERT]]></title>
    <url>%2F2019%2F05%2F27%2Fpaper-reading-nlp%2F</url>
    <content type="text"><![CDATA[nlp 论文阅读笔记, 随时 update… attention is all you needSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The ouput is computed as a weighted sum of the values, where the weight assigned to each value is computed cy a compatibility function of the query with the corresponding key. 中文的理解：深度学习里的Attention model其实模拟的是人脑的注意力模型，举个例子来说，当我们观赏一幅画时，虽然我们可以看到整幅画的全貌，但是在我们深入仔细地观察时，其实眼睛聚焦的就只有很小的一块，这个时候人的大脑主要关注在这一小块图案上，也就是说这个时候人脑对整幅图的关注并不是均衡的，是有一定的权重区分的。这就是深度学习里的AttentionModel的核心思想。所谓注意力机制，就是说在生成每个词的时候，对不同的输入词给予不同的关注权重。通过注意力机制，我们将输入句子编码为一个向量序列，并自适应地选择这些向量的一个子集，同时对译文进行译码，例如where are you——&gt;你在哪？现在我们在翻译“你”的时候给”you”更多的权重，那么就可以有效的解决对齐问题。 Background: 主要是面临的三个问题。 Transformer 的结构示意图:(transformer 就是讨论了如何实现上述的 self-attention 结构) Encoder: encoder由6个相同的层堆叠而成，每个层有两个子层。第一个子层是多头自我注意力机制(multi-head self-attention mechanism)，第二层是简单的位置的全连接前馈网络(position-wise fully connected feed-forward network)。在两个子层中会使用一个残差连接，接着进行层标准化(layer normalization)。也就是说每一个子层的输出都是LayerNorm(x + sublayer(x))。网络输入是三个相同的向量q, k和v，是word embedding和position embedding相加得到的结果。为了方便进行残差连接，我们需要子层的输出和输入都是相同的维度。 Decoder: decoder也是由N（N=6）个完全相同的Layer组成，decoder中的Layer由encoder的Layer中插入一个Multi-Head Attention + Add&amp;Norm组成。输出的embedding与输出的position embedding求和做为decoder的输入，经过一个Multi-HeadAttention + Add&amp;Norm（（MA-1）层，MA-1层的输出做为下一Multi-Head Attention + Add&amp;Norm（MA-2）的query（Q）输入，MA-2层的Key和Value输入（从图中看，应该是encoder中第i（i = 1,2,3,4,5,6）层的输出对于decoder中第i（i = 1,2,3,4，5,6）层的输入）。MA-2层的输出输入到一个前馈层（FF），经过AN操作后，经过一个线性+softmax变换得到最后目标输出的概率。 对于decoder中的第一个多头注意力子层，需要添加masking，确保预测位置i的时候仅仅依赖于位置小于i的输出。 层与层之间使用的Position-wise feed forward network。 transformer 的结构谈及 transformer，首先应该提到是 计算效率的大大提高，从原先的RNN 的线性O(N)提升的很多，这个的实现是基于多线程的。而后者是因为是有顺序的线性模型，所以是无法使用并行运算的。 对于 RNN 来说，句首的信息要传递到句尾，需要经过 n 次 RNN 的计算；而 Self-Attention 可以直接连接任意两个节点. 从整体上来看，Transformer依旧是一个“Sequence to Sequence”框架，拥有Encoder和Decoder两部分： transformer 的结构 论文中encoder层由6个encoder堆叠在一起，decoder层也一样。 每一个 encoder 和 decoder 的内部简图如下： encoder 部分 对于encoder，包含两层，一个self-attention层和一个前馈神经网络，self-attention能帮助当前节点不仅仅只关注当前的词，从而能获取到上下文的语义。decoder也包含encoder提到的两层网络，但是在这两层中间还有一层attention层，帮助当前节点获取到当前需要关注的重点内容。 self-attention 先说一下 attention 机制的实现： 当使用 self的时候，query, key and value 这三个就都是相同的。经过softmax() 得到就是一个权重，用于标记和 当前处理的词语的关系。self-attention是Transformer用来将其他相关单词的“理解”转换成我们正在处理的单词的一种思路， Multi-Headed Attention 我的理解就是 在CNN中使用多个filter 的类似产物。该机制理解起来很简单，就是说不仅仅只初始化一组Q、K、V的矩阵，而是初始化多组，tranformer是使用了8组，所以最后得到的结果是8个矩阵。 这样做的主要目的是从不同的语义空间投射原文本，能够从更多的角度表征，并且能够拓展模型对不同位置的关注能力。 这给我们留下了一个小的挑战，前馈神经网络没法输入8个矩阵呀，这该怎么办呢？所以我们需要一种方式，把8个矩阵降为1个，首先，我们把8个矩阵连在一起，这样会得到一个大的矩阵，再随机初始化一个矩阵和这个组合好的矩阵相乘，最后得到一个最终的矩阵。这个就是 multi-head attention 机制的全部的流程了。 Positional Encoding transformer给encoder层和decoder层的输入添加了一个额外的向量Positional Encoding，维度和embedding的维度一样，这个向量采用了一种很独特的方法来让模型学习到这个值，这个向量能决定当前词的位置，或者说在一个句子中不同的词之间的距离。这个位置向量的具体计算方法有很多种，论文中的计算方法如下： $$P E ( p o s , 2 i ) = \sin \left( p o s / 10000 ^ { 2 i } / d _ { m } \text {odel} \right)$$ $$P E ( p o s , 2 i + 1 ) = \cos \left( p o s / 10000 ^ { 2 i } / d _ { m } o d e l \right)$$其中pos是指当前词在句子中的位置，i是指向量中每个值的index，可以看出，在偶数位置，使用正弦编码，在奇数位置，使用余弦编码. 最后把这个Positional Encoding与embedding的值相加，作为输入送到下一层。 layer normalization Normalization有很多种，但是它们都有一个共同的目的，那就是把输入转化成均值为0方差为1的数据。我们在把数据送入激活函数之前进行normalization（归一化），因为我们不希望输入数据落在激活函数的饱和区。 (一个很自然的想法，batch normalization 和 layer normalization 有什么区别? 为什么可以在不同的维度上进行 normalization) BN的主要思想就是：在每一层的每一批数据上进行归一化。我们可能会对输入数据进行归一化，但是经过该网络层的作用后，我们的数据已经不再是归一化的了。随着这种情况的发展，数据的偏差越来越大，我的反向传播需要考虑到这些大的偏差，这就迫使我们只能使用较小的学习率来防止梯度消失或者梯度爆炸。可以看到，右半边求均值是沿着数据 batch_size的方向进行的 不过 LN 是在每一个样本上计算均值和方差，而不是BN那种在批方向计算均值和方差！ decoder 部分 decoder部分其实和encoder部分大同小异，不过在最下面额外多了一个masked mutil-head attetion，这里的mask也是transformer一个很关键的技术。 Transformer 模型里面涉及两种 mask，分别是 padding mask 和 sequence mask。其中 padding mask 在所有的 scaled dot-product attention 里面都需要用到，而 sequence mask 只有在 decoder 的 self-attention 里面用到。前者就是一种填充技术，使得 不定长的sequence 变成定长的sequence之后做出的一些处理。 Padding Mask 什么是 padding mask 呢？因为每个批次输入序列长度是不一样的也就是说，我们要对输入序列进行对齐。具体来说，就是给在较短的序列后面填充 0。但是如果输入的序列太长，则是截取左边的内容，把多余的直接舍弃。因为这些填充的位置，其实是没什么意义的，所以我们的attention机制不应该把注意力放在这些位置上，所以我们需要进行一些处理。具体的做法是，把这些位置的值加上一个非常大的负数(负无穷)，这样的话，经过 softmax，这些位置的概率就会接近0！ Sequence masksequence mask 是为了使得 decoder 不能看见未来的信息。也就是对于一个序列，在 time_step 为 t 的时刻，我们的解码输出应该只能依赖于 t 时刻之前的输出，而不能依赖 t 之后的输出。因此我们需要想一个办法，把 t 之后的信息给隐藏起来。那么具体怎么做呢？也很简单：产生一个上三角矩阵，上三角的值全为0。把这个矩阵作用在每一个序列上，就可以达到我们的目的。 缺点： 问题一： 长输入 对于长输入的任务，典型的比如篇章级别的任务（例如文本摘要），因为任务的输入太长，Transformer会有巨大的计算复杂度，导致速度会急剧变慢。所以估计短期内这些领地还能是RNN或者长成Transformer模样的CNN的天下（其实目前他俩这块做得也不好），也是目前看两者的希望所在，尤其是CNN模型，希望更大一些。 思路： 比如可以把长输入切断分成K份，强制把长输入切短，再套上Transformer作为特征抽取器，高层可以用RNN或者另外一层Transformer来接力，形成Transformer的层级结构，这样可以把n平方的计算量极大减少。 问题二： 网络结构过于复杂 如何更深刻认识它的作用机理，然后进一步简化它，这也是一个好的探索方向。 上面在做语义特征抽取能力比较时，结论是对于距离远与13的长距离特征，Transformer性能弱于RNN 分界线 - - - - - – - - - - - - - - - - - 分 界线 Encoder和Decoder的内部结构： 模型的特点：Positional embedding；（位置嵌入向量——其实类似word2vec，处理的语序的信息）。multi-head attention; (多头注意力机制——点乘注意力的升级版本， 这个就类似ensemble的思想，不同的子空间的attention 进行融合）Position-wise Feed-Forward Networks（位置全链接前馈网络——MLP变形） 有两种常用的注意力函数，一种是加法注意力(additive attention)，另外一种是点乘注意力(dot-productattention)，论文所采用的就是点乘注意力，这种注意力机制对于加法注意力而言，更快，同时更节省空间。 加法注意力还是以传统的RNN的seq2seq问题为例子，加性注意力是最经典的注意力机制，它使用了有一个隐藏层的前馈网络（全连接）来计算注意力分配： 公式:$$\alpha _ { i j } = \frac { \exp \left( e _ { i j } \right) } { \sum _ { k = 1 } ^ { L } e _ { i k } }$$ Scaled Dot-Product这篇论文计算query和key相似度使用了dot-product attention，即query和key进行点乘（内积）来计算相似度。 Multi-Head Attention: 在实际中为了并行计算，可以在一组queries上计算注意力函数，将多个query堆叠成Q，同理keys和values也被堆叠成K和V，通过下面的公式来计算矩阵输出:self-attention 模型就是自己对自己求attention，即𝑄=𝐾=𝑉$$\text { Attention } ( Q , K , V ) = \operatorname { softmax } \left( \frac { Q K ^ { T } } { \sqrt { d _ { k } } } \right) V$$之所以用内积除以维度的开方，论文给出的解释是：假设Q和K都是独立的随机变量，满足均值为0，方差为1，则点乘后结果均值为0，方差为dk。也即方差会随维度dk的增大而增大，而大的方差导致极小的梯度(我认为大方差导致有的输出单元a（a是softmax的一个输出）很小，softmax反向传播梯度就很小（梯度和a有关））。为了避免这种大方差带来的训练问题，论文中用内积除以维度的开方，使之变为均值为0，方差为1。 除了计算一个单独的注意力函数，论文提出对queries，keys和values做h次不同的投影, 然后都经过Scaled Dot-Product Attention，将结果拼接在一起，最后通过一个线性映射输出，通过多头注意力，模型能够获得不同子空间下的位置信息。如下图所示，公式如下:$$\text {MultiHead} ( Q , K , V ) =Concat(head_1, head_2, …, head_h) W ^ { o }$$ Self-Attention那么首先要明白什么是Attention。从语言学的角度，它是表示词与词之间的关联关系，像下图所示，这是一个Self-Attention的示意，它这个it会和其他位置的词发生关系，颜色越深的是说关系越紧密，从中图中看到它很正确的关联到了animal它实际指代的一个词。从机器学习的角度，这个Attention是神经网络隐层之间一个相似度的表示，什么是Self-Attention？就是表示句子内部词与词之间的关联关系，就像这里的it到animal，可以用于指代消解等问题。 Positional EncodingPosition Embedding，也就是“位置向量”，将每个位置编号，然后每个编号对应一个向量，通过结合位置向量和词向量，就给每个词都引入了一定的位置信息，这样Attention就可以分辨出不同位置的词了。 Residual connection和layer-normalization（这两个操作主要是应对 深度网络而提出的）对于学习CV的人估计对这个结构一点也不陌生，Residual connection是对于较为深层的神经网络有比较好的作用，比如网络层很深时，数值的传播随着weight不断的减弱，Residual connection是从输入的部分，就是图中虚线的部分，实际连到它输出层的部分，把输入的信息原封不动copy到输出的部分，减少信息的损失。 layer-normalization这种归一化层是为了防止在某些层中由于某些位置过大或者过小导致数值过大或过小，对神经网络梯度回传时有训练的问题，保证训练的稳定性，这是神经网络设计比较常用的case。 结论：self-attention层的好处是能够一步到位捕捉到全局的联系，解决了长距离依赖，因为它直接把序列两两比较（代价是计算量变为 O(n2)，当然由于是纯矩阵运算，这个计算量相当也不是很严重），而且最重要的是可以进行并行计算。相比之下，RNN 需要一步步递推才能捕捉到，并且对于长距离依赖很难捕捉。而 CNN 则需要通过层叠来扩大感受野，这是 Attention 层的明显优势。 deep contextualized word representationsintroduction: 这种embedding -context 必要性的介绍，感觉是有更好，没有也是能够理解的。Why do we need contextualized representations?As an illustrative example, take the following two sentences: “The bank on the other end of the street was robbed”“We had a picnic on the bank of the river” Both sentences use the word “bank”, but the meaning of the word differs completely between them. This phenomenon where two identical words change meaning depending on the context is known as “polysemy“, and has been an issue in the NLP deep learning community ever since word embeddings really took off. Most current neural networks are bad at handling polysemy because they use a single vector to represent the meaning of the word “bank”, regardless of the context. In reality, the vector representing any word should change depending on the words around it. 什么是一个好的词向量：ELMo能够学习到词汇用法的复杂性，比如语法、语义。ELMo能够学习不同上下文情况下的词汇多义性。 之前的做法的缺点是对于每一个单词都有唯一的一个embedding表示, 而对于多义词显然这种做法不符合直觉, 而单词的意思又和上下文相关, ELMo的做法是我们只预训练language model, 而word embedding是通过输入的句子实时输出的, 这样单词的意思就是上下文相关的了, 这样就很大程度上缓解了歧义的发生. 这种算法的特点是：每一个word representation都是整个输入语句的函数。具体做法就是先在大语料上以language model为目标训练出bidirectional LSTM模型，然后利用LSTM产生词语的表征。ELMo故而得名(Embeddings from Language Models)。为了应用在下游的NLP任务中，一般先利用下游任务的语料库(注意这里忽略掉label)进行language model的微调,这种微调相当于一种domain transfer; 然后才利用label的信息进行supervised learning。 （这个描述跟 cv 是惊人的相似）ELMo表征是“深”的，就是说它们是biLM的所有层的内部表征的函数。这样做的好处是能够产生丰富的词语表征。高层的LSTM的状态可以捕捉词语意义中和语境相关的那方面的特征(比如可以用来做语义的消歧)，而低层的LSTM可以找到语法方面的特征(比如可以做词性标注)。如果把它们结合在一起，在下游的NLP任务中会体现优势。 Salient featuresELMo representations are: Contextual: The representation for each word depends on the entire context in which it is used. Deep: The word representations combine all layers of a deep pre-trained neural network. Character based: ELMo representations are purely character based, allowing the network to use morphological clues to form robust representations for out-of-vocabulary tokens unseen in training. related work: 针对传统词向量是固定的，与上下文语境无关的缺点，先前的工作多通过两种方式来解决： (1) 通过引入字符级(subword)信息丰富词向量表达； (2) 学习每个单词不同含义的独立向量； ELMo也利用了字符卷积（Character-Convolutions）引入字符级信息，并同时结合了深度双向语言模型的各层隐状态来丰富词向量表达。 P.s.：基于字符的模型不仅能够通过引入字符级信息丰富词向量表达，也能够在很大程度上解决NLP领域的OOV（Out-Of-Vocabulary）问题。 ELMo用到上文提到的双向的language model, 给定N个tokens (t1, t2,…,tN), language model通过给定前面的k-1个位置的token序列计算第k个token的出现的概率:$$p \left( t _ { 1 } , t _ { 2 } , \ldots , t _ { N } \right) = \prod _ { k = 1 } ^ { N } p \left( t _ { k } | t _ { 1 } , t _ { 2 } , \ldots , t _ { k - 1 } \right)$$后向的计算方法与前向相似: $$p \left( t _ { 1 } , t _ { 2 } , \ldots , t _ { N } \right) = \prod _ { k = 1 } ^ { N } p \left( t _ { k } | t _ { k + 1 } , t _ { k + 2 } , \ldots , t _ { N } \right)$$biLM训练过程中的目标就是最大化:$$\sum _ { k = 1 } ^ { N } \left( \log p \left( t _ { k } | t _ { 1 } , \ldots , t _ { k - 1 } ; \Theta _ { x } , \vec { \Theta } _ { L S T M } , \Theta _ { s } \right) + \log p \left( t _ { k } | t _ { k + 1 } , \ldots , t _ { N } ; \Theta _ { x } , \overline { \Theta } _ { L S T M } , \Theta _ { s } \right) \right)$$ ELMo对于每个token $t_k$, 通过一个L层的biLM计算出2L+1个表示:$$R_{ k } = \left{ x _ { k } ^ { L M } , \vec { h } _ { k , j } ^ { L M } , h _ { k , j } ^ { L M } | j = 1 , \ldots , L \right} = \left{ h _ { k , j } ^ { L M } | j = 0 , \ldots , L \right}$$其中$h _ { k , 0 } ^ { L M }$是对token进行直接编码的结果(这里是字符通过CNN编码), $h _ { k , j } ^ { L M } = \left[ \vec { h } _ { k , j } ^ { L M } ; \overline { h } _ { k , j } \right]$ 是每个biLSTM层输出的结果. 在实验中还发现不同层的biLM的输出的token表示对于不同的任务效果不同. 应用中将ELMo中所有层的输出R压缩为单个向量, ELMok=E(Rk;Θϵ), 最简单的压缩方法是取最上层的结果做为token的表示:$E \left( R _ { k } \right) = h _ { k , L } ^ { L M }$ 更通用的做法是通过一些参数来联合所有层的信息:$$E L M o _ { k } ^ { t a s k } = E \left( R _ { k } ; \Theta ^ { t a s k } \right) = \gamma ^ { t a s k } \sum _ { j = 0 } ^ { L } s _ { j } ^ { t a s k } h _ { k , j } ^ { L M }$$ 其中$s_j$是一个softmax出来的结果, $γ$是一个任务相关的scale参数, 我试了平均每个层的信息和学出来$s_j$发现学习出来的效果会好很多. 文中提到$γ$在不同任务中取不同的值效果会有较大的差异, 需要注意, 在SQuAD中设置为0.01取得的效果要好于设置为1时. ELMo: Context Matters Instead of using a fixed embedding for each word, ELMo looks at the entire sentence before assigning each word in it an embedding. It uses a bi-directional LSTM trained on a specific task to be able to create those embeddings. ELMo provided a significant step towards pre-training in the context of NLP. The ELMo LSTM would be trained on a massive dataset in the language of our dataset, and then we can use it as a component in other models that need to handle language. What’s ELMo’s secret? ELMo gained its language understanding from being trained to predict the next word in a sequence of words - a task called Language Modeling. This is convenient because we have vast amounts of text data that such a model can learn from without needing labels. We can see the hidden state of each unrolled-LSTM step peaking out from behind ELMo’s head. Those come in handy in the embedding proecss after this pre-training is done. ELMo actually goes a step further and trains a bi-directional LSTM – so that its language model doesn’t only have a sense of the next word, but also the previous word.ELMo comes up with the contextualized embedding through grouping together the hidden states (and initial embedding) in a certain way (concatenation followed by weighted summation). lstm-based language modelIn case you are unfamiliar with language models, a language model is simply a model that can predict how “likely” a certain sequence of words is to be a real piece of text. This is generally done by training a model to take a part of sentence (say, the first n words) and predict the next word – or more precisely, output the probability of each word in the vocabulary being the next word (In this blog post, we’ll focus on LSTM-based language models which are the focus of this paper). One trick that this paper uses is to train a language model with reversed sentences that the authors call the “backward” language model.这种模型：上一个模型的输出到下一个模型输入Furthermore, instead of using a single-layer LSTM, this paper uses a stacked, multi-layer LSTM. Whereas a single-layer LSTM would take the sequence of words as input, a multi-layer LSTM trains multiple LSTMs to take the output sequence of the LSTM in the previous layer as input (of course, the first layer takes the sequence of words as input). This is best illustrated in the following illustration: 最后的embedding 是是将不同的层 combination起来，这个系数是通过学习出来的。In ELMo, the part that is task specific is the combination of the task-agnostic representations. The weight is learned for each task and normalized using the softmax function. The parameter $\gamma$ is a task-dependent value that allows for scaling the entire vector, which is important during optimization. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding嵌入（Embedding）的新时代到目前为止，词嵌入一直是影响NLP模型处理语言的主要力量。但是我们使用GloVe，那么“stick”这个词将由一个向量表示，无论上下文是什么。所以词嵌入的方式并没有解决上下文的问题。ELMo 解决了这个问题：捕捉单词的上下文信息。ELMo不是对每个单词使用固定嵌入，而是在为其中的每个单词分配嵌入之前查看整个句子，它使用在特定任务上训练的双向LSTM来创建这些嵌入。 BERT全称是Bidirectional Encoder Representations from Transformers，取了核心单词的首字母而得名，从名字我们能看出该模型两个核心特质：依赖于Transformer以及双向。 首先介绍BERT的两种型号： BERT BASE：与OpenAI Transformer的尺寸相当，性价比很高； BERT LARGE：一个非常庞大的模型，它的性能最好； 一般来说使用BASE 版本就足以。 预训练过程BERT为了能够在大规模语料上进行无监督学习，非常巧妙的设计了两个预训练任务：一个是随机遮蔽（mask）掉一个句子中的词，利用上下文进行预测 （换句话说，为了训练深度双向Transformer表示）；另一个是预测下一个句子（类似QA场景） BERT 的目标是生成语言模型，所以只需要 encoder 机制。Transformer 的 encoder 是一次性读取整个文本序列，而不是从左到右或从右到左地按顺序读取，这个特征使得模型能够基於单词的两侧学习，相当于是一个双向的功能。这个是解决了 word embedding的问题，但是在其他的任务中 word2vec 的作用是有限的。所以BERT 提供了两种策略： 蒙面语言模型（NLM：Masked Language Model）和 两个句子的任务（Two-sentence Tasks）。 比较 ELMO 和BERT: 从结构上我们可以看出ELMo的基础是使用了LSTM，而BERT使用了Transformer作为基本模型 核心的是两者的目标函数是不一致的 ELMO: $P \left( w _ { i } | w _ { 1 } , \ldots , w _ { i - 1 } \right) $ 和 $P \left( w _ { i } | w _ { i + 1 } , \ldots , w _ { n } \right)$ BERT：$$P \left( w _ { i } | w _ { 1 } , \ldots , w _ { i - 1 } , w _ { i + 1 } , \ldots , w _ { n } \right)$$ 所以BERT 在训练的时候和 ELMO 是不太一样的，前者使用了 masked LM tricks. Masked LM (MLM) Input:the man [MASK1] to [MASK2] storeLabel:[MASK1] = went; [MASK2] = store 该任务就是BERT为了做到双向深度上下文表示设计的预训练trick任务，而在mask单词的时候，作者也采用了一些技巧，随机mask掉15%的token，最终的损失函数只计算mask掉的token。而对于被mask掉的词也并非简单粗暴的将全部替换成[MASK]标签完事，会遵循如下步骤： 80%即大部分情况下，被mask掉的词会被[MASK]标签代替； 10%的情况下，将该词用一个随机的词替换掉； 10%的情况下，保留该词在原位置。 这样做的目的是偏向代表实际观察到的词。另外模型在预训练时，Transformer编码器并不知道哪些词被mask掉了，所以模型对每个词都会关注。同时，因为随机替换仅发生在所有词的1.5％（即15％*10％），对模型的语言理解能力影响很小。Transformer编码器不知道它将被要求预测哪些单词，或者哪些已经被随机单词替换，因此它必须对每个输入词保持分布式的上下文表示。此外，由于随机替换在所有词中只发生1.5%，所以并不会影响模型对于语言的理解。 Next Sentence Prediction (NSP) Input:the man went to the store [SEP] he bought a gallon of milkLabel:IsNextInput:the man went to the store [SEP] penguins are flightless birdsLabel:NotNext 由于在LM的下游任务还会涉及到问答（Question Answering (QA) ）和推理（ Natural Language Inference (NLI)）的任务，这需要LM有理解句子间关系的能力，所以作者新增了一个预训练任务，输入句子A和B，预测B是否为A的下一个句子，以50%的概率配对A和B，即50%B是真的，50%B是随机选取的一个句子。 所以作者提示在选取预训练语料时，要尽可能选取document-level的语料而非segment-level混合在一起的语料 在 BERT 的训练过程中，模型接收成对的句子作为输入，并且预测其中第二个句子是否在原始文档中也是后续句子。在训练期间，50％ 的输入对在原始文档中是前后关系，另外 50％ 中是从语料库中随机组成的，并且是与第一句断开的。 在训练 BERT 模型时，Masked LM 和 Next Sentence Prediction 是一起训练的，目标就是要最小化两种策略的组合损失函数。 BERT 模型的输入 输入表示可以在一个词序列中表示单个文本句或一对文本(例如，[问题，答案])。对于给定的词，其输入表示是可以通过三部分Embedding求和组成。Embedding的可视化表示如下图所示： token Embeddings表示的是词向量，第一个单词是CLS标志，可以用于之后的分类任务，对于非分类任务，可以忽略词向量； Segment Embeddings用来区别两种句子，因为预训练不只做语言模型还要做以两个句子为输入的分类任务； Position Embeddings是通过模型学习得到的。 应用BERT 可以使用在各种NLP 任务中，只需要在核心模型中添加一个层，比如： 在分类任务中，例如情感分析等，只需要在 Transformer 的输出之上加一个分类层 在问答任务（例如SQUAD v1.1）中，问答系统需要接收有关文本序列的 question，并且需要在序列中标记 answer。 可以使用 BERT 学习两个标记 answer 开始和结尾的向量来训练Q＆A模型。 在命名实体识别（NER）中，系统需要接收文本序列，标记文本中的各种类型的实体（人员，组织，日期等）。 可以用 BERT 将每个 token 的输出向量送到预测 NER 标签的分类层。 句子分类如果使用B ERT 模型进行句子分类，那么在训练阶段模型发生的变化很小，这个过程称为微调。 机器翻译* 事实上Transformer比LSTM更好地处理长期依赖性，使其非常适合机器翻译。从一种语言到另一种语言的转换。 参考文献 Transformer &amp; BERTBERT解析及文本分类应用 A simple but tough-to-beat baseline for sentence embeddings这种motivation 还是很值得好好看的，验证论文的好坏是可以通过看最后的效果/ 结果是否按照 motivation 那样的。 “relative weights” 是针对 word2vec 中的效果改进的，从motivation 的角度没有考虑到 melo or BERT 中的 context . 当然论文的创新点在于 无监督学习方法（可以对大规模的语料加以应用，相比于有监督的优势），当别人都在转向有监督和多任务的时候，在缩短运行时间的同时，最后的效果和神经网络旗鼓相当。 Taking the average of the word embeddings in a sentence tends to give too much weight to words that are quite irrelevant, semantically speaking. Smooth Inverse Frequency tries to solve this problem in two ways: Weighting: like our tf-idf baseline above, SIF takes the weighted average of the word embeddings in the sentence. Every word embedding is weighted by a/(a + p(w)), where a is a parameter that is typically set to 0.001 and p(w) is the estimated frequency of the word in a reference corpus. (使用新的词权重计算方法，不是tf-idf, 频率越高，权重越低，抑制高频词) Common component removal: next, SIF computes the principal component of the resulting embeddings for a set of sentences. It then subtracts from these sentence embeddings their projections on their first principal component. This should remove variation related to frequency and syntax that is less relevant semantically.As a result, SIF downgrades unimportant words such as but, just, etc., and keeps the information that contributes most to the semantics of the sentence. 第一步中的$p(w) $ 是在语料中的词频，第二步中对整个句子集合进行一次PCA，然后对每个句子上面得到的向量减去它在第一奇异向量或者说主成分上的投影。最后初步的句子向量减去对应句子向量的共性成分(起到平滑作用),得到最后的独有的句子向量(使得各个句子向量间的耦合度降低,增强句子的鲁棒性).耦合性越低（模块之间的关联性越小） 如何体现了 smooth？ 使用了两项平滑项, 出于如下的考虑: 有些单词在规定的上下文范围之外出现, 也可能对discourse vector产生影响 （开始得到词向量使用 word2vec） 有限单词的出现(如常见的停止词)与discourse vector没有关系 出于这两点考虑, 引入了两种平滑项, 首先是对数线性模型中的一个累加项(additive term) $ ( \alpha p(w)) $,其中$(p(w))$是单词 $(w) $在整个语料中出现的概率(词频角度), $ (\alpha) $是一个超参数. 这样, 即使和 $ (c_s) $的内积很小, 这个单词也有概率出现. 第二步骤中的减去 主成分，可以理解为让各个词向量更好的分开，减去公共的部分，减少耦合性，使得相似的句子聚类在一起。因为这个主成分是整个语料库中的主成分（句子最频繁的意义，可以认为是句子中最重要的成分） 一种解读方式，来自文章 由于有king-queue=man-woman这个属性，并且这个属性大概率是沿着主成分方向分布，当减去之后，我们可以发现不管是king、queue、man、women都会被泛化成类似people之类的向量，这样的话相似意思句子就会具有相似向量。（这里没有验证） 另外论文中还提到了这种方法的鲁棒性: 使用不同语料(多种领域)训练得到的不同的word embedding, 均取得了很好的效果, 说明了对各种语料的友好. 使用不同语料得到的词频, 作为计算词权重的因素, 对最终的结果影响很小. 对于方法中的超参数, 在很大范围内, 获得的结果都是区域一直的, 即超参数的选择没有太大的影响. 本文是用无监督方法做句子级别的 embedding，用的是一个十分简单但却又很有效的传统方法，这在神经网络泛滥的年代算是一股清流了。这张图上的信息还是很多的，所以好好归纳整理一下。尽管长期以来句子的无监督表示学习是主流，最近几个月（2017年末/2018年初），我们看到了许多非常有趣的工作，显示了向监督学习和多任务学习转向的趋势。 强力/迅速的基线：FastText、词袋（Bag-of-Words） 当前最先进模型：ELMo、Skip-Thoughts、Quick-Thoughts、 InferSent、MILA/MSR的General Purpose Sentence Representations、Google的Universal Sentence Encoder 关于nlp 中的word embedding 是可以有 phrases, sentences, and paragraphs 三个不同类别的 embedding，所以还是挺好的。 近五年来提出了大量词嵌入方法。其中最常用的模型是word2vec和GloVe，这两个模型都是基于分布假说（distributional hypothesis）的无监督方法。（根据分布假说，出现在相同上下文中的单词倾向于具有相似的含义）。尽管有一些工作通过并入语义或语法知识等增强这些无监督方法，纯无监督方法在2017-2018年期间取得了有趣的进展，其中最重大的是FastText（word2vec的扩展）和ELMo（当前最先进的上下文词向量）。 在ELMo中，嵌入基于一个双层的双向语言模型（biLM）的内部状态计算，ELMo也是因此得名的：Embeddings from Language Models（来自语言模型的嵌入）。ELMo的特性：ELMo的输入是字符而不是单词。这使得它可以利用子字（sub-word）单元为词汇表以外的单词计算有意义的表示（和FastText类似）。ELMo是biLM的多层激活的连接（concatenation）。语言模型的不同层编码了单词的不同信息。连接所有层使得ELMo可以组合多种词表示，以提升下游任务的表现。 普适句嵌入词袋方法这一领域的一般共识是，直接平均一个句子的词向量这一简单方法（所谓词袋方法），为许多下游任务提供了强力的基线。Arora等去年在ICLR发表的论文A Simple but Tough-to-Beat Baseline for Sentence Embeddings提供了一个很好的算法：选择一种流行的词嵌入，编码句子为词向量的线性加权组合，然后进行相同成分移除（根据首要主成分移除向量投影）。这一通用方法具有深刻而强大的理论动机，基于在语篇向量上随机行走以生成文本的生成式模型。 （有人实践）这个方法在短文本上效果更好，在语料不足的时候效果不能保证。这种模型没有考虑词顺序（也可以说只能理解词意思，但是不能理解语义），而深度网络模型是可以考虑语义的。可能再相似度问题上可以取得比较好的效果，但是在文本分类，情感分类上效果一般。 思考：从直觉上理解, 短文本上的 word2vec、SIF 这种没有 handle 语序的模型得到的效果就已经足够的好，对于中长文本（句子、段落等）elmo 和BERT 这种模型的效果是更加的。 程序的运行只需要十几分钟，与神经网络的效果旗鼓相当。优缺点： 由于这是一种无监督学习，那么就可以对大规模的语料加以利用，这是该方法相比于一般有监督学习的一大优势。 通过对实验的复现，发现运行一次程序只需要十几分钟，并且主要的运行耗时都在将词向量模型载入内存这个过程中，这比动不动就需要训练几周的神经网络模型确实要好很多，并且在这个词相似性任务中，与神经网络旗鼓相当。 缺点就是没有考虑句子的语序,导致不能辨别(“我爱你”还是”你爱我”),优点就是速度快,无监督学习 Supervised Learning of Universal Sentence Representations from Natural Language Inference Data文章成功的找到了NLP领域的ImageNet — SNLI (Stanford Natural Language Inference dataset), 并且试验了不同的深度学习模型，最终确定bi-LSTM max pooled 为最佳模型。 域 数据 任务 模型(编码器) CV ImageNet image classification Le-Net, VGG-Net, Google-Net, ResNet, DenseNet NLP SNLI NLI ? 基于监督学习方法学习sentence embeddings可以归纳为两个步骤：第一步选择监督训练数据，设计相应的包含句子编码器Encoder的模型框架；第二步选择（设计）具体的句子编码器，包括DAN、基于LSTM、基于CNN和Transformer等。 数据集： 本文采用的是Stanford Natural Language Inference Datasets，简称SNLI （NLP领域的ImageNet ）。SNLI包含570K个人类产生的句子对，每个句子对都已经做好了标签，标签总共分为三类：蕴含、矛盾和中立（Entailment、contradiction and neutral）。下面是这些数据集的一个例子： 从上图可以看出，每个句子对为（text, hypothesis）,中间的judgments为它们的标签。可以看到标签是综合了5个专家的意见，根据少数服从多数的原则得到的。 7种不同的architectures： standard recurrent encoders with LSTM ，取最后一个隐状态 standard recurrent encoders with GRU ，取最后一个隐状态上述两种是基础的recurrent encoder，在句子建模中通常将网络中的最后一个隐藏状态作为sentence representation； conncatenation of last hidden states of forward and backward GRU这种方法是将单向的网络变成了双向的网络，然后用将前向和后向的最后一个状态进行连接，得到句子向量； Bi-directional LSTMs (BiLSTM) with mean pooling Bi-directional LSTMs (BiLSTM) with max pooling这两种方法使用了双向LSTM结合一个pooling层的方法来获取句子表示，具体公式如下： self-attentive network这个网络在双向LSTM的基础上加入了attention机制，具体网络结构如下： hierarchical convolutional networks Now that we have discussed the various sentence encoding architectures used in the paper, let’s go through the part of the network which takes these sentence embeddings and predicts the output label. After the sentence vectors are fed as input to this model, 3 matching methods are applied to extract relations between the text, u and hypothesis, v – concatenation of the two representations (u, v) element-wise product u * v and, absolute element-wise difference |u – v | The resulting vector captures information from both the text, u and the hypothesis, v, and is fed into a 3-class classifier consisting of multiple fully connected layers followed by a softmax layer. Universal Sentence Encoder这篇文章基于InferSent， 也是想找到一个universal encoder。不同之处在于文章把InferSent的bi-lstm换成了DAN（或者Transformer)，而使用DAN这样“简单”的encoder的效果竟然相当好（尤其是时间和内存消耗和其他算法比小很多。） The Google Sentence Encoder is Google’s answer to Facebook’s InferSent. It comes in two forms: an advanced model that takes the element-wise sum of the context-aware word representations produced by the encoding subgraph of a Transformer model. a simpler Deep Averaging Network (DAN) where input embeddings for words and bigrams are averaged together and passed through a feed-forward deep neural network.The Transformer-based model tends to give better results, but at the time of writing, only the DAN-based encoder was available. In contrast to InferSent, the Google Sentence Encoder was trained on a combination of unsupervised data (in a skip-thought-like task) and supervised data (the SNLI corpus). DAN其实DAN(Deep Averaging Networks)应该属于Bag of Words类的算法。因为比较特殊，单独列出来。 它是在对所有词语取平均后，在上面加上几层神经网络。特殊的地方在于它在sentiment analysis中表现也不错，这在BOW类方法中比较罕见。 新方法 类型 基于的旧算法 贡献 SIF 无监督 BOW 一个简单而有效的baseline算法 InferSent 监督 NA 找到了NLP领域的ImageNet – SNLI， 并给出了一个state-of-art 算法 P-mean 无监督 BOW 比SIF更简单且有效的一个算法且适用于cross-lingual Universal-sentence-encoder 监督 InferSent 更加简单的encoder 文章共提出两种基于不同网络架构的Universal Sentence Encoder：Transformer and Deep Averaging Network (DAN).Our two encoders have different design goals. One based on the transformer architecture targets high accuracy at the cost of greater model complexity and resource consumption. The other targets efficient inference with slightly reduced accuracy.]]></content>
      <categories>
        <category>NLP</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Dynamic Programming Examples]]></title>
    <url>%2F2019%2F05%2F27%2FDynamic-Programming-Examples%2F</url>
    <content type="text"><![CDATA[Dynamic Programming is a method for solving a complex problem by breaking it down into a collection of simpler subproblems, solving each of those subproblems just once, and storing their solutions using a memory-based data structure (array, map, etc.). So the next time the same sub-problem occurs, instead of recomputing its solution, one simply looks up the previously computed solution, thereby saving computation time. knapsack problem: https://leetcode.com/problems/partition-equal-subset-sum/ Given a non-empty array containing only positive integers, find if the array can be partitioned into two subsets such that the sum of elements in both subsets is equal. Actually, this is a 0/1 knapsack problem, for each number, we can pick it or not. Let us assume dp[i][j] means whether the specific sum j can be gotten from the first i numbers. If we can pick such a series of numbers from 0-i whose sum is j, dp[i][j] is true, otherwise it is false.Base case: dp[0][0] is true; (zero number consists of sum 0 is true)Transition function: For each number, if we don’t pick it, dp[i][j] = dp[i-1][j], which means if the first i-1 elements has made it to j, dp[i][j] would also make it to j (we can just ignore nums[i]). If we pick nums[i]. dp[i][j] = dp[i-1][j-nums[i]], which represents that j is composed of the current value nums[i] and the remaining composed of other previous numbers. Thus, the transition function is dp[i][j] = dp[i-1][j] || dp[i-1][j-nums[i]] 12345678910111213141516171819202122class Solution(object): def canPartition(self, nums): """ :type nums: List[int] :rtype: bool """ n =len(nums) s = sum(nums) if s &amp;1 ==1: return False dp =[0 for _ in range(s+1)] dp[0] =1 #import ipdb for num in nums: for i in range(s, -1, -1): # ipdb.set_trace() if dp[i]: dp[i+num] =1 if dp[s//2]: return True return False longest commone substring Given two strings ‘X’ and ‘Y’, find the length of the longest common substring. Input : X = “GeeksforGeeks”, y = “GeeksQuiz”Output : 5 1234567891011121314151617181920212223242526272829"""Given two strings ‘X’ and ‘Y’, find the length of the longest common substring."""class Solution(object): def minDistance(self, word1, word2): m =len(word1) n =len(word2) #dp=[ [None] for _ in range(n+1) for _ in range(m+1)] dp = [[None] *(n +1) for _ in range(m+1) ] for i in range(m+1): for j in range(n+1): if i ==0 or j ==0: dp[i][j] =0 # 这个是python 中语法决定的 word1[len(word1)] 是访问不到的，这个访问是从0开始的，所以只能是这样的 elif word1[i-1] == word2[j-1]: dp[i][j] =dp[i-1][j-1] +1 else: dp[i][j] =max(dp[i-1][j], dp[i][j-1]) return dp[m][n]solution =Solution()word1 ='abcdaf'word2 ='acbcf'result =solution.minDistance(word1, word2)print(result) matrix chain multiplication Given a sequence of matrices, find the most efficient way to multiply these matrices together. The problem is not actually to perform the multiplications, but merely to decide in which order to perform the multiplications. dp 的思想就是借助之前的 subquestion 的结果，然后计算更大的question，这个的核心在于减少了重复子单元的计算。 1234567891011121314151617181920212223242526272829303132333435import sys# 就是这个 l 和1 是很不容易分清楚的，所以这个是慎重使用的 def MatrixChainOrder(list1, len1): """ :param list1: list of matrix style :param l: len1gth of list :return: """ dp = [[0 for _ in range(len1)] for _ in range(len1)] for i in range(1, len1): dp[i][i] = 0 # l 个matrix 连成的意思 for ll in range(2, len1): for i in range(1, len1 - ll + 1): j = i + ll - 1 dp[i][j] = sys.maxint for k in range(i, j): tmp = dp[i][k] + dp[k + 1][j] + list1[i - 1] * list1[k] * list1[j] if tmp &lt; dp[i][j]: dp[i][j] = tmp return dp[1][len1 - 1]# Driver program to test above functionarr = [1, 2, 3, 4]arr1 = [2, 3, 6, 4, 5]size = len(arr)size1 = len(arr1)print("Minimum number of multiplications is " + str(MatrixChainOrder(arr, size)))print("Minimum number of multiplications is " + str(MatrixChainOrder(arr1, size1)))]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Beyond Word Embedding]]></title>
    <url>%2F2019%2F05%2F22%2Fbeyond-word-embedding%2F</url>
    <content type="text"><![CDATA[从one-hot 到 word2vec， 到elmo，简单介绍一下 NLP中词向量的过程。 进入正题之前，思考为什么要将词用向量来表示呢？这样可以给词语一个数学上的表示，使之可以适用于某些算法或数学模型。通常将词语表示成向量有如下两种方法: one-hot and distributed 表示法。前者属于稀疏向量，后者属于稠密向量。前者的缺点向量是相互独立的，无法通过距离函数比如 cosine 进行相似度的比较，并且如果维度 N非常大，俺么这个高纬度的表示也可能引发维度灾难。于是接着往下看吧… Traditional Word VectorsBefore diving directly into Word2Vec it’s worth while to do a brief overview of some of the traditional methods that pre-date neural embeddings. 这个是用来描述文章的，有一个大的dict，然后一片文章是如何进行表示、Bag of Words or BoW vector representations are the most common used traditional vector representation. Each word or n-gram is linked to a vector index and marked as 0 or 1 depending on whether it occurs in a given document. An example of a one hot bag of words representation for documents with one word. 局限性: 一方面只是一种counter，没有考虑语义信息；另一方面有些 words 是明显的 relevant than others.BoW representations are often used in methods of document classification where the frequency of each word, bi-word or tri-word is a useful feature for training classifiers. One challenge with bag of word representations is that they don’t encode any information with regards to the meaning of a given word.In BoW word occurrences are evenly weighted independently of how frequently or what context they occur. However in most NLP tasks some words are more relevant than others. 这个是可以认识是对于 bag of words “relevant” 上的改进：使得 选择的words 更加的 “representative” 文章的调性。TF-IDF, short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word or n-gram is to a document in a collection or corpus. They provide some weighting to a given word based on the context it occurs.The tf–idf value increases proportionally to the number of times a word appears in a document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently than others. 但是对于 bag of words 中“没有体现语义” 的缺陷还是没有 deal with。However even though tf-idf BoW representations provide weights to different words they are unable to capture the word meaning. 这个名字只是因为有定义而存在的名字（网络模型 or 深度网络的出现就是为了 handle 语义信息）Distributional Embeddings enable word vectors to encapsulate contextual context. Each embedding vector is represented based on the mutual information it has with other words in a given corpus.重点就是这种方式是要 predict a target word from context words，一定是要能够体现语境的。Predictive models learn their vectors in order to improve their predictive ability of a loss such as the loss of predicting the vector for a target word from the vectors of the surrounding context words. word2vec 两种类型 word2vec 是一种思想，有两种CBOW 和skip-gram 两种实现。Word2Vec is a predictive embedding model. There are two main Word2Vec architectures that are used to produce a distributed representation of words: Continuous bag-of-words (CBOW) — The order of context words does not influence prediction (bag-of-words assumption). Continuous skip-gram weighs nearby context words more heavily than more distant context words. While order still is not captured each of the context vectors are weighed and compared independently vs CBOW which weighs against the average context. word2vec 就是2 种算法+ 2种模型，总共是四种实现。 展示一下结构图： word2vec 的训练过程： 中文分词，然后保存所有的语料到一个文件中，可以使用 换行符进行分开 扫描语料库统计词频，取词频最高的V 个词，构成词汇表，然后建立Huffman 树，叶子节点表示的表示词，路径表示这个词的one-hot 编码（左为0 右为1）其余（出现频率很低）的词都用一个特殊符号代替掉。 词向量是从输入层到隐藏层的weights，随着初始化而存在，然后之后是不断优化的产物 训练的目标的，以skip-gram 为例，输入中心词然后最大化输出周围的词 (context )词汇。 输入层的输入：每个词存在一个one-hot向量，向量的维度是V（词典大小），如果该词在词汇表中出现过，则向量中词汇表中对应的位置为1，其他位置全为0。如果词汇表中不出现，则向量为全0 负采样（Negative Sample）和层次softmax（Hierarchical Softmax）则是两种加速训练的方法。都是优化最后的softmax 层（输出层），因为这个大小就是词典的大小，计算量太大了，如果知道softmax，是存在指数计算的。 loss function 在cbow模型中，所有的词被编码成ont-hot向量，V为总词语数。input层的one-hot vector经过 $W_{VXN} $矩阵后，被压缩为只有N个元素的向量h，之后经过W′矩阵出来，得到u。于是根据公式，有 $$p \left( w _ { t } | w _ { \text {input} } \right) = y _ { j } = \frac { \exp \left( u _ { j } \right) } { \sum \exp \left( u _ { j } \prime \right) }$$ 最大化该条件概率，得到 $$\max p \left( w _ { t } | w _ { \text {input} } \right) = \max \log y _ { j } = u _ { j } - \operatorname { log } \sum \exp \left( u _ { j } \right)$$于是得到了 词袋模型的 loss function：$$E = - \log p \left( w _ { t } | w _ { \text {input} } \right) = \log \sum \exp \left( u _ { j } \right) - u _ { j }$$这里，$u _ { j }$ 表示第 $j$ 个词向量， 有了 loss function，就可以进行词向量的训练了。 层次softmax 比如说一个二叉树结构，“我”肯定是第一层叶子节点，“涮羊肉”肯定是在最后一层的叶子节点。Hierarchical Softmax是用输出值的霍夫曼编码代替原本的One-Hot向量，用霍夫曼树替代Softmax的计算过程。 （现在这one-hot 编码 和 huffman 编码都是存在的，只不过一个是输入层的输入，一个是输出层的输入） 这个存储的目的是遍历的次数少了，因为是使用二分类去做多分类，如果词频高的编码少，那么最后的结果是比较少的。word2vec训练的时候按照词频将每个词语Huffman编码，由于Huffman编码中词频越高的词语对应的编码越短。所以越高频的词语在 Hierarchical Softmax过程中经过的二分类节点就越少，整体计算量就更少了。 总的特点：使用 context words 去predict 中心词 or 相反的过程，最大化这种概率关系。CBOW is faster while skip-gram is slower but does a better job for infrequent words.那么为什么快呢？ 答： cbow只要 把窗口内的其他词相加一次作为输入来预测 一个单词。不管窗口多大，只需要一次运算。而skip-gram直接受窗口影响，窗口越大，需要预测的周围词越多。在训练中，通过调整窗口大小明显感觉到训练速度受到很大影响。前者是复杂度大概是O(V)，后者的时间的复杂度为O(KV）(假设K 是窗口的大小) 为什么skip-gram 的准确率高一些，对于生僻词的效果更好一些？ 在skip-gram当中，每个词都要收到周围的词的影响，每个词在作为中心词的时候，都要进行K次的预测、调整。因此， 当数据量较少，或者词为生僻词出现次数较少时， 这种多次的调整会使得词向量相对的更加准确。因为尽管cbow从另外一个角度来说，某个词也是会受到多次周围词的影响（多次将其包含在内的窗口移动），进行词向量的跳帧，但是他的调整是跟周围的词一起调整的，grad的值会平均分到该词上， 相当于该生僻词没有收到专门的训练，它只是沾了周围词的光而已。 算法参数总体上对于效果影响不大，最重要的是语料。相对来说，比较重要的常用的参数： min-count： 最小词频训练阀值，这个根据训练语料大小设置，只有词频超过这个阀值的词才能被训练。根据经验，如果切词效果不好，会切错一些词，比如 “在深圳”，毕竟切错的是少数情况，使得这种错词词频不高，可以通过设置相对大一点的 min-count 过滤掉切错的词。（这种是对于新词处理的一种补救方法） 向量维度： 如果词量大，训练得到的词向量还要做语义层面的叠加，比如 句子 的向量表示 用 词的向量叠加，为了有区分度，语义空间应该要设置大一些，所以维度要偏大。一般 情况下200维够用。 负采样： 负采样（negative sampling）解决了这个问题，它是用来提高训练速度并且改善所得到词向量的质量的一种方法。不同于原本每个训练样本更新所有的权重，负采样每次让一个训练样本仅仅更新一小部分的权重，这样就会降低梯度下降过程中的计算量。 在论文中，作者指出指出对于小规模数据集，选择5-20个negative words会比较好，对于大规模数据集可以仅选择2-5个negative words。 任何采样算法都应该保证频次越高的样本越容易被采样出来。基本的思路是对于长度为1的线段，根据词语的词频将其公平地分配给每个词语：$$\operatorname { len } ( w ) = \frac { \operatorname { counter } ( w ) } { \sum _ { u \in \mathcal { D } } \operatorname { counter } ( u ) }$$ 在word2vec中，该“刻度尺”对应着table数组。具体实现时，对词频取了0.75次幂：$$\operatorname { len } ( w ) = \frac { [ \operatorname { counter } ( w ) ] ^ { 0.75 } } { \sum _ { u \in \mathcal { D } } [ \operatorname { counter } ( u ) ] ^ { 0.75 } }$$这个幂实际上是一种“平滑”策略，能够让低频词多一些出场机会，高频词贡献一些出场机会，劫富济贫。 （这个是一种计算上的优化，通过选取一部分结点（词汇）更新权重）负采样越低，对高频词越不利，对低频词有利。可以这么理解，本来高频词 词被迭代50次，低频词迭代10次，如果采样频率降低一半，高频词失去了25次迭代，而低频词只失去了5次。一般设置成le-5. ( 这个就是 $10^{-5}$ ) 在 fasttext 实现的时候 使用下面的超参数记性控制。 -neg number of negatives sampled [5] 窗口大小： 窗口大小影响 词 和前后多少个词的关系，和语料中语句长度有关，建议可以统计一下语料中，句子长度的分布，再来设置window大小。一般设置成8。 负采样 vs 窗口大小 负采样主要是为了降低模型计算量。如果没有负采样，模型需要把词汇表中没有出现在滑动窗口的词语当作负样本。然而在实际训练过程中，并不需要这么多的负样本，过多的负样本会导致模型学偏。 负采样的个数和滑动窗口的比例尽量控制在0.1-10之间，滑动窗口决定了正样本的数量，负采样的个数决定了负样本的个数，正负样本尽量不要差距太大，建议负采样的个数和滑动窗口的比例控制为1：1。 比较详细的介绍可以查看这里 如何评估 word2vec 训练的好坏？ 词聚类 （可以采用 kmeans 聚类，看聚类簇的分布） 词cos 相关性（查找cos相近的词） Analogy对比 （man-king， woman-queen） 使用tnse，pca等降维可视化展示 更多的评价方法可以参见这里. glove （g lou v） Intuition: Both CBOW and Skip-Grams are “predictive” models, in that they only take local contexts into account. word2vec does not take advantage of global context.(细节 能看懂就看)GloVe embeddings by contrast leverage the same intuition behind the co-occurrence matrix (共生矩阵) used distributional embeddings, but uses neural methods to decompose the co-occurrence matrix into more expressive and dense word vectors. 模型目标：进行词的向量化表示，使得向量之间尽可能多地蕴含语义和语法的信息。输入：语料库输出：词向量方法概述：首先基于语料库构建词的共现矩阵，然后基于共现矩阵和GloVe模型学习词向量。 下面是一个例子： sparse vectors 词-文档矩阵(Term-document matrix) 和 词共现矩阵(Term-term matrix)。Term-document matrix表示每个单词在文档中出现的次数(词频)，每一行是一个 term，每一列是一个 document两篇文档的向量相似 =&gt; 两篇文档相似，如上图 doc3 和 doc4，我们就认为它们是相似的。两个单词的向量相似 =&gt; 两个单词相似，如上图的 fool 和 clown，就是相似的。 Term-term matrix然后我们可以考虑更小的粒度，更小的上下文，也就是不用整篇文档，而是用段落(paragraph)，或者小的窗口(window of ±4 words)，所以这个时候，向量就是对上下文单词的计数，大小不再是文档长度 |D|，而是窗口长度 |V| 了，所以现在 word-word matrix 是 |V|*|V| 而 word2vec 得到的向量 dense vectors。不使用 negative sampling 的Wordvec 非常快，但准确率不高（57.4\%）,毕竟模型没有告诉什么是无关的word，模型很难对无关词汇进行惩戒，提高准确率。对于 synonym 问题，word2vec 是好于 glove，但从最终的效果上看，两者是不分彼此的。glove 使用了整体的信息，word2vec 只是使用了局部信息（local context）。 word2vec 和glove 的区别：Predictive的模型，如Word2vec，根据context预测中间的词汇，要么根据中间的词汇预测context，分别对应了word2vec的两种训练方式cbow和skip-gram。 Count-based模型，如GloVe，本质上是对共现矩阵进行降维。首先，构建一个词汇的共现矩阵，每一行是一个word，每一列是context。共现矩阵就是计算每个word在每个context出现的频率。由于context是多种词汇的组合，其维度非常大，我们希望像network embedding一样，在context的维度上降维，学习word的低维表示。该向量表示属于 sparse vectors 和word2vec 的效果比较：glove 和word2vec 相比没有 definitively better results，还是通过实验进行说话吧。While GloVe vectors are faster to train, neither GloVe or Word2Vec has been shown to provide definitively better results rather they should both be evaluated for a given dataset. fasttext这个主要是 each word + n-gram within each word， 最后的效果是好于 word2vec 的。FastText, builds on Word2Vec by learning vector representations for each word and the n-grams found within each word. The values of the representations are then averaged into one vector at each training step. While this adds a lot of additional computation to training it enables word embeddings to encode sub-word information. FastText vectors have been shown to be more accurate than Word2Vec vectors by a number of different measures. 简单说 fasttext 和word2vec 模型上的不同有两点： 模型的输出层：word2vec的输出层，对应的是每一个term，计算某term的概率最大；而fasttext的输出层对应的是 分类的label。不过不管输出层对应的是什么内容，起对应的vector都不会被保留和使用； fasttext则充分利用了h-softmax的分类功能，遍历分类树的所有叶节点，找到概率最大的label（一个或者N个） 模型的输入层：word2vec的输出层，是 context window 内的term；而fasttext对应的整个sentence的内容，包括term，也包括 n-gram的内容 更多信息可以参看博客。 overview of Neural NLP ArchitecturesDeep Feed Forward Networks 1D CNNs RNNs (LSTM/GRU) encoder- decoder 结构 attention and copy mechanisms这个是 attention 机制提出的背景：解决 句子中的长依赖；contextual impact (specific words may carry more importance at different steps)While in theory they can capture long term dependencies they tend to struggle modeling longer sequences, this is still an open problem. One cause for sub-optimal performance standard RNN encoder-decoder models for sequence to sequence tasks such as NER or translation is that they weight the impact each input vector evenly on each output vector when in reality specific words in the input sequence may carry more importance at different time steps.Attention mechanisms provide a means of weighting the contextual impact of each input vector on each output prediction of the RNN. These mechanisms are responsible for much of the current or near current state of the art in Natural language processing. attention In sum, algorithms can allocate attention, and they can learn how to do so, by adjusting the weights they assign to various inputs. Imagine a heat map over a photo. The heat is attention. 这个是要引出来 context word embeddings.One of the limits of traditional word vectors is that they presume that a word’s meaning is relatively stable across sentences.并不是物理上的二维关系能够表示词语之间的 relationship，有时候是需要高纬空间进行表示的。In fact, the strongest relationships binding a given word to the rest of the sentence may be with words quite distant from it.从 credit assignment的角度阐述了 neural networks 就是 allocating importance to input features。The fundamental task of all neural networks is credit assignment. Credit assignment is allocating importance to input features through the weights of the neural network’s model. Learning is the process by which neural networks figure out which input features correlate highly with the outcomes the net tries to predict, and their learnings are embodied in the adjusted quantities of the weights that result in accurate decisions about the data they’re exposed to.这个是传统的 LSTM （encoder -decoder） 模型，问题在于当句子过长（比如说大于20 words）之后，encoder 是无法 memory 之前的所有 words，所以效果就会变得差一些。 但是 attention 就是模仿了人翻译过程，一段作为一个单位，然后进行翻译。这样就可以持续保证较高中确率的输出。In neural networks, attention primarily serves as a memory-access mechanism. 每次的输出都是关注不同的地方，但是至于哪里更加重要，这个交给了 feedback mechanism 反向传播。下面的图片十分清晰的展示了 在翻译的过程中 “focus” 是不断地变化的。Above, a model highlights which pixels it is focusing on as it predicts the underlined word in the respective captions. Below, a language model highlights the words from one language, French, that were relevant as it produced the English words in the translation. As you can see, attention provides us with a route to interpretability. We can render attention as a heat map over input data such as words and pixels, and thus communicate to human operators how a neural network made a decision. (This could be the basis of a feedback mechanism whereby those humans tell the network to pay attention to certain features and not others.) (This could be the basis of a feedback mechanism whereby those humans tell the network to pay attention to certain features and not others.) Additionally in Machine Reading Comprehension and Summarization systems RNNs often tend to generate results, that while on first glance look structurally correct are in reality hallucinated or incorrect. One mechanism that helps mitigate some of these issues is the Copy Mechanism.copy mechanism 简单说来就是 word embedding or raw text.The copy mechanism is an additional layer applied during decoding that decides whether it is better to generate the next word from the source sentence or from the general embedding vocabulary. reading comprehension and summary 上面是说的在 machine translation，下面说的是 阅读理解 和 summary领域。Additionally in Machine Reading Comprehension and Summarization systems RNNs often tend to generate results, that while on first glance look structurally correct are in reality hallucinated or incorrect. One mechanism that helps mitigate some of these issues is the Copy Mechanism.copy mechanism 简单说来就是 decide word embedding from model or raw text.The copy mechanism is an additional layer applied during decoding that decides whether it is better to generate the next word from the source sentence or from the general embedding vocabulary. Taming Recurrent Neural Networks for Better Summarization有两种不同的 summarization:Two types of summarization：Extractive （You might think of these approaches as like a highlighter.） Abstractive（By the same analogy, these approaches are like a pen.）The great majority of existing approaches to automatic summarization are extractive – mostly because it is much easier to select text than it is to generate text from scratch.但是一个问题在于，只是使用 extrative way 可能得到相同的words，Problem 1: The summaries sometimes reproduce factual details inaccurately (e.g. Germany beat Argentina 3-2). This is especially common for rare or out-of-vocabulary words such as 2-0.Problem 2: The summaries sometimes repeat themselves (e.g. Germany beat Germany beat Germany beat…)Easier Copying with Pointer-Generator Networks。这个跟 attention 不是很相关，简单说就是In this way, the pointer-generator network is a best of both worlds, combining both extraction (pointing) and abstraction (generating). To tackle Problem 2 (repetitive summaries), we use a technique called coverage. The idea is that we use the attention distribution to keep track of what’s been covered so far, and penalize the network for attending to same parts again. elmo (e l mo)elmo 产生一个 embedding 是根据 context 产生的。ELMo is a model generates embeddings for a word based on the context it appears thus generating slightly different embeddings for each of its occurrence.（感觉理解一个概念都是 根据其 for example 进行理解的）For example, the word “play” in the sentence above using standard word embeddings encodes multiple meanings such as the verb to play or in the case of the sentence a theatre production. In standard word embeddings such as Glove, Fast Text or Word2Vec each instance of the word play would have the same representation. 参考blog:https://towardsdatascience.com/beyond-word-embeddings-part-2-word-vectors-nlp-modeling-from-bow-to-bert-4ebd4711d0ec http://www.abigailsee.com/2017/04/16/taming-rnns-for-better-summarization.html https://skymind.ai/wiki/word2vec]]></content>
      <categories>
        <category>NLP</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Hyper-parameter Optimization for Machine Learning]]></title>
    <url>%2F2019%2F05%2F22%2FHyperparameter-optimization-for-machine-learning%2F</url>
    <content type="text"><![CDATA[Following are four common methods of hyper-parameter optimization for machine learning in order of increasing efficiency: Manual Grid search Random search Bayesian model-based optimization 简单说一下前两种， manual 适合比较有经验的人进行调参，整一个模型好久了，对参数的设置比较熟悉；Grid Search 最好的理解方式就是 枚举，如果时间充足那么可以找到 search space 中的最优解。但如果是 limited time and space， 那么请往下看。 Random SearchFirst we will implement a common technique for hyper-parameter optimization: random search. Each iteration, we choose a random set of model hyper-parameters from a search space. Random search uses the following four parts: Domain: values over which to search Optimization algorithm: pick the next values at random! (yes this qualifies as an algorithm) Objective function to minimize: in this case our metric is cross validation ROC AUC Results history that tracks the hyper-parameters tried and the cross validation metric Random search can be implemented in the Scikit-Learn library using RandomizedSearchCV, however, because we are using Early Stopping (to determine the optimal number of estimators), we will have to implement the method ourselves (more practice!). This is pretty straightforward, and many of the ideas in random search will transfer over to Bayesian hyper-parameter optimization. Empirically, random search is very effective, returning nearly as good results as grid search with a significant reduction in time spent searching. However, it is still an uninformed method in the sense that it does not use past evaluations of the objective function to inform the choices it makes for the next evaluation. Case Study of random search 123456789101112131415161718192021222324252627282930313233343536373839# Load librariesfrom scipy.stats import uniformfrom sklearn import linear_model, datasetsfrom sklearn.model_selection import RandomizedSearchCV# data and model# Load datairis = datasets.load_iris()X = iris.datay = iris.target# Create logistic regressionlogistic = linear_model.LogisticRegression()# Create hyper-parameter Search Space# Create regularization penalty space# 如果比较少，那么久枚举出来penalty = ['l1', 'l2']# 如果是有规律的连续的，就使用这种方式列举出来# Create regularization hyper-parameter distribution using uniform distributionC = uniform(loc=0, scale=4)# Create hyper-parameter optionshyper-parameters = dict(C=C, penalty=penalty)# cv: cross validation, This approach involves randomly dividing the set of observations into k groups, or folds, of approximately equal size. The first fold is treated as a validation set, and the method is fit on the remaining k − 1 folds.# Create randomized search 5-fold cross validation and 100 iterationsclf = RandomizedSearchCV(logistic, hyper-parameters, random_state=1, n_iter=100, cv=5, verbose=0, n_jobs=-1)# Fit randomized searchbest_model = clf.fit(X, y)# View best hyper-parameters# 注意这种获取best params 的方式print('Best Penalty:', best_model.best_estimator_.get_params()['penalty'])print('Best C:', best_model.best_estimator_.get_params()['C'])# Predict target vectorbest_model.predict(X) Comparison between Grid Search and Random Search 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364import numpy as npfrom time import timefrom scipy.stats import randint as sp_randintfrom sklearn.model_selection import GridSearchCVfrom sklearn.model_selection import RandomizedSearchCVfrom sklearn.datasets import load_digitsfrom sklearn.ensemble import RandomForestClassifier# get some datadigits = load_digits()X, y = digits.data, digits.target# build a classifierclf = RandomForestClassifier(n_estimators=20)# Utility function to report best scoresdef report(results, n_top=3): for i in range(1, n_top + 1): candidates = np.flatnonzero(results['rank_test_score'] == i) for candidate in candidates: print("Model with rank: &#123;0&#125;".format(i)) print("Mean validation score: &#123;0:.3f&#125; (std: &#123;1:.3f&#125;)".format( results['mean_test_score'][candidate], results['std_test_score'][candidate])) print("Parameters: &#123;0&#125;".format(results['params'][candidate])) print("")# specify parameters and distributions to sample fromparam_dist = &#123;"max_depth": [3, None], "max_features": sp_randint(1, 11), "min_samples_split": sp_randint(2, 11), "bootstrap": [True, False], "criterion": ["gini", "entropy"]&#125;# run randomized searchn_iter_search = 20random_search = RandomizedSearchCV(clf, param_distributions=param_dist, n_iter=n_iter_search, cv=5, iid=False)start = time()random_search.fit(X, y)print("RandomizedSearchCV took %.2f seconds for %d candidates" " parameter settings." % ((time() - start), n_iter_search))report(random_search.cv_results_)# use a full grid over all parametersparam_grid = &#123;"max_depth": [3, None], "max_features": [1, 3, 10], "min_samples_split": [2, 3, 10], "bootstrap": [True, False], "criterion": ["gini", "entropy"]&#125;# run grid searchgrid_search = GridSearchCV(clf, param_grid=param_grid, cv=5, iid=False)start = time()grid_search.fit(X, y)print("GridSearchCV took %.2f seconds for %d candidate parameter settings." % (time() - start, len(grid_search.cv_results_['params'])))report(grid_search.cv_results_) Random search without in-built function: An example of Random Search all by yourself Bayesian hyper-parameter OptimizationThe one-sentence summary of Bayesian hyper-parameter optimization is: build a probability model of the objective function and use it to select the most promising hyper-parameters to evaluate in the true objective function. The basic idea is: spend a little more time selecting the next hyper-parameters in order to make fewer calls to the objective function. In the case of hyper-parameter optimization, the objective function is the validation error of a machine learning model using a set of hyper-parameters. The aim is to find the hyper-parameters that yield the lowest error on the validation set in the hope that these results generalize to the testing set. Evaluating the objective function is expensive because it requires training the machine learning model with a specific set of hyper-parameters. Ideally, we want a method that can explore the search space while also limiting evaluations of poor hyper-parameter choices. Bayesian hyper-parameter tuning uses a continually updated probability model to “concentrate” on promising hyper-parameters by reasoning from past results. 有很多基于这种思想的实现，hyperopt 只是其中一种There are several Bayesian optimization libraries in Python which differ in the algorithm for the surrogate of the objective function. In this article, we will work with Hyperopt, which uses the Tree Parzen Estimator (TPE) Other Python libraries include Spearmint (Gaussian Process surrogate) and SMAC (Random Forest Regression). There are four parts to a Bayesian Optimization problem: Objective Function: what we want to minimize, in this case the validation error of a machine learning model with respect to the hyper-parameters （原来model 中的 objective function） Domain Space: hyper-parameter values to search over （调参空间） Optimization algorithm: method for constructing the surrogate model and choosing the next hyper-parameter values to evaluate （loss 和调参空间的 新的关系） Result history: stored outcomes from evaluations of the objective function consisting of the hyper-parameters and validation loss （result 没有什么好说的） 其中的 Bayesian hyper-parameter Optimization using Hyperopt是可以好好学习的。 data scientists 这种东西更加贴近于 data scientist 真的。 给出两个参考代码:一二]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Introduction to Natural Language Processing for Text]]></title>
    <url>%2F2019%2F05%2F21%2FIntroduction-to-Natural-Language-Processing-for-Text%2F</url>
    <content type="text"><![CDATA[Natural Language Processing is used to apply machine learning algorithms to text and speech. For example, we can use it to create systems like speech recognition, document summarization, machine translation, spam detection, named entity recognition, question answering, autocomplete, predictive typing and so on. NLTK (Natural Language Toolkit) is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to many corpora and lexical resources. Also, it contains a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning. Best of all, NLTK is a free, open source, community-driven project. In this article, we’ll cover the following topics.这些功能都是可以使用nltk 进行实现的。text Lemmatization 比如，单词“cars”词形还原后的单词为“car”，单词“ate”词形还原后的单词为“eat”。 Sentence Tokenization段落成句。Sentence tokenization (also called sentence segmentation) is the problem of dividing a string of written language into its component sentences. The idea here looks very simple. In English and some other languages, we can split apart the sentences whenever we see a punctuation mark.（标点符号） Word Tokenization句子成词，颗粒度变得更小。Word tokenization (also called word segmentation) is the problem of dividing a string of written language into its component words. In English and many other languages using some form of Latin alphabet, space is a good approximation of a word divider. Text Lemmatization and Stemming这种操作如果被认为是一种 normalization，那么一个优点就是加快了运行的速度。从不同的形式到统一的形式，这可以认为减少了变量。感觉这个更加涉及语法，语法树之类的东西。For grammatical reasons, documents can contain different forms of a word such as drive, drives, driving. Also, sometimes we have related words with a similar meaning, such as nation, national, nationality. Stemming and lemmatization are special cases of normalization. However, they are different from each other. Stemming usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes.Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma. Stop Words因为 stop words往往是带了 noise rather than useful information，所以这个是要去掉的。Stop words are words which are filtered out before or after processing of text. When applying machine learning to text, these words can add a lot of noise. That’s why we want to remove these irrelevant words. stop words dictionary 可以理解成一种过滤词表，是可以根据应用的不同，然后 change的。Stop words usually refer to the most common words such as “and”, “the”, “a” in a language, but there is no single universal list of stopwords. The list of the stop words can change depending on your application. 在存储 stopword 的时候使用 set rather than list 主要原因是 much faster than search operations in a set.You might wonder why we convert our list into a set. Set is an abstract data type that can store unique values, without any particular order. The search operation in a set is much faster than the search operation in a list. For a small number of words, there is no big difference, but if you have a large number of words it’s highly recommended to use the set type. RegexA kind of search pattern. A regular expression, regex, or regexp is a sequence of characters that define a search pattern. Let’s see some basics. 12345678910. - match any character except newline\w - match word\d - match digit\s - match whitespace\W - match not word\D - match not digit\S - match not whitespace[abc] - match any of a, b, or c[^abc] - not match a, b, or c[a-g] - match a character between a &amp; g 这个解释说明了为什么在正则表达式 中使用 r”” 作为一种前缀。因为正则表达是中 ”\“ 的使用和 python 中的”\” 使用有冲突。简而言之，如果加上了 r”” 那么这个就是一种完全的 正则表达式的语法了。 Regular expressions use the backslash character (‘\’) to indicate special forms or to allow special characters to be used without invoking their special meaning. This collides with Python’s usage of the same character for the same purpose in string literals; for example, to match a literal backslash, one might have to write ‘\\‘ as the pattern string, because the regular expression must be \, and each backslash must be expressed as \ inside a regular Python string literal.The solution is to use Python’s raw string notation for regular expression patterns; backslashes are not handled in any special way in a string literal prefixed with ‘r’. So r”\n” is a two-character string containing ‘\’ and ‘n’, while “\n” is a one-character string containing a newline. Usually, patterns will be expressed in Python code using this raw string notation. An example, 1234import resentence = "The development of snowboarding was inspired by skateboarding, sledding, surfing and skiing."pattern = r"[^\w]"print(re.sub(pattern, " ", sentence)) Bag of words Machine learning algorithms cannot work with raw text directly, we need to convert the text into vectors of numbers. This is called feature extraction.The bag-of-words model is a popular and simple feature extraction technique used when we work with text. It describes the occurrence of each word within a document. 这个是 bag of words的”特点“： order or structure of words 没有体现出来。Any information about the order or structure of words is discarded. That’s why it’s called a bag of words. This model is trying to understand whether a known word occurs in a document, but don’t know where is that word in the document. The intuition is that similar documents have similar contents. Also, from a content, we can learn something about the meaning of the document. To use this model, we need to: Design a vocabulary of known words (also called tokens) Choose a measure of the presence of known words 1) 最简单的方式是 “occurrence” ，如果出现了 标为1 否则标为0；这种是最为简单的 bag of words 最的方式，这四个是一一对应的。注意体会。 The complexity of the bag-of-words model comes in deciding how to design the vocabulary of known words (tokens) and how to score the presence of known words. bag of words 中使用 “occurrence” 的方式的缺点：稀疏矩阵（当dict 很大的时候，文章的 representation中有相当成分的0）。 In some cases, we can have a huge amount of data and in this cases, the length of the vector that represents a document might be thousands or millions of elements. Furthermore, each document may contain only a few of the known words in the vocabulary.Therefore the vector representations will have a lot of zeros. These vectors which have a lot of zeros are called sparse vectors. They require more memory and computational resources.We can decrease the number of the known words when using a bag-of-words model to decrease the required memory and computational resources. We can use the text cleaning techniques we’ve already seen in this article before we create our bag-of-words model: 减少 dictionary size 的方式。 Ignoring punctuationRemoving the stop words from our documentsReducing the words to their base form (Text Lemmatization and Stemming)Fixing misspelled words n-gram 的思想是很广泛：通过 sequence of words，这个是可以增加文本的表达力的。An n-gram is a sequence of a number of items (words, letter, numbers, digits, etc.). In the context of text corpora, n-grams typically refer to a sequence of words. A unigram is one word, a bigram is a sequence of two words, a trigram is a sequence of three words etc. 关于如何去 score the presence of word： 这里是有三种方式的。We saw one very simple approach - the binary approach (1 for presence, 0 for absence).Some additional scoring methods are:2) Counts. Count the number of times each word appears in a document.3) Frequencies. Calculate the frequency that each word appears in document out of all the words in the document. TF-IDF 这个语境 是相对于 frequency 而言的，关键词是不一定有 频率所决定，而一些 rarer or domain-specific words 可能是更加常见的。One problem with scoring word frequency is that the most frequent words in the document start to have the highest scores. These frequent words may not contain as much “informational gain” to the model compared with some rarer and domain-specific words. One approach to fix that problem is to penalize words that are frequent across all the documents. This approach is called TF-IDF. TF-IDF 的关键在于体现了“语料库”。TF-IDF, short for term frequency-inverse document frequency is a statistical measure used to evaluate the importance of a word to a document in a collection or corpus. 参考资料https://towardsdatascience.com/introduction-to-natural-language-processing-for-text-df845750fb63]]></content>
      <categories>
        <category>NLP</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[剑指offer-其他]]></title>
    <url>%2F2019%2F05%2F17%2F%E5%89%91%E6%8C%87offer-%E5%85%B6%E4%BB%96%2F</url>
    <content type="text"><![CDATA[这是剑指offer 系列四部曲中的最后一部，因为有些算法题目类别数量太少就汇总到了”其他“, 比如位运算、正则匹配等。第一部关于字符串和数组，第二部是栈、队列、链表和树， 第三部递归、回溯和动态规划。 二进制中1的个数 输入一个整数，输出该数二进制表示中1的个数。其中负数用补码表示。 Tips：首先处理正数, bitwise and operation， 很简单。对于负数，需要转换成 正数然后进行处理，math。 123456789101112131415161718192021class Solution: def NumberOf1(self, n): # write code here if n == 0: return 0 if n &gt; 0: counts = self.number_of_positive(n) else: n = abs(n) - 1 counts = 32 - self.number_of_positive(n) return counts def number_of_positive(self, n): if n == 0: return 0 counts = 0 while n: counts += (n &amp; 1) n = n &gt;&gt; 1 return counts 数值的整数次方 给定一个double类型的浮点数base和int类型的整数exponent。求base的exponent次方。 Tips: 次方使用乘法来进行累乘 1234567891011121314151617181920212223242526272829303132333435class Solution: """ 这个就是边界条件比较多而已，需要分别判断 base 和 exponent 的正负 """ def Power(self, base, exponent): # write code here if base == 0 and exponent != 0: return 0 if base != 0 and exponent == 0: return 1 flag = 1 if base &lt;= 0 and (exponent % 2 == 1): flag = -1 base = abs(base) result = 1 if exponent &gt; 0: reverse = 0 else: reverse = 1 exponent = abs(exponent) if exponent % 2 == 0: result = base * base for i in range(exponent // 2 - 1): result = result * result else: result = base * base for i in range(exponent // 2 - 1): result = result * result result = result * base if reverse: result = 1.0 / result return result * flag 最小的K个数 输入n个整数，找出其中最小的K个数。例如输入4,5,1,6,2,7,3,8这8个数字，则最小的4个数字是1,2,3,4,。 Tips： 这个有点投机取巧了，使用了 “heapq” 的库函数。这个题目跟 第 K个 smallest 是有差别的，快排中的 partition 是找到了 一个数字在最后排序结果中的位置。对于有”累加“前 K个数字还是要使用常规的排序。比较好的就是堆排序。 123456789class Solution: # 想说的是既然是使用这种开源的库函数 那么就记住这种函数名字 def GetLeastNumbers_Solution(self, tinput, k): # write code here if len(tinput) &lt; k: return [] import heapq res = heapq.nsmallest(k, tinput) return res The function partition puts the numbers smaller than nums[left] to its left and then returns the new index of nums[left]. The returned index is actually telling us how small nums[left] ranks in nums. 123456789101112131415161718192021222324252627282930313233343536class Solution(object): def findKthLargest(self, nums, k): """ :type nums: List[int] :type k: int :rtype: int """ left, right = 0, len(nums) - 1 while True: pos = self.partition(nums, left, right) # 这个在排序的时候，是把大的数字放到前面，而前面是pos 是从0 开始的， # 所以这里是 k-1 if pos == k - 1: return nums[pos] # 左边的并不足以构成k 个， 那么在右边 elif pos &lt; k - 1: left = pos + 1 else: right = pos - 1 def partition(self, nums, left, right): # choose nums[left] as pivot pivot = nums[left] # p1, p2就类似 working 中的left right p1, p2 = left + 1, right while p1 &lt;= p2: if nums[p1] &lt; pivot and nums[p2] &gt; pivot: nums[p1], nums[p2] = nums[p2], nums[p1] p1, p2 = p1 + 1, p2 - 1 elif nums[p1] &gt;= pivot: p1 += 1 else: #nums[p2] &lt;= pivot: p2 -=1 nums[left], nums[p2] = nums[p2], nums[left] return p2 整数中1出现的次数（从1到n整数中1出现的次数） 求出1~13的整数中1出现的次数,并算出100~1300的整数中1出现的次数？为此他特别数了一下1~13中包含1的数字有1、10、11、12、13因此共出现6次,但是对于后面问题他就没辙了。ACMer希望你们帮帮他,并把问题更加普遍化,可以很快的求出任意非负整数区间中1出现的次数（从1 到 n 中1出现的次数）。 Tips：math, 计数原理，按位统计该位为1时可能包含的数字总数.由低位向高位依次遍历数字n的每一位curn。记当前位数为c，curn左边（高位）的数字片段为highn，cur右边（低位）的数字片段为lown，lowc = 10 ^ c 若curn = 0，则高位范围为0 ~ highn - 1，低位0 ~ lowc - 1 若curn = 1，则高位范围为0 ~ highn - 1，低位0 ~ lowc - 1；或者 高位为highn， 低位0 ~ lown 若curn ＞ 1，则高位范围为0 ~ highn， 低位为0 ~ lowc - 1 一个小的例子 N= abcde,分别是各个位数上的数字。如果要统计 百位上出现 1的次数，那么他将受到三个因素的影响： 百位上的数字，百位以下的数字(低位) 和百位以上的数字( 高位)。 如果百位上的数字是 0 ， 高位数字 * 当前的数字 如果百位上的数字是1，高位数字 * 当前数字 + ( 低位数字+1) 如果百位数字大于1 (2-9), （高位数字+1 )* 当前的数字 初级版本: 通过不断的求余 和整除，计算每个位置上 1的个数。12345678910111213141516171819int Count1InInteger(int n)&#123; int counts =0; while (n!=0) &#123; counts += (n%10) ==1? 1:0; n/=10; &#125; return counts;&#125;int main()&#123; int total =0; for (int i=0; i&lt;N; i++) &#123; total += Count1InInteger(i) &#125; return total;&#125; 进阶版本：python 版本，思路见上面分析。1234567891011121314151617181920212223242526272829303132333435363738class Solution: # 数字的基本结构分成 weights +working_num + n%base 这三个部分 # 然后一个while 循环是处理一个数字 def NumberOf1Between1AndN_Solution(self, n): # write code here if n &lt; 1: return 0 num = n counts = 0 base = 1 while num: cur = num % 10 num = num // 10 #高位数字 counts += base * num if cur == 1: counts += (n % base) + 1 elif cur &gt; 1: counts += base base *= 10 return counts ``` - 把数组排成最小的数&gt; 输入一个正整数数组，把数组里所有数字拼接起来排成一个数，打印能拼接出的所有数字中最小的一个。例如输入数组&#123;3，32，321&#125;，则打印出这三个数字能排成的最小数字为321323。Tips：使用sorted() 函数， string 类型的排序 和 int 类型的排序是一样的，在python 里面来说。```pythonclass Solution: def PrintMinNumber(self, numbers): # write code here sorted_list = sorted(numbers, cmp=lambda a, b: cmp(str(a) + str(b), str(b) + str(a))) # 这个时候已经排好序，然后只要一个个连接起来就行了 return ''.join(map(str, sorted_list)) 丑数 把只包含质因子2、3和5的数称作丑数（Ugly Number）。例如6、8都是丑数，但14不是，因为它包含质因子7。 习惯上我们把1当做是第一个丑数。求按从小到大的顺序的第N个丑数。 Tips：之后的丑数肯定是2，3或5 的倍数，分别单独计数，然后选择最小的。 123456789101112131415161718192021class Solution: # 在进行 append 操作的时候去重， def GetUglyNumber_Solution(self, index): # write code here if index &lt;1: return 0 list1 = [1] # 意味着只能是 append() 操作了 i, j, k = 0, 0, 0 while len(list1) &lt; index: num = min(list1[i] * 2, list1[j] * 3, list1[k] * 5) if num &gt; list1[-1]: list1.append(num) if num == list1[i] * 2: i += 1 elif num == list1[j] * 3: j += 1 else: k += 1 return list1[-1] 正则表达式匹配 请实现一个函数用来匹配包括’.’和’‘的正则表达式。模式中的字符’.’表示任意一个字符，而’‘表示它前面的字符可以出现任意次（包含0次）。 在本题中，匹配是指字符串的所有字符匹配整个模式。例如，字符串”aaa”与模式”a.a”和”abaca”匹配，但是与”aa.a”和”ab*a”均不匹配 Tips: dp 问题。转换方程 dp[i][j] i 表示 string 的index j表示 pattern 的index， dp[i][j] ==dp[i-1][j-1] or dp[i][j] =dp[i][j-2] or dp[i][j-1] 。 123456789101112131415161718192021222324class Solution: # s, pattern都是字符串 # https://www.youtube.com/watch?v=l3hda49XcDE 心中一定要有这个表格, a[i][j] 这个更像是一种指针 def match(self, s, pattern): if len(s) == 0 and len(pattern) == 0: return True dp = [[False for _ in range(len(pattern) + 1)] for _ in range(len(s) + 1)] dp[0][0] = True for j in range(1, len(pattern) + 1): if pattern[j - 1] == "*": dp[0][j] = dp[0][j - 2] for i in range(1, len(s) + 1): for j in range(1, len(pattern) + 1): if pattern[j - 1] == s[i - 1] or pattern[j - 1] == ".": dp[i][j] = dp[i - 1][j - 1] elif pattern[j - 1] == "*": dp[i][j] = dp[i][j - 2] if s[i - 1] == pattern[j - 2] or pattern[j - 2] == ".": dp[i][j] = dp[i][j] or dp[i - 1][j] else: dp[i][j] = False return dp[len(s)][len(pattern)] 数据流中的中位数 如何得到一个数据流中的中位数？如果从数据流中读出奇数个数值，那么中位数就是所有数值排序之后位于中间的数值。如果从数据流中读出偶数个数值，那么中位数就是所有数值排序之后中间两个数的平均值。我们使用Insert()方法读取数据流，使用GetMedian()方法获取当前读取数据的中位数。 Tips：主要是体现了数据流，要求能够 insert 元素，然后基于当前的状态去 getmedian() ，是动态的，而不是静态的。 1234567891011121314151617181920class Solution: """ 对于数据流 这个应该是第二次接触了，需要使用一个全局变量 """ # 虽然知道这个使用 堆的思想是更优的，搜索时间可以O（1）， 堆的调整是 O(log n) # 但是没有什么很好的教程，所以我也没有学会啊 def __init__(self): self.list1 = [] def Insert(self, num): self.list1.append(num) def GetMedian(self, ch): length = len(self.list1) # 我记得有一个更加快一些 self.list1 = sorted(self.list1) if length % 2 == 0: return (self.list1[length // 2] + self.list1[length // 2 - 1]) / 2.0 else: return self.list1[length // 2] 滑动窗口的最大值 给定一个数组和滑动窗口的大小，找出所有滑动窗口里数值的最大值。例如，如果输入数组{2,3,4,2,6,2,5,1}及滑动窗口的大小3，那么一共存在6个滑动窗口，他们的最大值分别为{4,4,6,6,6,5}； 针对数组{2,3,4,2,6,2,5,1}的滑动窗口有以下6个： {[2,3,4],2,6,2,5,1}， {2,[3,4,2],6,2,5,1}， {2,3,[4,2,6],2,5,1}， {2,3,4,[2,6,2],5,1}， {2,3,4,2,[6,2,5],1}， {2,3,4,2,6,[2,5,1]}。 Tips：使用max(list1) 这样的操作是可行的。 123456789class Solution: # 最简单的模拟滑动窗口 的过程 def maxInWindows(self, num, size): slip = [] if not num or len(num) &lt; size or size == 0: return [] for i in range(len(num) - size + 1): slip.append(max(num[i:i + size])) return slip]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>剑指offer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[剑指offer-递归、回溯和动态规划]]></title>
    <url>%2F2019%2F05%2F17%2F%E5%89%91%E6%8C%87offer-%E9%80%92%E5%BD%92-%E5%9B%9E%E6%BA%AF%E5%92%8C%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%2F</url>
    <content type="text"><![CDATA[这是剑指offer 系列四部曲中的第三部：递归、回溯和动态规划。第一部关于字符串和数组，第二部是栈、队列、链表和树， 最后一部分在这里。 斐波那契数列 大家都知道斐波那契数列，现在要求输入一个整数n，请你输出斐波那契数列的第n项（从0开始，第0项为0）。n&lt;=39 Tips: 简单的递归，可以转换成循环。 1234567891011121314class Solution: # python 中list 的初始化，最开始的是从0 开始，所以是需要多进行一个初始化的 def Fibonacci(self, n): # write code here if n == 0: return 0 if n == 1: return 1 arr = [0] * (n + 1) arr[0] = 0 arr[1] = 1 for i in range(2, n + 1): arr[i] = arr[i - 1] + arr[i - 2] return arr[n] 跳台阶 一只青蛙一次可以跳上1级台阶，也可以跳上2级。求该青蛙跳上一个n级的台阶总共有多少种跳法（先后次序不同算不同的结果）。 Tips： 同上。 123456789101112131415class Solution: def jumpFloor(self, number): # write code here if number == 1: return 1 if number == 2: return 2 arr = [0] * (number + 1) arr[1] = 1 arr[2] = 2 for i in range(3, number + 1): arr[i] = arr[i - 1] + arr[i - 2] return arr[number] 跳台阶2 一只青蛙一次可以跳上1级台阶，也可以跳上2级……它也可以跳上n级。求该青蛙跳上一个n级的台阶总共有多少种跳法。 Tips：同上。 12345678910111213class Solution: """ 在使用 for循环的时候，注意 range() 这种取值，究竟是使用 range() 作为次数的计量； 还是要使用range 中的index 。两者是不相同的操作，尤其是对于前后的取值。 """ def jumpFloorII(self, number): # write code here if number == 1: return 1 nums = 1 for i in range(number - 1): nums = nums * 2 return nums 矩形覆盖 我们可以用2*1的小矩形横着或者竖着去覆盖更大的矩形。请问用n个2*1的小矩形无重叠地覆盖一个2*n的大矩形，总共有多少种方法？ Tips: math, 找出递归方程。 1234567891011121314151617"""既然结果只是最后一个解，那么就没有必要保存中间变量，所以只是，所以空间复杂度从O（n） -&gt; O(1) ，这个是超级nice的"""class Solution: def rectCover(self, number): # write code here if number &lt;= 0: return 0 if number &lt;= 2: return number # 只是两个变量罢了 a, b = 1, 2 while number &gt; 2: a, b = b, a + b number -= 1 return b 机器人的运动范围 地上有一个m行和n列的方格。一个机器人从坐标0,0的格子开始移动，每一次只能向左，右，上，下四个方向移动一格，但是不能进入行坐标和列坐标的数位之和大于k的格子。 例如，当k为18时，机器人能够进入方格（35,37），因为3+5+3+7 = 18。但是，它不能进入方格（35,38），因为3+5+3+8 = 19。请问该机器人能够达到多少个格子？ Tips：递归，转移方程不难，在上下左右四个方向进行尝试，需要判断的条件比较多，比如是否访问过，数位之和等一些条件。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465class Solution: def movingCount(self, threshold, rows, cols): # visited 不一定是二维的，只要是能够"自圆其说" 就行。 visited = [False] * (rows * cols) count = self.movingCountCore(threshold, rows, cols, 0, 0, visited) return count def movingCountCore(self, threshold, rows, cols, row, col, visited): count = 0 # 就是这个访问记录是需要进行变化的， 如果是false ，然后访问之后 是需要设置为 true的 if self.check(threshold, rows, cols, row, col, visited): visited[row * cols + col] = True count = 1 + self.movingCountCore(threshold, rows, cols, row, col - 1, visited) + \ self.movingCountCore(threshold, rows, cols, row, col + 1, visited) + \ self.movingCountCore(threshold, rows, cols, row + 1, col, visited) + \ self.movingCountCore(threshold, rows, cols, row - 1, col, visited) return count def check(self, threshold, rows, cols, row, col, visited): if row &gt;= 0 and row &lt; rows and col &gt;= 0 and col &lt; cols and self.judge(threshold, row, col) and not visited[row * cols + col]: return True else: return False def judge(self, threshold, i, j): if sum(map(int, str(i) + str(j))) &lt;= threshold: return True else: return False``` - 矩阵中的路径&gt; 请设计一个函数，用来判断在一个矩阵中是否存在一条包含某字符串所有字符的路径。路径可以从矩阵中的任意一个格子开始，每一步可以在矩阵中向左，向右，向上，向下移动一个格子。如果一条路径经过了矩阵中的某一个格子，则之后不能再次进入这个格子。 例如 a b c e s f c s a d e e 这样的3 X 4 矩阵中包含一条字符串"bcced"的路径，但是矩阵中不包含"abcb"路径，因为字符串的第一个字符b占据了矩阵中的第一行第二个格子之后，路径不能再次进入该格子。Tips： 在二维数组中每个点上都进行尝试，每个点上同样是上下左右进行尝试，返回符合条件的。```pythonclass Solution: # 递归 这个是 true or false 判断类型的。 # 思路：先是 rows* cols 这样的全部遍历 def hasPath(self, matrix, rows, cols, path): # 如果使用 [ for _in range(rows) ] for _ in range(cols) ， 这个是有结构的 rows* cols assist = [True] * rows * cols for i in range(rows): for j in range(cols): if self.rightPath(matrix, rows, cols, i, j, path, assist): return True return False def rightPath(self, matrix, rows, cols, i, j, path, assist): if not path: return True index = i * cols + j if i &lt; 0 or i &gt;= rows or j &lt; 0 or j &gt;= cols or matrix[index] != path[0] or assist[index] == False: return False assist[index] = False if (self.rightPath(matrix, rows, cols, i + 1, j, path[1:], assist) or self.rightPath(matrix, rows, cols, i - 1, j, path[1:], assist) or self.rightPath(matrix, rows, cols, i, j - 1, path[1:], assist) or self.rightPath(matrix, rows, cols, i, j + 1, path[1:], assist)): return True assist[index] = True return False]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>剑指offer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[剑指Offer-栈、队列、链表和树]]></title>
    <url>%2F2019%2F05%2F17%2F%E5%89%91%E6%8C%87Offer-%E6%A0%88-%E9%98%9F%E5%88%97-%E9%93%BE%E8%A1%A8%E5%92%8C%E6%A0%91%2F</url>
    <content type="text"><![CDATA[这是剑指offer 系列四部曲中的第二部：栈、队列、链表和树。第一部关于字符串和数组，第三部是递归、回溯和动态规划， 最后一部分在这里。 从尾到头打印链表 输入一个链表，按链表值从尾到头的顺序返回一个ArrayList。 Tips: 从尾到头，考察是栈的数据结构，在python 中使用list 来实现栈。 123456789101112131415161718# -*- coding:utf-8 -*-# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: # 返回从尾部到头部的列表值序列，例如[1,2,3] # += , -= 这个都是同一种类型的 def printListFromTailToHead(self, listNode): # write code here arraylist =[] head = listNode while head != None: arraylist += [head.val] # 这个在这里等效于 arraylist.append(head.val) head = head.next return arraylist[::-1] % 重建二叉树 输入某二叉树的前序遍历和中序遍历的结果，请重建出该二叉树。假设输入的前序遍历和中序遍历的结果中都不含重复的数字。例如输入前序遍历序列{1,2,4,7,3,5,6,8}和中序遍历序列{4,7,2,1,5,3,8,6}，则重建二叉树并返回。 Tips: 递归，二叉树的题目大多数都是可以使用递归的思想进行解决，因为二叉树本身结构就是递归定义的。递归优点在于代码量比较少。从先序遍历中找出根节点，从中序遍历中找出左右子树。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152# -*- coding:utf-8 -*-# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: # 返回构造的TreeNode根节点 # 需要理解在前序遍历中是先遍历左子树的，并且中序和前序中左子树的个数是不会变的 def reConstructBinaryTree(self, pre, tin): # write code here if len(pre) == 0: return None root = TreeNode(pre[0]) # 这个index 函数是需要记住的 index = tin.index(pre[0]) # 这里也是需要修改的 # pre 和 tin都是需要空出一个 root.value 的位置，只不过选择空的位置是不一样的 root.left = self.reConstructBinaryTree(pre[1:index + 1], tin[:index]) root.right = self.reConstructBinaryTree(pre[index + 1:], tin[index + 1:]) return root``` %- 用两个栈实现队列&gt; 用两个栈来实现一个队列，完成队列的Push和Pop操作。 队列中的元素为int类型。Tips： 在python 中栈等同于使用list 实现。使用两个栈，意味着一个是push_stack 一个是pop_stack，使用两个栈的“后进先出”表示队列的先进先出（push and pop）从语法上讲 ，if list1 ==[], 那么 list1 ==None, 这两个条件是可以交换判断的。（在list 中）```pythonclass Solution: def __init__(self): self.list1 =[] self.list2 =[] def push(self, node): # write code here self.list1.append(node) def pop(self): # return if not self.list1 and not self.list2 : return None if self.list2 : return self.list2.pop() else: while self.list1: self.list2.append(self.list1.pop()) return self.list2.pop() 链表中倒数第k个结点 输入一个链表，输出该链表中倒数第k个结点。 Tips： 两种解法。一种是遍历存储到list 中，空间复杂度是O(N), 另外一种是两个指针p1，p2，距离相差k，当p2 到达链表尾部，p1 就在导数第k 个位置。 1234567891011121314151617181920212223242526272829303132# -*- coding:utf-8 -*-# class ListNode:# def __init__(self, x):# self.val = x# self.next = None"""尝试使用两个指针版本p1 p2 并且这种 length 在命名上是需要规范的, 并且这种指针操作，最好是拷贝出来进行操作不管怎么说，还是应该求解出来 length of listNode，这种才是正途可以使用两个指针，"""class Solution: def FindKthToTail(self, head, k): # write code here if head == None or k &lt;= 0: return None p1 = head p2 = head len1 = 0 while p1: len1 += 1 p1 = p1.next if k &gt; len1: return None p1 = head while k: p1 = p1.next k -= 1 while p1: p1 = p1.next p2 = p2.next return p2 反转链表 输入一个链表，反转链表后，输出新链表的表头。 Tips： 需要三个指针，cur，next_node, pre。 1234567891011121314151617181920212223242526# -*- coding:utf-8 -*-# class ListNode:# def __init__(self, x):# self.val = x# self.next = None"""修改链表是需要三个指针的 pre, cur, next_node 如果对三个指针名进行命名好了，那么这个就是成功的一般了， 这个不容易想到的是设置pre =None ，这个是一个细节经验性的问题"""class Solution: # 返回ListNode def ReverseList(self, pHead): # write code here if pHead ==None: return None pre =None cur =pHead while cur: next_node =cur.next cur.next =pre pre, cur =cur, next_node return pre 合并两个排序的链表 Tips： 归并排序中的“并” 操作，只不过由原来的list 操作到现在的 linkedlist 操作。 输入两个单调递增的链表，输出两个链表合成后的链表，当然我们需要合成后的链表满足单调不减规则。 1234567891011121314151617181920212223242526272829303132# def __init__(self, x):# self.val = x# self.next = None"""就是在使用两个或者多个 index (p1 or p2) 遍历的时候，一个常见的错误就是忘记了不断更新index"""class Solution: # 返回合并后列表 def Merge(self, pHead1, pHead2): # write code here if pHead1 == None: return pHead2 if pHead2 == None: return pHead1 head = ListNode(-1) head1 = head p1 = pHead1 p2 = pHead2 while p1 and p2: if p1.val &lt; p2.val: head.next = p1 p1 = p1.next else: head.next = p2 p2 = p2.next head = head.next if p1 == None: head.next = p2 if p2 == None: head.next = p1 return head1.next 树的子结构 输入两棵二叉树A，B，判断B是不是A的子结构。（ps：我们约定空树不是任意一个树的子结构） Tips：根节点相同，左右子树相同。（数值和结构）。实现的使用有两个递归程序，意味着有两个跳出的条件。一个是从A树中找结点和B 树的根节点，一个是根节点相同之后，判断左右子树是否相同。 12345678910111213141516171819202122232425262728293031323334# -*- coding:utf-8 -*-# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = None"""分成两部：首先寻找两个根节点的值是否相同；然后判断子树是否完全相同subTree 这个函数就是判断子树是否完全相同的，所以函数的功能一定要搞好"""class Solution: def HasSubtree(self, pRoot1, pRoot2): if not pRoot1: return False if not pRoot2: return False result =False if pRoot1.val ==pRoot2.val: result =self.subTree(pRoot1, pRoot2) if result ==False: result = self.HasSubtree(pRoot1.left, pRoot2) or self.HasSubtree(pRoot1.right, pRoot2) return result def subTree(self, root1, root2): if not root2: return True if not root1: return False if root1.val ==root2.val: return self.subTree(root1.left, root2.left) and self.subTree(root1.right, root2.right) return False 二叉树的镜像 操作给定的二叉树，将其变换为源二叉树的镜像。 Tips：求解二叉树镜像，A 的左右子树对应着B 的右左子树。 123456789101112131415161718192021# -*- coding:utf-8 -*-# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = None"""就是在某个左（右）子树是None 的情况下，这个也是可以进行交换的，结束的标志应该是根节点是否为空"""class Solution: # 返回镜像树的根节点 def Mirror(self, root): # write code here if not root: return None root.left , root.right =root.right, root.left if root.left: self.Mirror(root.left) if root.right: self.Mirror(root.right) return root 包含min函数的栈 定义栈的数据结构，请在该类型中实现一个能够得到栈中所含最小元素的min函数（时间复杂度应为O（1））。 Tips: 这个跟“使用两个栈表示队列” 是差不多的，就是单独使用一个list 存储min 函数调用的一个列表，这样的话能达到时间复杂度是 O(1). 1234567891011121314151617181920212223242526272829303132# -*- coding:utf-8 -*-"""这个栈中最小的元素是变化的，好好理解一下，如果弹出了一个比较大的元素，那么栈中最小的元素是不变的所含元素的最小元素top() and min() 操作是不需要删除元素的， pop 是删除了元素"""class Solution: def __init__(self): self.all_list = [] self.min_list = [] def push(self, node): # write code here if not self.min_list: self.min_list.append(node) else: self.min_list.append(min(node, self.min())) self.all_list.append(node) def pop(self): self.all_list.pop() self.min_list.pop() # write code here def top(self): return self.all_list[-1] # write code here def min(self): return self.min_list[-1] 栈的压入、弹出序列 输入两个整数序列，第一个序列表示栈的压入顺序，请判断第二个序列是否可能为该栈的弹出顺序。假设压入栈的所有数字均不相等。例如序列1,2,3,4,5是某栈的压入顺序，序列4,5,3,2,1是该压栈序列对应的一个弹出序列，但4,3,5,1,2就不可能是该压栈序列的弹出序列。（注意：这两个序列的长度是相等的） Tips: 使用一个list 来模拟压入和弹出过程，遍历弹出序列popV，如果结束，那么return True。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849class Solution:def IsPopOrder(self, pushV, popV): if not pushV: return False tmp =[] while popV: if tmp and popV[0] == tmp[-1]: popV.pop(0) tmp.pop() elif pushV: tmp.append(pushV.pop(0)) else: return False return True ``` - 从上往下打印二叉树&gt; 从上往下打印出二叉树的每个节点，同层节点从左至右打印。Tips： 层次遍历，遍历根节点之后加入左右结点。```python# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: # 返回从上到下每个节点值列表，例：[1,2,3] # 层序遍历二叉树， 这个跟数据结构 队列有类似的 # nodes 装上结点，然后vlaues 装上数值 def PrintFromTopToBottom(self, root): # write code here if not root: return [] nodes =[] values = [] nodes.append(root) while nodes: node = nodes.pop(0) values.append(node.val) if node.left: nodes.append(node.left) if node.right: nodes.append(node.right) return values 二叉搜索树的后序遍历序列 输入一个整数数组，判断该数组是不是某二叉搜索树的后序遍历的结果。如果是则输出Yes,否则输出No。假设输入的数组的任意两个数字都互不相同。 Tips：二叉搜索树，按照中序遍历的话，就是一个排序的二叉树，根节点大于左子树，右子树大于根节点。后序遍历序列中最后一个是根节点，小于根节点是左子树，大于根节点的是右子树，这样进行判断。 123456789101112131415class Solution: # 后序遍历结果， 最后一个是根节点，这个是递归的思想 # 二叉搜索树， 左子树小于根节点，右子树大于根节点 def VerifySquenceOfBST(self, sequence): # write code here if not sequence: return False root = sequence[-1] for i in range(len(sequence)): if sequence[i] &gt; root: break for j in range(i, len(sequence)): if sequence[j] &lt; root: return False return True 二叉树中和为某一值的路径 输入一颗二叉树的跟节点和一个整数，打印出二叉树中结点值的和为输入整数的所有路径。路径定义为从树的根结点开始往下一直到叶结点所经过的结点形成一条路径。(注意: 在返回值的list中，数组长度大的数组靠前) Tips： 树的遍历，深度优先算法（dfs） 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: # 返回二维列表，内部每个列表表示找到的路径 # 深度优先 dfs() 这样的一个算法 def FindPath(self, root, expectNumber): # write code here if not root: return [] self.target = expectNumber paths = [] self.dfs(root, [root.val], paths) return pathsdef dfs(self, root, path, paths): if not root.left and not root.right and sum(path) == self.target: paths.append(path) if root.left: self.dfs(root.left, path + [root.left.val], paths) if root.right: self.dfs(root.right, path + [root.right.val], paths) ``` - 复杂链表的复制&gt; 输入一个复杂链表（每个节点中有节点值，以及两个指针，一个指向下一个节点，另一个特殊指针指向任意一个节点），返回结果为复制后复杂链表的head。（注意，输出结果中请不要返回参数中的节点引用，否则判题程序会直接返回空）Tips: 先是在原来的链表上进行了相同结点的copy和next 指针的指向，然后是random 指针的指向，最后是将原始链表和copy 的链表进行分离。```python # -*- coding:utf-8 -*- # class RandomListNode: # def __init__(self, x): # self.label = x # self.next = None # self.random = None class Solution: # 返回 RandomListNode # 首先是结点的复制和 next 指针的连接， 然后是random 指针的连接，最后是选择出复制的结点 def Clone(self, pHead): # write code here if not pHead: return None self.clone_nodes(pHead) self.connect_nodes(pHead) return self.select_nodes(pHead) def clone_nodes(self, head): if not head: return None while head: cloned = RandomListNode(head.label) cloned.next = head.next head.next = cloned head = cloned.next def connect_nodes(self, head): if not head: return None while head: cloned = head.next if head.random: cloned.random = head.random.next head = cloned.next def select_nodes(self, head): if not head: return None cloned =cloned_head =None # 这个if 的作用是为了保存一个 cloned_head的结点， # 一定要从这个功能出发 if head: cloned =cloned_head =head.next head.next =cloned.next head =head.next while head: cloned.next =head.next cloned =cloned.next head.next =cloned.next head =head.next return cloned_head 二叉搜索树与双向链表 输入一棵二叉搜索树，将该二叉搜索树转换成一个排序的双向链表。要求不能创建任何新的结点，只能调整树中结点指针的指向。 Tips：中序遍历二叉搜索树就是一种排序的书的结点，然后树的左右指针可以作为链表中的指向使用。 12345678910111213141516171819202122232425262728293031323334# -*- coding:utf-8 -*-# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: # 使用的树的结构 表示一种双向链表 # 二叉搜索树 ，左子树小于根节点，右子树大于根节点 # 中序遍历得到就是一种排好序的结构 # 只能调整树中结点指针的指向 def Convert(self, pRootOfTree): # write code here if not pRootOfTree: return None tree = pRootOfTree res = [] self.helper(tree, res) for i in range(len(res) - 1): res[i].right = res[i + 1] res[i + 1].left = res[i] # 这个返回值也是比较鬼畜呀， 就是需要这样返回 return res[0]def helper(self, root, res): if not root: return None if root.left: self.helper(root.left, res) res.append(root) if root.right: self.helper(root.right, res) 两个链表的第一个公共结点 输入两个链表，找出它们的第一个公共结点。 Tips：就是一个 m*n 的问题（m，n 分别代表两个链表的长度） 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647# -*- coding:utf-8 -*-# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: # 两个指针指向的是 一个结点，一个内存的两个指向 # 将可能不同长度的两个链表转换成相同长度的两个链表的比较，使用 def FindFirstCommonNode(self, pHead1, pHead2): # write code here if not pHead1 or not pHead2: return None p1 = pHead1 p2 = pHead2 while p1 != p2: # 这个p1 只能指向了最后一个结点，但最后一个节点不一定相同 p1 = pHead2 if not p1 else p1.next p2 = pHead1 if not p2 else p2.next return p1 ``` - 二叉树的深度&gt; 输入一棵二叉树，求该树的深度。从根结点到叶结点依次经过的结点（含根、叶结点）形成树的一条路径，最长路径的长度为树的深度。Tips：递归，相比于二叉树的路径，这个只是返回一个数值就行。```python# -*- coding:utf-8 -*-# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: """ 分别求解 左右子树的深度，然后max(left, right) 这样的操作 """ def TreeDepth(self, pRoot): if not pRoot: return 0 left = self.TreeDepth(pRoot.left) + 1 right = self.TreeDepth(pRoot.right) + 1 # 这个return 是最后执行一次的，然后上面那个都是不断的在进行递归加深 # 这个 left right 已经完成了，最后的效果只是 返回 max(left, right) 这样子 return max(left, right) 平衡二叉树 输入一棵二叉树，判断该二叉树是否是平衡二叉树。 Tips： 左右子树的深度差最大不超过1。两个递归，一个是计算树的深度的递归，一个是判断左右子树是否是平衡二叉树的递归。 123456789101112131415161718192021222324252627# -*- coding:utf-8 -*-# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: # 递归常见的都会有两个return 跳出条件，一个是异常的条件，一个是正确的返回 def get_depth(self, root): if not root: return 0 left =self.get_depth(root.left) right =self.get_depth(root.right) return max(left, right) +1 def IsBalanced_Solution(self, pRoot): if not pRoot: return True left =self.get_depth(pRoot.left) right =self.get_depth(pRoot.right) if abs(left-right) &gt;1: return False return self.IsBalanced_Solution(pRoot.left) and self.IsBalanced_Solution(pRoot.right) 链表中环的入口结点 给一个链表，若其中包含环，请找出该链表的环的入口结点，否则，输出null。 Tips： 两个快慢指针，开指针在环内相遇慢指针。（两个指针一个需要再环外，一个在环内，然后同样的速度走，最后才能相遇）重置快指针到头结点，两个指针相同速度，当再次相遇时候，那就是入口结点。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556 # -*- coding:utf-8 -*- # class ListNode: # def __init__(self, x): # self.val = x # self.next = None class Solution: # 现在长个记性吧，在使用next 这样的时候 要先判断这个是不是存在的 def EntryNodeOfLoop(self, pHead): # write code here if not pHead or not pHead.next or not pHead.next.next: return None twoTimes =pHead.next.next oneTime =pHead.next while twoTimes != oneTime: twoTimes =twoTimes.next.next oneTime =oneTime.next twoTimes =pHead while twoTimes != oneTime: twoTimes =twoTimes.next oneTime =oneTime.next return twoTimes ``` - 删除链表中重复的结点&gt; 在一个排序的链表中，存在重复的结点，请删除该链表中重复的结点，重复的结点不保留，返回链表头指针。 例如，链表1-&gt;2-&gt;3-&gt;3-&gt;4-&gt;4-&gt;5 处理后为 1-&gt;2-&gt;5```python# -*- coding:utf-8 -*-# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: def deleteDuplication(self, pHead): # write code here head = ListNode(-1) head.next = pHead curr = pHead last = head while curr and curr.next: # val =curr.val # 这个条件比较简单，所以可以放到前面 if curr.val != curr.next.val: curr = curr.next last = last.next else: # 这个条件 curr 还是需要注意一下的 val = curr.val # python 中 condition1 and condition2 这种是有先后顺序的 # 可能是存在短路现象的， 如果 curr 不成立，那么后面的是不会执行的 # 草拟 while curr and val == curr.val: curr = curr.next last.next = curr return head.next 二叉树的下一个结点 给定一个二叉树和其中的一个结点，请找出中序遍历顺序的下一个结点并且返回。注意，树中的结点不仅包含左右子结点，同时包含指向父结点的指针。 Tips：中序遍历的下一个结点，如果存在右节点，那么下一个结点是右节点最左边的一个点；如果该结点是其父节点的左结点，那么下一节点是其父节点，否则一直回溯。 123456789101112131415161718192021222324252627282930# -*- coding:utf-8 -*-# class TreeLinkNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = None# self.next = Noneclass Solution: # https://blog.csdn.net/fuxuemingzhu/article/details/79723819 # 这个是求解中序遍历中某个结点的下一个结点 # 这pNode 就是一个普通的结点 def GetNext(self, pNode): # write code here if not pNode: return None # 如果存在右结点 if pNode.right: pNode = pNode.right while pNode.left: pNode = pNode.left return pNode # 如果是父节点的左子树 else: # 这里使用 pNode.next 表示父节点 while pNode.next: if pNode == pNode.next.left: return pNode.next # 这个是右结点 pNode = pNode.next return None 对称的二叉树 请实现一个函数，用来判断一颗二叉树是不是对称的。注意，如果一个二叉树同此二叉树的镜像是同样的，定义其为对称的。 Tips: 判断镜像和递归生成进行还是不太一样的哈。递归判断，根节点相同，然后左右子树是否是对称。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677# -*- coding:utf-8 -*-# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: # 镜像的概念 和递归 # isSame() 这个就是判断两个子树是否镜像的操作 def isSame(self, p, q): if not p and not q: return True # 好好思考 下面这两个跳出条件为什么是不合适的 if p and q and p.val == q.val: return self.isSame(p.left, q.right) and self.isSame(p.right, q.left) def isSymmetrical(self, pRoot): # write code here # 最开始的条件 如果都是 none 那么这个是对称的 if not pRoot: return True if pRoot.left and not pRoot.right: return False if not pRoot.left and pRoot.right: return False return self.isSame(pRoot.left, pRoot.right) ``` - 按之字形顺序打印二叉树&gt; 请实现一个函数按照之字形打印二叉树，即第一行按照从左到右的顺序打印，第二层按照从右至左的顺序打印，第三行按照从左到右的顺序打印，其他行以此类推。Tips：层序遍历的升级版，有两种思路，一种是使用单独 stack (list) 的思想存储偶数层数，一种是先按照原先层序遍历的思想，最后对于偶数的结果进行“翻转” 处理。选择后者，因为代码上比较简单。```python# -*- coding:utf-8 -*-# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: # 层序遍历 + 偶数翻转 # https://blog.csdn.net/fuxuemingzhu/article/details/79724959 def level(self, root, level, res): """ root: the root of tree level: res: result """ if not root: return if len(res) == level: res.append([]) res[level].append(root.val) if root.left: self.level(root.left, level + 1, res) if root.right: self.level(root.right, level + 1, res) def Print(self, pRoot): # write code here if not pRoot: return [] res = [] self.level(pRoot, 0, res) for level in range(1, len(res), 2): res[level] = res[level][::-1] return res 把二叉树打印成多行 从上到下按层打印二叉树，同一层结点从左至右输出。每一层输出一行。 Tips: 和上一个题目类似，在遍历二叉树的时候，关键是加入了 [level] 层数这种信息。 123456789101112131415161718192021222324252627282930# -*- coding:utf-8 -*-# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: # 返回二维列表[[1,2],[4,5]] def level(self, root, level, res): # 你这里也没有说要返回值的意思呀，这个直接是 return if not root: return if level == len(res): res.append([]) res[level].append(root.val) if root.left: self.level(root.left, level + 1, res) if root.right: # res[level] =self.level(root.right, level+1, res) # 因为这个是 传的值，所以不需要使用返回值的 self.level(root.right, level + 1, res) def Print(self, pRoot): if not pRoot: return [] res = [] self.level(pRoot, 0, res) return res 序列化二叉树 请实现两个函数，分别用来序列化和反序列化二叉树 Tips：序列号和反序列化只是一种约定的存储的形式。 # -*- coding:utf-8 -*- # class TreeNode: # def __init__(self, x): # self.val = x # self.left = None # self.right = None class Solution: """ 序列化就是从树结构 转成字符串的结构；反之，也是成立的。 使用先序遍历的方法。 https://suixinblog.cn/2019/03/target-offer-serialize-binary-tree.html#%E4%BB%A3%E7%A0%81 """ def __init__(self): self.flag = -1 def Serialize(self, root): # write code here if not root: return "#" return str(root.val) + "," + self.Serialize(root.left) + "," + self.Serialize(root.right) def Deserialize(self, s): # write code here self.flag += 1 string = s.split(',') if self.flag &gt; len(string): return None root = None if string[self.flag] != '#': root = TreeNode(int(string[self.flag])) root.left = self.Deserialize(s) root.right = self.Deserialize(s) return root 二叉搜索树的第k个结点 给定一棵二叉搜索树，请找出其中的第k小的结点。例如， （5，3，7，2，4，6，8） 中，按结点数值大小顺序第三小结点的值为4。 Tips: 二叉搜索树，中序遍历之后有序，然后取第 k 个结点。 # class TreeNode: # def __init__(self, x): # self.val = x # self.left = None # self.right = None class Solution: def middle(self, root, result): if not root: return if root.left: self.middle(root.left, result) result.append(root) if root.right: self.middle(root.right, result) def KthNode(self, pRoot, k): # write code here if not pRoot: return result = [] self.middle(pRoot, result) if len(result) &lt; k or k &lt; 1: return return result[k - 1]]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>剑指offer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[剑指Offer-字符串和数组]]></title>
    <url>%2F2019%2F05%2F17%2F%E5%89%91%E6%8C%87Offer-%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%92%8C%E6%95%B0%E7%BB%84%2F</url>
    <content type="text"><![CDATA[题目都是来自牛客网在线刷题中的剑指offer。最近找实习工作，作为刷题记录，顺便从考察知识点的角度分类整理。主要分成以下四大类： 字符串、数组 链表、树 递归、回溯、动态规划 其他, 比如位运算、正则匹配等 不同类别以一章介绍，之后可能会随时update。这是剑指offer 系列四部曲中的第一部。第一部关于字符串和数组，第二部是栈、队列、链表和树，第三部递归、回溯和动态规划， 最后一部分在这里。 二维数组中的查找 在一个二维数组中（每个一维数组的长度相同），每一行都按照从左到右递增的顺序排序，每一列都按照从上到下递增的顺序排序。请完成一个函数，输入这样的一个二维数组和一个整数，判断数组中是否含有该整数。 Tips: 数组是从左上方到右下方这样的递增，指针（两个）有两个运动方向，一个是向左一个是向下。 12345678910111213141516171819class Solution:# array 二维列表sdef Find(self, target, array): # write code here # 可以尝试一下 坐标移动的思想, 所以这个就是一种坐标移动的思想 # 就是一个条件有了之后 那么接下来的else 也可以顺着就写上来的 rows =len(array) -1 cols =len(array[0]) -1 row =0 col =cols while row &lt;= rows and col &gt;=0: if array[row][col] == target: return True elif array[row][col] &gt; target: col -=1 else: row +=1 return False 替换空格 请实现一个函数，将一个字符串中的每个空格替换成“\%20”。例如，当字符串为We Are Happy.则经过替换之后的字符串为We\%20Are\%20Happy。 Tips： 字符串的遍历对于 python 而言是比较简单的。 123456789101112class Solution:# s 源字符串def replaceSpace(self, s): # write code here # python 中的 str 就是 array of char converted ="" for ch in s: if ch ==" ": converted += "%20" else: converted += ch return converted 旋转数组的最小数字 把一个数组最开始的若干个元素搬到数组的末尾，我们称之为数组的旋转。 输入一个非减排序的数组的一个旋转，输出旋转数组的最小元素。 例如数组{3,4,5,1,2}为{1,2,3,4,5}的一个旋转，该数组的最小值为1。 NOTE：给出的所有元素都大于0，若数组大小为0，请返回0。 123456789101112131415161718192021222324class Solution:'''题目是不减，所以下面判断的时候也是不减。解题的关键点： 设置第一个元素为假定的min_value,然后注意这个是非减的数组（注意处理等号的情况）'''def minNumberInRotateArray(self, rotateArray): # write code here arr =rotateArray if len(arr) ==0: return 0 min_value =arr[0] left, right =0, len(arr)-1 while right-left &gt;1: mid =(left+right) //2 if arr[left] &lt;=arr[mid]: left =mid elif arr[mid] &lt;= arr[right]: right =mid min_value =arr[right] return min_value 调整数组顺序使奇数位于偶数前面 输入一个整数数组，实现一个函数来调整该数组中数字的顺序，使得所有的奇数位于数组的前半部分，所有的偶数位于数组的后半部分，并保证奇数和奇数，偶数和偶数之间的相对位置不变。 Tips： 这个跟快速排序是有点像的，从右左各找到一个不符合条件，然后交换位置。快排中这个过程是线性的，非并行。下面的实现从算法角度并不是nice的，使用了python 中的list，最好的应该是 inplace 的那种，立地交换。 1234567891011121314151617"""list 和list 之间的连接，使用 list1+ list2 就是可以的"""class Solution: def reOrderArray(self, array): # write code here if array ==[]: return [] odd_list =[] even_list =[] for item in array: if item% 2 ==1: odd_list.append(item) else: even_list.append(item) return odd_list+even_list 上面版本保留了原始数字相对的顺序，下面这个没有保留相对的顺序。前者的空间复杂度是O(N), 后者的空间复杂度是O(1). 123456789101112131415161718class Solution: def reOrderArray(self, array): if len(array) ==0: return [] left =0 right =len(array)-1 while left &lt; right: # 如果是奇数 key =array[left] while left &lt; right and array[right] &amp; 1 == 0: right -= 1 array[left] = array[right] # 如果是偶数 while left &lt; right and array[left] &amp; 1 == 1: left += 1 array[right] = key return array 顺时针打印矩阵 输入一个矩阵，按照从外向里以顺时针的顺序依次打印出每一个数字，例如，如果输入如下4 X 4矩阵： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 则依次打印出数字1,2,3,4,8,12,16,15,14,13,9,5,6,7,11,10. Tips: 给定一个起始点，然后按照顺时针旋转去遍历。最后处理单行或者单列的情况。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768class Solution: """ 最后的 必须是三种特殊情况，如果修改成两种，那么”只剩下一个“ 这种特殊情况就被计算了两次 """# matrix类型为二维列表，需要返回列表def printMatrix(self, matrix): rows = len(matrix) cols = len(matrix[0]) top = 0 left = 0 down = rows - 1 right = cols - 1 result = [] while top &lt; down and left &lt; right: for j in range(left, right + 1): result.append(matrix[top][j]) top += 1 for i in range(top, down + 1): result.append(matrix[i][right]) right -= 1 for j in range(right, left - 1, -1): result.append(matrix[down][j]) down -= 1 for i in range(down, top - 1, -1): result.append(matrix[i][left]) left += 1 if top == down and left &lt; right: for j in range(left, right + 1): result.append(matrix[top][j]) if top &lt; down and left == right: for i in range(top, down + 1): result.append(matrix[i][left]) if top == down and left == right: result.append(matrix[top][left]) return result ``` - 字符串的排列&gt; 输入一个字符串,按字典序打印出该字符串中字符的所有排列。例如输入字符串abc,则打印出由字符a,b,c所能排列出来的所有字符串abc,acb,bac,bca,cab和cba。Tips: 递归```pythonclass Solution:# 递归： 变换方程： 第一字母和 剩下的所有的字母def Permutation(self, ss): # write code here if not ss: return [] res = [] self.helper(ss, '', res) return sorted(list(set(res)))def helper(self, ss, path, res): if not ss: res.append(path) for i in range(len(ss)): self.helper(ss[:i] + ss[i + 1:], path + ss[i], res) 数组中出现次数超过一半的数字 数组中有一个数字出现的次数超过数组长度的一半，请找出这个数字。例如输入一个长度为9的数组{1,2,3,2,2,2,5,4,2}。由于数字2在数组中出现了5次，超过数组长度的一半，因此输出2。如果不存在则输出0。 Tips： 如果某个数字出现的次数多于一半，那么其他所有非该数字的出现的频数是小于该数字的，所以形成一个二分类。 1234567891011121314151617181920class Solution:# 如果存在这样的数字，那么这个数字的频数一定是大于其他所有的频数# 所以可以统计一下这个def MoreThanHalfNum_Solution(self, numbers): # write code here if not numbers: return 0 target = numbers[0] nums = 0 # 统计出现次数最多的数字 for i in numbers: if target == i: nums += 1 elif nums == 0: target = i nums = 1 else: nums -= 1 res = target if numbers.count(target) &gt; len(numbers) // 2 else 0 return res 连续子数组的最大和 HZ偶尔会拿些专业问题来忽悠那些非计算机专业的同学。今天测试组开完会后,他又发话了:在古老的一维模式识别中,常常需要计算连续子向量的最大和,当向量全为正数的时候,问题很好解决。但是,如果向量中包含负数,是否应该包含某个负数,并期望旁边的正数会弥补它呢？例如:{6,-3,-2,7,-15,1,2,2},连续子向量的最大和为8(从第0个开始,到第3个为止)。给一个数组，返回它的最大连续子序列的和，你会不会被他忽悠住？(子向量的长度至少是1) Tips: 一维数组，遍历一遍，然后最大子数组和的过程。 12345678910111213141516class Solution:# largest , sum 这是两个不同的状态# 注意初始化def FindGreatestSumOfSubArray(self, array): # write code here if not array: return [] largest =array[0] sum_of_array =0 for i in array: sum_of_array += i if sum_of_array &gt; largest: largest =sum_of_array elif sum_of_array &lt;0: sum_of_array =0 return largest 第一个只出现一次的字符 在一个字符串(0&lt;=字符串长度&lt;=10000，全部由字母组成)中找到第一个只出现一次的字符,并返回它的位置, 如果没有则返回 -1（需要区分大小写）. 1234567891011121314151617181920212223242526272829303132class Solution: """ Three ways to get dictionary of string s """ def FirstNotRepeatingChar(self, s): if not s: return -1 # get dictionary from collections import defaultdict dict1 =defaultdict(int) for string in s: dict1[string] += 1 # or this way # from collections import Counter # dict1 =Counter(s) # or do it yourself #dict1 =self.Counter_self(s) for index, val in enumerate(s): if dict1[val] == 1: return -1 def Counter_self(self, s): dict1 = &#123;&#125; for val in s: if val not in dict1: dict1[val] = 1 else: dict1[val] = dict1[val] + 1 return dict1 数组中的逆序对 在数组中的两个数字，如果前面一个数字大于后面的数字，则这两个数字组成一个逆序对。输入一个数组,求出这个数组中的逆序对的总数P。并将P对1000000007取模的结果输出。 即输出P%1000000007 123456789101112131415161718192021222324252627282930313233343536373839404142# -*- coding:utf-8 -*-# 之前在牛客网上是可以通过的，现在因为时间复杂度又没办法通过了class Solution: # 这个一斤难出天机了 先不看了 # 这个到后来就已经背下来了 def InversePairs(self, data): # write code here if not data: return 0 temp = [i for i in data] return self.mergeSort(temp, data, 0, len(data ) -1) % 1000000007 def mergeSort(self, temp, data, low, high): if low &gt;= high: temp[low] = data[low] return 0 mid = (low + high) / 2 # 不懂 data 和 temp 为什么是颠倒顺序 left = self.mergeSort(data, temp, low, mid) right = self.mergeSort(data, temp, mid +1, high) count = 0 i = low j = mid +1 index = low while i &lt;= mid and j &lt;= high: if data[i] &lt;= data[j]: temp[index] = data[i] i += 1 else: temp[index] = data[j] count += mid - i +1 j += 1 index += 1 while i &lt;= mid: temp[index] = data[i] i += 1 index += 1 while j &lt;= high: temp[index] = data[j] j += 1 index += 1 return count + left + right 数字在排序数组中出现的次数 统计一个数字在排序数组中出现的次数。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859class Solution:# 二分查找，当 data[mid] ==key 的时候是顺序查找，是递归跳出的条件def GetNumberOfK(self, data, k): # write code here # 这个是有两个跳出条件的，一个是正确的跳出，一个是不正确的跳出 if not data: return 0 mid =len(data) // 2 if data[mid] == k: left = right = mid for i in range(mid - 1, -1, -1): if data[i] == k: left -= 1 for i in range(mid + 1, len(data)): if data[i] == k: right += 1 return right - left + 1 # 一半一半的舍去数据 elif data[mid] &lt; k: return self.GetNumberOfK(data[mid + 1:], k) else: return self.GetNumberOfK(data[:mid - 1], k) ``` - 数组中只出现一次的数字&gt; 一个整型数组里除了两个数字之外，其他的数字都出现了两次。请写程序找出这两个只出现一次的数字。Tips: 异或操作，两个十进制数字经过异或（二级制计算过程），最后的结果是10进制的形式。如果两个相同的数字异或，那么最后的结果是0，如果是不同的数字，最后的结果是非0.比如&gt; 2^4 # 4&gt; 3^4 # 7&gt; 40^42 # 2结果的二进制形式一定至少有一个 "1". 使用index 得到两个不同的数字二进制形式下的位置，然后从该位置将原来的数组分成两类，那么每类中只含有一个出现一次的数字，接着使用异或操作。```python class Solution: # 返回[a,b] 其中ab是出现一次的两个数字 # 使用异或的性质，如果只有一个不同，其他的偶次出现，那么全部异或的结果 # 就是那个单一的数字 def FindNumsAppearOnce(self, array): # write code here remain, index =0, 1 for num in array: remain = remain ^ num # 找出第一个是1 的位置 # index 都是 while (remain &amp; index) ==0: index = index &lt;&lt;1 res1, res2 =0,0 for num in array: # 这个条件必须是0, 表示两个在这个位数是相同的， if num &amp; index ==0: res1 =res1 ^ num else: res2 =res2 ^ num return [res1, res2] 和为S的连续正数序列 小明很喜欢数学,有一天他在做数学作业时,要求计算出9~16的和,他马上就写出了正确答案是100。但是他并不满足于此,他在想究竟有多少种连续的正数序列的和为100(至少包括两个数)。没多久,他就得到另一组连续正数和为100的序列:18,19,20,21,22。现在把问题交给你,你能不能也很快的找出所有和为S的连续正数序列? Tips: 滑动窗口（两个指针） 12345678910111213141516def FindContinuousSequence(self, tsum): # write code here if tsum &lt; 2: return [] left = 1 right = left + 1 res = [] while left &lt; tsum // 2 + 1: if sum(range(left, right)) == tsum: res.append(range(left, right)) left += 1 elif sum(range(left, right)) &lt; tsum: right += 1 else: left += 1 return res 和为S的两个数字 输入一个递增排序的数组和一个数字S，在数组中查找两个数，使得他们的和正好是S，如果有多对数字的和等于S，输出两个数的乘积最小的。 和上一个题目的不同点在于，该题目是给定了某个递增的数组。上一个题目默认的是 (0, tsum//2+1) 这样的序列。 12345678910111213141516class Solution: def FindNumbersWithSum(self, array, tsum): # write code here if len(array) &lt; 2: return [] left = 0 right = len(array) - 1 while left &lt; right: if array[left] + array[right] == tsum: return [array[left], array[right]] elif array[left] + array[right] &lt; tsum: left += 1 else: right -= 1 return [] 左旋转字符串 汇编语言中有一种移位指令叫做循环左移（ROL），现在有个简单的任务，就是用字符串模拟这个指令的运算结果。对于一个给定的字符序列S，请你把其循环左移K位后的序列输出。例如，字符序列S=”abcXYZdef”,要求输出循环左移3位后的结果，即“XYZdefabc”。是不是很简单？OK，搞定它！ Tips: python中字符串的处理是没有压力的 1234567891011121314151617181920212223242526272829303132333435363738394041424344class Solution: def LeftRotateString(self, s, n): # write code here if len(s) &lt; n: return '' return s[n:] + s[:n]``` - 翻转单词顺序列&gt; 牛客最近来了一个新员工Fish，每天早晨总是会拿着一本英文杂志，写些句子在本子上。同事Cat对Fish写的内容颇感兴趣，有一天他向Fish借来翻看，但却读不懂它的意思。例如，“student. a am I”。后来才意识到，这家伙原来把句子单词的顺序翻转了，正确的句子应该是“I am a student.”。Cat对一一的翻转这些单词顺序可不在行，你能帮助他么？Tips：首先定义一个helper() 功能是翻转字符串，然后两次翻转。第一次是全部翻转，第二次是单词翻转。```pythonclass Solution: def Reverse(self, s, left, right): while left &lt;right: s[left], s[right] = s[right], s[left] left +=1 right -=1 def ReverseSentence(self, s): # write code here if not s: return s # from immutable string to mutable list s =list(s) self.Reverse(s, 0, len(s) - 1) start, end = 0, 0 # 这个小于号 是python 中特有的坑，真正能够访问的区间是 [0, len(s)-1] 这样的区间 while start &lt; len(s): if s[start] == " ": start += 1 end += 1 elif end == len(s) or s[end] == " ": self.Reverse(s, start, end - 1) # update 操作 end += 1 start = end else: end += 1 return "".join(s) 扑克牌顺子 LL今天心情特别好,因为他去买了一副扑克牌,发现里面居然有2个大王,2个小王(一副牌原本是54张^_^)…他随机从中抽出了5张牌,想测测自己的手气,看看能不能抽到顺子,如果抽到的话,他决定去买体育彩票,嘿嘿！！“红心A,黑桃3,小王,大王,方片5”,“Oh My God!”不是顺子…..LL不高兴了,他想了想,决定大/小 王可以看成任何数字,并且A看作1,J为11,Q为12,K为13。上面的5张牌就可以变成“1,2,3,4,5”(大小王分别看作2和4),“So Lucky!”。LL决定去买体育彩票啦。 现在,要求你使用这幅牌模拟上面的过程,然后告诉我们LL的运气如何， 如果牌能组成顺子就输出true，否则就输出false。为了方便起见,你可以认为大小王是0。 Tips：list 中的空缺数量 需要不大于 大小王总数，这样才能构成顺子。 12345678910111213141516171819202122232425class Solution: """ 空缺是1 意味着这两个数字是连续的 比如说 1 2， 这个big -small ==1, 所以这个空缺是0，不用进行填充。 """ def IsContinuous(self, numbers): # write code here if not numbers: return False numbers.sort() # sort() sorted() 这种怎么使用，返回值是什么，这些基本的东西 zeros =numbers.count(0) gaps = 0 left = zeros # 因为这个是排序之后的结果，所以可以这样进行操作 right = left + 1 # 实际上还是两个指针， 所以可以使用两个指针进行操作 # 本质上是两个 相邻指针在进行移动，因为是排序之后，所以没有问题 while right &lt; len(numbers): if numbers[left] == numbers[right]: return False gaps += numbers[right] - numbers[left] - 1 left = right right += 1 # 这种是真的 很简洁， gaps &lt;= zeros 少去了很多if else的判断 return gaps &lt;= zeros 孩子们的游戏(圆圈中最后剩下的数) 每年六一儿童节,牛客都会准备一些小礼物去看望孤儿院的小朋友,今年亦是如此。HF作为牛客的资深元老,自然也准备了一些小游戏。其中,有个游戏是这样的:首先,让小朋友们围成一个大圈。然后,他随机指定一个数m,让编号为0的小朋友开始报数。每次喊到m-1的那个小朋友要出列唱首歌,然后可以在礼品箱中任意的挑选礼物,并且不再回到圈中,从他的下一个小朋友开始,继续0…m-1报数….这样下去….直到剩下最后一个小朋友,可以不用表演,并且拿到牛客名贵的“名侦探柯南”典藏版(名额有限哦!!^_^)。请你试着想下,哪个小朋友会得到这份礼品呢？(注：小朋友的编号是从0到n-1) Tips: 约瑟夫环的问题， 要求求解的是最后胜利者的编号，所以应用数学技巧就可以了。 $ f(x) = (f( x-1) + m ) % (x) $ , 共有 m 个编号，n 个人 12345678910111213class Solution:# mod 求余 的操作， a mod b ==c ,说明 a除以b 之后余数是c# https://blog.csdn.net/gatieme/article/details/51435055， 从做题思路上讲解的比较好# n 个小朋友，然后是m 个编号def LastRemaining_Solution(self, n, m): # write code here if n&lt; 1 or m&lt;1: return -1 last =0 for i in range(2, n+1): # 这个相当于 是一个 “挑选人” 的逆过程， 因为使用的 mod 操作就是取余的操作 last =(last +m) %i return last 求1+2+3+…+n 求1+2+3+…+n，要求不能使用乘除法、for、while、if、else、switch、case等关键字及条件判断语句（A?B:C）。 Tips： 不使用条件判断，来控制跳出；这里使用的是 “短路条件” 来 控制 递归的跳出。 12345678910111213141516class Solution: # 如果你想使用全局变量，那么放在 __init__ 中就是一个很好的方式 def __init__(self): self.ans =0 def Sum_Solution(self, n): # write code here self.recur(n) return self.ans # n&gt;0 就是一个短路条件，这个直接决定了后面递归会不会继续执行下去，也就是跳出的条件 # 至于会不会回到原来最初的状态，这个是不重要的，最后的结果是 self.ans ，false之后直接使用这个就行了 def recur(self, n): self.ans += n n -= 1 return n &gt; 0 and self.Sum_Solution(n) 不用加减乘除做加法 写一个函数，求两个整数之和，要求在函数体内不得使用+、-、*、/四则运算符号。 Tips: 使用 异或和与 来进行 “加”、“减”的操作。加法是分成当前位相加 和进位两个部分的。ps，python 中 函数名是分大小写的。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253class Solution: """ 不能使用四则运算符，我们就可以使用位运算符。对这两个数在更底层的角度上进行运算。也就是从 01 这种子串的角度进行考虑 https://blog.csdn.net/derrantcm/article/details/46798763 这个博客对于数的运算过程和 位运算是如何一一对应的。分为不进位相加和进位相加。 """ # 这个只能是正整数的操作，真是热力狗 # 这个弄出来是真心不容易哈 def add(self, num1, num2): while num2 != 0: carry = num1 &amp; num2 num1 = num1 ^ num2 # 这个应该理解为到高位 而不是*2 这样的操作 num2 = carry &lt;&lt; 1 return num1 def sub(self, num1, num2): while num2 != 0: carry = (~num1) &amp; num2 num1 = num1 ^ num2 num2 = carry &lt;&lt; 1 return num1 def Add(self, num1, num2): if num1 &gt;= 0 and num2 &gt;= 0: result = self.add(num1, num2) elif num1 &gt; 0 and num2 &lt; 0: flag = 1 if num1 &gt; abs(num2) else -1 # num2 =abs(num2) # keep num1 bigger than num2 if num1 &lt; abs(num2): num1, num2 = abs(num2), num1 result = self.sub(num1, abs(num2)) result = result * flag elif num1 &lt; 0 and num2 &gt; 0: flag = 1 if abs(num1) &lt; num2 else -1 if abs(num1) &lt; num2: num1, num2 = num2, abs(num1) result = self.sub(abs(num1), num2) result = result * flag else: flag = -1 num1 = abs(num1) num2 = abs(num2) result = self.add(num1, num2) result = result * flag return result 把字符串转换成整数 将一个字符串转换成一个整数(实现Integer.valueOf(string)的功能，但是string不符合数字要求时返回0)，要求不能使用字符串转换整数的库函数。 数值为0或者字符串不是一个合法的数值则返回0。 Tips：还是python 中处理 string， 使用 dictionary 处理字符串和 数字的匹配。 12345678910111213141516171819202122class Solution:# 有很多不合法的输入，比如空字符串，正负号，非数字字符 数据溢出，所以从反面考虑更加简单一些# 合法的输入只有数字和符号位 + 和-def StrToInt(self, s): # write code here int_list=['0', '1', '2', '3', '4', '5', '6','7', '8', '9', '+', '-'] if s ==" ": return 0 sum1 =0 flag =1 # 正负号 for string in s: if string not in int_list: return 0 if string =="+": flag =1 continue elif string =="-": flag = -1 continue else: sum1 =sum1 *10 +int_list.index(string) return sum1*flag 数组中重复的数字 在一个长度为n的数组里的所有数字都在0到n-1的范围内。 数组中某些数字是重复的，但不知道有几个数字是重复的。也不知道每个数字重复几次。请找出数组中任意一个重复的数字。 例如，如果输入长度为7的数组{2,3,1,0,2,5,3}，那么对应的输出是第一个重复的数字2。 Tips： 当然这个是可以使用dictionary，需要O(N) 这样的空间。还有一种思路，这个所有的数字都是 (0, n-1) 这样的区间，所以是可以和index 进行联系一下的。这个 “交换” 是会进行排序的（index 对应着 number），但排序不是最终的目的，最终目的是得到首个重复的数字。 12345678910111213141516171819202122232425262728293031323334353637383940414243class Solution: # 这里要特别注意~找到任意重复的一个值并赋值到duplication[0] # 函数返回True/False # 第二种方式，如果这个是有序的 那么 numbers[i] ==i 这个是成立的 def duplicate(self, numbers, duplication): # write code here length =len(numbers) for i in range(length): while i != numbers[i]: if numbers[numbers[i]] == numbers[i]: duplication[0] = numbers[i] return True else: numbers[numbers[i]], numbers[i] = numbers[i], numbers[numbers[i]] return False``` - 构建乘积数组&gt; 给定一个数组A[0,1,...,n-1],请构建一个数组B[0,1,...,n-1],其中B中的元素B[i]=A[0]*A[1]*...*A[i-1]*A[i+1]*...*A[n-1]。不能使用除法。Tips: 分成上下三角形进行计算，不能每个B[i] 都进行单独重复计算。下三角形是从上往下遍历，上三角形是从下往上遍历。ans 存储各个不同的结果。```python class Solution: # 思路： 转换成图形的就容易想一些， https://blog.csdn.net/u010005281/article/details/80200398 # 代码：https://blog.csdn.net/fuxuemingzhu/article/details/79718543 # A 是一个list ，只是自己构建的是一个矩阵 def multiply(self, A): # write code here ans =[] tmp =1 length =len(A) # 值得是 rows # 首先是下三角形 各个部分的数值的相乘， 从上往下遍历 for i in range(length): ans.append(tmp) tmp *= A[i] tmp =1 # 上三角形 从下往上进行遍历 for i in range(length-1, -1, -1): ans[i] *= tmp tmp *= A[i] return ans 表示数值的字符串 请实现一个函数用来判断字符串是否表示数值（包括整数和小数）。例如，字符串”+100”,”5e2”,”-123”,”3.1416”和”-1E-16”都表示数值。 但是”12e”,”1a3.14”,”1.2.3”,”+-5”和”12e+4.3”都不是。 123456789101112131415161718192021222324252627282930313233343536373839class Solution: # s字符串 # 第一种方法是 float()强转，一种是 re 正则表达式匹配 最后一种逻辑判断之类的 # 以 e 为分割符，分成front and behind 两部分，behind 长度不能为0 或者出现 . # digit的判断，+- 只能出现在首位， . 只能出现一次 """ https://github.com/leeguandong/Interview-code-practice-python/blob/master/%E5%89%91%E6%8C%87offer/%E8%A1%A8%E7%A4%BA%E6%95%B0%E5%80%BC%E7%9A%84%E5%AD%97%E7%AC%A6%E4%B8%B2.py """ def isNumeric(self, s): if not s or len(s) == 0: return s s = [i.lower() for i in s] if 'e' in s: index = s.index('e') front = s[:index] behind = s[index + 1:] if len(behind) == 0 or '.' in behind: return False f = self.Digit(front) b = self.Digit(behind) return f and b else: isNum = self.Digit(s) return isNum def Digit(self, s): dotNum = 0 allowNum = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '.', '+', '-'] for i in range(len(s)): if s[i] not in allowNum: return False if s[i] == '.': dotNum += 1 if s[i] in '+-' and i != 0: return False if dotNum &gt; 1: return False return True 字符流中第一个不重复的字符 请实现一个函数用来找出字符流中第一个只出现一次的字符。例如，当从字符流中只读出前两个字符”go”时，第一个只出现一次的字符是”g”。当从该字符流中读出前六个字符“google”时，第一个只出现一次的字符是”l”。 12345678910111213141516171819class Solution: # 返回对应char # 这个没有了dict 那么依赖于 count 函数 # 主要差别在于有了一个 字符流，是动态的，所以需要有一个大的存储的list def __init__(self): self.list1 =[] def FirstAppearingOnce(self): # write code here for string in self.list1: if self.list1.count(string) == 1: return string return "#" def Insert(self, char): # write code here self.list1.append(char)]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>剑指offer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pygen]]></title>
    <url>%2F2019%2F04%2F24%2Fpygen%2F</url>
    <content type="text"><![CDATA[pygen功能：有关联的随机生成人名，邮箱，ID Card (ssn)，电话，地址等信息，并且可以选择保存为 pandas dataframe格式, 数据库”.db” 文件, Excel 文件和csv 文件格式，用于机器学习训练。项目地址github。 随机生成虚假个人信息具有很大的应用空间。首先，虚假的生成数据可以用于机器学习模型的“准备数据”，当真实的数据比较少或者难以获得的时候，可以使用生成数据进行训练模型，待模型调通之后，然后使用真实的模型。并且，当真实的数据集中缺少某些特征时候，可以使用这种方法进行特征的填充。比如大的数据集中缺少现居城市地址的时候，可以调用该库中的 “city_real” 进行填充。 当前使用最为广泛的是 Faker 开源库用于数据的生成。虽然该库支持中文，但是对于中文的支持力度有限，所以有时候并不能满足我的需求，比如说生成的身份证 (ssn) 和姓名所能体现的性别是不匹配(了解更多可以参考这里)、生成的姓名中缺少复姓和电话邮箱等信息不符合我们的使用习惯等等。所以我将从以下几点改进： 增强数据之间相关性 生成名字的多样性 符合国人使用习惯的邮箱电话 提供保存多种保存文件格式，更加适合机器学习的训练 中文名字有很强的性别属性。例如名字中带有“杰”“志”“宏”等字的一般为男性，带有“琬”“佩”“梅”等字的一般为女性。当然也有一些比较中性的字，例如“文”“安”“清”等，比较难猜测性别，关于这点会在另一个博客中展开，请期待。 faker 对中文的支持有限，比如下面这种情况。 1234from faker import Fakerfake = Faker('zh_CN')for _ in range(10): print(fake.name(),fake.ssn(),fake.phone_number()) 从图中可以明显的看出 “王玉梅”和 “李桂花”都是两个女性，但是这种身份证信息（ssn）都没有体现这点。关于身份证的科普信息可以从这里获得。简单来说倒数第二位表示性别信息，如果是男性就是奇数如果是女性就是偶数。faker 生成的数据是不具有数据之间的相关性的。 基于此，我们进行了改进。首先是姓名的生成，然后是性别的判断，最后再生成相应性别的身份证号码。 123from pygen import pygendb =pygen()db.gen_dataframe(fields =['name', 'ssn', 'phone', 'email']) 效果如下： 红色线条表示姓名和性别对应一致，蓝色线条表示结果不确定（“镜阳炎” 像是一个中性的名字），绿色表示生成了含有复姓的名字，增强了数据的多样性。 从上图的 “mail” 一列可以看出邮箱前缀的命名基本上是中文名字中“姓” 和“民”的拼音组合，加强了数据之间的相关性和真实度。 另外，电话号码按照运营商分为三类：0 表示移动，1表示联通，2表示电信。 print(&apos;移动字段:&apos;) for _ in range(5): print(db.simple_ph_num(types =0)) print(&apos;联通字段:&apos;) for _ in range(5): print(db.simple_ph_num(types =1)) print(&apos;电信字段：&apos;) for _ in range(5): print(db.simple_ph_num(types =2)) 输出： 移动字段: 15023689929 16771753917 16790223946 15950129353 15271129554联通字段: 13869739303 13786227031 13950354445 15137578545 15240836142电信字段： 17172983067 15658567011 18562313243 17073127396 15543448286 最后提供了多种文件保存格式，包括”.csv”, “.db” 和”.xlsx”等格式。可以使用如下： from pygen import pygen db =pygen() db.gen_table(filename =filename, fields =[&apos;name&apos;, &apos;ssn&apos;, &apos;phone&apos;, &apos;email&apos;]) db.gen_excel(filename =filename, fields =[&apos;name&apos;, &apos;ssn&apos;, &apos;phone&apos;, &apos;email&apos;]) db.gen_csv(filename =filename, fields =[&apos;name&apos;, &apos;ssn&apos;, &apos;phone&apos;, &apos;email&apos;])]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Data Enhancement</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mode Collapse in GANs]]></title>
    <url>%2F2019%2F04%2F18%2Fmode-collapse-in-gan%2F</url>
    <content type="text"><![CDATA[Mode collapse, a failure case for GANs where the generator generate a limited diversity of samples, regardless of the input. But what causes the mode collapse? There are four reasons for that. The objective of GANs The generator, generates new data, while the discriminator evaluates it for authenticity but not for the diversity of generated instances. the generator can win by producing a polynomial number of training examples. And a low capacity discriminator cannot detect this process, thus, it cannot guide the generator to approximate the target distribution. Even if a high discriminator identifies and assigns the collapse part a low probability, then the generator will simply move from its collapsed output to focus on another fixed output point. Generator No matter the objective function is, if it only considers individual samples (without looking forward or backward) then the generator is not directly incentivised to produce diverse examples. From [1], standard GAN training corresponds exactly to updating the generator parameters using only the first term in this gradient because of a fixed discriminator during GAN training. Therefore, in standard GAN training, each generator update step is a partial collapse towards a delta function. $$\frac { \mathrm { d } f _ { K } \left( \theta _ { G } , \theta _ { D } \right) } { \mathrm { d } \theta _ { G } } = \frac { \partial f \left( \theta _ { G } , \theta _ { D } ^ { K } \left( \theta _ { G } , \theta _ { D } \right) \right) } { \partial \theta _ { G } } + \frac { \partial f \left( \theta _ { G } , \theta _ { D } ^ { K } \left( \theta _ { G } , \theta _ { D } \right) \right) } { \partial \theta _ { D } ^ { K } \left( \theta _ { G } , \theta _ { D } \right) } \frac { \mathrm { d } \theta _ { D } ^ { K } \left( \theta _ { G } , \theta _ { D } \right) } { \mathrm { d } \theta _ { G } }$$ Some methods have been proposed. Multiple generators and weight-sharing generators are developed to capture more modes of the distribution. Discriminator The mode collapse is often explained as gradient exploding of discriminator, which comes from the imbalance between the discriminator and the generator. For example, the technique of TTUR could help discriminator to keep its optimality. But some researchers believe that this is a desirable goal since a good discriminator can give good feedback and ignore the fact. In addition, the discriminator process each example independently, the generator depends on discriminator, thus no mechanism to tell the outputs of the generator to become more similar to each other. The idea from [2], that we could use mini-batch discrimination to help generator give better feedback A straightforward approach to handle multimodality is to take random noise vectors along with the conditional contexts as inputs, where the contexts determine the main content and noise vectors are responsible for variations.The noise vectors are ignored or of minor impacts, since cGANs pay more attention to learn from the high-dimensional and structured conditional contexts. Another question Mode collapse may happen only partially?since training is stochastic progress, the input of generator network will vary and the sample drawn from the real distribution will also vary But sometimes mode collapse is not all bad news. In style transfer using GAN, we are happy to convert one image to just a good one, rather than finding all variants. Indeed, the specialization in the partial mode collapse sometimes creates higher quality images. referrences:[1]. Section 2.4 of Unrolled Generative Adversarial Networks[2]. Section 3.2 of Improved Techniques for Training GANs[3]. Mode Seeking Generative Adversarial Networks for Diverse Image Synthesis[4]. Improving Generalization and Stability of Generative Adversarial Networks]]></content>
      <categories>
        <category>Deep learning</category>
      </categories>
      <tags>
        <tag>GANs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[A Not-So-Gentle Introduction to Hyper-parameters Tuning]]></title>
    <url>%2F2019%2F04%2F17%2Fa-not-so-gentle-introduction-to-hyperparameters-tuning%2F</url>
    <content type="text"><![CDATA[Setting the hyper-parameters seems like a black art that requires years of experience to acquire. Currently, there are no simple and easy ways to set hyper-parameters, especifically, batch size, learning rate, momentum, and weight decay. A grid search or random search maybe sounds like a good idea. In this blog, I’d like to share you my idea from reading papers and my projects. Hyper-parametersBatch SizeLearning rate is maybe the most important hyper-parameters, but we choose batch size firstly because large batch size needs a large learning rate in most circumstances. A general principle is: use as a large batch size as possible to fit your CPU memory or/both GPU memory. There are several reasons: larger batch sizes permit the use of larger learning rates A constant number of iterations favors larger batch sizes However, small batch sizes add regularization while large batch sizes add less. So utilize it while balancing the proper amount of regularization. Learning RateWe will introduce the idea from [Cyclical Learning Rates for Training Neural Networks][1]: Cyclical Learning Rates. Instead of monotonically decreasing the learning rate, this method lets the learning rate cyclically vary between reasonable boundary values. The essence of this learning rate policy comes from the observation that increasing the learning rate might have a short term negative effect and yet achieve a longer term beneficial effect. This observation leads to the idea of letting the learning rate vary within a range of values rather than adopting a stepwise fixed or exponentially decreasing value. That is, one sets minimum and maximum boundaries and the learning rate cyclically varies between these bounds.%From Cyclical Learning Rates for Training Neural Networks An intuitive understanding of why CLR methods work comes from considering the loss function topology. Dauphin et al. argue that the difficulty in minimizing the loss arises from saddle points rather than poor local minima. Saddle points have small gradients that slow the learning process. However, increasing the learning rate allows for more rapid traversal of saddle point plateaus. But the question is that how can we find the Minimum bound and Maximum bound. There is a simple way to estimate the reasonable minimum and maximum boundary values with one training run of the network for a few epochs. It is a “LR range test”; run your model for several epochs while letting the learning rate increase linearly between low and high LR values. For example, set both the step size and maxiter to the same number of iterations. In this case, the learning rate will increase linearly from the minimum value to the maximum value during this short run. Next, plot the accuracy versus learning rate. Note the learning rate value when the accuracy starts to increase and when the accuracy slows, becomes ragged, or starts to fall. These two learning rates are good choices for bounds; that is, set $base-{lr}$ to the first value and set $max-{lr}$ to the latter value. MomentumSince learning rate is regarded as the most important hyper-parameter to tune then momentum is also important. Like learning rates, it is valuable to set momentum as large as possible without causing instabilities during training. The large learning rate can deal with local minimum but works fail when it comes to saddle point where momentum comes to rescue. The local minimum is like the following picture.In mathematics, a saddle point or minimax point is a point on the surface of the graph of a function where the slopes (derivatives) in orthogonal directions are all zero (a critical point), but which is not a local extremum of the function. Your first step from the very top would likely take you down, but then you’d be on a flat rice terrace. The gradient would be zero, and you’d have nowhere to go. To remedy this, we employ momentum - the algorithm remembers its last step and adds some psroportion of it to the current step. This way, even if the algorithm is stuck in a flat region, or a small local minimum, it can get out and continue towards the true minimum. In summary: when performing gradient descent, learning rate measures how much the current situation affects the next step, while momentum measures how much past steps affect the next step. Weights DecayWhen training neural networks, it is common to use “weight decay,” where after each update, the weights are multiplied by a factor slightly less than 1. This prevents the weights from growing too large and can be seen as gradient descent on a quadratic regularization term. But why? Large weights might correlate with certain patterns in the input data (x), this means that the model almost hard codes certain values. This then makes our training data fit well but our test data fit less well. The idea of weight decay is simple: to prevent overfitting, every time we update a weight $w$ with the gradient $∇J$ in respect to $w$, we also subtract from it $λ∙w$. This gives the weights a tendency to decay towards zero, hence the name. L2 is a type of weights decay.$$J ( W ; X , y ) + \frac { 1 } { 2 } \lambda \cdot | W | ^ { 2 }$$ But weights decay is not necessarily true for all gradient-base algorithms and was recently shown to not be the case for adaptive gradient algorithms, such as Adam. In addition, weight decay is not the only regularization technique. In the past few years, some other approaches have been introduced such as Dropout, Bagging, Early Stop, and Parameter Sharing which work very well in NNs. Takeoff Batch Size Use as a large batch size as possible to fit your memory Learning Rate Perform a learning rate range test to identify a “large” learning rate. Momentum Test with short runs of momentum values 0.99, 0.97, 0.95, and 0.9 to get the best value for momentum. If using the 1-cycle learning rate schedule, it is better to use a cyclical momentum (CM) that starts at this maximum momentum value and decreases with increasing learning rate to a value of 0.8 or 0.85. Weights Decay A grid search to determine the proper magnitude but usually does not require more than one significant figure accuracy.A more complex dataset requires less regularization so test smaller weight decay values, such as $10^{−4} $, $10^{−5} $, $10^{−6} $, 0.A shallow architecture requires more regularization so test larger weight decay values, such as $10^{−2} $, $10^{−3} $, $10^{−4} $. References[1]. Cyclical Learning Rates for Training Neural Networks[2]. A disciplined approach to neural network hyper-parameters]]></content>
      <categories>
        <category>Deep learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Weights Initialization]]></title>
    <url>%2F2019%2F04%2F17%2Fweights-initialization%2F</url>
    <content type="text"><![CDATA[weights initialization 知识点 和分享两个图像领域的架构（resnet 和 inception v3）。 Training a neural network consists of four steps: initialize weights and biases, forward propagation, compute the loss function and backward propagation. This blog mainly focuses on the first part: weights initialization. After completing this tutorial, you will know: Four main types of weights initialization How to choose between Xavier /zivier/ initialization and He initialization Types of Weights Initialization Initializing weights with zero When you set all weights in a neural network to zero, the derivative with respect to loss function is the same for every $ w$ in the same layer, thus all the weights have the same values in the subsequent iteration, which makes your model equivalent to a linear model. Initializing weights randomly You can get weights like this (Python): w =np.random.randn(layer_size[l],layer_size[l-1]) The weighs follows standard normal distribution while it can potentially lead to two issues: vanishing gradients and exploding gradients.下面的情况是很容易发生，因为网络中特征足够多（网络结构足够宽），所以 random 得到数值有足够的 coverage，所以就会出现 weights too small or too large 这种情况。 If the weights start too small, then the signal shrinks as it passes through each layer until it’s too small to be useful. If the weights start too large, then the signal grows as it passes through each layer until it’s too massive to be useful (big value in sigmoid function). Thus there are two necessary conditions to consider: The values of each activation layer won’t be zero The values of each activation layer won’t go into the area of saturation Xavier/Glorot Initialization For deep networks, we can use a heuristic to initialize the weights depending on the non-linear activation function. This applies to Xavier and He initialization. Xavier/Glorot Initialization initializes the weights in your network by drawing them from a distribution with zero mean and a specific variance.$$ { var } ( w _ { i } ) = \frac { 1 } { layer_{l-1}}$$ w=np.random.randn(layer_size[l],layer_size[l-1])*np.sqrt(1/layer_size[l-1]) In practice, it works better for layers with sigmoid or tanh function. He Initialization Using RELU or Leaky RELU is relatively robust to the vanishing/ exploding gradient issues compared with sigmoid function especially for networks that are not too deep. And it the case of Leaky RELU, it never has zero gradients. For RELU, we multiply the randomly generated values of $w$ by: $$\sqrt { \frac { 2 } { layer _ { [ l - 1 ] } } }$$ w=np.random.randn(layer_size[l],layer_size[l-1])*np.sqrt(2/layer_size[l-1]) Sometimes, we combine the idea of Xavier initialization and He initializaiton so the variance becomes the following: $$\sqrt { \frac { 2 } { layer _ { [ l - 1 ] } + \operatorname { layer } _ { [ l ] } } }$$ w=np.random.randn(layer_size[l],layer_size[l-1])*np.sqrt(2/(layer_size[l-1]+layer_size[l])) The idea behind this is that we set the weights neither too much bigger than 1 nor too much less than 1 so the gradients do not vanish or explode too quickly. TakeoffIn summary, the main difference in machine learning is the following: He initialization works better for layers with ReLu(s) activation. Xavier initialization works better for layers with sigmoid activation. Referrence:He initialization Xavier initialization ResNetResNet 诞生于一个美丽而简单的观察：为什么非常深度的网络在增加更多层时会表现得更差？ResNet 的作者将这些问题归结成了一个单一的假设：直接映射是难以学习的。而且他们提出了一种修正方法：不再学习从 x 到 H(x) 的基本映射关系，而是学习这两者之间的差异，也就是「残差（residual）」。然后，为了计算 H(x)，我们只需要将这个残差加到输入上即可。假设残差为$F(x)=H(x)-x $，那么现在我们的网络不会直接学习$ H(x)$ 了，而是学习 $F(x)+x$。 ResNet 的每一个「模块（block）」都由一系列层和一个「捷径（shortcut）」连接组成，这个「捷径」将该模块的输入和输出连接到了一起。然后在元素层面上执行「加法（add）」运算，如果输入和输出的大小不同，那就可以使用零填充或投射（通过 1×1 卷积）来得到匹配的大小。 Inception V3如果 ResNet 是为了更深，那么 Inception 家族就是为了更宽。第一个见解与对层的操作有关。在传统的卷积网络中，每一层都会从之前的层提取信息，以便将输入数据转换成更有用的表征。 见解 1：为什么不让模型选择？这种模型架构的信息密度更大了，这就带来了一个突出的问题：计算成本大大增加。不仅大型（比如 5×5）卷积过滤器的固有计算成本高，并排堆叠多个不同的过滤器更会极大增加每一层的特征映射的数量。而这种计算成本增长就成为了我们模型的致命瓶颈。 这就涉及到了见解 2：使用 1×1 卷积来执行降维。为了解决上述计算瓶颈，Inception 的作者使用了 1×1 卷积来「过滤」输出的深度。一个 1×1 卷积一次仅查看一个值，但在多个通道上，它可以提取空间信息并将其压缩到更低的维度。比如，使用 20 个 1×1 过滤器，一个大小为 64×64×100（具有 100 个特征映射）的输入可以被压缩到 64×64×20。通过减少输入映射的数量，Inception 可以将不同的层变换并行地堆叠到一起，从而得到既深又宽（很多并行操作）的网络。 Inception Net v3 incorporated all of the above upgrades stated for Inception v2, and in addition used the following: RMSProp Optimizer. Factorized 7x7 convolutions. BatchNorm in the Auxillary Classifiers. Label Smoothing (A type of regularizing component added to the loss formula that prevents the network from becoming too confident about a class. Prevents over fitting). GANs 网络结构]]></content>
      <categories>
        <category>Deep learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[CycleGAN & StyleGAN]]></title>
    <url>%2F2019%2F04%2F09%2Fcyclegan-stylegan%2F</url>
    <content type="text"><![CDATA[In the past few yeas, GANs have been used in lots of different applications such as generating synthetic data, style transfer, super-resolution and text2image generation. But we donn’t aim to give a overview of what GANs are made for. There are many great and detailed blogs for your understanding. What this post is about Main ideas of CycleGAN Keypoints in StyleGAN A Gentle Introduction of GANsWe assume the reader has some prior experience with neural networks. In addition, if you are familiar with GANs you can skip this section. The famous minimax objective function can be formulated as following:$$\min _ { \theta _ { g } } \max _ { \theta _ { d } } \left[ \mathbb { E } _ { x \sim p _ { d a t a } } \log D _ { \theta _ { d } } ( x ) + \mathbb { E } _ { z \sim p ( z ) } \log \left( 1 - D _ { \theta _ { d } } \left( G _ { \theta _ { g } } ( z ) \right) \right) \right)$$But in practical, the loss function cannot work very well. So we have alternative objective function: Gradient ascent on discriminator $$\max _ { \theta _ { d } } \left[ \mathbb { E } _ { x \sim p _ { d a t a } } \log D _ { \theta _ { d } } ( x ) + \mathbb { E } _ { z \sim p ( z ) } \log \left( 1 - D _ { \theta _ { d } } \left( G _ { \theta _ { g } } ( z ) \right) \right) \right]$$ Gradient ascent on generator$$\max _ { \theta _ { g } } \mathbb { E } _ { z \sim p ( z ) } \log \left( D _ { \theta _ { d } } \left( G _ { \theta _ { g } } ( z ) \right) \right)$$ The reasoning behind this can be found in original paper. Simplily speaking, we can get higher gradient signal for bad samples, which works much better in practice.bad case 的时候，使用原来的min-max function会使得学习率不够，使用 gradient ascent 就会好一些。 From Stanford CS231 Lecture 13 — Generative Models Main ideas of CycleGANCycleGAN was introduced in 2017 out of Berkeley, Unpaired Image-to-Image Translation Using Cycle-Coonsistent Adversarial Networks. This task is performed on unpaired data. Recent methods such as Pix2Pix depend on the availability of training examples where the samee data is availabel in both domains. However, CycleGAN is able to learning such pair information without one-to-one mapping between training data in source and target domains. Network ArchitectureWe build three networks. A generator $F$ to convert image $y$ to image $ \hat{x}$ A generator $G$ to convert image $\hat{x}$ to image $ \hat{y}$ A discriminator $D$ to identify real image or generated picture Simplified version of CycleGAN architecture can be showed in the following.The function $F$ and $G$ are generator network, which consists of encoder, transformer and decoder. Encoder is extracting the features from an image which is done by convolution networks. Each convolution layer leads to extraction of progressively higher level features. We would like to transform the feature emebdding of an image from domain $X$ to that of domain $Y$. So for this, authors have used 6 layers of ResNet blocks. ResNet block is a neural network layer which consists of two convolutiona layers when a residue of input ia added to the output. This is done to ensure properties of input of previous layers are available for later layers as well. ResNet block can be summarized in following imageThe decoder transfer embedding from $y$ back to original embedding $x$. Loss function（对于loss 实际上只有两个，只不过在第一个loss 中 x 和y 分别使用了两次，所以变成了两个公式。）There are two types of losses in CycleGAN. Besides adversarial loss, we have another loss named reconstruction cost.Adversarial loss is similaity to original GAN.$$\operatorname { Loss } _ { a d v } \left( F , D _ { x } , Y \right) = \frac { 1 } { m } \sum _ { i = 1 } ^ { m } \left( 1 - D _ { x } \left( F \left( y _ { i } \right) \right) \right) ^ { 2 }$$$$\operatorname { Loss } _ { a d v } \left( G , D _ { y } , X \right) = \frac { 1 } { m } \sum _ { i = 1 } ^ { m } \left( 1 - D _ { y } \left( G \left( x _ { i } \right) \right) \right) ^ { 2 }$$However, the adversarial loss alone is not sufficient to produce good looking images, which can not enfore that the input and output are recognizably the same. The cycle consistency loss addresses this issue. It relies on the expectation that if you convert an image to the other domain and back again, and then you should get back something similar to what you put in. It enforces that $F ( G ( x ) ) \approx x$ and $G ( F ( y ) ) \approx y$.$$\operatorname { Loss } _ { c y c } ( G , F , X , Y ) = \frac { 1 } { m } \sum _ { i = 1 } ^ { m } \left[ F \left( G \left( x _ { i } \right) \right) - x _ { i } \right] + \left[ G \left( F \left( y _ { i } \right) \right) - y _ { i } \right]$$We can get the full objective function by putting these two together.$$\mathcal { L } \left( G , F , D _ { x} , D _ { y } \right) = \mathcal { L } _ { \text { GAN } } \left( G , D _ { y } , X , Y \right) + \mathcal { L } _ { \text { GAN } } \left( F , D _ { x } , Y , X \right) + \lambda \mathcal { L } _ { \text { cyc } } ( G , F )$$ Keypoints of StyleGAN(想要解决的问题，ProGAN 是训练过程中不是一个 fine-tune 的过程，而是从一种状态到另一种状态的过程，所以styleGAN 想要 control specific features during training)The StyleGAN offers an upgrade version of ProGAN’s image generator, with a focus on the generator. ProGAN generates high-quality images but, in most models, its ability to control specific features of the generated image is very limited. In other word, the features are entangled and therefore attempting to tweak the input, even a bit, usually affects multiple features at the same time. A good illustrations would be following pictures.Compared with first version (ProGAN), the new generator includes several additions to ProGAN’s generators. Mapping NetworkThe mapping network’s goal in to encode the input vector into an intermediate vector whose different elements control different visual features, which consists of 8 fully connected layers and its output $w$ is of the same size as the input. Style Modules (AdaIN)The AdaIn (Adaptive Instance Normalization) module transfers the encoded information $w$, created by the mapping network, into the generated image. Removing traditional inputSince the encoded information $w$ from mapping network was used into generator image, the traditional random input can be omitted and replaced by constant values.]]></content>
      <categories>
        <category>Deep learning</category>
      </categories>
      <tags>
        <tag>GANs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python from Beginner to Master]]></title>
    <url>%2F2019%2F04%2F09%2Fpython-for-beginners%2F</url>
    <content type="text"><![CDATA[Basic Skillsmodulepython 文件可以当做主文件进行运行或者当做函数的集合进行调用。如果是前者一般是需要包含”__name__ ==”__main__”。对于后者就是在其他的python文件中进行调用。12import my_module # python文件from my_module import my_object packagesfrom packageroot.packagefolder.mod import my_object Note: Ensure each directory within your package import contains a file __init__.py python-pathpython2 和python3 使用不同的解释器，导致在一些函数命名和计算上有一些差别，最好在文件的开头标明使用的解释器。 while or forwhile : provide a condition and run the loop until the condition is not met. for: loop for a number of specific times; loop over items or characters of a string. examples:1234[Variable] AggregateFunction([Value] for [item] in [collection])x =[1, 2,3, 4, 5]y =[ 2*a for a in x if a%2 ==0]y &gt;&gt; [4, 8] 或者可以使用这样更加简洁的语句：12345678910111213 lambda arguments : expression fun1 = lambda a,b,c : a+b+c print(fun1(5,6,2))``` 来个比较复杂的例子:```python nums =[1,2,3,4,5] letters =['a', 'b', 'c','d','e'] # 两个for 循环也是要熟练 nums_letters =[[n, l] for n in nums for l in letters ] nums_letters break, continue, or passThe break, continue, and pass statements in Python will allow you to use for loops and while loops more effectively in your code.12345678910number = 0for number in range(10): number = number + 1 if number == 5: pass # pass here print('Number is ' + str(number))print('Out of loop') The pass statement occurring after the if conditional statement is telling the program to continue to run the loop and ignore the fact that the variable number evaluates as equivalent to 5 during one of its iterations. yield 可以用用作新的 if的测试, return results without termination The pass statement can create minimal classes, or act as a placeholder when working on new code and thinking on an algorithmic level before hammering out details.pass 的存在就是占坑，否则这个地方就是报错（IndentationError）。用于想要扩展的地方，但是现在还没有扩展。比如在某个method 下面或者某个 if 条件下。 yield or return经常被用来作为生成器。 when you call a normal function with a return statement the function is terminated whenever it encounters a return statement. In a function with a yield statement the state of the function is ‘saved’ from the last call and can be picked up the next time you call a generator function. for examples12345678910111213141516171819gen_exp =(x **2 for x in range(10) if x %2 ==0)for x in gen_exp: print(x)def my_gen(): for x in range(5): yield xgen1 =my_gen()next(gen1)def my_generator1(): yield 1 yield 2 yield 3 my_gen =my_generator1()# 使用 next() 进行调用下一个next(my_gen) recursionA function calling itself is known as recursion. list, tuples, or dictionary在python 中是使用频繁的data structure，这个是属于 collection 类别，里面放的是element. list: to add/update/ delete an item of a collection 123456my_list.append('C') #adds at the endmy_list[1] = 'D' #updatemy_list.pop(1) # removesmylist.pop() # 默认就是类似 栈的结构，就是pop 出来最后一个mylist.pop(0) # 当然也可以根据index 指定特定的 pop(delete) 的element or 12del mylist[1:2] # 通过指定 index range 然后进行delmylist.sort() # 支持 sorting 然后是从小到大, 这个sort是一种操作，inplace 的操作 tuples: tuples store a sequence of objects, the object can be of any type. Tuples are faster than lists. dictionary: It stores key/value pair objects. 12345678910111213141516171819202122232425262728293031 my_dict =dict() my_dict['key'] ='value' or my_dict =&#123;'key': 'value', ...&#125; for key in my_dict: # do something if 'some key' in my_dict: # do something``` ### Iterators###```pythonclass yrange: def __init__(self, n): self.i = 0 self.n = n # 这个表明是一个 iterator，make an object iterable def __iter__(self): return self # 这个next 函数就被当做是 class的属性，可以被外部调用的， def next(self): if self.i &lt; self.n: i = self.i self.i += 1 return i else: raise StopIteration() shallow vs deep copypython3 中：对于简单的数据类型，像int ，string，这种 copy() 和copy.deepcopy() 这两者都是相同的，copy 都是一种映射，都是相当于”值“ 上的引用；12345aa =2bb =aaprint(id(aa), id(bb)) # 相同bb =3print(id(aa), id(bb)) # 不同，因为把3 这个值重新复制给了变量bb 对于复杂的数据类型，使用deepcopy() 的时候，本来就是会重新拷贝一份到内存中。在python3 中copy() 和deepcopy() 这个是没有什么区别的。12345list1 =['a', 'b']list2 =list1 # 这个是引用，所以和list1 是相同的list3 =copy.copy(list1) # 这个id 和list1 不同list4 =copy.deepcopy(list1)# 这个id 和list1 不同 print(id(list1), id(list2), id(list3), id(list4)) object oriented design1234567891011121314151617181920212223242526272829303132333435363738394041424344454647 class ParentClass: def my_function(self): print 'I am here' class SubClass1(ParentClass): class SubClass2(ParentClass): ``` 对于多继承的支持 （接口）```python class A(B,C): #A implments B and C``` 如果想要call parent class function then you can dp:```python super(A, self).funcion_name()``` ### garbage collection###all the objects in python are stored in a heap space. Python has an in-built garbage collection mechanism. Memory management in Python involves a private heap containing all Python objects and data structures. The management of this private heap is ensured internally by the Python memory manager.In computer science, a heap is a specialized tree-based data structure which is essentially an almost complete[1] tree that satisfies the heap property: if P is a parent node of C, then the key(the value) of P is either greater than or equal to (in a max heap) or less than or equal to (in a min heap) the key of C.### try...catch###```python # raise exceptions try: raise TyeError except: print('exception') # catching exceptions try: do_something() except: print('exception') # try/ catch /finally try: do_something() except TypeError: print('exception') finally: close_connections() Advanced FeaturesLet’s move on to advanced features. Lambda functionsA Lambda Function is a small, anonymous function — anonymous in the sense that it doesn’t actually have a name. A lambda function can take any number of arguments, but must always have only one expression: 1234x = lambda a, b : a * b print(x(5, 6)) # prints '30' # 匿名函数也是函数，调用的时候使用这样的方式 x = lambda a : a*3 + 3 print(x(3)) # prints '12' MapsMap() is a built-in Python function used to apply a function to a sequence of elements like a list or dictionary. It’s a very clean and most importantly readable way to perform such an operation.相对于 lambda, map 使用的频率更少了。 最后返回的是一个list。 1234567891011def square_it_func(a): return a * ax = map(square_it_func, [1, 4, 7])print(x) # prints '[1, 16, 49]'def multiplier_func(a, b): return a * bx = map(multiplier_func, [1, 4, 7], [2, 5, 8])print(x) # prints '[2, 20, 56]' FilteringThe Filter built-in function is quite similar to the Map function in that it applies a function to a sequence (list, tuple, dictionary). The key difference is that filter() will only return the elements which the applied function returned as True. 123456789101112 # Our numbersnumbers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]# Function that filters out all numbers which are odddef filter_odd_numbers(num): if num % 2 == 0: return True else: return Falsefiltered_numbers = filter(filter_odd_numbers, numbers)print(filtered_numbers)# filtered_numbers = [2, 4, 6, 8, 10, 12, 14] 123456from itertools import *def check_for_drop(x): print ('Checking: ', x) return (x &gt; 5)for i in dropwhile(check_for_drop, [2, 4, 6, 8, 10, 12]): print ('Result: ', i) Itertools123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263from itertools import *# zip 就是一块访问的那种形式，返回的是一个tuple 数据类型# zip ,joing two lists into a list of tuples# Easy joining of two lists into a list of tuplesfor i in zip([1, 2, 3], ['a', 'b', 'c']): print (i)# ('a', 1)# ('b', 2)# ('c', 3)# 就是一个count() 计数功能# The count() function returns an interator that # produces consecutive integers, forever. This # one is great for adding indices next to your list # elements for readability and convenience# in python3, no need to import izip, use zip directly# 这个 count() 只有在这里才有意义，如果只是单独调用，没有感觉有多大的意义for i in zip(count(1), ['Bob', 'Emily', 'Joe']): print (i)# (1, 'Bob')# (2, 'Emily')# (3, 'Joe') # check ， becomes false for the first time 这个条件很关键, 可以理解成只是找到第一个false 的条件，然后就不再执行该函数# The dropwhile() function returns an iterator that returns # all the elements of the input which come after a certain # condition becomes false for the first time. def check_for_drop(x): print 'Checking: ', x return (x &gt; 5)for i in dropwhile(check_for_drop, [2, 4, 6, 8, 10, 12]): print 'Result: ', i# 注意理解这个输出# Checking: 2# Result: 2# Result: 4# Result: 6# Result: 8# Result: 10# Result: 12# 我的理解这个 groupby 就和数据库中的groupby 是相同的效果# The groupby() function is great for retrieving bunches# of iterator elements which are the same or have similar # propertiesfrom itertools import groupbythings = [("animal", "bear"), ("animal", "duck"), ("plant", "cactus"), ("vehicle", "speed boat"), ("vehicle", "school bus")]for key, group in groupby(things, lambda x: x[0]): for thing in group: print ("A %s is a %s." % (thing[1], key))#A bear is a animal.#A duck is a animal.#A cactus is a plant.#A speed boat is a vehicle.#A school bus is a vehicle. GeneratorGenerator functions allow you to declare a function that behaves like an iterator, i.e. it can be used in a for loop. This greatly simplifies your code and is much more memory efficient than a simple for loop. 12345678910111213141516numbers = list()# range()for i in range(1000): numbers.append(i + 1)total = sum(numbers)# (2) Using a generatordef generate_numbers(n): num = 0 while num &lt; n: yield num # 这个yield 之后，函数并没有结束，不像 return 那种函数 num += 1total = sum(generate_numbers(1000))print(total)total = sum(range(1000 + 1))print(total) Decorator简单地说，decorator就像一个wrapper一样，在函数执行之前或者之后修改该函数的行为，而无需修改函数本身的代码，这也是修饰器名称的来由。总的来说，decorator在你希望在不修改函数本身代码的前提下扩展函数的功能时非常有用。 python 中函数的”人设“, function 也是一种对象，内部函数可以访问外部的 function的变量，但是权限是”只读“。 123456789def greet(name): def get_message(): return "Hello "+name result = get_message()+name return resultprint(greet("John")) 于是乎， 1234567891011def get_text(name): return "lorem ipsum, &#123;0&#125; dolor sit amet".format(name)def p_decorate(func): def func_wrapper(name): return "&lt;p&gt;&#123;0&#125;&lt;/p&gt;".format(func(name)) return func_wrappermy_get_text = p_decorate(get_text)print(my_get_text("John")) 输出：1&lt;p&gt;Outputs lorem ipsum, John dolor sit amet \&lt;/p&gt; 这就是我们的第一个修饰器。一个函数接收另一个函数作为参数，并且产生一个新的函数，注意观察是如何调用和 调用的顺序，体会 decorator 产生的背景，是可以简化这个调用过程的。 下面代码和上面是相同的功能，p_decorate 就像是 function get_text() 的一个外套， 其作为一种输入到 p_decorate() 中。 12345678910def p_decorate(func): def func_wrapper(name): return "&lt;p&gt;&#123;0&#125;&lt;/p&gt;".format(func(name)) return func_wrapper@p_decoratedef get_text(name): return "lorem ipsum, &#123;0&#125; dolor sit amet".format(name)print (get_text("John")) 输出结果： 1&lt;p&gt;lorem ipsum, John dolor sit amet&lt;/p&gt; 在给个例子，理解调用过程。 123456789def hello_decorator(original_fn): def decorator_fn(): print("Hello from new") original_fn() # original function must be invoked return decorator_fn@hello_decoratordef hello(): print("Hello from original") 输出结果：12Hello from newHello from original 另外，一个函数是可以添加多个 修饰器的，并且修饰器的顺序也是有关系的，如果顺序不同，那么最后的结果也是不同的。 感觉装饰器很难的原因在于没有理清它的逻辑关系，本质上装饰器也是函数，但它是对核心程序的闭包封装，在原有的基础上增加更多的功能。细细回顾几遍上面的例子能够加深对装饰器的理解。 使用修饰器实现单例模式 单例是一种设计模式，应用该模式的类只会生成一个实例。这种方式是可以代替全局变量的。比如一些配置、日志等只需要初始化一次的文件，就可以使用这种方式。 123456789101112131415161718192021# 函数修饰器实现单例def singleton(cls): _instance =&#123;&#125; def inner(): if cls not in _instance: _instance[cls] =cls return _instance[cls] # 函数调用 return inner@singletonclass Cls(object): def __init__(self): passcls1 =Cls()cls2 =Cls()print(id(cls1) ==id(cls2)) List Comprehension常见的几种形式：(An iterable is something you can loop over) list comprehensions vs loops: list comprehensions are more efficient both computationally and coding space Every list comprehension can be rewritten as a for loop, but not every for loop can be rewritten as a list comprehension. 从优化的角度 list comprehensions是优于 for loop 中的if else 操作的。因为前者是 predicatable pattern 是可以预测的。However, keep in mind that list comprehensions are faster because they are optimized for the Python interpreter to spot a predictable pattern during looping. a small code demo:在于使用功能 timeit libary 进行函数的计时比较。12345678910111213import timeitdef squares(size): result = [] for number in range(size): result.append(number * number) return resultdef squares_comprehension(size): return [number * number for number in range(size)] print(timeit.timeit("squares(50)", "from __main__ import squares", number=1_000_000))print(timeit.timeit("squares_comprehension(50)", "from __main__ import squares_comprehension", number=1_000_000)) more complex list comprehensions: 这种if 的写法 是两个进行并列的。其实可以写成 123456numbers = [1, 2, 3, 4, 5, 6, 18, 20]squares = [number for number in numbers if number % 2 == 0 if number % 3 == 0]# orsquares = [number for number in numbers if number % 2 == 0 and number % 3 == 0]print(squares)# output: [6, 18] 在 output expression 中，也是可以使用 if else 进行进一步输出筛选。12345numbers = [1, 2, 3, 4, 5, 6, 18, 20] squares = ["small" if number &lt; 10 else "big" for number in numbers if number % 2 == 0 if number % 3 == 0] print(squares)ouput: ['small', 'big'] converting nested loops into list comprehension代码功能： 都是把二维的 matrix 转成了一个 list （flattened）123456matrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]flattened = []for row in matrix: for item in row: flattened.append(item)print(flattened) 注意这个顺序，先是row in matrix 然后是 item in row.123 matrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]flattened = [item for row in matrix for item in row] print(flattened) ouput matric from nested list comprehensions:12matrix = [[item for item in range(5)] for row in range(3)]print(matrix) 对于 dictionary 的支持： 主要是 dict1.items() 和 key, value 的使用123prices = &#123;"beer": 2, "fish": 5, "apple": 1&#125;float_prices = &#123;key:float(value) for key, value in prices.items()&#125;print(float_prices) 从代码的角度，可以看出，操作和最后的返回的形式是没有很大的关系，上面是 [], 这个是 {}, 分别对应的是 list 和 set 两种不同的格式。123numbers = [10, 10, 20, 30, 12, -20, 0, 1]unique_squares = &#123;number**2 for number in numbers&#125;print(unique_squares) python operatorsPython Arithmetic Operator Addition(+) Subtraction(-) Multiplication(*) Division(/) Exponentiation(**) Floor Division(//) 向下取整 Modulus(%) Python Relational Operator Less than(&lt;) Greater than(&gt;) Less than or equal to(&lt;=) Greater than or equal to(&gt;=) Equal to(= =) Not equal to(!=) Python Assignment Operator Assign(=) Add and Assign(+=) Subtract and Assign(-=) Divide and Assign(/=) Divide and Assign(/=) Modulus and Assign(%=) Exponent and Assign(**=) Floor-Divide and Assign(//=) Python Logical Operator(会有某种机制简化运算，比如 condition1 or condition2 ，如果condition1 是正确的，那么最后的结果就是正确的。)用于逻辑判断 and or not Python Membership Operator in ,not in Python Identity Operator is, is not , Python Bitwise Operator (这其中的 | 表现的是一种二级制’ 和’的 关系，如果在二进制下，0 | 1 那么就是1 ) 属于集合操作。 Binary AND(&amp;) Binary OR(|) Binary XOR(^) Binary XOR(^) Binary Left-Shift(&lt;&lt;) Binary Right-Shift(&gt;&gt;) Working with files Working with CSV, Json and XML Over the years, the list of possible formats that you can store your data in has grown significantly. But, there are 3 that dominate in their everyday usage: CSV, JSON, and XML. In this article, I’m going to share with you the easiest ways to work with these 3 popular data formats in Python! 有两种方式去读写 csv file：一种是 pd.read_csv() ，一种是built-in 的library 中的库函数之前一直使用的pd.read_csv(), 现在才发现python 有built-in 的library。We can do both read and write of a CSV using the built-in Python csv library. Usually, we’ll read the data into a list of lists. python in-built function.12345678910111213141516171819import csv filename = "my_data.csv"fields = [] rows = [] with open(filename, 'r') as csvfile: csvreader = csv.reader(csvfile) # 如果单单是这个for，那么内存是消耗比较大的 # fields = csvreader.next() for row in csvreader: rows.append(row)for row in rows[:5]: print(row)# Writing to csv file with open(filename, 'w+') as csvfile: csvwriter = csv.writer(csvfile) csvwriter.writerow(fields) csvwriter.writerows(rows) 12345678910111213141516171819202122import pandas as pdfrom dicttoxml import dicttoxmlimport json# Building our dataframedata = &#123;'Name': ['Emily', 'Katie', 'John', 'Mike'], 'Goals': [12, 8, 16, 3], 'Assists': [18, 24, 9, 14], 'Shots': [112, 96, 101, 82] &#125;df = pd.DataFrame(data, columns=data.keys())# Converting the dataframe to a dictionary# Then save it to filedata_dict = df.to_dict(orient="records")with open('output.json', "w+") as f: json.dump(data_dict, f, indent=4)# Converting the dataframe to XML# Then save it to filexml_data = dicttoxml(data_dict).decode()with open("output.xml", "w+") as f: f.write(xml_data) 12345678910111213141516171819import jsonimport pandas as pd# Read the data from file# We now have a Python dictionarywith open('data.json') as f: data_listofdict = json.load(f) # We can do the same thing with pandasdata_df = pd.read_json('data.json', orient='records')# We can write a dictionary to JSON like so# Use 'indent' and 'sort_keys' to make the JSON# file look nicewith open('new_data.json', 'w+') as json_file: json.dump(data_listofdict, json_file, indent=4, sort_keys=True)# And again the same thing with pandasexport = data_df.to_json('new_data.json', orient='records') text filepython 中 open( mode =’rt’) 的选项：w,r,wt,rt都是python里面文件操作的模式。w是写模式，r是读模式。t是windows平台特有的所谓text mode(文本模式）,区别在于会自动识别windows平台的换行符。类Unix平台的换行符是\n，而windows平台用的是\r\n两个ASCII字符来表示换行，python内部采用的是\n来表示换行符。rt模式下，python在读取文本时会自动把\r\n转换成\n.wt模式下，Python写文件时会用\r\n来表示换行。 参考资料：https://towardsdatascience.com/the-easy-way-to-work-with-csv-json-and-xml-in-python-5056f9325ca9 其他python 中 in 操作 在不同的数据集合中的时间复杂度 操作 平均情况 最坏情况 说明 列表 list O(n) O(n) list 是由数组实现的 字典 dict O(1) O(n) 集合 set O(1) O(n) 内部实现和 dict 很像]]></content>
      <categories>
        <category>Python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[The Evaluation of Sentence Similarity]]></title>
    <url>%2F2019%2F04%2F06%2FThe-evaluation-of-sentence-similarity%2F</url>
    <content type="text"><![CDATA[I am trying to write my first english blog based on two reasons: First, the data set used in this blog is english; Second, I’d like to expand my reach and attract more audiences, although I should admit that nobody cares. DataInitially I want to use chinese corpus, but I cannot find a proper one. The data should sound like this one: word1 word2 similarity score阿拉伯人 阿拉伯 7.2畜产 农业 5.6垂涎 崇敬 3.4次序 秩序 4.7定心丸 药品 4.3房租 价格 5.2翡翠 宝石 6.7高科技 技术 7.5购入 购买 8.5观音 菩萨 8.2归并 合并 7.7 not like this: 为何我无法申请开通花呗信用卡收款 支付宝开通信用卡花呗收款不符合条件怎么回事 1花呗分期付款会影响使用吗 花呗分期有什么影响吗 0为什么我花呗没有临时额度 花呗没有临时额度怎么可以负 0能不能开花呗老兄 花呗逾期了还能开通 0我的怎么开通花呗收钱 这个花呗是个什么啥？我没开通 我怎么有账单 0蚂蚁借呗可以停掉么 蚂蚁借呗为什么给我关掉了 0我想把花呗功能关了 我去饭店吃饭，能用花呗支付吗 0为什么我借呗开通了又关闭了 为什么借呗存在风险 0支付宝被冻了花呗要怎么还 支付功能冻结了，花呗还不了怎么办 1 If you can find the dataset where ‘similarity score’ is double, please donot hesitate to email me. So, the choice has to be enlgish corpus. The dataset used in this experiment are STSbenchmark and SICK data. The SICK data contains 10,000 sentence paris labeled with semantic relatedness and entailment relation. Similarity MethodsBaselineAs the baseline, we just take the embedding of the words in sentence, and compute the average, weighted by frequency of each word. 1234567891011121314151617181920212223242526272829303132def run_avg_benchmark(sentences1, sentences2, model=None, use_stoplist=False, doc_freqs=None): if doc_freqs is not None: N = doc_freqs["NUM_DOCS"] sims = [] for (sent1, sent2) in zip(sentences1, sentences2): tokens1 = sent1.tokens_without_stop if use_stoplist else sent1.tokens tokens2 = sent2.tokens_without_stop if use_stoplist else sent2.tokens tokens1 = [token for token in tokens1 if token in model] tokens2 = [token for token in tokens2 if token in model] if len(tokens1) == 0 or len(tokens2) == 0: sims.append(0) continue tokfreqs1 = Counter(tokens1) tokfreqs2 = Counter(tokens2) weights1 = [tokfreqs1[token] * math.log(N / (doc_freqs.get(token, 0) + 1)) for token in tokfreqs1] if doc_freqs else None weights2 = [tokfreqs2[token] * math.log(N / (doc_freqs.get(token, 0) + 1)) for token in tokfreqs2] if doc_freqs else None embedding1 = np.average([model[token] for token in tokfreqs1], axis=0, weights=weights1).reshape(1, -1) embedding2 = np.average([model[token] for token in tokfreqs2], axis=0, weights=weights2).reshape(1, -1) sim = cosine_similarity(embedding1, embedding2)[0][0] sims.append(sim) return sims Smooth Inverse FrequencyThe baseline, like we did before, is very simple and crude of computing sentence embedding. Word frequency cannot reliably reflect its importance to sentence, semantically speaking. Smooth Inverse Frequency (SIF) tries to solve this problem. SIF is very similar to the weighted average we used before, with the difference that it’s weighted by this formular. $$\operatorname { SIF } ( w ) = \frac { a } { ( a + p ( w ) )}$$where $a$ is a hyper-parameter (set to 0.001 by default) and $ p(w)$ is the estimated word frequency in the corpus. we need to perform common component removal: subtract from the sentence embedding obtained above the first principal component of the matrix. This corrects for the influence of high-frequency words that have syntactic or dicourse function, such as ‘but’, ‘and’, etc. You can find more information from this paper. 12345678910111213141516171819202122232425262728293031323334353637def remove_first_principal_component(X): svd = TruncatedSVD(n_components=1, n_iter=7, random_state=0) svd.fit(X) pc = svd.components_ XX = X - X.dot(pc.transpose()) * pc return XXdef run_sif_benchmark(sentences1, sentences2, model, freqs=&#123;&#125;, use_stoplist=False, a=0.001): total_freq = sum(freqs.values()) embeddings = [] # SIF requires us to first collect all sentence embeddings and then perform # common component analysis. for (sent1, sent2) in zip(sentences1, sentences2): tokens1 = sent1.tokens_without_stop if use_stoplist else sent1.tokens tokens2 = sent2.tokens_without_stop if use_stoplist else sent2.tokens tokens1 = [token for token in tokens1 if token in model] tokens2 = [token for token in tokens2 if token in model] weights1 = [a / (a + freqs.get(token, 0) / total_freq) for token in tokens1] weights2 = [a / (a + freqs.get(token, 0) / total_freq) for token in tokens2] embedding1 = np.average([model[token] for token in tokens1], axis=0, weights=weights1) embedding2 = np.average([model[token] for token in tokens2], axis=0, weights=weights2) embeddings.append(embedding1) embeddings.append(embedding2) embeddings = remove_first_principal_component(np.array(embeddings)) sims = [cosine_similarity(embeddings[idx * 2].reshape(1, -1), embeddings[idx * 2 + 1].reshape(1, -1))[0][0] for idx in range(int(len(embeddings) / 2))] return sims Google Sentence EncoderInferSent is a pre-trained encoder that produces sentence embedding, which opensourced by Facebook. The Google Sentence Encoder is Google’s answer to Facebook’s InferSent. In contrast to InferSent, the Google Sentence Encoder was trained on a combination of unsupervised data and supervised data (SNLI corpus), which tends to give better results. The codes can be used in Google Jupyter Notebook 12345678910111213141516171819202122232425import tensorflow_hub as hubtf.logging.set_verbosity(tf.logging.ERROR)embed = hub.Module("https://tfhub.dev/google/universal-sentence-encoder/1")def run_gse_benchmark(sentences1, sentences2): sts_input1 = tf.placeholder(tf.string, shape=(None)) sts_input2 = tf.placeholder(tf.string, shape=(None)) sts_encode1 = tf.nn.l2_normalize(embed(sts_input1)) sts_encode2 = tf.nn.l2_normalize(embed(sts_input2)) sim_scores = tf.reduce_sum(tf.multiply(sts_encode1, sts_encode2), axis=1) with tf.Session() as session: session.run(tf.global_variables_initializer()) session.run(tf.tables_initializer()) [gse_sims] = session.run( [sim_scores], feed_dict=&#123; sts_input1: [sent1.raw for sent1 in sentences1], sts_input2: [sent2.raw for sent2 in sentences2] &#125;) return gse_sims Experiments1234567891011121314def run_experiment(df, benchmarks): sentences1 = [Sentence(s) for s in df['sent_1']] sentences2 = [Sentence(s) for s in df['sent_2']] pearson_cors, spearman_cors = [], [] for label, method in benchmarks: sims = method(sentences1, sentences2) pearson_correlation = scipy.stats.pearsonr(sims, df['sim'])[0] print(label, pearson_correlation) pearson_cors.append(pearson_correlation) spearman_correlation = scipy.stats.spearmanr(sims, df['sim'])[0] spearman_cors.append(spearman_correlation) return pearson_cors, spearman_cors Helper function: 1234567891011import functools as ftbenchmarks = [ ("AVG-GLOVE", ft.partial(run_avg_benchmark, model=glove, use_stoplist=False)), ("AVG-GLOVE-STOP", ft.partial(run_avg_benchmark, model=glove, use_stoplist=True)), ("AVG-GLOVE-TFIDF", ft.partial(run_avg_benchmark, model=glove, use_stoplist=False, doc_freqs=doc_frequencies)), ("AVG-GLOVE-TFIDF-STOP", ft.partial(run_avg_benchmark, model=glove, use_stoplist=True, doc_freqs=doc_frequencies)), ("SIF-W2V", ft.partial(run_sif_benchmark, freqs=frequencies, model=word2vec, use_stoplist=False)), ("SIF-GLOVE", ft.partial(run_sif_benchmark, freqs=frequencies, model=glove, use_stoplist=False)),] Results123import matplotlib.pyplot as pltplt.rcParams['figure.figsize'] = (20,13)spearman[['AVG-GLOVE', 'AVG-GLOVE-STOP','AVG-GLOVE-TFIDF', 'AVG-GLOVE-TFIDF-STOP','GSE']].plot(kind="bar").legend(loc="lower left") Take Off Smooth Inverse Frequency methods are better than baseline, no matter with word2vec or Glove embeddings. Google Sentence Encoder has the similar performance as Smooth Inverse Frequency. Using tf-idf weights does not help and using a stoplist looks like a reasonable choice. Pearson CorrelationSpearman Correlation Full codes can be found in here.]]></content>
      <categories>
        <category>NLP</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[深度网络中的碎碎念]]></title>
    <url>%2F2019%2F03%2F26%2F%E6%B7%B1%E5%BA%A6%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E7%A2%8E%E7%A2%8E%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[介绍深度网络中小的基本概念，比如权重初始化、激活函数和优化器 和常见的技术比如 dropout。 Weights Initializationweights 的初始化在网络的训练起到重要的作用，初始化的好坏能够直接影响到网络是否可以正常收敛。这里的初始化都是指的是weights初始化。bias 表示偏差，噪声，作用在于企图去描述真实的分布（高斯分布），通过引入随机性来表示这个是具有推广性的。主要介绍常见的三种初始化方法和选择方法。 Here’s another trick — before squishing our scalar value (called an activation) into the sigmoid function, we can add a little number called the bias, b. There will be two biases, one for each user and one for each movie. Random Initialization总结来说就是容易出现梯度消失和梯度爆炸，尤其是在layer_size（特征数量） 比较大的时候。从均值方差的角度进行分析。 a) If weights are initialized with very high values the term np.dot(W,X)+becomes significantly higher and if an activation function like sigmoid() is applied, the function maps its value near to 1 where slope of gradient changes slowly and learning takes a lot of time.b) If weights are initialized with low values it gets mapped to 0, where the case is same as above. 1w =np.random.randn(layer_size[l],layer_size[l-1]) 另外的表述方式： If the weights start too small, then the signal shrinks as it passes through each layer until it’s too small to be useful.If the weights start too large, then the signal grows as it passes through each layer until it’s too massive to be useful (big value in sigmoid function). Xavier initialization /ˈzeɪvjər/sFor deep networks, we can use a heuristic to initialize the weights depending on the non-linear activation function. This applies to Xavier and He initialization. Xavier/Glorot Initialization initializes the weights in your network by drawing them from a distribution with zero mean and a specific variance.$$ { var } ( w _ { i } ) = \frac { 1 } { layer_{l-1}}$$ 1w=np.random.randn(layer_size[l],layer_size[l-1])*np.sqrt(1/layer_size[l-1]) In practice, it works better for layers with sigmoid or tanh function. 总的思想原则：They set the weights neither too much bigger that 1, nor too much less than 1.就是本来就是在 (0,1) 标准正太分布出来，然后进行了进一步的约束条件。思想当特征数量越大的时候，weights 的波动情况是成反比的，最后的weights 数值越接近于均值附近。 He InitializationUsing RELU or Leaky RELU is relatively robust to the vanishing/ exploding gradient issues compared with sigmoid function especially for networks that are not too deep. And it the case of Leaky RELU, it never has zero gradients. For RELU, we multiply the randomly generated values of $w$ by: $$\sqrt { \frac { 2 } { layer _ { [ l - 1 ] } } }$$1w=np.random.randn(layer_size[l],layer_size[l-1])*np.sqrt(2/layer_size[l-1]) Sometimes, we combine the idea of Xavier initialization and He initializaiton so the variance becomes the following: $$\sqrt { \frac { 2 } { layer _ { [ l - 1 ] } + \operatorname { layer } _ { [ l ] } } }$$ 12w=np.random.randn(layer_size[l],layer_size[l-1])*np.sqrt(2/(layer_size[l-1]+layer_size[l]))# 代码中 random() 中的两个参数是 shape，最后的np.sqrt 标准差 The idea behind this is that we set the weights neither too much bigger than 1 nor too much less than 1 so the gradients do not vanish or explode too quickly. 所以上述的初始化从数学的角度去理解： 从(0,1) 标准正太分布 转换成了 (0, np.sqrt(2/ size_l -1)) 这样的分布，就是你网络结构是越宽，那么这个方差就是越小的，最后的结果是越集中的，就越集中的 均值 u 左右。从图像的角度看，方差越大，图像越矮胖；方差越小，图像越瘦高。 Takeoff In summary, the main difference in machine learning is the following: He initialization works better for layers with ReLu(s) activation. Xavier initialization works better for layers with sigmoid activation. Activation function总的说可以分为线性和非线性的激活函数， activation function 的作用 就是对于网络的输出 说yes or no. It maps the resulting values in between 0 to 1 or -1 to 1 etc. Sigmoid function (Logistic Activation)the only reason why we use sigmoid is because it exists between 0 to 1. 这个非常有利于 predict probability. 因为自然映射到 0 是不存在 然后1 是存在。而当多分类的时候，使用softmax。 Tanh function The advantage is that the negative inputs will be mapped strongly negative and the zero inputs will be mapped near zero in the tanh graph. Relu (Rectified Linear Unit) Activation本质上是分段函数。 range: [0, infinity]. The function and its derivative both are monotonic. Leaky Relu每当一个伟大的东西产出，总会伴随着比较明显的错误，然后紧跟着就是有一个 rectified(improved) 这种版本。这个相比之前就是修正了 当输入是负数的时候，怎么办的问题。 Softmax这个需要从数学的角度去理解，数学公式就是下面。有一个真实的例子： 如果一张”4” 的图片，输出一个网络，最后是softmax 激活函数，那么最后得到的 “4” 的概率是最大的，但是也是有其他的可能性存在的，这个就是softmax 的效果。 最主要的功能是 convert number into probabilities. 这种效果，不像sigmoid 那样有很明确的数学公式。 $$\sigma ( z ) _ { j } = \frac { e ^ { z _ { j } } } { \sum _ { k = 1 } ^ { K } e ^ { z _ { k } } }$$ Softmax function, a wonderfulactivation function that turns numbers aka logits into probabilities that sum to one.Remember the takeaway is: the essential goal of softmax is to turn numbers into probabilities. OptimizerGradient Descent最基本的应该是 gradient descent，这个算是开山鼻祖。给出了每次梯度变化的方向 Momentum个人更喜欢将其翻译成动量，这个gradient 考虑之前的gradient 的方向和大小，有点像动量的的含义。不仅考虑了current step, 而且accumulates gradient of the past steps. A very popular technique that is used along with SGD is called Momentum. Instead of using only the gradient of the current step to guide the search, momentum also accumulates the gradient of the past steps to determine the direction to go. 好处在于: most recent is weighted than the less recent onesthe weightage of the most recent previous gradients is more than the less recent ones.for example: RMSpropRMSprop, or Root Mean Square Propogation has an interesting history. RMSProp also tries to dampen the oscillations, but in a different way than momentum. Adam这种套路总是一样的，总是有一个中和者，关键是如何整合到一起。 Adam or Adaptive Moment Optimization algorithms combines the heuristics of both Momentum and RMSProp. The hyperparameter beta1 is generally kept around 0.9 while beta_2 is kept at 0.99. Epsilon is chosen to be 1e-10 generally. 对于公式的解释，Eq 1 and Eq 2是come from RMSprop, Eq 3 comes from Adam。最后一步是 update 操作，条理清楚，没有问题。(公式中的v 表示导数，g 表示导数e 不太清楚)Here, we compute the exponential average of the gradient as well as the squares of the gradient for each parameters (Eq 1, and Eq 2). To decide our learning step, we multiply our learning rate by average of the gradient (as was the case with momentum) and divide it by the root mean square of the exponential average of square of gradients (as was the case with momentum) in equation 3. Then, we add the update.) 卷积网络一个卷积神经网络由若干卷积层、Pooling层、全连接层组成。 卷积层$Input \rightarrow [Conv] \times N \rightarrow [Pool] \times M \rightarrow [FK] \times K$ 下面的动画显示了包含两个filter的卷积层的计算。我们可以看到 $7 \times 7 \times 3$ 输入，经过两个$3 \times 3 \times 3 $filter的卷积(步幅为2)，得到了$ 3 \times 3 \times 2 $的输出。另外我们也会看到下图的Zero padding是1，也就是在输入元素的周围补了一圈0。Zero padding对于图像边缘部分的特征提取是很有帮助的。 对于包含两个333的fitler的卷积层来说，其参数数量仅有 $(3 \times 3 \times3+1) \times 2 =56 $个，且参数数量与上一层神经元个数无关。与全连接神经网络相比，其参数数量大大减少了。 卷积层输出大小计算：输入大小： $W 1 \times H 1 \times D 1$超参数（filter信息 +是否填充）： filter 个数( K), filter 大小( F), 步长 (S )，边界填充( P)输出：$$\begin{split}W _ { 2 } &amp; = \left( W _ { 1 } - F + 2 P \right) / S + 1 \\H _ { 2 } &amp;= \left( H _ { 1 } - F + 2 P \right) / S + 1 \\D _ { 2 } &amp;= K\end{split}$$ 卷积层参数量计算： 这里有一个重要的概念叫做权值共享，给定一张图片，用filter 去扫描这图片，filter 里面的数叫权重，这张图是被同样的filter 扫描的，权值是一样的，所以叫做权值共享。这个概念是和全连接层中的权值进行比较的，简单来说就是降低了权重的使用。权重共享即 filter 值的共享。对于 三维图片来说，每个filter需要FFD1个权重值，总共K个filter，需要FFD1*K权重值。和一维一样，整个滑动过程中filter W0和W1值保持不变，可称作权值共享。而且，补充一句，对于三维的input，权值只是在input的每个depth slice上共享的。对于一层的 filter 只是有一个bias。 for example:Filter个数：32原始图像shape：$224 \times 224 \times 3$卷积核大小为：$2 \times 2$一个卷积核的参数：$ 2 \times 2 \times 3=12 $16个卷积核的参数总额：$ 16 \times 12 + 16 =192 + 16 = 208 $$ weight \times x + bias $根据这个公式，即可算的最终的参数总额为：208 Pooling 层Pooling层主要的作用是下采样，主要有两点作用，一个是提取重要特征，一个是简化网络的计算。Pooling的方法很多，最常用的是Max Pooling。Max Pooling实际上就是在n*n的样本中取最大值，作为采样后的样本值。下图是 max pooling： 除了Max Pooing之外，常用的还有Mean Pooling——取各样本的平均值。 池化层往往在卷积层后面，通过池化来降低卷积层输出的特征向量，同时改善结果（不易出现过拟合）。 池化层参数个数计算： 这个很明显是没有参数的。 全连接层连接所有的特征，将输出值送给分类器（如softmax分类器） 比如说上一层（池化层）的输出为：$(111 \times 111 \times 16) $，从第一层到第二层，只是图片大小发生了变化，深度没有发生变化，而Dense对应的神经元个数为133个，那么还是根据公式：$weight \times x + bias$，计算得：$133 \times16+133=2261$ Dropout 概念 dropout 是指在深度学习网络的训练过程中，按照一定的概率将一部分神经网络单元暂时从网络中丢弃，相当于从原始的网络中找到一个更瘦的网络。 why 我们在训练神经网络的时候，会遇到两大缺点： 容易过拟合 费时 dropout 主要是为了在一定程度上减少过拟合。 工作原理 首先随机（临时）删掉网络中一半的隐藏神经元，输入输出神经元保持不变（图中虚线为部分临时被删除的神经元） 然后把输入x通过修改后的网络前向传播，然后把得到的损失结果通过修改的网络反向传播。一小批训练样本执行完这个过程后，在没有被删除的神经元上按照随机梯度下降法更新对应的参数（w，b） 继续重复这一过程：恢复被删掉的神经元（此时被删除的神经元保持原样，而没有被删除的神经元已经有所更新）。从隐藏层神经元中随机选择一个一半大小的子集临时删除掉（备份被删除神经元的参数）。 不断的重复着一过程 怎么理解测试时权重参数w要乘以概率p？ 假设总共有100个神经元，训练的时候我们加上dropout，p=0.5，那么我们就有50个神经元参与训练，那么我们每次50个神经元训练出来的模型参数w是要比直接100个神经元要小的，因为它更新的次数会更少。我们测试的时候100个神经元是都会参与计算的，这就跟训练的时候我们使用50个神经元产生差异了，如果要保证测试的时候每个神经元的关联计算不能少，只能从通过改变w来达到跟训练时一样输出，所以才会有权重参数w乘以p。 为什么 dropout 可以有效的减少过拟合？（类似取平均的活动）因为不同的网络可能产生不同的过拟合，取平均则有可能让一些“相反的”拟合互相抵消。dropout掉不同的隐藏神经元就类似在训练不同的网络，随机删掉一半隐藏神经元导致网络结构已经不同，整个dropout过程就相当于对很多个不同的神经网络取平均。 梯度消失/ 梯度爆炸 首先一个观点，梯度消失和梯度爆炸本质上一回事。 理由：sigmoid 导数的最大值为0.25，通常 abs(w) &lt; 1,则上述分析中的激活函数的导数与权重的积小于0.25，前面的层比后面的层梯度变化更小，故变化更慢，从而引起了梯度消失问题。当权值过大，前面层比后面层梯度变化更快，则引起梯度爆炸问题。所以后面的梯度消失和梯度爆炸只是前面初始化值的一种蝴蝶效应，只是数值问题。 解决方法 重新设计网络结构 使用比较浅的网络结构 使用残差结构, 这种方式在图像处理中更加常见 激活函数使用 relu or leaky relu 而不是 sigmoid or tanh 关于weights 方面 使用梯度截断（Gradient Clipping），检查误差梯度的值是否超过阈值，如果超过，则截断梯度，将梯度设置为阈值。 使用权重正则化（Weight Regularization），常用的是 ，通常使用的是 L1 惩罚项（权重绝对值）或 L2 惩罚项（权重平方）这个是在损失函数上的操作 batch normalization （关于BN 的位置是可以再查一下的，现在有两个位置，一个是在激活函数之前一个是 激活函数之后，个人倾向于激活函数之前，因为这样才有可以减少梯度消失和梯度爆炸的发生呀） dropout一定在激活函数之后 ps: CONV / FC - &gt; BatchNorm - &gt; ReLu（或其他激活） - &gt; Dropout - &gt; CONV / FC对于 cnn 还有一种常见的结构：先卷积，再batchnorm, 然后激活函数，最后pooling在fully connection中的应用，用在全连接层之后激活函数之前 理解 正则化如何减少模型过拟合程度 High Bias（高偏差）就是欠拟合，High Variance（高方差）就是过拟合。 简单来说，正则化就是在原来的 cost function 中添加 正则项。 正则化项能减少模型的非线性程度，从而降低模型的过拟合。从图中来看，正则化项能将过拟合的模型（蓝色）变为Just Right的模型（粉红色）。 为什么正则化是有效的？ 对于线性模型，其添加了正则化项的Cost Function如下图。 对于神经网络，其激活函数（以tanh为例）如下图 直观的理解，如果我们的正则化系数（lambda）无穷大，则权重w就会趋近于0。权重变小，激活函数输出z变小。z变小，就到了激活函数的线性区域，从而降低了模型的非线性化程度。]]></content>
      <categories>
        <category>Deep learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[siamese network]]></title>
    <url>%2F2019%2F03%2F26%2Fsiamese-network%2F</url>
    <content type="text"><![CDATA[主要是介绍自己论文中的网络结构：siamese network。 但凡优化，无非两条路：在好的基础上更快，在快的基础上效果更好。 Siamese network训练速度快，所以只是需要其在训练效果上更好。 先来分析一下为什么训练速度快。那么不得不分析该网络结构。整个网络的输入是 (img1, img2, y) 这样的三元组，img 表示图片，y表示label。图片可以是同一类别的图片，也可以是不同类别的，y表示两张图片之间的相似程度，y的取值 (0,1)，0表示相似（同一类别），1 表示不相似（不同类别）。并且y 是double 类型，属于0-1 区间任意的数字。网路结构最后的输出是 0-1 区间的任意数字。通常是以0.5 作为分界线，如果小于0.5 那么认为两种图片是属于同一类别，或者说更相似；反之也成立。重要的一点是中间的weights 实现了权值共享，这样可以加快网络的训练速度。 loss function这个是属于经典的 contrastive loss function。当y 接近于0的时候，右半部分消失，这个是表示两张图片很是相似，然后就不断使得 欧氏距离减少；当y 接近于1的时候，左半部分消失，这个时候两张图片很不相似，然后右边就是 hinge loss。参数m 作为一种margin 是是可以调节，我的实验中 m 取1.总的思想：就是使得相近的图像距离相近，不想近的图像距离变远。 $L ( W , ( Y , X _ { 1 } , X _ { 2 } ) ) = ( 1 - Y ) \frac { 1 } { 2 } ( D _ { W } ) ^ { 2 } + ( Y ) \frac { 1 } { 2 } { \max ( 0 , m - D _ { W } ) } ^ { 2 }$ Spectral Normalization图像输入到网络之前使用正则化，然后输入到激活函数之前也是使用正则化，所以这种效果也是扩展到 weights，直接对 weights 进行正则化使其符合 Lipschitz 约束，避免使用大的gradients。在GAN 网络中的 discriminator 或者 generator 都发现了其可以稳定训练的过程。在实验中，我们扩大了了这种使用范围，把其应用到所有的网络的layer上。 self-attention mechanismAttention 机制自从 “Attention Is All You Need” 开始火爆，并且实验的效果也是很好的，然后在图像领域也开始尝试使用 attention 机制来解决长依赖的问题。应用到图像领域主要是 explore spatial locality information, 说白了就是细节的信息。 If we look at the DCGAN model, we see that regular GANs are heavily based on convolution operations, which use a local receptive field (convolutional kernel) to learn representations. Simple features like edges and corners are learned in the first few layers. Also, ConvNets are able to use these simple representations to learn more complex ones. However, long-range dependency might be hard to learn. Long-range dependency (long-term dependency) is from RNN, which we can say anything larger than trigram as a long term dependency. Thus, most of the image content does not exhibit elaborated shape such as sky or the ocean looks fine. The task of creating geometrically complex forms, such as four-legged animals, is far more challenging. This is where attention comes into play. 而 self-attention 中QKV 三个部分是相同的，对于这种处理方法和Res_block 还是有点相似的。 结果训练数据集使用是 Cifar-10，记录了训练过程中 acc 和loss 的变化情况。除了训练的效果比较好外，训练速度也是非常快的，可以清楚的看到model acc 在接近25 epoches的时候就开始收敛。]]></content>
      <categories>
        <category>Deep learning</category>
      </categories>
      <tags>
        <tag>siamese network</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[fastText & faiss]]></title>
    <url>%2F2019%2F03%2F25%2FfastText-faiss%2F</url>
    <content type="text"><![CDATA[fastTextfastText结合了自然语言处理和机器学习中最成功的理念。这些包括了使用词袋以及n-gram袋表征语句，还有使用子字(subword)信息，效果上的提升。另外采用了一个softmax层级(利用了类别不均衡分布的优势)来加速运算过程。 fastText 主要是用来解决 word representations和 sentence classification. 有趣的是前者是无监督的学习方式，后者是有监督的学习方式。分别主要来自 ”Enriching Word Vectors with Subword Information“ 和 “Bag of Tricks for Efficient Text Classification” 两篇论文。并且使用的是 shallow neural network 而不是深度网络。 FastText is a library created by the Facebook Research Team for efficient learning of word representations and sentence classification. Take off:fastText 方法包含三部分：模型架构、层次 Softmax 和 N-gram 特征。fasttext 有两个用处： text classification 和 word embedding 。使用场景：大型数据，高效计算 下面进行细说： 模型架构这个是总的框架图。 抱歉哈 这个引用找不见了，如果有侵权，please email me..分为两个部分介绍这个网络结构：从input -&gt; hidden:输入层输入的是一个已经分词后短文本。短文本中每个词的词向量是由该短文本的one-hot矩阵乘以一个初始化的矩阵w得到的。（原理图：下图是fasttext 运行的时候，这个分词是再处理成单词和n-gram 组成的特征，这个是不需要我们进行显性的操作的）从 hidden -&gt; output：插播一句，我们经常使用的预训练模型中的weights 是从input-&gt; hidden。 Hierarchical SoftmaxHierarchical Softmax 不是fasttext 首创，它的改进之处在于实现结构上基于 huffman 树而不是普通的二叉树，属于运算上的优化。具体说来：利用了类别（class）不均衡这个事实（一些类别出现次数比其他的更多），通过使用 Huffman 算法建立用于表征类别的树形结构。对标签进行编码，能够极大地缩小模型预测目标的数量。 这个是softmax 的原始的计算公式：$$p \left( w _ { j } | w _ { I } \right) = y _ { j } = \frac { \exp \left( u _ { j } \right) } { \sum _ { j ^ { \prime } = 1 } ^ { V } \exp \left( u _ { j ^ { \prime } } \right) }$$ 采用二叉树的结构之后，时间上优化不少。$O ( N) \rightarrow O \left( \log _ { 2 } N \right)$。见下图。 抱歉哈 这个引用找不见了，如果有侵权，please email me..和之前的神经网络模型相比，这里的huffmax树的所有内部节点就类似之前神经网络隐藏层的神经元。其中，根节点的词向量对应我们的投影后的词向量，而所有的叶子节点就类似于之前神经网络softmax输出层的神经元。叶子节点的个数就是词汇表的大小. 和之前的相比，从隐藏层到输出层的softmax映射不是一下就完成的，而是沿着 huffman树一步步完成的，因此这种 softmax取名为”Hierarchical softmax”. N-gram 特征N-gram是基于这样的思想：某个词的出现依赖于其他若干个词；我们获得的信息越多，预测越准确。我想说，我们每个人的大脑中都有一个N-gram模型，而且是在不断完善和训练的。我们的见识与经历，都在丰富着我们的阅历，增强着我们的联想能力。N-gram 是一种思想，可以有两种level 的实现，一种是基于 character-level，一种是基于 word-level，前者是扩充了对于”不常见“单词，后者是考虑了部分的词的顺序，都是考虑了”周边“ 信息,用流行的话就是 context 的信息。所以比较难界定 fasttext 训练出来的是不是有比较强的词序。 N-gram模型是一种语言模型（Language Model，LM），语言模型是一个基于概率的判别模型，它的输入是一句话（单词的顺序序列），输出是这句话的概率，即这些单词的联合概率（joint probability）。 这样的作用，使用N-gram来给文本添加额外的特征获得关于局部词顺序的部分信息。举个栗子：对于句子：“我 喜欢 喝 咖啡”, 如果不考虑顺序，那么就是每个词，“我”，“喜欢”，“喝”，“咖啡”这五个单词的word embedding求平均。如果考虑2-gram, 那么除了以上五个词，还有“我喜欢”，“喜欢喝”，“喝咖啡”等词。“我喜欢”，“喜欢喝”，“喝咖啡”这三个词就作为这个句子的文本特征。我们经常见到的场景：输入法的预选词汇。就是可以通过这种方式实现的。 当然使用了更多的特征意味着计算量的增加，计算效率下降，于是该作者提出了两种解决方法： 过滤掉低词频 使用词粒度代替字粒度。 比如说海慧寺使用上面那个句子”我喜欢喝咖啡“，如果使用子粒度的2-gram，那么产生的特征是“我喜”，“喜欢”，“欢喝”，“喝咖”，“咖啡”。如果使用词粒度为2-gram，那么产生的特征是“我喜欢”，“喜欢喝”，“喝咖啡”。 补充一句，subwords就是一个词的character-level的n-gram。比如单词”hello”，长度至少为3的char-level的ngram有”hel”,”ell”,”llo”,”hell”,”ello”以及本身”hello”。n-gram 是一种思想，可以是针对words 之间的，也是可以针对一个word 内部的。前者就是候选词，哪些词语容易组合出现，后者主要是对于单词本身的伸展，可以把没有在dict 中的单词，使用字词（subword） 进行表示，扩充了模型的表达能力。 Negative Sampling但凡效果优化提升，要么是基于计算上的，要么是基于最后效果上的。 主要是减轻计算量的角度考虑的，每次让一个训练样本仅仅更新一部分的权重参数，这个技术不是 fastText 首创的，但是本着总结知识点的，也就放在这里了。 CBOW / Skip-gram模型 （这个论文中）提出了两种方法，一种是Hierarchical Softmax，另一种是Negative Sampling。论文中提出的两种方法都是用来提高计算效率的，下面说一下负采样。 在训练神经网络时，每当接受一个训练样本，然后调整所有神经单元权重参数，来使神经网络预测更加准确。换句话说，每个训练样本都将会调整所有神经网络中的参数。而 Negative Sampling 每次让一个训练样本仅仅更新一小部分的权重参数，从而降低梯度下降过程中的计算量。如果 vocabulary 大小为1万时， 当输入样本 ( “fox”, “quick”) 到神经网络时， “ fox” 经过 one-hot 编码，在输出层我们期望对应 “quick” 单词的那个神经元结点输出 1，其余 9999 个都应该输出0。在这里，这9999个我们期望输出为0的神经元结点所对应的单词我们称为 negative word，随机选择一小部分的 negative words，比如选 10个 negative words 来更新对应的权重参数。 解决的问题，在最后一层 softmax 的计算量太大，相当于每一次word 都是需要整个dict 量的级别的更新。然后选择 k 个negative words，只是计算这些softmax 的值。 Training a neural network means taking a training example and adjusting all of the neuron weights slightly so that it predicts that training sample more accurately. In other words, each training sample will tweak all of the weights in the neural network.As we discussed above, the size of our word vocabulary means that our skip-gram neural network has a tremendous number of weights, all of which would be updated slightly by every one of our billions of training samples!Negative sampling addresses this by having each training sample only modify a small percentage of the weights, rather than all of them. Here’s how it works.When training the network on the word pair (“fox”, “quick”), recall that the “label” or “correct output” of the network is a one-hot vector. That is, for the output neuron corresponding to “quick” to output a 1, and for all of the other thousands of output neurons to output a 0.With negative sampling, we are instead going to randomly select just a small number of “negative” words (let’s say 5) to update the weights for. (In this context, a “negative” word is one for which we want the network to output a 0 for). We will also still update the weights for our “positive” word (which is the word “quick” in our current example).The paper says that selecting 5-20 words works well for smaller datasets, and you can get away with only 2-5 words for large datasets.Recall that the output layer of our model has a weight matrix that’s 300 x 10,000. So we will just be updating the weights for our positive word (“quick”), plus the weights for 5 other words that we want to output 0. That’s a total of 6 output neurons, and 1,800 weight values total. That’s only 0.06% of the 3M weights in the output layer!In the hidden layer, only the weights for the input word are updated (this is true whether you’re using Negative Sampling or not). Positive samples and Negative samplesOne little detail that’s missing from the description above is how do we select the negative samples.（下面说的是如何进行选择negative sample的问题：基本思路是根据出现频率进行选择）The negative samples are chosen using the unigram distribution. Essentially, the probability of selecting a word as a negative sample is related to its frequency, with more frequent words being more likely to be selected as negative samples. Instead of using the raw frequency in the original word2vec paper, each word is given a weight that’s equal to it’s frequency (word count) raised to the 3/4 power. The probability for selecting a word is just it’s weight divided by the sum of weights for all words.$$P \left( w _ { i } \right) = \frac { f \left( w _ { i } \right) ^ { 3 / 4 } } { \sum _ { i = 0 } ^ { n } \left( f \left( w _ { j } \right) ^ { 3 / 4 } \right) }$$This decision to raise the frequency to the 3/4 power appears to be empirical; as the author claims it outperformed other functions (e.g. just using unigram distribution).Side note: The way this selection is implemented in the original word2vec C code is interesting. They have a large array with 100M elements (which they refer to as the unigram table). They fill this table with the index of each word in the vocabulary multiple times, and the number of times a word’s index appears in the table is given by Then, to actually select a negative sample, we just generate a random integer between 0 and 100M, and use the word at that index in the table. Since the higher probability words occur more times in the table, we’re more likely to pick those. 这个也是有讲 任何进行negative sample的选择http://jalammar.github.io/illustrated-word2vec/一般来说在 word2vec 中context 是会选择到 5，然后这个 positive / negative sample 会是(1/6), 然后 nagative sample 是随机在 dictionary里面选的（所以有可能选到 positive sample）， 这个是这个dictionary 是根据频率，出现次数越多的，被选中的可能性也越大。The number of negative samples is another factor of the training process. The original paper prescribes 5-20 as being a good number of negative samples. It also states that 2-5 seems to be enough when you have a large enough dataset. The Gensim default is 5 negative samples. To address this, we need to introduce negative samples to our dataset – samples of words that are not neighbors. Our model needs to return 0 for those samples. Now that’s a challenge that the model has to work hard to solve – but still at blazing fast speed.This idea is inspired by Noise-contrastive estimation. We are contrasting the actual signal (positive examples of neighboring words) with noise (randomly selected words that are not neighbors). This leads to a great tradeoff of computational and statistical efficiency. Essentially, the probability for selecting a word as a negative sample is related to its frequency, with more frequent words being more likely to be selected as negative samples. They fill this table with the index of each word in the vocabulary multiple times, and the number of times a word’s index appears in the table is given by $P(wi)*P(wi)$ table_size. Then, to actually select a negative sample, you just generate a random integer between 0 and 100M, and use the word at that index in the table. Since the higher probability words occur more times in the table, you’re more likely to pick those. (ps 这种数量比不是 1：1，常常是 positive ： negative =1：5， 这个是经验值，在传统机器学习中可能认为是 data unbalanced)It’s now time to build out our skip-gram generator which will give us pair of words and their relevance (word, word in the same window), with label 1 (positive samples). (word, random word from the vocabulary), with label 0 (negative samples). 使用第一个应用场景：词向量。fastText作为训练词向量认为可以有两种模式，一种是根据周围词语预测中心词汇的CBOW （continuous bag-of-words）模型，另一种是根据中心词汇预测上下文的 skip-gram 模型。 ./fasttext – It is used to invoke the FastText library. skipgram/cbow – It is where you specify whether skipgram or cbow is to be used to create the word representations. -input – This is the name of the parameter which specifies the following word to be used as the name of the file used for training. This argument should be used as is. data.txt – a sample text file over which we wish to train the skipgram or cbow model. Change this name to the name of the text file you have. -output – This is the name of the parameter which specifies the following word to be used as the name of the model being created. This argument is to be used as is. model – This is the name of the model created.Running the above command will create two files named model.bin and model.vec. model.bin contains the model parameters, dictionary and the hyperparameters and can be used to compute word vectors. model.vec is a text file that contains the word vectors for one word per line. 最后生成有两个文件，一个 xxx.bin 文件，一个是 xxx.vec 文件，前者是预训练模型，后者是词向量。这两个可能是最重要的格式了。 The most important parameters of the model are its dimension and the range of size for the subwords. 常见的代码格式： ./fasttext skipgram -input data/fil9 -output result/fil9 -minn 2 -maxn 5 -dim 300 跑偏一下说一下shell的小技巧。使用echo 或者 &lt; 这样进行单个词或者多个词的词向量的查询。 ./fasttext print-word-vectors model.bin &lt; queries.txtecho “word” | ./fasttext print-word-vectors model.bin Finding simialr words: ./fasttext nn model.bin 最重要的几个参数： The most important parameters of the model are its dimension and the range of size for the subwords. The dimension (dim) controls the size of the vectors, the larger they are the more information they can capture but requires more data to be learned. But, if they are too large, they are harder and slower to train. By default, we use 100 dimensions, but any value in the 100-300 range is as popular. The subwords are all the substrings contained in a word between the minimum size (minn) and the maximal size (maxn). By default, we take all the subword between 3 and 6 characters, but other range could be more appropriate to different languages: 1$ ./fasttext skipgram -input data/fil9 -output result/fil9 -minn 2 -maxn 5 -dim 300 The following arguments for the dictionary are optional: -minCount 词出现的最少次数 [5] -minCountLabel 标签出现的最少次数 [0] -wordNgrams 单词 ngram 的最大长度 [1] -bucket 桶的个数 [2000000] -minn char ngram 的最小长度 [3] -maxn char ngram 的最大长度 [6] The following arguments for training are optional -dim 字向量的大小 [100] -ws 上下文窗口的大小 [5] -epoch 迭代次数 [5] -neg 负样本个数 [5] -loss 损失函数 {ns, hs, softmax} [ns] 第二个应用场景：文本分类。 Sentiment analysis and email classification are classic examples of text classification （BERT 也是采用的这种label 的格式）在训练数据集中label 默认是使用 “__label__” 进行表示的，当然也是可以进行自定义的。 ./fasttext supervised -input train.ft.txt -output model_kaggle -label __label__ -lr 0.5 就是进行predict的时候，有时候并不是很能想起来只是predict top 3 这样的东西。 # Predicting on the test dataset ./fasttext predict model_kaggle.bin test.ft.txt # Predicting the top 3 labels ./fasttext predict model_kaggle.bin test.ft.txt 3 faiss用途：相似度检测和稠密向量的聚类。 Faiss is a library for efficient similarity search and clustering of dense vectors. 之前的实习经历主要是用faiss 处理文本的representation，但是这个是有偏差的，凡是能够打成词向量，都是可以使用faiss 进行计算的，当然这词向量需要满足：相近内容在相近的空间。 Once the vectors are extracted by learning machinery (from images, videos, text documents, and elsewhere), they’re ready to feed into the similarity search library. faiss的实现过程首先使用 index对于向量进行预处理，然后选择不同的模式。 牺牲了一些精确性来使得运行速度更快。 Similarity search can be made orders of magnitude faster if we’re willing to trade some accuracy; that is, deviate a bit from the reference result. For example, it may not matter much if the first and second results of an image similarity search are swapped, since they’re probably both correct results for a given query. Accelerating the search involves some pre-processing of the data set, an operation that we call indexing. ( 下面这句话的观点是什么，感觉不知道逻辑在哪里啊)向量的比较有两种metric：一种是L2 一种是基于consine (点乘)进行检索。前者是求解最小的值，后者是通过inner——product 求解maximum. 并且是支持GPU的，在原来CPU上建立的index，然后很好的迁移到 GPU上。 faiss 中的三种基本索引 IndexFlatL2 基于brute-force计算向量的L2距离，就是暴搜。检索速度慢，适用于小数据量。 在计算上进行了优化，比如使用堆存储结构，寻找最接近的 K 个元素时候后，进行分段计算，把 d 维向量分成几段分别进行计算；建立倒排索引( id -contents) ，先使用聚类，然后再类内和相近的类进行寻找而非整个空间。 12345678910111213141516171819202122232425262728293031323334353637383940import numpy as npd = 64 # 维度nb = 100000 # 数据库大小nq = 10000 # 要搜索的querynp.random.seed(1234) # 确定种子，使随机数可重现xb = np.random.random((nb, d)).astype('float32')xb[:, 0] += np.arange(nb) / 1000. # 每一行的第一个列增加一个等差数列的对应项数xq = np.random.random((nq, d)).astype('float32')xq[:, 0] += np.arange(nq) / 1000.print(xq.shape) # (10000, 64)print(xb.shape) # (100000, 64)import faiss # make faiss availableindex = faiss.IndexFlatL2(d) # 构建FlatL2索引print(index.is_trained)print(index.ntotal)index.add(xb) # 向索引中添加向量。add操作如果没有提供id，则使用向量序号作为id。print(index.ntotal)k = 4 # 搜索多少个临近向量D, I = index.search(xb[:5], k) # 用xb的前五行本身自己搜索自己，完整性检查，用于测试print("I=")print(I)#I=#[[ 0 393 363 78 924]# [ 1 555 277 364 617]# [ 2 304 101 13 801]# [ 3 173 18 182 484]# [ 4 288 370 531 178]]# I输出类似于上面，每行对应着相应向量的搜索结果。k为多少就有多少列，distance低的排在前面。# 可以看到前五行的第一列确实是0~4print("D=")print(D)#[[0. 7.1751733 7.207629 7.2511625]# [0. 6.3235645 6.684581 6.7999454]# [0. 5.7964087 6.391736 7.2815123]# [0. 7.2779055 7.5279865 7.6628466]# [0. 6.7638035 7.2951202 7.3688145]]# 可以看到第一行第一列都是0，意思是向量与自己本身的距离为0D, I = index.search(xq, k) # 搜索print(I[:5]) # 最初五个向量查询的结果print(I[-5:]) # 最后五个向量查询的结果 IndexIVFFlat (加速搜索) 对于暴搜来说，海量数据搜索速度太慢，那么需要预训练把向量都聚类。这里使用IndexIVFFlat来加快搜索速度。IndexIVFFlat是faiss的倒排索引，把数据构成的向量空间切割为Voronoi细胞，每个向量落入其中一个Voronoi细胞中。在搜索时，只有查询x所在细胞中包含的数据库向量y与少数几个相邻查询向量进行比较。 训练的时候还需要有一个量化器，用于决定以什么方式将向量分配给Voronoi细胞。每个细胞由一个质心定义，找到一个向量所在的Voronoi细胞包括在质心集中找到该向量的最近邻居。 搜索方法有两个参数： nlist 划分Voronoi细胞的数量 nprobe 执行搜索访问的单元格数(不包括nlist)，该参数调整结果速度和准确度之间折中的一种方式。如果设置nprobe=nlist则结果与暴搜一致。 加快索引的方式之一，与暴搜对比就是需要train，把向量空间下的数据切割为Voronoi细胞，检索只对向量所在细胞和周围细胞进行检索。 123456789101112131415161718192021222324252627import numpy as npd = 64 # dimensionnb = 100000 # database sizenq = 10000 # nb of queriesnp.random.seed(1234) # make reproduciblexb = np.random.random((nb, d)).astype('float32')xb[:, 0] += np.arange(nb) / 1000.xq = np.random.random((nq, d)).astype('float32')xq[:, 0] += np.arange(nq) / 1000.import faissnlist = 100k = 4quantizer = faiss.IndexFlatL2(d) # 内部的索引方式index = faiss.IndexIVFFlat(quantizer, d, nlist, faiss.METRIC_L2)# here we specify METRIC_L2, by default it performs inner-product searchprint("before train")assert not index.is_trainedindex.train(xb)assert index.is_trainedprint("before add")index.add(xb) # add may be a bit slower as wellD, I = index.search(xq, k) # actual searchprint(I[-5:]) # neighbors of the 5 last queriesindex.nprobe = 10 # default nprobe is 1, try a few moreD, I = index.search(xq, k)print(I[-5:]) # neighbors of the 5 last queries IndexIVFPQ (减少内存使用) 上面两种索引都是存储的完整向量，下面介绍一种压缩向量的方法。IndexIVFPQ基于PQ (Product Quantizer)算法压缩向量。在这种情况下，由于向量没有精确存储，搜索方法返回的距离也是近似值。上面我们看到的索引IndexFlatL2和IndexIVFFlat都会全量存储所有的向量在内存中，为满足大的数据量的需求，faiss提供一种基于Product Quantizer(乘积量化)的压缩算法编码向量大小到指定的字节数。此时，存储的向量时压缩过的，查询的距离也是近似的。 原理：简单来说就是通过PCA将高纬空间转换成低维空间。 原来的数据 train 得到一个转换矩阵P，然后这个矩阵和原来的数据X得到新的降维之后的Y ($PX =Y$)。这样转换过程中信息损失的更少，在faiss 中使用 train() 函数进行实现。 123456789101112131415161718192021222324252627282930313233343536373839import numpy as npd = 64 # dimensionnb = 100000 # database sizenq = 10000 # nb of queriesnp.random.seed(1234) # make reproduciblexb = np.random.random((nb, d)).astype('float32')xb[:, 0] += np.arange(nb) / 1000.xq = np.random.random((nq, d)).astype('float32')xq[:, 0] += np.arange(nq) / 1000.import faissnlist = 100m = 8k = 4quantizer = faiss.IndexFlatL2(d) # 内部的索引方式index = faiss.IndexIVFPQ(quantizer, d, nlist, m, 8)# 每个向量都被编码为8个字节大小index.train(xb)index.add(xb)D, I = index.search(xb[:5], k) # sanity checkprint(I)print(D)#[[ 0 78 714 372]# [ 1 1063 555 277]# [ 2 304 134 46]# [ 3 773 64 8]# [ 4 288 531 827]]#[[1.6675376 6.1988335 6.4136653 6.4228306]# [1.4083313 6.023788 6.025648 6.284443 ]# [1.6988016 5.592166 6.139589 6.6717234]# [1.7987373 6.625978 6.7166452 6.865783 ]# [1.5371588 5.7953157 6.38059 6.4141784]]# 可以看到确实搜索到了正确的结果，但是第一行第一列的distance不为零，属于有损压缩。# 虽然与接下来的几列（其他几个搜索结果）对比还是有几倍的优势。index.nprobe = 10 # 与以前的方法相比D, I = index.search(xq, k) # searchprint(I[-5:]) 在涉及index使用考虑速度，acc和内存大小三个不同的维度。然后不同的index 是有不同的侧重的。 安装参考 Take Off(这个是有三方面需要权衡的： query time、 query accuracy and preprocessing time) As with anything, there is a tradeoff between improving query time versus query accuracy versus preprocessing/index build time versus data storage: no build time, high query time, high storage, exact accuracy: Faiss IndexFlat low build time, med query time, high storage, high accuracy: Faiss IndexIVFFlat med build time, low query time, low-med storage, med-high accuracy: Faiss IndexIVFPQ very high build time, low query time, low-high storage (whether stored as a k-NN graph or raw data), high accuracy: NN-Descent by Dong et al. (e.g., nmslib) IndexIVFPQ with perhaps IMI is typically what we concentrate on, seems to be a reasonable sweet spot for billion-scale datasets. product quantization 算法这里的乘积是指笛卡尔积（Cartesian product），意思是指把原来的向量空间分解为若干个低维向量空间的笛卡尔积，并对分解得到的低维向量空间分别做量化（quantization）。这样每个向量就能由多个低维空间的量化code组合表示。 The idea is to decomposes the space into a Cartesian product of low dimensional subspaces and to quantize each subspace separately. A vector is represented by a short code composed of its subspace quantization indices. Image Vector Dataset: 存储的是离 embedding 最近的centroid (质心) 的编号 而非向量本身。 Let’s say you have a collection of 50,000 images, and you’ve already performed some feature extraction with a convolutional neural network, and now you have a dataset of 50,000 feature vectors with 1,024 components each. The first thing we’re going to do is compress our dataset. The number of vectors will stay the same, but we’ll reduce the amount of storage required for each vector. Note that what we’re going to do is not the same as “dimensionality reduction”! This is because the values in the compressed vectors are actually symbolic rather than numeric, so we can’t compare the compressed vectors to one another directly. Two important benefits to compressing the dataset are that (1) memory access times are generally the limiting factor on processing speed, and (2) sheer memory capacity can be a problem for big datasets. Here’s how the compression works. For our example we’re going to chop up the vectors into 8 sub-vectors, each of length 128 (8 sub vectors x 128 components = 1,024 components). This divides our dataset into 8 matrices that are [50K x 128] each. These centroids are like “prototypes”. They represent the most commonly occurring patterns in the dataset sub-vectors. We’re going to use these centroids to compress our 1 million vector dataset. Effectively, we’re going to replace each subregion of a vector with the closest matching centroid, giving us a vector that’s different from the original, but hopefully still close. Doing this allows us to store the vectors much more efficiently—instead of storing the original floating point values, we’re just going to store cluster ids. For each subvector, we find the closest centroid, and store the id of that centroid. Each vector is going to be replaced by a sequence of 8 centroid ids. I think you can guess how we pick the centroid ids–you take each subvector, find the closest centroid, and replace it with that centroid’s id. Note that we learn a different set of centroids for each subsection. And when we replace a subvector with the id of the closest centroid, we are only comparing against the 256 centroids for that subsection of the vector. Because there are only 256 centroids, we only need 8-bits to store a centroid id. Each vector, which initially was a vector of 1,024 32-bit floats (4,096 bytes) is now a sequence of eight 8-bit integers (8 bytes total per vector!). K-means 算法k-Means算法是一种聚类算法，它是一种无监督学习算法，目的是将相似的对象归到同一个蔟中。蔟内的对象越相似，聚类的效果就越好。聚类和分类最大的不同在于，分类的目标事先已知，而聚类则不一样。其产生的结果和分类相同，而只是类别没有预先定义。 K-means 是一种聚类算法，先说一下什么是聚类：聚类分析是在数据中发现数据对象之间的关系，将数据进行分组，组内的相似性越大，组间的差别越大，则聚类效果越好。 K-Means算法思想：对给定的样本集，事先确定聚类簇数K，让簇内的样本尽可能紧密分布在一起，使簇间的距离尽可能大。该算法试图使集群数据分为n组独立数据样本，使n组集群间的方差相等，数学描述为最小化惯性或集群内的平方和。K-Means作为无监督的聚类算法，实现较简单，聚类效果好，因此被广泛使用。 算法步骤 创建k个点作为k个簇的起始质心（经常随机选择）。 分别计算剩下的元素到k个簇中心的相异度（距离），将这些元素分别划归到相异度最低的簇。 根据聚类结果，重新计算k个簇各自的中心，计算方法是取簇中所有元素各自维度的算术平均值。 将D中全部元素按照新的中心重新聚类。 重复第4步，直到聚类结果不再变化。 最后，输出聚类结果。 K-Means算法优缺点 优点 原理易懂、易于实现； 当簇间的区别较明显时，聚类效果较好； Trains quickly 缺点 当样本集规模大时，收敛速度会变慢； 对孤立点数据敏感，少量噪声就会对平均值造成较大影响, 所以离群点的检测和删除对于最后的结果有很大的帮助。 k的取值十分关键，对不同数据集，k选择没有参考性，需要大量实验 Choosing K The algorithm explained above finds clusters for the number k that we chose. So, how do we decide on that number? 尝试法： 计算每个点到最近的簇的距离的总和，如果增加 k 导致的总和下降不明显，那么就接近临界点了。 To find the best k we need to measure the quality of the clusters. The most traditional and straightforward method is to start with a random k, create centroids, and run the algorithm as we explained above. A sum is given based on the distances between each point and its closest centroid. As an increase in clusters correlates with smaller groupings and distances, this sum will always decrease when k increases; as an extreme example, if we choose a k value that is equal to the number of data points that we have, the sum will be zero. The goal with this process is to find the point at which increasing k will cause a very small decrease in the error sum, while decreasing k will sharply increase the error sum. This sweet spot is called the “elbow point.” In the image below, it is clear that the “elbow” point is at k-3.­ 总的来说faiss 高效实现了PCA 算法, k-means 算法 和PQ 算法。 ref 1ref 2ref 3ref 4]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>k-means</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NLP中的碎碎念]]></title>
    <url>%2F2019%2F03%2F25%2FNLP%E4%B8%AD%E7%9A%84%E7%A2%8E%E7%A2%8E%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[顾名思义，主要整理一下自己在文本处理中遇到的小的知识点，比如关键词提取技术，分词软件包的原理。 关键词提取TF-IDF这个是可以参看之前自己写的一个博客 卡方分布卡方检验是以χ2分布为基础的一种常用假设检验方法。该检验的基本思想是：首先假设H0成立，基于此前提计算出χ2值，它表示观察值与理论值之间的偏离程度。 官方定义： 若k 个随机变量$Z_1$、……、$Z_k $相互独立，且数学期望为0、方差为 1(即服从标准正态分布)，则随机变量X$$X = \sum _ { n = 1 } ^ { k } Z _ { n } ^ { 2 }$$被称为服从自由度为 k 的卡方分布，记作$$X \sim \chi ^ { 2 } ( k )$$ 从直观的角度感受一下自由度k 和图像形状的关系，自由度越大卡方分布越接近正太分布。 用于特征选择 如果文章是否包含“篮球“与文章是否属于体育类别是独立无关的。且一个新闻文章属于体育类别的概率是0.609，那么可以得到下面的表格。因为文章是否包含“篮球“与文章是否属于体育类别是独立无关的，所以不管文章中是不是包含”篮球“，其属于体育类别的概率都是0.609。 列联表 组别 体育 非体育 包含”篮球“ 44 * 0.609 = 26.8 44 * 0.391 = 17.2 不包含”篮球“ 43 * 0.609 = 26.2 43 * 0.391 = 16.8 如果两个分类变量真的是独立无关的，那么四格表的实际值与理论值得差值应该非常小（有差值的原因是因为抽样误差）。那么如何衡量实际值与理论值得差值呢？ 步骤 统计每个词的正文档出现频率（A）、负文档出现频率（B）、正文档不出现频率）、负文档不出现频率。-计算每个词的卡方值 将每个词按卡方值从大到小排序，选取前k个词作为特征，k即特征维数。 $$\operatorname { CHI } ( \mathrm { x } , \mathrm { y } ) = \chi ^ { 2 } ( x , y ) = \sum \frac { ( A - T ) ^ { 2 } } { T }$$ 该公式可以进一步简化成$$\mathrm { CHI } ( \mathrm { x } , \mathrm { y } ) = \chi ^ { 2 } ( x , y ) = \frac { N ( A D - B C ) } { ( A + B ) ( A + C ) ( B + D ) ( C + D ) }$$ 组别 体育 非体育 合计 包含”篮球“ 34 (A) 10 (B) 44 (A+B) 不包含”篮球“ 19 (C) 24 (D) 43 (C+D) 合计 53(A +C) 34 (B+D) 87 (N) 卡方分布的临界值 自由度F = （行数 - 1） * （列数 - 1） = 1，对于四格表，F = 1。 由于自由度F = 1，所以只需要看分布表的第一行。可以看到，随着CHI的增大，原假设成立的概率就越小。因为10.10 &gt; 6.64，所以原假设成立是概率是小于1%。反之，也就是说，原假设不成立（即两个分类变量不是独立无关）的概率大于99% 如何应用于特征选择 CHI值越大，说明两个变量越不可能是独立无关的，也就是说X2越大，两个变量的相关程序也就越高。对于特征变量x1,x2,…,xn，以及分类变量y。只需要计算CHI(x1, y)、CHI(x2, y)、…、CHI(xn, y)，并按照CHI的值从大到小将特征排序，然后选择阈值，大于阈值的特征留下，小于阈值的特征删除。这样就筛选出一组特征子集了，接着使用这组特征子集去训练分类器，然后评估分类器的性能。 因为只要比较CHI值得相对大小，所以上述的分布表就没用了。 使用范围：一般是在离散变量上进行使用。 另外一个例子 In the case of classification problems where input variables are also categorical, we can use statistical tests to determine whether the output variable is dependent or independent of the input variables. If independent, then the input variable is a candidate for a feature that may be irrelevant to the problem and removed from the dataset. The Pearson’s chi-squared statistical hypothesis is an example of a test for independence between categorical variables. Contingency TableFor example, the Sex=rows and Interest=columns table with contrived counts might look as follows:下面是一个列联表123 Science, Math, ArtMale 20, 30, 15Female 20, 15, 30 The Pearson’s Chi-Squared test, or just Chi-Squared test for short, is named for Karl Pearson, although there are variations on the test. 如何去解读这种信息？ We can interpret the test statistic in the context of the chi-squared distribution with the requisite number of degress of freedom as follows: If Statistic &gt;= Critical Value: significant result, reject null hypothesis (H0), dependent.If Statistic &lt; Critical Value: not significant result, fail to reject null hypothesis (H0), independent.The degrees of freedom for the chi-squared distribution is calculated based on the size of the contingency table as: degrees of freedom: (rows - 1) * (cols - 1) In terms of a p-value and a chosen significance level (alpha), the test can be interpreted as follows: If p-value &lt;= alpha: significant result, reject null hypothesis (H0), dependent.If p-value &gt; alpha: not significant result, fail to reject null hypothesis (H0), independent.For the test to be effective, at least five observations are required in each cell of the contingency table. case study： Chi-square Test for feature selection $$X ^ { 2 } = \frac{ {(Observed frequency - Expected frequency)} ^ 2 } { Expected frequency }$$ 12345678910111213141516171819202122# Load libraries from sklearn.datasets import load_iris from sklearn.feature_selection import SelectKBest from sklearn.feature_selection import chi2 # Load iris data iris_dataset = load_iris() # Create features and target X = iris_dataset.data y = iris_dataset.target # Convert to categorical data by converting data to integers X = X.astype(int) # Two features with highest chi-squared statistics are selected chi2_features = SelectKBest(chi2, k = 2) X_kbest_features = chi2_features.fit_transform(X, y) # Reduced features print('Original feature number:', X.shape[1]) print('Reduced feature number:', X_kbest.shape[1]) Original feature number: 4Reduced feature number : 2 而实际应用到特征选择中的时候，我们不需要知道自由度，不要知道卡方分布，我们只需要根据算出来的χ2 进行排序就好了，越大我们就越喜欢！挑选最大的一堆，于是就完成了利用卡方检验来进行特征提取。卡方分布的缺点：没有考虑词频，它只统计文档是否出现词，而不管出现了几次。这会使得他对低频词有所偏袒（因为它夸大了低频词的作用）。 参考一参考二 Textrank有一个与之很像的概念 pageRanking，最开始是用来计算网页的重要性。Textrank 主要用来提取文章的关键词，然后比较适合长文本。 CBOW和skip-gram举一个简单的小例子说明 CBOW和skip-gram的区别：skip-gram 是根据中心词汇然后预测上下文词汇，这个不是一下子输入上下文词汇的，而是一个过程，中心词汇和上下文词汇1 ，中心词汇和上下文词汇2，这样的进行输入。 cbow 和其的区别，在于简单相加了上下文词汇作为一个整体，然后和中心词汇进行输入，所以最后是这样的结果。 使用skip gram训练的时间更长，但是对于出现频率不高的词汇，效果比较好。但CBOW的训练速度是相对来说比较快一些。 分词软件常见的中文分词服务 分词服务 开源/商业 支持语言 词性标注 命名实体识别 jieba 开源 Python, Java, C++ 无 无 HanLP 开源 Python，Java， C++ 有 有 StandFord CoreNLP 开源 Java 百度NLP 商业 阿里NLP 商业 分词软件的原理 下面是常见的分词方式，但是并没有说明哪个软件是什么分词方法。 基于规则的分词方法 这种方法又叫作机械分词方法、基于字典的分词方法，它是按照一定的策略将待分析的汉字串与一个“充分大的”机器词典中的词条进行匹配，比如说正向最大匹配法、逆向最大匹配法 基于统计的分词方法 该方法的主要思想：词是稳定的组合，因此在上下文中，相邻的字同时出现的次数越多，就越有可能构成一个词。因此字与字相邻出现的概率或频率能较好地反映成词的可信度。可以对训练文本中相邻出现的各个字的组合的频度进行统计，计算它们之间的互现信息。互现信息体现了汉字之间结合关系的紧密程度。当紧密程 度高于某一个阈值时，便可以认为此字组可能构成了一个词。该方法又称为无字典分词。 比如说CRF 算法。 基于语义的分词方法 语义分词法引入了语义分析，对自然语言自身的语言信息进行更多的处理，如扩充转移网络法、知识分词语义分析法、邻接约束法、综合匹配法、后缀分词法、特征词库法、矩阵约束法、语法分析法等。 逆向最大匹配分词是中文分词基本算法之一，因为是机械切分，所以它也有分词速度快的优点，且逆向最大匹配分词比起正向最大匹配分词更符合人们的语言习惯。逆向最大匹配分词需要在已有词典的基础上，从被处理文档的末端开始匹配扫描，每次取最末端的i个字符（分词所确定的阈值i）作为匹配字段，若匹配失败，则去掉匹配字段最前面的一个字，继续匹配。而且选择的阈值越大，分词越慢，但准确性越好。这种方法经常被用来解决歧义词的问题，所以单独说一下。 算法：事先设置一个k值，下面的程序k值设为5，然后从最后一个字开始向前截取k个字，先把这k个字和字典匹配，看能否找到匹配的词语，若不能，则剔除这k个字最左边的字，然后再把这k-1个字与字典匹配…一直到匹配成功，或者前k-1个字都没匹配成功，那就把第k个字当成一个独立的词，然后再向前移动分出来的词的长度，再截取k个字……一直到全部分好词为止。“我爱北京天安门” 先从后面开始截取k(这里是5)个字，然后把”北京天安门”五个字与字典匹配，字典中没有这个词，然后就去掉”北”字，把剩下的”京天安门”与字典匹配，字典中还是没有这个词，再去掉”京”，然后再把”天安门”与字典匹配，发现匹配到了这个词，于是就把”天安门”划为一个词语，然后指针向前移动三个字。再截取k个字，这里因为就剩下4个字了，所以就截取4个字，把”我爱北京”与字典匹配，没成功，去掉”我”，再把”爱北京”与字典匹配，还是没成功，再去掉”爱”，然后发现”北京”匹配成功，把”北京”划为一个词语，再把指针向前移动两个字， CRF算法基本思路是对汉字进行标注训练，不仅考虑了词语出现的频率，还考虑上下文，具备较好的学习能力，因此其对歧义词和未登录词的识别都具有良好的效果。 近年来，随着硬件计算能力的发展以及词的分布式表示（word embedding）的提出，神经网络可以有效处理许多NLP任务。这类方法对于序列标注任务（如CWS、POS、NER）的处理方式是类似的：将token从离散one-hot表示映射到低维空间中成为稠密的embedding，随后将句子的embedding序列输入到RNN中，用神经网络自动提取特征，Softmax来预测每个token的标签。这种方法使得模型的训练成为一个端到端的过程，而非传统的pipeline，不依赖于特征工程，是一种数据驱动的方法，但网络种类繁多、对参数设置依赖大，模型可解释性差。 推荐博客讲解 CRF算法，反正我是没有看懂，哈哈哈。【中文分词】条件随机场CRF, 浅谈分词算法（4）基于字的分词方法（CRF） 和 CRF算法学习——自己动手实现一个简单的CRF分词 分词粒度目前，分词主要包含细粒度分词和粗粒度分词两种，在不同的应用场景需要用到不同的粒度。细粒度分词是指将原始语句切分成最基本的词语，而粗粒度分词是指将原始语句中的多个基本词组合起来切成一个词，进而组成语义相对明确的实体。 原始串：浙江大学坐落在西湖旁边 细粒度：浙江/大学/坐落/在/西湖/旁边 粗粒度：浙江大学/坐落/在/西湖/旁边 举例说明分词粒度和应用场景：对于query为“耐克鞋”来说，词典中是包含这个实体词的，分词的切分结果就是“耐克鞋”。但是，考拉用来建索引的商品描述中，这三个字是没有连续出现的，自然就没有“耐克鞋”对应的文档，这个时候就无法召回结果。那么，这时候你会说那就使用最小粒度的分词就解决这个问题了。相反，某一款商品可能有“参数表”这三个字，如果有最小粒度的分词策略，分词的结果为“参数/表”。很不幸，当query为“表”时，你会发现会召回莫名其妙的结果。一般来说，是使用词频表来进行粒度控制，基本可以解决绝大多数问题。 搜索引擎展现与粒度：显而易见，粒度越小，展现就越多，建立倒排索引时，索引的长度就越长;粒度的层次越多，索引的数量就越多。一个多，一个长，就对搜索系统的性能构成了极大的考验。搜索引擎并不会对所有小粒度词都建索引，而是选择“更有可能展现相关结果”的小粒度词。所以在一般情况下，切分文本粒度越大，索引越多，相关性越好，但展现越少;切分文本粒度越小，索引越少，相关性越差，但展现越好。 词频决定分词粒度：具体算法是，当发现一个由多个短词组成的长词时，判断每个短词中最小的词频，如果这个词频还是大于长词的词频，则按该组合进行拆分。如果多种组合，按词频最大的组合拆分。如上面例子，”中央饭店”，中央的词频为1000，饭店为900,饭为200,店为600，而中央饭店为500 。 新词发现在中文分词的世界里，最主要的挑战有两个：歧义词识别，未登录词（新词）识别。对于歧义词简单说一下，比如“乒乓球拍卖完了”，切分为以下两种情况都是合理的，“乒乓球拍/卖/完了”，“乒乓球/拍卖/完了”。这个就是典型的歧义词。而处理这种问题常用的一种手段是使用逆向最大匹配法去处理歧义词，由于汉语中偏正结构较多，若从后向前匹配，可以适当提高精确度。比如取出“南京市长江大桥”的后四个字“长江大桥”，发现词典中有匹配，切割下来；对剩余的“南京市”进行分词，整体结果为：“南京市/长江大桥”。 新词对最后结果的影响程度是大于歧义词（20倍左右吧）。 so，这里主要讲的是新词的发现 而非处理歧义词的识别。 中英文分词的区别 分词 / 词干提取和词形还原。 中文和英文的nlp各有各的难点，中文的难点在于需要进行分词，将一个个句子分解成一个单词数组。而英文虽然不需要分词，但是要处理各种各样的时态，所以要进行词干提取和词形还原。 对于中文，一个Word可以是一个单词，也可以是一个词。 分词的重要性 在处理文本对象时，非常关键的问题在于“切词”这个环节，几乎所有的后续结果都依赖第一步的切词。因此切词的准确性在很大程度上影响着后续的处理，切词结果的不同，也就影响了特征的提取，跟数据挖掘一样，特征提取的好坏特别重要，不论用什么算法，特征好数据好结果才会好。 分词的目的 当模型的记忆和拟合能力足够强（或者简单点，足够智能）的时候，我们完全可以不用分词的，直接基于字的模型就可以做，比如基于字的文本分类、问答系统等，早已有人在研究。但是，即便这些模型能够成功，也会因为模型复杂而导致效率下降，因此，很多时候（尤其是生产环境中），我们会寻求更简单、更高效的方案。比如之前我们一直是word embedding，但是 sentence embedding 也是在学术界很流行的，最后没有大规模的采用，无非就是工业级要求更加高效的方式，有时候会牺牲一些精度。 目前很多的切词模块可以处理大部分的通用语料，然而有两类文本集仍然处理的不是很好，就是： 网络文档 领域文档 后者可以有对应的专家 handle，但涉及到商用这种人力成本也是比较高的了，所以一般使用基础词汇+ 各领域的常用词汇。即使这样的方案仍然是无法可持续的，所以需要一种算法去判断是否是新词。在当下的互联网时代，人们还会不断的创造出一些新词出来，比如：“神马”、“不明觉厉”等。未登录词辨别未登录词包括是种类繁多，形态组合各异，规模宏大的一个领域。对这些词语的自动辨识，是一件非常困难的事。比如，“美的”、“快的”、“英雄联盟”应该被作为一个词，却被切成了两个词，失去了原有的语义。未登录词（out-of-vocabulary, OOV）笼统地之未在词典中出现的词， 人工标注可以解决很好识别，比如最典型的未登录词就是人名，尤其是明星，然后最简单的是手动的维护，但是人力成本也是比较高昂的。 从分词的角度来看，新词一般表现为细粒度切分后相邻词的组合。 基于统计的新词发现 基于信息熵的新词发现算法，从左右信息熵和互信息入手，成词的标准有两个： 内部凝固度 自由运用程度 所谓内部凝固度，用来衡量词搭配（collocation）是否合理。比如，对于“的电影”、“电影院”这两个搭配，直观上讲“电影院”更为合理，即“电影”和“院”凝固得更紧一些。在计算语言学中，PMI (Pointwise mutual information)被用来度量词搭配与关联性，定义如下： $$p m i ( x , y ) = \log \frac { P ( x , y ) } { P ( x ) P ( y ) }$$ 若PMI高，即两个词共现（co-occurrence）的频率远大于两个词自由拼接的乘积概率，则说明这两个词搭配更为合理一些。针对一个词有多种搭配组合，比如“电影院”可以由“电影”+“院”构成，也可以由“电”+“影院”构成，那么取其所有pmi最小值（去掉log）作为内部凝固度：$$\operatorname { solid } \left( c _ { 1 } ^ { m } \right) = \min \frac { P \left( c _ { 1 } ^ { m } \right) } { \prod P \left( c _ { i } ^ { j } \right) } = \frac { P \left( c _ { 1 } ^ { m } \right) } { \max \prod P \left( c _ { i } ^ { j } \right) }$$ 其中， $c _ { 1 } ^ { m } = c _ { 1 } c _ { 2 } \cdots c _ { m }$表示长度为 $m$ 的字符串，$P \left( c _ { 1 } ^ { m } \right)$ 表示$c _ { 1 } ^ { m }$ 的频率。 光看文本片段内部的凝合程度还不够，我们还需要从整体来看它在外部的表现。考虑“被子”和“辈子”这两个片段。我们可以说“买被子”、“盖被子”、“进被子”、“好被子”、“这被子”等，在“被子”前面加各种字；但“辈子”的用法却非常固定，除了“一辈子”、“这辈子”、“上辈子”、“下辈子”，基本上“辈子”前面不能加别的字了。“辈子”这个文本片段左边可以出现的字太有限，以至于直觉上我们可能会认为，“辈子”并不单独成词，真正成词的其实是“一辈子”、“这辈子”之类的整体。 先简单的介绍熵的概念，熵是一种表示信息量的指标，熵越高就意味着信息含量越大，不确定性越高，越难以预测，信息也就越丰富。$$H ( X ) = - \sum _ { x \in X } p ( x ) \log _ { 2 } p ( x )$$ 所以，提出了自由运用程度，用以衡量一个词的左邻字与右邻字的丰富程度。正好信息熵可以完美地诠释了这种丰富程度，熵越大则丰富程度越高。“被子”和“辈子”这两个片段的左邻字熵le与右邻字熵re分别如下 le(被子) = 3.67453re(被子) = 3.8740le(辈子) = 1.25963re(辈子) = 4.11644 可以看出，“被子”的左邻字熵与右邻字熵都较高，而“辈子”的左邻字熵较小，即左邻字非常贫乏。因此，“被子”较“辈子”更有可能成词。自由运用程度的定义如下： 给频数、内部凝固度与自由运用程度设定一个阈值，提取出来符合阈值的候选词，去掉词典中存在的词即为新词了。所以两者都高于某个对应的阈值，那么说明这个是一个新词。 实现： 使用射雕英雄传txt 作为文本，然后词频、内部凝固度 和自由程度进行新词识别。代码。 词频这点很好理解,因为不是词的话出现的频率一般比较低,词的出现频率会比较高.所以可以设置一个词频阀值,高于这个阀值的判断为词,否则判定为不是词. 凝固度 基于上一步词频的挑选。 自由度 基于上述分词进行挑选。 NLP 中的三类特征提取器 NLP 和图像中数据的特征 NLP的输入往往是一句话或者一篇文章，所以它有几个特点：首先，输入是个一维线性序列，这个好理解；其次，输入是不定长的，有的长有的短，而这点其实对于模型处理起来也会增加一些小麻烦；再次，单词或者子句的相对位置关系很重要，两个单词位置互换可能导致完全不同的意思。 NLP 中的四大任务 序列标注： 分词/ POS Tag /NER /语义标注 分类任务： 文本分类/ 情感计算 句子关系判断： Entailment /QA / 自然语言推理 生成式任务： 机器翻译/ 文本摘录 一类是序列标注，这是最典型的NLP任务，比如中文分词，词性标注，命名实体识别，语义角色标注等都可以归入这一类问题，它的特点是句子中每个单词要求模型根据上下文都要给出一个分类类别。第二类是分类任务，比如我们常见的文本分类，情感计算等都可以归入这一类。它的特点是不管文章有多长，总体给出一个分类类别即可。第三类任务是句子关系判断，比如Entailment，QA，语义改写，自然语言推理等任务都是这个模式，它的特点是给定两个句子，模型判断出两个句子是否具备某种语义关系；第四类是生成式任务，比如机器翻译，文本摘要，写诗造句，看图说话等都属于这一类。它的特点是输入文本内容后，需要自主生成另外一段文字。 深度学习最大的优点是 “端到端” NLP中的任务很多，哪些任务是最具有代表性的呢？答案是机器翻译。 回归主题，特征提取器 RNNRNN模型结构参考上图，核心是每个输入对应隐层节点，而隐层节点之间形成了线性序列，信息由前向后在隐层之间逐步向后传递。 RNN也存在问题，它采取线性序列结构不断从前往后收集输入信息，但这种线性序列结构在反向传播的时候存在优化困难问题，因为反向传播路径太长，容易导致严重的梯度消失或梯度爆炸问题。为了解决这个问题，后来引入了LSTM和GRU模型，通过增加中间状态信息直接向后传播，以此缓解梯度消失问题，获得了很好的效果，于是很快LSTM和GRU成为RNN的标准模型。 （Attention机制最早是在视觉图像领域提出来的，但是真正火起来应该算是2014年google mind团队的论文《Recurrent Models of Visual Attention》，他们在RNN模型上使用了attention机制来进行图像分类） 为什么RNN能够这么快在NLP流行并且占据了主导地位呢？主要原因还是因为RNN的结构天然适配解决NLP的问题，NLP的输入往往是个不定长的线性序列句子，而RNN本身结构就是个可以接纳不定长输入的由前向后进行信息线性传导的网络结构，而在LSTM引入三个门后，对于捕获长距离特征也是非常有效的。所以RNN特别适合NLP这种线形序列应用场景，这是RNN为何在NLP界如此流行的根本原因。 那么为什么有衰弱了？RNN本身的序列依赖结构对于大规模并行计算来说相当之不友好。通俗点说，就是RNN很难具备高效的并行计算能力。如果适合在学术界发论文，那么不太可能在工业界广泛的使用。 CNN 特征提取器卷积层本质上是个特征抽取层，可以设定超参数F来指定卷积层包含多少个卷积核（Filter）。对于某个Filter来说，可以想象有一个d*k大小的移动窗口从输入矩阵的第一个字开始不断往后移动，其中k是Filter指定的窗口大小，d是Word Embedding长度。对于某个时刻的窗口，通过神经网络的非线性变换，将这个窗口内的输入值转换为某个特征值，随着窗口不断往后移动，这个Filter对应的特征值不断产生，形成这个Filter的特征向量。这就是卷积核抽取特征的过程。卷积层内每个Filter都如此操作，就形成了不同的特征序列。Pooling 层则对Filter的特征进行降维操作，形成最终的特征。一般在Pooling层之后连接全联接层神经网络，形成最后的分类过程。 CNN 捕捉到的是什么信息？ 关键在于卷积核覆盖的那个滑动窗口，CNN能捕获到的特征基本都体现在这个滑动窗口里了。大小为k的滑动窗口轻轻的穿过句子的一个个单词，荡起阵阵涟漪，那么它捕获了什么?其实它捕获到的是单词的k-gram片段信息，这些k-gram片段就是CNN捕获到的特征，k的大小决定了能捕获多远距离的特征。 卷积操作是通过加深层数，然后获得远距离的特征的。所以有两种解题思路： 一种是增加窗口大小k 增大；一种是加深深度。 简单谈一下CNN的位置编码问题和并行计算能力问题。CNN的卷积层其实是保留了相对位置信息的，只要你在设计模型的时候别手贱，中间层不要随手瞎插入Pooling层，问题就不大，不专门在输入部分对position进行编码也行。至于CNN的并行计算能力，那是非常强的，这其实很好理解。我们考虑单层卷积层，首先对于某个卷积核来说，每个滑动窗口位置之间没有依赖关系，所以完全可以并行计算；另外，不同的卷积核之间也没什么相互影响，所以也可以并行计算。CNN的并行度是非常自由也非常高的，这是CNN的一个非常好的优点。 Transformer看看 Transformer 对于NLP 任务中的解决方案： 不定长的输入：Transformer 一般设置最大的长度，不够了 就padding，然后多了就 去尾。 单词之间的相对位置： Transformer是用位置函数来进行位置编码的，而Bert等模型则给每个单词一个Position embedding，将单词embedding和单词对应的position embedding加起来形成单词的输入embedding 长依赖问题： self attention机制 对于Transformer来说，Multi-head attention的head数量严重影响NLP任务中Long-range特征捕获能力：结论是head越多越有利于捕获long-range特征。（对标filter 的个数） 我个人意见是：这说明Transformer之所以能够效果这么好，不仅仅multi-head attention在发生作用，而是几乎所有构件都在共同发挥作用，是一个小小的系统工程。 参考文献 放弃幻想，全面拥抱Transformer BLEUBLEU (bilingual evaluation understudy) 读音:波勒 not blue. 完美匹配的得分为1.0，而完全不匹配则得分为0.0。 虽然是为机器翻译提出，但后来也广泛用于NLP其他领域。 原理介绍下面三个概念： N-gram， 惩罚因子和Bleu 算法。 N-gram N-gram是一种统计语言模型，该模型可以将一句话表示n个连续的单词序列，利用上下文中相邻词间的搭配信息，计算出句子的概率，从而判断一句话是否通顺。 (和 fasttext 中的使用是不太一样的) 通过例子学习 原文： 猫坐在垫子上机器翻译：The cat sat on the mat.人工翻译：The cat is on the mat. 1 gram 匹配度是 5/6 同理可以得到 2 gram, 3 gram, 4 gram 分别是 3/5, 1/4 和 0. 处理一些特殊的情况： 原文：猫坐在垫子上机器译文： the the the the the the the.参考译文：The cat is on the mat. 所以当 1 gram 的时候，匹配度是 7/7，不合理的。 BLEU修正了这个算法，提出取机器翻译译文N-gram的出现次数和参考译文中N-gram最大出现次数中的最小值的算法，具体如下： $$Count_{clip} =min(Count, Max_Ref_Count)$$ 那么修正之后的 1 gram 的匹配度就是 2/7 论文中 N-gram 的计算公式： $$P_n =\frac{ \sum_{C \in {Candidates} } \sum_{n-gram \in C} Count_{clip}(n-gram)}{\sum_{C \in {Candidates} } \sum_{n-gram \in C^{‘}} Count_{clip}(n-gram^{‘})} $$ 分子表示翻译译文（机器翻译），然后分母表示参考译文（人工翻译） 惩罚因子 如果出现了这样的情况： 机器译文：The cat参考译文：The cat is on the mat. 如果出现这种短句子，你会发现计算n-gram的精度会得很高分，很显然这次的得分为1，但实际上它的得分应该是比较低的。针对翻译译文长度比参考译文要短的情况，就需要一个惩罚的机制去控制。BP 主要是处理句子长度的问题。 $$ BP = \begin{cases}1 &amp; c &gt;r \\e ^ { ( 1 - r / c ) } &amp; c &lt;= r\end{cases}$$ 这里的c是机器译文的词数，r是参考译文的词数 所以计算得：$ BP = e^(1- 6 / 2) = 7.38905609893065 $ BLEU 算法 $$\mathrm { B } \mathrm { LEU } = \mathrm { BP } \cdot \exp \left( \sum _ { n = 1 } ^ { N } w _ { n } \log p _ { n } \right)$$ 对于BP 已经知道，后面的其实就是一些数学运算，它的作用就是让各阶n-gram取权重服从均匀分布，就是说不管是1-gram、2-gram、3-gram还是4-gram它们的作用都是同等重要的。由于随着n-gram的增大，总体的精度得分是呈指数下降的，所以一般N-gram最多取到4-gram. 计算 第一步：计算各阶n-gram的精度第二步：加权求和 (一般就是平均)第三步：求BP最后求BLEU. 优缺点优点： 计算速度快；容易理解；已经被广泛使用 缺点： 短译句的测评精度有时会较高 它没有考虑句子意义 参考文献机器翻译质量评测算法-BLEU]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>关键词提取</tag>
        <tag>新词发现</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python中的多线程和多进程]]></title>
    <url>%2F2019%2F03%2F25%2FPython%E4%B8%AD%E7%9A%84%E5%A4%9A%E7%BA%BF%E7%A8%8B%E5%92%8C%E5%A4%9A%E8%BF%9B%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[多线程和多进程问题是可以对应到 并发 （cncurrency）和并行(parallelism)上的。 并发，就是一个单核cpu同时开始了多个任务，但是这个任务并不是同时独立进行的，而是通过cpu的不断切换，保存现场，然后重启这样的快速的切换，给用户的感觉是并发，但是实际上是cpu的计算能力受到了限制，用户体验比较好一些。如果在多核cpu （比如我的mac 是一cpu 6核）这样的话完全是可以达到并行的，这个是真正的独立操作(parallelism)，对应的是多进程的。对应python 中的实现多线程是使用threading，处理的是io 响应；多进程是Concurrency，使用multiprocessing包，处理的是多核cpu的操作。 Take off: 如果处理io 响应，那么使用多线程；如果是计算，那么使用多进程。 So, before we go deeper into the multiprocessing module, it’s worthwhile ensuring you know the advantages of using multiprocessing over multithreading. The general rule of thumb is that, if you are trying to improve the performance of CPU-bound tasks, multiprocessing is what you want to use. However, if your particular task is Input/Output bound, then you’ll generally want to use multithreading to improve the performance of your applications. 这个是多线程的demo 响应的io 请求。 1234567891011121314151617181920212223242526272829import threadingclass Worker(threading.Thread): # Our workers constructor, note the super() method which is vital if we want this # to function properly def __init__(self): super(Worker, self).__init__() def run(self): for i in range(10): print(i)def main(): thread1 = Worker() thread1.start() thread2 = Worker() thread2.start() thread3 = Worker() thread3.start() thread4 = Worker() thread4.start()if __name__ == "__main__": main() 下面是多进程的demo响应的计算请求。 123456789101112131415161718import multiprocessing as mpdef my_func(x): print(mp.current_process()) return x ** xdef main(): pool = mp.Pool(mp.cpu_count()) # 这个还是很好的 pool 这个的个数和你的cpu count 是保持一致的 result = pool.map(my_func, [4, 2, 3, 5, 3, 2, 1, 2]) result_set_2 = pool.map(my_func, [4, 6, 5, 4, 6, 3, 23, 4, 6]) print(result) print(result_set_2)if __name__ == "__main__": main()]]></content>
      <categories>
        <category>Python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Data Pre-processing 学习笔记]]></title>
    <url>%2F2019%2F03%2F25%2FData-Pre-processing%2F</url>
    <content type="text"><![CDATA[主要介绍机器学习中的数据预处理，包括 data cleaning、data integration、 data transformation、data reduction、data imbalanced 和一些概念。 Data Cleaning:这个步骤主要处理 missing values 和 noisy data (outlier).对于missing values ，可以分成两个问题，要不要处理 和如何处理，具体说来有一下处理手段： ignore the tuple; fill in the missing value manually use a global constant to fill in the missing value use the attribute mean to fill in the missing value (均值) use the most probable value to fill in the missing value (mode 众数) 有时候就是根据某几个特征然后弄一个简单的回归模型，根据模型进行predict 关于这几种方法如何去选择，我如果说 “it depends”，那么其他人不认为这是一个具有说服力的答案，他们更像知道 it depends what, and when and why to use specific method? 我认为应该是根据缺省值程度和重要性进行经验性的选择，这也去就是 empirical study吧。 接着是 noisy data (outlier)，我的观点是首先得认识到这个是错误的数据，不是真实的数据来源，可能是来自人为的笔误 或者仪器记录的问题，这个是需要修改的。可以使用聚类 (clustering) 进行noisy data 的检测，找到之后这个就类似 missing value了，可以采取以上的手段进行操作，应该注意到的这个 noisy data 所占比例不会很高，否则就成了主要的数据分布了。 Data Integration:处理数据库数据，经常是需要处理子表信息的，那么必然存在着主表，而子表系信息往往是主表信息的某一方面的细化。所以有必要将两者连接起来。 Data Transformation:In data transformation, the data are transformed or consolidated into forms appropriate for mining.这里想要澄清的是很多相同的内容都可以用不同的方式表达，并且可以放在数据处理的不同阶段，并且这种工作不是一次性完成的，而是迭代的 until you run out your patience and time.首先我接触的最常见的就是 discrete variables -&gt; continuous variables. 当然对于 discrete variables，基于树结构的机器学习模型是可以处理的，这里想说的是有这种方式。这种 transformation 常见的处理方式: one-hot 或者 label encoding. 如果按照 data transformation的预设，那么 normalization 就也属于该模块的内容。 不论是在 machine learning 还是在 图像处理的时候，对于原始的数据经常采取 normalization. 一方面这个是可以预防梯度消失 或者 gradient exploding, 如果你采用了 Sigmoid的激活函数的话。另一方面我认为更加重要的原因是将 不同的数据放在了同一个尺度下，如果你采取了 normalization之后。 Data Reduction:一般来说很少提及到到 data reduction的必要性，如果非要给出原因，那么可以从时间和空间的角度进行考虑。更加需要关注的是如何做的问题。 我的理解reduction 可以从两个维度进行考虑，假设一个 matrics A 是 m*n，这个是一个二维的矩阵，那么可以从 行列两方面入手。映射到机器学习中一般这样描述 从dimension 和 data两个角度去描述，分别称之为 dimension reduction 和 data compression. 前者指的是特征的选取，后者是数据size的减少。dimension reduction: where irrelevant, weakly relevant, or redundant attributes or dimensions may be detected and removed.data compression: PCA 线性降维 to reduce the data set size. 这个是针对某一个特征展开的。 why normalization？映射到 N(0,1) 的这种行为，叫做归一化。Feature scaling is the method to limit the range of variables so that they can be compared on common grounds. 有三个主要的原因。 Because most of the Machine Learning models are based on Euclidean Distance. Age- 40 and 27Salary- 72000 and 48000 这两个特征，这两个距离相差很大；但是这个并不是我们想要的，我们想要的是相对值，而不是绝对值。 即使最后的loss function不是 euclidean distance，比如说decision tree，实践证明经过正则化的之后的数据的训练速度是快于 没有经过正则化的数据的。 经过归一化之后，数据是不容易出现梯度消失或者梯度爆炸的。 很多模型的基本假设 就是 N(0,1) 高斯分布。 实现的三种手段： rescaling (min-max normalization) $$x ^ { \prime } = \frac { x - \min ( x ) } { \max ( x ) - \min ( x ) }$$ mean normalization $$x ^ { \prime } = \frac { x - \operatorname { average } ( x ) } { \max ( x ) - \min ( x ) }$$ standardization $$x ^ { \prime } = \frac { x - \overline { x } } { \sigma }$$ data imbalanced这个举例子应该很好举，使用 auc 的理由也应该是会的。 机器学习中的特征工程是有一定技巧可言，其中我觉得最为有趣的是: generation or you can call it abstraction. 对于特征的泛的提取才是对于问题本身或者特征的理解，这不仅需要积累，更需要对于该问题领域的专业知识， that’s all.举个栗子，在 “Home Credit Default Risk” (kaggle 竞赛)中，原始的训练数据有信贷金额和客户的年收入，这个时候 “credit_income_percent” 就是类似这种性质的提取特征。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Dimension Reduction]]></title>
    <url>%2F2019%2F03%2F25%2FDimension-Reduction%2F</url>
    <content type="text"><![CDATA[对于dimension reduction最近有了新的理解，介绍一下降维的概念和相关的分类，最后说一下无监督中的knn 和k-means及相关的差别。 降维概念广义上将降维就是使用更少的数据 (bits) 却保存了尽可能多的信息。You can use this concept to reduce the number of features in your dataset without having to lose much information and keep (or improve) the model’s performance. 不必纠结于采用降维的必要性，直接进入 techniques of dimension reduction. low variances这个是针对一个特征内部的，如果一个特征的数据本身没有什么变化，那么这个类似就是一种“死”数据。 high correlation filter用来判别特征 x 和最后的 target之间的相关性 常见的降维手段 principal component analysis (PCA)our old good friend. 如果你提降维，但是你不知道PCA，那么就说不过去。该方法的基本思路：一个基（向量空间）的变换，使得变换后的数据有着最大的方差。It works on a condition that while the data in a higher dimensional space is mapped to data in a lower dimension space, the variance of the data in the lower dimensional space should be maximum. 下面是PCA的一些特点： A principal component is a linear combination of the original variablesPrincipal components are extracted in such a way that the first principal component explains maximum variance in the datasetSecond principal component tries to explain the remaining variance in the dataset and is uncorrelated to the first principal componentThird principal component tries to explain the variance which is not explained by the first two principal components and so on 主成分是不断生成的，在前者基础之上生成的。 The first component is the most important one, followed by the second, then the third, and so on. Singular Value Decomposition (SVD) 翻译成中文感觉还是挺别扭的，奇异值分解。关于奇异值，特征值这些数学概念打算另外写一个主题，wait a moment. 简单理解PCA 是针对方阵 (mm), SVD是针对矩阵(m n)，所以后者是具有更大的适用范围。 Independent Component Analysis (ICA) 这个是在面试的时候被问道的一种降维方法。抓住独立向量应该就没有问题。 Independent Component Analysis (ICA) is based on information-theory and is also one of the most widely used dimensionality reduction techniques. The major difference between PCA and ICA is that PCA looks for uncorrelated factors while ICA looks for independent factors. pca 和 ica 的差别在于，相关性和独立性的差别。 基本假设： This algorithm assumes that the given variables are linear mixtures of some unknown latent variables. It also assumes that these latent variables are mutually independent, i.e., they are not dependent on other variables and hence they are called the independent components of the observed data. ICA 和PCA的异同：从线性代数的角度去理解，PCA和ICA都是要找到一组基，这组基张成一个特征空间，数据的处理就都需要映射到新空间中去。ICA相比与PCA更能刻画变量的随机统计特性，且能抑制高斯噪声。 T-SNE 就是指出 t-SNE 这个可以非线性降维。有local approaches 和 global approaches 两种方式，我大概的理解就是：类之间还能尽可能的远离，类内保持差异性。这个使用场景是在可视化中，经常会看见将数据或者 So far we have learned that PCA is a good choice for dimensionality reduction and visualization for datasets with a large number of variables. But what if we could use something more advanced? What if we can easily search for patterns in a non-linear way? t-SNE is one such technique. There are mainly two types of approaches we can use to map the data points:Local approaches : They maps nearby points on the manifold to nearby points in the low dimensional representation.Global approaches : They attempt to preserve geometry at all scales, i.e. mapping nearby points on manifold to nearby points in low dimensional representation as well as far away points to far away points. 下面介绍两种不是那么“常规”，但是也符合”dimension reduction” 定义的方式。 projection By projecting one vector onto the other, dimensionality can be reduced. autoencoder 网络结构通常有 encoder和decoder两部分组成，那么encoder 就作为 information abstraction,而 decoder作为一种重新映射。从这个角度NLP中的词向量也是可以是一种降维手段。 kmeans &amp; knn简而言之，knn 是有监督分类学习；kmeans 是无监督聚类。 In short, the algorithms are trying to accomplish different goals. K-nearest neighbor is a subset of supervised learning classification (or regression) algorithms (it takes a bunch of labeled points and uses them to learn how to label other points). It is supervised because you are trying to classify a point based on the known classification of other points. In contrast, K-means is a subset of unsupervised learning clustering algorithms (it takes a bunch of unlabeled points and tries to group them into clusters). It is unsupervised because the points have no external classification. The $ k $ in each case mean different things. In K-NN, the $ k $ represents the number of neighbors who have a vote in determining a new player’s position. The $ k $ in K-means, determine the number of clusters we want to end up. In a K-NN algorithm, a test sample is given as the class of majority of its nearest neighbours. For example, if we have three classes and the goal is to find a class label for the unknown example $ x_j $ then, by using the Euclidean distance and a value of $ k=5 $ neighbors, the unknown sample is classified to the category of the most voted neighbors. How it works?Step 1: Determine the value for KStep 2: Calculate the distances between the new input (test data) and all the training data. The most commonly used metrics for calculating distance are Euclidean, Manhattan and MinkowskiStep 3: Sort the distance and determine k nearest neighbors based on minimum distance valuesStep 4: Analyze the category of those neighbors and assign the category for the test data based on majority voteStep 5: Return the predicted class The situation with K-means is that, given some data you will cluster them in k-groups or clusters. The initial step of the algorithm is to randomly spawn $ k $ centroids (centers). At every iteration the center of each cluster is moved slightly to minimize the objective function. The algorithm will terminate if the iterations are maximized or if the centroids stop to move. The objective function of K-means is $ J = \sum_{j=1}^{k}\sum_{i=1}^{n}\left |x_i^{j}-c_j \right |^{2} $ How it works?Step 1: Determine K value by Elbow method and specify the number of clusters KStep 2: Randomly assign each data point to a clusterStep 3: Determine the cluster centroid coordinatesStep 4: Determine the distances of each data point to the centroids and re-assign each point to the closest cluster centroid based upon minimum distanceStep 5: Calculate cluster centroids againStep 6: Repeat steps 4 and 5 until we reach global optima where no improvements are possible and no switching of data points from one cluster to other.]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>knn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linear Algebra in ML]]></title>
    <url>%2F2019%2F03%2F25%2FLinear-Algebra-in-ML%2F</url>
    <content type="text"><![CDATA[我觉得到 ML 中的一个难点：就是由原来简单的 linear equations 直接过渡到了 matrics and vectors。这个过程是没有人跟你说的。网络结构可以当作是一个complicated 并且是无法表示的函数，于是很多使用者把它当作黑匣子，关心于输入和输出，中间过程 don’t care. 变量（特征个数）和解的关系多变量和最后target的关系是可以使用 matrices 进行表示的，这就是一种数学公式化。 Broadly speaking, in linear algebra data is represented in the form of linear equations. These linear equations are in turn represented in the form of matrices and vectors. 先直观的感受一下变量和图形（可视化）的关系。两个变量组成的equations 是两条线的相交情况。而三个变量在空间中有三种情况： 相交，平行，不在一个平面上。三个变量组成的equations 是三个面的相交情况。有四种情况 (try hard to figure it out)：No intersection at all.Planes intersect in a line.They can intersect in a plane.All the three planes intersect at a point. 当到达4 dims 的时候，it’s impossible to visulize it. terms in related to matrix这些词汇 (terms) 经常在文献中出现，需要对于其含义有个比较好的认识。Order of matrix – If a matrix has 3 rows and 4 columns, order of the matrix is 34 i.e. rowcolumn. (翻译成 矩阵的阶)Square matrix – The matrix in which the number of rows is equal to the number of columns.Diagonal matrix – A matrix with all the non-diagonal elements equal to 0 is called a diagonal matrix.Upper triangular matrix – Square matrix with all the elements below diagonal equal to 0.Lower triangular matrix – Square matrix with all the elements above the diagonal equal to 0.Scalar matrix – Square matrix with all the diagonal elements equal to some constant k.Identity matrix – Square matrix with all the diagonal elements equal to 1 and all the non-diagonal elements equal to 0.Column matrix – The matrix which consists of only 1 column. Sometimes, it is used to represent a vector.Row matrix – A matrix consisting only of row.Trace – It is the sum of all the diagonal elements of a square matrix.Rank of a matrix – Rank of a matrix is equal to the maximum number of linearly independent row vectors in a matrix.Determinant of a matrix - 矩阵的行列式转置 -在图形 matrix中还是很常见的。$$\mathrm { A } _ { \mathrm { ij } } ^ { \mathrm{T}} = \mathrm { A } _ { \mathrm { ji } }$$ 这个矩阵乘法和元素相称的区别，后者是element-wise 进行的。可以从另外一个角度去列及矩阵相称： This operation on a vector is called linear transformation. 就是后面的vector 映射到了前面的矩阵空间。 特征值和奇异值着两个是分别对应着PCA 和SVD。Eigenvalues and Eigenvectors如公式所示，特征值和特征向量的乘积就是方阵和特征向量的乘积，原先的方阵是可以降维表示成特征向量和特征值的。$ A x = \lambda x $ 对于奇异值分解，最常见的就是这种表达：$A = U \Sigma V ^ { T }$特征值分解和奇异值分解都是给一个矩阵(线性变换)找一组特殊的基，特征值分解找到了特征向量这组基，在这组基下该线性变换只有缩放效果。而奇异值分解则是找到另一组基，这组基下线性变换的旋转、缩放、投影三种功能独立地展示出来了。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Back to my blog]]></title>
    <url>%2F2019%2F03%2F07%2FBack-to-my-blog%2F</url>
    <content type="text"><![CDATA[直到某一天发现个人网站中的图片都显示不出来了，查了一下才发现之前的图床不能用了（点名批评七牛），果断弃之，换了个大厂子产品。证明一下图片是能出来的。ps：之前的图片有时间再整理到新的平台上。]]></content>
      <categories>
        <category>人间不值得</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[基于simhash的大文本相似度比较]]></title>
    <url>%2F2018%2F08%2F23%2F%E5%9F%BA%E4%BA%8Esimhash%E7%9A%84%E6%96%87%E6%9C%AC%E7%9B%B8%E4%BC%BC%E5%BA%A6%E6%AF%94%E8%BE%83%2F</url>
    <content type="text"><![CDATA[本文主要记录使用simhash比较中文大文本的相似度问题。先说一下文本特征，数据属于中文文本，每篇文章的字数大于500,小于2000,基本上属于大文本。步骤如下： 基于tf-idf提取文本的关键词。如果这些关键词在之后的比较中是相同的，那么认为对应的文章也是相同。简而言之，这些提取的关键词可以看做原文章的”代表”. 根据关键字计算simhash编码，然后使用hamming distance进行比较两者的不同。如果对于上述概念比较模糊，建议首先阅读该篇博客。 实战顺滑过渡到代码实现：123456789101112131415# 常规导包import sys,codecsimport pandas as pdimport numpy as npimport jieba.possegimport jieba.analysefrom sklearn import feature_extractionfrom sklearn.feature_extraction.text import TfidfTransformerfrom sklearn.feature_extraction.text import CountVectorizer# 数据集的路径path =&quot;../tianmao2.csv&quot;names =[&apos;where&apos;, &apos;time&apos;, &apos;title&apos;, &apos;url&apos;, &apos;contents&apos;]data =pd.read_csv(path, delimiter=&apos;\t&apos;, names= names, nrows=200)data[&apos;id&apos;] =data.index+1data.head() 我们使用title和contents 组合作为原始处理的数据，我们认为该数据能够就是文章的内容。1stopkey = [w.strip() for w in codecs.open(&apos;../keyword_extraction/data/stopWord.txt&apos;, &apos;r&apos;).readlines()] 该stop words是中文停用词，就是常见的”的 了”。常见的有百度停用词表、哈尔滨工业大学停用词表以及中科院的停用词表。这里使用的是中科院的停用词。对于停用词的存储，可以使用set ，因为set 要比 list的检索要快。12345678def dataPrepos(text, stopkey): l = [] pos = [&apos;n&apos;, &apos;nz&apos;, &apos;v&apos;, &apos;vd&apos;, &apos;vn&apos;, &apos;l&apos;, &apos;a&apos;, &apos;d&apos;] # 定义选取的词性 seg = jieba.posseg.cut(text) # 分词 for i in seg: if i.word not in stopkey and i.flag in pos: # 去停用词 + 词性筛选 l.append(i.word) return l 我们选择名词作为主要的分析对象。12345678idList, titleList, abstractList = data[&apos;id&apos;], data[&apos;title&apos;], data[&apos;contents&apos;]corpus = [] # 将所有文档输出到一个list中，一行就是一个文档# 这个 虽然使用 &quot; &quot; 进行分割 但是实际上还是一个打的listfor index in range(len(idList)): text = &apos;%s。%s&apos; % (titleList[index], abstractList[index]) # 拼接标题和摘要 text = dataPrepos(text, stopkey) # 文本预处理 text = &quot; &quot;.join(text) # 连接成字符串，空格分隔 corpus.append(text) 这里的corus 是将所有的经过预处理文档作为当前计算 idf 的语料库。123456789vectorizer = CountVectorizer()X = vectorizer.fit_transform(corpus) # 词频矩阵,a[i][j]:表示j词在第i个文本中的词频# 2、统计每个词的tf-idf权值transformer = TfidfTransformer()tfidf = transformer.fit_transform(X)# 3、获取词袋模型中的关键词word = vectorizer.get_feature_names()# 4、获取tf-idf矩阵，a[i][j]表示j词在i篇文本中的tf-idf权重weight = tfidf.toarray() 使用sklearn 内置的函数计算tf-idf。1234567891011121314151617181920212223242526272829topK = 10ids, titles, keys, weights = [], [], [], []for i in range(len(weight)): print(&quot;-------这里输出第&quot;, i + 1, &quot;篇文本的词语tf-idf------&quot;) ids.append(idList[i]) titles.append(titleList[i]) df_word, df_weight = [], [] # 当前文章的所有词汇列表、词汇对应权重列表 for j in range(len(word)): # print(word[j],weight[i][j]) df_word.append(word[j]) df_weight.append(weight[i][j]) df_word = pd.DataFrame(df_word, columns=[&apos;word&apos;]) df_weight = pd.DataFrame(df_weight, columns=[&apos;weight&apos;]) word_weight = pd.concat([df_word, df_weight], axis=1) # 拼接词汇列表和权重列表 word_weight = word_weight.sort_values(by=&quot;weight&quot;, ascending=False) # 按照权重值降序排列 # 在这里可以查看 k的选取的数值应该是多大， # from ipdb import set_trace # set_trace() keyword = np.array(word_weight[&apos;word&apos;]) # 选择词汇列并转成数组格式 word_split = [keyword[x] for x in range(0, topK)] # 抽取前topK个词汇作为关键词 word_split = &quot; &quot;.join(word_split) keys.append(word_split) wei = np.array(word_weight[&apos;weight&apos;]) wei_split = [str(wei[x]) for x in range(0, topK)] wei_split = &quot; &quot;.join(wei_split) weights.append(wei_split) # 这里的命名 容易混淆result = pd.DataFrame(&#123;&quot;id&quot;: ids, &quot;title&quot;: titles, &quot;key&quot;: keys, &apos;weight&apos;: weights&#125;, columns=[&apos;id&apos;, &apos;title&apos;, &apos;key&apos;, &apos;weight&apos;]) 选择前10个频率最高的词语作为该篇文章的代表，当然这个参数是可以调整，需要根据具体的问题和结果进行调整。1result.head() 最后的效果如上。至此我们第一步的提取文章的关键词就已经做完。下面进行相似度的比较。 1234import jiebaimport jieba.analyseimport pandas as pd#日常导包 数据和上述的一样，所以就不截图了。123datasets =pd.read_csv(&quot;../tianmao2-tf-idf.csv&quot;)tokens =datasets[&apos;key&apos;]weights =datasets[&apos;weight&apos;] 提取关键词和对应的权重。123456789101112print(tokens[0], len(tokens[0]))print(weights[0], len(weights[0]))tokens0 =tokens[0].split()weights0 =weights[0].split()len(tokens0)len(weights0)tokens1 =tokens[1].split()weights1 =weights[1].split()import astweights0 =[ ast.literal_eval(i) for i in weights0]weights1 =[ ast.literal_eval(i) for i in weights1] 构造测试用例。因为权重是字符串，所以简单处理转成整数。 12dict0 =dict(zip(tokens0, weights0))dict1 =dict(zip(tokens1, weights1)) 定义一个Simhash，提供对文档的数值映射和文档间相似度计算的功能.123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081class Simhash(object): # 初始化函数 def __init__(self, weights_dict, tokens=&apos;&apos;, hashbits=64): self.hashbits = hashbits self.hash = self.simhash_function(tokens, weights_dict) # toString函数 # 不懂这个 self.hash ，凡是带有self 的函数都是可以类变量，所以这个就是返回的 self.hash这个变量 #凡是使用__str__ 这种类型的函数 都是重写 原来的函数 def __str__(self): return str(self.hash) &quot;&quot;&quot; ord() 函数是 chr() 函数（对于8位的ASCII字符串）或 unichr() 函数（对于Unicode对象）的配对函数，它以一个字符（长度为1的字符串）作为参数，返回对应的 ASCII 数值，或者 Unicode 数值 &quot;&quot;&quot; # 给每一个单词生成对应的hash值 # 这个操作搞懂之后一定很简洁， 但是现在很难理解，因为不是字符串，而是位操作 def _string_hash(self, source): if source == &apos;&apos;: return 0 else: x = ord(source[0]) &lt;&lt; 7 # &lt;&lt; 表示 乘以2^7 ; &gt;&gt; 表示除以 ; ** 表示次方的意思 # ^ : 按位异或 (二进制进行异或)； &amp; 按位进行与 操作 # 左移位操作也是可以理解为 2^x 的操作，因为存储是二进制，这样左移一位 表示×2 一次 m = 1000003 mask = 2 ** self.hashbits - 1 for c in source: x = ((x * m) ^ ord(c)) &amp; mask x ^= len(source) if x == -1: x = -2 return x # 生成simhash值 def simhash_function(self, tokens, weights_dict): v = [0] * self.hashbits # 这种使用 &#123;&#125; dictionary 然后强行得到item 再进行遍历也是牛逼 for key, t in &#123;x: self._string_hash(x) for x in tokens&#125;.items(): for i in range(self.hashbits): bitmask = 1 &lt;&lt; i if t &amp; bitmask: v[i] += weights_dict[key] else: v[i] -= weights_dict[key] fingerprint = 0 for i in range(self.hashbits): if v[i] &gt;= 0: fingerprint += 1 &lt;&lt; i return fingerprint # 求文档间的海明距离 def hamming_distance(self, other): x = (self.hash ^ other.hash) &amp; ((1 &lt;&lt; self.hashbits) - 1 ) tot = 0 while x : tot += 1 x &amp;= x - 1 return tot #求相似度 # 这个相似度的计算，十分简单，如果两个数接近，那么就是认为相似。越是接近1 越是相似， # 不是原先那种以某一个参数整数 如3 为距离的相似度 def similarity(self, other): a = float(self.hash) b = float(other.hash) if a &gt; b: return b / a else: return a / b if __name__ == &apos;__main__&apos;: hash0 = Simhash(weights_dict=dict0, tokens=tokens0) print(hash0) hash1 = Simhash(weights_dict=dict1, tokens=tokens1) print(hash1) print(hash0.hamming_distance(hash1)) print(hash0.similarity(hash1)) 结果如上。可以看出该例子中使用的两两比较的方式，对于大数据来说，一般可能会用到倒排索引和cpu并行技术。 原理：步骤Simhash 分为5个步骤：分词、hash（md5 要求均匀映射到某空间就行，不要求反应原始样本的相关性）、加权、合并（列项相加）、降维（正数为1 负数为0），得到每篇文章的simhash 之后，计算两个文章的海明距离（两个字符串对应位置的不同字符的个数）。对于64 位的simhash 值，在3以内就可以认为是比较相似的。两张比较能说明问题的图片。 第一步：文本预处理得到分词（去重，去除的了stop words）,然后weight权重可以使用分词的frequency 或者tfidf 进行得到第二步：进行hash 映射（可以使用md5这种传统的映射方式，基本的要求就是均匀映射到一个空间，这种映射并不能反映原始样本的相关性）第三步：hash 映射值和weight 进行相乘，如果原来是1 则乘以1，如果是0 则乘以-1，第四步： 列向相加，得到summing weights,进行降维如果是正数那么为1，如果是负数那么为-1然后这个simhash就出来了之后进行的文本比较常常使用hamming distance，操作就是64位两两比较如果如果不同counter就+1。最后的就得到了两两文本的distance ，然后一般使用3，作为阈值，如果大于3，那么就认为两者是不相同的。 评价：simhash算法有缺点是：只考虑到文章存在哪些词，没有考虑到词的顺序。不过相应的优点是，可以实现海量文章相似度计算。文章相似度计算忽略词的顺序之后效果更好。当文本内容较长时，使用SimHash准确率很高，SimHash处理短文本内容准确率往往不能得到保证。文本内容中每个term对应的权重如何确定要根据实际的项目需求，一般是可以使用IDF权重来进行计算。 SimHash与传统hash函数的区别:传统的Hash算法只负责将原始内容尽量均匀随机地映射为一个签名值，原理上仅相当于伪随机数产生算法。如果原始内容在不相同，使用传统hash 得到的signature除了是不同，不能提供其他的信息；而simhash 能够在一定程度上反映这种不相同的程度，表征原内容的相似度。一个字节的差异和五个字节的差异得到的相似度是不一样的。普通hash是random 的完全没有这种信息。因为我们使用的simhash是局部敏感哈希，这个算法的特点是只要相似的字符串只有个别的位数是有差别变化。 补充材料：基于不同表示带来的不同的相似度比较的指标： jaccard 还是余弦距离，海明距离Jaccard系数：简单说就是交集/ 并集两个集合A和B交集元素的个数在A、B并集中所占的比例，称为这两个集合的杰卡德系数，用符号 J(A,B) 表示。杰卡德相似系数是衡量两个集合相似度的一种指标（余弦距离也可以用来衡量两个集合的相似度）。]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>文本相似度</tag>
        <tag>Simhash</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[文本相似度比较基本知识]]></title>
    <url>%2F2018%2F08%2F23%2F%E6%96%87%E6%9C%AC%E7%9B%B8%E4%BC%BC%E5%BA%A6%E6%AF%94%E8%BE%83%E5%9F%BA%E6%9C%AC%E7%9F%A5%E8%AF%86%2F</url>
    <content type="text"><![CDATA[本文介绍的SimHash是一种局部敏感hash，它也是Google公司进行海量网页去重使用的主要算法。 simhash对于文本去重而言，目前有很多NLP相关的算法可以在很高精度上来解决，但是我们现在处理的是大数据维度上的文本去重，这就对算法的效率有着很高的要求。但是在小的样本上这个是不一定有保证有效的。SimHash算法是Google公司进行海量网页去重的高效算法，它通过将原始的文本映射为64位的二进制数字串，然后通过比较二进制数字串的差异进而来表示原始文本内容的差异。本文服务于该篇博客,主要进行名词解释。 基本概念simhash 也是一种hash，一般的hash 函数映射规则只需要满足以下两个条件： 对很多不同的特征来说，它们对所对应的向量是均匀随机分布的 相同的特征来说对应的向量是唯一简单来说普通的hash映射需要满足随机分布和唯一性两个条件。simhash想要实现的是，如果原来的文本的特征是相似，那么映射之后的编码也是相似。这里使用 hamming distance 进行比较simhash映射之后的距离。根据经验值，对64位的 SimHash值，海明距离在3以内的可认为相似度比较高。编码之后的表示在英文中是 fingerprint(指纹)。simhash最初被google 用于网页去重，当时使用的fingerprint 是64,所以这里沿用了这个传统。64位的签名可以表示多达264个象限，因此只保存所在象限的信息也足够表征一个文档了。更进一步，表示的文档的数字最多是多少？这个应该可以准确计算特征的个数应为如果用三位(01) 表示，那么有8种，那么2^64 这么多种特征，所以16*10^18 这么多。算法步骤第一步：文本预处理得到分词（去重，去除的了stop words）,然后weight权重可以使用分词的frequency 或者tfidf 进行得到第二步：进行hash 映射（可以使用md5这种传统的映射方式，基本的要求就是均匀映射到一个空间，这种映射并不能反映原始样本的相关性）第三步：hash 映射值和weight 进行相乘，如果原来是1 则乘以1，如果是0 则乘以-1，第四步： 列向相加，得到summing weights,进行降维如果是正数那么为1，如果是负数那么为-1然后这个simhash就出来了.有图有真相 simhash的局限性：只考虑到文章存在哪些词，没有考虑到词的顺序。不过相应的优点是，可以实现海量文章相似度计算。文章相似度计算忽略词的顺序之后效果更好。所以在处理大文本时候，simhash是有效的，但是在处理小文本，这种效果往往不能被保证。直观上理解，在一片段文章或者段落中，词语出现的顺序还是比较重要的。 minhash可以参考该视频和这篇文章。 Locality Sensitive HashingLocality Sensitive Hashing(局部敏感哈希)作用就是从海量的数据中挖掘出相似的数据，可以具体应用到文本相似度检测、网页搜索等领域。上面的simhah和minhash 就是该思想的实现。 距离函数这里的距离函数都是用来文本相似度。 Jaccard相似度简单来说交集除以并集。这个集合中存放的是文章或者段落的关键词。1234567891011def JaccardSim(str_a, str_b): &apos;&apos;&apos; Jaccard相似性系数 计算sa和sb的相似度 len（sa &amp; sb）/ len（sa | sb） &apos;&apos;&apos; seta = splitWords(str_a)[1] setb = splitWords(str_b)[1] sa_sb = 1.0 * len(seta &amp; setb) / len(seta | setb) return sa_sb 可以看到核心代码很简单，经过分词之后，就是seta 和setb 进行的操作。 cosine12345def cos_sim(a, b): a = np.array(a) b = np.array(b) # return &#123;&quot;文本的余弦相似度:&quot;:np.sum(a*b) / (np.sqrt(np.sum(a ** 2)) * np.sqrt(np.sum(b ** 2)))&#125; return np.sum(a * b) / (np.sqrt(np.sum(a ** 2)) * np.sqrt(np.sum(b ** 2))) 将文本的关键词映射成某种高维函数，然后在高维空间中计算两者的相似度。 tf-idf在simhash 中使用 tf-idf作为我们的比较函数。TF-IDF的主要思想就是：如果某个词在一篇文档中出现的频率高，也即TF高；并且在语料库中其他文档中很少出现，即DF的低，也即IDF高，则认为这个词具有很好的类别区分能力。$$ TF-IDF = 词频(TF) x 逆文档频率(IDF) $$ 算法步骤： 计算词频$$ 词频(TF) = 某个词在文章中出现的次数( 频数) $$“标准化”$$ 词频( TF) = \frac{某次在文中出现的次数}{文章的总词语数} $$ 逆文档频率这个时候需要一个语料库 (corpus)，模拟语言环境$$ 逆文档频率 (IDF) = log(\frac{语料中的文档总数}{ 包含该词的文档数 +1}) $$ TF-IDF 优点是简单快速，比较符合实际。缺点，无法体现词的位置信息，所有的位置都是被认为重要性相同，但是开头结尾，段落的开头和段落的结尾，therefore，so，but这些词语都是没有体现的；还有一个缺点是，是基于统计的，没有表达出词语的语意信息 or context 上下文的信息。 Hamming distancehamming distance就是比较01串的不同，按照位进行比较。算法：异或时，只有在两个比较的位不同时其结果是1 ，否则结果为0，两个二进制“异或”后得到1的个数即为海明距离的大小。123456789101112131415161718hashbits =64 # 使用64位进行编码def simhash_function(tokens, weights_dict): v = [0] * hashbits # 这种 &#123;key: value&#125;.item() 的操作也是没有了谁了 for key, t in &#123;x: _string_hash(x) for x in tokens&#125;.items(): for i in range(hashbits): bitmask = 1 &lt;&lt; i if t &amp; bitmask: v[i] += weights_dict[key] else: v[i] -= weights_dict[key] fingerprint = 0 for i in range(hashbits): if v[i] &gt;= 0: fingerprint += 1 &lt;&lt; i return fingerprintfingerprint = simhash_function(tokens, weights) min edit distance123456789101112131415161718192021222324# 最小编辑距离def min_edit_distance(str1, str2): rows =len(str2) +1 cols =len(str1) +1 arr =[[0 for _ in range(cols)] for _ in range(rows)] # 这种简洁的代码也是牛逼 for j in range(cols): arr[0][j] =j for i in range(rows): arr[i][0] =i for i in range(1, rows): for j in range(1, cols): # 因为string 是从0 ，len(str) -1的 if str2[i-1] ==str1[j-1]: arr[i][j] =arr[i-1][j-1] else: # 以后见到这样的式子，就要想到这个二维的数组，因为这个是可以帮助记忆的 arr[i][j] =1 +min(arr[i-1][j-1], arr[i-1][j], arr[i][j-1]) # 右下角就是距离 return arr[rows-1][cols-1]str_a =&quot;abcdef&quot;str_b =&quot;azced&quot;result =min_edit_distance(str_a, str_b)result 具体可以参看该视频讲解。(ps. 如果刷leetcode,也可以参看该视频) 分词在英文中存在天然的空格可以进行分词操作，但是中文的分词就比较复杂了。常用的中文分词开源工具有 jieba和HanLP前者简单易行，容易上手；后者在自然语言处理作为汉语言处理包，可以用于词性标注，命名实体识别等一系列功能。常用的英文分词 corenlp 倒排索引倒排索引使用python在实现上就是一个dictionary 嵌套一个 set(). 一般的索引都是数字或者英文字母映射内容，具体在放到simhash的情景下就是使用文章的序列号对应提取出来的关键词。但是倒排索引就是关键词对应文章的序列号，类似与原来的”值”对应这”键”，所以称之为倒排索引。一般使用在召回的场景下，使用关键词然后出现了该关键词下的index 的集合。可以参考这篇文章。]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>Simhash</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Differences Between l1 and l2 as Loss Function and Regularization]]></title>
    <url>%2F2018%2F07%2F21%2Fdifferences-between-l1-and-l2-as-loss-function-and-regularization%2F</url>
    <content type="text"><![CDATA[本文主要介绍 L1 和L2 分别作为 loss function 和 regularization，最后谈一下 overfitting的事情。 L1 和L2 作为Loss function和 regularization，个人感觉是一个经常容易混淆的概念。但是如果读者觉得很清楚，那么就可以跳过了。本文大量借鉴于该博客，原文是英文，如果读者英文能够handle，建议读原文。 As loss functionloss function or error function 是用来衡量真实y 和生成的f(x) 之间差距的函数。在模型训练中我们一般情况下不断训练模型使得loss function不断下降（如果task要求loss function是增大，这时候一般加上符号或者转换成 1- loss fucntion，最后实现的还是loss function下降）。好的回到L1 loss function和L2 loss function. 概括：从鲁棒性（对待异常值）的角度看，L1 是比L2 具有更好的性质，因为L2 是把误差进行了平方处理，误差回放大；从稳定性角度（数据集的一个小的移动），L2 是比L1 具有更好的性质，这个是可以从实验的角度进行总结。 L1-norm loss function is also known as least absolute deviations(LAD), least absolute errors. It is basically minimizing the sum of the absolute differences between the target value Y and the estimated values (f(x)). $$S = \sum _ { i = 1 } ^ { n } \left| y _ { i } - f \left( x _ { i } \right) \right|$$ L2-norm loss function is also known as least squares error(LSE). It is basically minimizing the sum of the square of the differences between the target value Y and the estimated values (f(x)). $$S = \sum _ { i = 1 } ^ { n } \left( y _ { i } - f \left( x _ { i } \right) \right) ^ { 2 }$$ 比较L1-norm 和L2-norm在前两个评价指标中的表现： 对于是否具有唯一解，可以从这个角度分析：L1是可以有多种解，而L2只有一种解。 As regularization从XGBoost调参指南中我们知道objective function = loss funcion + regularization. 而我们大多数情况下提及的都是loss function,常常忽略了regularization 的作用。 The regularization term controls the complexity of the model, which helps us to avoid overfitting.对于模型训练，一开始的想法是尽量的overfitting, 因为就现在不成熟的经验而言，对于overfitting这个问题有很多处理方法，比如卷积深度神经网络中的dropout, LightGBM中的early stop 和随机采样的思想。 这些方法都是可以缓解overfitting，所以可以出现overfitting。相反，如果你的模型是underfitting，那么你就微显尴尬了。好，收回到L1 and L2。 总结： L1 在系数weights 中使用，L2 在非稀疏的情况下使用更好。可以从计算效率、是否稀疏输出和特征选择进行分析。 计算效率 稀疏结果 特征选择 L1 在稀疏解 上效率比较高，在非稀疏解上效率比较低 产生稀疏解 具有特征选择的功能 L2 在非稀疏解上效率高 不产生稀疏解 没有特征选择的功能 首先理解为什么要正则化？ 如果网络足够的强大，数据量不是那么充足，那么网络完全是可以通过“记住” 这些样本，然后得到很高的 training acc，但是这个 test error 会比较大，这个时候就出现了过拟合。正则化就类似减轻了数据的复杂程度。关于过拟合是可以从两个维度进行解决：网络和数据。 常常使用的 dropout 就是减弱网络复杂性的一种手段，随机减少了某些网络中的节点，那么网络就没有那么强的“记忆”功能，但是最后的loss（要求）还是不变的，所以只能去寻找数据更加“简单普遍”的规律；正则化是对数据和weights 两种类型的，前者是对于数据的操作，然后weights 更加倾向于对于网络结构的操作，使得系数更加“光滑”。 为什么L1 相比于L2 产生了更加稀疏的解？这里从数学角度和空间角度进行了解释。 但是从更加 tuition 的角度去理解，L1 没有一个连续的导数，能产生权值为0，所以类似剔除了某些特征，产生了稀疏的权值，而L2 是具有比较连续的导数，产生了比较平滑的权值。 更加数学化的理解： $$f ( x ) = \frac { 1 } { \sqrt { 2 \pi } \sigma } \exp \left( - \frac { ( x - \mu ) ^ { 2 } } { 2 \sigma ^ { 2 } } \right)$$如果把高斯密度函数取对数，那么就只会剩下一个平方项，这个就是L2正则项的来源，所以数据是比较稠密的。 同样，如果是数据比较稀疏，不妨假设来自 laplace 分布，下面是其中的 $u$ 表示位置参数， $b$ 表示尺度参数。这两个都是可以对标正太分布的。可以看到图像的尾部都是比较平滑，然后大多数是接近于0的。公式：$$f ( x | \mu , b ) = \frac { 1 } { 2 b } \exp \left( - \frac { | x - \mu | } { b } \right)$$同样取对数，那么得到是L1 正则项。 所以L2 得到的是一个比较平滑的weights 适合处理稠密的向量，而L1 得到是一个稀疏的结果，因为有很大程度的被置为0. 先上公式L1 regularization on least squares:$$\mathbf { w } ^ { * } = \underset { \mathbf { w } } { \arg \min } \sum _ { j } \left( t \left( \mathbf { x } _ { j } \right) - \sum _ { i } w _ { i } h _ { i } \left( \mathbf { x } _ { j } \right) \right) ^ { 2 }$$ L2 regularization on least squares: The difference between their properties can be promptly summarized as follows: 对于第一点computational efficient的理解：平方比绝对值更容易计算，平方可以求导直接求最值，但是绝对值就无法求导。并且L1 regularization在 non-sparse cases中是 computational inefficient，但是在 sparse(0比较多) cases中是有相应的稀疏算法来进行优化的，所以是computational efficient.对于第二点是否具有sparse solution可以从几何意义的角度解读：The green line(L2-norm) is the unique shortest path, while the red, blue yellow(L1-norm) are all same length for the same route. Built-in feature selection is frequently mentioned as a useful property of the L1-norm, which the L2-norm does not. This is actually a result of the L1-norm, which tends to produces sparse coefficients (explained below). Suppose the model have 100 coefficients but only 10 of them have non-zero coefficients, this is effectively saying that “the other 90 predictors are useless in predicting the target values”. L2-norm produces non-sparse coefficients, so does not have this property. 所以表格中第三点也是顺理成章的了。至此，我们区分了L1-norm vs L2-norm loss function 和L1-regularization vs L2-regularization，下面说一下 overfitting的东西。 overfitting现在，我们的训练优化算法是一个由两项内容组成的函数：一个是损失项，用于衡量模型与数据的拟合度，另一个是正则化项，用于衡量模型复杂度。 对于过拟合有两种解读方式，一种是模型是复杂，然后是去拟合了所有的数据，没有了泛化性能； 一种是从数据角度，模型去拟合了noise ，这些random的数据。然后从loss function的角度去优化的话，就是降低模型的复杂度，正则项就是降低模型复杂度的一种手段。所以从这个角度，L2 是降低模型复杂度的一种手段，也是一种减轻overfit的手段，这两者是相辅相成的。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>l1</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LightGBM和XGBoost及其调参]]></title>
    <url>%2F2018%2F07%2F21%2FLightGBM%E5%92%8CXGBoost%E5%8F%8A%E5%85%B6%E8%B0%83%E5%8F%82%2F</url>
    <content type="text"><![CDATA[先主要介绍树的基本知识，然后介绍LightGBM和XGBoost及其调参. 进入正文之前简单的说一下决策树。一棵树很容易过拟合或者欠拟合（根据树的深度），然后需要使用多棵树进行组合预测，而GBDT是实现这个手段的方式之一。sklearn中也是实现了 GBDT 这种思想，但是比较难用，训练速度跟不上。但是 lightGBM 和XGBoost 实现的效果就比较好。 lightGBM调参(常用参数)Since lightGBM is based on decision tree algorithms, it splits the tree with the best fit whereas boosting algorithms split the tree depth wise or level wise rather than leaf-wise. So when growing on the same leaf in Light GBM, the leaf-wise algorithm can reduce more loss than the level-wise algorithm and hence results in much better accuracy which can rarely be achieved by any of the existing boosting algorithms. Also, it is surprisingly very fast, hence the word ‘Light’. Leaf wise splits lead to increase in complexity and may lead to overfitting and it can be overcome by specifying another parameter max_depth parameter. Advantages of LightGBM faster training speed and higher efficiencyLight GBM use histogram based algorithm i.e it buckets continuous feature values into discrete bins which fasten the training procedure. lower memory usageReplaces continuous values to discrete bins which result in lower memory usage. better accuracy than any other boosting algorithmIt produces much more complex trees by following leaf wise split approach rather than a level-wise approach which is the main factor in achieving higher accuracy. compatibility with large datasetsIt is capable of performing equally good with large datasets with a significant reduction in training time as compared to XGBOOST. parallel learning supported lightGBM调参(常用参数) taskdefault= train, option: train, prediction applicationdefault= regression, option: regression, binary, multiclass, lambdarank(lambdarank application) datatraining data, 这个比较诡异，你需要创建一个lightGBM类型的data num_iterationsdefault =100, 可以设置为的大一些，然后使用early_stopping进行调节。 early_stopping_rounddefault =0, will stop training if one metric of one validation data doesn’t improve in last early_stopping_round rounds. num_leavesdefault =31, number of leaves in a tree devicedefault =cpu, options: gpu, cpu, choose gpu for faster training. max_depthspecify the max depth to which tree will grow, which is very important. feature_fractiondefault =1, specifies the fraction of features to be taken for each iteration. bagging_fractiondefault =1, spefifies the fraction of data to be used for each iteration and it generally used to speed up the training and avoid overfitting. max_binmax number of bins to bucket the feature values.因为模型是基于bin训练的，如果bin 数量越多，得到better accuracy,同时更加容易 overfitting. num_threads labelspecify the label columns. categorical_featurespecify the categorical features num_classdefault =1, used only for multi-class classification referrencewhich-algorithm-takes-the-crown-light-gbm-vs-xgboostLightGBM 如何调参官方文档param_tuning官方文档parameter XGBoost调参Advantage of XGBoost regularizationstandard GBM implementation has no regularization, in fact, XGBoost is also known as ‘regularized boosting’ technique. parallel processingwe know that boosting is sequential process so how can it be parallelized? this link to explore further. high flexibilityXGBoost allow users to define custom optimization objectives and evaluation criteria handling missing valuesvery useful property. XGBoost has an in-built routine to handle missing values. User is required to supply a different value than other observations and pass that as a parameter. XGBoost tries different things as it encounter a missing value on each node and learns which path to take for missing values in future. Tree pruningA GBM would stop splitting a node when it encounters a negative loss in the split. Thus it is more of a greedy algorithm. XGBoost on the other hand make splits upon the max_depth specified and then start pruning the tree backwards and remove splits beyond which there is no positive gain. built-in cross-validationThis is unlike GBM where we have to run a grid-search and only a limited values can be tested. continue on existing model XGBoost Parametersgeneral parametersGeneral Parameters: Guide the overall functioning booster:default =gbtree, can be gbtree, gblinear or dart. 一般使用gbtree. silent:default =0, silent mode is activated if set to 1(no running messages will be printed) nthread:default to maximum of threads. booster parametersBooster Parameters: Guide the individual booster (tree/regression) at each step eta(learning rate):default=0.3, typical final values to be used: 0.01-0.2, using CV to tune min_child_weight:minimum sum of instance weight (hessian) needed in a child. If the tree partition step results in a leaf node with the sum of instance weight less than min_child_weight, then the building process will give up further partitioning. In linear regression mode, this simply corresponds to minimum number of instances needed to be in each node. The larger, the more conservative the algorithm will be.default =1,too high values can lead to under-fitting, it should be tuned using CV. 数值越小越容易过拟合，越大越容易 under-fitting. max_depth:default =6, typical values: 3-10, should be tuned using CV. gamma:default =0, Gamma specifies the minimum loss reduction required to make a split.如果在分裂过程中小于该值，那么就不会继续分裂。 subsample:default =1, typical values: 0.5-1. Denotes the fraction of observations to be randomly samples for each tree. colsample_bytree:default =1, typical values: 0.5-1. colsample_bytree和subsample不同点：colsample_by是特征的随机fraction, subsample是rows的随机fraction。 lambda:default =1, L2 regularization term on weights(analogous to Ridge regression). Though many data scientists don’t use it often, it should be explored to reduce overfitting. alpha:default =0, L1 regularization term on weight (analogous to Lasso regression) scale_pos_weight:default =1, a value greater than 0 should be used in case of high class imbalance as it helps in faster convergence. learning task parametersLearning Task Parameters: Guide the optimization performed objectivebinary: logistic- returns predicated probability(not class)multi: softmax- returns predicated class(not probabilities)multi: softprob- returns predicated probability of each data point belonging to each class. eval_metircdefault according to objective(rmse for regression and error for classification), used for validation data.typical values: rmse(root mean square error), mse(mean absolute error), logloss(negative log-likelihood), error(binary classification error rate, 0.5 threshold), auc(area under the curve) seeddefault =0, used for reproducible results and also for parameter tuning. Control OverfittingThere are in general two ways that you can control overfitting in xgboost. The first way is to directly control model complexity. The second way is to add regularization parameters Referrencecomplete guide parameter tuning xgboost with codes python官方文档 param_tuning官方文档 parameter 补充一些理论知识 LightGBM 和 XGBoost 的一些区别 树的增长方式 这个原先是lightGBM所特有的，然后xgboost 在最新的版本上也实现该中方式，这估计就是开源，并不是一成不变的。两者都是基于叶子进行增长的，但是增长的方式是不同的。一种是 level-wise 一种是 leaf-wiseboth xgboost and lightGBM use the leaf-wise growth strategy when growing the decision tree.there are two strategies that can be employed: level-wise and leaf-wise. level-wise maintains a balanced tree（平衡树，左右子树的高度差不超过1）;但是 leaf-wise 这个就比较随意了。这个是这两者的区别：当叶子总数相同的时候，leaf-wise 这种生长方式得到的树的深度是大于 level-wise 的深度的。Compared to the case of level-wise growth, a tree grown with leaf-wise growth will be deeper when the number of leaves is the same. find the best split The key challenge in training a GBDT is the process of finding the best split for each leaf. The computational complexity is thus $O \left( n _ { \text {data} } n _ { \text {features} } \right)$.这两者采用的方式都是：现阶段的中的的数据 数据量和特征量都是很大的，所以这种方式是不可取的。然后这两种方法都是采用了 Histogram-based methods，这样最后的时间复杂度降低到：reducing the computational complexity to $O \left( n _ { d a t a } n _ { b i n s } \right)$. 这个复杂度是取决于number of bins。这个是引入了一个超参数，number of bins ，trade off between precision and time, 当更多的 bins 的时候，这个precision 会提高，但是 time 也会增大。 Ignoring sparse inputs 这个是处理缺省值（或者 0）的手段：两者在split 分裂点的时候，都是先不处理数值 0；然后找到分裂点之后，把0 放到哪边造成的loss 下降的比较大，然后就放到哪边。 Subsampling the data: Gradient-based One-Side Sampling (lightGBM)biased sampling(抽样的基本原则是随机性，但是在抽样过程中由于一系列因素造成偏差抽样，造成样本是不符合真实样本的分布)这个就是缓解，也不是为了彻底让其均衡，lightgbm increases the weigths of the samples.This means that it is more efficient to concentrate on data points with larger gradients.In order to mitigate this problem, lightGBM also randomly samples from data with small gradients.lightGBM increases the weight of the samples with small gradients when computing their contribution to the change in loss (this is a form of importance sampling, a technique for efficient sampling from an arbitrary distribution).]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>XGBoost</tag>
        <tag>LightGBM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[那些年的算法题目（二）]]></title>
    <url>%2F2018%2F07%2F21%2F%E9%82%A3%E4%BA%9B%E5%B9%B4%E7%9A%84%E7%AE%97%E6%B3%95%E9%A2%98%E7%9B%AE%EF%BC%88%E4%BA%8C%EF%BC%89%2F</url>
    <content type="text"><![CDATA[最长01相同子串已知一个长度为N的字符串，只由0和1组成， 求一个最长的子串，要求该子串出现0和1的次数相等。思路：最简单的方式是先生成字串，然后判断每个字串是否满足0的个数和1的个数相同。这种暴力求解时间复杂度O(n^3),明显是不合理的。下面说一下简单的做法：定义一个数组B[N]，B[i]表示从A[0…i]中 num_of_0 - num_of_1，0的个数与1的个数的差 。那么如果A[i] ~ A[j]是符合条件的子串，一定有 B[i] == B[j]，因为中间的部分0、1个数相等，相减等于0。 时间复杂度：O(n)，空间复杂度：O(n) 算法的思路是一样的。 Following is a solution that uses O(n) extra space and solves the problem in O(n) time complexity.Let input array be arr[] of size n and maxsize be the size of output subarray.1) Consider all 0 values as -1. The problem now reduces to find out the maximum length subarray with sum = 0.2) Create a temporary array sumleft[] of size n. Store the sum of all elements from arr[0] to arr[i] in sumleft[i]. This can be done in O(n) time.3) There are two cases, the output subarray may start from 0th index or may start from some other index. We will return the max of the values obtained by two cases.4) To find the maximum length subarray starting from 0th index, scan the sumleft[] and find the maximum i where sumleft[i] = 0.5) Now, we need to find the subarray where subarray sum is 0 and start index is not 0. This problem is equivalent to finding two indexes i &amp; j in sumleft[] such that sumleft[i] = sumleft[j] and j-i is maximum. To solve this, we can create a hash table with size = max-min+1 where min is the minimum value in the sumleft[] and max is the maximum value in the sumleft[]. The idea is to hash the leftmost occurrences of all different values in sumleft[]. The size of hash is chosen as max-min+1 because there can be these many different possible values in sumleft[]. Initialize all values in hash as -16) To fill and use hash[], traverse sumleft[] from 0 to n-1. If a value is not present in hash[], then store its index in hash. If the value is present, then calculate the difference of current index of sumleft[] and previously stored value in hash[]. If this difference is more than maxsize, then update the maxsize.7) To handle corner cases (all 1s and all 0s), we initialize maxsize as -1. If the maxsize remains -1, then print there is no such subarray. 代码实现12345678910111213141516171819202122232425262728def lengest01SubStr(s): &apos;&apos;&apos; 最长0,1 相等的子串长度 &apos;&apos;&apos; count =[0, 0] B =[0]*len(s) dic =&#123;&#125; # 保存 0 1 的差值 lengest =0 for i in range(len(s)): count[int(s[i])] +=1 B[i] =count[0] - count[1] # start from 0th index if B[i] ==0: lengest +=1 continue if B[i] in dic: # i -dic[B[i]] , not from 0th index lengest =max(lengest, i- dic[B[i]]) else: dic[B[i]] =i return lengesta =&apos;1011010&apos;b =&apos;10110100&apos;print(lengest01SubStr(a)) # 6 # &apos;011010&apos;print(lengest01SubStr(b)) # 8 # &apos;10110100&apos; 顺时针打印矩阵输入一个矩阵，按照从外向里以顺时针的顺序依次扫印出每一个数字。思路：发现网上有很多使用递归的，但是使用四个循环就可以解决这个问题。找到每次开始的起点，然后按照最上面一行，最右面一列，最小面一行和最左面一行这样的顺序进行打印即可。 思路：找到左上角的，一个start_point， 然后根据这个点进行上下左右的循环。 123456789101112131415161718192021222324252627282930313233343536373839404142def printMatrix(matrix): if matrix ==[[]]: return # 第一次见这样判断空的matrix row =len(matrix) column =len(matrix[0]) # 这里的left, right, up, down 都是真实能够access到数据的 left =0 right =column -1 up =0 down =row -1 res =[] while right &gt;left and up &lt;down: # from left to right for i in range(left, right+1): res.append(matrix[up][i]) # from up to down for i in range(up+1, down+1): res.append(matrix[i][right]) # from right to left for i in range(right-1, left-1, -1): res.append(matrix[down][i]) for i in range(down-1, up, -1): res.append(matrix[i][left]) left +=1 right -=1 up +=1 down -=1 # 最后对于这种特殊情况的处理是容易忘记的 # left one row 这种情况很特殊，只是从左往右遍历 if up ==down and left &lt;right: for i in range(left, right+1): res.append(matrix[up][i]) # left one column 只有可能是从上往下遍历 if left ==right and up &lt;down: for i in range(up, down+1): res.append(matrix[i][left]) if up ==down and left ==right: res.append(matrix[left][up]) return resprint(printMatrix(matrix)) 下面这个版本并没有运行成功，但是中间有个语法点是可以学习的。12345678910111213141516171819202122232425262728293031323334353637def printMatrix(matrix): res =[] # 第一个坐标表示行数，第二个坐标表示列数 # m 表示行数，n 表示列数 m =len(matrix) n = len(matrix[0]) if m ==1 and n ==1: res =[matrix[0][0]] return res else: for o in range(int((min(m,n)+1)/2)): # 不加这个[] 会有语法错误 [res.append(matrix[o][i]) for i in range(o, n-o)] [res.append(matrix[j][n-o-1]) for j in range(o, m-o) if matrix[j][n-o-1] not in res ] # 感觉这个最后的not in 是不合理的，万一这两个数字就是相同呢，试一下 [res.append(matrix[m-o-1][k]) for k in range(n-1, o-1, -1) if matrix[m-o-1][k] not in res] [res.append(matrix[l][o]) for l in range(m-1-o, o-1,-1) if matrix[l][o] not in res] # 这种写法还是少用，可以使用append,就少用这种，可读性不强，容易错 return resdef printMatrix(matrix): res =[] # 第一个坐标表示行数，第二个坐标表示列数 # m 表示行数，n 表示列数 m =len(matrix) n = len(matrix[0]) if m ==1 and n ==1: res =[matrix[0][0]] return res else: for o in range(int((min(m,n)+1)/2)): # 不加这个[] 会有语法错误 [res.append(matrix[o][i]) for i in range(o, n-o)] [res.append(matrix[j][n-o-1]) for j in range(o, m-o) if matrix[j][n-o-1] not in res ] # 感觉这个最后的not in 是不合理的，万一这两个数字就是相同呢，试一下 [res.append(matrix[m-o-1][k]) for k in range(n-1, o-1, -1) if matrix[m-o-1][k] not in res] [res.append(matrix[l][o]) for l in range(m-1-o, o-1,-1) if matrix[l][o] not in res] # 这种写法还是少用，可以使用append,就少用这种，可读性不强，容易错 return res]]></content>
      <categories>
        <category>算法</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Loss Activation and Optimisation Function]]></title>
    <url>%2F2018%2F07%2F07%2FLoss-Activation-and-Optimisation-Function%2F</url>
    <content type="text"><![CDATA[主要介绍 activation function， loss function 等等概念和分类。 Activation FunctionWhat? It’s just a thing (node) that you add to the output end of any neural network. It is also known as Transfer Function. It can also be attached in between two Neural Networks. $$Output = activation function \left( x _ { 1 } w _ { 1 } + x _ { 2 } w _ { 2 } + \cdots + x _ { n } w _ { n } + b i a s \right)$$ A weighted sum is computed as:$$x _ { 1 } w _ { 1 } + x _ { 2 } w _ { 2 } + \cdots + x _ { n } w _ { n }$$Then, the computed value is fed into the activation function, which then prepares an output.$$activation function \left( x _ { 1 } w _ { 1 } + x _ { 2 } w _ { 2 } + \cdots + x _ { n } w _ { n } + b i a s \right)$$ Think of the activation function as a mathematical operation that normalises the input and produces an output. The output is then passed forward onto the neurons on the subsequent layer. 作用：增加非线性 The thresholds are pre-defined numerical values in the function. This very nature of the activation functions can add non-linearity to the output. Activation Function Types Linear Activation Function: $$ output = k * x$$where $k$ is a scalar value, as an instance 2, and $x$ is the input. Sigmoid or Logistic Activation Function The sigmoid activation function is “S” shaped. It can add non-linearity to the output and returns a binary value of 0 or 1. $$Output = \frac { 1 } { 1 + e ^ { - x } }$$ 这个函数有一个很好的导数形式，在反向传播的时候，效果比较明显。 Tanh Activation Function Tanh is an extension of the sigmoid activation function. Hence Tanh can be used to add non-linearity to the output. The output is within the range of -1 to 1. Tanh function shifts the result of the sigmoid activation function: $$\text { Output } = \frac { 2 } { 1 + e ^ { - 2 x } } - 1$$ Rectified Linear Unit Activation Function (RELU) RELU is one of the most used activation functions. It is preferred to use RELU in the hidden layer. The concept is very straight forward. It also adds non-linearity to the output. However the result can range from 0 to infinity. $$ Output = \max ( 0 , x )$$这个是很高的评价了。If you are unsure of which activation function you want to use then use RELU. Softmax Activation Function Softmax is an extension of the Sigmoid activation function. Softmax function adds non-linearity to the output, however it is mainly used for classification examples where multiple classes of results can be computed. $$Output = \frac { e ^ { x } } { \operatorname { sum } \left( e ^ { x } \right) }$$ 这个一般使用在最后，作为多分类的结束。 Loss Function(Error Function)机器学习中所有的算法都需要最大化或最小化一个函数，这个函数被称为“目标函数”。其中，我们一般把最小化的一类函数，称为“损失函数”。它能根据预测结果，衡量出模型预测能力的好坏。 损失函数 (Loss function) 是用来衡量模型的预测值 $f(x)$ 和真实值 $Y$ 的不一样的程度，通常使用 $L (Y, f(x))$ 来进行表示，损失函数越小，模型的鲁棒性越强。 选择loss 的时候需要考虑两点：分类or 回归问题 和结果的输出情况。 the choice of loss function must match the framing of the specific predictive modeling problem, such as classification or regression. Further, the configuration of the output layer must also be appropriate for the chosen loss function. 总的来说是可以分成三类：回归模型，二分类模型和多分类模型 Regression Loss Functionsa. Mean Squared Error Lossb. Mean Squared Logarithmic Error Lossc. Mean Absolute Error Loss Binary Classification Loss Functionsa. Binary Cross-Entropyb. Hinge Lossc. Squared Hinge Loss Multi-Class Classification Loss Functionsa. Multi-Class Cross-Entropy Lossb. Sparse Multiclass Cross-Entropy Lossc. Kullback Leibler Divergence Loss Regression Loss Function (回归) 平方损失函数 定义： Mean Squared Error (MSE), or quadratic, loss function is widely used in linear regression as the performance measure, and the method of minimizing MSE is called Ordinary Least Squares (OSL)。 To calculate MSE, you take the difference between your predictions and the ground truth, square it, and average it out across the whole dataset. $$Loss = \frac { 1 } { n } \sum _ { i = 1 } ^ { n } \left( y ^ { ( i ) } - \hat { y } ^ { ( i ) } \right) ^ { 2 }$$ 在线性回归中，它假设样本和噪声都服从高斯分布（为什么假设成高斯分布呢？其实这里隐藏了一个小知识点，就是中心极限定理），最后通过极大似然估计MLE可以推导出最小二乘式子，即平方损失函数可以通过线性回归在假设样本是高斯分布的条件下推导得到。 $$S E = \sum _ { i = 1 } ^ { n } \left( y _ { i } - y _ { i } ^ { p } \right) ^ { 2 }$$ 为什么选择欧式距离作为误差的度量？ 简单，计算方便 欧式距离是一种很好的相似度衡量标准 在不同的表示域变换之后，特征的性质能够保持不变。 在实际应用中，通常会使用 均方差作为一种衡量指标，就是在上面的公式中除以 N. 使用说明： 如果 target 是服从高斯分布，那么使用 mean squared error 是没有问题；并且没有很好的理由进行替换的话，那么就是他了。 Mathematically, it is the preferred loss function under the inference framework of maximum likelihood if the distribution of the target variable is Gaussian. It is the loss function to be evaluated first and only changed if you have a good reason. Mean Squared Logarithmic Error Loss 和上面的的mse 有一点差别。这个是先记性log 求结果，然后再计算 mse. you can first calculate the natural logarithm of each of the predicted values, then calculate the mean squared error. This is called the Mean Squared Logarithmic Error loss, or MSLE for short. 好处:It has the effect of relaxing the punishing effect of large differences in large predicted values. 使用说明：如果最后的结果的数值有大值，那么可以尝试一下。不是那么符合高斯分布，就可以尝试一下。 Mean Absolute Error Loss 定义：Mean Absolute Error (MAE) is a quantity used to measure how close forecasts or predictions are to the eventual outcomes, which is computed by $$Loss = \frac { 1 } { n } \sum _ { i = 1 } ^ { n } \left| y ^ { ( i ) } - \hat { y } ^ { ( i ) } \right|$$ where $| .|$ denotes the absolute value. Albeit, both MSE and MAE are used in predictive modeling, there are several differences between them. MSE has nice mathematical properties which makes it easier to compute the gradient. However, MAE requires more complicated tools such as linear programming to compute the gradient. Because of the square, large errors have relatively greater influence on MSE than do the smaller error. Therefore, MAE is more robust to outliers since it does not make use of square. On the other hand, MSE is more useful if concerning about large errors whose consequences are much bigger than equivalent smaller ones. MSE also corresponds to maximizing the likelihood of Gaussian random variables. 使用说明： 当有很多的点偏离 mean and variance 的时候，可以尝试使用 mae loss function，这个显著的特点在于 对outlier 是有抵抗作用的。 The Mean Absolute Error, or MAE, loss is an appropriate loss function in this case as it is more robust to outliers. It is calculated as the average of the absolute difference between the actual and predicted values. Binary Classification Loss Function (二分)在二分类的网络结构中经常 出现 Dense() 网络层( 如果使用 keras 进行实现的话), 因为最后想要得到的是一个结点。 Binary Cross-Entropy Loss Cross-entropy loss is often simply referred to as “cross-entropy,” “logarithmic loss,” “logistic loss,” or “log loss” for short. Cross-entropy is the default loss function to use for binary classification problems. 如果没有更好的二分类的选择（理由），那么这个就是首选。 数学定义： Cross-entropy will calculate a score that summarizes the average difference between the actual and predicted probability distributions for predicting class 1. The score is minimized and a perfect cross-entropy value is 0. 二分类问题的交叉熵 loss 主要是有两种形式。第一种是输出的label 是 {0, 1}，也是最为常见的。$$Loss= - [ y \log \hat { y } + ( 1 - y ) \log ( 1 - \hat { y } ) ]$$ 这个公式可以先从一个分段函数推导，然后从最大似然的角度出发，预测类别的概率是:$$P ( y | x ) = \hat { y } ^ { y } \cdot ( 1 - \hat { y } ) ^ { ( 1 - y ) }$$然后取对数：$$log(loss)= - [ y \log \hat { y } + ( 1 - y ) \log ( 1 - \hat { y } ) ]$$ 当 $y =1$ 的时候，$$L = - \log \hat { y }$$因为，$$\hat { y } = \frac { 1 } { 1 + e ^ { - s } }$$所以得到$$L = \log \left( 1 + e ^ { - s } \right)$$loss 的曲线如下图所示： 从图中明显能够看出，s 越大于零，L 越小，函数的变化趋势也完全符合实际需要的情况。（y =1 是这个时候的目标） 当 $y =0$ 的时候：同理可以得到，$$log(loss)= \log \left( 1 + e ^ { s } \right)$$ 从图中明显能够看出，s 越小于零，L 越小，函数的变化趋势也完全符合实际需要的情况。 第二种情况是基于输出label 表示方式 {-1, 1}，这个时候的loss 表达式为: $$loss = \log \left( 1 + e ^ { - y s } \right)$$ 这个时候只不过使用 $ys$ 代替上面的s ，实际上的分析还是一样的。其中 y 表示真实的标签，s 表示 sigmoid 中的s，见下面的公式。$$g ( s ) = \frac { 1 } { 1 + e ^ { - s } }$$当 ys &gt;0 的时候，表示的预测正确，否则是预测错误。 交叉熵损失函数经常使用sigmoid 函数作为激活函数，因为这个可以完美解决平方损失函数中权重更新比较慢点的情况。 使用说明：Mathematically, it is the preferred loss function under the inference framework of maximum likelihood. It is the loss function to be evaluated first and only changed if you have a good reason.It is intended for use with binary classification where the target values are in the set {0, 1}. Hinge Loss An alternative to cross-entropy for binary classification problems is the hinge loss function, primarily developed for use with Support Vector Machine (SVM) models. Hinge Loss，又称合页损失，其表达式如下： $$Loss = \max ( 0,1 - y s )$$图像如下： 如同合起来的书，所以称之为 合页损失。显然，只有当 ys &lt; 1 时，Loss 才大于零；对于 ys &gt; 1 的情况，Loss 始终为零。Hinge Loss 一般多用于支持向量机（SVM）中，体现了 SVM 距离最大化的思想。 而且，当 Loss 大于零时，是线性函数，便于梯度下降算法求导。Hinge Loss 的另一个优点是使得 ys &gt; 0 的样本损失皆为 0，由此带来了稀疏解，使得 SVM 仅通过少量的支持向量就能确定最终超平面。 使用说明：要求 target 转换成 {-1, 1} ，效果有时候比 cross-binary 要好。 It is intended for use with binary classification where the target values are in the set {-1, 1}.Reports of performance with the hinge loss are mixed, sometimes resulting in better performance than cross-entropy on binary classification problems. ( 这句话比较迷离呀) The hinge loss function encourages examples to have the correct sign, assigning more error when there is a difference in the sign between the actual and predicted class values. Squared Hinge Loss (通常上讲，最后的结果更加光滑是没有什么劣势的)A popular extension is called the squared hinge loss that simply calculates the square of the score hinge loss. It has the effect of smoothing the surface of the error function and making it numerically easier to work with. 需要从 {0, 1} -&gt; {-1, 1} 这样target 的转换, 非常容易实现。 123# change y from &#123;0,1&#125; to &#123;-1,1&#125;y[where(y == 0)] = -1` 使用说明： （很强的相关性了）If using a hinge loss does result in better performance on a given binary classification problem, is likely that a squared hinge loss may be appropriate. Multi-Class Classification Loss Functions对于多类问题的定义： The problem is often framed as predicting an integer value, where each class is assigned a unique integer value from 0 to (num_classes – 1). The problem is often implemented as predicting the probability of the example belonging to each known class. Multi-Class Cross-Entropy Loss Cross-entropy is the default loss function to use for multi-class classification problems. （可见 交叉熵对于分类问题的重要性） 同理，如果是最大似然，概率模型，donot hesitate.( 数学基础就是在这里)Mathematically, it is the preferred loss function under the inference framework of maximum likelihood. It is the loss function to be evaluated first and only changed if you have a good reason. Cross-entropy will calculate a score that summarizes the average difference between the actual and predicted probability distributions for all classes in the problem. The score is minimized and a perfect cross-entropy value is 0. 基于 keras实现的时候需要先把 target (label) 转成 one-hot 类型的，当然这个可能造成 loss 曲线的波动（后话）。12# one hot encode output variabley = to_categorical(y) Sparse Multiclass Cross-Entropy Loss 和上面的区别主要在于 label 是不需要转成 one-hot 类型的，保持这原来的 number 形式。Sparse cross-entropy addresses this by performing the same cross-entropy calculation of error, without requiring that the target variable be one hot encoded prior to training. Kullback Leibler Divergence Loss Kullback Leibler Divergence, or KL Divergence for short, is a measure of how one probability distribution differs from a baseline distribution. 数学原理：(以 bit 为单位的 信息熵) A KL divergence loss of 0 suggests the distributions are identical. In practice, the behavior of KL Divergence is very similar to cross-entropy. It calculates how much information is lost (in terms of bits) if the predicted probability distribution is used to approximate the desired target probability distribution. 使用说明：更常使用 在复杂的模型上，比如 dense representation 之列。当然也是可以使用在多分类的情况下，这个时候如同 multi-class cross-entropy. As such, the KL divergence loss function is more commonly used when using models that learn to approximate a more complex function than simply multi-class classification, such as in the case of an autoencoder used for learning a dense feature representation under a model that must reconstruct the original input. In this case, KL divergence loss would be preferred. Nevertheless, it can be used for multi-class classification, in which case it is functionally equivalent to multi-class cross-entropy. 参考文献： How to Choose Loss Functions When Training Deep Learning Neural Networks log 对数损失函数 在逻辑回归的推导中，它假设样本服从伯努利分布（0-1分布），然后求得满足该分布的似然函数，接着取对数求极值等等。 指数损失函数 公式如下：$$loss = e ^ { - y s }$$ 曲线如下： Exponential Loss 与交叉熵 Loss 类似，但它是指数下降的，因此梯度较其它 Loss 来说，更大一些。 Exponential Loss 一般多用于AdaBoost 中。因为使用 Exponential Loss 能比较方便地利用加法模型推导出 AdaBoost算法。 $$L = - \log \frac { e ^ { s } } { \sum _ { j = 1 } ^ { C } e ^ { s _ { j } } } = - s + \log \sum _ { j = 1 } ^ { C } e ^ { s _ { j } }$$ softmax loss 的曲线如下图所示： 上图中，当 s &lt;&lt; 0 时，Softmax 近似线性；当 s&gt;&gt;0 时，Softmax 趋向于零。Softmax 同样受异常点的干扰较小，多用于神经网络多分类问题中。 若我们把 ys 的坐标范围取得更大一些，上面 5 种 Loss 的差别会更大一些，如图： 显然，这时候 Exponential Loss 会远远大于其它 Loss。从训练的角度来看，模型会更加偏向于惩罚较大的点，赋予其更大的权重。如果样本中存在离群点，Exponential Loss 会给离群点赋予更高的权重，但却可能是以牺牲其他正常数据点的预测效果为代价，可能会降低模型的整体性能，使得模型不够健壮（robust）。 相比 Exponential Loss，其它四个 Loss，包括 Softmax Loss，都对离群点有较好的“容忍性”，受异常点的干扰较小，模型较为健壮。 Softmax loss 对于多分类问题，可以使用 softmax loss。 其中，C 为类别个数，小写字母 s 是正确类别对应的 Softmax 输入，大写字母 S 是正确类别对应的 Softmax 输出。 由于 log 运算符不会影响函数的单调性，我们对 S 进行 log 操作。另外，我们希望 log(S) 越大越好，即正确类别对应的相对概率越大越好，那么就可以对 log(S) 前面加个负号，来表示损失函数： 如何选择损失函数？ 对于异常点的处理是一个维度，比如L1 损失函数处理异常点更加稳定，相对L2 损失函数。 what? 衡量模型好坏的 function，如果模型表现好，那么loss 应该是小；如果模型表现不好，那么loss 应该是大的。 At its core, a loss function is incredibly simple: it’s a method of evaluating how well your algorithm models your dataset. If your predictions are totally off, your loss function will output a higher number. If they’re pretty good, it’ll output a lower number. As you change pieces of your algorithm to try and improve your model, your loss function will tell you if you’re getting anywhere. Log Loss (Cross Entropy Loss) Cross-entropy loss, or log loss, measures the performance of a classification model whose output is a probability value between 0 and 1. Cross-entropy loss increases as the predicted probability diverges from the actual label. So predicting a probability of .012 when the actual observation label is 1 would be bad and result in a high loss value. A perfect model would have a log loss of 0. The graph above shows the range of possible loss values given a true observation (isDog = 1). As the predicted probability approaches 1, log loss slowly decreases. As the predicted probability decreases, however, the log loss increases rapidly. Log loss penalizes both types of errors, but especially those predictions that are confident and wrong! Cross-entropy and log loss are slightly different depending on context, but in machine learning when calculating error rates between 0 and 1 they resolve to the same thing. In binary classification, where the number of classes M equals 2, cross-entropy can be calculated as: $$- ( y \log ( y ) + ( 1 - y ) \log ( 1 - y ) )$$ If $ M&gt;$2 (i.e. multiclass classification), we calculate a separate loss for each class label per observation and sum the result. $$- \sum _ { c = 1 } ^ { M } y _ { o , c } \log \left( y _ { o , c } \right)$$ M - number of classes (dog, cat, fish)log - the natural logy - binary indicator (0 or 1) if class label c is the correct classification for observation o 想要表达的是 log loss 是从 Likelihood Loss，改进过来的，有没有发现最大似然的痕迹。log loss 表达式如下：$$\begin{split}P(Y | X) &amp;= P(X_1 | Y) \times P(X_2 | Y) \times \dots \times P(X_n | Y) \times P(Y) = P(Y) \prod_{i}^{n} P(X_i | Y) \\&amp;\Rightarrow log(P(Y | X)) = log(\prod_{i}^{n} P(X_i | Y) \Rightarrow \sum_{i}^{n} log(P(X_i | Y))\end{split}$$交叉熵表达式：$$CE(\hat{y}, y) = - \sum_{i=1}^{n} y_i log(\hat{y}) + (1 - y_i) log(1 - \hat{y})$$ L2 这两个loss function 在这里介绍过，所以本博客中简单说一下。 L2 loss function is the square of the L2 norm of the difference between actual value and predicted value. It is mathematically similar to MSE, only do not have division byn, it is computed by $$ Loss = \sum _ { i = 1 } ^ { n } \left( y ^ { ( i ) } - \hat { y } ^ { ( i ) } \right) ^ { 2 }$$ Kullback Leibler (KL) Divergence（计算的是两个分布的问题）KL Divergence, also known as relative entropy, information divergence/gain, is a measure of how one probability distribution diverges from a second expected probability distribution. KL divergence loss function is computed by$$D _ { K L } ( p | q ) = \sum _ { x } p ( x ) \log \frac { p ( x ) } { q ( x ) }$$ 交叉熵的定义：$$H ( p , q ) = - \sum _ { x } p ( x ) \log q ( x )$$两者的关系推导，$$\begin{split}D _ { K L } ( p | q ) &amp;= \sum _ { x } p ( x ) \log \frac { p ( x ) } { q ( x ) } \\&amp;= \sum _ { x } ( p ( x ) \log p ( x ) - p ( x ) \log q ( x ) ) \\&amp;= - H ( p ) - \sum _ { x } p ( x ) \log q ( x ) \\&amp;= - H ( p ) + H ( p , q )\end{split}$$所以说， cross entropy 也是可以写成这样：$$H ( p , q ) = D _ { K L } ( p | q ) + H ( p )$$ logistic loss 和 cross entropy的关系 当 $ p \in { y , 1 - y }$, $q \in { \hat { y } , 1 - \hat { y } }$ ，cross entropy 可以写成 logistic loss: $$H ( p , q ) = - \sum _ { x } p ( x ) \log q ( x ) = - y \log \hat { y } - ( 1 - y ) \log ( 1 - \hat { y } )$$]]></content>
      <categories>
        <category>Deep learning</category>
      </categories>
      <tags>
        <tag>loss function</tag>
        <tag>Activation Function</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[特征工程]]></title>
    <url>%2F2018%2F06%2F29%2F%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[关于特征工程的概念，特征离散化、特征生成、组合特征、特征选取的方法。 特征生成机器学习的一大任务在于手工 create 特征。特征主要来源有两个，一个统计学知识，一个该场景下的特征。前者属于常规操作，后者需要有对业务领域比较熟悉的了解。后者举个例子，比如在衡量个人信用的时候， days_employed / days_birth 就是一个属于金融（保险）领域的较为熟悉，才能理解的一个特征, 这种百分比，占比的思想还是非常常见的。前者同样举个栗子，对于子表（传统机器学习还是有很多来自数据库的信息的）常用的操作是 aggregation 操作（min max sum variance mean）等聚合操作，然后和主表进行连接。一般来说子表是可以广泛的使用 aggregation 操作，但是对于主表来说，这个就取决于信息是什么，特征是什么内容，聚合函数的本质在于”总结“，就是你操作的变量是否有必要这样做，看一下数据是不是流水账。 特征离散化？连续化特征就是一些不可枚举的有理数。那么什么是离散化特征呢？ 离散化特征就是可枚举的特征。离散化的作用是把数据变成可计算状态。而特征工程就是从原始字段中根据业务提取出对模型有效的特征出来。 在线性模型下(w.x)，w已经确定的情况下，x的某个特征的值是20，或者30，w.x的值相差是很大的，但是往往20岁的人跟30岁的人对同一个广告的兴趣差距不会那么大。连续特征离散化的基本假设，是默认连续特征不同区间的取值对结果的贡献是不一样的。离散化和连续化最大的区别是，对一个字段做连续化后的结果就还只是一个特征，而离散化后的这一列有多少个key(字段可能的值)就会抽取出多少个特征。当经过离散化之后，特征各有各的权重，彼此之间就没有关系了。 模型是使用离散特征还是连续特征, 其实是一个“海量离散特征+简单模型” 同 “少量连续特征+复杂模型”的权衡。既可以离散化用线性模型，也可以用连续特征加深度学习。 常用的选取离散点的方法：等距离离散，等样本离散、画图观察趋势和决策树模型(天生就可以对连续特征分段)。 在工业界，很少直接将连续值作为特征喂给逻辑回归模型，而是将连续特征离散化为一系列0、1特征交给逻辑回归模型，这样做的优势有以下几点： 单变量离散化为N个后，每个变量有单独的权重，在激活函数的作用下相当于为模型增加了非线性，能够提升模型表达能力，加大拟合。 离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄&gt;30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰，因为特征值的异常会导致权重也就是w的值也会异常。 一定有同学担心特征过多会导致运算缓慢，但是LR是线性模型，我们在内部计算的时候是向量化计算，而不是循环迭代。稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展。 所以不用担心像GBDT算法那样，特征多了就跑不动了(我们都说GBDT不能用离散特征不是因为它处理不了离散特征，而是因为离散化特征后会产生特别多的特征，决策树的叶子节点过多，遍历的时候太慢了)。 所以海量离散特征＋LR是业内常见的一个做法。而少量连续特征+复杂模型是另外一种做法，例如GBDT。 当然也可以通过深度学习来进行特征选择：目前这种手段正在随着深度学习的流行而成为一种手段，尤其是在计算机视觉领域，原因是深度学习具有自动学习特征的能力，这也是深度学习又叫unsupervised feature learning的原因。从深度学习模型中选择某一神经层的特征后就可以用来进行最终目标模型的训练了。 参考文献:https://blog.csdn.net/lujiandong1/article/details/52412123 组合特征先是离散化，然后是特征组合。交叉从理论上而言是为了引入特征之间的交互，也即为了引入非线性。LR(逻辑回归）分类算法:因为线性函数的表达能力有限，所以我们引入激活函数就是给LR增加非线性关系。能让一条直线变成曲线。这样可以拟合出更好的效果。（也由此才后了后来说的过拟合问题而引入了正则化超参数） LR模型之所以很受欢迎，主要是因为LR模型本质是对数线性模型，实现简单，易于并行，大规模扩展方便，迭代速度快，同时使用的特征比较好解释，预测输出在0与1之间契合概率模型。（模型的可解释性举例，比如A-B的权重比较大，A代表用户，B代表物品，那么可以认为A是对B比较感兴趣的）但是，线性模型对于非线性关系缺乏准确刻画，特征组合正好可以加入非线性表达，增强模型的表达能力。另外，广告LR中，基本特征可以认为是用于全局建模，组合特征更加精细，是个性化建模，因为在这种大规模离散LR中，单对全局建模会对部分用户有偏，对每一用户建模又数据不足易过拟合同时带来模型数量爆炸，所以基本特征+组合特征兼顾了全局和个性化。 从统计的角度解释，基本特征仅仅是真实特征分布在低维的映射，不足以描述真实的分布，加入特征在高维空间拟合真实分布，使得预测更加准确。 寻找高级特征最常用的方法有：若干项特征加和： 我们假设你希望根据每日销售额得到一周销售额的特征。你可以将最近的7天的销售额相加得到。若干项特征之差： 假设你已经拥有每周销售额以及每月销售额两项特征，可以求一周前一月内的销售额。若干项特征乘积： 假设你有商品价格和商品销量的特征，那么就可以得到销售额的特征。若干项特征除商： 假设你有每个用户的销售额和购买的商品件数，那么就是得到该用户平均每件商品的销售额。 正则化真正测试一个模型的不是简单与否，更重要在于它在预测新的情况时表现如何。小权重意味着网络的行为不会因为我们随意更改了一些输入而改变太多。这是我们加了正则化之后的成本函数，可以看我们后面加入了正则化 λ 的表达式来完善成本函数。为什么加入λ能够减轻过拟合呢？直观一点的解释是设置的λ值越大，那么参数w的值就会被压缩的越小(在梯度下降中, 每次迭代的步长，也就是这个公式w=w - 学习率*成本函数对w的导数， 现在由于成本函数增加了正则项，使得J和w变得数值相关了)。 假设λ设置的足够大，那么w会无限的趋近于0. 把多隐藏层的单元的权重设置为0以后，那么基本上就是消除掉了这些单元的作用，而使得网络模型得到简化，就像下面的图一样。由于正则化的设置，消除了一些隐藏单元的作用。而使得整个模型越来越接近于线性化，也就是从下图中的过拟合往欠拟合偏转。当然我们有一个适合的λ的值，能让我们的拟合状态达到最佳。所以我们在训练模型的时候，往往都会有一个Ｌ２正则项的超参数需要我们设置。 这是我们的tanh激活函数， 可以看到当z的值越大时，整个函数的非线性就越大，而z的值越小(图中红色加粗部分),函数就越是呈现出线性分布。 所以当我们增加λ的值， w得值就越小，相应的z的值也就越小。因为z = wx + b。 而我们第一次说激活函数的时候就说过神经网络中基本上是不使用线性函数作为激活函数的，因为不论有多少层，多少个单元，线性激活函数会使得所有单元所计算的都呈现线性状态。 feature selectionfeature selection 的过程就是dimension reduction的过程。就是说由较多的数据集 映射到 较少的数据集，这种方式就叫做降维。 Using dimensionality reduction techniques, of course. You can use this concept to reduce the number of features in your dataset without having to lose much information and keep (or improve) the model’s performance. 必要性分析：时间角度，空间（内存）角度。减少冗余信息，就减少了模型去拟合”噪声“数据的可能性。 特征的 常见的有以下几种方式。 Remove collinear features具体说来 显示 corr,然后上三角形，然后根据阈值筛选相关性比较强的特征，最后drop。 PCA之类的，解决的是feature 个数的问题。 Remove features with greater than a threshold percentage of missing values根据isnull() 计算百分比，然后排序比较， 解决的是feature 个数的问题。 Keep only the most relevant features using feature importances from a model根据feature importance 进行feature selection， 解决的feature 个数的问题。 根据数值型数据本身变化，如果variance 很小，则舍去； 还有一种说法（这应该是最简单的特征选择方法了：假设某特征的特征值只有0和1，并且在所有输入样本中，95%的实例的该特征取值都是1，那就可以认为这个特征作用不大。如果100%都是1，那这个特征就没意义了。当特征值都是离散型变量的时候这种方法才能用，如果是连续型变量，就需要将连续变量离散化之后才能用，而且实际当中，一般不太会有95%以上都取某个值的特征存在，所以这种方法虽然简单但是不太好用。可以把它作为特征选择的预处理，先去掉那些取值变化小的特征，然后再从接下来提到的的特征选择方法中选择合适的进行进一步的特征选择。）。所以这个规则是针对离散变量还是连续变量，是可以在实际问题中进行考虑的。 单变量特征选择 ( 单变量特征选择能够对每一个特征进行测试，衡量该特征和响应变量之间的关系，根据得分扔掉不好的特征。对于回归和分类问题可以采用卡方检验等方式对特征进行测试。) Model based ranking (基于学习模型的特征选择排序) Pearson 相关系数，特征和响应变量之间关系的方法，结果的取值区间为[-1，1]，-1表示完全的负相关(这个变量下降，那个就会上升)，+1表示完全的正相关，0表示没有线性相关因为网络本身就是学习的该特征的波动是如何影响最后的结果，所以如果数据本身没有波动性，那么是没有什么价值的。解决的是feature 个数的问题。 插一句，如何 handle 单个特征本身表示长度的问题，思考一下 word2vec 是怎样进行操作的。 Principal Component Analysis (PCA)为什么要进行降维数据的处理？ (1) 在原始的高维空间中，包含有冗余信息以及噪音信息，在实际应用例如图像识别中造成了误差，降低了准确率；而通过降维,我们希望减少冗余信息所造成的误差,提高识别（或其他应用）的精度。(2) 或者希望通过降维算法来寻找数据内部的本质结构特征。(3) 通过降维来加速后续计算的速度(4) 还有其他很多目的，如解决数据的sparse问题 这个属于一句话的的思想：PCA的问题其实是一个基的变换，使得变换后的数据有着最大的方差。 pca 的假设：这个说得很清楚，higher dimensional space is mapped to lower dimension, 然后这种在lower dimensional 中数据的variance应该是保持max的。It works on a condition that while the data in a higher dimensional space is mapped to data in a lower dimension space, the variance of the data in the lower dimensional space should be maximum. 就是需要理解这个主成分是不断的生成的，在前者的基础之上生成的。below are some of the key points you should know about PCA before proceeding further: A principal component is a linear combination of the original variables Principal components are extracted in such a way that the first principal component explains maximum variance in the dataset Second principal component tries to explain the remaining variance in the dataset and is uncorrelated to the first principal component Third principal component tries to explain the variance which is not explained by the first two principal components and so on The first component is the most important one, followed by the second, then the third, and so on. SVDSVD decomposes the original variables into three constituent matrices. It is essentially used to remove redundant features from the dataset. It uses the concept of Eigenvalues and Eigenvectors to determine those three matrices. We will not go into the mathematics of it due to the scope of this article, but let’s stick to our plan, i.e. reducing the dimensions in our dataset. $$\operatorname { Data } _ { m \times n } = U _ { m \times m } \Sigma _ { m \times n } V _ { n \times n } ^ { T }$$SVD将原始的数据集矩阵Data分解成三个矩阵：U、Sigma、VT，如果原始矩阵是m行n列，那么U、Sigma和VT分别就是m行m列、m行n列、n行n列。比较值得一提的是矩阵Sigma，该矩阵只有对角元素，其他元素均为0，有一个惯例是：Sigma的对角元素是从大到小排列的。这些对角元素就称为奇异值. 投影也是一种降维手段这种思想真的是服气，虽然我也不是很懂，但是思想是很好的By projecting one vector onto the other, dimensionality can be reduced.By projecting one vector onto the other, dimensionality can be reduced.讲解了什么是 manifold 这个概念，还是很好的These small portions where the Earth looks flat are manifolds, T-sne就是指出 t-sne 这个可以非线性降维。有local approaches 和 global approaches 两种方式，我大概的理解就是：类之间还能尽可能的远离，类内保持差异性。（不知道对不对）So far we have learned that PCA is a good choice for dimensionality reduction and visualization for datasets with a large number of variables. But what if we could use something more advanced? What if we can easily search for patterns in a non-linear way? t-SNE is one such technique. There are mainly two types of approaches we can use to map the data points:Local approaches : They maps nearby points on the manifold to nearby points in the low dimensional representation.Global approaches : They attempt to preserve geometry at all scales, i.e. mapping nearby points on manifold to nearby points in low dimensional representation as well as far away points to far away points. 杂货铺 对于encodding 的理解，从字符转成数字，并且尽可能的保留原来的信息。接触的有三种，一种 label encoding，适合类别信息只有两类。如果大于两类那么就使用one-hot。第三种就是万物可以embedding，使用神经网络的思想。 特征工程可以分为特征处理、Feature Selection（特征选择）、Feature Extraction（特征提取）和Feature construction（特征构造）等阶段。归一化（去中心，方差归一）是属于特征(预)处理:把特征值压缩成0~1的区间。 One-hot（也叫One-of-k）的方法把每个无序特征转化为一个数值向量。比如一个无序特征color有三种取值：red，green，blue。那么可以用一个长度为3的向量来表示它，向量中的各个值分别对应于red，green，blue。如：这种方法在NLP里用的很多，就是所谓的词向量模型。变换后的向量长度对于词典长度，每个词对应于向量中的一个元素。 对于欠拟合: 增加神经网络复杂度，出现欠拟合的原因之一是由于函数的非线性不足，所以用更复杂的网络模型进行训练来加深拟合。对于过拟合：增加数据规模， 出现过拟合的原因之一是数据规模不足而造成的数据分布不均，扩展数据规模能比较好的解决这个问题。当然另一个做法是正则化，我们采取使用正则化来解决过拟合问题，常用的是L2正则，其他的还有L1和 Dropout正则。 很多人会想着既然线性分类器搞不定，那就直接找个非线性的好了，比如高斯核的SVM。我们确实可以通过这种简单换算法的方式解决这个简单的问题。但对于很多实际问题（如广告点击率预测），往往特征非常多，这时候时间约束通常不允许我们使用很复杂的非线性分类器。这也是为什么算法发展这么多年，广告点击率预测最常用的方法还是Logistic Regression (LogisticReg)。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>PCA</tag>
        <tag>SVD</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常见的排序算法总结]]></title>
    <url>%2F2018%2F06%2F29%2F%E5%B8%B8%E8%A7%81%E7%9A%84%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[分类和总结 根据待排序的数据大小不同，使得排序过程中所涉及的存储器不同，可分为内部排序和外部排序。 排序关键字可能出现重复，根据重复关键字的排序情况可分为稳定排序和不稳定排序。冒泡、插入和归并是稳定，在这里提到的其他的几个都是不稳定的。 对于内部排序，依据不同的排序原则，可分为插入排序、交换(快速)排序、选择排序、归并排序和计数排序。 针对内部排序所需的工作量划分，可分为:简单排序 O(n^2)、先进排序 O(nlogn)和基数排序 O(d*n)。常见算法的性质总结： 排序算法实现默认都是升序… 插入排序(Insert Sort)思想： 有序数组+insert one every one time 插入排序的工作方式非常像人们排序一手扑克牌一样。开始时，我们的左手为空并且桌子上的牌面朝下。然后，我们每次从桌子上拿走一张牌并将它插入左手中正确的位置。为了找到一张牌的正确位置，我们从右到左将它与已在手中的每张牌进行比较，如下图所示：步骤： 从第一个元素开始，该元素可以认为已经被排序 取出下一个元素，在已经排序的元素序列中从后向前扫描 如果该元素（已排序）大于新元素，将该元素移到下一位置 重复步骤3，直到找到已排序的元素小于或者等于新元素的位置 将新元素插入到该位置后 重复步骤2~5 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647def insert_sort(lists): count = len(lists) for i in range(1, count): key =lists[i] j =i-1 while j &gt;= 0: if lists[j] &gt;key: lists[j+1] =lists[j] lists[j] =key j -= 1 return lists# testlists =[1,2,-3, 90,34]print(insert_sort(lists))```看### 选择排序(Select Sort)思想和步骤：select one every time简单选择排序是最简单直观的一种算法，基本思想为每一趟从待排序的数据元素中选择最小（或最大）的一个元素作为首元素，直到所有元素排完为止，简单选择排序是不稳定排序。![](http://ww1.sinaimg.cn/large/e9a223b5ly1g3dcxrw5sbg20mj06wdsb.gif)分析：无论什么数据进去都是 O(n²) 的时间复杂度。所以用到它的时候，数据规模越小越好。唯一的好处可能就是不占用额外的内存空间了吧。代码实现:```pythondef select_sort(lists): count =len(lists) for i in range(0, count): min =i for j in range(i+1, count): if lists[min] &gt;lists[j]: min =j lists[min], lists[i] =lists[i], lists[min] return lists# testlists =[1, 23,45, 0,-1]print(select_sort(lists)) 冒泡排序(Bubble Sort)思想： move smallest one time (假设不减) 一遍排序之后，右边是最大，也就是排好序的。这个规则是以此类推的。 有两种说法 从右往左： 最小值被移到了最左边。 【冒泡法】的本意 从左往右： 最大值被移到了最右边。 此时其实应该叫 【沉石法】 从左向右 沉石法 图解： 分析：平均时间复杂度：O(n^2)最坏空间复杂度： 总共 O(n)，需要辅助空间 O(1) 1234567891011def bubble_sort(lists): count =len(lists) for i in range(0, count): for j in range(i+1, count): if lists[i]&gt; lists[j]: lists[i], lists[j] =lists[j], lists[i] return lists# testlists =[1,34,45,0,89]print(bubble_sort(lists)) 归并排序(Merge Sort)思想： divide-and-conquer +递归 归并排序（MERGE-SORT）是利用归并的思想实现的排序方法，该算法采用经典的分治（divide-and-conquer）策略（分治法将问题分(divide)成一些小的问题然后递归求解，而治(conquer)的阶段则将分的阶段得到的各答案”修补”在一起，即分而治之)。 分析：时间复杂度：O(nlogn)空间复杂度：O(N)，归并排序需要一个与原数组相同长度的数组做辅助来排序 1234567891011121314151617181920212223242526def merge(left, right): i, j =0,0 result =[] while i&lt;len(left) and j &lt;len(right): if left[i] &lt;= right[j]: result.append(left[i]) i +=1 else: result.append(right[j]) j +=1 result += left[i:] result += right[j:] return resultdef merge_sort(lists): if len(lists) &lt;=1: return lists num =int(len(lists)/2) left =merge_sort(lists[:num]) right = merge_sort(lists[num:]) return merge(left, right)# merge_sort 是先切分，然后再整合，quick sort 是两个指针# testlists =[1, 34, 23,45,0,9]print(merge_sort(lists)) f 快速排序(Quick Sort)思想：任意选择一个key(通常选择a[0])，将比他小的数据放在它的前面，比他大的数字放在它的后面。递归进行。 步骤： 从数列中挑出一个基准值。 将所有比基准值小的摆放在基准前面，所有比基准值大的摆在基准的后面(相同的数可以到任一边)；在这个分区退出之后，该基准就处于数列的中间位置。 递归地把”基准值前面的子数列”和”基准值后面的子数列”进行排序。 分析：快速排序的时间复杂度在最坏情况下是O($N^2$)，平均的时间复杂度是O(N*logN)，采用的是分治的思想，二叉树的结构。 下面以数列 a={30,40,60,10,20,50} 为例，演示它的快速排序过程(如下图)。 这个是经过一个迭代的结果，每一个迭代，都排好了基准数字的位置。按照同样的方法，对子数列进行递归遍历。最后得到有序数组！ 在实现的过程中，key 值的选择 while 的遍历顺序是相关的。如果key =arr[left] 那么第一个while 循环是从right 开始的（从后往前）；如果key =arr[right] 那么是从left 进行遍历的。 12345678910111213141516171819202122232425def quick_sort(lists, left, right): if left &gt;= right: return lists key =lists[left] low =left high =right while left &lt; right: # 因为你最初key 取得是left，然后从右边找到一个比key小的，然后替换left 的位置 while left &lt;right and lists[right]&gt;=key: right -= 1 lists[left] =lists[right] while left &lt; right and lists[left] &lt;= key: left +=1 lists[right] =lists[left] lists[left] =key # 这里的left 和right 都是可以的，因为从while 中出来之后两者是相同的 # 这个步伐是1,所以只能是一个个变化 quick_sort(lists, low, left-1) quick_sort(lists, left+1, high) # 因为left的位置已经被占了，所以只是划分左边一块，右边一块就是可以的 return lists# testlists =[3,2,45, 100,1,56,56]#lists =[1,2,3,2,2,2,5,4,2]print(quick_sort(lists, 0, len(lists)-1)) 给出两种 C ++ 的代码，发现有两点。 分开写之后，可以发现和 下面那个算法题目的关系： 这个基准点就是那个 Kth 的一个参考 关于while 条件的分析， 如果是i &lt;=j 那么返回的是j， 如果是 i&lt;j 那么返回的i 作为基准点 12345678910111213141516171819202122232425262728293031323334void QuickSort(int array[], int start, int last)&#123;int i = start;int j = last;int temp = array[i];if (i &lt; j)&#123;while (i &lt; j)&#123;//while (i &lt; j &amp;&amp; array[j]&gt;=temp )j--;if (i &lt; j)&#123;array[i] = array[j];i++;&#125;while (i &lt; j &amp;&amp; temp &gt; array[i])i++;if (i &lt; j)&#123;array[j] = array[i];j--;&#125;&#125;//把基准数放到i位置array[i] = temp;//递归方法QuickSort(array, start, i - 1);QuickSort(array, i + 1, last);&#125;&#125; 第二种写法： 123456789101112131415161718192021222324252627282930313233int partition(int arr[], int left, int right) //找基准数 划分&#123;int i = left + 1 ;int j = right;int temp = arr[left];while(i &lt;= j)&#123;while (arr[i] &lt; temp)&#123;i++;&#125;while (arr[j] &gt; temp )&#123;j--;&#125;if (i &lt; j)swap(arr[i++], arr[j--]);else i++;&#125;swap(arr[j], arr[left]);return j;&#125;void quick_sort(int arr[], int left, int right) &#123;if (left &gt; right)return;int j = partition(arr, left, right);quick_sort(arr, left, j - 1);quick_sort(arr, j + 1, right);&#125; Kth Largest Element in an Array Find the kth largest element in an unsorted array. Note that it is the kth largest element in the sorted order, not the kth distinct element. Tips: 使用的是 快排的思想，最后的平均时间复杂度是O(N) ，所以是一个不错的算法。 kth 和 ”快排“ 实现的时候稍微有一些区别，前者只有发现 一组数据（一个比pivot 大，一个比 pivot 小）才进行交换，后者是是 两个while， 只要发现一个就进行交换。这个是细微的差别。 这个代码是可以和上面 quickSort() 进行好好比较的。 123456789101112131415161718192021222324252627282930def partition(arr, left, right): key =arr[right] low, high =left, right while left&lt; right: while left &lt;right and arr[left]&gt;=key: left +=1 arr[right] =arr[left] while left&lt;right and arr[right] &lt;=key: right -=1 arr[left] =arr[right] arr[right] =key return leftdef findKthLargest(arr, k): left, right =0, len(arr)-1 while True: pos =partition(arr, left, right) if pos ==k-1: return arr[pos] elif pos &gt; k-1: right =pos -1 else: left = pos+1 将上面的想法和到一起。这两道题目是比较有意思的， 很好哈。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748class Solution &#123;public:/** @param n: An integer* @param nums: An array* @return: the Kth largest element*/int kthLargestElement(int k, vector&lt;int&gt; nums)&#123;//第k大 下标k-1return QuickSort(nums, 0, nums.size() - 1, k-1);&#125;int QuickSort(vector&lt;int&gt; &amp;nums, int left, int right, int k) &#123;if (left == right) &#123;return nums[left];&#125;if(left&lt;right)&#123; //快排思想 从大至小排序int i=left;int j=right; int key=nums[left]; while(i&lt;j)&#123; while(i&lt;j&amp;&amp;nums[j]&lt;key) j--;if(i&lt;j)&#123; nums[i]=nums[j]; i++;&#125; while(i&lt;j&amp;&amp;nums[i]&gt;=key) i++;if(i&lt;j)&#123; nums[j]=nums[i]; j--;&#125; &#125; //i=j退出循环 nums[i]=key; //进行基准的新下标与k的比较if(i&gt;k)return QuickSort(nums,left,i-1,k);else if(i&lt;k)return QuickSort(nums,i+1,right,k);elsereturn nums[i];&#125;&#125;&#125;; 堆排序参看另外一篇博客 参考文献算法动图效果排序算法分类]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>排序算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[那些年的算法题目（一）]]></title>
    <url>%2F2018%2F06%2F22%2F%E9%82%A3%E4%BA%9B%E5%B9%B4%E7%9A%84%E7%AE%97%E6%B3%95%E9%A2%98%E7%9B%AE%EF%BC%88%E4%B8%80%EF%BC%89%2F</url>
    <content type="text"><![CDATA[完全二叉树插入问题描述已知一个完全二叉树的结构，现在需要将一个节点插入到这颗完全二叉树的最后，使得它还是一个完全二叉树。第一种解法：如果该树为满二叉树或者左子树不为满二叉树，那么就进入左子树，否则进入右子树，递归进行。 二叉树(Binary Tree)强行补充一下关于二叉树概念的知识。完全二叉树(Complete Binary Tree):若设二叉树的深度为h，除第h层外，其它各层(1～h-1)的结点数都达到最大个数，第h层所有的结点都连续集中在最左边，这就是完全二叉树。满二叉树:树中除了叶子节点，每个节点都有两个子节点。满二叉树是一种特殊的完全二叉树。二叉搜索树(binary search tree):所有非叶子结点至多拥有两个儿子（Left和Right）；所有结点存储一个关键字,非叶子结点的左指针指向小于其关键字的子树，右指针指向大于其关键字的子树。平衡二叉树(AVL树)：它是一颗空树或它的左右两个子树的高度差的绝对值不超过1。哈夫曼树：带权路径长度达到最小的二叉树，也叫做最优二叉树。树的深度和高度：深度是从上往下数；高度是从下往上数 代码实现平滑过渡到本问题的代码实现。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152#include&lt;iostream&gt;using namespace std;typedef struct Node&#123; int value; struct Node *lchild, *rchild;&#125;Tree;int GetLeftDepth(Tree* root)&#123; Tree* pNode =root-&gt;lchild ; int depth =0; while(pNode != NULL) &#123; depth ++; pNode =pNode-&gt;lchild; &#125; return depth;&#125;int GetRightDepth(Tree* root)&#123; Tree* pNode =root-&gt;rchild; int depth =0 ; while(pNode != NULL) &#123; depth ++ ; pNode =pNode-&gt;rchild ; &#125; return depth;&#125;bool IsFullBinaryTree(Tree* root)&#123; return GetLeftDepth(root) == GetRightDepth(root) ;&#125;void insert(Tree* root, Tree * node)&#123; if (IsFullBinaryTree(root) || !IsFullBinaryTree(root-&gt;lchild))&#123; insert(root-&gt;lchild, node); return ; &#125; if (root-&gt;rchild ==NULL)&#123; root-&gt;rchild =node ; return ; &#125; insert(root-&gt;rchild, node) ;&#125;int main()&#123; Node* a = new Node(); a-&gt;value =1;&#125; 第二种思路，如果已知之前树的个数，那么可以使用前序遍历的方式，得到将要插入的的节点的位置，然后插入。123456789101112int insert(Tree *t, int n, struct Node *node);# n表示原来二叉树节点的个数# 前序遍历 void printTree(Tree* root)&#123; if(root ==NULL) &#123; return ; &#125; print(root-&gt;value) printTree(root-&gt;lchild) printTree(root-&gt;rchild)&#125; 第三种思路：因为是完全二叉树，那么插入的点只能在最下一层。于是我们可以去找最下一层的中间点，找到根的左子树的最右下的节点，如果这个点存在，那么说明，最下一层的左边已经填满，递归右子树，否则递归左子树。 参考文献http://www.voidcn.com/article/p-kbxsvnyq-yq.htmlhttps://www.toutiao.com/i6192546626911126017/https://blog.csdn.net/psc0606/article/details/48742239 inplace 去除连续的 0给定一个一维整数数组，不使用额外的空间，本地去掉数组中连续的0。123456789101112131415161718192021222324252627282930#include&lt;iostream&gt;using namespace std;int RemoveDuplicates(int* sortBuffer,int length)&#123; if(sortBuffer == NULL || length == 0) &#123; return false; &#125; int count = 0; for(int i = 1; i &lt; length; i++) &#123; if(sortBuffer[i] ==0 &amp;&amp; 0 == sortBuffer[i-1]) &#123; continue; &#125; else &#123; sortBuffer[count]=sortBuffer[i]; count++; &#125; &#125; return count; &#125;int main()&#123; int length =sizeof(array)/sizeof(int); &#125; 最大连续子数组和已知一个整数二维数组，求最大的子数组和(子数组的定义从左上角(x0,y0) 到右下角(x1,y1)的数组)先考虑一维整数数组的情况。最大连续子序列的DP动态转移方程为 1234567891011121314151617181920212223242526#include&lt;iostream&gt;using namespace std;int Max(int a, int b)&#123; return a&gt;b ?a:b;&#125;int FindGreatestSubarray(int *arr, int n)&#123; int sum =arr[0]; int max =arr[0]; for(int i =1; i&lt;n; i++)&#123; sum =Max(sum+arr[i], arr[i]); max =Max(sum, max) /** if(sum &gt;=max)&#123; max =sum; &#125; */ &#125; return max;&#125;int main()&#123; return 0;&#125;讲解链接：http://kubicode.me/2015/06/23/Algorithm/Max-Sum-in-SubMatrix/ 本题目的要求是从二位的数组中求解最大的子矩阵。我们可以将其转化成一维数组的问题。如果是二维数组可以压缩为一维数组（我当时也是不懂这里）。如果最大子矩阵和原矩阵等高，就可以这样压缩。12345678910111213141516171819202122232425262728293031323334353637383940414243#include&lt;stdio.h&gt;#include&lt;iostream&gt;#include&lt;vector&gt;using namespace std;#define inf 0x3f3f3f3fint Max(int a, int b)&#123; return a&gt;b? a:b;&#125;// 求解一维数组的最大连续子数列int FindGreatestSubarray(int *arr, int n)&#123; int sum =arr[0]; int max =arr[0]; for(int i =1;i&lt;n;i++)&#123; sum =Max(sum+arr[i], arr[i]) if(sum &gt;=max)&#123; max =sum; &#125; &#125; return max;&#125;int GreatestMatrix(int[][] arr, int rows, int cols)&#123; int maxVal =- inf for(int i =0 ; i &lt;rows; i++)&#123; vector&lt;int&gt; temp(arr[i]); maxVal =Max(maxVal, FindGreatestSubarray(temp)); // 得到第一行的最大和 // 将行的n个元素加到上一行，然后计算最大和 for(int j =i+1; j&lt;rows; j++)&#123; for(int k =0;k&lt;cols ;k++)&#123; temp[k] =arr[j][k]; &#125; // 依次0~k行的最大和 maxVal =Max(maxVal, FindGreatestSubarray(temp)) &#125; &#125;&#125;int main()&#123;&#125; 聚类(clustering )聚类是一种无监督学习(Unsupervised Learning)，该算法基于数据内部的特征寻找样本的自然族群(集群)。通常使用数据可视化来评价结果。应用场景：新闻聚类，文章推荐，细分客户。 最常见的聚类算法就是K均值(K-Means)：以空间中k个点为中心进行聚类，对最靠近他们的对象归类，通过迭代的方法，逐次更新各聚类中心的值，直到得到最好的聚类结果。最后希望达到的目的：聚类中的对象相似度较高，聚类之间的相似度比较低。 相似度计算：不同的算法需要的”相似度”是不一样的，对于空间中的点，我们一般选取欧式距离来进行衡量，认为距离越近，数据之间越相似。 堆排序堆是一棵顺序存储的完全二叉树，堆排序是一种树形选择排序，其时间复杂度为O(nlogn)，空间复杂度:对于记录较少的文件不推荐使用，对于较大的文件还是有效的.堆分为大根堆和小根堆。大根堆的要求是每个节点的值都不大于其父节点的值，即A[PARENT[i]] &gt;= A[i]。小根堆的要求是每个节点的值都不小于其父节点的值，即A[PARENT[i]] &lt;= A[i]。 堆的每次调整交换堆顶和最后一个元素，然后只是调整堆顶和堆顶的左右孩子树的关系。 这个是讲解视频 1234567891011121314151617181920212223242526272829303132333435# heap modifydef MAX_Heapify(heap, HeapSize, root): left =2* root+1 right = left +1 larger =root if left &lt;HeapSize and heap[larger] &lt;heap[left]: larger =left if right &lt; HeapSize and heap[larger] &lt;heap[right]: larger =right # if modify the larger then exchange it if larger != root: heap[larger], heap[root] =heap[root], heap[larger] MAX_Heapify(heap, HeapSize, larger)# Build the heapdef Build_MAX_Heap(heap): HeapSize =len(heap) # from the end to the begin for i in range((HeapSize -2)//2, -1,-1): MAX_Heapify(heap, HeapSize, i)# sort after building the heapdef HeapSort(heap): Build_MAX_Heap(heap) for i in range(len(heap)-1, -1, -1): heap[0], heap[i] =heap[i], heap[0] MAX_Heapify(heap, i, 0) return heapif __name__ =="__main__": a =[30, 50, 57, 77, 62, 78, 94, 80, 84] print(a) print("without sort but with build heap") Build_MAX_Heap(a) print(a) 补充：stack vs heap vs queue: 中文翻译的时候有偏差，最好使用因为来进行理解。stack 就是常见的栈， we say Last in first Out (LIFO) or First in Last out (FILO); 于此相对应的是 queue, the first person in line is the first person to get out of line. This is FIFO. 这个和数据结构中的 栈和队列是一一对应的。然后说一下 heap，是算法中的一种特殊的树形结构。Heap is a tree with some special property. That special property of the heap is, the value of a node must be &gt;= or &lt;= to its children. And one most important property of heap is all leaves must be at level h or at h-1. (where h is the height of the tree). This also called heap must be a complete binary tree. 参考文献https://blog.csdn.net/minxihou/article/details/51850001https://blog.csdn.net/chibangyuxun/article/details/53018294 KMP(字符串高效查找)假设两个字符串的长度分别是m，n(m&gt;n)，在长度为m中的字符串查找长度为n的字符串，最常见的方式是暴力求解，但是这个常规解法的时间复杂度是O(nm)。KMP通过一个O(n)的预处理，可以使得时间复杂度降为O(n+m).代码实现(视频讲解(科学上网))123456789101112131415161718192021222324def kmp_match(s, p): m, n =len(s) ,len(p) cur =0 table = partial_table(p) while cur &lt;= m-n: for i in range(n): if s[i+cur] != p[i]: cur += max(i -table[i-1], 1) break else: return True return Falsedef partial_table(p): prefix =set() postfix =set() ret =[0] for i in range(1, len(p)): prefix.add(p[:i]) postfix =&#123; p[j:i+1] for j in range(1, i+1)&#125; ret.append(len((prefix &amp; postfix or &#123;&apos;&apos;&#125;).pop())) # &amp;两个set求交集 return retprint(partial_table(&apos;ABCDABD&apos;))print(kmp_match(&quot;BBC ABCDAB ABCDABCDABDE&quot;, &quot;ABCDABD&quot;)) 参考文献https://www.cnblogs.com/fanguangdexiaoyuer/p/8270332.html 二叉树的遍历在python中二叉树的结构:12345class BinNode(): def __init__(self, val): self.value =val self.lchild =None self.rchild =None 先序遍历(preOrder)第一种思路是递归实现，第二种思路借助栈的结构来实现。栈的大小空间为O(h)，h为二叉树高度；时间复杂度为O(n)，n是树的节点的个数。123456789101112131415161718192021# 递归def preOrder(self, root): if root == None: return print(root.val) self.preOrder(root.lchild) self.preOrder(root.rchild)# 借助栈结构def preOrder(self, root): if root == None: return myStack =[] node =root while node or myStack: while node: print(node.val) myStack.append(node) node =node.lchild node =myStack.pop() node =node.rchild 中序遍历(inOrder)递归和非递归两种实现思路。入栈的顺序是一样的，只是改变的遍历(print())的顺序.123456789101112131415161718192021# 递归def inOrder(self, root): if root ==None: return self.inOrder(root.lchild) print(root.val) self.inOrder(root.rchild)# 借助栈结构def inOrder(self, root): if root ==None: return myStack =[] node =root while node or myStack: while node: myStack.append(node) node =node.lchild node = myStack.pop() print(node.val) node =node.rchild 后序遍历(post order)仍然是递归和非递归版本，非递归中使用两个stack,两个stack的后进先出等于一个先进先出。12345678910111213141516171819202122232425# 递归def postOrder(self, root): if root == None: return self.postOrder(root.lchild) self.postOrder(root.rchild) print(root.val)# 借助栈结构def postOrder(self, root): if root ==None: return myStack1 =[] myStack2 =[] node =root myStack1.append(node) while myStack1: node =myStack1.pop() if node.lchild: myStack1.append(node.lchild) if node.rchild: myStack1.append(node.rchild) myStack2.append(node) while myStack2: print(myStack2.pop().val) 层序遍历使用到了队列的思想，先进先出。实际上，用的是Python中list.pop(0).注意默认是list.pop(-1),也就是默认弹出的是最后一个元素。1234567891011121314def levelOrder(self, root): if root ==None: return myQueue =[] node =root myQueue.append(node) while myQueue: # remove and return item at index (default last) node =myQueue.pop(0) print(node.val) if node.lchild != None: myQueue.append(node.lchild) if node.rchild != None: myQueue.append(node.rchild) 参考文献https://blog.yangx.site/2016/07/22/Python-binary-tree-traverse/ 旋转数组找最小值把一个数组最开始的若干个元素搬到数组的末尾，我们称之为数组的旋转。 输入一个非递减排序的数组的一个旋转，输出旋转数组的最小元素。 例如数组{3,4,5,1,2}为{1,2,3,4,5}的一个旋转，该数组的最小值为1。 NOTE：给出的所有元素都大于0，若数组大小为0，请返回0。 暴力求解大多数的问题都是可以暴力求解的–鲁迅。(In China 凡是不知道是谁说，都可以说是鲁迅说的; In US，凡是不知道谁说的，as said by Albert Einstein)因为原来的数组假设是增序，所以如果出现了的某一个元素比上一个元素小，该元素就是这个序列中的最小值。(这个情况具有唯一性吧).时间复杂度O(N)123456789101112def minNumberInRotateArray(rotateArray): arr =rotateArray # just because of laziness if not arr: return 0 if len(arr) ==2: return arr[1] num =arr[0] for i in range(1, len(arr)): if arr[i] &gt;= num: num =arr[i] else: return arr[i] 递归版本旋转数组也是一种有序数组，时间复杂度O(N)…，面试官说改进吧…使用二分法降到O(logN)。 In big-O() notation, constant factors are removed. Converting from one logarithm base to another involves multiplying by a constant factor. 所以这关键是后面的变量而不是以2 或者e 为底。这两种写法都是成立的。 So O(log N) is equivalent to O(log2 N) due to a constant factor.12345678910111213def minNumberInRotateArray(rotateArray): arr = rotateArray if not arr: return 0 if len(arr) ==2: return arr[1] mid =int(len(arr) /2) if arr[mid] &gt; arr[0]: return minNumberInRotateArray(arr[mid:]) elif arr[mid] &lt;arr[0]: return minNumberInRotateArray(arr[:mid+1]) else: return minNumberInRotateArray(arr[1:]) 非递归版本递归版本占的内存比较多，改进吧..于是非递归的版本就出来了。需要注意的是该版本的判断比较条件(其中一点是和 arr[right]进行比较)一定要小心，都是小坑…这种解法关键是需要找到非减序列（和原序列相同的形式），然后就变得可预测，可以排除这部分其他的数字。 下面这个确实是是正确的代码，注意体会细节。非递减的array，然后是从后往前比较的。if lese 中的条件是可以调换的，虽然这话听起来像是废话。这说明两个条件的判断的顺序不应该产生不同的结果。1234567891011121314def minNumberInRotateArray(rotateArray): arr =rotateArray left =0 right =len(arr) -1 while left &lt; right: mid = int((left+ right)/2) if arr[mid] &gt;arr[right]: left =mid+1 # 不包含mid 因为mid 绝不可能是 最小值 elif arr[mid] &lt;arr[right]: right =mid # 包含mid 因为mid 可能是最小值 else: right -=1 # 这个也是可以换成 left +=1 ，只要是能够渐进的 return arr[left] 文章的小标题是求解最小(大)值，上述讲述的都是最小值。如果求解最大值，稍微修改一下特殊情况的判断条件，将返回的index-1 即可。因为最小值的位置是”某一个元素比上一个元素小”，那么 index-1 之后这个元素就是该数组序列中最大的。 参考文献https://blog.csdn.net/u010005281/article/details/79823154 单链表反转单链表的反转有循环迭代和递归两种方法。单链表节点123456class Node(object): def __init__(self): self.value =None self.next =None def __str__(self): return str(self.value) 循环迭代循环迭代需要维持三个变量：pre, head, next。pre是head的pre，next是head的next.(废话)1234567891011def reverse_Linkedlist(head): if not head or not head.next : #空指针或者只有一个结点 return head pre =None # 需要创建一个None 作为最后的指向 while head: next = head.next head.next =pre pre = head head =next # 最后一次循环迭代 Head==None，而pre指向了头结点 return pre 递归一开始正常情况下不会执行if判断，利用递归走到链表的末端，new_head的值没有发生改变，为链表的最后一个节点，反转之后就成为了新链表的head。12345678def reverse_Linkedlist(head): if not head or not head.next: return head new_head = reverse_Linkedlist(head.next) # 将当前节点设置为后面节点的后续节点 head.next.next =head head.next =None return new_head 测试12345678910111213141516171819202122232425if __name__ == &quot;__main__&quot;: three = Node() three.value =3 two =Node() two.value =2 two.next =three one =Node() one.value =1 one.next =two head =Node() head.value =0 head.next =one &quot;&quot;&quot; while head: print(head.value, ) head =head.next print(&quot;******&quot;) &quot;&quot;&quot; newhead = reverse_Linkedlist(head) while newhead: print(newhead.value) newhead =newhead.next 参考文献https://foofish.net/linklist-reverse.html]]></content>
      <categories>
        <category>算法</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Introduction to Ensemble]]></title>
    <url>%2F2018%2F06%2F13%2FIntroduction-to-Ensemble%2F</url>
    <content type="text"><![CDATA[虽然在Titanic Challenge博客中介绍了模型融合(Ensemble)，但是其是以特征提取为主旋律，所以这篇博客将结合实战从最基本的Ensemble到比较复杂的Ensemble,最后安利一个用于模型融合的package. 在文中可能出现一些概念，如果不太熟悉，可以先去文末找找看，如果我没有补充，那么请自行Google吧。为了不影响阅读的流畅性，我可能在其中不会涉及很多概念解释，比如说ROC曲线和AUC值。好，我们进入正文。 Dataset本文书写过程中沿用参考博客(Introduction to Python Ensembles)的数据集。可以去这里下载，当然推荐使用原作者处理之后的数据集，you can find here。 简单介绍一下这个数据集：Federal Election Commission这个组织收集了2007到2016年的donations记录，最后得出 When Scientists Donate To Politicians, It’s Usually To Democrats这样的结论。 好了，我不想在数据集这里花太多时间，即使你不太明白数据集的具体含义，完全不影响下文的阅读，因为你很快就会发现下文并没有进行很多和原数据集相关的内容，更多的是模型融合。当然你如果能够看懂，可以感受一下上述结论的有趣之处。 Give me codes:1234567891011121314151617181920import numpy as npimport pandas as pdimport matplotlib.pyplot as plt%matplotlib inline# import data# Always good to set a seed for reproducibilitySEED = 222np.random.seed(SEED)df = pd.read_csv(&apos;input.csv&apos;)from sklearn.model_selection import train_test_splitfrom sklearn.metrics import roc_auc_scoredef get_train_test(test_size= 0.95): y =1*(df.cand_pty_affiliation ==&apos;REP&apos;) X = df.drop([&quot;cand_pty_affiliation&quot;], axis=1) X = pd.get_dummies(X, sparse=True) X.drop(X.columns[X.std() == 0], axis=1, inplace=True) return train_test_split(X, y, test_size=test_size, random_state=SEED)xtrain, xtest, ytrain, ytest = get_train_test()df.head() 简单看一下数据长什么样子，虽然有人可能不太懂。 Begin with ensemble之前的博客主要从ensemble分类的角度阐述，现在从概念的角度阐述。Ensemble: combining predictions from several models averages out idiosyncratic(怪异的) errors and yield better overall predictions.(有时候我觉得英文说得很清楚，所以就不翻译成中文了，求不被打。) 简单的说，就是可以防止过拟合。当过拟合时，边界曲线就回去过分考虑某一个或某一些少数的点，这时候 ensemble通过某种combine机制，然后得到一个泛化性能比较好的边界曲线，也就是比较好的模型。 Decision Tree首先我们从 decision tree开始。A decision tree, which is a tree of if-then rules. The deeper the tree, the more complex the patterns it can capture, but the more prone to overfitting it will be. Because of this, we will need an alternative way of building complex models of decision trees, and an ensemble of different decision trees is one such way.我们先使用 depth =112345678from IPython.display import Imagefrom sklearn.metrics import roc_auc_scorefrom sklearn.tree import DecisionTreeClassifiert1 = DecisionTreeClassifier(max_depth=1, random_state=SEED)t1.fit(xtrain, ytrain)p = t1.predict_proba(xtest)[:, 1]print(&quot;Decision tree ROC-AUC score: %.3f&quot; % roc_auc_score(ytest, p)) Decision tree ROC-AUC score: 0.672 发现结果不太理想，加深depth.1234t2 =DecisionTreeClassifier(max_depth=3, random_state=SEED)t2.fit(xtrain, ytrain)p =t2.predict_proba(xtest)[:, 1]print(&quot;Decision tree ROC-AUC score: %.3f&quot; % roc_auc_score(ytest, p)) Decision tree ROC-AUC score: 0.751 由于我们最后要ensemble，而这要求我们要构造有差异但每个不是那么差的模型。首先我们考虑到使用不同的数据集。1234567xtrain_slim =xtrain.drop([&apos;transaction_amt&apos;], axis=1)xtest_slim =xtest.drop([&apos;transaction_amt&apos;], axis=1)t3=DecisionTreeClassifier(max_depth=3, random_state=SEED)t3.fit(xtrain_slim, ytrain)p =t3.predict_proba(xtest_slim)[:,1]print(&apos;Decision tree ROC-AUC score:&#123;&#125;&apos;.format(roc_auc_score(ytest, p))) Decision tree ROC-AUC score:0.7403182587884118通过corr()来检验两者的相关性(差异性)123p1 =t2.predict_proba(xtest)[:,1]p2 =t3.predict_proba(xtest_slim)[:,1]pd.DataFrame(&#123;&apos;full_data&apos;:p1, &apos;red_data&apos;:p2&#125;).corr() 发现有一定的相关性，但是还是可以容忍的。于是开始融合。1234p1 = t2.predict_proba(xtest)[:, 1]p2 = t3.predict_proba(xtest_slim)[:, 1]p = np.mean([p1, p2], axis=0)print(&quot;Average of decision tree ROC-AUC score: %.3f&quot; % roc_auc_score(ytest, p)) Average of decision tree ROC-AUC score: 0.783我们发现：两个旗鼓相当（0.74 0.73）的可以得到一个更好的结果，可以减少决策失误 的平均是0.78。 需要注意的是我们在构造第二个模型时候通过drop()丢掉一个feature，反而得到一个更好的模型。于是乎，我们想通过使用不同的子集（不同的特征）构造不同的模型，是不是能得到更好的模型？ Random Forest(Bagging)A fast approach that works well in practice is to randomly select a subset of features, fit one decision tree on each draw and average their predictions.This process is known as bootstrapped averaging (often abbreviated bagging), and when applied to decision trees, the resultant model is a Random Forest. (我觉得原作者比我说的清楚，借用了)我的理解，在上面小节中我们使用的是Decision Tree,在实际应用中发现有差异的多个子树的效果要更好一些。而实现这个途径快速的方法就是 Random Forest。Random 在这里表示任意几个子树(特征)，然后这些Tree组成了Forest。123456789from sklearn.ensemble import RandomForestClassifierrf =RandomForestClassifier( n_estimators=10, max_features= 3, random_state=SEED)rf.fit(xtrain, ytrain)p =rf.predict_proba(xtest)[:, 1]print(&apos;Average of decision tree ROC-AUC score:&#123;&#125;&apos;.format(roc_auc_score(ytest, p))) Average of decision tree ROC-AUC score:0.844018408542404这就是叫做”质的飞跃”从那个0.783-&gt; 0.844(将近6个百分点，好吧，有点神经质了…)From nobody to somebody, we are on something.. Ensemble of various models可以看出上述模型中，最后的模型（Random Forest）的子模型(Decision Tree)。但是子模型并不是局限树这一种结构，我们更多的选择：linear models, kernel-based models, non-parametric models, neural networks or even other ensembles! 为了避免代码的冗余构造了以下的helper function.123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657# A host of Scikit-learn modelsfrom sklearn.svm import SVC, LinearSVCfrom sklearn.naive_bayes import GaussianNBfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifierfrom sklearn.linear_model import LogisticRegressionfrom sklearn.neighbors import KNeighborsClassifierfrom sklearn.neural_network import MLPClassifierfrom sklearn.kernel_approximation import Nystroemfrom sklearn.kernel_approximation import RBFSamplerfrom sklearn.pipeline import make_pipelinedef get_models(): &quot;&quot;&quot;Generate a library of base learners.&quot;&quot;&quot; nb = GaussianNB() svc = SVC(C=100, probability=True) # C越大边界越复杂，会导致过拟合 knn = KNeighborsClassifier(n_neighbors=3) # KNN算法寻找训练数据中的K个最近的数据，它使用指向最多的那个类别来作为预测的输出。 lr = LogisticRegression(C=100, random_state=SEED) # 对于这个 c 能知道的就是正则化系数 smaller values specify stronger regularization. nn = MLPClassifier((80, 10), early_stopping=False, random_state=SEED) gb = GradientBoostingClassifier(n_estimators=100, random_state=SEED) # 子模型的数量，默认是100, gbc通常 robust to over-fitting, so a large number results in better performance rf = RandomForestClassifier(n_estimators=10, max_features=3, random_state=SEED) models = &#123;&apos;svm&apos;: svc, &apos;knn&apos;: knn, &apos;naive bayes&apos;: nb, &apos;mlp-nn&apos;: nn, &apos;random forest&apos;: rf, &apos;gbm&apos;: gb, &apos;logistic&apos;: lr, &#125; return models def train_predict(model_list): P =np.zeros((ytest.shape[0], len(model_list))) P =pd.DataFrame(P) print(&apos;Fitting models&apos;) cols =list() for i, (name, m) in enumerate(models.items()): print(&quot;%s...&quot; % name, end=&quot; &quot;, flush=False) m.fit(xtrain, ytrain) P.iloc[:, i] =m.predict_proba(xtest)[:, 1] cols.append(name) print(&apos;Done&apos;) P.columns =cols print(&apos;Done.\n&apos;) return P def score_models(P, y): &quot;&quot;&quot;Score model in predication DF&quot;&quot;&quot; print(&apos;Scoring models&apos;) for m in P.columns: score =roc_auc_score(y, P.loc[:, m]) print(&quot;%-26s: %.3f&quot; % (m, score)) print(&apos;Done, \n&apos;) let’s go…123models =get_models()P =train_predict(models)score_models(P, ytest) This is our base line.Gradient Boosting Machine(GBM) 果然名不虚传, does best我们来分析一下模型之间的相关性，原作者使用的mlens package(You can install it with: pip install mlens)，我这里用的是seaborn(install it with: pip install seaborn).在检查相关性(Pearson相关性:衡量两个数据集合的线性相关性)时候，我们使用的是经过处理的相关性。具体说来可以称之为 error correlation,详细见代码。 12345678# look at error correlations is more promising, errors are significantly correlated import seaborn as snsplt.subplots(figsize=(10,8)) corrmat = P.apply(lambda pred: 1*(pred &gt;= 0.5) - ytest.values).corr()sns.set(font_scale=1.5) hm = sns.heatmap(corrmat, cbar=True, annot=True, square=True, fmt=&apos;.2f&apos;, annot_kws=&#123;&apos;size&apos;: 10&#125;) plt.title(&apos;Pearson Correlation of Features&apos;, y=1.05, size=15) plt.show() 预测值和真实值之间的差异称之为error，查看error的 Pearson correlation，效果更加明显。1print(&quot;Ensemble ROC-AUC score: %.3f&quot; % roc_auc_score(ytest, P.mean(axis=1))) Ensemble ROC-AUC score: 0.884结果高于每一个单独的模型，但是不是那么明显。 Visualize Curve ROC(helper function)我们注意到之前使用的所有的结果的评价标准都是 roc_auc_score，但是并没有提及这是什么。当然在正文中也不打算解释，如果不是很清楚，可以查看本文最后补充概念.简单来说AUC可以用来衡量”二分问题”的泛化能力，是一种评价指标。我们这里想说的是 visualize Curve ROC,可视化。12345678910111213141516171819202122# a helper function for roc_curvefrom sklearn.metrics import roc_curvedef plot_roc_curve(ytest, P_base_learners, P_ensemble, labels, ens_label): &quot;&quot;&quot;Plot the roc curve for base learners and ensemble.&quot;&quot;&quot; plt.figure(figsize=(10, 8)) plt.plot([0, 1], [0, 1], &apos;k--&apos;) cm = [plt.cm.rainbow(i) for i in np.linspace(0, 1.0, P_base_learners.shape[1] + 1)] for i in range(P_base_learners.shape[1]): p = P_base_learners[:, i] fpr, tpr, _ = roc_curve(ytest, p) plt.plot(fpr, tpr, label=labels[i], c=cm[i + 1]) fpr, tpr, _ = roc_curve(ytest, P_ensemble) plt.plot(fpr, tpr, label=ens_label, c=cm[0]) plt.xlabel(&apos;False positive rate&apos;) plt.ylabel(&apos;True positive rate&apos;) plt.title(&apos;ROC curve&apos;) plt.legend(frameon=False) plt.show()plot_roc_curve(ytest, P.values, P.mean(axis=1), list(P.columns), &quot;ensemble&quot;) 我们顺便把刚才的模型可视化，发现ensemble的AUC是最大的，意味这泛化性能是最好，这也是符合我们的认知的 。 Beyond ensembles as a simple average我们回到主线上，在上一个模型中我们提及最后的 ensemble的结果是好于每个单独的模型，但是没有那么突出。根据ROC曲线我们也可以看出，有的模型(KNN)在这里表现的没有那么好，我们在想是不是由于因为这个而拉底了最后的结果，当然这只是猜测，于是我们找到了 try的方向。可能第一直觉去掉这个模型再进行融合，在这个实验表明该策略最后的结果0.883，相比与0.884，你懂得，并没有变好。我们还有一种策略:learn a sensible set of weights to use when averaging predictions.让模型自己去学习如何调整各个模型之间的比例。 Learning to combine predications为了让模型自学习各个之间的预测比例，我们引入了 meta learner(meta是元，理解为最基础的) to learn how to best combine these predictions. 除此之外，我们将使用不同的数据集，像Random Forest使用不同的数据子集(不同的特征组成的数据集)。于是我们需要a method for splitting the training data between the base learners and the meta learner.12345678910base_learners =get_models()meta_learner = GradientBoostingClassifier( n_estimators=1000, loss=&quot;exponential&quot;, max_features=4, max_depth=3, subsample=0.5, learning_rate=0.005, random_state=SEED) 使用最强模型GBM作为 meta learner并定义好 base_learners.To keep things simple, we split the full training set into a training and prediction set of the base learners. This method is sometimes referred to as Blending. Unfortunately, the terminology differs between communities, so it’s not always easy to know what type of cross-validation the ensemble is using.12345678910# sefine a procedure for generating train and test setsxtrain_base , xpred_base, ytrain_base, ypred_base =train_test_split(xtrain, ytrain, test_size=0.5, random_state=SEED)def train_base_learners(base_learners, inp, out, verbose =True): if verbose: print(&apos;Fitting models&apos;) for i, (name, m) in enumerate(base_learners.items()): if verbose: print(&quot;%s...&quot; % name, end=&quot; &quot;, flush=False) m.fit(inp, out) if verbose: print(&apos;Done.&apos;)train_base_learners(base_learners, xtrain_base, ytrain_base) (注意我们只是使用了50%的data去train, test_size =0.5)1234567891011121314def predict_base_learners(pred_base_learners, inp, verbose=True): &quot;&quot;&quot;Generate a prediction matrix.&quot;&quot;&quot; P = np.zeros((inp.shape[0], len(pred_base_learners))) if verbose: print(&quot;Generating base learner predictions.&quot;) for i, (name, m) in enumerate(pred_base_learners.items()): if verbose: print(&quot;%s...&quot; % name, end=&quot; &quot;, flush=False) p = m.predict_proba(inp) # With two classes, need only predictions for one class P[:, i] = p[:, 1] if verbose: print(&quot;done&quot;) return PP_base = predict_base_learners(base_learners, xpred_base) 现在我们得到了base_learners的predications，接下来我们应该使用的是这个流程，在base learners的基础上（类似两层结构了 meta learner 学习的如何搭配这些base learner使得最后的结果 predications最大）进行训练，而不是训练原来的数据集。 1meta_learner.fit(P_base, ypred_base) 1234567#meta_learner.fit(P_base, ypred_base)def ensemble_predict(base_learners, meta_learner, inp, verbose=True): &quot;&quot;&quot;Generate predictions from the ensemble.&quot;&quot;&quot; P_pred = predict_base_learners(base_learners, inp, verbose=verbose) return P_pred, meta_learner.predict_proba(P_pred)[:, 1]P_pred, p = ensemble_predict(base_learners, meta_learner, xtest)print(&quot;\nEnsemble ROC-AUC score: %.3f&quot; % roc_auc_score(ytest, p)) 最后的结果是0.881 相比与之前最好的0.884（使用相同的数据集，没有进行 meta_learner的操作）这是因为我们在划分数据集的使用只是使用了0.5的数据集，而前者的模型使用了全部的train sets。有人不免疑问：为什么不使用全部的data？我的理解是划分xtrain_base , xpred_base, ytrain_base, ypred_base使用的是 train_test_split()，总是需要设定一个数值的，即使train_size =0.01，也是没有用到全部的datas. Training with cross-validation我们使用cross-validation 来缓解上面那个问题。During cross-validated training of the base learners, a copy of each base learner is fitted on K−1 folds, and predict the left-out fold. This process is iterated until every fold has been predicted. The more folds we specify, the less data is being left out in each training pass. This makes cross-validated predictions less noisy and a better reflection of performance during test time. The cost is significantly increased training time. Fitting an ensemble with cross-validation is often referred to as stacking, while the ensemble itself is known as the Super Learner. To understand how cross-validation works, we can think of it as an outer loop over our previous ensemble. The outer loop iterates over K distinct test folds, with the remaining data used for training. The inner loop trains the base learners and generate predictions for the held-out data. Here’s a simple stacking implementation:12345678910111213141516171819202122232425262728293031323334353637383940414243from sklearn.base import clonedef stacking(base_learners, meta_learner, X, y, generator): &quot;&quot;&quot;Simple training routine for stacking.&quot;&quot;&quot; # Train final base learners for test time print(&quot;Fitting final base learners...&quot;, end=&quot;&quot;) train_base_learners(base_learners, X, y, verbose=False) print(&quot;done&quot;) # Generate predictions for training meta learners # Outer loop: print(&quot;Generating cross-validated predictions...&quot;) cv_preds, cv_y = [], [] for i, (train_idx, test_idx) in enumerate(generator.split(X)): fold_xtrain, fold_ytrain = X[train_idx, :], y[train_idx] fold_xtest, fold_ytest = X[test_idx, :], y[test_idx] # Inner loop: step 4 and 5 fold_base_learners = &#123;name: clone(model) for name, model in base_learners.items()&#125; train_base_learners( fold_base_learners, fold_xtrain, fold_ytrain, verbose=False) fold_P_base = predict_base_learners( fold_base_learners, fold_xtest, verbose=False) cv_preds.append(fold_P_base) cv_y.append(fold_ytest) print(&quot;Fold %i done&quot; % (i + 1)) print(&quot;CV-predictions done&quot;) # Be careful to get rows in the right order cv_preds = np.vstack(cv_preds) cv_y = np.hstack(cv_y) # Train meta learner print(&quot;Fitting meta learner...&quot;, end=&quot;&quot;) meta_learner.fit(cv_preds, cv_y) print(&quot;done&quot;) return base_learners, meta_learner 尤其在cv_preds和cv_y的维度问题上，注意小心。The basic difference between blending and stacking is therefore that stacking allows both base learners and the meta learner to train on the full data set. Using 2-fold cross-validation, we can measure the difference this makes in our case:1234567from sklearn.model_selection import KFold# Train with stackingcv_base_learners, cv_meta_learner = stacking( get_models(), clone(meta_learner), xtrain.values, ytrain.values, KFold(2))P_pred, p = ensemble_predict(cv_base_learners, cv_meta_learner, xtest, verbose=False)print(&quot;\nEnsemble ROC-AUC score: %.3f&quot; % roc_auc_score(ytest, p)) Ensemble ROC-AUC score: 0.889这是目前为止最好的结果了。Stacking yields a sizeable increase in performance: in fact, it gives us our best score so far. This outcome is typical for small and medium-sized data sets, where the effect of blending can be severe. As the data set size increases, blending and stacking performs similarly. Use packages快要接近尾声了，在文章的开始，我们提及要安利一个resemble好用的package. So, it’s now.Here, we use Ensemble and build our previous generalized ensemble, but now using 10-fold cross-validation:123456789101112131415161718192021from mlens.ensemble import SuperLearner# Instantiate the ensemble with 10 foldssl = SuperLearner( folds=10, random_state=SEED, verbose=2, backend=&quot;multiprocessing&quot;)# Add the base learners and the meta learnersl.add(list(base_learners.values()), proba=True) sl.add_meta(meta_learner, proba=True)# Train the ensemblesl.fit(xtrain, ytrain)# Predict the test setp_sl = sl.predict_proba(xtest)print(&quot;\nSuper Learner ROC-AUC score: %.3f&quot; % roc_auc_score(ytest, p_sl[:, 1])) So simple!1plot_roc_curve(ytest, p.reshape(-1, 1), P.mean(axis=1), [&quot;Simple average&quot;], &quot;Super Learner&quot;) 发现super learner(meta learner)和 basic learner的mean的结果已经不相上下了。super learner得到了很好的训练。 ROC曲线和AUC值ROC全称是“受试者工作特征”（Receiver Operating Characteristic）。ROC曲线的面积就是AUC（Area Under the Curve）。AUC用于衡量“二分类问题”机器学习算法性能（泛化能力）。说到这里不得不提及就是经常使用的符号，TP(True Positive), FP(False, Positive), TN(True Negative),FN(False Negative)。他们是根据原来真实数据和预测类别进行的排列组合（当然这是针对二分问题）。 ROC 曲线: ROC 曲线（接收者操作特征曲线）是一种显示分类模型在所有分类阈值下的效果的图表。该曲线绘制了以下两个参数：真正例率 和 假正例率。 真正例率 (TPR) 是召回率的同义词，因此定义如下：$$T P R = \frac { T P } { T P + F N }$$假正例率 (FPR) 的定义如下：$$F P R = \frac { F P } { F P + T N }$$ ROC 中 TPR =(True positive / ( True positive +False negative)), 那个false negative 也是真实的类别，只不过是错误的当做了 negative（false negative） 2针对一个二分类问题，将实例分成正类(postive)或者负类(negative)。但是实际中分类时，会出现四种情况. 若一个实例是正类并且被预测为正类，即为真正类(True Postive TP) 若一个实例是正类，但是被预测成为负类，即为假负类(False Negative FN) 若一个实例是负类，但是被预测成为正类，即为假正类(False Postive FP) 若一个实例是负类，但是被预测成为负类，即为真负类(True Negative TN) ROC 曲线是如何绘制的: 采用不同分类阈值时的 TPR 与 FPR。降低分类阈值会导致将更多样本归为正类别，从而增加假正例（FP）和真正例（TP）的个数，可以理解为降低了被认为正确的标准，数量自然就增多了。下图显示了一个典型的 ROC 曲线。注意观察图中TPR 和 FPR是呈正相关的，验证了上述的结论。为了计算 ROC 曲线上的点，我们可以使用不同的分类阈值多次评估逻辑回归模型，但这样做效率非常低。幸运的是，有一种基于排序的高效算法可以为我们提供此类信息，这种算法称为曲线下面积。 ROC曲线，一般适用于你的分类器输出一个“概率值”，即这个样本属于某个类的概率是多少。 如此的话，你就需要设定一个阈值， 大于这个阈值属于正类，小于这个阈值属于负类。 从而，对于这个阈值P0， 就会得到对应的TPR, FPR, 也就是ROC曲线上的一个点，你设置不同的阈值，就会得到不同的TPR, FPR， 从而构成ROC曲线。 通常来说 阈值降低，即进入正类的门槛变低， TPR会变大，但是FPR也会变大， 看他们谁变的快。 1234567891011121314gbc = GradientBoostingClassifier()gbc.fit(x_train, y_train)resu = gbc.predict(x_test) #进行预测y_pred_gbc = gbc.predict_proba(x_test)[:,1] ###这玩意就是预测概率的fpr, tpr, threshold = roc_curve(y_test, y_pred_gbc) ###画图的时候要用预测的概率，而不是你的预测的值plt.plot(fpr, tpr, 'b', label='AUC = %0.2f' % rocauc)#生成ROC曲线plt.legend(loc='lower right')plt.plot([0, 1], [0, 1], 'r--')plt.xlim([0, 1])plt.ylim([0, 1])plt.ylabel('真正率')plt.xlabel('假正率')plt.show() 接着我们计算TRP(True Positive Radio)，FRP(False Positive Ratio)用于描述ROC曲线，分别表示该曲线的Y轴，X轴。TPR=TP/(TP+FN)FPR=FP/(FP+TN)最后就形成了类似这样的图像(来源于上述的训练模型) 我们希望的结果是TRU越大（接近1），FRU越小（接近0）。AUC的值是ROC所覆盖的面积，当AUC越大时候，分类器的效果越好。从图中可以看出模型(ensemble)的面积是最大的，分类效果也是最好的。关于该图像还有一点，如果你的曲线拟合对角线（图中虚线），那么相当于随机猜测。 TakeOff If you have an imbalanced dataset accuracy can give you false assumptions regarding the classifier’s performance, it’s better to rely on precision and recall, in the same way a Precision-Recall curve is better to calibrate the probability threshold in an imbalanced class scenario as a ROC curve. ROC Curves: summarise the trade-off between the true positive rate and false positive rate for a predictive model using different probability thresholds. Precision-Recall curves: summarise the trade-off between the true positive rate and the positive predictive value for a predictive model using different probability thresholds. ROC curves are appropriate when the observations are balanced between each class, whereas precision-recall curves are appropriate for imbalanced datasets. In both cases the area under the curve (AUC) can be used as a summary of the model performance. Why a ROC curve cannot measure well? The Receiver Operating Characteristic (ROC) curves plot FPR vs. TPR as shown below. Because TPR only depends on positives, ROC curves do not measure the effects of negatives. The area under the ROC curve (AUC) assesses overall classification performance . AUC does not place more emphasis on one class over the other, so it does not reflect the minority class well. Davis and Goadrich in this paper propose that Precision-Recall (PR) curves will be more informative than ROC when dealing with highly skewed datasets. The PR curves plot precision vs. recall (FPR). Because Precision is directly influenced by class imbalance so the Precision-recall curves are better to highlight differences between models for highly imbalanced data sets. When you compare different models with imbalanced settings, the area under the Precision-Recall curve will be more sensitive than the area under the ROC curve. 最强讲解 how to handle unbalanced dataImbalanced data typically refers to a problem with classification problems where the classes are not represented equally. The accuracy paradox is the name for the exact situation in the introduction to this post. Data approachOversample minority class and Undersample majority class Over-sampling increases the number of minority class members in the training set. The advantage of over-sampling is that no information from the original training set is lost, as all observations from the minority and majority classes are kept. On the other hand, it is prone to overfitting. (You can add copies of instances from the under-represented class called over-sampling or more formally sampling with replacement) Under-sampling, on contrary to over-sampling, aims to reduce the number of majority samples to balance the class distribution. Since it is removing observations from the original data set, it might discard useful information. (You can delete instances from the over-represented class, called under-sampling.) Some Rules of Thumb Consider testing under-sampling when you have an a lot data (tens- or hundreds of thousands of instances or more) Consider testing over-sampling when you don’t have a lot of data (tens of thousands of records or less) Consider testing random and non-random (e.g. stratified) sampling schemes. Consider testing different resampled ratios (e.g. you don’t have to target a 1:1 ratio in a binary classification problem, try other ratios) Try Different Algorithms基于树的这种结构的模型还是表现比较给力的。That being said, decision trees often perform well on imbalanced datasets. The splitting rules that look at the class variable used in the creation of the trees, can force both classes to be addressed. Try Penalized ModelsPenalized classification imposes an additional cost on the model for making classification mistakes on the minority class during training. These penalties can bias the model to pay more attention to the minority class. Try Changing Your Performance Metric 使用 precision and recall curves or F1 去评价你的网络效果 Precision: A measure of a classifiers exactness. Recall: A measure of a classifiers completeness F1 Score (or F-score): A weighted average of precision and recall. 而 ROC curves 通常不是一个最好的选择。 https://medium.com/anomaly-detection-with-python-and-r/sampling-techniques-for-extremely-imbalanced-data-281cc01da0a8 GBC参数这些参数中，类似于Adaboost，我们把重要参数分为两类，第一类是Boosting框架的重要参数，第二类是弱学习器即CART回归树的重要参数。n_estimators: 也就是弱学习器的最大迭代次数，或者说最大的弱学习器的个数。learning_rate: 即每个弱学习器的权重缩减系数ν，也称作步长对于分类模型，有对数似然损失函数”deviance”和指数损失函数”exponential”两者输入选择。默认是对数似然损失函数”deviance”。一般来说，推荐使用默认的”deviance”。它对二元分离和多元分类各自都有比较好的优化。而指数损失函数等于把我们带到了Adaboost算法。对于回归模型，有均方差”ls”, 绝对损失”lad”, Huber损失”huber”和分位数损失“quantile”。默认是均方差”ls”。一般来说，如果数据的噪音点不多，用默认的均方差”ls”比较好。如果是噪音点较多，则推荐用抗噪音的损失函数”huber”。而如果我们需要对训练集进行分段预测的时候，则采用“quantile”。max_features:可以使用很多种类型的值，默认是”None”,意味着划分时考虑所有的特征数.subsample: 选择小于1的比例可以减少方差，即防止过拟合，但是会增加样本拟合的偏差，因此取值不能太低。推荐在[0.5, 0.8]之间，默认是1.0，即不使用子采样。 参考文献GBC参数设置ROC曲线和AUC值Introduction to Python Ensembles]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>ROC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[对抗生成网络实验对比]]></title>
    <url>%2F2018%2F06%2F05%2F%E5%AF%B9%E6%8A%97%E6%80%A7%E7%94%9F%E6%88%90%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[GAN模型是典型的隐式无监督生成模型，建模过程中没有利用到数据的语义标签。但在实际应用中生成模型的可控性至关重要，根据标签生成对可控有样本更具实际应用价值。图像生成模型被广泛应用于数据增强、风格转换和数据补全领域，需要可控且语义完备的生成模型。条件GAN模型在GAN模型建模思路的基础上，将语义标签加入了建模过程，将无监督生成模型转变为有监督的条件生成模型。条件GAN模型具体包括Conditional GAN模型、Semi-GAN模型和AC-GAN模型。 实验模型介绍（1）Conditional GAN模型GAN模型中判别器D对输入的数据样本的来源进行判别，是典型的判别模型流程。如果在GAN框架中加入有监督信息来辅助训练，如图像的类别信息来辅助判别器D进行判别，则会帮助生成更加真实的样本。其中最初的尝试方式是Conditional GAN模型，结构如图所示。Conditional GAN模型在生成器G和判别器D输入中都加入了标签信息，试图让生成器G学习到从数据标签y到样本x的映射，让判别器D学习对样本x和标签y的组合进行判别。与GAN模型相比，Conditional GAN模型增加了标签信息的输入将模型转变为条件生成模型，在一定程度上提高了模型的稳定性。Conditional GAN模型训练过程中，判别器D对样本x和标签y的类别组合进行训练，并没有输入样本x和标签y的类别错误组合进行训练，因此模型并没有学习样本x和标签y的联合分布。（2）Semi-GAN模型Conditional GAN模型利用了标签信息进行建模，但没有对标签语义的信息进行表征，导致模型能够学到的信息有限。在生成模型过程中，如果判别器D能够明确指出生成样本的类别错误，则可为生成器G提供更加精确的梯度信息，最终能生成更加真实的样本。Semi-GAN基于此思路进行改进，具体结构如图9所示。Semi-GAN模型在Conditional GAN模型的基础上，对判别器D的分类输出进行细化加入了半监督学习过程。（3）AC-GAN模型与Conditional GAN模型相比，Semi-GAN模型中判别器D能够判别真实样本的来源，增强了判别器D的判别能力。但研究表明过强的判别信息会影响生产样本的质量，具体原因为Semi-GAN模型的建模过程为半监督分类过程，目标优化函数为无监督分类和有监督分类目标函数之和。若判别器D的监督分类信息过强，则会削弱判别器D对样本来源的判别能力。Conditional GAN模型能够生成指定类别的样本，Semi-GAN模型能够判别样本的类别信息。AC-GAN模型将以上两个模型思路进行整合，得到够进行条件生成的生成器G，和能够判别样本类别和来源信息的判别器D。AC-GAN模型的结构如图10所示。AC-GAN模型在Conditional GAN的基础上，让判别器D在判别样本来源的同时，让样本进行分类。此时的判别器D的输出分为样本来源信息LS和样本分类LC信息。 不同模型比较下面用表格的方式对比在实验中使用的模型的目标函数(ps,图画比较丑，之后再修改) Name Paper Link Value Function GAN Arxiv DCGAN Arxiv Semi-GAN Arxiv 和GAN 模型相同 CGAN Arxiv ACGAN Arxiv our model Arxiv 数据集介绍在常用于图像生成的图像数据集中，大部分数据的标签类型为离散类型。其中MNIST和Fashion-MNIST为常用的灰度图像数据，每类样本分布较为独立，常用于进行图像分类和样本生成的实验；SVHN和CIFAR10为彩色数据集图像像素分布较为复杂，其中CIFAR10常用来检验分类网络性能的评价数据集；CelebA为大规模的人脸识别和属性分类数据集，每幅人脸图像包括40个属性标签；ImageNet为图像分类和识别数据集，数据集类别分布比较复杂具体包括自然图像和人为图像。UnityEyes为人眼视觉合成数据集，数据集标签包括瞳孔标签和视觉方向标签，其中视觉方向标签为连续的语言标签。常见的离散标签图像数据集的样例: 我们的模型在原始GAN模型中，目标函数定义为生成器G和判别器D的博弈过程，定义V(G;D)为模型的目标函数，由生成器G和判别器D组成。语义匹配目标函数FMloss。基于语义匹配的条件生成网络模型的生成器G和判别器D的目标函数分别为：如上式，LS为样本来源，LC为分类结果。判别器D目标为最大化LC+ LS + FMloss，其试图对输入样本进行分类，并通过来源和语义匹配区分生成样本和原始样本。生成器G的目的是最大化LC − LS −FMloss，其试图通过样本分类结果、样本来源和语义匹配结果来欺骗判别器D。LS来源损失与原始GAN模型相同，LC为语义标签分类损失，在类别分类中使用交叉信息熵，在数值回归中则使用均方差回归。 生成结果对比MNIST数据集生成结果Fashion-MNIST数据集生成结果SVHN数据集生成结果CIFAR10数据集生成结果CelebA数据集生成结果UnityEyes数据集生成结果]]></content>
      <categories>
        <category>Deep learning</category>
      </categories>
      <tags>
        <tag>GANs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Titanic Challenge]]></title>
    <url>%2F2018%2F06%2F05%2FTitanic-Challenge%2F</url>
    <content type="text"><![CDATA[用此博客记录自己解决 Kaggle Titanic challenge过程中的个人总结。 问题描述说道Titanic(泰坦尼克号)，最熟悉莫过于Titanic(1997 film),这部由著名导演詹姆斯·卡梅隆执导，莱昂纳多·迪卡普里奥、凯特·温斯莱特领衔主演的电影，经久不衰…但是我们今天的画风不是这样的… 我们今天解决的问题是以该事件问背景，但是没有那么浪漫。关于这个案例的介绍网上有很多内容，为了避免累赘，在这里就不进行详述。总的要求：预测在这个事件中乘客是否死亡。附上对于train sets和 test sets中数据的介绍。更多详细的内容参看：https://www.kaggle.com/c/titanic 数据分析顺滑过渡到第二阶段，数据分析，对于竞赛而言，我感觉对于数据的认识的重要性完全不亚于模型的重要性。之后我们将再次提到这句话。我将结合代码进行数据分析。 pandas 原生数据分析函数1234import pandas as pdtrain =pd.read_csv(&apos;data/train.csv&apos;)test =pd.read_csv(&apos;data/test.csv&apos;)train.describe(include=&apos;all&apos;) 除了上面 train.describe()，下面这两个也是比较常用的12train.head()train.columns 因为总体的数据量比较少，所以我们选择把train set 和test set连接起来进行数据分析和处理。1combined2 = pd.concat([train_data, test_data], axis=0) 数据质量分析 缺省值对于缺省值，常用的手段就是填充，但是针对不同的数据有不同的填充手段，有的是均值填充，有的是默认值填充还有的是根据现有数据训练一个 regression进行拟合(这种情况出现在缺省的数据比较重要，对于结果的预测有比较强的相关性的时候)。1combined2.Embarked.fillna(&apos;S&apos;, inplace=True) Embardked(上船港口)不是那么能表现出和结果(survival)相关的变量，我么可以直接采用某个默认值进行填充。1combined2.Fare.fillna(np.median(combined2.Fare[combined2.Fare.notnull()]), inplace=True) Fare(船票)我们选择使用均值填充 12345678classers = [&apos;Fare&apos;,&apos;Parch&apos;,&apos;Pclass&apos;,&apos;SibSp&apos;,&apos;TitleCat&apos;,&apos;CabinCat&apos;,&apos;Sex_female&apos;,&apos;Sex_male&apos;, &apos;EmbarkedCat&apos;, &apos;FamilySize&apos;, &apos;NameLength&apos;, &apos;FamilyId&apos;]age_et = ExtraTreesRegressor(n_estimators=200)X_train = full_data.loc[full_data.Age.notnull(),classers]Y_train = full_data.loc[full_data.Age.notnull(),[&apos;Age&apos;]]X_test = full_data.loc[full_data.Age.isnull(),classers]age_et.fit(X_train,np.ravel(Y_train))age_preds = age_et.predict(X_test)full_data.loc[full_data.Age.isnull(),[&apos;Age&apos;]] = age_preds 因为在特征提取看来 age 是一个比较重要的属性（下文中使用age来进一步计算性别特征，而性别特征对于survival 是重要的因素），所以需要通过 fit来进行填充 null 值。 异常值异常值的检测12combined2.boxplot()plt.ylim(0, 1000) 异常值处理大多数情况下我们都采取忽视，但是有时候异常值中却跟结果有比较强的相关性，比如说该题目分数在0.9的一位大神在博客中使用的特征包含名字长度。这个在我一开始的特征提取中确实没有太在意名字长度也可以当作一种和结果(servival or dead)相关的特征。 重复值重复值的检测1train[train.duplicated()==True] 如果运行结果为空，那么就是没有重复值,如果有重复值，一般使用下面类似的代码都是可以去除掉的。1df.drop_duplicates() 分布特征分析1234567#分布分析fig ,ax =plt.subplots(2,2, figsize=(8,6))sns.countplot(&apos;Embarked&apos;, data =train, ax =ax[0,0])sns.countplot(&apos;Pclass&apos;, data =train, ax =ax[0,1])sns.violinplot(&apos;Survived&apos;, &apos;Age&apos;, data= train, ax =ax[1,0]).set(ylim =(-10, 80))sns.countplot(x =&apos;Survived&apos;, data= train, ax =ax[1,1])plt.tight_layout() 运行需要导入 seaborn1import seaborn as sns 这里安利一个数据可视化工具-seaborn。回正题 counplot()可以直观的看出单个数据的特征，但是我们更加关心的是数据和数据之间的关系，更准确的是数据和预测数据(survival )之间的关系。所以,我们进行相关性分析。123456plt.subplots(figsize=(10,8)) corrmat = train[train.columns[1:]].corr()sns.set(font_scale=1.5) hm = sns.heatmap(corrmat, cbar=True, annot=True, square=True, fmt=&apos;.2f&apos;, annot_kws=&#123;&apos;size&apos;: 10&#125;) plt.title(&apos;Pearson Correlation of Features&apos;, y=1.05, size=15) plt.show() 从图中可以看出，在原始数据集特征中(为什么这么说，嗯，这意味着我们下文还要进行 generate new features),Fare特征是和 survival最相关的。这从数据角度这你船票价钱越高，你生存的几率就越大(三观尽毁)。嗯，这是符合经济社会的运行规律的。我们在分析 Pearson Correlation的时候，关注的是数值的绝对值，如果是正值，表示正相关；如果是负值，表示负相关。 如果细心的小伙伴发现，这个并没有把所有的变量的相关性表示出来，是的，下文我将给出一个加强版的。 特征工程当我们对数据有了一个初步的认识，这时候就可以进行特征工程了。网上流传很广的一句话”数据特征决定了机器学习的上限，而算法优化只是尽可能逼近这个上限”，我深有体会。因为之前进行特征提取，然后在kaggle的submission score是0.73205,经过模型融合然后达到了0.78947，提高了5个百分点。当自己在思考数据特征重新进行特征提取的时候，最后的score 是0.82296.这都是以10个百分点的提高啊。所以这句话很有道理，我试图找到这句话的出处，以表示我对于版权的尊重，但是科学上网能力有限，没有找见，也许这句话来自群众的智慧吧。图：我在kaggle 的submission 和相应的score 但是我想强调的是特征提取是个很难有模板化的东西，这得看个人对于这个问题的理解和对于数据的理解，对于数据异常值的处理。并且还想说的是这个一个迭代的过程，不是一步到位的。当初步构造好自己特征之后可以使用图形化工具进行简单的分析一下。(下图是我第一次构造的特征工程的图形化)12345678910111213141516171819def correlation_heatmap(df): _ , ax = plt.subplots(figsize =(14, 12)) colormap = sns.diverging_palette(220, 10, as_cmap = True) _ = sns.heatmap( df.corr(), cmap = colormap, square=True, cbar_kws=&#123;&apos;shrink&apos;:.9 &#125;, ax=ax, annot=True, linewidths=0.1,vmax=1.0, linecolor=&apos;white&apos;, annot_kws=&#123;&apos;fontsize&apos;:12 &#125; ) plt.title(&apos;Pearson Correlation of Features&apos;, y=1.05, size=15) train2 =train1.drop([&apos;NullCabin&apos;], axis =1)correlation_heatmap(train2) 上图是第一次特征分析的结果(kaggle score: 0.73205),survival和Pclass和Fare是有较强的正相关性。 该图是第二次特征分析分析结果（kaggle score:0.82296,survival与male_adult(-0.56)、sex_male(-0.54)负相关,和female_adult(0.54)、sex_female(0.52)正相关。你的survival的概率和你的性别和年龄有关，如果你是成年女子，那么你很大的概率不会死亡(像Rose那样)；如果你是成年男子，那么你有很大概率体现英伦的绅士风度，主动(Jack那样)或者被选择死亡。瞬间想起了Titanic电影中Jack和Rose 的场景，好感人啊!!! 模型训练发现写了这么久，还没有开始训练模型。加快脚步…下面的内容以第一次训练模型为例。在建立基本模型之前我们需要先引入评价函数，以评价不同模型性能的好坏。 通过均值和方差来评价模型性能的优劣1234from sklearn import cross_validation def rmsl(clf): s = cross_validation.cross_val_score(clf, X_train, y_train, cv=5) return (s.mean(),s.std()) 建立基本模型1234567891011121314151617181920212223242526272829303132333435363738394041424344from sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis,gaussian_processNLA =[ #ensemble methods ensemble.AdaBoostClassifier(), ensemble.BaggingRegressor(), ensemble.GradientBoostingClassifier(), ensemble.RandomForestClassifier(n_estimators=60), # Gaussian process gaussian_process.GaussianProcessClassifier(), # LM linear_model.LogisticRegression(C=1.0, penalty=&apos;l1&apos;), #Navies Bayes naive_bayes.GaussianNB(), # Nearest Neighbor neighbors.KNeighborsClassifier(n_neighbors=3), # Svm svm.SVC(probability=True), svm.LinearSVC(), #Tree tree.DecisionTreeClassifier(), tree.ExtraTreeClassifier() ]#create table to compare MLAMLA_columns = [&apos;MLA Name&apos;, &apos;MLA Parameters&apos;,&apos;MLA Train Accuracy Mean&apos;, &apos;MLA Test Accuracy Mean&apos;, &apos;MLA Test Accuracy Min&apos; ,&apos;MLA Time&apos;] MLA_compare = pd.DataFrame(columns = MLA_columns) row_index = 0 for alg in MLA: #set name and parameters MLA_compare.loc[row_index, &apos;MLA Name&apos;] = alg.__class__.__name__ MLA_compare.loc[row_index, &apos;MLA Parameters&apos;] = str(alg.get_params()) #score model with cross validation: cv_results = model_selection.cross_validate(alg, X_train, y_train, cv =5,return_train_score=True) MLA_compare.loc[row_index, &apos;MLA Time&apos;] = cv_results[&apos;fit_time&apos;].mean() MLA_compare.loc[row_index, &apos;MLA Train Accuracy Mean&apos;] = cv_results[&apos;train_score&apos;].mean() MLA_compare.loc[row_index, &apos;MLA Test Accuracy Mean&apos;] = cv_results[&apos;test_score&apos;].mean() MLA_compare.loc[row_index, &apos;MLA Test Accuracy Min&apos;] = cv_results[&apos;test_score&apos;].min() #let&apos;s know the worst that can happen! row_index+=1MLA_compare.sort_values(by = [&apos;MLA Test Accuracy Mean&apos;], ascending = False, inplace = True) 当我们发现某个模型效果比较好的时候，我们仍然可以进一步调参。但是这种调参并不是每次会得到better result,有时候只是一个decent result。调参是个技术活。以上图中的 DecisionTreeClassifier为例进行调参。12345678param_grid = &#123;&apos;criterion&apos;: [&apos;gini&apos;, &apos;entropy&apos;], &apos;splitter&apos;: [&apos;best&apos;, &apos;random&apos;], &apos;max_depth&apos;: [None, 2,4,6,8,10], &apos;min_samples_split&apos;: [5,10,15,20,25], &apos;max_features&apos;: [None, &apos;auto&apos;, &apos;sqrt&apos;, &apos;log2&apos;] &#125;tune_model = model_selection.GridSearchCV(tree.DecisionTreeClassifier(), param_grid=param_grid, scoring = &apos;accuracy&apos;, cv = 5) cv_results = model_selection.cross_validate(tune_model, X_train, y_train, cv = 5) 使用的是sklearn 中的model_selection 模块，进行GridSearch，其实只是把调参过程自动化程序化。得到的结果是1230.863068608315 #train mean0.79803322024 # test mean0.77094972067 #test min 我们通过比对发现这个结果和上图的结果是稍微变差的。可视化显示各个算法的效率：1234sns.barplot(x=&apos;MLA Test Accuracy Mean&apos;, y = &apos;MLA Name&apos;, data = MLA_compare, color = &apos;m&apos;) plt.title(&apos;Machine Learning Algorithm Accuracy Score \n&apos;) plt.xlabel(&apos;Accuracy Score (%)&apos;) plt.ylabel(&apos;Algorithm&apos;) 对比之后我们选取几个效果比较“好”的模型，然后进行下一步的模型融合。12345678910111213141516171819MLA_best = [ #Ensemble Methods ensemble.AdaBoostClassifier(), # 0.76076 ensemble.BaggingClassifier(), # 0.72248 ensemble.GradientBoostingClassifier(), # 0.73684 ensemble.RandomForestClassifier(n_estimators = 60), # 0.72727 #GLM linear_model.LogisticRegression(C=1.0, penalty=&apos;l1&apos;, tol=1e-6), # 0.77990 linear_model.RidgeClassifierCV(), # 0.77033 linear_model.LogisticRegressionCV() #0.77033 ] row_index = 0 for alg in MLA_best: algname = alg.__class__.__name__ alg.fit(X_train, y_train) predictions = alg.predict(X_test) result = pd.DataFrame(&#123;&apos;PassengerId&apos;:test[&apos;PassengerId&apos;].as_matrix(), &apos;Survived&apos;:predictions.astype(np.int32)&#125;) result.to_csv(algname+&quot;.csv&quot;, index=False) # save the results row_index+=1 模型融合简单的说模型融合就是通过多个decent模型的结果通过某种方式的结合，产生了比原来单个模型better的结果。关于模型融合的详细内容，请移步另一篇文章模型融合(Ensemble learning)我们这里以stacking(二层)为例说明模型融合。1234567891011121314151617181920212223ntrain = train.shape[0] #891 ntest = test.shape[0] #418 SEED = 0 # for reproducibility NFOLDS = 5 # set folds for out-of-fold prediction kf =model_selection.KFold(n_splits=NFOLDS, random_state=SEED)# 封装算法基本操作 class SklearnHelper(object): def __init__(self, clf, seed=0, params=None): params[&apos;random_state&apos;] = seed self.clf = clf(**params) def train(self, x_train, y_train): self.clf.fit(x_train, y_train) def predict(self, x): return self.clf.predict(x) def fit(self,x,y): return self.clf.fit(x,y) def feature_importances(self,x,y): print(self.clf.fit(x,y).feature_importances_) return self.clf.fit(x,y).feature_importances_ 下面是定义五折交叉验证的方法，默认是三折。123456789101112131415161718def get_oof(clf, x_train, y_train, x_test): oof_train = np.zeros((ntrain,)) oof_test = np.zeros((ntest,)) oof_test_skf = np.empty((NFOLDS, ntest)) for i, (train_index, test_index) in enumerate(kf.split(x_train)): x_tr = x_train[train_index] y_tr = y_train[train_index] x_te = x_train[test_index] clf.train(x_tr, y_tr) oof_train[test_index] = clf.predict(x_te) oof_test_skf[i, :] = clf.predict(x_test) oof_test[:] = oof_test_skf.mean(axis=0) return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1) # 想让z变成只有一列，行数不知道多少 1234567891011121314151617181920212223242526272829303132from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier,AdaBoostClassifier, GradientBoostingClassifier# 定义四个不同的弱分类器的参数值 # Random Forest parameters rf_params = &#123; &apos;n_jobs&apos;: -1,&apos;n_estimators&apos;: 500,&apos;warm_start&apos;: True, &apos;max_depth&apos;: 6,&apos;min_samples_leaf&apos;: 2, &apos;max_features&apos; : &apos;sqrt&apos;,&apos;verbose&apos;: 0#&apos;max_features&apos;: 0.2, &#125; # Extra Trees Parameters et_params = &#123; &apos;n_jobs&apos;: -1,&apos;n_estimators&apos;:500,&apos;max_depth&apos;: 8,&apos;min_samples_leaf&apos;: 2,&apos;verbose&apos;: 0 #&apos;max_features&apos;: 0.5, &#125; # AdaBoost parameters ada_params = &#123; &apos;n_estimators&apos;: 500,&apos;learning_rate&apos; : 0.75 &#125; # Gradient Boosting parameters gb_params = &#123; &apos;n_estimators&apos;: 500,&apos;max_depth&apos;: 5,&apos;min_samples_leaf&apos;: 2, &apos;verbose&apos;: 0 #&apos;max_features&apos;: 0.2, &#125; # Support Vector Classifier parameters # svc_params = &#123; # &apos;kernel&apos; : &apos;linear&apos;,&apos;C&apos; : 0.025 # &#125; # 创建四个若分类器模型 rf = SklearnHelper(clf=RandomForestClassifier, seed=SEED, params=rf_params) et = SklearnHelper(clf=ExtraTreesClassifier, seed=SEED, params=et_params) ada = SklearnHelper(clf=AdaBoostClassifier, seed=SEED, params=ada_params) gb = SklearnHelper(clf=GradientBoostingClassifier, seed=SEED, params=gb_params) 1234567891011121314151617181920#X_train =X_train.values#X_test =X_test.values# 使用五折交叉方法分别计算出使用不同算法的预测结果，这些结果将用于Stacking的第二层预测 et_oof_train, et_oof_test = get_oof(et, X_train, y_train, X_test) # Extra Trees rf_oof_train, rf_oof_test = get_oof(rf,X_train, y_train, X_test) # Random Forest ada_oof_train, ada_oof_test = get_oof(ada, X_train, y_train, X_test) # AdaBoost gb_oof_train, gb_oof_test = get_oof(gb,X_train, y_train, X_test) # Gradient Boost rf_feature = rf.feature_importances(X_train,y_train) et_feature = et.feature_importances(X_train, y_train) ada_feature = ada.feature_importances(X_train, y_train) gb_feature = gb.feature_importances(X_train,y_train) feature_dataframe = pd.DataFrame( &#123;&apos;features&apos;: cols, &apos;Random Forest feature importances&apos;: rf_feature, &apos;Extra Trees feature importances&apos;: et_feature, &apos;AdaBoost feature importances&apos;: ada_feature, &apos;Gradient Boost feature importances&apos;: gb_feature &#125;) 接下来以第一层为为基础训练第二层12345678base_predictions_train = pd.DataFrame( &#123; &apos;RandomForest&apos;: rf_oof_train.ravel(),# # ravel函数在降维时默认是行序优先 &apos;ExtraTrees&apos;: et_oof_train.ravel(), &apos;AdaBoost&apos;: ada_oof_train.ravel(), &apos;GradientBoost&apos;: gb_oof_train.ravel() &#125;) X_train2 = np.concatenate(( et_oof_train, rf_oof_train, ada_oof_train, gb_oof_train), axis=1) X_test2 = np.concatenate(( et_oof_test, rf_oof_test, ada_oof_test, gb_oof_test), axis=1) 使用XGBoost训练第二层的数据。关于XGBoost为什么是有效的和相关的概念，请移步XGBoost123456789101112131415# XGboost import xgboost as xgbgbm = xgb.XGBClassifier( #learning_rate = 0.02, n_estimators= 2000, max_depth= 4, min_child_weight= 2, #gamma=1, gamma=0.9, subsample=0.8, colsample_bytree=0.8, objective= &apos;binary:logistic&apos;, nthread= -1, scale_pos_weight=1).fit(X_train2, y_train) predictions = gbm.predict(X_test2) 最后产生结果文件 StackingSubmission.csv12StackingSubmission = pd.DataFrame(&#123;&apos;PassengerId&apos;:test.PassengerId, &apos;Survived&apos;: predictions &#125;) StackingSubmission.to_csv(&quot;StackingSubmission.csv&quot;, index=False) # 0.78947 参考文献本文在效果可视化中借鉴该博客特征提取参看kaggle多位大神，在这里就谢过…]]></content>
      <categories>
        <category>竞赛</category>
      </categories>
      <tags>
        <tag>Kaggle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[模型融合]]></title>
    <url>%2F2018%2F06%2F05%2F%E6%A8%A1%E5%9E%8B%E8%9E%8D%E5%90%88%2F</url>
    <content type="text"><![CDATA[多个有差异性的模型融合可以提高整体的性能。它能同时降低最终模型的bias 和variance，从而在提高竞赛分数的同时降低overfitting 的风险。 从结果文件中融合这种做法不需要重新训练模型，融合竞赛提交的结果文件就可以，简单便捷。 Voting投票制：少数服从多数。如一个分类问题，多个模型的投票（当然可以设置权重，若没有就是平均投票），最终投票数最多的类就是被预测的类。对于加权表决融合，性能表现较差的模型（权值比较低）只能通过和其他模型保持一致增强自己的说服力。对于结果取平均融合，在不同的评估准则上也能获得不错的效果在于：取均值常常能减少过拟合的现象。如图所示：如果单个模型过拟合产生了绿色的边缘，这时候去平均这种策略使得决策边界变成黑色的边缘，这样的效果更好。机器学习的目的并不是让模型记住训练数据，而是具有更好的泛化性。 RankingRank的思想其实和Averaging一致，但Rank是把排名做平均，对于AUC指标比较有效。 训练模型融合 Bagging:使用训练数据的不同随机子集来训练每个 Base Model，最后进行每个 Base Model 权重相同的 Vote。也即 Random Forest 的原理。 Boosting:Boosting是一种迭代的方法，每一次训练会更关心上一次被分错的样本，比如改变被错分的样本的权重的Adaboost方法。也即 Gradient Boosting，Adaboost 的原理。比 Bagging 效果好，但更容易 Overfit。 Blending用不相交的数据训练不同的 Base Model，将它们的输出取（加权）平均。实现简单，但对训练数据利用少了。 Stackinig(以二层为例)在网上为数不多的关于stacking的内容中，相信你已经看过这张图片：PS:这不是原图，是在原图的基础上经过修改（把最上面标题的model 1，2,3,4,5 修改为model 1,1,1,1,1。因为这个一个model 的不同阶段，不是多个模型） 想比较而言我更加喜欢下面这张图片，因为它把stacking 的不同阶段表达的更加清楚，尤其是经过model 1之后，model 2是在model 1的基础上进行训练的。 对于第二阶段使用的样本集合，上图使用的是第一阶段的结果数据集，当然还有一种方式，如下图所示。 在该图中上一阶段的结果(prob 1-N)列和原始数据集组成新的特征向量，训练第二阶段模型。 名词解释 cross validation交叉验证当评估不同的参数设置，对算法表现的影响时，仍然存在则过拟合的风险。因为在调整参数，优化测试集的算法表现时，测试集的信息已经泄漏进模型中了。为了解决这个问题，需要一部分数据作为验证集(validation set)。 这样，用训练集(Train set)的数据训练模型；用验证集对模型参数调剂，如上述程序中的C值；最后，算法的评价在测试集(Test set)上完成。但是当数据有原来的两份化成三份之后，降低了寻数据量；另外算法的表现依赖于三个数据集的划分。解决上述两个问题的常见方法: cross validation. 测试集仍然单独划分出来，但是 validation set不用单独划分。将训练集划分为k个小的数据集，称之为k-fold CV。对每个fold进行下列过程： 用其他k-1 folds 作为training sets，训练模型 模型的结果用剩下的一个 fold进行评价模型的性能用上述循环中的 k-fold 交叉验证集的平均值表现，这样的做法增加了计算量，但是提高数据的利用效率。 参考文献https://blog.csdn.net/u013395516/article/details/79745063https://blog.csdn.net/u012969412/article/details/76636336https://blog.csdn.net/u012604810/article/details/77579782]]></content>
      <categories>
        <category>竞赛</category>
      </categories>
      <tags>
        <tag>Kaggle</tag>
        <tag>优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[XGBoost]]></title>
    <url>%2F2018%2F06%2F05%2FXGBoost%2F</url>
    <content type="text"><![CDATA[Introduction to XGBoostXGBoost is short for “Extreme Gradient Boosting”, where the term “Gradient Boosting” is proposed in the paper Greedy Function Approximation: A Gradient Boosting Machine, by Friedman. XGBoost is based on this original model. And most of the content is based on the websit(http://xgboost.readthedocs.io/en/latest/model.html) element of Supervised learningXGBoost is used for supervised learning problems, where we use the training data (with multiple features) xi to predict a target variable yi. Objective Function: Training Loss + RegularizationA very important fact about objective functions is they must always contain two parts: training loss and regularization. For example, a commomly used training loss is mean squared error. Another commonly used loss function is logistic loss for logistic regression The regularization term is what people usually forget to add. The regularization term controls the complexity of the model, which helps us to avoid overfitting.The general principle is we want both a simple and predictive model. The tradeoff between the two is also referred as bias-variance tradeoff in machine learning. Decision Tree and random forest Decision Trees: It is one of my favorite techniques. It can be used as a ultimate solution to tackle multiple challenges like missing values, outliers and identifying significant variables. It worked well in our Data Hackathon also. 现在理解可能是基于”增益“ 之类的东西，关于 RF 最后一条。 Random Forest: Similar to decision tree is Random Forest. I would also recommend using the in-built feature importance provided by random forests to select a smaller subset of input features. Just be careful that random forests have a tendency to bias towards variables that have more no. of distinct values i.e. favor numeric variables over binary/categorical values. Tree EnsembleThe tree ensemble model is a set of classification and regression trees(CART). Here is a simple example of a CART that classifies whether someone will like computer games. A CART is a bit different from decision trees, where the leaf only contains decision values. In CART, a real score is associated with each of the leaves, which gives us richer interpretations that go beyond classification. This also makes the unified optimization step easier, as we will see in a later part of this tutorial. Usually, a single tree is not strong enough to be used in practice. What is actually used is the so-called tree ensemble model, which sums the prediction of multiple trees together. Here is an example of a tree ensemble of two trees. The prediction scores of each individual tree are summed up to get the final score. If you look at the example, an important fact is that the two trees try to complement each other. Now here comes the question, what is the model for random forests? It is exactly tree ensembles! So random forests and boosted trees are not different in terms of model, the difference is how we train them. This means if you write a predictive service of tree ensembles, you only need to write one of them and they should directly work for both random forests and boosted trees. One example of why elements of supervised learning rock. Tree BoostingAfter introducing the model, let us begin with the real training part. How should we learn the trees? The answer is, as is always for all supervised learning models: define an objective function, and optimize it! Assume we have the following objective function (remember it always needs to contain training loss and regularization) Additive TrainingFirst thing we want to ask is what are the parameters of trees? You can find that what we need to learn are those functions fi, with each containing the structure of the tree and the leaf scores. This is much harder than traditional optimization problem where you can take the gradient and go. It is not easy to train all the trees at once. Instead, we use an additive strategy: fix what we have learned, and add one new tree at a time. It remains to ask, which tree do we want at each step? A natural thing is to add the one that optimizes our objective. Training modelThe XGBoost model for classification is called XGBClassifier(regression is called XGBRegressor). We can create and and fit it to our training dataset. Models are fit using the scikit-learn API and the model.fit() function.]]></content>
      <categories>
        <category>竞赛</category>
      </categories>
      <tags>
        <tag>XGBoost</tag>
        <tag>Kaggle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[等概率生成器]]></title>
    <url>%2F2018%2F05%2F28%2F%E7%AD%89%E6%A6%82%E7%8E%87%E7%94%9F%E6%88%90%E5%99%A8%2F</url>
    <content type="text"><![CDATA[总体来说是可以有两种思路：一种是二级制的理解，一种是生成更大的数字的集合，然后求余+剪枝（重投）。 给定一个可以等概率生成1-3的rand3()函数生成器，求解可以随机等概率生成1-7的rand7()函数生成器。 解题思路 使用rand3()函数生成1-9的数字，然后丢弃8,9两种可能。 实现代码 12345678910class Solution: def random7(self): x =8 while x&gt;7: x =self.random3() + (self.random3()-1)*3 return x def random3(self): return if __name__ =="__main__": print(Solution().random7()) Given a random number generator rand5() generate gen7() 12345def rand7a(): rand7 =22 while rand7&gt;=21: rand7 =rand5()+ rand5()*5 return rand7 %7 1st approach:rand2() in binary is 000 or 001 with 50-50 probability.rand2()2 is 000 or 010 with 50-50 probability.rand2()4 is 000 or 100 with 50-50 probability.So the sum is binary xxx where each x has a 50-50 probability of 0 or 1; so each 000 to 111 has a probablilty of 1/8. 1234567def rand7b(): rand7 =7 while rand7 ==7: rand7 =rand2() +rand2()*2+ rand2()*4 return rand7 2nd approach: going through each of the 8 possibilities of the 3 rand2()s:0+0+0 or 1+0+0 or 0+2+0 or 1+2+0 or 0+0+4 or 1+0+4 or 0+2+4 or 1+2+4all of these with equal probablilty 1/8. 从大数字的 random 到小数字的random，需要做的是等概率的生成，所以对于不符合要求的是 就是 重新rand() 12345int rand2() &#123; int x = rand5(); if x == 4 return rand2(); // restart else return x % 2;&#125; Given rand2(), you should get rand5() 这个是可以从二进制的角度进行解析的。 12345int rand7() &#123; int x = rand2() * 4 + rand2() * 2 + rand2(); if (x == 7) return rand7(); // restart else return x;&#125; 其实不管 rand7() 是不是产生了7，即使产生了7 那么也是可以通过 “if condition” 进行提前进行处理的。 这种算法都不是唯一的。]]></content>
      <categories>
        <category>算法</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[机器学习概念]]></title>
    <url>%2F2018%2F05%2F28%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[解释一些机器学习中小的基本的概念。机器学习的优化方法有很多，这里主要解释的是 Gradient Descent 和 Newton’s Method. 优化方法 梯度下降（gradient descent） 梯度下降法是最早最简单，也是最为常用的最优化方法。梯度下降法实现简单，当目标函数是凸函数时，梯度下降法的解是全局解。接下来衍生的的子类:批量梯度下降法（Batch Gradient Descent，BGD）随机梯度下降（Stochastic Gradient Descent，SGD） 牛顿法和拟牛顿法（Newton’s method &amp; Quasi-Newton Methods） 牛顿法最大的特点就在于它的收敛速度很快。个人感觉牛顿法只是在在每次迭代的时候进行了两次运算，和梯度下降在总的运算次数上并没有很大的差别，为什么会收敛速度更快呢？ 名词解释 凸优化:对凸优化的问题我们在基础数学上面已经有了很多解决方法，例如可以将凸优化问题Lagerange做对偶化，然后用Newton、梯度下降算法求解。凸集合: 凸函数：Jacobian矩阵和Hessian矩阵：在向量分析中, 雅可比矩阵是一阶偏导数以一定方式排列成的矩阵, 其行列式称为雅可比行列式。Hessian矩阵： bias and variancebias variance tradeoff 实际上是有三个名词 bias (偏差)，variance (方差) and noise (噪声)。偏差是预测值和实际值的差值，是准不准的问题；而方差是预测值的属性，如果最后的结果比较分散，那么方差就比较大。所以是两个不同的维度。最好的模型当然是低方差并且低偏差了。一图胜千言 ß 常见的几种说法： 当网络结构比价简单的时候，容易出现高偏差，一般来说这个时候是欠拟合 当网络比较复杂的时候，容易出现高方差，一般来说这个时候是过拟合 如果出现了过拟合，一般说拟合了过多的noise (噪声)，没有抓住问题的本质。 问题 解决方案 高方差 采集更多的样本， 降低特征的维度，降低参数 高偏差 采集更多的特征，增大参数 在训练方法上也是可以改进的， 比如使用 K-Fold 交叉验证。简单的说，就是将训练样本分成k份，每次取其中一份作为验证集，另外 k-1 份作训练集。这样进行 k 次训练得到 k 个模型。这 k 个模型对各自的验证集进行预测，得到 k 个评估值（可以是误差、准确率，或按某种规则计算的得分等等）。注意到每个样本参与了 k-1 个模型的训练（导致模型之间存在关联），每个样本有一次被用作测试（没有用另外的从未见过的测试集数据），所以这与标准的计算过程是不一样的。 当 K值比较大的时候，模型容易过拟合数据集，bias 比较小，但是当 测试集上的时候，variance 比较大； 同理，当K 值标胶小的时候，bias 比较大，没有过拟合数据，因此在 test 数据集上 bias 是比较小的。 偏差和方差又与「欠拟合」及「过拟合」紧紧联系在一起。由于随机误差是不可消除的，所以此篇我们讨论在偏差和方差之间的权衡（Bias-Variance Tradeoff）。 当模型处于欠拟合状态时，训练集和验证集上的误差都很高； 当模型处于过拟合状态时，训练集上的误差低，而验证集上的误差会非常高。 这两个概念和模型的复杂度又有着很深的联系： 生成式模型和判别式比较对于判别式模型来说求得P(Y|X)，对未见示例X，根据P(Y|X)可以求得标记Y，即可以直接判别出来，如上图的左边所示，实际是就是直接得到了判别边界，所以传统的、耳熟能详的机器学习算法如线性回归模型、支持向量机SVM等都是判别式模型，这些模型的特点都是输入属性X可以直接得到Y。 而生成式模型求得P(Y,X)，对于未见示例X，你要求出X与不同标记之间的联合概率分布，然后大的获胜，如上图右边所示，并没有什么边界存在，对于未见示例（红三角），求两个联合概率分布（有两个类），比较一下，取那个大的。比如朴素贝叶斯算法和隐式马尔科夫模型。 判别式模型举例：要确定一个羊是山羊还是绵羊，用判别模型的方法是从历史数据中学习到模型，然后通过提取这只羊的特征来预测出这只羊是山羊的概率，是绵羊的概率。生成式模型举例：利用生成模型是根据山羊的特征首先学习出一个山羊的模型，然后根据绵羊的特征学习出一个绵羊的模型，然后从这只羊中提取特征，放到山羊模型中看概率是多少，在放到绵羊模型中看概率是多少，哪个大就是哪个。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>bias-variance</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[面试算法题]]></title>
    <url>%2F2018%2F05%2F28%2F%E6%B1%82%E8%A7%A3%E5%B9%B3%E6%96%B9%E6%A0%B9%2F</url>
    <content type="text"><![CDATA[求解平方根求解平方根的算法主要有两种：二分法(binary search) 和牛顿迭代法(Newton’s Method) Just give me codes… 1234567891011121314151617181920212223242526272829from math import sqrt# you needn't import sqrt, I do that just for comparing the results of different methodsclass Solution: # in-built function def my_sqrt(self, n): return sqrt(n) def sqrt_binary(self, n): low, high =0,n mid =int((low+high)/2) while abs(mid*mid- n)&gt; 1e-9: if mid*mid&gt;n: high =mid else: low =mid mid =(low+high)/2 return mid # Newton method: math def newton_method(self, n): k =1 while abs(k*k-n) &gt;1e-9: k =(k+n/k)/2 return k if __name__ =="__main__": print(Solution().my_sqrt(2)) print(Solution().sqrt_binary(2)) print(Solution().newton_method(2)) 运行结果 1231.4142135623730951 #built-in function1.4142135623842478 # binary search1.4142135623746899 # Newton method 从结果中看，Newton method比 binary sqrt更加接近系统自带的sqrt function. 并且从数学上可以证明 Newton method 比 binary sqrt需要更少的迭代次数。 附录：牛顿迭代法是求方程根的重要方法之一，其最大优点是在方程f(x) = 0的单根附近具有平方收敛，而且该法还可以用来求方程的重根,复根。牛顿迭代法结论其实就是取泰勒级数前两项等于0求得的,泰勒公式表示为更简练的写法为：通过 Newton methond逼近方程的解的过程可以表示为下图： referrences:https://blog.csdn.net/ycf74514/article/details/48996383 Consecutive Numbers Sum百度搜索三面 Tips: 这个定义为数学问题，是因为是 consecutive number，是一个连续的数字串 。连续自然数的和其实就是一个公差为1的等差数列的和，设这个数列是 $x,x+1,x+2,…,x+n$，那么这个数列的和是$ (n+1)x+n(n+1)/2 $，这个数列的长度是n+1，第一个数字是x，然后就可以从数字长度为1开始逐渐尝试，看N能否分解成为长度为n的等差数列的和，如果可以，那么一定可以求得整数解x使得$ x=(N-n*(n+1))/(n+1) $ 1234567891011121314151617181920class Solution(object): def consecutiveNumbersSum(self, N): """ :type N: int :rtype: int """ res =0 n =0 while True: top =N -(n *n +n)/2 if top &lt;=0: break if top %(n +1) ==0: res +=1 n +=1 return res 如何近似的求解 piTips: 蒙特卡洛(Monte Carlo)方法，又称随机抽样或统计试验方法。当所求解的问题是某种事件出现的概率，或某随机变量的期望值时，可以通过某种“试验”方法求解。使用概率的方式，如果使用python 中的random 函数实现注意使用 range() 均匀分布而不是 random() 这个正太分布(0, 1) 12345678910111213from random import randrangetotal =10000count =0while total: x =randrange(0, 10001)/10000.0 y =randrange(0, 10001)/10000.0 if pow(x, 2) +pow(y,2) &lt;=1: count +=1 total -=1 pai =count *4.0/total 将中文数字转换成阿拉伯数字Tips: 唯一需要思考一下的是，为什么「万」、「亿」和 [兆]不能与「千」、「百」、「十」放在一起处理。这个问题想通了，代码也就好理解了。因为前者除了单独出现外，还可以和其他的进行组合出现，比如百万，千亿。但是后者就不行。前者分别有4 8 12 个零。 这个是一种解法： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135CN_NUM = &#123; u'〇': 0, u'一': 1, u'二': 2, u'三': 3, u'四': 4, u'五': 5, u'六': 6, u'七': 7, u'八': 8, u'九': 9, u'零': 0, u'壹': 1, u'贰': 2, u'叁': 3, u'肆': 4, u'伍': 5, u'陆': 6, u'柒': 7, u'捌': 8, u'玖': 9, u'貮': 2, u'两': 2,&#125;CN_UNIT = &#123; u'十': 10, u'拾': 10, u'百': 100, u'佰': 100, u'千': 1000, u'仟': 1000, u'万': 10000, u'萬': 10000, u'亿': 100000000, u'億': 100000000, u'兆': 1000000000000,&#125;def cn2dig(cn): lcn = list(cn) unit = 0 # 当前的单位 ldig = [] # 临时数组 while lcn: cndig = lcn.pop() #if CN_UNIT.has_key(cndig): if cndig in CN_UNIT: unit = CN_UNIT.get(cndig) if unit == 10000: ldig.append('w') # 标示万位 unit = 1 elif unit == 100000000: ldig.append('y') # 标示亿位 unit = 1 elif unit == 1000000000000: # 标示兆位 ldig.append('z') unit = 1 continue else: dig = CN_NUM.get(cndig) if unit: dig = dig * unit unit = 0 ldig.append(dig) # 这个是 if unit == 10: # 处理10-19的数字 ldig.append(10) print(ldig) ret = 0 tmp = 0 import ipdb ipdb.set_trace() while ldig: x = ldig.pop() # 这个进制一定是出现在后面的， tmp *= 10000 是没有问题的 if x == 'w': tmp *= 10000 ret += tmp tmp = 0 elif x == 'y': tmp *= 100000000 ret += tmp tmp = 0 elif x == 'z': tmp *= 1000000000000 ret += tmp tmp = 0 else: tmp += x ret += tmp return ret # ldig.reverse() # print ldigß # print CN_NUM[u'七']if __name__ == '__main__': test_dig = [u'九', u'十一', u'十九万零十九', u'一百二十三', u'一千二百零三', u'一万一千一百零一', u'十万零三千六百零九', u'一百二十三万四千五百六十七', u'一千一百二十三万四千五百六十七', u'一亿一千一百二十三万四千五百六十七', u'一百零二亿五千零一万零一千零三十八', u'一千一百一十一亿一千一百二十三万四千五百六十七', u'一兆一千一百一十一亿一千一百二十三万四千五百六十七', ] for cn in test_dig: print (cn2dig(cn)) ß 二进制数中的1 123456789101112131415161718#include&lt;iostream&gt;using namespace std;// 遍历整个二进制的数组int count(Byte v)&#123; int count =0; while(v)&#123; if (v %2 ==1) &#123; count ++; &#125; v =v/2 &#125; &#125; 使用位操作代替求余和除二 12345678910int count2(Byte v)&#123; int count =0; while(v)&#123; count += v &amp; 0x01; v &gt;&gt;=1; &#125; &#125; 如果1出现的次数比较少， 这个从 O(N) -&gt; O(K) K 表示的个数， N表示总的子串的长度 1234567891011121314int count3(Byte v)&#123; int count =0; while (v) &#123; v &amp;= (v-1);// 使得最大位的1变成0 ，在二进制的角度上 count +=1 ; &#125;&#125;// 使用空间代替时间, 这个是可以进行穷举的int count4(Byte v)&#123; int countTable [256] =&#123;0, 1,1,2...8&#125; int count =countTable[v]; return count;&#125; 给定一个数字N N的阶乘有多少个0? Tips: 数学问题，N 是可以通过质因数(2, 3, 5)进行分析 $N =(2^x +3^y+ 5^z) $， 然后0的个数是 $min(x, z) -&gt; z $, 最后就相当于每个数字的5的个数, 1234567891011int count(Byte， v)&#123; int count; for(int i =0; i&lt;=v; i++) &#123; while (i) &#123; count +=1; i /=5; &#125; &#125; return count;&#125; 123456789int count(Byte, v)&#123; int count =0; while (v &gt;4) &#123; count += v/5; v =v /5; &#125; return count;&#125; 求解 N! 中的二级制表示中最低位1的位置 Tips: 如果能够被2 整除表示二进制表示中是 0，否则就是1，所以只要找到能有多少个被 2整除的个数，然后这个个数+1 就ok了 -&gt; 这个如同就是求解 N！ 中含有质因数 2的个数 123456789int count(int N)&#123; int res =0; while(N)&#123; N &gt;&gt;=1; res += N; &#125; return N;&#125; 寻找每个 ID，该ID 出现的频率是大于 0.5, 出现的次数大于总数的一半 Tips: 如果每次删除两个不同的ID，那么 最多的ID 出现的次数还是大于总数的一般，不断的重复这个过程 1234567891011121314151617181920212223242526Type Find(Type *ID, int N)&#123; Type candidate; int nTimes =0, i; for(i =0; i&lt; N; i++)&#123; if (nTimes ==0) &#123; candidate =ID[i]; nTimes =0 &#125; else &#123; if(ID[i] ==candidate)&#123; nTimes +=1; &#125; else&#123; nTimes -=1; &#125; &#125; &#125; // 这个应该还是需要最后判断一下 candidate 是不是出现了 [2/N] 次数，因为有一种情况是不存在的&#125; 精确表达浮点数 将小数使用分数的形式进行表示 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102#include&lt;iostream&gt;#include&lt;stdio.h&gt;using namespace std;//最大公约数(greatest common divisor)// f(x, y) =f(x-y, y) ，通过减法进行gcd 的求解， 缺点在于减法次数太多long gcd1(long x, long y)&#123; if (x&lt; y)&#123; return gcd1(y, x); &#125; if (y ==0) &#123; return x; &#125; else return gcd1(x-y, y);&#125;// 使用位运算 如果是偶数，进行优化bool IsEven(long x)&#123; if (x &amp; 0x01) &#123; return false; &#125; else return true;&#125;long gcd2(long x, long y)&#123; if (x&lt;y) &#123; return gcd2(y, x); &#125; if (y ==0)&#123; return x; &#125; if(IsEven(x)) &#123; if (IsEven(y)) &#123; return (gcd2(x &gt;&gt;1, y &gt;&gt;1)&lt;&lt;1); &#125; else return gcd2(x&gt;&gt;1, y); &#125; else&#123; if(IsEven(y)) return gcd2(x, y&gt;&gt;1); else return gcd2(x-y, y); // 尽量做除法，是在不行就做减法 &#125;&#125;int main()&#123; long a =0, b =0, c =0; cin&gt;&gt;c&gt;&gt;a&gt;&gt;b; // c 是整数部分， a b 分别是有限小数 和 循环小数 if (a==0 &amp;&amp; b ==0) cout &lt;&lt;c; else&#123; long up =c; long down =1; long ta =a; while(ta) &#123; down *=10; ta /=10; &#125; up =c*down +a; if (b!=0) &#123; long wb =1; long tb =b; while(tb) &#123; wb *=10; tb /=10; &#125; up =up *(wb -1)+b ; down =down *(wb -1); &#125; long fac =gcd2(up, down); cout &lt;&lt;up/fac &lt;&lt;"/"&lt;&lt;down/fac&lt;&lt;endl; &#125;&#125; 尾递归尾调用定义:在某个函数的最后一步调用另一个函数. 123456function f(x) &#123; if (x &gt; 0) &#123; return m(x) &#125; return n(x); &#125; 上面代码中，函数m和n都属于尾调用，因为它们都是函数f的最后一步操作。不一定是最后一行。 同理是可以推广到尾递归。 12345678910111213141516function f() &#123; let m = 1; let n = 2; return g(m + n);&#125;f();// 等同于function f() &#123; return g(3);&#125;f();// 等同于g(3); 尾递归的意义在于防止栈溢出。因为递归的调用是非常消耗内存的，需要再栈中保存N 个调调用记录，很容易发生栈溢出 (stack overflow) 。但是对于为递归来说，只有一个调用记录，因为上一个已经结束，所以是不会发生 “栈溢出”错误的。 Fibonacci 数列最原始的解法，递归，其中有很多重复的计算子单元 12345678910111213141516int Fibonacci(int n)&#123; if (n &lt;0) &#123; return 0; &#125; else if(n ==1) &#123; return 1; &#125; else &#123; return Fibonacci(n -1) + Fibonacci(n -2); &#125; &#125; 解法二： 第二种解法， 时空都是 O(N) 123456789101112131415def Fibonacci(n ): if n ==0: return 0 elif n ==1: return 1 arr =[0] *(n+1) arr[0] =0 arr[1] =1 for i in range(2, n+1): arr[i] =arr[i-1] +arr[i-2] return arr[-1] 在上面的基础上进行优化，空间复杂度变成 O(1) a, b =b, a+b # 在python，连续赋值是从左往右的。也就是说， 先保存 b, 和 a+b，然后执行 a =b, b = a+b 而在C 语言中是从右往左进行赋值的，所以注意这点。 123456789101112131415161718class Solution(object): def fib(self, N): """ :type N: int :rtype: int """ if N &lt;=0: return 0 elif N ==1: return 1 a, b =0, 1 for i in range(N ): a, b =b, a+b # 在python，连续赋值是从左往右的。 return a 矩阵乘法，时间复杂度 O(lgN),空间 O(1) 这个实现代码还没有想到一种比较简洁的方式，所以暂时先留个坑 寻找数组中的最大值和最小值这个从时间上是最优的 O(1.5 N)， 空间上是O(1)。关键点就是使用两个指针，然后双重if 进行判断。 12345678910111213141516171819202122232425262728293031323334353637#include&lt;iostream&gt;using namespace std;#define INF 10^9void FindMinMax(int a[], int size, int &amp;min, int &amp;max)&#123; max =-INF; min =INF; for (int i =0; i&lt;size -1; i++) &#123; if (a[i] &lt; a[i+1]) &#123; if(a[i+1]&gt; max ) max =a[i+1]; if(a[i] &lt; min) min =a[i]; &#125; else &#123; if(a[i] &gt; max) max =a[i] ; if(a[i+1] &lt; min) min =a[i+1]; &#125; &#125;&#125;int main()&#123; int arr[10]; int length =sizeof(arr)/ sizeof(arr[0]); int min, max; FindMinMax(arr, length, min, max); &#125; 寻找最近点对 在二维平面上的n个点中，找出最接近的一对点 解法一： 暴力法，时间复杂度是$O(n^2)$ 123456789101112131415161718192021222324252627282930313233343536373839404142#include&lt;iostream&gt;#include&lt;cmath&gt;using namespace std;// 降序bool myfunction (int i, int j) &#123;return (i&gt;j) ;&#125;typedef struct Point&#123; float x, y;&#125;Point;float countDistance(Point a, Point b)&#123; return sqrt( (a.x -b.x)*(a.x -b.x) + (a.y- b.y)*(a.y -b.y));&#125;double MinDifference(Point point[], int n)&#123; double minDiff =0.0; double tmp =0.0; int firstIndex, secondIndex; for (int i =0; i&lt;n; i++) &#123; for (int j =i+1; j&lt;n ;j++) &#123; tmp =countDistance(point[i], point[j]); if (tmp&lt; minDiff) &#123; minDiff =tmp; firstIndex = i; secondIndex =j; &#125; &#125; &#125; return minDiff ;&#125; 解法二：分治法 首先看一下一维的情况。这样的时间复杂度是 $O(nlogn)$123456789101112131415161718double MinDifference(double arr[], int n)&#123; if (n &lt;2) return 0; sort(arr, arr+n); // sort(arr, arr+n myfunction) 降序 double minDiff =arr[1] - arr[0]; for (int i =2; i&lt;n;i++) &#123; double tmp =arr[i] -arr[i-1]; if (tmp &lt;minDiff) &#123; minDiff =tmp; &#125; &#125; return minDiff; &#125; 从而把这种想法推广到 二维数组。 参考这里 快速寻找满足条件的两个数 第一种解法： 排序之后 然后二分查找 O(nlogn) +O(logn) -&gt; O(nlogn) 123456789101112131415161718192021222324252627282930313233343536373839404142434445#include&lt;iostream&gt;#include&lt;algorithm&gt;using namespace std;#define MAXN 1001int arr[MAXN];bool findSum(int arr[], int n, int sum)&#123; int i =0; int j =n-1; while (i&lt;j) &#123; int tmp =arr[i] +arr[j]; if (tmp ==sum) &#123; printf("(%d, %d)", arr[i], arr[j]); return true; &#125; else if(tmp &lt;sum) i ++; else j --; &#125; return false;&#125;int main()&#123; int n, sum, i,j; cin &gt;&gt; n &gt;&gt;sum; for(i =0; i&lt;n;i++) cin &gt;&gt;arr[i]; sort(arr, arr+n); bool res =findSum(arr, n, sum); return 0;&#125; 第二种解法：使用辅助的空间 dictionary 存储，空间复杂度O(N) + 时间复杂度 O(N) 12345678910111213141516def twoSum(arr, total): if not arr: return arr from collections import defaultdict dic =defaultdict(int) for num in arr: dic[num] = dic[num] +1 for key in dic: if total -key in dic: return (key, total -key) return (-1, -1)]]></content>
      <categories>
        <category>算法</category>
      </categories>
  </entry>
</search>
