<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Beyond Word Embedding]]></title>
    <url>%2F2019%2F05%2F22%2Fbeyond-word-embedding%2F</url>
    <content type="text"><![CDATA[Traditional Word VectorsBefore diving directly into Word2Vec it’s worth while to do a brief overview of some of the traditional methods that pre-date neural embeddings. 这个是用来描述文章的，有一个大的dict，然后一片文章是如何进行表示、Bag of Words or BoW vector representations are the most common used traditional vector representation. Each word or n-gram is linked to a vector index and marked as 0 or 1 depending on whether it occurs in a given document. An example of a one hot bag of words representation for documents with one word. 优缺点（重点是缺点）: 一方面只是一种counter，没有考虑语义信息；另一方面有些 words 是明显的 relevant than others.BoW representations are often used in methods of document classification where the frequency of each word, bi-word or tri-word is a useful feature for training classifiers. One challenge with bag of word representations is that they don’t encode any information with regards to the meaning of a given word.In BoW word occurrences are evenly weighted independently of how frequently or what context they occur. However in most NLP tasks some words are more relevant than others. 这个是可以认识是对于 bag of words “relevant” 上的改进：使得 选择的words 更加的 “representative” 文章的调性。TF-IDF, short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word or n-gram is to a document in a collection or corpus. They provide some weighting to a given word based on the context it occurs.The tf–idf value increases proportionally to the number of times a word appears in a document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently than others. 但是对于 bag of words 中“没有体现语义” 的缺陷还是没有 deal with。However even though tf-idf BoW representations provide weights to different words they are unable to capture the word meaning. 这个名字只是因为有定义而存在的名字Distributional Embeddings enable word vectors to encapsulate contextual context. Each embedding vector is represented based on the mutual information it has with other words in a given corpus.重点就是这种方式是要 predict a target word from context words，一定是要能够体现语境的。Predictive models learn their vectors in order to improve their predictive ability of a loss such as the loss of predicting the vector for a target word from the vectors of the surrounding context words.word2vec 是一种思想，有两种CBOW 和skip-gram 两种实现。Word2Vec is a predictive embedding model. There are two main Word2Vec architectures that are used to produce a distributed representation of words: CBOW没有体现 context words 的order，但是训练速度快。Continuous bag-of-words (CBOW) — The order of context words does not influence prediction (bag-of-words assumption). skip-gram weights nearbt context words heavily 效果相对于 cbow 更加， 但训练速度相对比较慢。Continuous skip-gram weighs nearby context words more heavily than more distant context words. While order still is not captured each of the context vectors are weighed and compared independently vs CBOW which weighs against the average context. CBOW is faster while skip-gram is slower but does a better job for infrequent words. glove （g lou v）这个不属于 word2vec，其只是考虑了 local context，没有考虑 global contextBoth CBOW and Skip-Grams are “predictive” models, in that they only take local contexts into account. word2vec does not take advantage of global context.(细节 能看懂就看) GloVe embeddings by contrast leverage the same intuition behind the co-occurrence matrix (共生矩阵) used distributional embeddings, but uses neural methods to decompose the co-occurrence matrix into more expressive and dense word vectors.效果，和word2vec 相比没有 definitively better resultsWhile GloVe vectors are faster to train, neither GloVe or Word2Vec has been shown to provide definitively better results rather they should both be evaluated for a given dataset. fasttext这个主要是 each word + n-gram within each word， 最后的效果是好于 word2vec 的。FastText, builds on Word2Vec by learning vector representations for each word and the n-grams found within each word. The values of the representations are then averaged into one vector at each training step. While this adds a lot of additional computation to training it enables word embeddings to encode sub-word information. FastText vectors have been shown to be more accurate than Word2Vec vectors by a number of different measures overview of Neural NLP ArchitecturesDeep Feed Forward Networks 1D CNNs RNNs (LSTM/GRU) encoder- decoder 结构 attention and copy mechanisms这个是 attention 机制提出的背景：解决 句子中的长依赖。While in theory they can capture long term dependencies they tend to struggle modeling longer sequences, this is still an open problem. One cause for sub-optimal performance standard RNN encoder-decoder models for sequence to sequence tasks such as NER or translation is that they weight the impact each input vector evenly on each output vector when in reality specific words in the input sequence may carry more importance at different time steps.Attention mechanisms provide a means of weighting the contextual impact of each input vector on each output prediction of the RNN. These mechanisms are responsible for much of the current or near current state of the art in Natural language processing. attention In sum, algorithms can allocate attention, and they can learn how to do so, by adjusting the weights they assign to various inputs. Imagine a heat map over a photo. The heat is attention.One of the limits of traditional word vectors is that they presume that a word’s meaning is relatively stable across sentences.并不是物理上的二维关系能够表示词语之间的 relationship，有时候是需要高纬空间进行表示的。In fact, the strongest relationships binding a given word to the rest of the sentence may be with words quite distant from it.从 credit assignment的角度阐述了 neural networks 就是 allocating importance to input featueres。The fundamental task of all neural networks is credit assignment. Credit assignment is allocating importance to input features through the weights of the neural network’s model. Learning is the process by which neural networks figure out which input features correlate highly with the outcomes the net tries to predict, and their learnings are embodied in the adjusted quantities of the weights that result in accurate decisions about the data they’re exposed to.这个是传统的 LSTM （encoder -decoder） 模型，问题在于当句子过长（比如说大于20 words）之后，encoder 是无法 memory 之前的所有 words，所以效果就会变得差一些。 但是 attention 就是模仿了人翻译过程，一段作为一个单位，然后进行翻译。这样就可以持续保证较高中确率的输出。In neural networks, attention primarily serves as a memory-access mechanism. 就是每次的输出都是关注不同的地方，但是至于哪里更加重要，这个交给了 feedback mechanism 反向传播。Above, a model highlights which pixels it is focusing on as it predicts the underlined word in the respective captions. Below, a language model highlights the words from one language, French, that were relevant as it produced the English words in the translation. As you can see, attention provides us with a route to interpretability. We can render attention as a heat map over input data such as words and pixels, and thus communicate to human operators how a neural network made a decision. (This could be the basis of a feedback mechanism whereby those humans tell the network to pay attention to certain features and not others.) (This could be the basis of a feedback mechanism whereby those humans tell the network to pay attention to certain features and not others.) Additionally in Machine Reading Comprehension and Summarization systems RNNs often tend to generate results, that while on first glance look structurally correct are in reality hallucinated or incorrect. One mechanism that helps mitigate some of these issues is the Copy Mechanism.同上面那个是一样的The copy mechanism is an additional layer applied during decoding that decides whether it is better to generate the next word from the source sentence or from the general embedding vocabulary. reading comprehension and summary 上面是说的在 machine translation，下面说的是 阅读理解 和 summary领域。Additionally in Machine Reading Comprehension and Summarization systems RNNs often tend to generate results, that while on first glance look structurally correct are in reality hallucinated or incorrect. One mechanism that helps mitigate some of these issues is the Copy Mechanism.同上面那个是一样的The copy mechanism is an additional layer applied during decoding that decides whether it is better to generate the next word from the source sentence or from the general embedding vocabulary. Taming Recurrent Neural Networks for Better SummarizationTwo types of summarization：Extractive （You might think of these approaches as like a highlighter.） Abstractive（By the same analogy, these approaches are like a pen.）The great majority of existing approaches to automatic summarization are extractive – mostly because it is much easier to select text than it is to generate text from scratch.但是一个问题在于，只是使用 extrative way 可能得到相同的words，Problem 1: The summaries sometimes reproduce factual details inaccurately (e.g. Germany beat Argentina 3-2). This is especially common for rare or out-of-vocabulary words such as 2-0.Problem 2: The summaries sometimes repeat themselves (e.g. Germany beat Germany beat Germany beat…)Easier Copying with Pointer-Generator Networks。这个跟 attention 不是很相关，简单说就是In this way, the pointer-generator network is a best of both worlds, combining both extraction (pointing) and abstraction (generating). To tackle Problem 2 (repetitive summaries), we use a technique called coverage. The idea is that we use the attention distribution to keep track of what’s been covered so far, and penalize the network for attending to same parts again. elmo (e l mo)elmo 产生一个 embedding 是根据 context 产生的。ELMo is a model generates embeddings for a word based on the context it appears thus generating slightly different embeddings for each of its occurrence.（感觉理解一个概念都是 根据其 for example 进行理解的）For example, the word “play” in the sentence above using standard word embeddings encodes multiple meanings such as the verb to play or in the case of the sentence a theatre production. In standard word embeddings such as Glove, Fast Text or Word2Vec each instance of the word play would have the same representation. 参考blog:https://towardsdatascience.com/beyond-word-embeddings-part-2-word-vectors-nlp-modeling-from-bow-to-bert-4ebd4711d0ec http://www.abigailsee.com/2017/04/16/taming-rnns-for-better-summarization.html https://skymind.ai/wiki/word2vec]]></content>
      <categories>
        <category>NLP</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Hyper-parameter Optimization for Machine Learning]]></title>
    <url>%2F2019%2F05%2F22%2FHyperparameter-optimization-for-machine-learning%2F</url>
    <content type="text"><![CDATA[Following are four common methods of hyperparameter optimization for machine learning in order of increasing efficiency: Manual Grid search Random search Bayesian model-based optimization Random SearchFirst we will implement a common technique for hyperparameter optimization: random search. Each iteration, we choose a random set of model hyperparameters from a search space. Empirically, random search is very effective, returning nearly as good results as grid search with a significant reduction in time spent searching. However, it is still an uninformed method in the sense that it does not use past evaluations of the objective function to inform the choices it makes for the next evaluation. Random search uses the following four parts, which also are used in Bayesian hyperparameter optimization: Domain: values over which to search Optimization algorithm: pick the next values at random! (yes this qualifies as an algorithm) Objective function to minimize: in this case our metric is cross validation ROC AUC Results history that tracks the hyperparameters tried and the cross validation metric Random search can be implemented in the Scikit-Learn library using RandomizedSearchCV, however, because we are using Early Stopping (to determine the optimal number of estimators), we will have to implement the method ourselves (more practice!). This is pretty straightforward, and many of the ideas in random search will transfer over to Bayesian hyperparameter optimization. 123456789101112131415161718192021222324252627282930313233343536373839# Load librariesfrom scipy.stats import uniformfrom sklearn import linear_model, datasetsfrom sklearn.model_selection import RandomizedSearchCV# data and model# Load datairis = datasets.load_iris()X = iris.datay = iris.target# Create logistic regressionlogistic = linear_model.LogisticRegression()# Create Hyperparameter Search Space# Create regularization penalty space# 如果比较少，那么久枚举出来penalty = ['l1', 'l2']# 如果是有规律的连续的，就使用这种方式列举出来# Create regularization hyperparameter distribution using uniform distributionC = uniform(loc=0, scale=4)# Create hyperparameter optionshyperparameters = dict(C=C, penalty=penalty)# cv: cross validation, This approach involves randomly dividing the set of observations into k groups, or folds, of approximately equal size. The first fold is treated as a validation set, and the method is fit on the remaining k − 1 folds.# Create randomized search 5-fold cross validation and 100 iterationsclf = RandomizedSearchCV(logistic, hyperparameters, random_state=1, n_iter=100, cv=5, verbose=0, n_jobs=-1)# Fit randomized searchbest_model = clf.fit(X, y)# View best hyperparameters# 注意这种获取best params 的方式print('Best Penalty:', best_model.best_estimator_.get_params()['penalty'])print('Best C:', best_model.best_estimator_.get_params()['C'])# Predict target vectorbest_model.predict(X) grid search and random search 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364import numpy as npfrom time import timefrom scipy.stats import randint as sp_randintfrom sklearn.model_selection import GridSearchCVfrom sklearn.model_selection import RandomizedSearchCVfrom sklearn.datasets import load_digitsfrom sklearn.ensemble import RandomForestClassifier# get some datadigits = load_digits()X, y = digits.data, digits.target# build a classifierclf = RandomForestClassifier(n_estimators=20)# Utility function to report best scoresdef report(results, n_top=3): for i in range(1, n_top + 1): candidates = np.flatnonzero(results['rank_test_score'] == i) for candidate in candidates: print("Model with rank: &#123;0&#125;".format(i)) print("Mean validation score: &#123;0:.3f&#125; (std: &#123;1:.3f&#125;)".format( results['mean_test_score'][candidate], results['std_test_score'][candidate])) print("Parameters: &#123;0&#125;".format(results['params'][candidate])) print("")# specify parameters and distributions to sample fromparam_dist = &#123;"max_depth": [3, None], "max_features": sp_randint(1, 11), "min_samples_split": sp_randint(2, 11), "bootstrap": [True, False], "criterion": ["gini", "entropy"]&#125;# run randomized searchn_iter_search = 20random_search = RandomizedSearchCV(clf, param_distributions=param_dist, n_iter=n_iter_search, cv=5, iid=False)start = time()random_search.fit(X, y)print("RandomizedSearchCV took %.2f seconds for %d candidates" " parameter settings." % ((time() - start), n_iter_search))report(random_search.cv_results_)# use a full grid over all parametersparam_grid = &#123;"max_depth": [3, None], "max_features": [1, 3, 10], "min_samples_split": [2, 3, 10], "bootstrap": [True, False], "criterion": ["gini", "entropy"]&#125;# run grid searchgrid_search = GridSearchCV(clf, param_grid=param_grid, cv=5, iid=False)start = time()grid_search.fit(X, y)print("GridSearchCV took %.2f seconds for %d candidate parameter settings." % (time() - start, len(grid_search.cv_results_['params'])))report(grid_search.cv_results_) Random search without in-built function:write it yourself, more information.https://github.com/WillKoehrsen/hyperparameter-optimization/blob/master/Bayesian%20Hyperparameter%20Optimization%20of%20Gradient%20Boosting%20Machine.ipynb Bayesian Hyperparameter OptimizationThe one-sentence summary of Bayesian hyperparameter optimization is: build a probability model of the objective function and use it to select the most promising hyperparameters to evaluate in the true objective function. The basic idea is: spend a little more time selecting the next hyperparameters in order to make fewer calls to the objective function. In the case of hyperparameter optimization, the objective function is the validation error of a machine learning model using a set of hyperparameters. The aim is to find the hyperparameters that yield the lowest error on the validation set in the hope that these results generalize to the testing set. Evaluating the objective function is expensive because it requires training the machine learning model with a specific set of hyperparameters. Ideally, we want a method that can explore the search space while also limiting evaluations of poor hyperparameter choices. Bayesian hyperparameter tuning uses a continually updated probability model to “concentrate” on promising hyperparameters by reasoning from past results. 有很多基于这种思想的实现，hyperopt 只是其中一种There are several Bayesian optimization libraries in Python which differ in the algorithm for the surrogate of the objective function. In this article, we will work with Hyperopt, which uses the Tree Parzen Estimator (TPE) Other Python libraries include Spearmint (Gaussian Process surrogate) and SMAC (Random Forest Regression). There are four parts to a Bayesian Optimization problem: Objective Function: what we want to minimize, in this case the validation error of a machine learning model with respect to the hyperparameters （原来model 中的 objective function） Domain Space: hyperparameter values to search over （调参空间） Optimization algorithm: method for constructing the surrogate model and choosing the next hyperparameter values to evaluate （loss 和调参空间的 新的关系） Result history: stored outcomes from evaluations of the objective function consisting of the hyperparameters and validation loss （result 没有什么好说的） 其中的 Bayesian Hyperparameter Optimization using Hyperopt是可以好好学习的。 data scientists 这种东西更加贴近于 data scientist 真的。参考一：https://github.com/WillKoehrsen/hyperparameter-optimization/blob/master/Bayesian%20Hyperparameter%20Optimization%20of%20Gradient%20Boosting%20Machine.ipynb 参考二：https://github.com/WillKoehrsen/hyperparameter-optimization/blob/master/Introduction%20to%20Bayesian%20Optimization%20with%20Hyperopt.ipynb]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Introduction to Natural Language Processing for Text]]></title>
    <url>%2F2019%2F05%2F21%2FIntroduction-to-Natural-Language-Processing-for-Text%2F</url>
    <content type="text"><![CDATA[Natural Language Processing is used to apply machine learning algorithms to text and speech. For example, we can use it to create systems like speech recognition, document summarization, machine translation, spam detection, named entity recognition, question answering, autocomplete, predictive typingand so on. NLTK (Natural Language Toolkit) is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to many corpora and lexical resources. Also, it contains a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning. Best of all, NLTK is a free, open source, community-driven project. In this article, we’ll cover the following topics.这些功能都是可以使用nltk 进行实现的。text Lemmatization 比如，单词“cars”词形还原后的单词为“car”，单词“ate”词形还原后的单词为“eat”。 Sentence Tokenization这个的级别就是 将段落 tokenization成一个个的句子。Sentence tokenization (also called sentence segmentation) is the problem of dividing a string of written language into its component sentences. The idea here looks very simple. In English and some other languages, we can split apart the sentences whenever we see a punctuation mark.（标点符号） Word Tokenization这个token 相对于 sentence level ，颗粒度变得更小。Word tokenization (also called word segmentation) is the problem of dividing a string of written language into its component words. In English and many other languages using some form of Latin alphabet, space is a good approximation of a word divider. Text Lemmatization and Stemming这种操作如果被认为是一种 normalization，那么一个优点就是加快了运行的速度。从不同的形式到统一的形式，这可以认为减少了变量。感觉这个更加涉及语法，语法树之类的东西。For grammatical reasons, documents can contain different forms of a wordsuch as drive, drives, driving. Also, sometimes we have related words with a similar meaning, such as nation, national, nationality. Stemming and lemmatization are special cases of normalization. However, they are different from each other. Stemming usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes.Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma. Stop Words因为 stop words往往是带了 noise rather than useful information，所以这个是要去掉的。Stop words are words which are filtered out before or after processing of text. When applying machine learning to text, these words can add a lot of noise. That’s why we want to remove these irrelevant words. 这个是stop words dictionary 是一种过滤词表，是可以根据应用的不同，然后 change的。Stop words usually refer to the most common words such as “and”, “the”, “a” in a language, but there is no single universal list of stopwords. The list of the stop words can change depending on your application. 在存储 stopword 的时候使用 set rather than list 主要原因是 much faster than search operations in a set.You might wonder why we convert our list into a set. Set is an abstract data type that can store unique values, without any particular order. The search operation in a set is much faster than the search operation in a list. For a small number of words, there is no big difference, but if you have a large number of words it’s highly recommended to use the set type. RegexA kind of search pattern. A regular expression, regex, or regexp is a sequence of characters that define a search pattern. Let’s see some basics. 12345678910. - match any character except newline\w - match word\d - match digit\s - match whitespace\W - match not word\D - match not digit\S - match not whitespace[abc] - match any of a, b, or c[^abc] - not match a, b, or c[a-g] - match a character between a &amp; g 这个解释说明了为什么在正则表达式 中使用 r”” 作为一种前缀。因为正则表达是中 ”\“ 的使用和 python 中的”\” 使用有冲突。简而言之，如果加上了 r”” 那么这个就是一种完全的 正则表达式的语法了。 Regular expressions use the backslash character (‘\’) to indicate special forms or to allow special characters to be used without invoking their special meaning. This collides with Python’s usage of the same character for the same purpose in string literals; for example, to match a literal backslash, one might have to write ‘\\‘ as the pattern string, because the regular expression must be \, and each backslash must be expressed as \ inside a regular Python string literal.The solution is to use Python’s raw string notation for regular expression patterns; backslashes are not handled in any special way in a string literal prefixed with ‘r’. So r”\n” is a two-character string containing ‘\’ and ‘n’, while “\n” is a one-character string containing a newline. Usually, patterns will be expressed in Python code using this raw string notation. An example, 1234import resentence = "The development of snowboarding was inspired by skateboarding, sledding, surfing and skiing."pattern = r"[^\w]"print(re.sub(pattern, " ", sentence)) Bag of words Machine learning algorithms cannot work with raw text directly, we need to convert the text into vectors of numbers. This is called feature extraction.The bag-of-words model is a popular and simple feature extraction technique used when we work with text. It describes the occurrence of each word within a document. 这个是 bag of words的”特点“： order or structure of words 没有了。Any information about the order or structure of words is discarded. That’s why it’s called a bag of words. This model is trying to understand whether a known word occurs in a document, but don’t know where is that word in the document. The intuition is that similar documents have similar contents. Also, from a content, we can learn something about the meaning of the document. To use this model, we need to: Design a vocabulary of known words (also called tokens) Choose a measure of the presence of known words 第一种 measure 方式：最简单的方式是 “occurrence” ，如果出现了 标为1 否则标为0；这种是最为简单的 bag of words 最的方式，这四个是一一对应的。注意体会。 The complexity of the bag-of-words model comes in deciding how to design the vocabulary of known words (tokens) and how to score the presence of known words. In some cases, we can have a huge amount of data and in this cases, the length of the vector that represents a document might be thousands or millions of elements. Furthermore, each document may contain only a few of the known words in the vocabulary.Therefore the vector representations will have a lot of zeros. These vectors which have a lot of zeros are called sparse vectors. They require more memory and computational resources.We can decrease the number of the known words when using a bag-of-words model to decrease the required memory and computational resources. We can use the text cleaning techniques we’ve already seen in this article before we create our bag-of-words model: 减少 dictionary size 的方式。 Ignoring punctuationRemoving the stop words from our documentsReducing the words to their base form (Text Lemmatization and Stemming)Fixing misspelled words n-gram 的思想是很广泛：通过 sequence of words，这个是可以增加文本的表达力的。An n-gram is a sequence of a number of items (words, letter, numbers, digits, etc.). In the context of text corpora, n-grams typically refer to a sequence of words. A unigram is one word, a bigram is a sequence of two words, a trigram is a sequence of three words etc. 关于如何去 score the presence of word： 这里是有三种方式的。We saw one very simple approach - the binary approach (1 for presence, 0 for absence).Some additional scoring methods are:Counts. Count the number of times each word appears in a document.Frequencies. Calculate the frequency that each word appears in document out of all the words in the document. TF-IDF 这个语境 是相对于 frequency 而言的，关键词是不一定有 频率所决定，而一些 rarer or domain-specific words 可能是更加常见的。One problem with scoring word frequency is that the most frequent words in the document start to have the highest scores. These frequent words may not contain as much “informational gain” to the model compared with some rarer and domain-specific words. One approach to fix that problem is to penalize words that are frequent across all the documents. This approach is called TF-IDF. TF-IDF, short for term frequency-inverse document frequency is a statistical measure used to evaluate the importance of a word to a document in a collection or corpus. 参考资料https://towardsdatascience.com/introduction-to-natural-language-processing-for-text-df845750fb63]]></content>
      <categories>
        <category>NLP</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[剑指offer-其他]]></title>
    <url>%2F2019%2F05%2F17%2F%E5%89%91%E6%8C%87offer-%E5%85%B6%E4%BB%96%2F</url>
    <content type="text"><![CDATA[这是剑指offer 系列四部曲中的最后一部，因为有些算法题目类别数量太少就汇总到了”其他“, 比如位运算、正则匹配等。第一部关于字符串和数组，第二部是栈、队列、链表和树， 第三部递归、回溯和动态规划。 二进制中1的个数 输入一个整数，输出该数二进制表示中1的个数。其中负数用补码表示。123456789101112131415161718192021class Solution:def NumberOf1(self, n): # write code here if n ==0: return 0 counts =0 if n&gt; 0: counts =self.number_of_positive(n) else: n = abs(n)-1 counts =32 - self.number_of_positive(n) return countsdef number_of_positive(self, n): if n ==0: return 0 counts =0 while n: counts += (n&amp;1) n =n &gt;&gt;1 return count 数值的整数次方 给定一个double类型的浮点数base和int类型的整数exponent。求base的exponent次方。 1234567891011121314151617181920212223242526272829303132333435class Solution:"""这个就是边界条件比较多而已，需要分别判断 base 和 exponent 的正负"""def Power(self, base, exponent): # write code here if base ==0 and exponent != 0: return 0 if base != 0 and exponent ==0: return 1 flag =1 if base &lt;=0 and (exponent %2 ==1): flag =-1 base = abs(base) result =1 if exponent &gt; 0: reverse =0 else: reverse =1 exponent =abs(exponent) if exponent %2 ==0: result = base * base for i in range(exponent//2 -1): result = result * result else: result = base * base for i in range(exponent//2 -1): result = result * result result = result * base if reverse: result = 1.0/ result return result* flag 最小的K个数 输入n个整数，找出其中最小的K个数。例如输入4,5,1,6,2,7,3,8这8个数字，则最小的4个数字是1,2,3,4,。 12345678910class Solution:# 想说的是既然是使用这种开源的库函数 那么就记住这种函数名字def GetLeastNumbers_Solution(self, tinput, k): # write code here if len(tinput) &lt;k: return [] import heapq res = heapq.nsmallest(k, tinput) return res 整数中1出现的次数（从1到n整数中1出现的次数） 求出1~13的整数中1出现的次数,并算出100~1300的整数中1出现的次数？为此他特别数了一下1~13中包含1的数字有1、10、11、12、13因此共出现6次,但是对于后面问题他就没辙了。ACMer希望你们帮帮他,并把问题更加普遍化,可以很快的求出任意非负整数区间中1出现的次数（从1 到 n 中1出现的次数）。 12345678910111213141516171819202122232425262728293031323334 class Solution: # 数字的基本结构分成 weights +working_num + n%base 这三个部分 # 然后一个while 循环是处理一个数字 def NumberOf1Between1AndN_Solution(self, n): # write code here if n &lt;1: return 0 num =n counts =0 base =1 while num: weights =num%10 num = num//10 counts += base * num if weights ==1: counts += (n%base) +1 elif weights &gt;1: counts += base base *=10 return counts ``` - 把数组排成最小的数&gt; 输入一个正整数数组，把数组里所有数字拼接起来排成一个数，打印能拼接出的所有数字中最小的一个。例如输入数组&#123;3，32，321&#125;，则打印出这三个数字能排成的最小数字为321323。```python class Solution: def PrintMinNumber(self, numbers): # write code here sorted_list =sorted(numbers, cmp= lambda a,b :cmp(str(a)+ str(b), str(b)+str(a))) # 这个时候已经排好序，然后只要一个个连接起来就行了 return ''.join(map(str, sorted_list)) 丑数 把只包含质因子2、3和5的数称作丑数（Ugly Number）。例如6、8都是丑数，但14不是，因为它包含质因子7。 习惯上我们把1当做是第一个丑数。求按从小到大的顺序的第N个丑数。 123456789101112131415161718192021class Solution:# 在进行 append 操作的时候去重，def GetUglyNumber_Solution(self, index): # write code here if index &lt;1: return 0 list1 =[1] # 意味着只能是 append() 操作了 i, j, k =0,0,0 while len(list1) &lt; index: num =min( list1[i] *2, list1[j]*3, list1[k] *5) if num &gt; list1[-1]: list1.append(num) if num ==list1[i] *2: i +=1 elif num ==list1[j] *3: j +=1 else: k +=1 return list1[-1] 正则表达式匹配 请实现一个函数用来匹配包括’.’和’‘的正则表达式。模式中的字符’.’表示任意一个字符，而’‘表示它前面的字符可以出现任意次（包含0次）。 在本题中，匹配是指字符串的所有字符匹配整个模式。例如，字符串”aaa”与模式”a.a”和”abaca”匹配，但是与”aa.a”和”ab*a”均不匹配 123456789101112131415161718192021222324252627class Solution:# s, pattern都是字符串# 思想就是 dp, 转换方程 dp[i][j] i 表示 string 的index j表示 pattern 的index# 有三种转换方程 dp[i][j] ==dp[i-1][j-1] or dp[i][j] =dp[i][j-2] or dp[i][j-1] # https://www.youtube.com/watch?v=l3hda49XcDE 心中一定要有这个表格, a[i][j] 这个更像是一种指针def match(self, s, pattern): if len(s) ==0 and len(pattern) ==0: return True dp =[[ False for _ in range(len(pattern) +1)] for _ in range(len(s) +1)] dp[0][0] =True for j in range(1, len(pattern)+1): if pattern[j-1] =="*": dp[0][j] =dp[0][j-2] for i in range(1, len(s)+1): for j in range(1, len(pattern)+1): if pattern[j-1] ==s[i-1] or pattern[j-1] ==".": dp[i][j] =dp[i-1][j-1] elif pattern[j-1] =="*": dp[i][j] =dp[i][j-2] if s[i-1] ==pattern[j-2] or pattern[j-2] ==".": dp[i][j] =dp[i][j] or dp[i-1][j] else: dp[i][j] =False return dp[len(s)][len(pattern)] 数据流中的中位数 如何得到一个数据流中的中位数？如果从数据流中读出奇数个数值，那么中位数就是所有数值排序之后位于中间的数值。如果从数据流中读出偶数个数值，那么中位数就是所有数值排序之后中间两个数的平均值。我们使用Insert()方法读取数据流，使用GetMedian()方法获取当前读取数据的中位数。 123456789101112131415161718class Solution: """ 对于数据流 这个应该是第二次接触了，需要使用一个全局变量 """ # 虽然知道这个使用 堆的思想是更优的，搜索时间可以O（1）， 堆的调整是 O(log n) # 但是没有什么很好的教程，所以我也没有学会啊 def __init__(self): self.list1 = [] def Insert(self, num): self.list1.append(num) def GetMedian(self, ch): length = len(self.list1) # 我记得有一个更加快一些 self.list1 = sorted(self.list1) if length % 2 == 0: return (self.list1[length // 2] + self.list1[length // 2 - 1]) / 2.0 else: return self.list1[length // 2] 滑动窗口的最大值 给定一个数组和滑动窗口的大小，找出所有滑动窗口里数值的最大值。例如，如果输入数组{2,3,4,2,6,2,5,1}及滑动窗口的大小3，那么一共存在6个滑动窗口，他们的最大值分别为{4,4,6,6,6,5}； 针对数组{2,3,4,2,6,2,5,1}的滑动窗口有以下6个： {[2,3,4],2,6,2,5,1}， {2,[3,4,2],6,2,5,1}， {2,3,[4,2,6],2,5,1}， {2,3,4,[2,6,2],5,1}， {2,3,4,2,[6,2,5],1}， {2,3,4,2,6,[2,5,1]}。 123456789class Solution:# 最简单的模拟滑动窗口 的过程def maxInWindows(self, num, size): slip =[] if not num or len(num) &lt; size or size ==0: return [] for i in range(len(num) -size +1): slip.append(max(num[i:i+size])) return slip]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>剑指offer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[剑指offer-递归、回溯和动态规划]]></title>
    <url>%2F2019%2F05%2F17%2F%E5%89%91%E6%8C%87offer-%E9%80%92%E5%BD%92-%E5%9B%9E%E6%BA%AF%E5%92%8C%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%2F</url>
    <content type="text"><![CDATA[这是剑指offer 系列四部曲中的第三部：递归、回溯和动态规划。第一部关于字符串和数组，第二部是栈、队列、链表和树， 最后一部分在这里。 斐波那契数列 大家都知道斐波那契数列，现在要求输入一个整数n，请你输出斐波那契数列的第n项（从0开始，第0项为0）。n&lt;=39 1234567891011121314class Solution:# python 中list 的初始化，最开始的是从0 开始，所以是需要多进行一个初始化的def Fibonacci(self, n): # write code here if n ==0: return 0 if n ==1: return 1 arr = [0]*(n+1) arr[0] =0 arr[1] =1 for i in range(2,n+1): arr[i] =arr[i-1] + arr[i-2] return arr[n] 跳台阶 一只青蛙一次可以跳上1级台阶，也可以跳上2级。求该青蛙跳上一个n级的台阶总共有多少种跳法（先后次序不同算不同的结果）。 123456789101112131415class Solution:def jumpFloor(self, number): # write code here if number ==1: return 1 if number ==2: return 2 arr =[0]*(number +1) arr[1] =1 arr[2] =2 for i in range(3, number +1): arr[i] =arr[i-1] + arr[i-2] return arr[number] 跳台阶2 一只青蛙一次可以跳上1级台阶，也可以跳上2级……它也可以跳上n级。求该青蛙跳上一个n级的台阶总共有多少种跳法。 12345678910111213class Solution:"""在使用 for循环的时候，注意 range() 这种取值，究竟是使用 range() 作为次数的计量；还是要使用range 中的index 。两者是不相同的操作，尤其是对于前后的取值。"""def jumpFloorII(self, number): # write code here if number ==1: return 1 nums =1 for i in range(number-1): nums = nums *2 return nums 矩形覆盖 我们可以用21的小矩形横着或者竖着去覆盖更大的矩形。请问用n个21的小矩形无重叠地覆盖一个2*n的大矩形，总共有多少种方法？ 123456789101112131415161718"""既然结果只是最后一个解，那么就没有必要保存中间变量，所以只是，所以空间复杂度从O（n） -&gt; O(1) ，这个是超级nice的"""class Solution: def rectCover(self, number): # write code here if number &lt;=0: return 0 if number &lt;=2: return number # 只是两个变量罢了 a ,b = 1,2 while number &gt;2: a, b = b, a+b number -=1 return b 机器人的运动范围 地上有一个m行和n列的方格。一个机器人从坐标0,0的格子开始移动，每一次只能向左，右，上，下四个方向移动一格，但是不能进入行坐标和列坐标的数位之和大于k的格子。 例如，当k为18时，机器人能够进入方格（35,37），因为3+5+3+7 = 18。但是，它不能进入方格（35,38），因为3+5+3+8 = 19。请问该机器人能够达到多少个格子？ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960 class Solution: def movingCount(self, threshold ,rows, cols): visited =[False] *(rows*cols) count =self.movingCountCore(threshold, rows, cols, 0, 0, visited) return count def movingCountCore(self, threshold, rows, cols, row, col, visited): count =0 # 就是这个访问记录是需要进行变化的， 如果是false ，然后访问之后 是需要设置为 true的 if self.check(threshold, rows, cols, row, col, visited): visited[row*cols +col] =True count = 1+ self.movingCountCore(threshold, rows, cols, row, col-1, visited)+\ self.movingCountCore(threshold, rows, cols, row, col +1, visited) + \ self.movingCountCore(threshold, rows, cols, row+1, col, visited) + \ self.movingCountCore(threshold, rows, cols, row-1, col, visited) return count def check(self, threshold, rows, cols, row, col, visited): if row&gt;=0 and row&lt; rows and col&gt;=0 and col&lt;cols and self.judge(threshold, row, col) and not visited[row*cols +col]: return True else: return False def judge(self, threshold, i, j): if sum(map(int, str(i)+str(j))) &lt;= threshold: return True else: return False``` - 矩阵中的路径&gt; 请设计一个函数，用来判断在一个矩阵中是否存在一条包含某字符串所有字符的路径。路径可以从矩阵中的任意一个格子开始，每一步可以在矩阵中向左，向右，向上，向下移动一个格子。如果一条路径经过了矩阵中的某一个格子，则之后不能再次进入这个格子。 例如 a b c e s f c s a d e e 这样的3 X 4 矩阵中包含一条字符串"bcced"的路径，但是矩阵中不包含"abcb"路径，因为字符串的第一个字符b占据了矩阵中的第一行第二个格子之后，路径不能再次进入该格子。```python class Solution: # 递归 这个是 true or false 判断类型的。 # 思路：先是 rows* cols 这样的全部遍历 def hasPath(self, matrix, rows, cols, path): # 如果使用 [ for _in range(rows) ] for _ in range(cols) ， 这个是有结构的 rows* cols assist =[True] * rows*cols for i in range(rows): for j in range(cols): if self.rightPath( matrix, rows, cols, i, j, path, assist): return True return False def rightPath(self, matrix, rows, cols, i, j, path, assist): if not path: return True index =i*cols +j if i&lt;0 or i&gt;= rows or j&lt;0 or j&gt;=cols or matrix[index]!=path[0] or assist[index] ==False: return False assist[index] =False if (self.rightPath(matrix, rows, cols, i+1, j, path[1:], assist) or self.rightPath(matrix, rows, cols, i-1, j, path[1:], assist)or self.rightPath(matrix, rows, cols, i, j-1, path[1:], assist) or self.rightPath(matrix, rows, cols, i, j+1, path[1:], assist)): return True assist[index] =True return False]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>剑指offer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[剑指Offer-栈、队列、链表和树]]></title>
    <url>%2F2019%2F05%2F17%2F%E5%89%91%E6%8C%87Offer-%E6%A0%88-%E9%98%9F%E5%88%97-%E9%93%BE%E8%A1%A8%E5%92%8C%E6%A0%91%2F</url>
    <content type="text"><![CDATA[这是剑指offer 系列四部曲中的第二部：栈、队列、链表和树。第一部关于字符串和数组，第三部是递归、回溯和动态规划， 最后一部分在这里。 从尾到头打印链表 输入一个链表，按链表值从尾到头的顺序返回一个ArrayList。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374 # -*- coding:utf-8 -*- # class ListNode: # def __init__(self, x): # self.val = x # self.next = None class Solution: # 返回从尾部到头部的列表值序列，例如[1,2,3] # += , -= 这个都是同一种类型的 def printListFromTailToHead(self, listNode): # write code here arraylist =[] head =listNode while head !=None: arraylist += [head.val] # 这个在这里等效于 arraylist.append(head.val) head = head.next return arraylist[::-1] ``` - 重建二叉树&gt; 输入某二叉树的前序遍历和中序遍历的结果，请重建出该二叉树。假设输入的前序遍历和中序遍历的结果中都不含重复的数字。例如输入前序遍历序列&#123;1,2,4,7,3,5,6,8&#125;和中序遍历序列&#123;4,7,2,1,5,3,8,6&#125;，则重建二叉树并返回。```python # -*- coding:utf-8 -*- # class TreeNode: # def __init__(self, x): # self.val = x # self.left = None # self.right = None class Solution: # 返回构造的TreeNode根节点 # 需要理解在前序遍历中是先遍历左子树的，并且中序和前序中左子树的个数是不会变的 def reConstructBinaryTree(self, pre, tin): # write code here if len(pre) ==0: return None root =TreeNode(pre[0]) # 这个index 函数是需要记住的 index =tin.index(pre[0]) # 这里也是需要修改的 # pre 和 tin都是需要空出一个 root.value 的位置，只不过选择空的位置是不一样的 root.left = self.reConstructBinaryTree(pre[1:index+1], tin[:index]) root.right =self.reConstructBinaryTree(pre[index+1:], tin[index+1:]) return root``` - 用两个栈实现队列&gt; 用两个栈来实现一个队列，完成队列的Push和Pop操作。 队列中的元素为int类型。```python class Solution: # 首先是需要两个函数公共可以访问的stack，也就是list # 函数中如何去操作 公共变量 def __init__(self): self.stack1 =[] self.stack2 =[] def push(self, node): self.stack1 += [node] # 就是这个pop 是必须的操作，所以你需要提前解决这个异常 def pop(self): if self.stack2 ==[] and self.stack1 ==[]: return None if self.stack1 != [] and self.stack2 ==[]: while self.stack1: self.stack2 += [self.stack1.pop()] return self.stack2.pop() 链表中倒数第k个结点 输入一个链表，输出该链表中倒数第k个结点。 1234567891011121314151617181920212223242526272829303132# -*- coding:utf-8 -*-# class ListNode:# def __init__(self, x):# self.val = x# self.next = None"""尝试使用两个指针版本p1 p2 并且这种 length 在命名上是需要规范的, 并且这种指针操作，最好是拷贝出来进行操作不管怎么说，还是应该求解出来 length of listNode，这种才是正途可以使用两个指针，"""class Solution: def FindKthToTail(self, head, k): # write code here if head ==None or k&lt;=0: return None p1 =head p2 =head len1 =0 while p1: len1 +=1 p1 =p1.next if k&gt; len1: return None p1 =head while k: p1 =p1.next k -=1 while p1: p1 =p1.next p2 =p2.next return p2 反转链表 输入一个链表，反转链表后，输出新链表的表头。 1234567891011121314151617181920212223242526# -*- coding:utf-8 -*-# class ListNode:# def __init__(self, x):# self.val = x# self.next = None"""修改链表是需要三个指针的 pre, cur, next_node 如果对三个指针名进行命名好了，那么这个就是成功的一般了， 这个不容易想到的是设置pre =None ，这个是一个细节经验性的问题"""class Solution: # 返回ListNode def ReverseList(self, pHead): # write code here if pHead ==None: return None pre =None cur =pHead while cur: next_node =cur.next cur.next =pre pre, cur =cur, next_node return pre 合并两个排序的链表 输入两个单调递增的链表，输出两个链表合成后的链表，当然我们需要合成后的链表满足单调不减规则。 123456789101112131415161718192021222324252627282930313233# def __init__(self, x):# self.val = x# self.next = None"""就是在使用两个或者多个 index (p1 or p2) 遍历的时候，一个常见的错误就是忘记了不断更新index"""class Solution: # 返回合并后列表 def Merge(self, pHead1, pHead2): # write code here if pHead1 ==None: return pHead2 if pHead2 ==None: return pHead1 head =ListNode(-1) head1 =head p1 =pHead1 p2 =pHead2 while p1 and p2: if p1.val &lt; p2.val: head.next =p1 p1 =p1.next else: head.next =p2 p2 =p2.next head =head.next if p1 ==None: head.next =p2 if p2 ==None: head.next =p1 return head1.next 树的子结构 输入两棵二叉树A，B，判断B是不是A的子结构。（ps：我们约定空树不是任意一个树的子结构） 12345678910111213141516171819202122232425262728293031323334# -*- coding:utf-8 -*-# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = None"""分成两部：首先寻找两个根节点的值是否相同；然后判断子树是否完全相同subTree 这个函数就是判断子树是否完全相同的，所以函数的功能一定要搞好"""class Solution: def HasSubtree(self, pRoot1, pRoot2): if not pRoot1: return False if not pRoot2: return False result =False if pRoot1.val ==pRoot2.val: result =self.subTree(pRoot1, pRoot2) if result ==False: result = self.HasSubtree(pRoot1.left, pRoot2) or self.HasSubtree(pRoot1.right, pRoot2) return result def subTree(self, root1, root2): if not root2: return True if not root1: return False if root1.val ==root2.val: return self.subTree(root1.left, root2.left) and self.subTree(root1.right, root2.right) return False 二叉树的镜像 操作给定的二叉树，将其变换为源二叉树的镜像。 123456789101112131415161718192021# -*- coding:utf-8 -*-# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = None"""就是在某个左（右）子树是None 的情况下，这个也是可以进行交换的，结束的标志应该是根节点是否为空"""class Solution: # 返回镜像树的根节点 def Mirror(self, root): # write code here if not root: return None root.left , root.right =root.right, root.left if root.left: self.Mirror(root.left) if root.right: self.Mirror(root.right) return root 包含min函数的栈 定义栈的数据结构，请在该类型中实现一个能够得到栈中所含最小元素的min函数（时间复杂度应为O（1））。 1234567891011121314151617181920212223242526272829# -*- coding:utf-8 -*-"""这个栈中最小的元素是变化的，好好理解一下，如果弹出了一个比较大的元素，那么栈中最小的元素是不变的所含元素的最小元素top() and min() 操作是不需要删除元素的， pop 是删除了元素"""class Solution: def __init__(self): self.all_list =[] self.min_list =[] def push(self, node): # write code here if not self.min_list: self.min_list.append(node) else: self.min_list.append(min(node, self.min())) self.all_list.append(node) def pop(self): self.all_list.pop() self.min_list.pop() # write code here def top(self): return self.all_list[-1] # write code here def min(self): return self.min_list[-1] 栈的压入、弹出序列 输入两个整数序列，第一个序列表示栈的压入顺序，请判断第二个序列是否可能为该栈的弹出顺序。假设压入栈的所有数字均不相等。例如序列1,2,3,4,5是某栈的压入顺序，序列4,5,3,2,1是该压栈序列对应的一个弹出序列，但4,3,5,1,2就不可能是该压栈序列的弹出序列。（注意：这两个序列的长度是相等的） 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748 class Solution: # 使用一个list 模拟压站和出战的过程 def IsPopOrder(self, pushV, popV): if not pushV: return False tmp =[] while popV: if tmp and popV[0] ==tmp[-1]: popV.pop(0) tmp.pop() elif pushV: tmp.append(pushV.pop(0)) else: return False return True ``` - 从上往下打印二叉树&gt; 从上往下打印出二叉树的每个节点，同层节点从左至右打印。```python # class TreeNode: # def __init__(self, x): # self.val = x # self.left = None # self.right = None class Solution: # 返回从上到下每个节点值列表，例：[1,2,3] # 层序遍历二叉树， 这个跟数据结构 队列有类似的 # nodes 装上结点，然后vlaues 装上数值 def PrintFromTopToBottom(self, root): # write code here if not root: return [] nodes =[] values =[] nodes.append(root) while nodes: node =nodes.pop(0) values.append(node.val) if node.left: nodes.append(node.left) if node.right: nodes.append(node.right) return values 二叉搜索树的后序遍历序列 输入一个整数数组，判断该数组是不是某二叉搜索树的后序遍历的结果。如果是则输出Yes,否则输出No。假设输入的数组的任意两个数字都互不相同。 123456789101112131415class Solution:# 后序遍历结果， 最后一个是根节点，这个是递归的思想# 二叉搜索树， 左子树小于根节点，右子树大于根节点def VerifySquenceOfBST(self, sequence): # write code here if not sequence: return False root =sequence[-1] for i in range(len(sequence)): if sequence[i] &gt; root: break for j in range(i,len(sequence)): if sequence[j] &lt; root: return False return True 二叉树中和为某一值的路径 输入一颗二叉树的跟节点和一个整数，打印出二叉树中结点值的和为输入整数的所有路径。路径定义为从树的根结点开始往下一直到叶结点所经过的结点形成一条路径。(注意: 在返回值的list中，数组长度大的数组靠前) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586 # class TreeNode: # def __init__(self, x): # self.val = x # self.left = None # self.right = None class Solution: # 返回二维列表，内部每个列表表示找到的路径 # 深度优先 dfs() 这样的一个算法 def FindPath(self, root, expectNumber): # write code here if not root: return [] self.target =expectNumber paths =[] self.dfs(root, [root.val], paths) return paths def dfs(self, root, path, paths): if not root.left and not root.right and sum(path) ==self.target: paths.append(path) if root.left: self.dfs(root.left, path+[root.left.val], paths) if root.right: self.dfs(root.right, path+[root.right.val], paths) ``` - 复杂链表的复制&gt; 输入一个复杂链表（每个节点中有节点值，以及两个指针，一个指向下一个节点，另一个特殊指针指向任意一个节点），返回结果为复制后复杂链表的head。（注意，输出结果中请不要返回参数中的节点引用，否则判题程序会直接返回空）```python # -*- coding:utf-8 -*- # class RandomListNode: # def __init__(self, x): # self.label = x # self.next = None # self.random = None class Solution: # 返回 RandomListNode # 首先是结点的复制和 next 指针的连接， 然后是random 指针的连接，最后是选择出复制的结点 def Clone(self, pHead): # write code here if not pHead: return None self.clone_nodes(pHead) self.connect_nodes(pHead) return self.select_nodes(pHead) def clone_nodes(self, head): if not head: return None while head: cloned = RandomListNode(head.label) cloned.next = head.next head.next = cloned head = cloned.next def connect_nodes(self, head): if not head: return None while head: cloned = head.next if head.random: cloned.random = head.random.next head = cloned.next def select_nodes(self, head): if not head: return None cloned =cloned_head =None # 这个if 的作用是为了保存一个 cloned_head的结点， # 一定要从这个功能出发 if head: cloned =cloned_head =head.next head.next =cloned.next head =head.next while head: cloned.next =head.next cloned =cloned.next head.next =cloned.next head =head.next return cloned_head 二叉搜索树与双向链表 输入一棵二叉搜索树，将该二叉搜索树转换成一个排序的双向链表。要求不能创建任何新的结点，只能调整树中结点指针的指向。 12345678910111213141516171819202122232425262728293031323334# -*- coding:utf-8 -*-# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: # 使用的树的结构 表示一种双向链表 # 二叉搜索树 ，左子树小于根节点，右子树大于根节点 # 中序遍历得到就是一种排好序的结构 # 只能调整树中结点指针的指向 def Convert(self, pRootOfTree): # write code here if not pRootOfTree: return None tree = pRootOfTree res = [] self.helper(tree, res) for i in range(len(res) - 1): res[i].right = res[i + 1] res[i + 1].left = res[i] # 这个返回值也是比较鬼畜呀， 就是需要这样返回 return res[0]def helper(self, root, res): if not root: return None if root.left: self.helper(root.left, res) res.append(root) if root.right: self.helper(root.right, res) 两个链表的第一个公共结点 输入两个链表，找出它们的第一个公共结点。 123456789101112131415161718192021222324252627282930313233343536373839404142434445 # -*- coding:utf-8 -*- # class ListNode: # def __init__(self, x): # self.val = x # self.next = None class Solution: # 两个指针指向的是 一个结点，一个内存的两个指向 # 将可能不同长度的两个链表转换成相同长度的两个链表的比较，使用 def FindFirstCommonNode(self, pHead1, pHead2): # write code here if not pHead1 or not pHead2: return None p1 =pHead1 p2 =pHead2 while p1 !=p2: # 这个p1 只能指向了最后一个结点，但最后一个节点不一定相同 p1 = pHead2 if not p1 else p1.next p2 = pHead1 if not p2 else p2.next return p1 ``` - 二叉树的深度&gt; 输入一棵二叉树，求该树的深度。从根结点到叶结点依次经过的结点（含根、叶结点）形成树的一条路径，最长路径的长度为树的深度。```python # -*- coding:utf-8 -*- # class TreeNode: # def __init__(self, x): # self.val = x # self.left = None # self.right = None class Solution: """ 分别求解 左右子树的深度，然后max(left, right) 这样的操作 """ def TreeDepth(self, pRoot): if not pRoot: return 0 left =self.TreeDepth(pRoot.left) +1 right =self.TreeDepth(pRoot.right) +1 # 这个return 是最后执行一次的，然后上面那个都是不断的在进行递归加深 # 这个 left right 已经完成了，最后的效果只是 返回 max(left, right) 这样子 return max(left, right) 平衡二叉树 输入一棵二叉树，判断该二叉树是否是平衡二叉树。 123456789101112131415161718192021222324252627# -*- coding:utf-8 -*-# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: # 递归常见的都会有两个return 跳出条件，一个是异常的条件，一个是正确的返回 def get_depth(self, root): if not root: return 0 left =self.get_depth(root.left) right =self.get_depth(root.right) return max(left, right) +1 def IsBalanced_Solution(self, pRoot): if not pRoot: return True left =self.get_depth(pRoot.left) right =self.get_depth(pRoot.right) if abs(left-right) &gt;1: return False return self.IsBalanced_Solution(pRoot.left) and self.IsBalanced_Solution(pRoot.right) 链表中环的入口结点 给一个链表，若其中包含环，请找出该链表的环的入口结点，否则，输出null。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556 # -*- coding:utf-8 -*- # class ListNode: # def __init__(self, x): # self.val = x # self.next = None class Solution: # 现在长个记性吧，在使用next 这样的时候 要先判断这个是不是存在的 def EntryNodeOfLoop(self, pHead): # write code here if not pHead or not pHead.next or not pHead.next.next: return None twoTimes =pHead.next.next oneTime =pHead.next while twoTimes != oneTime: twoTimes =twoTimes.next.next oneTime =oneTime.next twoTimes =pHead while twoTimes != oneTime: twoTimes =twoTimes.next oneTime =oneTime.next return twoTimes ``` - 删除链表中重复的结点&gt; 在一个排序的链表中，存在重复的结点，请删除该链表中重复的结点，重复的结点不保留，返回链表头指针。 例如，链表1-&gt;2-&gt;3-&gt;3-&gt;4-&gt;4-&gt;5 处理后为 1-&gt;2-&gt;5```python # -*- coding:utf-8 -*- # class ListNode: # def __init__(self, x): # self.val = x # self.next = None class Solution: def deleteDuplication(self, pHead): # write code here head =ListNode(-1) head.next =pHead curr =pHead last =head while curr and curr.next: #val =curr.val # 这个条件比较简单，所以可以放到前面 if curr.val !=curr.next.val: curr =curr.next last =last.next else: # 这个条件 curr 还是需要注意一下的 val =curr.val # python 中 condition1 and condition2 这种是有先后顺序的 # 可能是存在短路现象的， 如果 curr 不成立，那么后面的是不会执行的 # 草拟 while curr and val ==curr.val: curr =curr.next last.next =curr return head.next 二叉树的下一个结点 给定一个二叉树和其中的一个结点，请找出中序遍历顺序的下一个结点并且返回。注意，树中的结点不仅包含左右子结点，同时包含指向父结点的指针。 123456789101112131415161718192021222324252627282930# -*- coding:utf-8 -*-# class TreeLinkNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = None# self.next = Noneclass Solution: # https://blog.csdn.net/fuxuemingzhu/article/details/79723819 # 这个是求解中序遍历中某个结点的下一个结点 # 这pNode 就是一个普通的结点 def GetNext(self, pNode): # write code here if not pNode: return None # 如果存在右结点 if pNode.right: pNode =pNode.right while pNode.left: pNode =pNode.left return pNode # 如果是父节点的左子树 else: # 这里使用 pNode.next 表示父节点 while pNode.next: if pNode == pNode.next.left: return pNode.next # 这个是右结点 pNode =pNode.next return None 对称的二叉树 请实现一个函数，用来判断一颗二叉树是不是对称的。注意，如果一个二叉树同此二叉树的镜像是同样的，定义其为对称的。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273 # -*- coding:utf-8 -*- # class TreeNode: # def __init__(self, x): # self.val = x # self.left = None # self.right = None class Solution: # 镜像的概念 和递归 # isSame() 这个就是判断两个子树是否镜像的操作 def isSame(self, p, q): if not p and not q: return True # 好好思考 下面这两个跳出条件为什么是不合适的 #if not p and q: # return False #if p and not q: # return False if p and q and p.val == q.val: return self.isSame(p.left, q.right) and self.isSame(p.right, q.left) def isSymmetrical(self, pRoot): # write code here # 最开始的条件 如果都是 none 那么这个是对称的 if not pRoot: return True if pRoot.left and not pRoot.right: return False if not pRoot.left and pRoot.right: return False return self.isSame(pRoot.left, pRoot.right) ``` - 按之字形顺序打印二叉树&gt; 请实现一个函数按照之字形打印二叉树，即第一行按照从左到右的顺序打印，第二层按照从右至左的顺序打印，第三行按照从左到右的顺序打印，其他行以此类推。```python # -*- coding:utf-8 -*- # class TreeNode: # def __init__(self, x): # self.val = x # self.left = None # self.right = None class Solution: # 层序遍历 + 偶数翻转 # https://blog.csdn.net/fuxuemingzhu/article/details/79724959 def level(self, root, level, res): """ root: the root of tree level: res: result """ if not root: return if len(res) ==level: res.append([]) res[level].append(root.val) if root.left: self.level(root.left, level+1, res) if root.right: self.level(root.right, level+1, res) def Print(self, pRoot): # write code here if not pRoot: return [] res =[] self.level(pRoot, 0, res) for level in range(1, len(res), 2): res[level] =res[level][::-1] return res 把二叉树打印成多行 从上到下按层打印二叉树，同一层结点从左至右输出。每一层输出一行。 123456789101112131415161718192021222324252627282930# -*- coding:utf-8 -*-# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: # 返回二维列表[[1,2],[4,5]] def level(self, root, level, res): # 你这里也没有说要返回值的意思呀，这个直接是 return if not root: return if level ==len(res): res.append([]) res[level].append(root.val) if root.left: self.level(root.left, level +1, res) if root.right: #res[level] =self.level(root.right, level+1, res) # 因为这个是 传的值，所以不需要使用返回值的 self.level(root.right, level+1, res)def Print(self, pRoot): if not pRoot: return [] res =[] self.level(pRoot, 0, res) return res 序列化二叉树 请实现两个函数，分别用来序列化和反序列化二叉树 # -*- coding:utf-8 -*- # class TreeNode: # def __init__(self, x): # self.val = x # self.left = None # self.right = None class Solution: """ 序列化就是从树结构 转成字符串的结构；反之，也是成立的。 使用先序遍历的方法。 https://suixinblog.cn/2019/03/target-offer-serialize-binary-tree.html#%E4%BB%A3%E7%A0%81 """ def __init__(self): self.flag =-1 def Serialize(self, root): # write code here if not root: return "#" return str(root.val) +","+self.Serialize(root.left) +","+self.Serialize(root.right) def Deserialize(self, s): # write code here self.flag +=1 string =s.split(',') if self.flag &gt; len(string): return None root =None if string[self.flag] !='#': root =TreeNode(int(string[self.flag])) root.left =self.Deserialize(s) root.right =self.Deserialize(s) return root 二叉搜索树的第k个结点 给定一棵二叉搜索树，请找出其中的第k小的结点。例如， （5，3，7，2，4，6，8） 中，按结点数值大小顺序第三小结点的值为4。 # class TreeNode: # def __init__(self, x): # self.val = x # self.left = None # self.right = None class Solution: # 返回对应节点TreeNode # 二叉搜索树： 左子树小于根节点 右子树大于根节点，前序遍历就是一个排序的 # 我的思路，前序遍历，存为list 然后就自然的返回就行 # 这个应该是中序遍历的方式最后是有序的，而非前序遍历 # 这个保存的结点，输出的也是结点，然后题目是为了便于理解，然后给出的是结点的值 def pre(self, root, result): if not root: return if root.left: self.pre(root.left, result) result.append(root) if root.right: self.pre(root.right, result) def KthNode(self, pRoot, k): # write code here if not pRoot: return result =[] self.pre(pRoot, result) if len(result) &lt; k or k&lt;1: return return result[k-1]]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>剑指offer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[剑指Offer-字符串和数组]]></title>
    <url>%2F2019%2F05%2F17%2F%E5%89%91%E6%8C%87Offer-%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%92%8C%E6%95%B0%E7%BB%84%2F</url>
    <content type="text"><![CDATA[题目都是来自牛客网在线刷题中的剑指offer。最近找实习工作，作为刷题记录，顺便从考察知识点的角度分类整理。主要分成以下四大类： 字符串、数组 链表、树 递归、回溯、动态规划 其他, 比如位运算、正则匹配等 不同类别以一章介绍，之后可能会随时update。这是剑指offer 系列四部曲中的第一部：。第一部关于字符串和数组，第二部是栈、队列、链表和树，第三部递归、回溯和动态规划， 最后一部分在这里。 二维数组中的查找 在一个二维数组中（每个一维数组的长度相同），每一行都按照从左到右递增的顺序排序，每一列都按照从上到下递增的顺序排序。请完成一个函数，输入这样的一个二维数组和一个整数，判断数组中是否含有该整数。 12345678910111213141516171819class Solution:# array 二维列表def Find(self, target, array): # write code here # 可以尝试一下 坐标移动的思想, 所以这个就是一种坐标移动的思想 # 就是一个条件有了之后 那么接下来的else 也可以顺着就写上来的 rows =len(array) -1 cols =len(array[0]) -1 row =0 col =cols while row &lt;= rows and col &gt;=0: if array[row][col] == target: return True elif array[row][col] &gt; target: col -=1 else: row +=1 return False 替换空格 请实现一个函数，将一个字符串中的每个空格替换成“\%20”。例如，当字符串为We Are Happy.则经过替换之后的字符串为We\%20Are\%20Happy。 123456789101112class Solution:# s 源字符串def replaceSpace(self, s): # write code here # python 中的 str 就是 array of char converted ="" for ch in s: if ch ==" ": converted += "%20" else: converted += ch return converted 旋转数组的最小数字 把一个数组最开始的若干个元素搬到数组的末尾，我们称之为数组的旋转。 输入一个非减排序的数组的一个旋转，输出旋转数组的最小元素。 例如数组{3,4,5,1,2}为{1,2,3,4,5}的一个旋转，该数组的最小值为1。 NOTE：给出的所有元素都大于0，若数组大小为0，请返回0。 12345678910111213141516171819202122232425262728class Solution:'''解题的关键点： 设置第一个元素为假定的min_value,然后注意这个是非减的数组（注意处理等号的情况）'''def minNumberInRotateArray(self, rotateArray): # write code here # 因为这个条件是非减，所以可能有重复的元素 min_value =rotateArray[0] left =0 right =len(rotateArray) -1 while right -left &gt;1: mid =int((left+right)/2) # if else 这种条件是具有唯一性的，所以颠倒顺序是没有问题的 if rotateArray[mid] &lt;= rotateArray[right]: right =mid elif rotateArray[left] &lt;= rotateArray[mid]: left =mid else: if rotateArray[left] ==rotateArray[mid] and rotateArray[left] ==rotateArray[right]: for i in range(len(rotateArrray)): if rotateArray[i] &lt; min_value: min_value =rotateArray[i] right =i # 现在有点感觉 为什么使用 min_value =array[right]，因为预设最小值是在左边 min_value =rotateArray[right] return min_value 调整数组顺序使奇数位于偶数前面 输入一个整数数组，实现一个函数来调整该数组中数字的顺序，使得所有的奇数位于数组的前半部分，所有的偶数位于数组的后半部分，并保证奇数和奇数，偶数和偶数之间的相对位置不变。 1234567891011121314151617"""list 和list 之间的连接，使用 list1+ list2 就是可以的"""class Solution: def reOrderArray(self, array): # write code here if array ==[]: return [] odd_list =[] even_list =[] for item in array: if item% 2 ==1: odd_list.append(item) else: even_list.append(item) return odd_list+even_list 顺时针打印矩阵 输入一个矩阵，按照从外向里以顺时针的顺序依次打印出每一个数字，例如，如果输入如下4 X 4矩阵： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 则依次打印出数字1,2,3,4,8,12,16,15,14,13,9,5,6,7,11,10. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364 class Solution: """ 最后的 必须是三种特殊情况，如果修改成两种，那么”只剩下一个“ 这种特殊情况就被计算了两次 """ # matrix类型为二维列表，需要返回列表 def printMatrix(self, matrix): rows = len(matrix) cols = len(matrix[0]) top = 0 left = 0 down = rows - 1 right = cols - 1 result = [] while top &lt; down and left &lt; right: for j in range(left, right + 1): result.append(matrix[top][j]) top += 1 for i in range(top, down + 1): result.append(matrix[i][right]) right -= 1 for j in range(right, left - 1, -1): result.append(matrix[down][j]) down -= 1 for i in range(down, top - 1, -1): result.append(matrix[i][left]) left += 1 if top == down and left &lt; right: for j in range(left, right + 1): result.append(matrix[top][j]) if top &lt; down and left == right: for i in range(top, down + 1): result.append(matrix[i][left]) if top ==down and left ==right: result.append(matrix[top][left]) return result ``` - 字符串的排列&gt; 输入一个字符串,按字典序打印出该字符串中字符的所有排列。例如输入字符串abc,则打印出由字符a,b,c所能排列出来的所有字符串abc,acb,bac,bca,cab和cba。```python class Solution: # 递归： 变换方程： 第一字母和 剩下的所有的字母 def Permutation(self, ss): # write code here if not ss: return [] res =[] self.helper(ss, '', res) return sorted(list(set(res))) def helper(self, ss, path, res): if not ss: res.append(path) for i in range(len(ss)): self.helper(ss[:i]+ss[i+1:], path+ss[i], res) 数组中出现次数超过一半的数字 数组中有一个数字出现的次数超过数组长度的一半，请找出这个数字。例如输入一个长度为9的数组{1,2,3,2,2,2,5,4,2}。由于数字2在数组中出现了5次，超过数组长度的一半，因此输出2。如果不存在则输出0。 1234567891011121314151617181920class Solution:# 如果存在这样的数字，那么这个数字的频数一定是大于其他所有的频数# 所以可以统计一下这个def MoreThanHalfNum_Solution(self, numbers): # write code here if not numbers: return 0 target =numbers[0] nums =0 # 统计出现次数最多的数字 for i in numbers: if target ==i: nums +=1 elif nums ==0: target =i nums =1 else: nums -=1 res = target if numbers.count(target) &gt; len(numbers)//2 else 0 return res 连续子数组的最大和 HZ偶尔会拿些专业问题来忽悠那些非计算机专业的同学。今天测试组开完会后,他又发话了:在古老的一维模式识别中,常常需要计算连续子向量的最大和,当向量全为正数的时候,问题很好解决。但是,如果向量中包含负数,是否应该包含某个负数,并期望旁边的正数会弥补它呢？例如:{6,-3,-2,7,-15,1,2,2},连续子向量的最大和为8(从第0个开始,到第3个为止)。给一个数组，返回它的最大连续子序列的和，你会不会被他忽悠住？(子向量的长度至少是1) 12345678910111213141516class Solution:# largest , sum 这是两个不同的状态# 注意初始化def FindGreatestSumOfSubArray(self, array): # write code here if not array: return [] largest =array[0] sum_of_array =0 for i in array: sum_of_array += i if sum_of_array &gt; largest: largest =sum_of_array elif sum_of_array &lt;0: sum_of_array =0 return largest 第一个只出现一次的字符 在一个字符串(0&lt;=字符串长度&lt;=10000，全部由字母组成)中找到第一个只出现一次的字符,并返回它的位置, 如果没有则返回 -1（需要区分大小写）. 123456789101112131415161718class Solution:def FirstNotRepeatingChar(self, s): if not s: return -1 dict1 =self.Counter(s) for index, value in enumerate(s): if dict1[value] ==1: return index return -1# 自己实现的Counter 是按照key 的字母顺序进行排列的，而from collections import Counter 什么排列都不是def Counter(self, s): dict1 =&#123;&#125; for val in s: if val not in dict1: dict1[val] =1 else: dict1[val] = dict1[val] +1 return dict1 数组中的逆序对 在数组中的两个数字，如果前面一个数字大于后面的数字，则这两个数字组成一个逆序对。输入一个数组,求出这个数组中的逆序对的总数P。并将P对1000000007取模的结果输出。 即输出P\%1000000007 1234567891011121314151617181920212223242526272829303132333435363738394041# -*- coding:utf-8 -*-class Solution: # 这个一斤难出天机了 先不看了 # 这个到后来就已经背下来了 def InversePairs(self, data): # write code here if not data: return 0 temp = [i for i in data] return self.mergeSort(temp, data, 0, len(data)-1) % 1000000007 def mergeSort(self, temp, data, low, high): if low &gt;= high: temp[low] = data[low] return 0 mid = (low + high) / 2 #不懂 data 和 temp 为什么是颠倒顺序 left = self.mergeSort(data, temp, low, mid) right = self.mergeSort(data, temp, mid+1, high) count = 0 i = low j = mid+1 index = low while i &lt;= mid and j &lt;= high: if data[i] &lt;= data[j]: temp[index] = data[i] i += 1 else: temp[index] = data[j] count += mid-i+1 j += 1 index += 1 while i &lt;= mid: temp[index] = data[i] i += 1 index += 1 while j &lt;= high: temp[index] = data[j] j += 1 index += 1 return count + left + right 数字在排序数组中出现的次数 统计一个数字在排序数组中出现的次数。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051 class Solution: # 二叉查找， 递归 def GetNumberOfK(self, data, k): # write code here # 这个是有两个跳出条件的，一个是正确的跳出，一个是不正确的跳出 if not data: return 0 mid =len(data)//2 if data[mid] ==k: left =right =mid for i in range(mid-1, -1, -1): if data[i] ==k: left -=1 for i in range(mid+1, len(data)): if data[i] ==k: right +=1 return right -left +1 # 一半一半的舍去数据 elif data[mid] &lt;k: return self.GetNumberOfK(data[mid+1:], k) else: return self.GetNumberOfK(data[:mid-1], k) ``` - 数组中只出现一次的数字&gt; 一个整型数组里除了两个数字之外，其他的数字都出现了两次。请写程序找出这两个只出现一次的数字。```python class Solution: # 返回[a,b] 其中ab是出现一次的两个数字 # 使用异或的性质，如果只有一个不同，其他的偶次出现，那么全部异或的结果 # 就是那个单一的数字 def FindNumsAppearOnce(self, array): # write code here remain, index =0, 1 for num in array: remain = remain ^ num # 找出第一个是1 的位置 # index 都是 while (remain &amp; index) ==0: index = index &lt;&lt;1 res1, res2 =0,0 for num in array: # 这个条件必须是0, 表示两个在这个位数是相同的， if num &amp; index ==0: res1 =res1 ^ num else: res2 =res2 ^ num return [res1, res2] 和为S的连续正数序列 小明很喜欢数学,有一天他在做数学作业时,要求计算出9~16的和,他马上就写出了正确答案是100。但是他并不满足于此,他在想究竟有多少种连续的正数序列的和为100(至少包括两个数)。没多久,他就得到另一组连续正数和为100的序列:18,19,20,21,22。现在把问题交给你,你能不能也很快的找出所有和为S的连续正数序列?Good Luck! 123456789101112131415161718class Solution:#滑动窗口的思想def FindContinuousSequence(self, tsum): # write code here if tsum&lt;2: return [] left =1 right =left +1 res =[] while left &lt; tsum//2+1: if sum(range(left,right)) ==tsum: res.append(range(left, right)) left +=1 elif sum(range(left,right)) &lt; tsum: right +=1 else: left +=1 return res 和为S的两个数字 输入一个递增排序的数组和一个数字S，在数组中查找两个数，使得他们的和正好是S，如果有多对数字的和等于S，输出两个数的乘积最小的。 12345678910111213141516171819class Solution:# 上一个题目是 一个区间，这个题目是连个数字# 限制条件，乘积最小，意味着连个相距最远， 所以在初始化的时候， 选择 0, len()-1 # 这种初始化def FindNumbersWithSum(self, array, tsum): # write code here if len(array) &lt;2: return [] left =0 right =len(array)-1 while left &lt; right: if array[left] +array[right] ==tsum: return [array[left], array[right]] elif array[left] +array[right] &lt; tsum: left +=1 else: right -=1 return [] 左旋转字符串 汇编语言中有一种移位指令叫做循环左移（ROL），现在有个简单的任务，就是用字符串模拟这个指令的运算结果。对于一个给定的字符序列S，请你把其循环左移K位后的序列输出。例如，字符序列S=”abcXYZdef”,要求输出循环左移3位后的结果，即“XYZdefabc”。是不是很简单？OK，搞定它！ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849 class Solution: def LeftRotateString(self, s, n): # write code here if len(s) &lt; n: return '' return s[n:]+s[:n]``` - 翻转单词顺序列&gt; 牛客最近来了一个新员工Fish，每天早晨总是会拿着一本英文杂志，写些句子在本子上。同事Cat对Fish写的内容颇感兴趣，有一天他向Fish借来翻看，但却读不懂它的意思。例如，“student. a am I”。后来才意识到，这家伙原来把句子单词的顺序翻转了，正确的句子应该是“I am a student.”。Cat对一一的翻转这些单词顺序可不在行，你能帮助他么？```python class Solution: """ 写一个 reverse() 函数, 用于 list 的reverse """ def Reverse(self, s, left, right): while left&lt;right: s[left], s[right] = s[right], s[left] left +=1 right -=1 def ReverseSentence(self, s): # write code here if not s: return s # from immutable string to mutable list s =list(s) self.Reverse(s, 0, len(s)-1) # 然后实现的是 每个单词的反转，使用 start, end 表示某个单词的首末 # 感觉python 中的这种赋值方式是特有的，应该是整体操作的机制，先是把右边的全部保存起来 # 然后依次赋值给左边 # 这种思想都是 一个变量，然后inplace 的操作, 所以对于空格的操作是 let it go # 这个是第二次翻转，属于每个单词进行翻转 start, end =0,0 # 这个小于号 是python 中特有的坑，真正能够访问的区间是 [0, len(s)-1] 这样的区间 while start &lt;len(s): if s[start] ==" ": start +=1 end +=1 elif end ==len(s) or s[end] ==" ": self.Reverse(s, start, end-1) # update 操作 end +=1 start =end else: end +=1 return "".join(s) 扑克牌顺子 LL今天心情特别好,因为他去买了一副扑克牌,发现里面居然有2个大王,2个小王(一副牌原本是54张^_^)…他随机从中抽出了5张牌,想测测自己的手气,看看能不能抽到顺子,如果抽到的话,他决定去买体育彩票,嘿嘿！！“红心A,黑桃3,小王,大王,方片5”,“Oh My God!”不是顺子…..LL不高兴了,他想了想,决定大\小 王可以看成任何数字,并且A看作1,J为11,Q为12,K为13。上面的5张牌就可以变成“1,2,3,4,5”(大小王分别看作2和4),“So Lucky!”。LL决定去买体育彩票啦。 现在,要求你使用这幅牌模拟上面的过程,然后告诉我们LL的运气如何， 如果牌能组成顺子就输出true，否则就输出false。为了方便起见,你可以认为大小王是0。 12345678910111213141516171819202122232425262728class Solution:# 我的理解这个实际上是一个填空游戏，如果大小王的总数大于等于空的个数， 那么就返回true，否则就是false# 有时候不会，是因为不知道这个输入样例 长什么样子，缺省的特殊符号是如何？ 所以导致不会# 而并不是算法不会"""空缺是1 意味着这两个数字是连续的 比如说 1 2， 这个big -small ==1, 所以这个空缺是0，不用进行填充。"""def IsContinuous(self, numbers): # write code here if not numbers: return False numbers.sort() # sort() sorted() 这种怎么使用，返回值是什么，这些基本的东西 zeros =numbers.count(0) gaps =0 left =zeros # 因为这个是排序之后的结果，所以可以这样进行操作 right=left +1 # 实际上还是两个指针， 所以可以使用两个指针进行操作 # 本质上是两个 相邻指针在进行移动，因为是排序之后，所以没有问题 while right &lt;len(numbers): if numbers[left] ==numbers[right]: return False gaps += numbers[right] -numbers[left] -1 left =right right +=1 # 这种是真的 很简洁， gaps &lt;= zeros 少去了很多if else的判断 return gaps &lt;=zeros 孩子们的游戏(圆圈中最后剩下的数) 每年六一儿童节,牛客都会准备一些小礼物去看望孤儿院的小朋友,今年亦是如此。HF作为牛客的资深元老,自然也准备了一些小游戏。其中,有个游戏是这样的:首先,让小朋友们围成一个大圈。然后,他随机指定一个数m,让编号为0的小朋友开始报数。每次喊到m-1的那个小朋友要出列唱首歌,然后可以在礼品箱中任意的挑选礼物,并且不再回到圈中,从他的下一个小朋友开始,继续0…m-1报数….这样下去….直到剩下最后一个小朋友,可以不用表演,并且拿到牛客名贵的“名侦探柯南”典藏版(名额有限哦!!^_^)。请你试着想下,哪个小朋友会得到这份礼品呢？(注：小朋友的编号是从0到n-1) 1234567891011121314class Solution:# mod 求余 的操作， a mod b ==c ,说明 a除以b 之后余数是c# https://blog.csdn.net/gatieme/article/details/51435055， 从做题思路上讲解的比较好# n 个小朋友，然后是m 个编号def LastRemaining_Solution(self, n, m): # write code here if n&lt; 1 or m&lt;1: return -1 last =0 for i in range(2, n+1): # 这个相当于 是一个 “挑选人” 的逆过程， 因为使用的 mod 操作就是取余的操作 last =(last +m) %i return last 求1+2+3+…+n 求1+2+3+…+n，要求不能使用乘除法、for、while、if、else、switch、case等关键字及条件判断语句（A?B:C）。 1234567891011121314class Solution:# 如果你想使用全局变量，那么放在 __init__ 中就是一个很好的方式def __init__(self): self.ans =0def Sum_Solution(self, n): # write code here self.recur(n) return self.ans# n&gt;0 就是一个短路条件，这个直接决定了后面递归会不会继续执行下去，也就是跳出的条件# 至于会不会回到原来最初的状态，这个是不重要的，最后的结果是 self.ans ，false之后直接使用这个就行了def recur(self, n): self.ans += n n -=1 return n &gt;0 and self.Sum_Solution(n) 不用加减乘除做加法 写一个函数，求两个整数之和，要求在函数体内不得使用+、-、*、/四则运算符号。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354class Solution:"""不能使用四则运算符，我们就可以使用位运算符。对这两个数在更底层的角度上进行运算。也就是从 01 这种子串的角度进行考虑至少这个是思路上清晰的https://blog.csdn.net/derrantcm/article/details/46798763 这个博客对于数的运算过程和 位运算是如何一一对应的。分为不进位相加和进位相加。"""# 这个只能是正整数的操作，真是热力狗# 这个弄出来是真心不容易哈def add(self, num1, num2): while num2 != 0: carry = num1 &amp; num2 num1 = num1 ^ num2 # 这个应该理解为到高位 而不是*2 这样的操作 num2 = carry &lt;&lt; 1 return num1 def sub(self, num1, num2): while num2 != 0: carry = (~num1) &amp; num2 num1 = num1 ^ num2 num2 = carry &lt;&lt; 1 return num1 def Add(self, num1, num2): if num1 &gt;= 0 and num2 &gt;=0: result =self.add(num1, num2) elif num1&gt;0 and num2 &lt;0: flag = 1 if num1 &gt; abs(num2) else -1 #num2 =abs(num2) # keep num1 bigger than num2 if num1 &lt; abs(num2): num1, num2 =abs(num2), num1 result =self.sub(num1, abs(num2)) result = result *flag elif num1 &lt; 0 and num2&gt;0: flag = 1 if abs(num1) &lt; num2 else -1 if abs(num1)&lt; num2: num1, num2 = num2, abs(num1) result =self.sub(abs(num1), num2) result =result * flag else: flag =-1 num1 =abs(num1) num2 =abs(num2) result =self.add(num1, num2) result =result * flag return result 把字符串转换成整数 将一个字符串转换成一个整数(实现Integer.valueOf(string)的功能，但是string不符合数字要求时返回0)，要求不能使用字符串转换整数的库函数。 数值为0或者字符串不是一个合法的数值则返回0。 12345678910111213141516171819202122class Solution:# 有很多不合法的输入，比如空字符串，正负号，非数字字符 数据溢出，所以从反面考虑更加简单一些# 合法的输入只有数字和符号位 + 和-def StrToInt(self, s): # write code here int_list=['0', '1', '2', '3', '4', '5', '6','7', '8', '9', '+', '-'] if s ==" ": return 0 sum1 =0 flag =1 # 正负号 for string in s: if string not in int_list: return 0 if string =="+": flag =1 continue elif string =="-": flag = -1 continue else: sum1 =sum1 *10 +int_list.index(string) return sum1*flag 数组中重复的数字 在一个长度为n的数组里的所有数字都在0到n-1的范围内。 数组中某些数字是重复的，但不知道有几个数字是重复的。也不知道每个数字重复几次。请找出数组中任意一个重复的数字。 例如，如果输入长度为7的数组{2,3,1,0,2,5,3}，那么对应的输出是第一个重复的数字2。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788 class Solution: # 这里要特别注意~找到任意重复的一个值并赋值到duplication[0] # 函数返回True/False # 第二种方式，如果这个是有序的 那么 numbers[i] ==i 这个是成立的 def duplicate(self, numbers, duplication): # write code here length =len(numbers) for i in range(length): while i != numbers[i]: if numbers[numbers[i]] == numbers[i]: duplication[0] =numbers[i] return True else: numbers[numbers[i]], numbers[i] =numbers[i], numbers[numbers[i]] return False``` - 构建乘积数组&gt; 给定一个数组A[0,1,...,n-1],请构建一个数组B[0,1,...,n-1],其中B中的元素B[i]=A[0]*A[1]*...*A[i-1]*A[i+1]*...*A[n-1]。不能使用除法。```python class Solution: # 思路： 转换成图形的就容易想一些， https://blog.csdn.net/u010005281/article/details/80200398 # 代码：https://blog.csdn.net/fuxuemingzhu/article/details/79718543 # A 是一个list ，只是自己构建的是一个矩阵 def multiply(self, A): # write code here ans =[] tmp =1 length =len(A) # 值得是 rows # 首先是下三角形 各个部分的数值的相乘， 从上往下遍历 for i in range(length): ans.append(tmp) tmp *= A[i] tmp =1 # 上三角形 从下往上进行遍历 for i in range(length-1, -1, -1): ans[i] *= tmp tmp *= A[i] return ans - 表示数值的字符串&gt; 请实现一个函数用来判断字符串是否表示数值（包括整数和小数）。例如，字符串"+100","5e2","-123","3.1416"和"-1E-16"都表示数值。&gt; 但是"12e","1a3.14","1.2.3","+-5"和"12e+4.3"都不是。```python class Solution: # s字符串 # 第一种方法是 float()强转，一种是 re 正则表达式匹配 最后一种逻辑判断之类的 # 以 e 为分割符，分成front and behind 两部分，behind 长度不能为0 或者出现 . # digit的判断，+- 只能出现在首位， . 只能出现一次 """ https://github.com/leeguandong/Interview-code-practice-python/blob/master/%E5%89%91%E6%8C%87offer/%E8%A1%A8%E7%A4%BA%E6%95%B0%E5%80%BC%E7%9A%84%E5%AD%97%E7%AC%A6%E4%B8%B2.py """ def isNumeric(self, s): if not s or len(s) ==0: return s s =[i.lower() for i in s] if 'e' in s: index =s.index('e') front =s[:index] behind =s[index+1:] if len(behind) ==0 or '.' in behind: return False f =self.Digit(front) b =self.Digit(behind) return f and b else: isNum =self.Digit(s) return isNum def Digit(self, s): dotNum =0 allowNum =['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '.', '+', '-'] for i in range(len(s)): if s[i] not in allowNum: return False if s[i] =='.': dotNum +=1 if s[i] in '+-' and i!=0: return False if dotNum &gt;1: return False return True 字符流中第一个不重复的字符 请实现一个函数用来找出字符流中第一个只出现一次的字符。例如，当从字符流中只读出前两个字符”go”时，第一个只出现一次的字符是”g”。当从该字符流中读出前六个字符“google”时，第一个只出现一次的字符是”l”。 123456789101112131415class Solution:# 返回对应char# 这个没有了dict 那么依赖于 count 函数# 主要差别在于有了一个 字符流，是动态的，所以需要有一个大的存储的listdef __init__(self): self.list1 =[]def FirstAppearingOnce(self): # write code here for string in self.list1: if self.list1.count(string) ==1: return string return "#"def Insert(self, char): # write code here self.list1.append(char)]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>剑指offer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pygen]]></title>
    <url>%2F2019%2F04%2F24%2Fpygen%2F</url>
    <content type="text"><![CDATA[pygen功能：有关联的随机生成人名，邮箱，ID Card (ssn)，电话，地址等信息，并且可以选择保存为 pandas dataframe格式, 数据库”.db” 文件, Excel 文件和csv 文件格式，用于机器学习训练。项目地址github。 随机生成虚假个人信息具有很大的应用空间。首先，虚假的生成数据可以用于机器学习模型的“准备数据”，当真实的数据比较少或者难以获得的时候，可以使用生成数据进行训练模型，待模型调通之后，然后使用真实的模型。并且，当真实的数据集中缺少某些特征时候，可以使用这种方法进行特征的填充。比如大的数据集中缺少现居城市地址的时候，可以调用该库中的 “city_real” 进行填充。 当前使用最为广泛的是 Faker 开源库用于数据的生成。虽然该库支持中文，但是对于中文的支持力度有限，所以有时候并不能满足我的需求，比如说生成的身份证 (ssn) 和姓名所能体现的性别是不匹配(了解更多可以参考这里)、生成的姓名中缺少复姓和电话邮箱等信息不符合我们的使用习惯等等。所以我将从以下几点改进： 增强数据之间相关性 生成名字的多样性 符合国人使用习惯的邮箱电话 提供保存多种保存文件格式，更加适合机器学习的训练 中文名字有很强的性别属性。例如名字中带有“杰”“志”“宏”等字的一般为男性，带有“琬”“佩”“梅”等字的一般为女性。当然也有一些比较中性的字，例如“文”“安”“清”等，比较难猜测性别，关于这点会在另一个博客中展开，请期待。在faker 中生成的结果如下图： from faker import Faker fake = Faker(&apos;zh_CN&apos;) for _ in range(10): print(fake.name(),fake.ssn(),fake.phone_number()) 从图中可以明显的看出 “王玉梅”和 “李桂花”都是两个女性，但是这种身份证信息（ssn）都没有体现这点。关于身份证的科普信息可以从这里获得。简单来说倒数第二位表示性别信息，如果是男性就是奇数如果是女性就是偶数。faker 生成的数据是不具有数据之间的相关性的。 基于此，我们进行了改进。首先是姓名的生成，然后是性别的判断，最后再生成相应性别的身份证号码。 from pygen import pygen db =pygen() db.gen_dataframe(fields =[&apos;name&apos;, &apos;ssn&apos;, &apos;phone&apos;, &apos;email&apos;]) 效果如下： 红色线条表示姓名和性别对应一致，蓝色线条表示结果不确定（“镜阳炎” 像是一个中性的名字），绿色表示生成了含有复姓的名字，增强了数据的多样性。 从上图的 “mail” 一列可以看出邮箱前缀的命名基本上是中文名字中“姓” 和“民”的拼音组合，加强了数据之间的相关性和真实度。 另外，电话号码按照运营商分为三类：0 表示移动，1表示联通，2表示电信。 print(&apos;移动字段:&apos;) for _ in range(5): print(db.simple_ph_num(types =0)) print(&apos;联通字段:&apos;) for _ in range(5): print(db.simple_ph_num(types =1)) print(&apos;电信字段：&apos;) for _ in range(5): print(db.simple_ph_num(types =2)) 输出： 移动字段: 15023689929 16771753917 16790223946 15950129353 15271129554联通字段: 13869739303 13786227031 13950354445 15137578545 15240836142电信字段： 17172983067 15658567011 18562313243 17073127396 15543448286 最后提供了多种文件保存格式，包括”.csv”, “.db” 和”.xlsx”等格式。可以使用如下： from pygen import pygen db =pygen() db.gen_table(filename =filename, fields =[&apos;name&apos;, &apos;ssn&apos;, &apos;phone&apos;, &apos;email&apos;]) db.gen_excel(filename =filename, fields =[&apos;name&apos;, &apos;ssn&apos;, &apos;phone&apos;, &apos;email&apos;]) db.gen_csv(filename =filename, fields =[&apos;name&apos;, &apos;ssn&apos;, &apos;phone&apos;, &apos;email&apos;])]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Data Enhancement</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mode Collapse in GANs]]></title>
    <url>%2F2019%2F04%2F18%2Fmode-collapse-in-gan%2F</url>
    <content type="text"><![CDATA[Mode collapse, a failure case for GANs where the generator generate a limited diversity of samples, regardless of the input. But what causes the mode collapse? There are four reasons for that. The objective of GANs The generator, generates new data, while the discriminator evaluates it for authenticity but not for the diversity of generated instances. the generator can win by producing a polynomial number of training examples. And a low capacity discriminator cannot detect this process, thus, it cannot guide the generator to approximate the target distribution. Even if a high discriminator identifies and assigns the collapse part a low probability, then the generator will simply move from its collapsed output to focus on another fixed output point. Generator No matter the objective function is, if it only considers individual samples (without looking forward or backward) then the generator is not directly incentivised to produce diverse examples. From [1], standard GAN training corresponds exactly to updating the generator parameters using only the first term in this gradient because of a fixed discriminator during GAN training. Therefore, in standard GAN training, each generator updata step is a partial collapse towards a delta function. $$\frac { \mathrm { d } f _ { K } \left( \theta _ { G } , \theta _ { D } \right) } { \mathrm { d } \theta _ { G } } = \frac { \partial f \left( \theta _ { G } , \theta _ { D } ^ { K } \left( \theta _ { G } , \theta _ { D } \right) \right) } { \partial \theta _ { G } } + \frac { \partial f \left( \theta _ { G } , \theta _ { D } ^ { K } \left( \theta _ { G } , \theta _ { D } \right) \right) } { \partial \theta _ { D } ^ { K } \left( \theta _ { G } , \theta _ { D } \right) } \frac { \mathrm { d } \theta _ { D } ^ { K } \left( \theta _ { G } , \theta _ { D } \right) } { \mathrm { d } \theta _ { G } }$$ Some methods have been proposed. Multiple generators and weight-sharing generators are developed to capture more modes of the distribution. Discriminator The mode collapse is often explained as gradient exploding of discriminator, which comes from the imbalance between the discriminator and the generator. For example, the technique of TTUR could help discriminator to keep its optimality. But some researchers believe that this is a desirable goal since a good discriminator can give good feedback and ignore the fact. In addition, the discriminator process each example independently, the generator depends on generator, thus no mechanism to tell the outputs of the generator to become more similar to each other. The idea from [2], that we could use minibatch discrimination to help generator give better feedback A straightforward approach to handle multimodality is to take random noise vectors along with the conditional contexts as inputs, where the contexts determine the main content and noise vectors are responsible for variations.The noise vectors are ignored or of minor impacts, since cGANs pay more attention to learn from the highdimensional and structured conditional contexts. Another question Mode collapse may happen only partially?since training is stochastic progress, the input of generator network will vary and the sample drawn from the real distribution will also vary But sometimes mode collapse is not all bad news. In style transfer using GAN, we are happy to convert one image to just a good one, rather than finding all variants. Indeed, the specialization in the partial mode collapse sometimes creates higher quality images. referrences:[1]. Section 2.4 of Unrolled Generative Adversarial Networks[2]. Section 3.2 of Improved Techniques for Training GANs[3]. Mode Seeking Generative Adversarial Networks for Diverse Image Synthesis[4]. Improving Generalization and Stability of Generative Adversarial Networks]]></content>
      <categories>
        <category>Deep learning</category>
      </categories>
      <tags>
        <tag>GANs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[A Not-So-Gentle Introduction to Hyperparameters Tuning]]></title>
    <url>%2F2019%2F04%2F17%2Fa-not-so-gentle-introduction-to-hyperparameters-tuning%2F</url>
    <content type="text"><![CDATA[Setting the hyper-parameters seems like a black art that requires years of experience to acquire. Currently, there are no simple and easy ways to set hyper-parameters- specifically, batch size, learning rate, momentum, and weight decay. A grid search or random search maybe sounds like a good idea. In this blog, I’d like to share you my idea from reading papers and my projects. Hyper-parametersBatch SizeLearning rate is maybe the most important hyper-parameters, but we choose batch size firstly because large batch size needs a large learning rate in most circumstances. A general principle is: use as a large batch size as possible to fit your CPU memory or/both GPU memory. There are several reasons: larger batch sizes permit the use of larger learning rates A constant number of iterations favors larger batch sizes However, small batch sizes add regularization while large batch sizes add less. So utilize it while balancing the proper amount of regularization. Learning RateWe will introduce the idea from [Cyclical Learning Rates for Training Neural Networks][1]: Cyclical Learning Rates. Instead of monotonically decreasing the learning rate, this method lets the learning rate cyclically vary between reasonable boundary values. The essence of this learning rate policy comes from the observation that increasing the learning rate might have a short term negative effect and yet achieve a longer term beneficial effect. This observation leads to the idea of letting the learning rate vary within a range of values rather than adopting a stepwise fixed or exponentially decreasing value. That is, one sets minimum and maximum boundaries and the learning rate cyclically varies between these bounds.From Cyclical Learning Rates for Training Neural Networks An intuitive understanding of why CLR methods work comes from considering the loss function topology. Dauphin et al. argue that the difficulty in minimizing the loss arises from saddle points rather than poor local minima. Saddle points have small gradients that slow the learning process. However, increasing the learning rate allows for more rapid traversal of saddle point plateaus. But the question is that how can we find the Minimum bound and Maximum bound. There is a simple way to estimate the reasonable minimum and maximum boundary values with one training run of the network for a few epochs. It is a “LR range test”; run your model for several epochs while letting the learning rate increase linearly between low and high LR values. For example, set both the step size and max iter to the same number of iterations. In this case, the learning rate will increase linearly from the minimum value to the maximum value during this short run. Next, plot the accuracy versus learning rate. Note the learning rate value when the accuracy starts to increase and when the accuracy slows, becomes ragged, or starts to fall. These two learning rates are good choices for bounds; that is, set base-lr to the first value and set max-lr to the latter value. MomentumSince learning rate is regarded as the most important hyper-parameter to tune then momentum is also important. Like learning rates, it is valuable to set momentum as large as possible without causing instabilities during training. The large learning rate can deal with local minimum but works fail when it comes to saddle point where momentum comes to rescue. The local minimum is like the following picture.In mathematics, a saddle point or minimax point is a point on the surface of the graph of a function where the slopes (derivatives) in orthogonal directions are all zero (a critical point), but which is not a local extremum of the function. Your first step from the very top would likely take you down, but then you’d be on a flat rice terrace. The gradient would be zero, and you’d have nowhere to go. To remedy this, we employ momentum - the algorithm remembers its last step and adds some psroportion of it to the current step. This way, even if the algorithm is stuck in a flat region, or a small local minimum, it can get out and continue towards the true minimum. In summary: when performing gradient descent, learning rate measures how much the current situation affects the next step, while momentum measures how much past steps affect the next step. Weights DecayWhen training neural networks, it is common to use “weight decay,” where after each update, the weights are multiplied by a factor slightly less than 1. This prevents the weights from growing too large and can be seen as gradient descent on a quadratic regularization term. But why? Large weights might correlate with certain patterns in the input data (x), this means that the model almost hard codes certain values. This then makes our training data fit well but our test data fit less well. The idea of weight decay is simple: to prevent overfitting, every time we update a weight $w$ with the gradient $∇J$ in respect to $w$, we also subtract from it $λ∙w$. This gives the weights a tendency to decay towards zero, hence the name. L2 is a type of weights decay.$$J ( W ; X , y ) + \frac { 1 } { 2 } \lambda \cdot | W | ^ { 2 }$$ But weights decay is not necessarily true for all gradient-base algorithms and was recently shown to not be the case for adaptive gradient algorithms, such as Adam. In addition, weight decay is not the only regularization technique. In the past few years, some other approaches have been introduced such as Dropout, Bagging, Early Stop, and Parameter Sharing which work very well in NNs. Takeoff Batch Size Use as a large batch size as possible to fit your memory Learning Rate Perform a learning rate range test to identify a “large” learning rate. Momentum Test with short runs of momentum values 0.99, 0.97, 0.95, and 0.9 to get the best value for momentum. If using the 1-cycle learning rate schedule, it is better to use a cyclical momentum (CM) that starts at this maximum momentum value and decreases with increasing learning rate to a value of 0.8 or 0.85. Weights Decay A grid search to determine the proper magnitude but usually does not require more than one significant figure accuracy.A more complex dataset requires less regularization so test smaller weight decay values, such as $10^{−4} $, $10^{−5} $, $10^{−6} $, 0.A shallow architecture requires more regularization so test larger weight decay values, such as $10^{−2} $, $10^{−3} $, $10^{−4} $. References[1]. Cyclical Learning Rates for Training Neural Networks[2]. A disciplined approach to neural network hyper-parameters]]></content>
      <categories>
        <category>Deep learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Weights Initialization]]></title>
    <url>%2F2019%2F04%2F17%2Fweights-initialization%2F</url>
    <content type="text"><![CDATA[Training a neural network consists of four steps: initialize weights and biases, forward propagation, compute the loss function and backward propagation. This blog mainly focuses on the first part: weights initialization. After completing this tutorial, you will know: Four main types of weights initialization How to choose between Xavier initialization and He initialization Types of Weights Initialization Initializing weights with zero When you set all weights in a neural network to zero, the derivative with respect to loss function is the same for every $ w$ in the same layer, thus all the weights have the same values in the subsequent iteration, which makes your model equivalent to a linear model. Initializing weights randomly You can get weights like this (Python): w =np.random.randn(layer_size[l],layer_size[l-1]) The weighs follows standard normal distribution while it can potentially lead to two issues: vanishing gradients and exploding gradients. If the weights start too small, then the signal shrinks as it passes through each layer until it’s too small to be useful. If the weights start too large, then the signal grows as it passes through each layer until it’s too massive to be useful (big value in sigmoid function). Thus there are two necessary conditions to consider: The values of each activation layer won’t be zero The values of each activation layer won’t go into the area of saturation Xavier/Glorot Initialization For deep networks, we can use a heuristic to initialize the weights depending on the non-linear activation function. This applies to Xavier and He initialization. Xavier/Glorot Initialization initializes the weights in your network by drawing them from a distribution with zero mean and a specific variance.$$ { var } ( w _ { i } ) = \frac { 1 } { layer_{l-1}}$$ w=np.random.randn(layer_size[l],layer_size[l-1])*np.sqrt(1/layer_size[l-1]) In practice, it works better for layers with sigmoid or tanh function. He Initialization Using RELU or Leaky RELU is relatively robust to the vanishing/ exploding gradient issues compared with sigmoid function especially for networks that are not too deep. And it the case of Leaky RELU, it never has zero gradients. For RELU, we multiply the randomly generated values of $w$ by: $$\sqrt { \frac { 2 } { layer _ { [ l - 1 ] } } }$$ w=np.random.randn(layer_size[l],layer_size[l-1])*np.sqrt(2/layer_size[l-1]) Sometimes, we combine the idea of Xavier initialization and He initializaiton so the variance becomes the following: $$\sqrt { \frac { 2 } { layer _ { [ l - 1 ] } + \operatorname { layer } _ { [ l ] } } }$$ w=np.random.randn(layer_size[l],layer_size[l-1])*np.sqrt(2/(layer_size[l-1]+layer_size[l])) The idea behind this is that we set the weights neither too much bigger than 1 nor too much less than 1 so the gradients do not vanish or explode too quickly. TakeoffIn summary, the main difference in machine learning is the following: He initialization works better for layers with ReLu(s) activation. Xavier initialization works better for layers with sigmoid activation. Referrence:He initialization Xavier initialization]]></content>
      <categories>
        <category>Deep learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[CycleGAN & StyleGAN]]></title>
    <url>%2F2019%2F04%2F09%2Fcyclegan-stylegan%2F</url>
    <content type="text"><![CDATA[In the past few yeas, GANs have been used in lots of different applications such as generating synthetic data, style transfer, super-resolution and text2image generation. But we donn’t aim to give a overview of what GANs are made for. There are many great and detailed blogs for your understanding. If you have fun about transfer learning, especially style transfer using GANs, this post might interest you! What this post is about Main ideas of CycleGAN Keypoints in StyleGAN A Gentle Introduction of GANsWe assume the reader has some prior experience with neural networks. In addition, if you are familiar with GANs you can skip this section. The famous minimax objective function can be formulated as following:$$\min _ { \theta _ { g } } \max _ { \theta _ { d } } \left[ \mathbb { E } _ { x \sim p _ { d a t a } } \log D _ { \theta _ { d } } ( x ) + \mathbb { E } _ { z \sim p ( z ) } \log \left( 1 - D _ { \theta _ { d } } \left( G _ { \theta _ { g } } ( z ) \right) \right) \right)$$But in practical, the loss function cannot work very well. So we have alternative objective function: Gradient ascent on discriminator $$\max _ { \theta _ { d } } \left[ \mathbb { E } _ { x \sim p _ { d a t a } } \log D _ { \theta _ { d } } ( x ) + \mathbb { E } _ { z \sim p ( z ) } \log \left( 1 - D _ { \theta _ { d } } \left( G _ { \theta _ { g } } ( z ) \right) \right) \right]$$ Gradient ascent on generator$$\max _ { \theta _ { g } } \mathbb { E } _ { z \sim p ( z ) } \log \left( D _ { \theta _ { d } } \left( G _ { \theta _ { g } } ( z ) \right) \right)$$The reasoning behind this can be found in original paper. Simplily speaking, we can get higher gradient signal for bad samples, which works much better in practice.From Stanford CS231 Lecture 13 — Generative Models Main ideas of CycleGANCycleGAN was introduced in 2017 out of Berkeley, Unpaired Image-to-Image Translation Using Cycle-Coonsistent Adversarial Networks. This task is performed on unpaired data. Recent methods such as Pix2Pix depend on the availability of training examples where the samee data is availabel in both domains. However, CycleGAN is able to learning such pair information without one-to-one mapping between training data in source and target domains. Network ArchitectureWe build three networks. A generator $F$ to convert image $y$ to image $ \hat{x}$ A generator $G$ to convert image $\hat{x}$ to image $ \hat{y}$ A discriminator $D$ to idenfify real image or generated pictureSimplified version of CycleGAN architecture can be showed in the following.The function $F$ and $G$ are generator network, which consists of encoder, transformer and decoder. Encoder is extracting the features from an image which is done by convolution networks. Each convolution layer leads to extraction of progressively higher level features. We would like to transform the feature emebdding of an image from domain $X$ to that of domain $Y$. So for this, authors have used 6 layers of resnet blocks. Resnet block is a neural network layer which consists of two convolutiona layers when a residue of input ia added to the output. This is done to ensure peoperties of input of previous layers are available for later layers as well.Resnet block can be summarized in following imageThe decoder transfer embedding from $y$ back to original embedding $x$. Loss functionThere are two types of losses in CycleGAN. Besides adversarial loss, we have another loss named reconstruction cost.Adversarial loss is similary to original GAN.$$\operatorname { Loss } _ { a d v } \left( F , D _ { x } , Y \right) = \frac { 1 } { m } \sum _ { i = 1 } ^ { m } \left( 1 - D _ { x } \left( F \left( y _ { i } \right) \right) \right) ^ { 2 }$$$$\operatorname { Loss } _ { a d v } \left( G , D _ { y } , X \right) = \frac { 1 } { m } \sum _ { i = 1 } ^ { m } \left( 1 - D _ { y } \left( G \left( x _ { i } \right) \right) \right) ^ { 2 }$$However, the adversarial loss alone is not sufficient to produce good looking images, which can not enfore that the input and output are recognizably the same. The cycle consistency loss addresses this issue. It relies on the expectation that if you convert an image to the other domain and back again, and then you should get back something similar to what you put in. It enforces that $F ( G ( x ) ) \approx x$ and $G ( F ( y ) ) \approx y$.$$\operatorname { Loss } _ { c y c } ( G , F , X , Y ) = \frac { 1 } { m } \sum _ { i = 1 } ^ { m } \left[ F \left( G \left( x _ { i } \right) \right) - x _ { i } \right] + \left[ G \left( F \left( y _ { i } \right) \right) - y _ { i } \right]$$We can get the full objective function by putting these two together.$$\mathcal { L } \left( G , F , D _ { x} , D _ { y } \right) = \mathcal { L } _ { \text { GAN } } \left( G , D _ { y } , X , Y \right) + \mathcal { L } _ { \text { GAN } } \left( F , D _ { x } , Y , X \right) + \lambda \mathcal { L } _ { \text { cyc } } ( G , F )$$ Keypoints of StyleGANThe StyleGAN offeras an upgrade version of ProGAN’s image generator, with a focus on the generator. ProGAN generates high-quality images but, in most models, its ability to control specific features of the generated image is very limited. In other word, the features are entangled and therefore attempting to tweak the input, even a bit, usually affects multiple features at the same time. A good illustrations would be following pictures.Compared with first version (ProGAN), the new generator includes several additions to ProGAN’s generators. Mapping NetworkThe mapping network’s goal in to encode the input vector into an intermediate vector whose different elements control different visual features, which consists of 8 fully connected layers and its output $w$ is of the same size as the input. Style Modules (AdaIN)The AdaIn (Adaptive Instance Normalization) module transfers the encoded information $w$, created by the mapping network, into the generated image. Removing traditional inputSince the encoded information $w$ from mapping network was used into generator image, the traditional random input can be omitted and replaced by constant values.]]></content>
      <categories>
        <category>Deep learning</category>
      </categories>
      <tags>
        <tag>GANs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python for Beginners]]></title>
    <url>%2F2019%2F04%2F09%2Fpython-for-beginners%2F</url>
    <content type="text"><![CDATA[Basic Skills 列表项 modulepython 文件可以当做主文件进行运行或者当做函数的集合进行调用。如果是前者一般是需要包含”__name__ ==”__main__”。对于后者就是在其他的python文件中进行调用。12import my_module # python文件from my_module import my_object packagesfrom packageroot.packagefolder.mod import my_object Note: Ensure each directory within your package import contains a file __init__.py python-pathpython2 和python3 使用不同的解释器，导致在一些函数命名和计算上有一些差别，最好在文件的开头标明使用的解释器。 while forwhile : provide a condition and run the loop until the condition is not met. loop for a number of specific times; loop over items or characters of a string. examples:1234[Variable] AggregateFunction([Value] for [item] in [collection])x =[1, 2,3, 4, 5]y =[ 2*a for a in x if a%2 ==0]y &gt;&gt; [4, 8] 或者可以使用这样更加简洁的语句：12345678910 lambda arguments : expressio fun1 = lambda a,b,c : a+b+c print(fun1(5,6,2))``` 来个比较复杂的例子```python nums =[1,2,3,4,5] letters =['a', 'b', 'c','d','e'] nums_letters =[[n, l] for n in nums for l in letters ] nums_letters break continue passThe break, continue, and pass statements in Python will allow you to use for loops and while loops more effectively in your code.12345678910number = 0for number in range(10): number = number + 1 if number == 5: pass # pass here print('Number is ' + str(number))print('Out of loop') The pass statement occurring after the if conditional statement is telling the program to continue to run the loop and ignore the fact that the variable number evaluates as equivalent to 5 during one of its iterations. 可以用用作新的 if的测试。The pass statement can create minimal classes, or act as a placeholder when working on new code and thinking on an algorithmic level before hammering out details. yield return经常被用来作为生成器。 when you call a normal function with a return statement the function is terminated whenever it encounters a return statement. In a function with a yield statement the state of the function is ‘saved’ from the last call and can be picked up the next time you call a generator function. for examples123456789101112131415161718gen_exp =(x **2 for x in range(10) if x %2 ==0)for x in gen_exp: print(x)def my_gen(): for x in range(5): yield xgen1 =my_gen()next(gen1)def my_generator1(): yield 1 yield 2 yield 3 my_gen =my_generator1()next(my_gen) recursionA function calling itself is known as recursion. list, tuples, dictionary在python 中是使用频繁的data structure，这个是属于 collection 类别，里面放的是elementto add/update/ delete an item of a colletion ( list)123456my_list.append('C') #adds at the endmy_list[1] = 'D' #updatemy_list.pop(1) # removesmylist.pop() # 默认就是类似 栈的结构，就是pop 出来最后一个mylist.pop(0) # 当然也可以根据index 指定特定的 pop(delete) 的element or 12del mylist[1:2] # 通过指定 index range 然后进行delmylist.sort() # 支持 sorting 然后是从小到大, 这个sort是一种操作，inplace 的操作 tuplestuples store a sequence of objects, the object can be of any typle. Tuples are faster than lists. dictionary:It stores key/value pair objects.123456789101112131415161718192021222324252627 my_dict =dict() my_dict['key'] ='value' or my_dict =&#123;'key': 'value', ...&#125; for key in my_dict: # do something if 'some key' in my_dict: # do something``` ### Iterators###```python class yrange: def __init__(self, n): self.i =0 self.n =n # 这个表明是一个 iterator，make an object iterable def __iter__(self): return self # 这个next 函数就被当做是 class的属性，可以被外部调用的， def next(self): if self.i &lt; self.n: i =self.i self.i +=1 return i else: raise StopIteration() shallow vs deep copypython3 中：对于简单的数据类型，像int ，string，这种 copy() 和copy.deepcopy() 这两者都是相同的，copy 都是一种映射，都是相当于”值“ 上的引用；12345aa =2bb =aaprint(id(aa), id(bb)) # 相同bb =3print(id(aa), id(bb)) # 不同，因为把3 这个值重新复制给了变量bb 对于复杂的数据类型，使用deepcopy() 的时候，本来就是会重新拷贝一份到内存中。在python3 中copy() 和deepcopy() 这个是没有什么区别的。12345list1 =['a', 'b']list2 =list1 # 这个是引用，所以和list1 是相同的list3 =copy.copy(list1) # 这个id 和list1 不同list4 =copy.deepcopy(list1)# 这个id 和list1 不同 print(id(list1), id(list2), id(list3), id(list4)) object oriented design123456789101112131415161718192021222324252627282930313233343536373839404142 class ParentClass: def my_function(self): print 'I am here' class SubClass1(ParentClass): class SubClass2(ParentClass): ``` 对于多继承的支持 （接口）```python class A(B,C): #A implments B and C``` 如果想要call parent class function then you can dp:```python super(A, self).funcion_name()``` ### garbage collection###all the objects in python are stored in a heap space. Python has an in-built garbage collection mechanism.&gt; In computer science, a heap is a specialized tree-based data structure which is essentially an almost complete[1] tree that satisfies the heap property: if P is a parent node of C, then the key(the value) of P is either greater than or equal to (in a max heap) or less than or equal to (in a min heap) the key of C.### try...catch###```python # raise exceptions try: raise TyeError except: print('exception') # catching exceptions try: do_something() except: print('exception') # try/ catch /finally try: do_something() except TypeError: print('exception') finally: close_connections() Advanced FeaturesLet’s move on to advanced features. Lambda functionsA Lambda Function is a small, anonymous function — anonymous in the sense that it doesn’t actually have a name. A lambda function can take any number of arguments, but must always have only one expression: 1234x = lambda a, b : a * b print(x(5, 6)) # prints '30' x = lambda a : a*3 + 3 print(x(3)) # prints '12' MapsMap() is a built-in Python function used to apply a function to a sequence of elements like a list or dictionary. It’s a very clean and most importantly readable way to perform such an operation. 123456789 def square_it_func(a): return a * ax = map(square_it_func, [1, 4, 7])print(x) # prints '[1, 16, 49]'def multiplier_func(a, b): return a * bx = map(multiplier_func, [1, 4, 7], [2, 5, 8])print(x) # prints '[2, 20, 56]' FilteringThe Filter built-in function is quite similar to the Map function in that it applies a function to a sequence (list, tuple, dictionary). The key difference is that filter() will only return the elements which the applied function returned as True. 123456789101112 # Our numbersnumbers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]# Function that filters out all numbers which are odddef filter_odd_numbers(num): if num % 2 == 0: return True else: return Falsefiltered_numbers = filter(filter_odd_numbers, numbers)print(filtered_numbers)# filtered_numbers = [2, 4, 6, 8, 10, 12, 14] 123456from itertools import *def check_for_drop(x): print ('Checking: ', x) return (x &gt; 5)for i in dropwhile(check_for_drop, [2, 4, 6, 8, 10, 12]): print ('Result: ', i) Itertools1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162from itertools import *# zip 就是一块访问的那种形式，返回的是一个tuple 数据类型# zip ,joing two lists into a list of tuples# Easy joining of two lists into a list of tuplesfor i in zip([1, 2, 3], ['a', 'b', 'c']): print (i)# ('a', 1)# ('b', 2)# ('c', 3)# 就是一个count() 计数功能# The count() function returns an interator that # produces consecutive integers, forever. This # one is great for adding indices next to your list # elements for readability and convenience# in python3, no need to import izip, use zip directlyfor i in zip(count(1), ['Bob', 'Emily', 'Joe']): print (i)# (1, 'Bob')# (2, 'Emily')# (3, 'Joe') # check ， becomes false for the first time 这个条件很关键# The dropwhile() function returns an iterator that returns # all the elements of the input which come after a certain # condition becomes false for the first time. def check_for_drop(x): print 'Checking: ', x return (x &gt; 5)for i in dropwhile(check_for_drop, [2, 4, 6, 8, 10, 12]): print 'Result: ', i# 注意理解这个输出# Checking: 2# Result: 2# Result: 4# Result: 6# Result: 8# Result: 10# Result: 12# 我的理解这个 groupby 就和数据库中的groupby 是相同的效果# The groupby() function is great for retrieving bunches# of iterator elements which are the same or have similar # propertiesfrom itertools import groupbythings = [("animal", "bear"), ("animal", "duck"), ("plant", "cactus"), ("vehicle", "speed boat"), ("vehicle", "school bus")]for key, group in groupby(things, lambda x: x[0]): for thing in group: print ("A %s is a %s." % (thing[1], key))#A bear is a animal.#A duck is a animal.#A cactus is a plant.#A speed boat is a vehicle.#A school bus is a vehicle. GeneratorGenerator functions allow you to declare a function that behaves like an iterator, i.e. it can be used in a for loop. This greatly simplifies your code and is much more memory efficient than a simple for loop. 12345678910111213141516numbers = list()# range()for i in range(1000): numbers.append(i + 1)total = sum(numbers)# (2) Using a generatordef generate_numbers(n): num = 0 while num &lt; n: yield num # 这个yield 之后，函数并没有结束，不像 return 那种函数 num += 1total = sum(generate_numbers(1000))print(total)total = sum(range(1000 + 1))print(total) List Comprehension常见的几种形式：(An iterable is something you can loop over) list comprehensions vs loops: list comprehensions are more efficient both computationally and coding space Every list comprehension can be rewritten as a for loop, but not every for loop can be rewritten as a list comprehension. 从优化的角度 list comprehensions是优于 for loop 中的if else 操作的。因为前者是 predicatable pattern 是可以预测的。However, keep in mind that list comprehensions are faster because they are optimized for the Python interpreter to spot a predictable pattern during looping. a small code demo:在于使用功能 timeit libary 进行函数的计时比较。123456789101112 import timeitdef squares(size): result = [] for number in range(size): result.append(number*number) return resultdef squares_comprehension(size): return [number*number for number in range(size)]print(timeit.timeit("squares(50)", "from __main__ import squares", number = 1_000_000))print(timeit.timeit("squares_comprehension(50)", "from __main__ import squares_comprehension", number = 1_000_000)) more complex list comprehensions: 1234numbers = [1, 2, 3, 4, 5, 6, 18, 20] squares = [number for number in numbers if number % 2 == 0 if number % 3 == 0] print(squares)output: [6, 18] 在 output expression 中，也是可以使用 if else 进行进一步输出筛选。12345numbers = [1, 2, 3, 4, 5, 6, 18, 20] squares = ["small" if number &lt; 10 else "big" for number in numbers if number % 2 == 0 if number % 3 == 0] print(squares)ouput: ['small', 'big'] converting nested loops into list comprehension代码功能： 都是把二维的 matrix 转成了一个 list （flattened）123456matrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]flattened = []for row in matrix: for item in row: flattened.append(item) print(flattened) 注意这个顺序，先是row in matrix 然后是 item in row.123 matrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]flattened = [item for row in matrix for item in row] print(flattened) ouput matric from nested list comprehensions:12matrix = [[item for item in range(5)] for row in range(3)]print(matrix) 对于 dictionary 的支持： 主要是 dict1.items() 和 key, value 的使用123prices = &#123;"beer": 2, "fish": 5, "apple": 1&#125;float_prices = &#123;key:float(value) for key, value in prices.items()&#125;print(float_prices) 从代码的角度，可以看出，操作和最后的返回的形式是没有很大的关系，上面是 [], 这个是 {}, 分别对应的是 list 和 set 两种不同的格式。123numbers = [10, 10, 20, 30, 12, -20, 0, 1]unique_squares = &#123;number**2 for number in numbers&#125;print(unique_squares) Working with CSV, Json and XMLOver the years, the list of possible formats that you can store your data in has grown significantly. But, there are 3 that dominate in their everyday usage: CSV, JSON, and XML. In this article, I’m going to share with you the easiest ways to work with these 3 popular data formats in Python! 有两种方式去读写 csv file：一种是 pd.read_csv() ，一种是built-in 的library 中的库函数之前一直使用的pd.read_csv(), 现在才发现python 有built-in 的library。We can do both read and write of a CSV using the built-in Python csv library. Usually, we’ll read the data into a list of lists. python in-built function.123456789101112131415161718import csv filename = "my_data.csv"fields = [] rows = [] with open(filename, 'r') as csvfile: csvreader = csv.reader(csvfile) fields = csvreader.next() for row in csvreader: rows.append(row)for row in rows[:5]: print(row)# Writing to csv file with open(filename, 'w+') as csvfile: csvwriter = csv.writer(csvfile) csvwriter.writerow(fields) csvwriter.writerows(rows) 12345678910111213141516171819202122import pandas as pdfrom dicttoxml import dicttoxmlimport json# Building our dataframedata = &#123;'Name': ['Emily', 'Katie', 'John', 'Mike'], 'Goals': [12, 8, 16, 3], 'Assists': [18, 24, 9, 14], 'Shots': [112, 96, 101, 82] &#125;df = pd.DataFrame(data, columns=data.keys())# Converting the dataframe to a dictionary# Then save it to filedata_dict = df.to_dict(orient="records")with open('output.json', "w+") as f: json.dump(data_dict, f, indent=4)# Converting the dataframe to XML# Then save it to filexml_data = dicttoxml(data_dict).decode()with open("output.xml", "w+") as f: f.write(xml_data) 12345678910111213141516171819import jsonimport pandas as pd# Read the data from file# We now have a Python dictionarywith open('data.json') as f: data_listofdict = json.load(f) # We can do the same thing with pandasdata_df = pd.read_json('data.json', orient='records')# We can write a dictionary to JSON like so# Use 'indent' and 'sort_keys' to make the JSON# file look nicewith open('new_data.json', 'w+') as json_file: json.dump(data_listofdict, json_file, indent=4, sort_keys=True)# And again the same thing with pandasexport = data_df.to_json('new_data.json', orient='records') 参考资料：https://towardsdatascience.com/the-easy-way-to-work-with-csv-json-and-xml-in-python-5056f9325ca9]]></content>
      <categories>
        <category>Python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[The Evaluation of Sentence Similarity]]></title>
    <url>%2F2019%2F04%2F06%2FThe-evaluation-of-sentence-similarity%2F</url>
    <content type="text"><![CDATA[I am trying to write my first english blog based on two reasons: First, the data set used in this blog is english; Second, I’d like to expand my reach and attract more audiences, although I should admit that nobody cares. DataInitially I want to use chinese corpus, but I cannot find a proper one. The data should sound like this one: word1 word2 similarity score阿拉伯人 阿拉伯 7.2畜产 农业 5.6垂涎 崇敬 3.4次序 秩序 4.7定心丸 药品 4.3房租 价格 5.2翡翠 宝石 6.7高科技 技术 7.5购入 购买 8.5观音 菩萨 8.2归并 合并 7.7 not like this: 为何我无法申请开通花呗信用卡收款 支付宝开通信用卡花呗收款不符合条件怎么回事 1花呗分期付款会影响使用吗 花呗分期有什么影响吗 0为什么我花呗没有临时额度 花呗没有临时额度怎么可以负 0能不能开花呗老兄 花呗逾期了还能开通 0我的怎么开通花呗收钱 这个花呗是个什么啥？我没开通 我怎么有账单 0蚂蚁借呗可以停掉么 蚂蚁借呗为什么给我关掉了 0我想把花呗功能关了 我去饭店吃饭，能用花呗支付吗 0为什么我借呗开通了又关闭了 为什么借呗存在风险 0支付宝被冻了花呗要怎么还 支付功能冻结了，花呗还不了怎么办 1 If you can find the dataset where ‘similarity score’ is double, please donot hesitate to email me. So, the choice has to be enlgish corpus. The dataset used in this experiment are STSbenchmark and SICK data. The SICK data contains 10,000 sentence paris labeled with semantic relatedness and entailment relation. Similarity MethodsBaselineAs the baseline, we just take the embedding of the words in sentence, and compute the average, weighted by frequency of each word. def run_avg_benchmark(sentences1, sentences2, model=None, use_stoplist=False, doc_freqs=None): if doc_freqs is not None: N = doc_freqs[&quot;NUM_DOCS&quot;] sims = [] for (sent1, sent2) in zip(sentences1, sentences2): tokens1 = sent1.tokens_without_stop if use_stoplist else sent1.tokens tokens2 = sent2.tokens_without_stop if use_stoplist else sent2.tokens tokens1 = [token for token in tokens1 if token in model] tokens2 = [token for token in tokens2 if token in model] if len(tokens1) == 0 or len(tokens2) == 0: sims.append(0) continue tokfreqs1 = Counter(tokens1) tokfreqs2 = Counter(tokens2) weights1 = [tokfreqs1[token] * math.log(N / (doc_freqs.get(token, 0) + 1)) for token in tokfreqs1] if doc_freqs else None weights2 = [tokfreqs2[token] * math.log(N / (doc_freqs.get(token, 0) + 1)) for token in tokfreqs2] if doc_freqs else None embedding1 = np.average([model[token] for token in tokfreqs1], axis=0, weights=weights1).reshape(1, -1) embedding2 = np.average([model[token] for token in tokfreqs2], axis=0, weights=weights2).reshape(1, -1) sim = cosine_similarity(embedding1, embedding2)[0][0] sims.append(sim) return sims Smooth Inverse FrequencyThe baseline, like we did before, is very simple and crude of computing sentence embedding. Word frequency cannot reliably reflect its importance to sentence, semantically speaking. Smooth Inverse Frequency (SIF) tries to solve this problem. SIF is very similar to the weighted average we used before, with the difference that it’s weighted by this formular.$$\operatorname { SIF } ( w ) = \frac { a } { ( a + p ( w ) )}$$where $a$ is a hyper-parameter (set to 0.001 by default) and $ p(w)$ is the estimated word frequency in the corpus. Next, we need to perform common component removal: subtract from the sentence embeddingh obtained above the first principal component of the matrix. This corrects for the influence of high-frequency words that have syntactic or dicourse function, such as ‘but’, ‘and’, etc. You can find more information from this paper. def remove_first_principal_component(X): svd = TruncatedSVD(n_components=1, n_iter=7, random_state=0) svd.fit(X) pc = svd.components_ XX = X - X.dot(pc.transpose()) * pc return XX def run_sif_benchmark(sentences1, sentences2, model, freqs={}, use_stoplist=False, a=0.001): total_freq = sum(freqs.values()) embeddings = [] # SIF requires us to first collect all sentence embeddings and then perform # common component analysis. for (sent1, sent2) in zip(sentences1, sentences2): tokens1 = sent1.tokens_without_stop if use_stoplist else sent1.tokens tokens2 = sent2.tokens_without_stop if use_stoplist else sent2.tokens tokens1 = [token for token in tokens1 if token in model] tokens2 = [token for token in tokens2 if token in model] weights1 = [a / (a + freqs.get(token, 0) / total_freq) for token in tokens1] weights2 = [a / (a + freqs.get(token, 0) / total_freq) for token in tokens2] embedding1 = np.average([model[token] for token in tokens1], axis=0, weights=weights1) embedding2 = np.average([model[token] for token in tokens2], axis=0, weights=weights2) embeddings.append(embedding1) embeddings.append(embedding2) embeddings = remove_first_principal_component(np.array(embeddings)) sims = [cosine_similarity(embeddings[idx * 2].reshape(1, -1), embeddings[idx * 2 + 1].reshape(1, -1))[0][0] for idx in range(int(len(embeddings) / 2))] return sims Google Sentence EncoderInferSent is a pre-trained encoder that produces sentence embedding, which opensourced by Facebook. The Google Sentence Encoder is Google’s answer to Facebook’s InferSent. In contrast to InferSent, the Google Sentence Encoder was trained on a combination of unsupervised data and supervised data (SNLI corpus), which tends to give better results. The codes can be used in Google Jupyter Notebook import tensorflow_hub as hub tf.logging.set_verbosity(tf.logging.ERROR) embed = hub.Module(&quot;https://tfhub.dev/google/universal-sentence-encoder/1&quot;) def run_gse_benchmark(sentences1, sentences2): sts_input1 = tf.placeholder(tf.string, shape=(None)) sts_input2 = tf.placeholder(tf.string, shape=(None)) sts_encode1 = tf.nn.l2_normalize(embed(sts_input1)) sts_encode2 = tf.nn.l2_normalize(embed(sts_input2)) sim_scores = tf.reduce_sum(tf.multiply(sts_encode1, sts_encode2), axis=1) with tf.Session() as session: session.run(tf.global_variables_initializer()) session.run(tf.tables_initializer()) [gse_sims] = session.run( [sim_scores], feed_dict={ sts_input1: [sent1.raw for sent1 in sentences1], sts_input2: [sent2.raw for sent2 in sentences2] }) return gse_sims Experimentsdef run_experiment(df, benchmarks): sentences1 = [Sentence(s) for s in df[&apos;sent_1&apos;]] sentences2 = [Sentence(s) for s in df[&apos;sent_2&apos;]] pearson_cors, spearman_cors = [], [] for label, method in benchmarks: sims = method(sentences1, sentences2) pearson_correlation = scipy.stats.pearsonr(sims, df[&apos;sim&apos;])[0] print(label, pearson_correlation) pearson_cors.append(pearson_correlation) spearman_correlation = scipy.stats.spearmanr(sims, df[&apos;sim&apos;])[0] spearman_cors.append(spearman_correlation) return pearson_cors, spearman_cors Helper function: import functools as ft benchmarks = [ (&quot;AVG-GLOVE&quot;, ft.partial(run_avg_benchmark, model=glove, use_stoplist=False)), (&quot;AVG-GLOVE-STOP&quot;, ft.partial(run_avg_benchmark, model=glove, use_stoplist=True)), (&quot;AVG-GLOVE-TFIDF&quot;, ft.partial(run_avg_benchmark, model=glove, use_stoplist=False, doc_freqs=doc_frequencies)), (&quot;AVG-GLOVE-TFIDF-STOP&quot;, ft.partial(run_avg_benchmark, model=glove, use_stoplist=True, doc_freqs=doc_frequencies)), (&quot;SIF-W2V&quot;, ft.partial(run_sif_benchmark, freqs=frequencies, model=word2vec, use_stoplist=False)), (&quot;SIF-GLOVE&quot;, ft.partial(run_sif_benchmark, freqs=frequencies, model=glove, use_stoplist=False)), ] Resultsimport matplotlib.pyplot as plt plt.rcParams[&apos;figure.figsize&apos;] = (20,13) spearman[[&apos;AVG-GLOVE&apos;, &apos;AVG-GLOVE-STOP&apos;,&apos;AVG-GLOVE-TFIDF&apos;, &apos;AVG-GLOVE-TFIDF-STOP&apos;,&apos;GSE&apos;]].plot(kind=&quot;bar&quot;).legend(loc=&quot;lower left&quot;) Take Off Smooth Inverse Frequency methods are better than baseline, no matter with word2vec or Glove embeddings. Google Sentence Encoder has the similar performance as Smooth Inverse Frequency. Using tf-idf weights does not help and using a stoplist looks like a reasonable choice. Pearson CorrelationSpearman Correlation Full codes can be found in here.]]></content>
      <categories>
        <category>NLP</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[深度网络中的碎碎念]]></title>
    <url>%2F2019%2F03%2F26%2F%E6%B7%B1%E5%BA%A6%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E7%A2%8E%E7%A2%8E%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[主要介绍了常见的网络中权重的初始化，激活函数和优化器。 Weights Initializationweights 的初始化在网络的训练起到重要的作用，初始化的好坏能够直接影响到网络是否可以正常收敛。这里的初始化都是指的是weights初始化。bias 这个变量就是在企图去描述真实的分布，通过引入随机性来表示这个是具有 推广性的。 Here’s another trick — before squishing our scalar value (called an activation) into the sigmoid function, we can add a little number called the bias, b. There will be two biases, one for each user and one for each movie. Random Initialization最基础的即使 bias 使用 zero initialization ，然后 weights 使用 random initialzation。这种方法的缺陷在于梯度消失。就是你的weights 如果很大或者很小的时候，再加上如果使用了sigmoid 那么很容易出现上述的现象。 a) If weights are initialized with very high values the term np.dot(W,X)+bbecomes significantly higher and if an activation function like sigmoid() is applied, the function maps its value near to 1 where slope of gradient changes slowly and learning takes a lot of time.b) If weights are initialized with low values it gets mapped to 0, where the case is same as above. He Initialization$$W^{[l]} =np.random.randn(size_l, size_l -1)* np.sqrt(2/size_l -1) $$这个是使用 relu 或者说 leaky relu 配合使用的。 所以上述的初始化从数学的角度去理解： 从(0,1) 标准正太分布 转换成了 (0, np.sqrt(2/ size_l -1)) 这样的分布，就是你网络结构是越宽，那么这个方差就是越小的，最后的结果是越集中的，就越集中的 均值 u 左右。从图像的角度看，方差越大，图像越矮胖；方差越小，图像越瘦高。 Xavier initialization这个是使用tanh() 作为 activation function的。$$W^{[l]} =np.random.randn(size_l, size_l -1)* np.sqrt( 1/size_l -1) $$总的思想原则：They set the weights neither too much bigger that 1, nor too much less than 1.就是本来就是在 (0,1) 标准正太分布出来，然后进行了进一步的约束条件。 Activation function总的说可以分为线性和非线性的激活函数， activation function 的作用 就是对于网络的输出 说yes or no. It maps the resulting values in between 0 to 1 or -1 to 1 etc. Sigmoid function (Logistic Activation)the only reason why we use sigmoid is because it exists between 0 to 1. 这个非常有利于 predict probability. 因为自然映射到 0 是不存在 然后1 是存在。而当多分类的时候，使用softmax。 Tanh function The advantage is that the negative inputs will be mapped strongly negative and the zero inputs will be mapped near zero in the tanh graph. Relu (Rectified Linear Unit) Activation本质上是分段函数。 range: [0, infinity]. The function and its derivative both are monotonic. Leaky Relu每当一个伟大的东西产出，总会伴随着比较明显的错误，然后紧跟着就是有一个 rectified(improved) 这种版本。这个相比之前就是修正了 当输入是负数的时候，怎么办的问题。 Softmax这个需要从数学的角度去理解，数学公式就是下面。有一个真实的例子： 如果一张”4” 的图片，输出一个网络，最后是softmax 激活函数，那么最后得到的 “4” 的概率是最大的，但是也是有其他的可能性存在的，这个就是softmax 的效果。 最主要的功能是 convert number into probabilities. 这种效果，不像sigmoid 那样有很明确的数学公式。 $$\sigma ( z ) _ { j } = \frac { e ^ { z _ { j } } } { \sum _ { k = 1 } ^ { K } e ^ { z _ { k } } }$$ Softmax function, a wonderfulactivation function that turns numbers aka logits into probabilities that sum to one.Remember the takeaway is: the essential goal of softmax is to turn numbers into probabilities. OptimizerGradient Descent最基本的应该是 gradient descent，这个算是开山鼻祖。给出了每次梯度变化的方向 Momentum个人更喜欢将其翻译成动量，这个gradient 考虑之前的gradient 的方向和大小，有点像动量的的含义。不仅考虑了current step, 而且accumulates gradient of the past steps. A very popular technique that is used along with SGD is called Momentum. Instead of using only the gradient of the current step to guide the search, momentum also accumulates the gradient of the past steps to determine the direction to go. 好处在于: most recent is weighted than the less recent onesthe weightage of the most recent previous gradients is more than the less recent ones.for example: RMSpropRMSprop, or Root Mean Square Propogation has an interesting history. RMSProp also tries to dampen the oscillations, but in a different way than momentum. Adam这种套路总是一样的，总是有一个中和者，关键是如何整合到一起。 Adam or Adaptive Moment Optimization algorithms combines the heuristics of both Momentum and RMSProp. The hyperparameter beta1 is generally kept around 0.9 while beta_2 is kept at 0.99. Epsilon is chosen to be 1e-10 generally. 对于公式的解释，Eq 1 and Eq 2是come from RMSprop, Eq 3 comes from Adam。最后一步是 update 操作，条理清楚，没有问题。(公式中的v 表示导数，g 表示导数e 不太清楚)Here, we compute the exponential average of the gradient as well as the squares of the gradient for each parameters (Eq 1, and Eq 2). To decide our learning step, we multiply our learning rate by average of the gradient (as was the case with momentum) and divide it by the root mean square of the exponential average of square of gradients (as was the case with momentum) in equation 3. Then, we add the update.)]]></content>
      <categories>
        <category>Deep learning</category>
      </categories>
      <tags>
        <tag>Loss Function</tag>
        <tag>Activation Function</tag>
        <tag>Optimizer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[siamese network]]></title>
    <url>%2F2019%2F03%2F26%2Fsiamese-network%2F</url>
    <content type="text"><![CDATA[主要是介绍自己论文中的网络结构：siamese network。 但凡优化，无非两条路：在好的基础上更快，在快的基础上效果更好。 Siamese network训练速度快，所以只是需要其在训练效果上更好。 先来分析一下为什么训练速度快。那么不得不分析该网络结构。整个网络的输入是 (img1, img2, y) 这样的三元组，img 表示图片，y表示label。图片可以是同一类别的图片，也可以是不同类别的，y表示两张图片之间的相似程度，y的取值 (0,1)，0表示相似（同一类别），1 表示不相似（不同类别）。并且y 是double 类型，属于0-1 区间任意的数字。网路结构最后的输出是 0-1 区间的任意数字。通常是以0.5 作为分界线，如果小于0.5 那么认为两种图片是属于同一类别，或者说更相似；反之也成立。重要的一点是中间的weights 实现了权值共享，这样可以加快网络的训练速度。 loss function这个是属于经典的 contrastive loss function。当y 接近于0的时候，右半部分消失，这个是表示两张图片很是相似，然后就不断使得 欧氏距离减少；当y 接近于1的时候，左半部分消失，这个时候两张图片很不相似，然后右边就是 hinge loss。参数m 作为一种margin 是是可以调节，我的实验中 m 取1.总的思想：就是使得相近的图像距离相近，不想近的图像距离变远。 $L ( W , ( Y , X _ { 1 } , X _ { 2 } ) ) = ( 1 - Y ) \frac { 1 } { 2 } ( D _ { W } ) ^ { 2 } + ( Y ) \frac { 1 } { 2 } { \max ( 0 , m - D _ { W } ) } ^ { 2 }$ Spectral Normalization图像输入到网络之前使用正则化，然后输入到激活函数之前也是使用正则化，所以这种效果也是扩展到 weights，直接对 weights 进行正则化使其符合 Lipschitz 约束，避免使用大的gradients。在GAN 网络中的 discriminator 或者 generator 都发现了其可以稳定训练的过程。在实验中，我们扩大了了这种使用范围，把其应用到所有的网络的layer上。 self-attention mechanismAttention 机制自从 “Attention Is All You Need” 开始火爆，并且实验的效果也是很好的，然后在图像领域也开始尝试使用 attention 机制来解决长依赖的问题。应用到图像领域主要是 explore spatial locality information, 说白了就是细节的信息。 If we look at the DCGAN model, we see that regular GANs are heavily based on convolution operations, which use a local receptive field (convolutional kernel) to learn representations. Simple features like edges and corners are learned in the first few layers. Also, ConvNets are able to use these simple representations to learn more complex ones. However, long-range dependency might be hard to learn. Long-range dependency (long-term dependency) is from RNN, which we can say anything larger than trigram as a long term dependency. Thus, most of the image content does not exhibit elaborated shape such as sky or the ocean looks fine. The task of creating geometrically complex forms, such as four-legged animals, is far more challenging. This is where attention comes into play. 而 self-attention 中QKV 三个部分是相同的，对于这种处理方法和Res_block 还是有点相似的。 结果训练数据集使用是 Cifar-10，记录了训练过程中 acc 和loss 的变化情况。除了训练的效果比较好外，训练速度也是非常快的，可以清楚的看到model acc 在接近25 epoches的时候就开始收敛。]]></content>
      <categories>
        <category>Deep learning</category>
      </categories>
      <tags>
        <tag>siamese network</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[fastText & faiss]]></title>
    <url>%2F2019%2F03%2F25%2FfastText-faiss%2F</url>
    <content type="text"><![CDATA[fastTextfastText结合了自然语言处理和机器学习中最成功的理念。这些包括了使用词袋以及n-gram袋表征语句，还有使用子字(subword)信息，并通过隐藏表征在类别间共享信息。另外采用了一个softmax层级(利用了类别不均衡分布的优势)来加速运算过程。 fastText 主要是用来解决 word representations和 sentence classification. 有趣的是前者是无监督的学习方式，后者是有监督的学习方式。分别主要来自 ”Enriching Word Vectors with Subword Information“ 和 “Bag of Tricks for Efficient Text Classification” 两篇论文。并且使用的是 shallow neural network 而不是深度网络。 FastText is a library created by the Facebook Research Team for efficient learning of word representations and sentence classification. Take off:fastText 方法包含三部分：模型架构、层次 Softmax 和 N-gram 特征。fasttext 有两个用处： text classification 和 word embedding 。使用场景：大型数据，高效计算 下面进行细说： 模型架构这个是总的框架图。 抱歉哈 这个引用找不见了，如果有侵权，please email me..分为两个部分介绍这个网络结构：从input -&gt; hidden:输入层输入的是一个已经分词后短文本。短文本中每个词的词向量是由该短文本的one-hot矩阵乘以一个初始化的矩阵w得到的。（原理图：下图是fasttext 运行的时候，这个分词是再处理成单词和n-gram 组成的特征，这个是不需要我们进行显性的操作的）从 hidden -&gt; output：插播一句，我们经常使用的预训练模型中的weights 是从input-&gt; hidden。 Hierarchical Softmax从名字上就知道这个是基于softmax的改进版本，主要是运算上的改进。具体说来：利用了类别（class）不均衡这个事实（一些类别出现次数比其他的更多），通过使用 Huffman 算法建立用于表征类别的树形结构。fastText 模型使用了层次 Softmax 技巧。层次 Softmax 技巧建立在Huffman的基础上，对标签进行编码，能够极大地缩小模型预测目标的数量。 这个是softmax 的原始的计算公式：采用二叉树的结构之后，时间上优化不少。$O ( N) \rightarrow O \left( \log _ { 2 } N \right)$。见下图。 抱歉哈 这个引用找不见了，如果有侵权，please email me..和之前的神经网络模型相比，这里的huffmax树的所有内部节点就类似之前神经网络隐藏层的神经元。其中，根节点的词向量对应我们的投影后的词向量，而所有的叶子节点就类似于之前神经网络softmax输出层的神经元。叶子节点的个数就是词汇表的大小. 和之前的相比，从隐藏层到输出层的softmax映射不是一下就完成的，而是沿着 huffman树一步步完成的，因此这种 softmax取名为”Hierarchical softmax”. N-gram 特征N-gram是基于这样的思想：某个词的出现依赖于其他若干个词；我们获得的信息越多，预测越准确。我想说，我们每个人的大脑中都有一个N-gram模型，而且是在不断完善和训练的。我们的见识与经历，都在丰富着我们的阅历，增强着我们的联想能力。 N-gram模型是一种语言模型（Language Model，LM），语言模型是一个基于概率的判别模型，它的输入是一句话（单词的顺序序列），输出是这句话的概率，即这些单词的联合概率（joint probability）。 这样的作用，使用N-gram来给文本添加额外的特征获得关于局部词顺序的部分信息。举个栗子：对于句子：“我 喜欢 喝 咖啡”, 如果不考虑顺序，那么就是每个词，“我”，“喜欢”，“喝”，“咖啡”这五个单词的word embedding求平均。如果考虑2-gram, 那么除了以上五个词，还有“我喜欢”，“喜欢喝”，“喝咖啡”等词。“我喜欢”，“喜欢喝”，“喝咖啡”这三个词就作为这个句子的文本特征。我们经常见到的场景：输入法的预选词汇。就是可以通过这种方式实现的。 当然使用了更多的特征意味着造成了效率下降，于是该作者提出了两种解决方法：过滤掉低词频；使用词粒度代替字粒度。比如说海慧寺使用上面那个句子”我喜欢喝咖啡“，如果使用子粒度的2-gram，那么产生的特征是“我喜”，“喜欢”，“欢喝”，“喝咖”，“咖啡”。如果使用词粒度为2-gram，那么产生的特征是“我喜欢”，“喜欢喝”，“喝咖啡”。 补充一句，subwords就是一个词的character-level的n-gram。比如单词”hello”，长度至少为3的char-level的ngram有”hel”,”ell”,”llo”,”hell”,”ello”以及本身”hello”。 Negative Sampling这个技术不是 fastText 首创的，但是本着总结知识点的，也就放在这里了。CBOW / Skip-gram模型 （这个论文中）提出了两种方法，一种是Hierarchical Softmax，另一种是Negative Sampling。论文中提出的两种方法都是用来提高计算效率的，下面说一下负采样。在训练神经网络时，每当接受一个训练样本，然后调整所有神经单元权重参数，来使神经网络预测更加准确。换句话说，每个训练样本都将会调整所有神经网络中的参数。而 Negative Sampling 每次让一个训练样本仅仅更新一小部分的权重参数，从而降低梯度下降过程中的计算量。如果 vocabulary 大小为1万时， 当输入样本 ( “fox”, “quick”) 到神经网络时， “ fox” 经过 one-hot 编码，在输出层我们期望对应 “quick” 单词的那个神经元结点输出 1，其余 9999 个都应该输出0。在这里，这9999个我们期望输出为0的神经元结点所对应的单词我们称为 negative word，随机选择一小部分的 negative words，比如选 10个 negative words 来更新对应的权重参数。 使用第一个应用场景：词向量。fastText作为训练词向量认为可以有两种模式，一种是根据周围词语预测中心词汇的CBOW （continuous bag-of-words）模型，另一种是根据中心词汇预测上下文的 skip-gram 模型。 ./fasttext – It is used to invoke the FastText library. skipgram/cbow – It is where you specify whether skipgram or cbow is to be used to create the word representations. -input – This is the name of the parameter which specifies the following word to be used as the name of the file used for training. This argument should be used as is. data.txt – a sample text file over which we wish to train the skipgram or cbow model. Change this name to the name of the text file you have. -output – This is the name of the parameter which specifies the following word to be used as the name of the model being created. This argument is to be used as is. model – This is the name of the model created.Running the above command will create two files named model.bin and model.vec. model.bin contains the model parameters, dictionary and the hyperparameters and can be used to compute word vectors. model.vec is a text file that contains the word vectors for one word per line. 最后生成有两个文件，一个 xxx.bin 文件，一个是 xxx.vec 文件，前者是预训练模型，后者是词向量。这两个可能是最重要的格式了。 The most important parameters of the model are its dimension and the range of size for the subwords. 常见的代码格式： ./fasttext skipgram -input data/fil9 -output result/fil9 -minn 2 -maxn 5 -dim 300 跑偏一下说一下shell的小技巧。使用echo 或者 &lt; 这样进行单个词或者多个词的词向量的查询。 ./fasttext print-word-vectors model.bin &lt; queries.txtecho “word” | ./fasttext print-word-vectors model.bin Finding simialr words: ./fasttext nn model.bin 第二个应用场景：文本分类。 Sentiment analysis and email classification are classic examples of text classification 在训练数据集中label 默认是使用 “__label__” 进行表示的，当然也是可以进行自定义的。 ./fasttext supervised -input train.ft.txt -output model_kaggle -label __label__ -lr 0.5 就是进行predict的时候，有时候并不是很能想起来只是predict top 3 这样的东西。 # Predicting on the test dataset ./fasttext predict model_kaggle.bin test.ft.txt # Predicting the top 3 labels ./fasttext predict model_kaggle.bin test.ft.txt 3 faiss用途：相似度检测和稠密向量的聚类。 Faiss is a library for efficient similarity search and clustering of dense vectors. 之前的实习经历主要是用faiss 处理文本的representation，但是这个是有偏差的，凡是能够打成词向量，都是可以使用faiss 进行计算的，当然这词向量需要满足：相近内容在相近的空间。 Once the vectors are extracted by learning machinery (from images, videos, text documents, and elsewhere), they’re ready to feed into the similarity search library. faiss的实现过程首先使用 index对于向量进行预处理，然后选择不同的模式。 牺牲了一些精确性来使得运行速度更快。 Similarity search can be made orders of magnitude faster if we’re willing to trade some accuracy; that is, deviate a bit from the reference result. For example, it may not matter much if the first and second results of an image similarity search are swapped, since they’re probably both correct results for a given query. Accelerating the search involves some pre-processing of the data set, an operation that we call indexing. 向量的比较有两种metric：一种是L2 一种是基于consine (点乘)进行检索。前者是求解最小的值，后者是通过inner——product 求解maximum. 并且是支持GPU的，在原来CPU上建立的index，然后很好的迁移到 GPU上。 在涉及index使用考虑速度，acc和内存大小三个不同的维度。然后不同的index 是有不同的侧重的。]]></content>
      <categories>
        <category>NLP</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[NLP中的碎碎念]]></title>
    <url>%2F2019%2F03%2F25%2FNLP%E4%B8%AD%E7%9A%84%E7%A2%8E%E7%A2%8E%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[主要介绍关键词提取并整理一下NLP相关的基础知识点。 关键词提取TF-IDF这个是可以参看之前自己写的一个博客 卡方分布卡方检验是以χ2分布为基础的一种常用假设检验方法，它的无效假设H0是：观察频数与期望频数没有差别。该检验的基本思想是：首先假设H0成立，基于此前提计算出χ2值，它表示观察值与理论值之间的偏离程度。而实际应用到特征选择中的时候，我们不需要知道自由度，不要知道卡方分布，我们只需要根据算出来的χ2 进行排序就好了，越大我们就越喜欢！挑选最大的一堆，于是就完成了利用卡方检验来进行特征提取。卡方分布的缺点：没有考虑词频 Textrank有一个与之很像的概念 pageRanking，最开始是用来计算网页的重要性。Textrank 主要用来提取文章的关键词，然后比较适合长文本。 CBOW和skip-gram举一个简单的小例子说明 CBOW和skip-gram的区别：skip-gram 是根据中心词汇然后预测上下文词汇，这个不是一下子输入上下文词汇的，而是一个过程，中心词汇和上下文词汇1 ，中心词汇和上下文词汇2，这样的进行输入。 cbow 和其的区别，在于简单相加了上下文词汇作为一个整体，然后和中心词汇进行输入，所以最后是这样的结果。 使用skip gram训练的时间更长，但是对于出现频率不高的词汇，效果比较好。但CBOW的训练速度是相对来说比较快一些。]]></content>
      <categories>
        <category>NLP</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Python中的多线程和多进程]]></title>
    <url>%2F2019%2F03%2F25%2FPython%E4%B8%AD%E7%9A%84%E5%A4%9A%E7%BA%BF%E7%A8%8B%E5%92%8C%E5%A4%9A%E8%BF%9B%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[多线程和多进程问题是可以对应到 并发 （cncurrency）和并行(parallelism)上的。 并发，就是一个单核cpu同时开始了多个任务，但是这个任务并不是同时独立进行的，而是通过cpu的不断切换，保存现场，然后重启这样的快速的切换，给用户的感觉是并发，但是实际上是cpu的计算能力受到了限制，用户体验比较好一些。如果在多核cpu （比如我的mac 是一cpu 6核）这样的话完全是可以达到并行的，这个是真正的独立操作(parallelism)，对应的是多进程的。对应python 中的实现多线程是使用threading，处理的是io 响应；多进程是Concurrency，使用multiprocessing包，处理的是多核cpu的操作。 Take off: 如果处理io 响应，那么使用多线程；如果是计算，那么使用多进程。 So, before we go deeper into the multiprocessing module, it’s worthwhile ensuring you know the advantages of using multiprocessing over multithreading. The general rule of thumb is that, if you are trying to improve the performance of CPU-bound tasks, multiprocessing is what you want to use. However, if your particular task is Input/Output bound, then you’ll generally want to use multithreading to improve the performance of your applications. 下面是多线程的demo。 import multiprocessing as mp def my_func(x): print(mp.current_process()) return x ** x def main(): pool = mp.Pool(mp.cpu_count()) # 这个还是很好的 pool 这个的个数和你的cpu count 是保持一致的 result = pool.map(my_func, [4, 2, 3, 5, 3, 2, 1, 2]) result_set_2 = pool.map(my_func, [4, 6, 5, 4, 6, 3, 23, 4, 6]) print(result) print(result_set_2) if __name__ == &quot;__main__&quot;: main() 这个是多进程的demo。 import threading class Worker(threading.Thread): # Our workers constructor, note the super() method which is vital if we want this # to function properly def __init__(self): super(Worker, self).__init__() def run(self): for i in range(10): print(i) def main(): thread1 = Worker() thread1.start() thread2 = Worker() thread2.start() thread3 = Worker() thread3.start() thread4 = Worker() thread4.start() if __name__ == &quot;__main__&quot;: main()]]></content>
      <categories>
        <category>Python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Data Pre-processing 学习笔记]]></title>
    <url>%2F2019%2F03%2F25%2FData-Pre-processing%2F</url>
    <content type="text"><![CDATA[Data Cleaning:这个步骤主要处理 missing values 和 noisy data (outlier).对于missing values 通常有一下处理手段： ignore the tuple; fill in the missing value manually use a global constant to fill in the missing value use the attribute mean to fill in the missing value (均值) use the most probable value to fill in the missing value (mode 众数) 有时候就是根据某几个特征然后弄一个简单的回归模型，根据模型进行predict关于这几种方法如何去选择，我如果说 “it depends”，那么其他人不认为这是一个具有说服力的答案，他们更像知道 it depends what, and when and why to use specific method? 我认为应该是根据缺省值程度和重要性进行经验性的选择，这也去就是 empirical study吧。 接着是 noisy data (outlier)，我的观点是首先得认识到这个是错误的数据，不是真实的数据来源，可能是来自人为的笔误 或者仪器记录的问题，这个是需要修改的。可以使用聚类 (clustering) 进行noisy data 的检测，找到之后这个就类似 missing value了，可以采取以上的手段进行操作，应该注意到的这个 noisy data 所占比例不会很高，否则就成了主要的数据分布了。 Data Integration:处理数据库数据，经常是需要处理子表信息的，那么必然存在着主表，而子表系信息往往是主表信息的某一方面的细化。所以有必要将两者连接起来。 Data Transformation:In data transformation, the data are transformed or consolidated into forms appropriate for mining.这里想要澄清的是很多相同的内容都可以用不同的方式表达，并且可以放在数据处理的不同阶段，并且这种工作不是一次性完成的，而是迭代的 until you run out your patience and time.首先我接触的最常见的就是 discrete variables -&gt; continuous variables. 当然对于 discrete variables，基于树结构的机器学习模型是可以处理的，这里想说的是有这种方式。这种 transformation 常见的处理方式: one-hot 或者 label encoding. 如果按照 data transformation的预设，那么 normalization 就也属于该模块的内容。 不论是在 machine learning 还是在 图像处理的时候，对于原始的数据经常采取 normalization. 一方面这个是可以预防梯度消失 或者 gradient exploding, 如果你采用了 SIgmoid的激活函数的话。另一方面我认为更加重要的原因是将 不同的数据放在了同一个尺度下，如果你采取了 normalization之后。实现这种normalization 经常采用的是以下三种方式：min-max normalization: $x ^ { \prime } = \frac { x - \min ( x ) } { \max ( x ) - \min ( x ) }$mean normalization:$x ^ { \prime } = \frac { x - \text { average } ( x ) } { \max ( x ) - \min ( x ) }$standardization: $x ^ { \prime } = \frac { x - \overline { x } } { \sigma }$ Data Reduction:一般来说很少提及到到 data reducation的必要性，如果非要给出原因，那么可以从时间和空间的角度进行考虑。更加需要关注的是如何做的问题。 我的理解reducation 可以从两个维度进行考虑，假设一个 matrics A 是 m*n，这个是一个二维的矩阵，那么可以从 行列两方面入手。映射到机器学习中一般这样描述 从dimension 和 data两个角度去描述，分别称之为 dimension reduction 和 data compression. 前者指的是特征的选取，后者是数据size的减少。dimension reduction: where irrelevant, weakly relevant, or redundant attributes or dimensions may be detected and removed.data compression: PCA 线性降维 to reduce the data set size. 这个是针对某一个特征展开的。 机器学习中的特征工程是有一定技巧可言，其中我觉得最为有趣的是: generation or you can call it abstraction. 对于特征的泛的提取才是对于问题本身或者特征的理解，这不仅需要积累，更需要对于该问题领域的专业知识， that’s all.举个栗子，在 “Home Credit Default Risk” (kaggle 竞赛)中，原始的训练数据有信贷金额和客户的年收入，这个时候 “credit_income_percent” 就是类似这种性质的提取特征。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Dimension Reduction]]></title>
    <url>%2F2019%2F03%2F25%2FDimension-Reduction%2F</url>
    <content type="text"><![CDATA[对于dimension reduction最近有了新的理解:广义上将降维就是使用更少的数据 (bits) 却保存了尽可能多的信息。You can use this concept to reduce the number of features in your dataset without having to lose much information and keep (or improve) the model’s performance. 不必纠结于采用降维的必要性，直接进入 techniques of dimension reduction. low variances这个是针对一个特征内部的，如果一个特征的数据本身没有什么变化，那么这个类似就是一种“死”数据。 high correlation filter用来判别特征 x 和最后的 target之间的相关性 principal component analysis (PCA)our old good friend. 如果你提降维，但是你不知道PCA，那么就说不过去。该方法的基本思路：一个基（向量空间）的变换，使得变换后的数据有着最大的方差。It works on a condition that while the data in a higher dimensional space is mapped to data in a lower dimension space, the variance of the data in the lower dimensional space should be maximum.下面是PCA的一些特点： A principal component is a linear combination of the original variablesPrincipal components are extracted in such a way that the first principal component explains maximum variance in the datasetSecond principal component tries to explain the remaining variance in the dataset and is uncorrelated to the first principal componentThird principal component tries to explain the variance which is not explained by the first two principal components and so on 主成分是不断生成的，在前者基础之上生成的。 The first component is the most important one, followed by the second, then the third, and so on. Singular Value Decomposition (SVD)翻译成中文感觉还是挺别扭的，奇异值分解。关于奇异值，特征值这些数学概念打算另外写一个主题，wait a moment. 简单理解PCA 是针对方阵 (mm), SVD是针对矩阵(m n)，所以后者是具有更大的适用范围。 Independent Component Analysis (ICA)这个是在面试的时候被问道的一种降维方法。抓住独立向量应该就没有问题。 Independent Component Analysis (ICA) is based on information-theory and is also one of the most widely used dimensionality reduction techniques. The major difference between PCA and ICA is that PCA looks for uncorrelated factors while ICA looks for independent factors. 基本假设： This algorithm assumes that the given variables are linear mixtures of some unknown latent variables. It also assumes that these latent variables are mutually independent, i.e., they are not dependent on other variables and hence they are called the independent components of the observed data. ICA 和PCA的异同：从线性代数的角度去理解，PCA和ICA都是要找到一组基，这组基张成一个特征空间，数据的处理就都需要映射到新空间中去。ICA相比与PCA更能刻画变量的随机统计特性，且能抑制高斯噪声。 T-SNE就是指出 t-SNE 这个可以非线性降维。有local approaches 和 global approaches 两种方式，我大概的理解就是：类之间还能尽可能的远离，类内保持差异性。这个使用场景是在可视化中，经常会看见将数据或者 So far we have learned that PCA is a good choice for dimensionality reduction and visualization for datasets with a large number of variables. But what if we could use something more advanced? What if we can easily search for patterns in a non-linear way? t-SNE is one such technique. There are mainly two types of approaches we can use to map the data points:Local approaches : They maps nearby points on the manifold to nearby points in the low dimensional representation.Global approaches : They attempt to preserve geometry at all scales, i.e. mapping nearby points on manifold to nearby points in low dimensional representation as well as far away points to far away points. 下面介绍两种不是那么“常规”，但是也符合”dimension reduction” 定义的方式。 projectionBy projecting one vector onto the other, dimensionality can be reduced. autoencoder网络结构通常有 encoder和decoder两部分组成，那么encoder 就作为 information abstraction,而 decoder作为一种重新映射。从这个角度NLP中的词向量也是可以是一种降维手段。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Linear Algebra in ML]]></title>
    <url>%2F2019%2F03%2F25%2FLinear-Algebra-in-ML%2F</url>
    <content type="text"><![CDATA[我觉得到 ML 中的一个难点：就是由原来简单的 linear equations 直接过渡到了 matrics and vectors。这个过程是没有人跟你说的。网络结构可以当作是一个complicated 并且是无法表示的函数，于是很多使用者把它当作黑匣子，关心于输入和输出，中间过程 don’t care. 变量（特征个数）和解的关系多变量和最后target的关系是可以使用 matrices 进行表示的，这就是一种数学公式化。 Broadly speaking, in linear algebra data is represented in the form of linear equations. These linear equations are in turn represented in the form of matrices and vectors. 先直观的感受一下变量和图形（可视化）的关系。两个变量组成的equations 是两条线的相交情况。而三个变量在空间中有三种情况： 相交，平行，不在一个平面上。三个变量组成的equations 是三个面的相交情况。有四种情况 (try hard to figure it out)：No intersection at all.Planes intersect in a line.They can intersect in a plane.All the three planes intersect at a point. 当到达4 dims 的时候，it’s impossible to visulize it. terms in related to matrix这些词汇 (terms) 经常在文献中出现，需要对于其含义有个比较好的认识。Order of matrix – If a matrix has 3 rows and 4 columns, order of the matrix is 34 i.e. rowcolumn. (翻译成 矩阵的阶)Square matrix – The matrix in which the number of rows is equal to the number of columns.Diagonal matrix – A matrix with all the non-diagonal elements equal to 0 is called a diagonal matrix.Upper triangular matrix – Square matrix with all the elements below diagonal equal to 0.Lower triangular matrix – Square matrix with all the elements above the diagonal equal to 0.Scalar matrix – Square matrix with all the diagonal elements equal to some constant k.Identity matrix – Square matrix with all the diagonal elements equal to 1 and all the non-diagonal elements equal to 0.Column matrix – The matrix which consists of only 1 column. Sometimes, it is used to represent a vector.Row matrix – A matrix consisting only of row.Trace – It is the sum of all the diagonal elements of a square matrix.Rank of a matrix – Rank of a matrix is equal to the maximum number of linearly independent row vectors in a matrix.Determinant of a matrix - 矩阵的行列式转置 -在图形 matrix中还是很常见的。$$\mathrm { A } _ { \mathrm { ij } } ^ { \mathrm{T}} = \mathrm { A } _ { \mathrm { ji } }$$ 这个矩阵乘法和元素相称的区别，后者是element-wise 进行的。可以从另外一个角度去列及矩阵相称： This operation on a vector is called linear transformation. 就是后面的vector 映射到了前面的矩阵空间。 特征值和奇异值着两个是分别对应着PCA 和SVD。Eigenvalues and Eigenvectors如公式所示，特征值和特征向量的乘积就是方阵和特征向量的乘积，原先的方阵是可以降维表示成特征向量和特征值的。$ A x = \lambda x $ 对于奇异值分解，最常见的就是这种表达：$A = U \Sigma V ^ { T }$特征值分解和奇异值分解都是给一个矩阵(线性变换)找一组特殊的基，特征值分解找到了特征向量这组基，在这组基下该线性变换只有缩放效果。而奇异值分解则是找到另一组基，这组基下线性变换的旋转、缩放、投影三种功能独立地展示出来了。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Back to my blog]]></title>
    <url>%2F2019%2F03%2F07%2FBack-to-my-blog%2F</url>
    <content type="text"><![CDATA[直到某一天发现个人网站中的图片都显示不出来了，查了一下才发现之前的图床不能用了（点名批评七牛），果断弃之，换了个大厂子产品。证明一下图片是能出来的。ps：之前的图片有时间再整理到新的平台上。]]></content>
      <categories>
        <category>人间不值得</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[基于simhash的文本相似度比较]]></title>
    <url>%2F2018%2F08%2F23%2F%E5%9F%BA%E4%BA%8Esimhash%E7%9A%84%E6%96%87%E6%9C%AC%E7%9B%B8%E4%BC%BC%E5%BA%A6%E6%AF%94%E8%BE%83%2F</url>
    <content type="text"><![CDATA[我只是想说我还没有放弃这个网站… 本文主要记录使用simhash比较中文文本的相似度问题。先说一下文本特征，数据属于中文文本，每篇文章的字数大于500,小于2000,基本上属于大文本。步骤如下： 基于tf-idf提取文本的关键词。如果这些关键词在之后的比较中是相同的，那么认为对应的文章也是相同。简而言之，这些提取的关键词可以看做原文章的”代表”. 根据关键字计算simhash编码，然后使用hamming distance进行比较两者的不同。如果对于上述概念比较模糊，建议首先阅读该篇博客。 顺滑过渡到代码实现：123456789101112131415# 常规导包import sys,codecsimport pandas as pdimport numpy as npimport jieba.possegimport jieba.analysefrom sklearn import feature_extractionfrom sklearn.feature_extraction.text import TfidfTransformerfrom sklearn.feature_extraction.text import CountVectorizer# 数据集的路径path =&quot;../tianmao2.csv&quot;names =[&apos;where&apos;, &apos;time&apos;, &apos;title&apos;, &apos;url&apos;, &apos;contents&apos;]data =pd.read_csv(path, delimiter=&apos;\t&apos;, names= names, nrows=200)data[&apos;id&apos;] =data.index+1data.head() 我们使用title和contents 组合作为原始处理的数据，我们认为该数据能够就是文章的内容。1stopkey = [w.strip() for w in codecs.open(&apos;../keyword_extraction/data/stopWord.txt&apos;, &apos;r&apos;).readlines()] 该stop words是中文停用词，就是常见的”的 了”。常见的有百度停用词表、哈尔滨工业大学停用词表以及中科院的停用词表。这里使用的是中科院的停用词。12345678def dataPrepos(text, stopkey): l = [] pos = [&apos;n&apos;, &apos;nz&apos;, &apos;v&apos;, &apos;vd&apos;, &apos;vn&apos;, &apos;l&apos;, &apos;a&apos;, &apos;d&apos;] # 定义选取的词性 seg = jieba.posseg.cut(text) # 分词 for i in seg: if i.word not in stopkey and i.flag in pos: # 去停用词 + 词性筛选 l.append(i.word) return l 我们选择名词作为主要的分析对象。12345678idList, titleList, abstractList = data[&apos;id&apos;], data[&apos;title&apos;], data[&apos;contents&apos;]corpus = [] # 将所有文档输出到一个list中，一行就是一个文档# 这个 虽然使用 &quot; &quot; 进行分割 但是实际上还是一个打的listfor index in range(len(idList)): text = &apos;%s。%s&apos; % (titleList[index], abstractList[index]) # 拼接标题和摘要 text = dataPrepos(text, stopkey) # 文本预处理 text = &quot; &quot;.join(text) # 连接成字符串，空格分隔 corpus.append(text) 这里的corus 是将所有的经过预处理文档作为当前计算 idf 的语料库。123456789vectorizer = CountVectorizer()X = vectorizer.fit_transform(corpus) # 词频矩阵,a[i][j]:表示j词在第i个文本中的词频# 2、统计每个词的tf-idf权值transformer = TfidfTransformer()tfidf = transformer.fit_transform(X)# 3、获取词袋模型中的关键词word = vectorizer.get_feature_names()# 4、获取tf-idf矩阵，a[i][j]表示j词在i篇文本中的tf-idf权重weight = tfidf.toarray() 使用sklearn 内置的函数计算tf-idf。1234567891011121314151617181920212223242526272829topK = 10ids, titles, keys, weights = [], [], [], []for i in range(len(weight)): print(&quot;-------这里输出第&quot;, i + 1, &quot;篇文本的词语tf-idf------&quot;) ids.append(idList[i]) titles.append(titleList[i]) df_word, df_weight = [], [] # 当前文章的所有词汇列表、词汇对应权重列表 for j in range(len(word)): # print(word[j],weight[i][j]) df_word.append(word[j]) df_weight.append(weight[i][j]) df_word = pd.DataFrame(df_word, columns=[&apos;word&apos;]) df_weight = pd.DataFrame(df_weight, columns=[&apos;weight&apos;]) word_weight = pd.concat([df_word, df_weight], axis=1) # 拼接词汇列表和权重列表 word_weight = word_weight.sort_values(by=&quot;weight&quot;, ascending=False) # 按照权重值降序排列 # 在这里可以查看 k的选取的数值应该是多大， # from ipdb import set_trace # set_trace() keyword = np.array(word_weight[&apos;word&apos;]) # 选择词汇列并转成数组格式 word_split = [keyword[x] for x in range(0, topK)] # 抽取前topK个词汇作为关键词 word_split = &quot; &quot;.join(word_split) keys.append(word_split) wei = np.array(word_weight[&apos;weight&apos;]) wei_split = [str(wei[x]) for x in range(0, topK)] wei_split = &quot; &quot;.join(wei_split) weights.append(wei_split) # 这里的命名 容易混淆result = pd.DataFrame(&#123;&quot;id&quot;: ids, &quot;title&quot;: titles, &quot;key&quot;: keys, &apos;weight&apos;: weights&#125;, columns=[&apos;id&apos;, &apos;title&apos;, &apos;key&apos;, &apos;weight&apos;]) 选择前10个频率最高的词语作为该篇文章的代表，当然这个参数是可以调整，需要根据具体的问题和结果进行调整。1result.head() 最后的效果如上。至此我们第一步的提取文章的关键词就已经做完。下面进行相似度的比较。 1234import jiebaimport jieba.analyseimport pandas as pd#日常导包 数据和上述的一样，所以就不截图了。123datasets =pd.read_csv(&quot;../tianmao2-tf-idf.csv&quot;)tokens =datasets[&apos;key&apos;]weights =datasets[&apos;weight&apos;] 提取关键词和对应的权重。123456789101112print(tokens[0], len(tokens[0]))print(weights[0], len(weights[0]))tokens0 =tokens[0].split()weights0 =weights[0].split()len(tokens0)len(weights0)tokens1 =tokens[1].split()weights1 =weights[1].split()import astweights0 =[ ast.literal_eval(i) for i in weights0]weights1 =[ ast.literal_eval(i) for i in weights1] 构造测试用例。因为权重是字符串，所以简单处理转成整数。 12dict0 =dict(zip(tokens0, weights0))dict1 =dict(zip(tokens1, weights1)) 定义一个Simhash，提供对文档的数值映射和文档间相似度计算的功能.123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081class Simhash(object): # 初始化函数 def __init__(self, weights_dict, tokens=&apos;&apos;, hashbits=64): self.hashbits = hashbits self.hash = self.simhash_function(tokens, weights_dict) # toString函数 # 不懂这个 self.hash ，凡是带有self 的函数都是可以类变量，所以这个就是返回的 self.hash这个变量 #凡是使用__str__ 这种类型的函数 都是重写 原来的函数 def __str__(self): return str(self.hash) &quot;&quot;&quot; ord() 函数是 chr() 函数（对于8位的ASCII字符串）或 unichr() 函数（对于Unicode对象）的配对函数，它以一个字符（长度为1的字符串）作为参数，返回对应的 ASCII 数值，或者 Unicode 数值 &quot;&quot;&quot; # 给每一个单词生成对应的hash值 # 这个操作搞懂之后一定很简洁， 但是现在很难理解，因为不是字符串，而是位操作 def _string_hash(self, source): if source == &apos;&apos;: return 0 else: x = ord(source[0]) &lt;&lt; 7 # &lt;&lt; 表示 乘以2^7 ; &gt;&gt; 表示除以 ; ** 表示次方的意思 # ^ : 按位异或 (二进制进行异或)； &amp; 按位进行与 操作 # 左移位操作也是可以理解为 2^x 的操作，因为存储是二进制，这样左移一位 表示×2 一次 m = 1000003 mask = 2 ** self.hashbits - 1 for c in source: x = ((x * m) ^ ord(c)) &amp; mask x ^= len(source) if x == -1: x = -2 return x # 生成simhash值 def simhash_function(self, tokens, weights_dict): v = [0] * self.hashbits # 这种使用 &#123;&#125; dictionary 然后强行得到item 再进行遍历也是牛逼 for key, t in &#123;x: self._string_hash(x) for x in tokens&#125;.items(): for i in range(self.hashbits): bitmask = 1 &lt;&lt; i if t &amp; bitmask: v[i] += weights_dict[key] else: v[i] -= weights_dict[key] fingerprint = 0 for i in range(self.hashbits): if v[i] &gt;= 0: fingerprint += 1 &lt;&lt; i return fingerprint # 求文档间的海明距离 def hamming_distance(self, other): x = (self.hash ^ other.hash) &amp; ((1 &lt;&lt; self.hashbits) - 1 ) tot = 0 while x : tot += 1 x &amp;= x - 1 return tot #求相似度 # 这个相似度的计算，十分简单，如果两个数接近，那么就是认为相似。越是接近1 越是相似， # 不是原先那种以某一个参数整数 如3 为距离的相似度 def similarity(self, other): a = float(self.hash) b = float(other.hash) if a &gt; b: return b / a else: return a / b if __name__ == &apos;__main__&apos;: hash0 = Simhash(weights_dict=dict0, tokens=tokens0) print(hash0) hash1 = Simhash(weights_dict=dict1, tokens=tokens1) print(hash1) print(hash0.hamming_distance(hash1)) print(hash0.similarity(hash1)) 结果如上。可以看出该例子中使用的两两比较的方式，对于大数据来说，一般可能会用到倒排索引和cpu并行技术。]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>文本相似度</tag>
        <tag>Simhash</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[文本相似度比较基本知识]]></title>
    <url>%2F2018%2F08%2F23%2F%E6%96%87%E6%9C%AC%E7%9B%B8%E4%BC%BC%E5%BA%A6%E6%AF%94%E8%BE%83%E5%9F%BA%E6%9C%AC%E7%9F%A5%E8%AF%86%2F</url>
    <content type="text"><![CDATA[本文服务于该篇博客,主要进行名词解释。 simhash基本概念simhash 也是一种hash，一般的hash 函数映射规则只需要满足以下两个条件： 对很多不同的特征来说，它们对所对应的向量是均匀随机分布的 相同的特征来说对应的向量是唯一简单来说普通的hash映射需要满足随机分布和唯一性两个条件。simhash想要实现的是，如果原来的文本的特征是相似，那么映射之后的编码也是相似。这里使用 hamming distance 进行比较simhash映射之后的距离。根据经验值，对64位的 SimHash值，海明距离在3以内的可认为相似度比较高。编码之后的表示在英文中是 fingerprint(指纹)。simhash最初被google 用于网页去重，当时使用的fingerprint 是64,所以这里沿用了这个传统。64位的签名可以表示多达264个象限，因此只保存所在象限的信息也足够表征一个文档了。更进一步，表示的文档的数字最多是多少？这个应该可以准确计算特征的个数应为如果用三位(01) 表示，那么有8种，那么2^64 这么多种特征，所以16*10^18 这么多。算法步骤第一步：文本预处理得到分词（去重，去除的了stop words）,然后weight权重可以使用分词的frequency 或者tfidf 进行得到第二步：进行hash 映射（可以使用md5这种传统的映射方式，基本的要求就是均匀映射到一个空间，这种映射并不能反映原始样本的相关性）第三步：hash 映射值和weight 进行相乘，如果原来是1 则乘以1，如果是0 则乘以-1，第四步： 列向相加，得到summing weights,进行降维如果是正数那么为1，如果是负数那么为-1然后这个simhash就出来了.有图有真相 simhash的局限性：只考虑到文章存在哪些词，没有考虑到词的顺序。不过相应的优点是，可以实现海量文章相似度计算。文章相似度计算忽略词的顺序之后效果更好。所以在处理大文本时候，simhash是有效的，但是在处理小文本，这种效果往往不能被保证。直观上理解，在一片段文章或者段落中，词语出现的顺序还是比较重要的。 minhash可以参考该视频和这篇文章。 Locality Sensitive HashingLocality Sensitive Hashing(局部敏感哈希)作用就是从海量的数据中挖掘出相似的数据，可以具体应用到文本相似度检测、网页搜索等领域。上面的simhah和minhash 就是该思想的实现。 距离函数这里的距离函数都是用来文本相似度。 Jaccard相似度简单来说交集除以并集。这个集合中存放的是文章或者段落的关键词。1234567891011def JaccardSim(str_a, str_b): &apos;&apos;&apos; Jaccard相似性系数 计算sa和sb的相似度 len（sa &amp; sb）/ len（sa | sb） &apos;&apos;&apos; seta = splitWords(str_a)[1] setb = splitWords(str_b)[1] sa_sb = 1.0 * len(seta &amp; setb) / len(seta | setb) return sa_sb 可以看到核心代码很简单，经过分词之后，就是seta 和setb 进行的操作。 cosine12345def cos_sim(a, b): a = np.array(a) b = np.array(b) # return &#123;&quot;文本的余弦相似度:&quot;:np.sum(a*b) / (np.sqrt(np.sum(a ** 2)) * np.sqrt(np.sum(b ** 2)))&#125; return np.sum(a * b) / (np.sqrt(np.sum(a ** 2)) * np.sqrt(np.sum(b ** 2))) 将文本的关键词映射成某种高维函数，然后在高维空间中计算两者的相似度。 tf-idf在simhash 中使用 tf-idf作为我们的比较函数。TF-IDF的主要思想就是：如果某个词在一篇文档中出现的频率高，也即TF高；并且在语料库中其他文档中很少出现，即DF的低，也即IDF高，则认为这个词具有很好的类别区分能力。词频(term frequency)有两种计算方式,后者考虑了相对的情况。计算idf(inverse document frequency):TF-IDF 优点是简单快速，比较符合实际。缺点，无法体现词的位置信息，所有的位置都是被认为重要性相同，但是开头结尾，段落的开头和段落的结尾，therefore，so，but这些词语都是没有体现的。 hamming distancehamming distance就是比较01串的不同，按照位进行比较。算法：异或时，只有在两个比较的位不同时其结果是1 ，否则结果为0，两个二进制“异或”后得到1的个数即为海明距离的大小。123456789101112131415161718hashbits =64 # 使用64位进行编码def simhash_function(tokens, weights_dict): v = [0] * hashbits # 这种 &#123;key: value&#125;.item() 的操作也是没有了谁了 for key, t in &#123;x: _string_hash(x) for x in tokens&#125;.items(): for i in range(hashbits): bitmask = 1 &lt;&lt; i if t &amp; bitmask: v[i] += weights_dict[key] else: v[i] -= weights_dict[key] fingerprint = 0 for i in range(hashbits): if v[i] &gt;= 0: fingerprint += 1 &lt;&lt; i return fingerprintfingerprint = simhash_function(tokens, weights) min edit distance123456789101112131415161718192021222324# 最小编辑距离def min_edit_distance(str1, str2): rows =len(str2) +1 cols =len(str1) +1 arr =[[0 for _ in range(cols)] for _ in range(rows)] # 这种简洁的代码也是牛逼 for j in range(cols): arr[0][j] =j for i in range(rows): arr[i][0] =i for i in range(1, rows): for j in range(1, cols): # 因为string 是从0 ，len(str) -1的 if str2[i-1] ==str1[j-1]: arr[i][j] =arr[i-1][j-1] else: # 以后见到这样的式子，就要想到这个二维的数组，因为这个是可以帮助记忆的 arr[i][j] =1 +min(arr[i-1][j-1], arr[i-1][j], arr[i][j-1]) # 右下角就是距离 return arr[rows-1][cols-1]str_a =&quot;abcdef&quot;str_b =&quot;azced&quot;result =min_edit_distance(str_a, str_b)result 具体可以参看该视频讲解。(ps. 如果刷leetcode,也可以参看该视频) 分词在英文中存在天然的空格可以进行分词操作，但是中文的分词就比较复杂了。常用的中文分词开源工具有 jieba和HanLP前者简单易行，容易上手；后者在自然语言处理作为汉语言处理包，可以用于词性标注，命名实体识别等一系列功能。常用的英文分词 corenlp 倒排索引倒排索引使用python在实现上就是一个dictionary 嵌套一个 set(). 一般的索引都是数字或者英文字母映射内容，具体在放到simhash的情景下就是使用文章的序列号对应提取出来的关键词。但是倒排索引就是关键词对应文章的序列号，类似与原来的”值”对应这”键”，所以称之为倒排索引。可以参考这篇文章。]]></content>
      <tags>
        <tag>文本相似度</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[differences-between-l1-and-l2-as-loss-function-and-regularization]]></title>
    <url>%2F2018%2F07%2F21%2Fdifferences-between-l1-and-l2-as-loss-function-and-regularization%2F</url>
    <content type="text"><![CDATA[如果补交作业也算作业的话，那么这篇博文就算做作业。L1 和L2 作为Loss function和 regularization，个人感觉是一个经常容易混淆的概念。但是如果读者觉得很清楚，那么就可以跳过了。本文大量借鉴于该博客，原文是英文，如果读者英文能够handle，建议读原文。 As loss functionloss function or error function 是用来衡量真实y 和生成的f(x) 之间差距的函数。在模型训练中我们一般情况下不断训练模型使得loss function不断下降（如果task要求loss function是增大，这时候一般加上符号或者转换成 1- loss fucntion，最后实现的还是loss function下降）。好的回到L1 loss function和L2 loss function. L1-norm loss function is also known as least absolute deviations(LAD), least absolute errors. It is basically minimizing the sum of the absolute differences between the target value Y and the estimated values (f(x)).L2-norm loss function is also known as least squares error(LSE). It is basically minimizing the sum of the square of the differences between the target value Y and the estimated values (f(x)).主要你从一下三个指标去衡量两者的不同： robustness，stability和是否具有唯一解。wiki 中关于robust 中是这样定义： A learning algorithm that can reduce the chance of fitting noise is called robust。具有更好的泛化性能，不去过度拟合noise. 关于stability) wiki 是这样定义：Stability, also known as algorithmic stability, is a notion in computational learning theory of how a machine learning algorithm is perturbed by small changes to its inputs. 我对于前两个指标的理解：robustness 是对于原始的train 样本中离群点的态度，如果某个模型是robustness的，那么对于该数据集中的离群点是能够抗干扰的。反之则是不具有robustness的。stability是对于原始train 数据的轻微的平移的反应，如果对于某个原始数据的轻微平移，最后的结果没有产生很大的波动，那么该模型就是具有stability, 反之，则不具有stability. 比较L1-norm 和L2-norm在前两个评价指标中的表现：对于第三点，我想在下面进行介绍。因为这点和后面和下面的solution uniqueness是相同的。 As regularization从XGBoost调参指南中我们知道objective function = loss funcion + regularization. 而我们大多数情况下提及的都是loss function,常常忽略了regularization 的作用。所以正确的姿态应该是这样的：The regularization term controls the complexity of the model, which helps us to avoid overfitting.对于模型训练，一开始的想法是尽量的overfitting, 因为就现在不成熟的经验而言，对于overfitting这个问题有很多处理方法，比如卷积深度神经网络中的dropout, LightGBM中的early stop 和随机采样的思想。 这些方法都是可以缓解overfitting，所以可以出现overfitting。相反，如果你的模型是underfitting，那么你就微显尴尬了。好，收回到L1 and L2。 先上公式L1 regularization on least squares:L2 regularization on least squares:The difference between their properties can be promptly summarized as follows: 对于第一点computational efficient的理解：平方比绝对值更容易计算，平方可以求导直接求最值，但是绝对值就无法求导。并且L1 regularization在 non-sparse cases中是 computational inefficient，但是在 sparse(0比较多) cases中是有相应的稀疏算法来进行优化的，所以是computational efficient.对于第二点是否具有sparse solution可以从几何意义的角度解读：The green line(L2-norm) is the unique shortest path, while the red, blue yellow(L1-norm) are all same length for the same route. Built-in feature selection is frequently mentioned as a useful property of the L1-norm, which the L2-norm does not. This is actually a result of the L1-norm, which tends to produces sparse coefficients (explained below). Suppose the model have 100 coefficients but only 10 of them have non-zero coefficients, this is effectively saying that “the other 90 predictors are useless in predicting the target values”. L2-norm produces non-sparse coefficients, so does not have this property. 所以表格中第三点也是顺理成章的了。至此，我们区分了L1-norm vs L2-norm loss function 和L1-regularization vs L2-regularization。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>l1</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LightGBM和XGBoost及其调参]]></title>
    <url>%2F2018%2F07%2F21%2FLightGBM%E5%92%8CXGBoost%E5%8F%8A%E5%85%B6%E8%B0%83%E5%8F%82%2F</url>
    <content type="text"><![CDATA[lightGBM调参(常用参数)Since lightGBM is based on decision tree algorithms, it splits the tree with the best fit whereas boosting algorithms split the tree depth wise or level wise rather than leaf-wise. So when growing on the same leaf in Light GBM, the leaf-wise algorithm can reduce more loss than the level-wise algorithm and hence results in much better accuracy which can rarely be achieved by any of the existing boosting algorithms. Also, it is surprisingly very fast, hence the word ‘Light’.Leaf wise splits lead to increase in complexity and may lead to overfitting and it can be overcome by specifying another parameter max_depth parameter. Advantages of LightGBM faster training speed and higher efficiencyLight GBM use histogram based algorithm i.e it buckets continuous feature values into discrete bins which fasten the training procedure. lower memory usageReplaces continuous values to discrete bins which result in lower memory usage. better accuracy than any other boosting algorithmIt produces much more complex trees by following leaf wise split approach rather than a level-wise approach which is the main factor in achieving higher accuracy. compatibility with large datasetsIt is capable of performing equally good with large datasets with a significant reduction in training time as compared to XGBOOST. parallel learning supported lightGBM调参(常用参数) taskdefault= train, option: train, prediction applicationdefault= regression, option: regression, binary, multiclass, lambdarank(lambdarank application) datatraining data, 这个比较诡异，你需要创建一个lightGBM类型的data num_iterationsdefault =100, 可以设置为的大一些，然后使用early_stopping进行调节。 early_stopping_rounddefault =0, will stop training if one metric of one validation data doesn’t improve in last early_stopping_round rounds. num_leavesdefault =31, number of leaves in a tree devicedefault =cpu, options: gpu, cpu, choose gpu for faster training. max_depthspecify the max depth to which tree will grow, which is very important. feature_fractiondefault =1, specifies the fraction of features to be taken for each iteration. bagging_fractiondefault =1, spefifies the fraction of data to be used for each iteration and it generally used to speed up the training and avoid overfitting. max_binmax number of bins to bucket the feature values.因为模型是基于bin训练的，如果bin 数量越多，得到better accuracy,同时更加容易 overfitting. num_threads labelspecify the label columns. categorical_featurespecify the categorical features num_classdefault =1, used only for multi-class classification referrencewhich-algorithm-takes-the-crown-light-gbm-vs-xgboostLightGBM 如何调参官方文档param_tuning官方文档parameter XGBoost调参Advantage of XGBoost regularizationstandard GBM implementation has no regularization, in fact, XGBoost is also known as ‘regularized boosting’ technique. parallel processingwe know that boosting is sequential process so how can it be parallelized? this link to explore further. high flexibilityXGBoost allow users to define custom optimization objectives and evaluation criteria handling missing valuesvery useful property. XGBoost has an in-built routine to handle missing values. User is required to supply a different value than other observations and pass that as a parameter. XGBoost tries different things as it encounter a missing value on each node and learns which path to take for missing values in future. Tree pruningA GBM would stop splitting a node when it encounters a negative loss in the split. Thus it is more of a greedy algorithm. XGBoost on the other hand make splits upon the max_depth specified and then start pruning the tree backwards and remove splits beyond which there is no positive gain. built-in cross-validationThis is unlike GBM where we have to run a grid-search and only a limited values can be tested. continue on existing model XGBoost Parametersgeneral parametersGeneral Parameters: Guide the overall functioning booster:default =gbtree, can be gbtree, gblinear or dart. 一般使用gbtree. silent:default =0, silent mode is activated if set to 1(no running messages will be printed) nthread:default to maximum of threads. booster parametersBooster Parameters: Guide the individual booster (tree/regression) at each step eta(learning rate):default=0.3, typical final values to be used: 0.01-0.2, using CV to tune min_child_weight:minimum sum of instance weight (hessian) needed in a child. If the tree partition step results in a leaf node with the sum of instance weight less than min_child_weight, then the building process will give up further partitioning. In linear regression mode, this simply corresponds to minimum number of instances needed to be in each node. The larger, the more conservative the algorithm will be.default =1,too high values can lead to under-fitting, it should be tuned using CV. 数值越小越容易过拟合，越大越容易 under-fitting. max_depth:default =6, typical values: 3-10, should be tuned using CV. gamma:default =0, Gamma specifies the minimum loss reduction required to make a split.如果在分裂过程中小于该值，那么就不会继续分裂。 subsample:default =1, typical values: 0.5-1. Denotes the fraction of observations to be randomly samples for each tree. colsample_bytree:default =1, typical values: 0.5-1. colsample_bytree和subsample不同点：colsample_by是特征的随机fraction, subsample是rows的随机fraction。 lambda:default =1, L2 regularization term on weights(analogous to Ridge regression). Though many data scientists don’t use it often, it should be explored to reduce overfitting. alpha:default =0, L1 regularization term on weight (analogous to Lasso regression) scale_pos_weight:default =1, a value greater than 0 should be used in case of high class imbalance as it helps in faster convergence. learning task parametersLearning Task Parameters: Guide the optimization performed objectivebinary: logistic- returns predicated probability(not class)multi: softmax- returns predicated class(not probabilities)multi: softprob- returns predicated probability of each data point belonging to each class. eval_metircdefault according to objective(rmse for regression and error for classification), used for validation data.typical values: rmse(root mean square error), mse(mean absolute error), logloss(negative log-likelihood), error(binary classification error rate, 0.5 threshold), auc(area under the curve) seeddefault =0, used for reproducible results and also for parameter tuning. Control OverfittingThere are in general two ways that you can control overfitting in xgboost. The first way is to directly control model complexity. The second way is to add regularization parameters Referrencecomplete guide parameter tuning xgboost with codes python官方文档 param_tuning官方文档 parameter]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>LightGBM</tag>
        <tag>XGBoost</tag>
        <tag>调参</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[那些年的算法题目（二）]]></title>
    <url>%2F2018%2F07%2F21%2F%E9%82%A3%E4%BA%9B%E5%B9%B4%E7%9A%84%E7%AE%97%E6%B3%95%E9%A2%98%E7%9B%AE%EF%BC%88%E4%BA%8C%EF%BC%89%2F</url>
    <content type="text"><![CDATA[最长01相同子串已知一个长度为N的字符串，只由0和1组成， 求一个最长的子串，要求该子串出0和1出现的次数相等。思路：最简单的方式是先生成字串，然后判断每个字串是否满足0的个数和1的个数相同。这种暴力求解时间复杂度O(n^3),明显是不合理的。下面说一下简单的做法：定义一个数组B[N]，B[i]表示从A[0…i]中 num_of_0 - num_of_1，0的个数与1的个数的差 。那么如果A[i] ~ A[j]是符合条件的子串，一定有 B[i] == B[j]，因为中间的部分0、1个数相等，相减等于0。 代码实现12345678910111213141516171819202122232425def lengest01SubStr(s): &apos;&apos;&apos; 最长0,1 相等的子串长度 &apos;&apos;&apos; count =[0, 0] B =[0]*len(s) dic =&#123;&#125; lengest =0 for i in range(len(s)): count[int(s[i])] +=1 B[i] =count[0] - count[1] if B[i] ==0: lengest +=1 continue if B[i] in dic: lengest =max(lengest, i- dic[B[i]]) else: dic[B[i]] =i return lengesta =&apos;1011010&apos;b =&apos;10110100&apos;print(lengest01SubStr(a))print(lengest01SubStr(b)) 顺时针打印矩阵输入一个矩阵，按照从外向里以顺时针的顺序依次扫印出每一个数字。思路：发现网上有很多使用递归的，但是使用四个循环就可以解决这个问题。找到每次开始的起点，然后按照最上面一行，最右面一列，最小面一行和最左面一行这样的顺序进行打印即可。123456789101112131415161718192021222324252627282930313233343536373839404142def printMatrix(matrix): if matrix ==[[]]: return # 第一次见这样判断空的matrix row =len(matrix) column =len(matrix[0]) # 这里的left, right, up, down 都是真实能够access到数据的 left =0 right =column -1 up =0 down =row -1 res =[] while right &gt;left and up &lt;down: # from left to right for i in range(left, right+1): res.append(matrix[up][i]) # from up to down for i in range(up+1, down+1): res.append(matrix[i][right]) # from right to left for i in range(right-1, left-1, -1): res.append(matrix[down][i]) for i in range(down-1, up, -1): res.append(matrix[i][left]) left +=1 right -=1 up +=1 down -=1 # 最后对于这种特殊情况的处理是容易忘记的 # left one row 这种情况很特殊，只是从左往右遍历 if up ==down and left &lt;right: for i in range(left, right+1): res.append(matrix[up][i]) # left one column 只有可能是从上往下遍历 if left ==right and up &lt;down: for i in range(up, down+1): res.append(matrix[i][left]) if up ==down and left ==right: res.append(matrix[left][up]) return resprint(printMatrix(matrix)) 下面这个版本并没有运行成功，但是中间有个语法点是可以学习的。12345678910111213141516171819202122232425262728293031323334353637def printMatrix(matrix): res =[] # 第一个坐标表示行数，第二个坐标表示列数 # m 表示行数，n 表示列数 m =len(matrix) n = len(matrix[0]) if m ==1 and n ==1: res =[matrix[0][0]] return res else: for o in range(int((min(m,n)+1)/2)): # 不加这个[] 会有语法错误 [res.append(matrix[o][i]) for i in range(o, n-o)] [res.append(matrix[j][n-o-1]) for j in range(o, m-o) if matrix[j][n-o-1] not in res ] # 感觉这个最后的not in 是不合理的，万一这两个数字就是相同呢，试一下 [res.append(matrix[m-o-1][k]) for k in range(n-1, o-1, -1) if matrix[m-o-1][k] not in res] [res.append(matrix[l][o]) for l in range(m-1-o, o-1,-1) if matrix[l][o] not in res] # 这种写法还是少用，可以使用append,就少用这种，可读性不强，容易错 return resdef printMatrix(matrix): res =[] # 第一个坐标表示行数，第二个坐标表示列数 # m 表示行数，n 表示列数 m =len(matrix) n = len(matrix[0]) if m ==1 and n ==1: res =[matrix[0][0]] return res else: for o in range(int((min(m,n)+1)/2)): # 不加这个[] 会有语法错误 [res.append(matrix[o][i]) for i in range(o, n-o)] [res.append(matrix[j][n-o-1]) for j in range(o, m-o) if matrix[j][n-o-1] not in res ] # 感觉这个最后的not in 是不合理的，万一这两个数字就是相同呢，试一下 [res.append(matrix[m-o-1][k]) for k in range(n-1, o-1, -1) if matrix[m-o-1][k] not in res] [res.append(matrix[l][o]) for l in range(m-1-o, o-1,-1) if matrix[l][o] not in res] # 这种写法还是少用，可以使用append,就少用这种，可读性不强，容易错 return res 最短摘要生成思路：两个指针begin和end, 首先将end 指针向后移动至包含所有的字符集q的位置；然后将begin向后移动直至再移动一次将不再包含q，记录该位置长度。然后将begin向后移动一位，end继续向后移动直至包含所有q，周而复始。比较位置长度，然后得出最短距离。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879#include &quot;stdio.h&quot;#include &quot;string.h&quot;#include &quot;assert.h&quot;#define MAX 1024int isMatchAll(const char *str,const char *key,int begin,int end)&#123; int ret =0; char hash[256]; int i =0; int lenK = strlen(key); memset(hash,0,sizeof(hash)); for(i=begin;i&lt;=end;i++) &#123; hash[str[i]]=1; &#125; // 标记string之后，然后再遍历key中的字符是否存在 for(i=0;i&lt;lenK;i++) &#123; if(hash[key[i]]==0) break; &#125; if(i == lenK ) ret =1; return ret;&#125;void find(const char *str,const char *key)&#123; int lenS = strlen(str); int lenK = strlen(key); int begin = 0; int end = 0; int minLength = 0x7FFFFFFF; int mstart = 0; int mend =0; assert(str&amp;&amp;key); for(;;) &#123; while(!isMatchAll(str,key,begin,end)&amp;&amp;end&lt;lenS) &#123; end++; &#125; while(isMatchAll(str,key,begin,end)) &#123; if(end-begin+1 &lt;minLength) &#123; minLength = end-begin+1; mstart =begin; mend =end; &#125; begin++; &#125; if(end&gt;=lenS) break; &#125; printf(&quot;%d\n&quot;,minLength); for(;mstart&lt;=mend;mstart++) printf(&quot;%c&quot;,str[mstart]);&#125; int main()&#123;/** char *str =&quot;hello are you bottom of do the is bot doke astring&quot;; char *key =&quot;abde&quot; */ char str[MAX]; char key[MAX]; gets(str); gets(key); find(str,key); return 0;&#125;]]></content>
      <categories>
        <category>算法</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Loss-Activation-and-Optimisation-Function]]></title>
    <url>%2F2018%2F07%2F07%2FLoss-Activation-and-Optimisation-Function%2F</url>
    <content type="text"><![CDATA[用图说话… Loss Function(Error Function)For accurate predictions, one needs to minimize the calculated error. In a neural network, this is done using back propagation.损失函数是用来估量你模型的预测值f(x)与真实值Y的不一致程度，它是一个非负实值函数,通常使用L(Y,f(x))来表示，损失函数越小，模型的鲁棒性就越好。按照函数种类可以划分一下主要的几个类别。 log损失函数常用于逻辑回归中。 平方损失函数又称为最小二乘法，最优拟合直线应该是使各点到回归直线的距离和最小的直线，即平方和最小。Y-f(X) 表示残差，残差在数理统计中是指实际观察值与估计值（拟合值）之间的差。整个式子表示的是残差的平方和。 指数损失函数Adaboost的目标式子就是指数损失，在给定n个样本的情况下，Adaboost的损失函数为： Hinge损失函数听着名字怪怪的，但是SVM（support vector machine）的损失函数就是这个。从目标函数看来，lr 采用的logistic loss 和 svm 采用的 hinge loss function 思想都是增加对分类影响较大点的权重，减少那些与分类相关不大点的权重。但是两个方法处理的方法不同: LR采用一个sigmod的映射函数，通过这样的非线性映射，大大降低了离分类平面点远的权重 ; SVM 采用的是一个hinge loss function，通过上图可以看到，对于那些离超平面比较远的点，直接设为0了，也就是说直接忽视，只考虑那些对分类平面有影响的点，这些点就是我们经常听到的支持向量。另一方面，支持向量机只考虑局部的边界线附近的点，而逻辑回归考虑全局（远离的点对边界线的确定也起作用，虽然作用会相对小一些）。 按照应用领域可以划分为:Regressive loss functions:They are used in case of regressive problems, that is when the target variable is continuous. Most widely used regressive loss function is Mean Square Error.Other loss functions are:Absolute error — measures the mean absolute value of the element-wise difference between input.Classification loss functions:The output variable in classification problem is usually a probability value f(x), called the score for the input x. Generally, the magnitude of the score represents the confidence of our prediction. The target variable y, is a binary variable, 1 for true and -1 for false.On an example (x,y), the margin is defined as yf(x). The margin is a measure of how correct we are. Most classification losses mainly aim to maximize the margin. Some classification algorithms are: Binary Cross Entropy,Negative Log Likelihood,Margin Classifier and Soft Margin Classifier.Embedding loss functions:It deals with problems where we have to measure whether two inputs are similar or dissimilar. Some examples are: L1 Hinge Error- Calculates the L1 distance between two inputs. Cosine Error- Cosine distance between two inputs. Visualising Loss Functions:We performed the task to reconstruct an image using a type of neural network called Autoencoders. Different results were obtained for the same task by using different Loss Functions, while everything else in the neural network architecture remained constant. Thus, the difference in result represents the properties of the different loss functions employed. A very simple data set, MNIST data set was used for this purpose. Three loss functions were used to reconstruct images. Activation FunctionWhat?It’s just a thing (node) that you add to the output end of any neural network. It is also known as Transfer Function. It can also be attached in between two Neural Networks. Sigmoid or Logistic Activation Function导数比较有特点： Softmax FunctionThe softmax function is a more generalized logistic activation function which is used for multiclass classification.使用softmax和多个logistic的多分类的区别：softmax回归进行的多分类，类与类之间是互斥的，即一个输入只能被归为一类；多个logistic回归进行多分类，输出的类别并不是互斥的，即”苹果”这个词语既属于”水果”类也属于”3C”类别。 Tanh ReLUReLU (Rectified Linear Unit) Activation Functionit is used in almost all the convolutional neural networks or deep learning.在卷积网络和深度网络中经常看到。 Leaky ReLU明显的发现，Leaky ReLU是对于在 X&lt;0时候的改进。Both Leaky and Randomized ReLU functions are monotonic in nature. Also, their derivatives also monotonic in nature. Optimisation AlgorithmsOptimisation Algoritms are used to update weights and biases i.e. the internal parameters of a model to reduce the error. They can be divided into two categories:Back Propogation and Optimisation Function: Error J(w) is a function of internal parameters of model i.e weights and bias. For accurate predictions, one needs to minimize the calculated error. In a neural network, this is done using back propagation. The current error is typically propagated backwards to a previous layer, where it is used to modify the weights and bias in such a way that the error is minimized. The weights are modified using a function called Optimization Function.Optimisation functions usually calculate the gradient i.e. the partial derivative of loss function with respect to weights, and the weights are modified in the opposite direction of the calculated gradient. This cycle is repeated until we reach the minima of loss function. Constant Learning Rate AlgorithmsHere η is called as learning rate which is a hyperparameter that has to be tuned. Choosing a proper learning rate can be difficult.选的小训练速度慢， While a learning rate that is too large can hinder convergence and cause the loss function to fluctuate around the minimum or even to divergeA similar hyperparameter is momentum(动量), which determines the velocity with which learning rate has to be increased as we approach the minima. Adaptive Learning Algorithms(自适应)Adaptive gradient descent algorithms such as Adagrad, Adadelta, RMSprop, Adam, provide an alternative to classical SGD. They have per-paramter learning rate methods, which provide heuristic approach without requiring expensive work in tuning hyperparameters for the learning rate schedule manually. The challenge of using gradient descent is that their hyper parameters have to be defined in advance and they depend heavily on the type of model and problem. Another problem is that the same learning rate is applied to all parameter updates. If we have sparse data, we may want to update the parameters in different extent instead.(假装翻译:gradient descent 的艰难之处在于需要提前 define这种hyper parameters, 并且不同数据的learning rate 应该是不同（学习率应该根据数据的稀疏与否变化)) 说到这我们就多说一些关于自适应算法的内容。We used three first order optimisation functions and studied their effect-Stochastic Gradient Decent, Adagrad and Adam. Gradient Descent calcultes gradient for the whole dataset and updates values in direction opposite to the gradients until we find a local minima. Stochastic Gradient Descent performs a parameter update for each training example unlike normal Gradient Descent which performs only one update. Thus it is much faster. Gradient Decent algorithms can further be improved by tuning important parametes like momentum, learning rate etc. Adagrad is more preferrable for a sparse data set as it makes big updates for infrequent parameters and small updates for frequent parameters. It uses a different learning Rate for every parameter θ at a time step based on the past gradients which were computed for that parameter. Thus we do not need to manually tune the learning rate. Adam stands for Adaptive Moment Estimation. It also calculates different learning rate. Adam works well in practice, is faster, and outperforms other techniques. Stochastic Gradient Decent was much faster than the other algorithms but the results produced were far from optimum. Both, Adagrad and Adam produced better results that SGD, but they were computationally extensive. Adam was slightly faster than Adagrad. Thus, while using a particular optimization function, one has to make a trade off between more computation power and more optimum results. SGD 和 GDD的区别：正如上所说，在∂E/∂wi=∑（h(x)-y）(xi) 的时候∑耗费了大量的时间，特别是在训练集庞大的时候。所以肯定有人会猜想，如果把求和去掉如何，即变为∂E/∂wi=（h(x)-y）(xi)。（只是专注于当前训练训练集合，当前的数据）对于步长η的取值，标准梯度下降的η比随机梯度下降的大。因为标准梯度下降的是使用准确的梯度，理直气壮地走，随机梯度下降使用的是近似的梯度，就得小心翼翼地走，怕一不小心误入歧途南辕北辙了。 事实证明：中文和英文混在一起，排版是难看的。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>优化</tag>
        <tag>Loss Function</tag>
        <tag>Activation Function</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[特征工程相关概念]]></title>
    <url>%2F2018%2F06%2F29%2F%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[特征离散化？连续化特征就是一些不可枚举的有理数。那么什么是离散化特征呢？ 离散化特征就是可枚举的特征。离散化的作用是把数据变成可计算状态。而特征工程就是从原始字段中根据业务提取出对模型有效的特征出来。 在线性模型下(w.x)，w已经确定的情况下，x的某个特征的值是20，或者30，w.x的值相差是很大的，但是往往20岁的人跟30岁的人对同一个广告的兴趣差距不会那么大。连续特征离散化的基本假设，是默认连续特征不同区间的取值对结果的贡献是不一样的。离散化和连续化最大的区别是，对一个字段做连续化后的结果就还只是一个特征，而离散化后的这一列有多少个key(字段可能的值)就会抽取出多少个特征。当经过离散化之后，特征各有各的权重，彼此之间就没有关系了。 模型是使用离散特征还是连续特征,其实是一个“海量离散特征+简单模型” 同 “少量连续特征+复杂模型”的权衡。既可以离散化用线性模型，也可以用连续特征加深度学习。 常用的选取离散点的方法：等距离离散，等样本离散、画图观察趋势和决策树模型(天生就可以对连续特征分段)。 在工业界，很少直接将连续值作为特征喂给逻辑回归模型，而是将连续特征离散化为一系列0、1特征交给逻辑回归模型，这样做的优势有以下几点： 单变量离散化为N个后，每个变量有单独的权重，在激活函数的作用下相当于为模型增加了非线性，能够提升模型表达能力，加大拟合。 离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄&gt;30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰，因为特征值的异常会导致权重也就是w的值也会异常。 一定有同学担心特征过多会导致运算缓慢，但是LR是线性模型，我们在内部计算的时候是向量化计算，而不是循环迭代。稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展。 所以不用担心像GBDT算法那样，特征多了就跑不动了(我们都说GBDT不能用离散特征不是因为它处理不了离散特征，而是因为离散化特征后会产生特别多的特征，决策树的叶子节点过多，遍历的时候太慢了)。 所以海量离散特征＋LR是业内常见的一个做法。而少量连续特征+复杂模型是另外一种做法，例如GBDT。 当然也可以通过深度学习来进行特征选择：目前这种手段正在随着深度学习的流行而成为一种手段，尤其是在计算机视觉领域，原因是深度学习具有自动学习特征的能力，这也是深度学习又叫unsupervised feature learning的原因。从深度学习模型中选择某一神经层的特征后就可以用来进行最终目标模型的训练了。 参考文献:https://blog.csdn.net/lujiandong1/article/details/52412123 组合特征先是离散化，然后是特征组合。交叉从理论上而言是为了引入特征之间的交互，也即为了引入非线性。LR(逻辑回归）分类算法:因为线性函数的表达能力有限，所以我们引入激活函数就是给LR增加非线性关系。能让一条直线变成曲线。这样可以拟合出更好的效果。（也由此才后了后来说的过拟合问题而引入了正则化超参数）LR模型之所以很受欢迎，主要是因为LR模型本质是对数线性模型，实现简单，易于并行，大规模扩展方便，迭代速度快，同时使用的特征比较好解释，预测输出在0与1之间契合概率模型。（模型的可解释性举例，比如A-B的权重比较大，A代表用户，B代表物品，那么可以认为A是对B比较感兴趣的）但是，线性模型对于非线性关系缺乏准确刻画，特征组合正好可以加入非线性表达，增强模型的表达能力。另外，广告LR中，基本特征可以认为是用于全局建模，组合特征更加精细，是个性化建模，因为在这种大规模离散LR中，单对全局建模会对部分用户有偏，对每一用户建模又数据不足易过拟合同时带来模型数量爆炸，所以基本特征+组合特征兼顾了全局和个性化。 从统计的角度解释，基本特征仅仅是真实特征分布在低维的映射，不足以描述真实的分布，加入特征在高维空间拟合真实分布，使得预测更加准确。 正则化真正测试一个模型的不是简单与否，更重要在于它在预测新的情况时表现如何。小权重意味着网络的行为不会因为我们随意更改了一些输入而改变太多。这是我们加了正则化之后的成本函数，可以看我们后面加入了正则化 λ 的表达式来完善成本函数。为什么加入λ能够减轻过拟合呢？直观一点的解释是设置的λ值越大，那么参数w的值就会被压缩的越小(在梯度下降中, 每次迭代的步长，也就是这个公式w=w - 学习率*成本函数对w的导数， 现在由于成本函数增加了正则项，使得J和w变得数值相关了)。 假设λ设置的足够大，那么w会无限的趋近于0. 把多隐藏层的单元的权重设置为0以后，那么基本上就是消除掉了这些单元的作用，而使得网络模型得到简化，就像下面的图一样。由于正则化的设置，消除了一些隐藏单元的作用。而使得整个模型越来越接近于线性化，也就是从下图中的过拟合往欠拟合偏转。当然我们有一个适合的λ的值，能让我们的拟合状态达到最佳。所以我们在训练模型的时候，往往都会有一个Ｌ２正则项的超参数需要我们设置。这是我们的tanh激活函数， 可以看到当z的值越大时，整个函数的非线性就越大，而z的值越小(图中红色加粗部分),函数就越是呈现出线性分布。 所以当我们增加λ的值， w得值就越小，相应的z的值也就越小。因为z = wx + b。 而我们第一次说激活函数的时候就说过神经网络中基本上是不使用线性函数作为激活函数的，因为不论有多少层，多少个单元，线性激活函数会使得所有单元所计算的都呈现线性状态。 杂货铺特征工程可以分为特征处理、Feature Selection（特征选择）、Feature Extraction（特征提取）和Feature construction（特征构造）等阶段。归一化（去中心，方差归一）是属于特征(预)处理:把特征值压缩成0~1的区间。 One-hot（也叫One-of-k）的方法把每个无序特征转化为一个数值向量。比如一个无序特征color有三种取值：red，green，blue。那么可以用一个长度为3的向量来表示它，向量中的各个值分别对应于red，green，blue。如：这种方法在NLP里用的很多，就是所谓的词向量模型。变换后的向量长度对于词典长度，每个词对应于向量中的一个元素。 对于欠拟合: 增加神经网络复杂度，出现欠拟合的原因之一是由于函数的非线性不足，所以用更复杂的网络模型进行训练来加深拟合。对于过拟合：增加数据规模， 出现过拟合的原因之一是数据规模不足而造成的数据分布不均，扩展数据规模能比较好的解决这个问题。当然另一个做法是正则化，我们采取使用正则化来解决过拟合问题，常用的是L2正则，其他的还有L1和 Dropout正则。 很多人会想着既然线性分类器搞不定，那就直接找个非线性的好了，比如高斯核的SVM。我们确实可以通过这种简单换算法的方式解决这个简单的问题。但对于很多实际问题（如广告点击率预测），往往特征非常多，这时候时间约束通常不允许我们使用很复杂的非线性分类器。这也是为什么算法发展这么多年，广告点击率预测最常用的方法还是Logistic Regression (LogisticReg)。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>特征工程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常见的排序算法总结]]></title>
    <url>%2F2018%2F06%2F29%2F%E5%B8%B8%E8%A7%81%E7%9A%84%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[分类和总结 根据待排序的数据大小不同，使得排序过程中所涉及的存储器不同，可分为内部排序和外部排序。 排序关键字可能出现重复，根据重复关键字的排序情况可分为稳定排序和不稳定排序。 对于内部排序，依据不同的排序原则，可分为插入排序、交换(快速)排序、选择排序、归并排序和计数排序。 针对内部排序所需的工作量划分，可分为:简单排序 O(n^2)、先进排序 O(nlogn)和基数排序 O(d*n)。常见算法的性质总结： 排序算法实现默认都是升序… 插入排序(Insert Sort)123456789101112131415def insert_sort(lists): count = len(lists) for i in range(1, count): key =lists[i] j =i-1 while j &gt;= 0: if lists[j] &gt;key: lists[j+1] =lists[j] lists[j] =key j -= 1 return lists# testlists =[1,2,-3, 90,34]print(insert_sort(lists)) 选择排序(Select Sort)为每个位置选择当前元素最小的。选择最小的元素和a[0]交换，选择次最小的和a[1]交换，以此类推。代码实现:12345678910111213def select_sort(lists): count =len(lists) for i in range(0, count): min =i for j in range(i+1, count): if lists[min] &gt;lists[j]: min =j lists[min], lists[i] =lists[i], lists[min] return lists# testlists =[1, 23,45, 0,-1]print(select_sort(lists)) 冒泡排序(Bubble Sort)重复遍历要排序的数列，一次比较两个元素，如果顺序错误就交换位置。1234567891011def bubble_sort(lists): count =len(lists) for i in range(0, count): for j in range(i+1, count): if lists[i]&gt; lists[j]: lists[i], lists[j] =lists[j], lists[i] return lists# testlists =[1,34,45,0,89]print(bubble_sort(lists)) 归并排序(Merge Sort)归并排序，应该是我第一个接触的排序算法。然后在在某次重要面试时候，使用该算法救急。该算法是采用采用分治法(Divide and Conquer)的思想:先使得子序列有序，然后合并子序列。1234567891011121314151617181920212223242526def merge(left, right): i, j =0,0 result =[] while i&lt;len(left) and j &lt;len(right): if left[i] &lt;= right[j]: result.append(left[i]) i +=1 else: result.append(right[j]) j +=1 result += left[i:] result += right[j:] return resultdef merge_sort(lists): if len(lists) &lt;=1: return lists num =int(len(lists)/2) left =merge_sort(lists[:num]) right = merge_sort(lists[num:]) return merge(left, right)# merge_sort 是先切分，然后再整合，quick sort 是两个指针# testlists =[1, 34, 23,45,0,9]print(merge_sort(lists)) 快速排序(Quick Sort)快速排序的思想：任意选择一个key(通常选择a[0])，将比他小的数据放在它的前面，比他大的数字放在它的后面。递归进行。12345678910111213141516171819202122232425def quick_sort(lists, left, right): if left &gt;= right: return lists key =lists[left] low =left high =right while left &lt; right: # 因为你最初key 取得是left，然后从右边找到一个比key小的，然后替换left 的位置 while left &lt;right and lists[right]&gt;=key: right -= 1 lists[left] =lists[right] while left &lt; right and lists[left] &lt;= key: left +=1 lists[right] =lists[left] lists[left] =key # 这里的left 和right 都是可以的，因为从while 中出来之后两者是相同的 # 这个步伐是1,所以只能是一个个变化 quick_sort(lists, low, left-1) quick_sort(lists, left+1, high) # 因为left的位置已经被占了，所以只是划分左边一块，右边一块就是可以的 return lists# testlists =[3,2,45, 100,1,56,56]#lists =[1,2,3,2,2,2,5,4,2]print(quick_sort(lists, 0, len(lists)-1)) 堆排序参看另一篇博客中堆排序。 参考文献算法动图效果排序算法分类]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>排序算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[那些年的算法题目（一）]]></title>
    <url>%2F2018%2F06%2F22%2F%E9%82%A3%E4%BA%9B%E5%B9%B4%E7%9A%84%E7%AE%97%E6%B3%95%E9%A2%98%E7%9B%AE%EF%BC%88%E4%B8%80%EF%BC%89%2F</url>
    <content type="text"><![CDATA[交作业了… 完全二叉树插入问题描述已知一个完全二叉树的结构，现在需要将一个节点插入到这颗完全二叉树的最后，使得它还是一个完全二叉树。第一种解法：如果该树为满二叉树或者左子树不为满二叉树，那么就进入左子树，否则进入右子树，递归进行。 二叉树(Binary Tree)强行补充一下关于二叉树概念的知识。完全二叉树(Complete Binary Tree):若设二叉树的深度为h，除第h层外，其它各层(1～h-1)的结点数都达到最大个数，第h层所有的结点都连续集中在最左边，这就是完全二叉树。满二叉树:树中除了叶子节点，每个节点都有两个子节点。满二叉树是一种特殊的完全二叉树。二叉搜索树(binary search tree):所有非叶子结点至多拥有两个儿子（Left和Right）；所有结点存储一个关键字,非叶子结点的左指针指向小于其关键字的子树，右指针指向大于其关键字的子树。平衡二叉树(AVL树)：它是一颗空树或它的左右两个子树的高度差的绝对值不超过1。哈夫曼树：带权路径长度达到最小的二叉树，也叫做最优二叉树。树的深度和高度：深度是从上往下数；高度是从下往上数 代码实现平滑过渡到本问题的代码实现。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152#include&lt;iostream&gt;using namespace std;typedef struct Node&#123; int value; struct Node *lchild, *rchild;&#125;Tree;int GetLeftDepth(Tree* root)&#123; Tree* pNode =root-&gt;lchild ; int depth =0; while(pNode != NULL) &#123; depth ++; pNode =pNode-&gt;lchild; &#125; return depth;&#125;int GetRightDepth(Tree* root)&#123; Tree* pNode =root-&gt;rchild; int depth =0 ; while(pNode != NULL) &#123; depth ++ ; pNode =pNode-&gt;rchild ; &#125; return depth;&#125;bool IsFullBinaryTree(Tree* root)&#123; return GetLeftDepth(root) == GetRightDepth(root) ;&#125;void insert(Tree* root, Tree * node)&#123; if (IsFullBinaryTree(root) || !IsFullBinaryTree(root-&gt;lchild))&#123; insert(root-&gt;lchild, node); return ; &#125; if (root-&gt;rchild ==NULL)&#123; root-&gt;rchild =node ; return ; &#125; insert(root-&gt;rchild, node) ;&#125;int main()&#123; Node* a = new Node(); a-&gt;value =1;&#125; 第二种思路，如果已知之前树的个数，那么可以使用前序遍历的方式，得到将要插入的的节点的位置，然后插入。123456789101112int insert(Tree *t, int n, struct Node *node);# n表示原来二叉树节点的个数# 前序遍历 void printTree(Tree* root)&#123; if(root ==NULL) &#123; return ; &#125; print(root-&gt;value) printTree(root-&gt;lchild) printTree(root-&gt;rchild)&#125; 第三种思路：因为是完全二叉树，那么插入的点只能在最下一层。于是我们可以去找最下一层的中间点，找到根的左子树的最右下的节点，如果这个点存在，那么说明，最下一层的左边已经填满，递归右子树，否则递归左子树。 参考文献http://www.voidcn.com/article/p-kbxsvnyq-yq.htmlhttps://www.toutiao.com/i6192546626911126017/https://blog.csdn.net/psc0606/article/details/48742239 inplace 去除连续的 0给定一个一维整数数组，不使用额外的空间，本地去掉数组中连续的0。123456789101112131415161718192021222324252627282930#include&lt;iostream&gt;using namespace std;int RemoveDuplicates(int* sortBuffer,int length)&#123; if(sortBuffer == NULL || length == 0) &#123; return false; &#125; int count = 0; for(int i = 1; i &lt; length; i++) &#123; if(sortBuffer[i] ==0 &amp;&amp; 0 == sortBuffer[i-1]) &#123; continue; &#125; else &#123; sortBuffer[count]=sortBuffer[i]; count++; &#125; &#125; return count; &#125;int main()&#123; int length =sizeof(array)/sizeof(int); &#125; 最大连续子数组和已知一个整数二维数组，求最大的子数组和(子数组的定义从左上角(x0,y0) 到右下角(x1,y1)的数组)先考虑一维整数数组的情况。最大连续子序列的DP动态转移方程为 1234567891011121314151617181920212223242526#include&lt;iostream&gt;using namespace std;int Max(int a, int b)&#123; return a&gt;b ?a:b;&#125;int FindGreatestSubarray(int *arr, int n)&#123; int sum =arr[0]; int max =arr[0]; for(int i =1; i&lt;n; i++)&#123; sum =Max(sum+arr[i], arr[i]); max =Max(sum, max) /** if(sum &gt;=max)&#123; max =sum; &#125; */ &#125; return max;&#125;int main()&#123; return 0;&#125;讲解链接：http://kubicode.me/2015/06/23/Algorithm/Max-Sum-in-SubMatrix/ 本题目的要求是从二位的数组中求解最大的子矩阵。我们可以将其转化成一维数组的问题。如果是二维数组可以压缩为一维数组（我当时也是不懂这里）。如果最大子矩阵和原矩阵等高，就可以这样压缩。12345678910111213141516171819202122232425262728293031323334353637383940414243#include&lt;stdio.h&gt;#include&lt;iostream&gt;#include&lt;vector&gt;using namespace std;#define inf 0x3f3f3f3fint Max(int a, int b)&#123; return a&gt;b? a:b;&#125;// 求解一维数组的最大连续子数列int FindGreatestSubarray(int *arr, int n)&#123; int sum =arr[0]; int max =arr[0]; for(int i =1;i&lt;n;i++)&#123; sum =Max(sum+arr[i], arr[i]) if(sum &gt;=max)&#123; max =sum; &#125; &#125; return max;&#125;int GreatestMatrix(int[][] arr, int rows, int cols)&#123; int maxVal =- inf for(int i =0 ; i &lt;rows; i++)&#123; vector&lt;int&gt; temp(arr[i]); maxVal =Max(maxVal, FindGreatestSubarray(temp)); // 得到第一行的最大和 // 将行的n个元素加到上一行，然后计算最大和 for(int j =i+1; j&lt;rows; j++)&#123; for(int k =0;k&lt;cols ;k++)&#123; temp[k] =arr[j][k]; &#125; // 依次0~k行的最大和 maxVal =Max(maxVal, FindGreatestSubarray(temp)) &#125; &#125;&#125;int main()&#123;&#125; 聚类(clustering )聚类是一种无监督学习(Unsupervised Learning)，该算法基于数据内部的特征寻找样本的自然族群(集群)。通常使用数据可视化来评价结果。应用场景：新闻聚类，文章推荐，细分客户。 最常见的聚类算法就是K均值(K-Means)：以空间中k个点为中心进行聚类，对最靠近他们的对象归类，通过迭代的方法，逐次更新各聚类中心的值，直到得到最好的聚类结果。最后希望达到的目的：聚类中的对象相似度较高，聚类之间的相似度比较低。 相似度计算：不同的算法需要的”相似度”是不一样的，对于空间中的点，我们一般选取欧式距离来进行衡量，认为距离越近，数据之间越相似。 堆排序堆是一种完全二叉树，堆排序是一种树形选择排序，其时间复杂度为O(nlogn)，空间复杂度:对于记录较少的文件不推荐使用，对于较大的文件还是有效的.堆分为大根堆和小根堆。大根堆的要求是每个节点的值都不大于其父节点的值，即A[PARENT[i]] &gt;= A[i]。小根堆的要求是每个节点的值都不小于其父节点的值，即A[PARENT[i]] &lt;= A[i]。 stack vs heap vs queue: 中文翻译的时候有偏差，最好使用因为来进行理解。stack 就是常见的栈， we say Last in first Out (LIFO) or First in Last out (FILO); 于此相对应的是 queue, the first person in line is the first person to get out of line. This is FIFO. 这个和数据结构中的 栈和队列是一一对应的。然后说一下 heap，是算法中的一种特殊的树形结构。Heap is a tree with some special property. That special property of the heap is, the value of a node must be &gt;= or &lt;= to its children. And one most important property of heap is all leaves must be at level h or at h-1. (where h is the height of the tree). This also called heap must be a complete binary tree. 堆的每次调整交换堆顶和最后一个元素，然后只是调整堆顶和堆顶的左右孩子树的关系。 1234567891011121314151617181920212223242526272829303132333435# heap modifydef MAX_Heapify(heap, HeapSize, root): left =2* root+1 right = left +1 larger =root if left &lt;HeapSize and heap[larger] &lt;heap[left]: larger =left if right &lt; HeapSize and heap[larger] &lt;heap[right]: larger =right # if modify the larger then exchange it if larger != root: heap[larger], heap[root] =heap[root], heap[larger] MAX_Heapify(heap, HeapSize, larger)# Build the heapdef Build_MAX_Heap(heap): HeapSize =len(heap) # from the end to the begin for i in range((HeapSize -2)//2, -1,-1): MAX_Heapify(heap, HeapSize, i)# sort after building the heapdef HeapSort(heap): Build_MAX_Heap(heap) for i in range(len(heap)-1, -1, -1): heap[0], heap[i] =heap[i], heap[0] MAX_Heapify(heap, i, 0) return heapif __name__ ==&quot;__main__&quot;: a =[30, 50, 57, 77, 62, 78, 94, 80, 84] print(a) print(&quot;without sort but with build heap&quot;) Build_MAX_Heap(a) print(a) 参考文献https://blog.csdn.net/minxihou/article/details/51850001https://blog.csdn.net/chibangyuxun/article/details/53018294 KMP(字符串高效查找)假设两个字符串的长度分别是m，n(m&gt;n)，在长度为m中的字符串查找长度为n的字符串，最常见的方式是暴力求解，但是这个常规解法的时间复杂度是O(nm)。KMP通过一个O(n)的预处理，可以使得时间复杂度降为O(n+m).代码实现(视频讲解(科学上网))123456789101112131415161718192021222324def kmp_match(s, p): m, n =len(s) ,len(p) cur =0 table = partial_table(p) while cur &lt;= m-n: for i in range(n): if s[i+cur] != p[i]: cur += max(i -table[i-1], 1) break else: return True return Falsedef partial_table(p): prefix =set() postfix =set() ret =[0] for i in range(1, len(p)): prefix.add(p[:i]) postfix =&#123; p[j:i+1] for j in range(1, i+1)&#125; ret.append(len((prefix &amp; postfix or &#123;&apos;&apos;&#125;).pop())) # &amp;两个set求交集 return retprint(partial_table(&apos;ABCDABD&apos;))print(kmp_match(&quot;BBC ABCDAB ABCDABCDABDE&quot;, &quot;ABCDABD&quot;)) 参考文献https://www.cnblogs.com/fanguangdexiaoyuer/p/8270332.html 二叉树的遍历在python中二叉树的结构:12345class BinNode(): def __init__(self, val): self.value =val self.lchild =None self.rchild =None 先序遍历(preOrder)第一种思路是递归实现，第二种思路借助栈的结构来实现。栈的大小空间为O(h)，h为二叉树高度；时间复杂度为O(n)，n是树的节点的个数。123456789101112131415161718192021# 递归def preOrder(self, root): if root == None: return print(root.val) self.preOrder(root.lchild) self.preOrder(root.rchild)# 借助栈结构def preOrder(self, root): if root == None: return myStack =[] node =root while node or myStack: while node: print(node.val) myStack.append(node) node =node.lchild node =myStack.pop() node =node.rchild 中序遍历(inOrder)递归和非递归两种实现思路。入栈的顺序是一样的，只是改变的遍历(print())的顺序.123456789101112131415161718192021# 递归def inOrder(self, root): if root ==None: return self.inOrder(root.lchild) print(root.val) self.inOrder(root.rchild)# 借助栈结构def inOrder(self, root): if root ==None: return myStack =[] node =root while node or myStack: while node: myStack.append(node) node =node.lchild node = myStack.pop() print(node.val) node =node.rchild 后序遍历(post order)仍然是递归和非递归版本，非递归中使用两个stack,两个stack的后进先出等于一个先进先出。12345678910111213141516171819202122232425# 递归def postOrder(self, root): if root == None: return self.postOrder(root.lchild) self.postOrder(root.rchild) print(root.val)# 借助栈结构def postOrder(self, root): if root ==None: return myStack1 =[] myStack2 =[] node =root myStack1.append(node) while myStack1: node =myStack1.pop() if node.lchild: myStack1.append(node.lchild) if node.rchild: myStack1.append(node.rchild) myStack2.append(node) while myStack2: print(myStack2.pop().val) 层序遍历使用到了队列的思想，先进先出。实际上，用的是Python中list.pop(0).注意默认是list.pop(-1),也就是默认弹出的是最后一个元素。1234567891011121314def levelOrder(self, root): if root ==None: return myQueue =[] node =root myQueue.append(node) while myQueue: # remove and return item at index (default last) node =myQueue.pop(0) print(node.val) if node.lchild != None: myQueue.append(node.lchild) if node.rchild != None: myQueue.append(node.rchild) 参考文献https://blog.yangx.site/2016/07/22/Python-binary-tree-traverse/ 旋转数组找最小值把一个数组最开始的若干个元素搬到数组的末尾，我们称之为数组的旋转。 输入一个非递减排序的数组的一个旋转，输出旋转数组的最小元素。 例如数组{3,4,5,1,2}为{1,2,3,4,5}的一个旋转，该数组的最小值为1。 NOTE：给出的所有元素都大于0，若数组大小为0，请返回0。 暴力求解大多数的问题都是可以暴力求解的–鲁迅。(In China 凡是不知道是谁说，都可以说是鲁迅说的; In US，凡是不知道谁说的，as said by Albert Einstein)因为原来的数组假设是增序，所以如果出现了的某一个元素比上一个元素小，该元素就是这个序列中的最小值。(这个情况具有唯一性吧).时间复杂度O(N)123456789101112def minNumberInRotateArray(rotateArray): arr =rotateArray # just because of laziness if not arr: return 0 if len(arr) ==2: return arr[1] num =arr[0] for i in range(1, len(arr)): if arr[i] &gt;= num: num =arr[i] else: return arr[i] 递归版本旋转数组也是一种有序数组，时间复杂度O(N)…，面试官说改进吧…使用二分法降到O(logN)。 In big-O() notation, constant factors are removed. Converting from one logarithm base to another involves multiplying by a constant factor. 所以这关键是后面的变量而不是以2 或者e 为底。这两种写法都是成立的。 So O(log N) is equivalent to O(log2 N) due to a constant factor.12345678910111213def minNumberInRotateArray(rotateArray): arr = rotateArray if not arr: return 0 if len(arr) ==2: return arr[1] mid =int(len(arr) /2) if arr[mid] &gt; arr[0]: return minNumberInRotateArray(arr[mid:]) elif arr[mid] &lt;arr[0]: return minNumberInRotateArray(arr[:mid+1]) else: return minNumberInRotateArray(arr[1:]) 非递归版本递归版本占的内存比较多，改进吧..于是非递归的版本就出来了。需要注意的是该版本的判断比较条件(其中一点是和 arr[right]进行比较)一定要小心，都是小坑…这种解法关键是需要找到非减序列（和原序列相同的形式），然后就变得可预测，可以排除这部分其他的数字。 下面这个确实是是正确的代码，注意体会细节。非递减的array，然后是从后往前比较的。if lese 中的条件是可以调换的，虽然这话听起来像是废话。这说明两个条件的判断的顺序不应该产生不同的结果。1234567891011121314def minNumberInRotateArray(rotateArray): arr =rotateArray left =0 right =len(arr) -1 while left &lt; right: mid = int((left+ right)/2) if arr[mid] &gt;arr[right]: left =mid+1 # 不包含mid 因为mid 绝不可能是 最小值 elif arr[mid] &lt;arr[right]: right =mid # 包含mid 因为mid 可能是最小值 else: right -=1 # 这个也是可以换成 left +=1 ，只要是能够渐进的 return arr[left] 文章的小标题是求解最小(大)值，上述讲述的都是最小值。如果求解最大值，稍微修改一下特殊情况的判断条件，将返回的index-1 即可。因为最小值的位置是”某一个元素比上一个元素小”，那么 index-1 之后这个元素就是该数组序列中最大的。 参考文献https://blog.csdn.net/u010005281/article/details/79823154 单链表反转单链表的反转有循环迭代和递归两种方法。单链表节点123456class Node(object): def __init__(self): self.value =None self.next =None def __str__(self): return str(self.value) 循环迭代循环迭代需要维持三个变量：pre, head, next。pre是head的pre，next是head的next.(废话)1234567891011def reverse_Linkedlist(head): if not head or not head.next : #空指针或者只有一个结点 return head pre =None # 需要创建一个None 作为最后的指向 while head: next = head.next head.next =pre pre = head head =next # 最后一次循环迭代 Head==None，而pre指向了头结点 return pre 递归一开始正常情况下不会执行if判断，利用递归走到链表的末端，new_head的值没有发生改变，为链表的最后一个节点，反转之后就成为了新链表的head。12345678def reverse_Linkedlist(head): if not head or not head.next: return head new_head = reverse_Linkedlist(head.next) # 将当前节点设置为后面节点的后续节点 head.next.next =head head.next =None return new_head 测试12345678910111213141516171819202122232425if __name__ == &quot;__main__&quot;: three = Node() three.value =3 two =Node() two.value =2 two.next =three one =Node() one.value =1 one.next =two head =Node() head.value =0 head.next =one &quot;&quot;&quot; while head: print(head.value, ) head =head.next print(&quot;******&quot;) &quot;&quot;&quot; newhead = reverse_Linkedlist(head) while newhead: print(newhead.value) newhead =newhead.next 参考文献https://foofish.net/linklist-reverse.html]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>递归</tag>
        <tag>迭代</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Introduction_to_Ensemble]]></title>
    <url>%2F2018%2F06%2F13%2FIntroduction-to-Ensemble%2F</url>
    <content type="text"><![CDATA[虽然在Titanic Challenge博客中介绍了模型融合(Ensemble)，但是其是以特征提取为主旋律，所以这篇博客将结合实战从最基本的Ensemble到比较复杂的Ensemble,最后安利一个用于模型融合的package. 在文中可能出现一些概念，如果不太熟悉，可以先去文末找找看，如果我没有补充，那么请自行Google吧。为了不影响阅读的流畅性，我可能在其中不会涉及很多概念解释，比如说ROC曲线和AUC值。好，我们进入正文。 Dataset本文书写过程中沿用参考博客(Introduction to Python Ensembles)的数据集。可以去这里下载，当然推荐使用原作者处理之后的数据集，you can find here。 简单介绍一下这个数据集：Federal Election Commission这个组织收集了2007到2016年的donations记录，最后得出 When Scientists Donate To Politicians, It’s Usually To Democrats这样的结论。 好了，我不想在数据集这里花太多时间，即使你不太明白数据集的具体含义，完全不影响下文的阅读，因为你很快就会发现下文并没有进行很多和原数据集相关的内容，更多的是模型融合。当然你如果能够看懂，可以感受一下上述结论的有趣之处。 Give me codes:1234567891011121314151617181920import numpy as npimport pandas as pdimport matplotlib.pyplot as plt%matplotlib inline# import data# Always good to set a seed for reproducibilitySEED = 222np.random.seed(SEED)df = pd.read_csv(&apos;input.csv&apos;)from sklearn.model_selection import train_test_splitfrom sklearn.metrics import roc_auc_scoredef get_train_test(test_size= 0.95): y =1*(df.cand_pty_affiliation ==&apos;REP&apos;) X = df.drop([&quot;cand_pty_affiliation&quot;], axis=1) X = pd.get_dummies(X, sparse=True) X.drop(X.columns[X.std() == 0], axis=1, inplace=True) return train_test_split(X, y, test_size=test_size, random_state=SEED)xtrain, xtest, ytrain, ytest = get_train_test()df.head() 简单看一下数据长什么样子，虽然有人可能不太懂。 Begin with ensemble之前的博客主要从ensemble分类的角度阐述，现在从概念的角度阐述。Ensemble: combining predictions from several models averages out idiosyncratic(怪异的) errors and yield better overall predictions.(有时候我觉得英文说得很清楚，所以就不翻译成中文了，求不被打。) 简单的说，就是可以防止过拟合。当过拟合时，边界曲线就回去过分考虑某一个或某一些少数的点，这时候 ensemble通过某种combine机制，然后得到一个泛化性能比较好的边界曲线，也就是比较好的模型。 Decision Tree首先我们从 decision tree开始。A decision tree, which is a tree of if-then rules. The deeper the tree, the more complex the patterns it can capture, but the more prone to overfitting it will be. Because of this, we will need an alternative way of building complex models of decision trees, and an ensemble of different decision trees is one such way.我们先使用 depth =112345678from IPython.display import Imagefrom sklearn.metrics import roc_auc_scorefrom sklearn.tree import DecisionTreeClassifiert1 = DecisionTreeClassifier(max_depth=1, random_state=SEED)t1.fit(xtrain, ytrain)p = t1.predict_proba(xtest)[:, 1]print(&quot;Decision tree ROC-AUC score: %.3f&quot; % roc_auc_score(ytest, p)) Decision tree ROC-AUC score: 0.672 发现结果不太理想，加深depth.1234t2 =DecisionTreeClassifier(max_depth=3, random_state=SEED)t2.fit(xtrain, ytrain)p =t2.predict_proba(xtest)[:, 1]print(&quot;Decision tree ROC-AUC score: %.3f&quot; % roc_auc_score(ytest, p)) Decision tree ROC-AUC score: 0.751 由于我们最后要ensemble，而这要求我们要构造有差异但每个不是那么差的模型。首先我们考虑到使用不同的数据集。1234567xtrain_slim =xtrain.drop([&apos;transaction_amt&apos;], axis=1)xtest_slim =xtest.drop([&apos;transaction_amt&apos;], axis=1)t3=DecisionTreeClassifier(max_depth=3, random_state=SEED)t3.fit(xtrain_slim, ytrain)p =t3.predict_proba(xtest_slim)[:,1]print(&apos;Decision tree ROC-AUC score:&#123;&#125;&apos;.format(roc_auc_score(ytest, p))) Decision tree ROC-AUC score:0.7403182587884118通过corr()来检验两者的相关性(差异性)123p1 =t2.predict_proba(xtest)[:,1]p2 =t3.predict_proba(xtest_slim)[:,1]pd.DataFrame(&#123;&apos;full_data&apos;:p1, &apos;red_data&apos;:p2&#125;).corr() 发现有一定的相关性，但是还是可以容忍的。于是开始融合。1234p1 = t2.predict_proba(xtest)[:, 1]p2 = t3.predict_proba(xtest_slim)[:, 1]p = np.mean([p1, p2], axis=0)print(&quot;Average of decision tree ROC-AUC score: %.3f&quot; % roc_auc_score(ytest, p)) Average of decision tree ROC-AUC score: 0.783我们发现：两个旗鼓相当（0.74 0.73）的可以得到一个更好的结果，可以减少决策失误 的平均是0.78。 需要注意的是我们在构造第二个模型时候通过drop()丢掉一个feature，反而得到一个更好的模型。于是乎，我们想通过使用不同的子集（不同的特征）构造不同的模型，是不是能得到更好的模型？ Random Forest(Bagging)A fast approach that works well in practice is to randomly select a subset of features, fit one decision tree on each draw and average their predictions.This process is known as bootstrapped averaging (often abbreviated bagging), and when applied to decision trees, the resultant model is a Random Forest. (我觉得原作者比我说的清楚，借用了)我的理解，在上面小节中我们使用的是Decision Tree,在实际应用中发现有差异的多个子树的效果要更好一些。而实现这个途径快速的方法就是 Random Forest。Random 在这里表示任意几个子树(特征)，然后这些Tree组成了Forest。123456789from sklearn.ensemble import RandomForestClassifierrf =RandomForestClassifier( n_estimators=10, max_features= 3, random_state=SEED)rf.fit(xtrain, ytrain)p =rf.predict_proba(xtest)[:, 1]print(&apos;Average of decision tree ROC-AUC score:&#123;&#125;&apos;.format(roc_auc_score(ytest, p))) Average of decision tree ROC-AUC score:0.844018408542404这就是叫做”质的飞跃”从那个0.783-&gt; 0.844(将近6个百分点，好吧，有点神经质了…)From nobody to somebody, we are on something.. Ensemble of various models可以看出上述模型中，最后的模型（Random Forest）的子模型(Decision Tree)。但是子模型并不是局限树这一种结构，我们更多的选择：linear models, kernel-based models, non-parametric models, neural networks or even other ensembles! 为了避免代码的冗余构造了以下的helper function.123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657# A host of Scikit-learn modelsfrom sklearn.svm import SVC, LinearSVCfrom sklearn.naive_bayes import GaussianNBfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifierfrom sklearn.linear_model import LogisticRegressionfrom sklearn.neighbors import KNeighborsClassifierfrom sklearn.neural_network import MLPClassifierfrom sklearn.kernel_approximation import Nystroemfrom sklearn.kernel_approximation import RBFSamplerfrom sklearn.pipeline import make_pipelinedef get_models(): &quot;&quot;&quot;Generate a library of base learners.&quot;&quot;&quot; nb = GaussianNB() svc = SVC(C=100, probability=True) # C越大边界越复杂，会导致过拟合 knn = KNeighborsClassifier(n_neighbors=3) # KNN算法寻找训练数据中的K个最近的数据，它使用指向最多的那个类别来作为预测的输出。 lr = LogisticRegression(C=100, random_state=SEED) # 对于这个 c 能知道的就是正则化系数 smaller values specify stronger regularization. nn = MLPClassifier((80, 10), early_stopping=False, random_state=SEED) gb = GradientBoostingClassifier(n_estimators=100, random_state=SEED) # 子模型的数量，默认是100, gbc通常 robust to over-fitting, so a large number results in better performance rf = RandomForestClassifier(n_estimators=10, max_features=3, random_state=SEED) models = &#123;&apos;svm&apos;: svc, &apos;knn&apos;: knn, &apos;naive bayes&apos;: nb, &apos;mlp-nn&apos;: nn, &apos;random forest&apos;: rf, &apos;gbm&apos;: gb, &apos;logistic&apos;: lr, &#125; return models def train_predict(model_list): P =np.zeros((ytest.shape[0], len(model_list))) P =pd.DataFrame(P) print(&apos;Fitting models&apos;) cols =list() for i, (name, m) in enumerate(models.items()): print(&quot;%s...&quot; % name, end=&quot; &quot;, flush=False) m.fit(xtrain, ytrain) P.iloc[:, i] =m.predict_proba(xtest)[:, 1] cols.append(name) print(&apos;Done&apos;) P.columns =cols print(&apos;Done.\n&apos;) return P def score_models(P, y): &quot;&quot;&quot;Score model in predication DF&quot;&quot;&quot; print(&apos;Scoring models&apos;) for m in P.columns: score =roc_auc_score(y, P.loc[:, m]) print(&quot;%-26s: %.3f&quot; % (m, score)) print(&apos;Done, \n&apos;) let’s go…123models =get_models()P =train_predict(models)score_models(P, ytest) This is our base line.Gradient Boosting Machine(GBM) 果然名不虚传, does best我们来分析一下模型之间的相关性，原作者使用的mlens package(You can install it with: pip install mlens)，我这里用的是seaborn(install it with: pip install seaborn).在检查相关性(Pearson相关性:衡量两个数据集合的线性相关性)时候，我们使用的是经过处理的相关性。具体说来可以称之为 error correlation,详细见代码。 12345678# look at error correlations is more promising, errors are significantly correlated import seaborn as snsplt.subplots(figsize=(10,8)) corrmat = P.apply(lambda pred: 1*(pred &gt;= 0.5) - ytest.values).corr()sns.set(font_scale=1.5) hm = sns.heatmap(corrmat, cbar=True, annot=True, square=True, fmt=&apos;.2f&apos;, annot_kws=&#123;&apos;size&apos;: 10&#125;) plt.title(&apos;Pearson Correlation of Features&apos;, y=1.05, size=15) plt.show() 预测值和真实值之间的差异称之为error，查看error的 Pearson correlation，效果更加明显。1print(&quot;Ensemble ROC-AUC score: %.3f&quot; % roc_auc_score(ytest, P.mean(axis=1))) Ensemble ROC-AUC score: 0.884结果高于每一个单独的模型，但是不是那么明显。 Visualize Curve ROC(helper function)我们注意到之前使用的所有的结果的评价标准都是 roc_auc_score，但是并没有提及这是什么。当然在正文中也不打算解释，如果不是很清楚，可以查看本文最后补充概念.简单来说AUC可以用来衡量”二分问题”的泛化能力，是一种评价指标。我们这里想说的是 visualize Curve ROC,可视化。12345678910111213141516171819202122# a helper function for roc_curvefrom sklearn.metrics import roc_curvedef plot_roc_curve(ytest, P_base_learners, P_ensemble, labels, ens_label): &quot;&quot;&quot;Plot the roc curve for base learners and ensemble.&quot;&quot;&quot; plt.figure(figsize=(10, 8)) plt.plot([0, 1], [0, 1], &apos;k--&apos;) cm = [plt.cm.rainbow(i) for i in np.linspace(0, 1.0, P_base_learners.shape[1] + 1)] for i in range(P_base_learners.shape[1]): p = P_base_learners[:, i] fpr, tpr, _ = roc_curve(ytest, p) plt.plot(fpr, tpr, label=labels[i], c=cm[i + 1]) fpr, tpr, _ = roc_curve(ytest, P_ensemble) plt.plot(fpr, tpr, label=ens_label, c=cm[0]) plt.xlabel(&apos;False positive rate&apos;) plt.ylabel(&apos;True positive rate&apos;) plt.title(&apos;ROC curve&apos;) plt.legend(frameon=False) plt.show()plot_roc_curve(ytest, P.values, P.mean(axis=1), list(P.columns), &quot;ensemble&quot;) 我们顺便把刚才的模型可视化，发现ensemble的AUC是最大的，意味这泛化性能是最好，这也是符合我们的认知的 。 Beyond ensembles as a simple average我们回到主线上，在上一个模型中我们提及最后的 ensemble的结果是好于每个单独的模型，但是没有那么突出。根据ROC曲线我们也可以看出，有的模型(KNN)在这里表现的没有那么好，我们在想是不是由于因为这个而拉底了最后的结果，当然这只是猜测，于是我们找到了 try的方向。可能第一直觉去掉这个模型再进行融合，在这个实验表明该策略最后的结果0.883，相比与0.884，你懂得，并没有变好。我们还有一种策略:learn a sensible set of weights to use when averaging predictions.让模型自己去学习如何调整各个模型之间的比例。 Learning to combine predications为了让模型自学习各个之间的预测比例，我们引入了 meta learner(meta是元，理解为最基础的) to learn how to best combine these predictions. 除此之外，我们将使用不同的数据集，像Random Forest使用不同的数据子集(不同的特征组成的数据集)。于是我们需要a method for splitting the training data between the base learners and the meta learner.12345678910base_learners =get_models()meta_learner = GradientBoostingClassifier( n_estimators=1000, loss=&quot;exponential&quot;, max_features=4, max_depth=3, subsample=0.5, learning_rate=0.005, random_state=SEED) 使用最强模型GBM作为 meta learner并定义好 base_learners.To keep things simple, we split the full training set into a training and prediction set of the base learners. This method is sometimes referred to as Blending. Unfortunately, the terminology differs between communities, so it’s not always easy to know what type of cross-validation the ensemble is using.12345678910# sefine a procedure for generating train and test setsxtrain_base , xpred_base, ytrain_base, ypred_base =train_test_split(xtrain, ytrain, test_size=0.5, random_state=SEED)def train_base_learners(base_learners, inp, out, verbose =True): if verbose: print(&apos;Fitting models&apos;) for i, (name, m) in enumerate(base_learners.items()): if verbose: print(&quot;%s...&quot; % name, end=&quot; &quot;, flush=False) m.fit(inp, out) if verbose: print(&apos;Done.&apos;)train_base_learners(base_learners, xtrain_base, ytrain_base) (注意我们只是使用了50%的data去train, test_size =0.5)1234567891011121314def predict_base_learners(pred_base_learners, inp, verbose=True): &quot;&quot;&quot;Generate a prediction matrix.&quot;&quot;&quot; P = np.zeros((inp.shape[0], len(pred_base_learners))) if verbose: print(&quot;Generating base learner predictions.&quot;) for i, (name, m) in enumerate(pred_base_learners.items()): if verbose: print(&quot;%s...&quot; % name, end=&quot; &quot;, flush=False) p = m.predict_proba(inp) # With two classes, need only predictions for one class P[:, i] = p[:, 1] if verbose: print(&quot;done&quot;) return PP_base = predict_base_learners(base_learners, xpred_base) 现在我们得到了base_learners的predications，接下来我们应该使用的是这个流程，在base learners的基础上（类似两层结构了 meta learner 学习的如何搭配这些base learner使得最后的结果 predications最大）进行训练，而不是训练原来的数据集。1meta_learner.fit(P_base, ypred_base) 1234567#meta_learner.fit(P_base, ypred_base)def ensemble_predict(base_learners, meta_learner, inp, verbose=True): &quot;&quot;&quot;Generate predictions from the ensemble.&quot;&quot;&quot; P_pred = predict_base_learners(base_learners, inp, verbose=verbose) return P_pred, meta_learner.predict_proba(P_pred)[:, 1]P_pred, p = ensemble_predict(base_learners, meta_learner, xtest)print(&quot;\nEnsemble ROC-AUC score: %.3f&quot; % roc_auc_score(ytest, p)) 最后的结果是0.881 相比与之前最好的0.884（使用相同的数据集，没有进行 meta_learner的操作）这是因为我们在划分数据集的使用只是使用了0.5的数据集，而前者的模型使用了全部的train sets。有人不免疑问：为什么不使用全部的data？我的理解是划分xtrain_base , xpred_base, ytrain_base, ypred_base使用的是 train_test_split()，总是需要设定一个数值的，即使train_size =0.01，也是没有用到全部的datas. Training with cross-validation我们使用cross-validation 来缓解上面那个问题。During cross-validated training of the base learners, a copy of each base learner is fitted on K−1 folds, and predict the left-out fold. This process is iterated until every fold has been predicted. The more folds we specify, the less data is being left out in each training pass. This makes cross-validated predictions less noisy and a better reflection of performance during test time. The cost is significantly increased training time. Fitting an ensemble with cross-validation is often referred to as stacking, while the ensemble itself is known as the Super Learner. To understand how cross-validation works, we can think of it as an outer loop over our previous ensemble. The outer loop iterates over K distinct test folds, with the remaining data used for training. The inner loop trains the base learners and generate predictions for the held-out data. Here’s a simple stacking implementation:12345678910111213141516171819202122232425262728293031323334353637383940414243from sklearn.base import clonedef stacking(base_learners, meta_learner, X, y, generator): &quot;&quot;&quot;Simple training routine for stacking.&quot;&quot;&quot; # Train final base learners for test time print(&quot;Fitting final base learners...&quot;, end=&quot;&quot;) train_base_learners(base_learners, X, y, verbose=False) print(&quot;done&quot;) # Generate predictions for training meta learners # Outer loop: print(&quot;Generating cross-validated predictions...&quot;) cv_preds, cv_y = [], [] for i, (train_idx, test_idx) in enumerate(generator.split(X)): fold_xtrain, fold_ytrain = X[train_idx, :], y[train_idx] fold_xtest, fold_ytest = X[test_idx, :], y[test_idx] # Inner loop: step 4 and 5 fold_base_learners = &#123;name: clone(model) for name, model in base_learners.items()&#125; train_base_learners( fold_base_learners, fold_xtrain, fold_ytrain, verbose=False) fold_P_base = predict_base_learners( fold_base_learners, fold_xtest, verbose=False) cv_preds.append(fold_P_base) cv_y.append(fold_ytest) print(&quot;Fold %i done&quot; % (i + 1)) print(&quot;CV-predictions done&quot;) # Be careful to get rows in the right order cv_preds = np.vstack(cv_preds) cv_y = np.hstack(cv_y) # Train meta learner print(&quot;Fitting meta learner...&quot;, end=&quot;&quot;) meta_learner.fit(cv_preds, cv_y) print(&quot;done&quot;) return base_learners, meta_learner 尤其在cv_preds和cv_y的维度问题上，注意小心。The basic difference between blending and stacking is therefore that stacking allows both base learners and the meta learner to train on the full data set. Using 2-fold cross-validation, we can measure the difference this makes in our case:1234567from sklearn.model_selection import KFold# Train with stackingcv_base_learners, cv_meta_learner = stacking( get_models(), clone(meta_learner), xtrain.values, ytrain.values, KFold(2))P_pred, p = ensemble_predict(cv_base_learners, cv_meta_learner, xtest, verbose=False)print(&quot;\nEnsemble ROC-AUC score: %.3f&quot; % roc_auc_score(ytest, p)) Ensemble ROC-AUC score: 0.889这是目前为止最好的结果了。Stacking yields a sizeable increase in performance: in fact, it gives us our best score so far. This outcome is typical for small and medium-sized data sets, where the effect of blending can be severe. As the data set size increases, blending and stacking performs similarly. Use packages快要接近尾声了，在文章的开始，我们提及要安利一个resemble好用的package. So, it’s now.Here, we use Ensemble and build our previous generalized ensemble, but now using 10-fold cross-validation:123456789101112131415161718192021from mlens.ensemble import SuperLearner# Instantiate the ensemble with 10 foldssl = SuperLearner( folds=10, random_state=SEED, verbose=2, backend=&quot;multiprocessing&quot;)# Add the base learners and the meta learnersl.add(list(base_learners.values()), proba=True) sl.add_meta(meta_learner, proba=True)# Train the ensemblesl.fit(xtrain, ytrain)# Predict the test setp_sl = sl.predict_proba(xtest)print(&quot;\nSuper Learner ROC-AUC score: %.3f&quot; % roc_auc_score(ytest, p_sl[:, 1])) So simple!1plot_roc_curve(ytest, p.reshape(-1, 1), P.mean(axis=1), [&quot;Simple average&quot;], &quot;Super Learner&quot;) 发现super learner(meta learner)和 basic learner的mean的结果已经不相上下了。super learner得到了很好的训练。 补充概念 ROC曲线和AUC值ROC全称是“受试者工作特征”（Receiver Operating Characteristic）。ROC曲线的面积就是AUC（Area Under the Curve）。AUC用于衡量“二分类问题”机器学习算法性能（泛化能力）。说到这里不得不提及就是经常使用的符号，TP(True Positive), FP(False, Positive), TN(True Negative),FN(False Negative)。他们是根据原来真实数据和预测类别进行的排列组合（当然这是针对二分问题）。好丑的图片啊…接着我们计算TRP(True Positive Radio)，FRP(False Positive Ratio)用于描述ROC曲线，分别表示该曲线的Y轴，X轴。TPR=TP/(TP+FN)FPR=FP/(FP+TN)最后就形成了类似这样的图像(来源于上述的训练模型)我们希望的结果是TRU越大（接近1），FRU越小（接近0）。AUC的值是ROC所覆盖的面积，当AUC越大时候，分类器的效果越好。从图中可以看出模型(ensemble)的面积是最大的，分类效果也是最好的。关于该图像还有一点，如果你的曲线拟合对角线（图中虚线），那么相当于随机猜测。在机器学习中关于这方面经常涉及到的还有precision(查准率), recall(查全率)两个概念，下面是计算公式。其中n= TP+FP总结：图中的x，y轴的计算和 precision, recall不是一个概念，虽然 recall 和y 轴在计算上是相同的。precision 是针对于预测数据(predication结果)来说的，而x轴，y轴(recall)的计算某种意义上是针对原来真实数据而言的。所以我们在训练模型过程可以追求 precision 和 recall的双高(即图像的左上角)。这时候引入了F1-measure(F1 =(2PR)/(R+P)).(P: precision, R: recall) majority/ soft/ hard votingan ensemble that averages classifier predictions is known as a majority voting classifier. When an ensembles averages based on probabilities (as above), we refer to it as soft voting, averaging final class label predictions is known as hard voting. Pearson相关性协方差除以各自的标准差 GBC参数这些参数中，类似于Adaboost，我们把重要参数分为两类，第一类是Boosting框架的重要参数，第二类是弱学习器即CART回归树的重要参数。n_estimators: 也就是弱学习器的最大迭代次数，或者说最大的弱学习器的个数。learning_rate: 即每个弱学习器的权重缩减系数ν，也称作步长对于分类模型，有对数似然损失函数”deviance”和指数损失函数”exponential”两者输入选择。默认是对数似然损失函数”deviance”。一般来说，推荐使用默认的”deviance”。它对二元分离和多元分类各自都有比较好的优化。而指数损失函数等于把我们带到了Adaboost算法。对于回归模型，有均方差”ls”, 绝对损失”lad”, Huber损失”huber”和分位数损失“quantile”。默认是均方差”ls”。一般来说，如果数据的噪音点不多，用默认的均方差”ls”比较好。如果是噪音点较多，则推荐用抗噪音的损失函数”huber”。而如果我们需要对训练集进行分段预测的时候，则采用“quantile”。max_features:可以使用很多种类型的值，默认是”None”,意味着划分时考虑所有的特征数.subsample: 选择小于1的比例可以减少方差，即防止过拟合，但是会增加样本拟合的偏差，因此取值不能太低。推荐在[0.5, 0.8]之间，默认是1.0，即不使用子采样。 参考文献GBC参数设置ROC曲线和AUC值Introduction to Python Ensembles]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>模型融合(Ensemble)</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[对抗生成网络实验对比]]></title>
    <url>%2F2018%2F06%2F05%2F%E5%AF%B9%E6%8A%97%E6%80%A7%E7%94%9F%E6%88%90%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[GAN模型是典型的隐式无监督生成模型，建模过程中没有利用到数据的语义标签。但在实际应用中生成模型的可控性至关重要，根据标签生成对可控有样本更具实际应用价值。图像生成模型被广泛应用于数据增强、风格转换和数据补全领域，需要可控且语义完备的生成模型。条件GAN模型在GAN模型建模思路的基础上，将语义标签加入了建模过程，将无监督生成模型转变为有监督的条件生成模型。条件GAN模型具体包括Conditional GAN模型、Semi-GAN模型和AC-GAN模型。 实验模型介绍（1）Conditional GAN模型GAN模型中判别器D对输入的数据样本的来源进行判别，是典型的判别模型流程。如果在GAN框架中加入有监督信息来辅助训练，如图像的类别信息来辅助判别器D进行判别，则会帮助生成更加真实的样本。其中最初的尝试方式是Conditional GAN模型，结构如图所示。Conditional GAN模型在生成器G和判别器D输入中都加入了标签信息，试图让生成器G学习到从数据标签y到样本x的映射，让判别器D学习对样本x和标签y的组合进行判别。与GAN模型相比，Conditional GAN模型增加了标签信息的输入将模型转变为条件生成模型，在一定程度上提高了模型的稳定性。Conditional GAN模型训练过程中，判别器D对样本x和标签y的类别组合进行训练，并没有输入样本x和标签y的类别错误组合进行训练，因此模型并没有学习样本x和标签y的联合分布。（2）Semi-GAN模型Conditional GAN模型利用了标签信息进行建模，但没有对标签语义的信息进行表征，导致模型能够学到的信息有限。在生成模型过程中，如果判别器D能够明确指出生成样本的类别错误，则可为生成器G提供更加精确的梯度信息，最终能生成更加真实的样本。Semi-GAN基于此思路进行改进，具体结构如图9所示。Semi-GAN模型在Conditional GAN模型的基础上，对判别器D的分类输出进行细化加入了半监督学习过程。（3）AC-GAN模型与Conditional GAN模型相比，Semi-GAN模型中判别器D能够判别真实样本的来源，增强了判别器D的判别能力。但研究表明过强的判别信息会影响生产样本的质量，具体原因为Semi-GAN模型的建模过程为半监督分类过程，目标优化函数为无监督分类和有监督分类目标函数之和。若判别器D的监督分类信息过强，则会削弱判别器D对样本来源的判别能力。Conditional GAN模型能够生成指定类别的样本，Semi-GAN模型能够判别样本的类别信息。AC-GAN模型将以上两个模型思路进行整合，得到够进行条件生成的生成器G，和能够判别样本类别和来源信息的判别器D。AC-GAN模型的结构如图10所示。AC-GAN模型在Conditional GAN的基础上，让判别器D在判别样本来源的同时，让样本进行分类。此时的判别器D的输出分为样本来源信息LS和样本分类LC信息。 不同模型比较下面用表格的方式对比在实验中使用的模型的目标函数(ps,图画比较丑，之后再修改) Name Paper Link Value Function GAN Arxiv DCGAN Arxiv Semi-GAN Arxiv 和GAN 模型相同 CGAN Arxiv ACGAN Arxiv our model Arxiv 数据集介绍在常用于图像生成的图像数据集中，大部分数据的标签类型为离散类型。其中MNIST和Fashion-MNIST为常用的灰度图像数据，每类样本分布较为独立，常用于进行图像分类和样本生成的实验；SVHN和CIFAR10为彩色数据集图像像素分布较为复杂，其中CIFAR10常用来检验分类网络性能的评价数据集；CelebA为大规模的人脸识别和属性分类数据集，每幅人脸图像包括40个属性标签；ImageNet为图像分类和识别数据集，数据集类别分布比较复杂具体包括自然图像和人为图像。UnityEyes为人眼视觉合成数据集，数据集标签包括瞳孔标签和视觉方向标签，其中视觉方向标签为连续的语言标签。常见的离散标签图像数据集的样例: 我们的模型在原始GAN模型中，目标函数定义为生成器G和判别器D的博弈过程，定义V(G;D)为模型的目标函数，由生成器G和判别器D组成。语义匹配目标函数FMloss。基于语义匹配的条件生成网络模型的生成器G和判别器D的目标函数分别为：如上式，LS为样本来源，LC为分类结果。判别器D目标为最大化LC+ LS + FMloss，其试图对输入样本进行分类，并通过来源和语义匹配区分生成样本和原始样本。生成器G的目的是最大化LC − LS −FMloss，其试图通过样本分类结果、样本来源和语义匹配结果来欺骗判别器D。LS来源损失与原始GAN模型相同，LC为语义标签分类损失，在类别分类中使用交叉信息熵，在数值回归中则使用均方差回归。 生成结果对比MNIST数据集生成结果Fashion-MNIST数据集生成结果SVHN数据集生成结果CIFAR10数据集生成结果CelebA数据集生成结果UnityEyes数据集生成结果]]></content>
      <categories>
        <category>Deep learning</category>
      </categories>
      <tags>
        <tag>GANs</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Titanic Challenge]]></title>
    <url>%2F2018%2F06%2F05%2FTitanic-Challenge%2F</url>
    <content type="text"><![CDATA[用此博客记录自己解决 Kaggle Titanic challenge过程中的个人总结。 问题描述说道Titanic(泰坦尼克号)，最熟悉莫过于Titanic(1997 film),这部由著名导演詹姆斯·卡梅隆执导，莱昂纳多·迪卡普里奥、凯特·温斯莱特领衔主演的电影，经久不衰…但是我们今天的画风不是这样的… 我们今天解决的问题是以该事件问背景，但是没有那么浪漫。关于这个案例的介绍网上有很多内容，为了避免累赘，在这里就不进行详述。总的要求：预测在这个事件中乘客是否死亡。附上对于train sets和 test sets中数据的介绍。更多详细的内容参看：https://www.kaggle.com/c/titanic 数据分析顺滑过渡到第二阶段，数据分析，对于竞赛而言，我感觉对于数据的认识的重要性完全不亚于模型的重要性。之后我们将再次提到这句话。我将结合代码进行数据分析。 pandas 原生数据分析函数1234import pandas as pdtrain =pd.read_csv(&apos;data/train.csv&apos;)test =pd.read_csv(&apos;data/test.csv&apos;)train.describe(include=&apos;all&apos;) 除了上面 train.describe()，下面这两个也是比较常用的12train.head()train.columns 因为总体的数据量比较少，所以我们选择把train set 和test set连接起来进行数据分析和处理。1combined2 = pd.concat([train_data, test_data], axis=0) 数据质量分析 缺省值对于缺省值，常用的手段就是填充，但是针对不同的数据有不同的填充手段，有的是均值填充，有的是默认值填充还有的是根据现有数据训练一个 regression进行拟合(这种情况出现在缺省的数据比较重要，对于结果的预测有比较强的相关性的时候)。1combined2.Embarked.fillna(&apos;S&apos;, inplace=True) Embardked(上船港口)不是那么能表现出和结果(survival)相关的变量，我么可以直接采用某个默认值进行填充。1combined2.Fare.fillna(np.median(combined2.Fare[combined2.Fare.notnull()]), inplace=True) Fare(船票)我们选择使用均值填充 12345678classers = [&apos;Fare&apos;,&apos;Parch&apos;,&apos;Pclass&apos;,&apos;SibSp&apos;,&apos;TitleCat&apos;,&apos;CabinCat&apos;,&apos;Sex_female&apos;,&apos;Sex_male&apos;, &apos;EmbarkedCat&apos;, &apos;FamilySize&apos;, &apos;NameLength&apos;, &apos;FamilyId&apos;]age_et = ExtraTreesRegressor(n_estimators=200)X_train = full_data.loc[full_data.Age.notnull(),classers]Y_train = full_data.loc[full_data.Age.notnull(),[&apos;Age&apos;]]X_test = full_data.loc[full_data.Age.isnull(),classers]age_et.fit(X_train,np.ravel(Y_train))age_preds = age_et.predict(X_test)full_data.loc[full_data.Age.isnull(),[&apos;Age&apos;]] = age_preds 因为在特征提取看来 age 是一个比较重要的属性（下文中使用age来进一步计算性别特征，而性别特征对于survival 是重要的因素），所以需要通过 fit来进行填充 null 值。 异常值异常值的检测12combined2.boxplot()plt.ylim(0, 1000) 异常值处理大多数情况下我们都采取忽视，但是有时候异常值中却跟结果有比较强的相关性，比如说该题目分数在0.9的一位大神在博客中使用的特征包含名字长度。这个在我一开始的特征提取中确实没有太在意名字长度也可以当作一种和结果(servival or dead)相关的特征。 重复值重复值的检测1train[train.duplicated()==True] 如果运行结果为空，那么就是没有重复值,如果有重复值，一般使用下面类似的代码都是可以去除掉的。1df.drop_duplicates() 分布特征分析1234567#分布分析fig ,ax =plt.subplots(2,2, figsize=(8,6))sns.countplot(&apos;Embarked&apos;, data =train, ax =ax[0,0])sns.countplot(&apos;Pclass&apos;, data =train, ax =ax[0,1])sns.violinplot(&apos;Survived&apos;, &apos;Age&apos;, data= train, ax =ax[1,0]).set(ylim =(-10, 80))sns.countplot(x =&apos;Survived&apos;, data= train, ax =ax[1,1])plt.tight_layout() 运行需要导入 seaborn1import seaborn as sns 这里安利一个数据可视化工具-seaborn。回正题 counplot()可以直观的看出单个数据的特征，但是我们更加关心的是数据和数据之间的关系，更准确的是数据和预测数据(survival )之间的关系。所以,我们进行相关性分析。123456plt.subplots(figsize=(10,8)) corrmat = train[train.columns[1:]].corr()sns.set(font_scale=1.5) hm = sns.heatmap(corrmat, cbar=True, annot=True, square=True, fmt=&apos;.2f&apos;, annot_kws=&#123;&apos;size&apos;: 10&#125;) plt.title(&apos;Pearson Correlation of Features&apos;, y=1.05, size=15) plt.show() 从图中可以看出，在原始数据集特征中(为什么这么说，嗯，这意味着我们下文还要进行 generate new features),Fare特征是和 survival最相关的。这从数据角度这你船票价钱越高，你生存的几率就越大(三观尽毁)。嗯，这是符合经济社会的运行规律的。我们在分析 Pearson Correlation的时候，关注的是数值的绝对值，如果是正值，表示正相关；如果是负值，表示负相关。 如果细心的小伙伴发现，这个并没有把所有的变量的相关性表示出来，是的，下文我将给出一个加强版的。 特征工程当我们对数据有了一个初步的认识，这时候就可以进行特征工程了。网上流传很广的一句话”数据特征决定了机器学习的上限，而算法优化只是尽可能逼近这个上限”，我深有体会。因为之前进行特征提取，然后在kaggle的submission score是0.73205,经过模型融合然后达到了0.78947，提高了5个百分点。当自己在思考数据特征重新进行特征提取的时候，最后的score 是0.82296.这都是以10个百分点的提高啊。所以这句话很有道理，我试图找到这句话的出处，以表示我对于版权的尊重，但是科学上网能力有限，没有找见，也许这句话来自群众的智慧吧。图：我在kaggle 的submission 和相应的score 但是我想强调的是特征提取是个很难有模板化的东西，这得看个人对于这个问题的理解和对于数据的理解，对于数据异常值的处理。并且还想说的是这个一个迭代的过程，不是一步到位的。当初步构造好自己特征之后可以使用图形化工具进行简单的分析一下。(下图是我第一次构造的特征工程的图形化)12345678910111213141516171819def correlation_heatmap(df): _ , ax = plt.subplots(figsize =(14, 12)) colormap = sns.diverging_palette(220, 10, as_cmap = True) _ = sns.heatmap( df.corr(), cmap = colormap, square=True, cbar_kws=&#123;&apos;shrink&apos;:.9 &#125;, ax=ax, annot=True, linewidths=0.1,vmax=1.0, linecolor=&apos;white&apos;, annot_kws=&#123;&apos;fontsize&apos;:12 &#125; ) plt.title(&apos;Pearson Correlation of Features&apos;, y=1.05, size=15) train2 =train1.drop([&apos;NullCabin&apos;], axis =1)correlation_heatmap(train2) 上图是第一次特征分析的结果(kaggle score: 0.73205),survival和Pclass和Fare是有较强的正相关性。 该图是第二次特征分析分析结果（kaggle score:0.82296,survival与male_adult(-0.56)、sex_male(-0.54)负相关,和female_adult(0.54)、sex_female(0.52)正相关。你的survival的概率和你的性别和年龄有关，如果你是成年女子，那么你很大的概率不会死亡(像Rose那样)；如果你是成年男子，那么你有很大概率体现英伦的绅士风度，主动(Jack那样)或者被选择死亡。瞬间想起了Titanic电影中Jack和Rose 的场景，好感人啊!!! 模型训练发现写了这么久，还没有开始训练模型。加快脚步…下面的内容以第一次训练模型为例。在建立基本模型之前我们需要先引入评价函数，以评价不同模型性能的好坏。 通过均值和方差来评价模型性能的优劣1234from sklearn import cross_validation def rmsl(clf): s = cross_validation.cross_val_score(clf, X_train, y_train, cv=5) return (s.mean(),s.std()) 建立基本模型1234567891011121314151617181920212223242526272829303132333435363738394041424344from sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis,gaussian_processNLA =[ #ensemble methods ensemble.AdaBoostClassifier(), ensemble.BaggingRegressor(), ensemble.GradientBoostingClassifier(), ensemble.RandomForestClassifier(n_estimators=60), # Gaussian process gaussian_process.GaussianProcessClassifier(), # LM linear_model.LogisticRegression(C=1.0, penalty=&apos;l1&apos;), #Navies Bayes naive_bayes.GaussianNB(), # Nearest Neighbor neighbors.KNeighborsClassifier(n_neighbors=3), # Svm svm.SVC(probability=True), svm.LinearSVC(), #Tree tree.DecisionTreeClassifier(), tree.ExtraTreeClassifier() ]#create table to compare MLAMLA_columns = [&apos;MLA Name&apos;, &apos;MLA Parameters&apos;,&apos;MLA Train Accuracy Mean&apos;, &apos;MLA Test Accuracy Mean&apos;, &apos;MLA Test Accuracy Min&apos; ,&apos;MLA Time&apos;] MLA_compare = pd.DataFrame(columns = MLA_columns) row_index = 0 for alg in MLA: #set name and parameters MLA_compare.loc[row_index, &apos;MLA Name&apos;] = alg.__class__.__name__ MLA_compare.loc[row_index, &apos;MLA Parameters&apos;] = str(alg.get_params()) #score model with cross validation: cv_results = model_selection.cross_validate(alg, X_train, y_train, cv =5,return_train_score=True) MLA_compare.loc[row_index, &apos;MLA Time&apos;] = cv_results[&apos;fit_time&apos;].mean() MLA_compare.loc[row_index, &apos;MLA Train Accuracy Mean&apos;] = cv_results[&apos;train_score&apos;].mean() MLA_compare.loc[row_index, &apos;MLA Test Accuracy Mean&apos;] = cv_results[&apos;test_score&apos;].mean() MLA_compare.loc[row_index, &apos;MLA Test Accuracy Min&apos;] = cv_results[&apos;test_score&apos;].min() #let&apos;s know the worst that can happen! row_index+=1MLA_compare.sort_values(by = [&apos;MLA Test Accuracy Mean&apos;], ascending = False, inplace = True) 当我们发现某个模型效果比较好的时候，我们仍然可以进一步调参。但是这种调参并不是每次会得到better result,有时候只是一个decent result。调参是个技术活。以上图中的 DecisionTreeClassifier为例进行调参。12345678param_grid = &#123;&apos;criterion&apos;: [&apos;gini&apos;, &apos;entropy&apos;], &apos;splitter&apos;: [&apos;best&apos;, &apos;random&apos;], &apos;max_depth&apos;: [None, 2,4,6,8,10], &apos;min_samples_split&apos;: [5,10,15,20,25], &apos;max_features&apos;: [None, &apos;auto&apos;, &apos;sqrt&apos;, &apos;log2&apos;] &#125;tune_model = model_selection.GridSearchCV(tree.DecisionTreeClassifier(), param_grid=param_grid, scoring = &apos;accuracy&apos;, cv = 5) cv_results = model_selection.cross_validate(tune_model, X_train, y_train, cv = 5) 使用的是sklearn 中的model_selection 模块，进行GridSearch，其实只是把调参过程自动化程序化。得到的结果是1230.863068608315 #train mean0.79803322024 # test mean0.77094972067 #test min 我们通过比对发现这个结果和上图的结果是稍微变差的。可视化显示各个算法的效率：1234sns.barplot(x=&apos;MLA Test Accuracy Mean&apos;, y = &apos;MLA Name&apos;, data = MLA_compare, color = &apos;m&apos;) plt.title(&apos;Machine Learning Algorithm Accuracy Score \n&apos;) plt.xlabel(&apos;Accuracy Score (%)&apos;) plt.ylabel(&apos;Algorithm&apos;) 对比之后我们选取几个效果比较“好”的模型，然后进行下一步的模型融合。12345678910111213141516171819MLA_best = [ #Ensemble Methods ensemble.AdaBoostClassifier(), # 0.76076 ensemble.BaggingClassifier(), # 0.72248 ensemble.GradientBoostingClassifier(), # 0.73684 ensemble.RandomForestClassifier(n_estimators = 60), # 0.72727 #GLM linear_model.LogisticRegression(C=1.0, penalty=&apos;l1&apos;, tol=1e-6), # 0.77990 linear_model.RidgeClassifierCV(), # 0.77033 linear_model.LogisticRegressionCV() #0.77033 ] row_index = 0 for alg in MLA_best: algname = alg.__class__.__name__ alg.fit(X_train, y_train) predictions = alg.predict(X_test) result = pd.DataFrame(&#123;&apos;PassengerId&apos;:test[&apos;PassengerId&apos;].as_matrix(), &apos;Survived&apos;:predictions.astype(np.int32)&#125;) result.to_csv(algname+&quot;.csv&quot;, index=False) # save the results row_index+=1 模型融合简单的说模型融合就是通过多个decent模型的结果通过某种方式的结合，产生了比原来单个模型better的结果。关于模型融合的详细内容，请移步另一篇文章模型融合(Ensemble learning)我们这里以stacking(二层)为例说明模型融合。1234567891011121314151617181920212223ntrain = train.shape[0] #891 ntest = test.shape[0] #418 SEED = 0 # for reproducibility NFOLDS = 5 # set folds for out-of-fold prediction kf =model_selection.KFold(n_splits=NFOLDS, random_state=SEED)# 封装算法基本操作 class SklearnHelper(object): def __init__(self, clf, seed=0, params=None): params[&apos;random_state&apos;] = seed self.clf = clf(**params) def train(self, x_train, y_train): self.clf.fit(x_train, y_train) def predict(self, x): return self.clf.predict(x) def fit(self,x,y): return self.clf.fit(x,y) def feature_importances(self,x,y): print(self.clf.fit(x,y).feature_importances_) return self.clf.fit(x,y).feature_importances_ 下面是定义五折交叉验证的方法，默认是三折。123456789101112131415161718def get_oof(clf, x_train, y_train, x_test): oof_train = np.zeros((ntrain,)) oof_test = np.zeros((ntest,)) oof_test_skf = np.empty((NFOLDS, ntest)) for i, (train_index, test_index) in enumerate(kf.split(x_train)): x_tr = x_train[train_index] y_tr = y_train[train_index] x_te = x_train[test_index] clf.train(x_tr, y_tr) oof_train[test_index] = clf.predict(x_te) oof_test_skf[i, :] = clf.predict(x_test) oof_test[:] = oof_test_skf.mean(axis=0) return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1) # 想让z变成只有一列，行数不知道多少 1234567891011121314151617181920212223242526272829303132from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier,AdaBoostClassifier, GradientBoostingClassifier# 定义四个不同的弱分类器的参数值 # Random Forest parameters rf_params = &#123; &apos;n_jobs&apos;: -1,&apos;n_estimators&apos;: 500,&apos;warm_start&apos;: True, &apos;max_depth&apos;: 6,&apos;min_samples_leaf&apos;: 2, &apos;max_features&apos; : &apos;sqrt&apos;,&apos;verbose&apos;: 0#&apos;max_features&apos;: 0.2, &#125; # Extra Trees Parameters et_params = &#123; &apos;n_jobs&apos;: -1,&apos;n_estimators&apos;:500,&apos;max_depth&apos;: 8,&apos;min_samples_leaf&apos;: 2,&apos;verbose&apos;: 0 #&apos;max_features&apos;: 0.5, &#125; # AdaBoost parameters ada_params = &#123; &apos;n_estimators&apos;: 500,&apos;learning_rate&apos; : 0.75 &#125; # Gradient Boosting parameters gb_params = &#123; &apos;n_estimators&apos;: 500,&apos;max_depth&apos;: 5,&apos;min_samples_leaf&apos;: 2, &apos;verbose&apos;: 0 #&apos;max_features&apos;: 0.2, &#125; # Support Vector Classifier parameters # svc_params = &#123; # &apos;kernel&apos; : &apos;linear&apos;,&apos;C&apos; : 0.025 # &#125; # 创建四个若分类器模型 rf = SklearnHelper(clf=RandomForestClassifier, seed=SEED, params=rf_params) et = SklearnHelper(clf=ExtraTreesClassifier, seed=SEED, params=et_params) ada = SklearnHelper(clf=AdaBoostClassifier, seed=SEED, params=ada_params) gb = SklearnHelper(clf=GradientBoostingClassifier, seed=SEED, params=gb_params) 1234567891011121314151617181920#X_train =X_train.values#X_test =X_test.values# 使用五折交叉方法分别计算出使用不同算法的预测结果，这些结果将用于Stacking的第二层预测 et_oof_train, et_oof_test = get_oof(et, X_train, y_train, X_test) # Extra Trees rf_oof_train, rf_oof_test = get_oof(rf,X_train, y_train, X_test) # Random Forest ada_oof_train, ada_oof_test = get_oof(ada, X_train, y_train, X_test) # AdaBoost gb_oof_train, gb_oof_test = get_oof(gb,X_train, y_train, X_test) # Gradient Boost rf_feature = rf.feature_importances(X_train,y_train) et_feature = et.feature_importances(X_train, y_train) ada_feature = ada.feature_importances(X_train, y_train) gb_feature = gb.feature_importances(X_train,y_train) feature_dataframe = pd.DataFrame( &#123;&apos;features&apos;: cols, &apos;Random Forest feature importances&apos;: rf_feature, &apos;Extra Trees feature importances&apos;: et_feature, &apos;AdaBoost feature importances&apos;: ada_feature, &apos;Gradient Boost feature importances&apos;: gb_feature &#125;) 接下来以第一层为为基础训练第二层12345678base_predictions_train = pd.DataFrame( &#123; &apos;RandomForest&apos;: rf_oof_train.ravel(),# # ravel函数在降维时默认是行序优先 &apos;ExtraTrees&apos;: et_oof_train.ravel(), &apos;AdaBoost&apos;: ada_oof_train.ravel(), &apos;GradientBoost&apos;: gb_oof_train.ravel() &#125;) X_train2 = np.concatenate(( et_oof_train, rf_oof_train, ada_oof_train, gb_oof_train), axis=1) X_test2 = np.concatenate(( et_oof_test, rf_oof_test, ada_oof_test, gb_oof_test), axis=1) 使用XGBoost训练第二层的数据。关于XGBoost为什么是有效的和相关的概念，请移步XGBoost123456789101112131415# XGboost import xgboost as xgbgbm = xgb.XGBClassifier( #learning_rate = 0.02, n_estimators= 2000, max_depth= 4, min_child_weight= 2, #gamma=1, gamma=0.9, subsample=0.8, colsample_bytree=0.8, objective= &apos;binary:logistic&apos;, nthread= -1, scale_pos_weight=1).fit(X_train2, y_train) predictions = gbm.predict(X_test2) 最后产生结果文件 StackingSubmission.csv12StackingSubmission = pd.DataFrame(&#123;&apos;PassengerId&apos;:test.PassengerId, &apos;Survived&apos;: predictions &#125;) StackingSubmission.to_csv(&quot;StackingSubmission.csv&quot;, index=False) # 0.78947 参考文献本文在效果可视化中借鉴该博客特征提取参看kaggle多位大神，在这里就谢过…]]></content>
      <categories>
        <category>竞赛</category>
      </categories>
      <tags>
        <tag>Kaggle</tag>
        <tag>Titanic</tag>
        <tag>泰坦尼克</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[模型融合]]></title>
    <url>%2F2018%2F06%2F05%2F%E6%A8%A1%E5%9E%8B%E8%9E%8D%E5%90%88%2F</url>
    <content type="text"><![CDATA[多个有差异性的模型融合可以提高整体的性能。它能同时降低最终模型的bias 和variance，从而在提高竞赛分数的同时降低overfitting 的风险。 从结果文件中融合这种做法不需要重新训练模型，融合竞赛提交的结果文件就可以，简单便捷。 Voting投票制：少数服从多数。如一个分类问题，多个模型的投票（当然可以设置权重，若没有就是平均投票），最终投票数最多的类就是被预测的类。对于加权表决融合，性能表现较差的模型（权值比较低）只能通过和其他模型保持一致增强自己的说服力。对于结果取平均融合，在不同的评估准则上也能获得不错的效果在于：取均值常常能减少过拟合的现象。如图所示：如果单个模型过拟合产生了绿色的边缘，这时候去平均这种策略使得决策边界变成黑色的边缘，这样的效果更好。机器学习的目的并不是让模型记住训练数据，而是具有更好的泛化性。 RankingRank的思想其实和Averaging一致，但Rank是把排名做平均，对于AUC指标比较有效。 训练模型融合 Bagging:使用训练数据的不同随机子集来训练每个 Base Model，最后进行每个 Base Model 权重相同的 Vote。也即 Random Forest 的原理。 Boosting:Boosting是一种迭代的方法，每一次训练会更关心上一次被分错的样本，比如改变被错分的样本的权重的Adaboost方法。也即 Gradient Boosting，Adaboost 的原理。比 Bagging 效果好，但更容易 Overfit。 Blending用不相交的数据训练不同的 Base Model，将它们的输出取（加权）平均。实现简单，但对训练数据利用少了。 Stackinig(以二层为例)在网上为数不多的关于stacking的内容中，相信你已经看过这张图片：PS:这不是原图，是在原图的基础上经过修改（把最上面标题的model 1，2,3,4,5 修改为model 1,1,1,1,1。因为这个一个model 的不同阶段，不是多个模型） 想比较而言我更加喜欢下面这张图片，因为它把stacking 的不同阶段表达的更加清楚，尤其是经过model 1之后，model 2是在model 1的基础上进行训练的。 对于第二阶段使用的样本集合，上图使用的是第一阶段的结果数据集，当然还有一种方式，如下图所示。 在该图中上一阶段的结果(prob 1-N)列和原始数据集组成新的特征向量，训练第二阶段模型。 名词解释 cross validation交叉验证当评估不同的参数设置，对算法表现的影响时，仍然存在则过拟合的风险。因为在调整参数，优化测试集的算法表现时，测试集的信息已经泄漏进模型中了。为了解决这个问题，需要一部分数据作为验证集(validation set)。 这样，用训练集(Train set)的数据训练模型；用验证集对模型参数调剂，如上述程序中的C值；最后，算法的评价在测试集(Test set)上完成。但是当数据有原来的两份化成三份之后，降低了寻数据量；另外算法的表现依赖于三个数据集的划分。解决上述两个问题的常见方法: cross validation. 测试集仍然单独划分出来，但是 validation set不用单独划分。将训练集划分为k个小的数据集，称之为k-fold CV。对每个fold进行下列过程： 用其他k-1 folds 作为training sets，训练模型 模型的结果用剩下的一个 fold进行评价模型的性能用上述循环中的 k-fold 交叉验证集的平均值表现，这样的做法增加了计算量，但是提高数据的利用效率。 参考文献https://blog.csdn.net/u013395516/article/details/79745063https://blog.csdn.net/u012969412/article/details/76636336https://blog.csdn.net/u012604810/article/details/77579782]]></content>
      <categories>
        <category>竞赛</category>
      </categories>
      <tags>
        <tag>Kaggle</tag>
        <tag>优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[XGBoost]]></title>
    <url>%2F2018%2F06%2F05%2FXGBoost%2F</url>
    <content type="text"><![CDATA[Introduction to XGBoostXGBoost is short for “Extreme Gradient Boosting”, where the term “Gradient Boosting” is proposed in the paper Greedy Function Approximation: A Gradient Boosting Machine, by Friedman. XGBoost is based on this original model. And most of the content is based on the websit(http://xgboost.readthedocs.io/en/latest/model.html) element of Supervised learningXGBoost is used for supervised learning problems, where we use the training data (with multiple features) xi to predict a target variable yi. Objective Function: Training Loss + RegularizationA very important fact about objective functions is they must always contain two parts: training loss and regularization.For example, a commomly used training loss is mean squared error.Another commonly used loss function is logistic loss for logistic regression The regularization term is what people usually forget to add. The regularization term controls the complexity of the model, which helps us to avoid overfitting.The general principle is we want both a simple and predictive model. The tradeoff between the two is also referred as bias-variance tradeoff in machine learning. Tree EnsembleThe tree ensemble model is a set of classification and regression trees(CART). Here is a simple example of a CART that classifies whether someone will like computer games.A CART is a bit different from decision trees, where the leaf only contains decision values. In CART, a real score is associated with each of the leaves, which gives us richer interpretations that go beyond classification. This also makes the unified optimization step easier, as we will see in a later part of this tutorial. Usually, a single tree is not strong enough to be used in practice. What is actually used is the so-called tree ensemble model, which sums the prediction of multiple trees together.Here is an example of a tree ensemble of two trees. The prediction scores of each individual tree are summed up to get the final score. If you look at the example, an important fact is that the two trees try to complement each other. Now here comes the question, what is the model for random forests? It is exactly tree ensembles! So random forests and boosted trees are not different in terms of model, the difference is how we train them. This means if you write a predictive service of tree ensembles, you only need to write one of them and they should directly work for both random forests and boosted trees. One example of why elements of supervised learning rock. Tree BoostingAfter introducing the model, let us begin with the real training part. How should we learn the trees? The answer is, as is always for all supervised learning models: define an objective function, and optimize it! Assume we have the following objective function (remember it always needs to contain training loss and regularization) Additive TrainingFirst thing we want to ask is what are the parameters of trees? You can find that what we need to learn are those functions fi, with each containing the structure of the tree and the leaf scores. This is much harder than traditional optimization problem where you can take the gradient and go. It is not easy to train all the trees at once. Instead, we use an additive strategy: fix what we have learned, and add one new tree at a time. It remains to ask, which tree do we want at each step? A natural thing is to add the one that optimizes our objective. Training modelThe XGBoost model for classification is called XGBClassifier(regression is called XGBRegressor). We can create and and fit it to our training dataset. Models are fit using the scikit-learn API and the model.fit() function.]]></content>
      <categories>
        <category>竞赛</category>
      </categories>
      <tags>
        <tag>XGBoost</tag>
        <tag>模型融合(Ensemble)</tag>
        <tag>Kaggle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[等概率生成器]]></title>
    <url>%2F2018%2F05%2F28%2F%E7%AD%89%E6%A6%82%E7%8E%87%E7%94%9F%E6%88%90%E5%99%A8%2F</url>
    <content type="text"><![CDATA[总体来说是可以有两种思路：一种是二级制的理解，一种是生成更大的数字的集合，然后求余+剪枝（重投）。 给定一个可以等概率生成1-3的rand3()函数生成器，求解可以随机等概率生成1-7的rand7()函数生成器。 解题思路 使用rand3()函数生成1-9的数字，然后丢弃8,9两种可能。 实现代码 12345678910class Solution: def random7(self): x =8 while x&gt;7: x =self.random3() + (self.random3()-1)*3 return x def random3(self): return if __name__ =="__main__": print(Solution().random7()) Given a random number generator rand5() generate gen7() 12345def rand7a(): rand7 =22 while rand7&gt;=21: rand7 =rand5()+ rand5()*5 return rand7 %7 1st approach:rand2() in binary is 000 or 001 with 50-50 probability.rand2()2 is 000 or 010 with 50-50 probability.rand2()4 is 000 or 100 with 50-50 probability.So the sum is binary xxx where each x has a 50-50 probability of 0 or 1; so each 000 to 111 has a probablilty of 1/8. 1234567def rand7b(): rand7 =7 while rand7 ==7: rand7 =rand2() +rand2()*2+ rand2()*4 return rand7 2nd approach: going through each of the 8 possibilities of the 3 rand2()s:0+0+0 or 1+0+0 or 0+2+0 or 1+2+0 or 0+0+4 or 1+0+4 or 0+2+4 or 1+2+4all of these with equal probablilty 1/8. 从大数字的 random 到小数字的random，需要做的是等概率的生成，所以对于不符合要求的是 就是 重新rand() 12345int rand2() &#123; int x = rand5(); if x == 4 return rand2(); // restart else return x % 2;&#125; Given rand2(), you should get rand5() 这个是可以从二进制的角度进行解析的。 12345int rand7() &#123; int x = rand2() * 4 + rand2() * 2 + rand2(); if (x == 7) return rand7(); // restart else return x;&#125; 其实不管 rand7() 是不是产生了7，即使产生了7 那么也是可以通过 “if condition” 进行提前进行处理的。 这种算法都不是唯一的。]]></content>
      <categories>
        <category>算法</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[机器学习优化方法]]></title>
    <url>%2F2018%2F05%2F28%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[机器学习的优化方法有很多，这里主要解释的是 Gradient Descent 和 Newton’s Method. 梯度下降（gradient descent） 梯度下降法是最早最简单，也是最为常用的最优化方法。梯度下降法实现简单，当目标函数是凸函数时，梯度下降法的解是全局解。接下来衍生的的子类:批量梯度下降法（Batch Gradient Descent，BGD）随机梯度下降（Stochastic Gradient Descent，SGD） 牛顿法和拟牛顿法（Newton’s method &amp; Quasi-Newton Methods） 牛顿法最大的特点就在于它的收敛速度很快。个人感觉牛顿法只是在在每次迭代的时候进行了两次运算，和梯度下降在总的运算次数上并没有很大的差别，为什么会收敛速度更快呢？ 名词解释 凸优化:对凸优化的问题我们在基础数学上面已经有了很多解决方法，例如可以将凸优化问题Lagerange做对偶化，然后用Newton、梯度下降算法求解。凸集合: 凸函数：Jacobian矩阵和Hessian矩阵：在向量分析中, 雅可比矩阵是一阶偏导数以一定方式排列成的矩阵, 其行列式称为雅可比行列式。Hessian矩阵：]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>优化</tag>
        <tag>Gradient Descent</tag>
        <tag>Newton&#39;s Method</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[求解平方根]]></title>
    <url>%2F2018%2F05%2F28%2F%E6%B1%82%E8%A7%A3%E5%B9%B3%E6%96%B9%E6%A0%B9%2F</url>
    <content type="text"><![CDATA[求解平方根的算法主要有两种：二分法(binary search) 和牛顿迭代法(Newton’s Method) Just give me codes…1234567891011121314151617181920212223242526272829from math import sqrt# you needn't import sqrt, I do that just for comparing the results of different methodsclass Solution: # in-built function def my_sqrt(self, n): return sqrt(n) def sqrt_binary(self, n): low, high =0,n mid =int((low+high)/2) while abs(mid*mid- n)&gt; 1e-9: if mid*mid&gt;n: high =mid else: low =mid mid =(low+high)/2 return mid # Newton method: math def newton_method(self, n): k =1 while abs(k*k-n) &gt;1e-9: k =(k+n/k)/2 return k if __name__ =="__main__": print(Solution().my_sqrt(2)) print(Solution().sqrt_binary(2)) print(Solution().newton_method(2)) 运行结果1231.4142135623730951 #built-in function1.4142135623842478 # binary search1.4142135623746899 # Newton method 从结果中看，Newton method比 binary sqrt更加接近系统自带的sqrt function. 并且从数学上可以证明 Newton method 比 binary sqrt需要更少的迭代次数。 附录：牛顿迭代法是求方程根的重要方法之一，其最大优点是在方程f(x) = 0的单根附近具有平方收敛，而且该法还可以用来求方程的重根,复根。牛顿迭代法结论其实就是取泰勒级数前两项等于0求得的,泰勒公式表示为更简练的写法为：通过 Newton methond逼近方程的解的过程可以表示为下图： referrences:https://blog.csdn.net/ycf74514/article/details/48996383]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>Newton&#39;s Method</tag>
        <tag>求解平方根</tag>
        <tag>binary search</tag>
      </tags>
  </entry>
</search>
