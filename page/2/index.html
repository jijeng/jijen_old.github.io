<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="Stay Hungry, Stay Foolish">
<meta property="og:type" content="website">
<meta property="og:title" content="Jijeng&#39;s blog">
<meta property="og:url" content="http://yoursite.com/page/2/index.html">
<meta property="og:site_name" content="Jijeng&#39;s blog">
<meta property="og:description" content="Stay Hungry, Stay Foolish">
<meta property="og:locale" content="default">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Jijeng&#39;s blog">
<meta name="twitter:description" content="Stay Hungry, Stay Foolish">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/page/2/"/>





  <title>Jijeng's blog</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Jijeng's blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="Searching..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/04/24/pygen/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jijeng Jia">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jijeng's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/04/24/pygen/" itemprop="url">pygen</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-04-24T10:54:05+08:00">
                2019-04-24
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/04/24/pygen/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2019/04/24/pygen/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><strong>pygen功能：</strong>有关联的随机生成人名，邮箱，ID Card (ssn)，电话，地址等信息，并且可以选择保存为 pandas dataframe格式, 数据库”.db” 文件, Excel 文件和csv 文件格式，用于机器学习训练。<a href="https://github.com/jijeng/pygen" target="_blank" rel="noopener">项目地址github</a>。</p>
<p>随机生成虚假个人信息具有很大的应用空间。首先，虚假的生成数据可以用于机器学习模型的“准备数据”，当真实的数据比较少或者难以获得的时候，可以使用生成数据进行训练模型，待模型调通之后，然后使用真实的模型。并且，当真实的数据集中缺少某些特征时候，可以使用这种方法进行特征的填充。比如大的数据集中缺少现居城市地址的时候，可以调用该库中的 “city_real” 进行填充。</p>
<p>当前使用最为广泛的是 <a href="https://faker.readthedocs.io/en/master/" target="_blank" rel="noopener">Faker</a> 开源库用于数据的生成。虽然该库支持中文，但是对于中文的支持力度有限，所以有时候并不能满足我的需求，比如说生成的身份证 (ssn) 和姓名所能体现的性别是不匹配(了解更多可以参考<a href="https://github.com/jayknoxqu/id-number-util" target="_blank" rel="noopener">这里</a>)、生成的姓名中缺少复姓和电话邮箱等信息不符合我们的使用习惯等等。所以我将从以下几点改进：</p>
<ul>
<li>增强数据之间相关性</li>
<li>生成名字的多样性</li>
<li>符合国人使用习惯的邮箱电话</li>
<li>提供保存多种保存文件格式，更加适合机器学习的训练</li>
</ul>
<p>中文名字有很强的性别属性。例如名字中带有“杰”“志”“宏”等字的一般为男性，带有“琬”“佩”“梅”等字的一般为女性。当然也有一些比较中性的字，例如“文”“安”“清”等，比较难猜测性别，关于这点会在另一个博客中展开，请期待。</p>
<p>faker 对中文的支持有限，比如下面这种情况。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> faker <span class="keyword">import</span> Faker</span><br><span class="line">fake = Faker(<span class="string">'zh_CN'</span>)</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    print(fake.name(),fake.ssn(),fake.phone_number())</span><br></pre></td></tr></table></figure>
<p><img src="http://ww1.sinaimg.cn/large/e9a223b5ly1g2dgyq294dj20kg09sq39.jpg" alt=""></p>
<p>从图中可以明显的看出 “王玉梅”和 “李桂花”都是两个女性，但是这种身份证信息（ssn）都没有体现这点。关于身份证的科普信息可以从这里获得。简单来说倒数第二位表示性别信息，如果是男性就是奇数如果是女性就是偶数。faker 生成的数据是不具有数据之间的相关性的。</p>
<p>基于此，我们进行了改进。首先是姓名的生成，然后是性别的判断，最后再生成相应性别的身份证号码。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pygen <span class="keyword">import</span> pygen</span><br><span class="line">db =pygen()</span><br><span class="line">db.gen_dataframe(fields =[<span class="string">'name'</span>, <span class="string">'ssn'</span>, <span class="string">'phone'</span>, <span class="string">'email'</span>])</span><br></pre></td></tr></table></figure>
<p>效果如下：<br><img src="http://ww1.sinaimg.cn/large/e9a223b5ly1g2dhis3u0qj20vm0iq75a.jpg" alt=""></p>
<p><img src="http://ww1.sinaimg.cn/large/e9a223b5ly1g2dhj273lrj20vc0iwjsf.jpg" alt=""></p>
<p>红色线条表示姓名和性别对应一致，蓝色线条表示结果不确定（“镜阳炎” 像是一个中性的名字），绿色表示生成了含有复姓的名字，增强了数据的多样性。</p>
<p>从上图的 “mail” 一列可以看出邮箱前缀的命名基本上是中文名字中“姓” 和“民”的拼音组合，加强了数据之间的相关性和真实度。</p>
<p>另外，电话号码按照运营商分为三类：0 表示移动，1表示联通，2表示电信。</p>
<pre><code>print(&apos;移动字段:&apos;)
for _ in range(5):
  print(db.simple_ph_num(types =0))

print(&apos;联通字段:&apos;)
for _ in range(5):
  print(db.simple_ph_num(types =1))

print(&apos;电信字段：&apos;)
for _ in range(5):
  print(db.simple_ph_num(types =2))
</code></pre><p>输出：</p>
<blockquote>
<p>移动字段: 15023689929 16771753917 16790223946 15950129353 15271129554<br>联通字段: 13869739303 13786227031 13950354445 15137578545 15240836142<br>电信字段： 17172983067 15658567011 18562313243 17073127396 15543448286</p>
</blockquote>
<p>最后提供了多种文件保存格式，包括”.csv”, “.db” 和”.xlsx”等格式。可以使用如下：</p>
<pre><code>from pygen import pygen
db =pygen()
db.gen_table(filename =filename, fields =[&apos;name&apos;, &apos;ssn&apos;, &apos;phone&apos;, &apos;email&apos;])
db.gen_excel(filename =filename, fields =[&apos;name&apos;, &apos;ssn&apos;, &apos;phone&apos;, &apos;email&apos;])
db.gen_csv(filename =filename, fields =[&apos;name&apos;, &apos;ssn&apos;, &apos;phone&apos;, &apos;email&apos;])
</code></pre>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/04/18/mode-collapse-in-gan/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jijeng Jia">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jijeng's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/04/18/mode-collapse-in-gan/" itemprop="url">Mode Collapse in GANs</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-04-18T21:42:01+08:00">
                2019-04-18
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Deep-learning/" itemprop="url" rel="index">
                    <span itemprop="name">Deep learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/04/18/mode-collapse-in-gan/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2019/04/18/mode-collapse-in-gan/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>Mode collapse, a failure case for GANs where the generator generate a limited diversity of samples, regardless of the input.</p>
<p>But what causes the mode collapse? There are four reasons for that.</p>
<p><strong>The objective of GANs</strong></p>
<p><strong>The generator, generates new data, while the discriminator evaluates it for authenticity but not for the diversity of generated instances.</strong></p>
<p>the generator can win by producing a polynomial number of training examples. And a low capacity discriminator cannot detect this process, thus, it cannot guide the generator to approximate the target distribution. Even if a high discriminator identifies and assigns the collapse part a low probability, then the generator will simply move from its collapsed output to focus on another fixed output point.</p>
<p><strong>Generator</strong></p>
<p>No matter the objective function is, if it only <strong>considers individual samples (without looking forward or backward)</strong> then the generator is not directly incentivised to produce diverse examples.</p>
<p>From [1], standard GAN training corresponds exactly to updating the generator parameters using only the first term in this gradient because of a fixed discriminator during GAN training. Therefore, in standard GAN training, each generator update step is a partial collapse towards a delta function.</p>
<p>$$<br>\frac { \mathrm { d } f _ { K } \left( \theta _ { G } , \theta _ { D } \right) } { \mathrm { d } \theta _ { G } } = \frac { \partial f \left( \theta _ { G } , \theta _ { D } ^ { K } \left( \theta _ { G } , \theta _ { D } \right) \right) } { \partial \theta _ { G } } + \frac { \partial f \left( \theta _ { G } , \theta _ { D } ^ { K } \left( \theta _ { G } , \theta _ { D } \right) \right) } { \partial \theta _ { D } ^ { K } \left( \theta _ { G } , \theta _ { D } \right) } \frac { \mathrm { d } \theta _ { D } ^ { K } \left( \theta _ { G } , \theta _ { D } \right) } { \mathrm { d } \theta _ { G } }<br>$$</p>
<p>Some methods have been proposed. Multiple generators and weight-sharing generators are developed to capture more modes of the distribution. </p>
<p><strong>Discriminator</strong></p>
<p>The mode collapse is often explained as gradient exploding of discriminator, which comes from the imbalance between the discriminator and the generator. For example, the technique of TTUR could help discriminator to keep its optimality. But some researchers believe that this is a desirable goal since a good discriminator can give good feedback and ignore the fact.</p>
<p>In addition, <strong>the discriminator process each example independently</strong>, the generator depends on discriminator, thus  no mechanism to tell the outputs of the generator to become more similar to each other. </p>
<p>The idea from [2], that we could use mini-batch discrimination to help generator give better feedback</p>
<p>A straightforward approach to handle multimodality is to take random noise vectors along with the conditional contexts as inputs, where the contexts determine the main content and noise vectors are responsible for variations.<br>The noise vectors are ignored or of minor impacts, since cGANs pay more attention to learn from the high-dimensional and structured conditional contexts. </p>
<p><strong>Another question</strong></p>
<p>Mode collapse may happen only partially?<br>since training is stochastic progress, the input of generator network will vary and the sample drawn from the real distribution will also vary</p>
<p>But sometimes mode collapse is not all bad news. In style transfer using GAN, we are happy to convert one image to just a good one, rather than finding all variants. Indeed, the specialization in the partial mode collapse sometimes creates higher quality images.</p>
<h2 id="referrences"><a href="#referrences" class="headerlink" title="referrences:"></a>referrences:</h2><p>[1]. Section 2.4 of <a href="https://arxiv.org/abs/1611.02163" target="_blank" rel="noopener">Unrolled Generative Adversarial Networks</a><br>[2]. Section 3.2 of <a href="https://arxiv.org/abs/1606.03498" target="_blank" rel="noopener">Improved Techniques for Training GANs</a><br>[3]. <a href="https://arxiv.org/abs/1903.05628" target="_blank" rel="noopener">Mode Seeking Generative Adversarial Networks for Diverse Image Synthesis</a><br>[4]. <a href="https://arxiv.org/abs/1902.03984" target="_blank" rel="noopener">Improving Generalization and Stability of Generative Adversarial Networks</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/04/17/a-not-so-gentle-introduction-to-hyperparameters-tuning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jijeng Jia">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jijeng's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/04/17/a-not-so-gentle-introduction-to-hyperparameters-tuning/" itemprop="url">A Not-So-Gentle Introduction to Hyper-parameters Tuning</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-04-17T21:14:15+08:00">
                2019-04-17
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Deep-learning/" itemprop="url" rel="index">
                    <span itemprop="name">Deep learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/04/17/a-not-so-gentle-introduction-to-hyperparameters-tuning/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2019/04/17/a-not-so-gentle-introduction-to-hyperparameters-tuning/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>Setting the hyper-parameters seems like a black art that requires years of experience to acquire. Currently, there are no simple and easy ways to set hyper-parameters, especifically, batch size, learning rate, momentum, and weight decay. A grid search or random search maybe sounds like a good idea. In this blog, I’d like to share you my idea from reading papers and my projects.</p>
<h2 id="Hyper-parameters"><a href="#Hyper-parameters" class="headerlink" title="Hyper-parameters"></a>Hyper-parameters</h2><h3 id="Batch-Size"><a href="#Batch-Size" class="headerlink" title="Batch Size"></a>Batch Size</h3><p>Learning rate is maybe the most important hyper-parameters, but we choose batch size firstly because large batch size needs a large learning rate in most circumstances.</p>
<p>A general principle is: use as a large batch size as possible to fit your CPU memory or/both GPU memory. There are several reasons:</p>
<ul>
<li>larger batch sizes permit the use of larger learning rates</li>
<li>A constant number of iterations favors larger batch sizes</li>
</ul>
<p>However, small batch sizes add regularization while large batch sizes add less. So utilize it while balancing the proper amount of regularization.</p>
<h3 id="Learning-Rate"><a href="#Learning-Rate" class="headerlink" title="Learning Rate"></a>Learning Rate</h3><p>We will introduce the idea from [Cyclical Learning Rates for Training Neural Networks][1]: Cyclical Learning Rates.</p>
<p>Instead of monotonically decreasing the learning rate, this method lets the learning rate cyclically vary between reasonable boundary values. <strong>The essence of this learning rate policy comes from the observation that increasing the learning rate might have a short term negative effect and yet achieve a longer term beneficial effect</strong>. This observation leads to the idea of letting the learning rate vary within a range of values rather than adopting a stepwise fixed or exponentially decreasing value. That is, one sets minimum and maximum boundaries and the learning rate cyclically varies between these bounds.<br><img src="http://ww1.sinaimg.cn/large/e9a223b5ly1g25wjj6cu9j20p80gcwh4.jpg" alt=""><br>%<small>From Cyclical Learning Rates for Training Neural Networks </small></p>
<p>An intuitive understanding of why CLR methods work comes from considering the loss function topology. Dauphin et al. argue that the difficulty in minimizing the loss arises from <strong> saddle points rather than poor local minima.</strong> Saddle points have small gradients that slow the learning process. However, increasing the learning rate allows for more rapid traversal of saddle point plateaus.</p>
<p>But the question is that how can we find the Minimum bound and Maximum bound. There is a simple way to estimate the reasonable minimum and maximum boundary values with one training run of the network for a few epochs. It is a “LR range test”; run your model for several epochs while letting the learning rate increase linearly between low and high LR values. For example, set both the step size and maxiter to the same number of iterations. In this case, the learning rate will increase linearly from the minimum value to the maximum value during this short run. Next, plot the accuracy versus learning rate. Note the learning rate value when the accuracy starts to increase and when the accuracy slows, becomes ragged, or starts to fall. These two learning rates are good choices for bounds; that is, set $base-<em>{lr}$ to the first value and set $max-</em>{lr}$ to the latter value.</p>
<h3 id="Momentum"><a href="#Momentum" class="headerlink" title="Momentum"></a>Momentum</h3><p>Since learning rate is regarded as the most important hyper-parameter to tune then momentum is also important. Like learning rates, it is valuable to set momentum as large as possible without causing instabilities during training.</p>
<p>The large learning rate can deal with local minimum but works fail when it comes to saddle point where momentum comes to rescue.</p>
<p>The local minimum is like the following picture.<br><img src="http://ww1.sinaimg.cn/large/e9a223b5ly1g25x29cb1dj20y00iogs2.jpg" alt=""><br>In mathematics, a saddle point or minimax point is a point on the surface of the graph of a function <strong>where the slopes (derivatives) in orthogonal directions are all zero</strong> (a critical point), but which is not a local extremum of the function.</p>
<p>Your first step from the very top would likely take you down, but then you’d be on a flat rice terrace. The gradient would be zero, and you’d have nowhere to go. To remedy this, we employ momentum - the algorithm remembers its last step and adds some psroportion of it to the current step. This way, even if the algorithm is stuck in a flat region, or a small local minimum, it can get out and continue towards the true minimum.</p>
<p>In summary: when performing gradient descent, learning rate measures how much the current situation affects the next step, while momentum measures how much past steps affect the next step.</p>
<h3 id="Weights-Decay"><a href="#Weights-Decay" class="headerlink" title="Weights Decay"></a>Weights Decay</h3><p>When training neural networks, it is common to use “weight decay,” where after each update, the weights are multiplied by a factor slightly less than 1. This prevents the weights from growing too large and can be seen as gradient descent on a quadratic regularization term.</p>
<p>But why?</p>
<p>Large weights might correlate with certain patterns in the input data (x), this means that the model almost hard codes certain values. This then makes our training data fit well but our test data fit less well.</p>
<p>The idea of weight decay is simple: to prevent overfitting, every time we update a weight $w$ with the gradient $∇J$ in respect to $w$, we also subtract from it $λ∙w$. This gives the weights a tendency to decay towards zero, hence the name. L2 is a type of weights decay.<br>$$<br>J ( W ; X , y ) + \frac { 1 } { 2 } \lambda \cdot | W | ^ { 2 }<br>$$</p>
<p>But weights decay is not necessarily true for all gradient-base algorithms and was recently shown to not be the case for adaptive gradient algorithms, such as Adam.</p>
<p>In addition, weight decay is not the only regularization technique. In the past few years, some other approaches have been introduced such as Dropout, Bagging, Early Stop, and Parameter Sharing which work very well in NNs.</p>
<h2 id="Takeoff"><a href="#Takeoff" class="headerlink" title="Takeoff"></a>Takeoff</h2><ol>
<li>Batch Size</li>
</ol>
<p>Use as a large batch size as possible to fit your memory</p>
<ol start="2">
<li>Learning Rate</li>
</ol>
<p>Perform a learning rate range test to identify a “large” learning rate.</p>
<ol start="3">
<li>Momentum</li>
</ol>
<p>Test with short runs of momentum values 0.99, 0.97, 0.95, and 0.9 to get the best value for momentum.</p>
<p>If using the 1-cycle learning rate schedule, it is better to use a cyclical momentum (CM) that starts at this maximum momentum value and decreases with increasing learning rate to a value of 0.8 or 0.85.</p>
<ol start="4">
<li>Weights Decay</li>
</ol>
<p>A grid search to determine the proper magnitude but usually does not require more than one significant figure accuracy.<br>A more complex dataset requires less regularization so test smaller weight decay values, such as $10^{−4} $, $10^{−5} $, $10^{−6} $, 0.<br>A shallow architecture requires more regularization so test larger weight decay values, such as $10^{−2} $, $10^{−3} $, $10^{−4} $.</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>[1]. <a href="https://arxiv.org/abs/1506.01186" target="_blank" rel="noopener">Cyclical Learning Rates for Training Neural Networks</a><br>[2]. <a href="https://arxiv.org/abs/1803.09820" target="_blank" rel="noopener">A disciplined approach to neural network hyper-parameters</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/04/17/weights-initialization/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jijeng Jia">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jijeng's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/04/17/weights-initialization/" itemprop="url">Weights Initialization</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-04-17T21:04:44+08:00">
                2019-04-17
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Deep-learning/" itemprop="url" rel="index">
                    <span itemprop="name">Deep learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/04/17/weights-initialization/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2019/04/17/weights-initialization/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>Training a neural network consists of four steps: initialize weights and biases, forward propagation, compute the loss function and backward propagation. This blog mainly focuses on the first part: weights initialization.</p>
<p>After completing this tutorial, you will know:</p>
<ul>
<li><p>Four main types of weights initialization</p>
</li>
<li><p>How to choose between Xavier /zivier/ initialization and He initialization</p>
</li>
</ul>
<h2 id="Types-of-Weights-Initialization"><a href="#Types-of-Weights-Initialization" class="headerlink" title="Types of Weights Initialization"></a>Types of Weights Initialization</h2><ol>
<li>Initializing weights with zero</li>
</ol>
<p>When you set all weights in a neural network to zero, the derivative with respect to loss function is the same for every $ w$ in the same layer, thus all the weights have the same values in the subsequent iteration, which makes your model equivalent to a linear model.</p>
<ol start="2">
<li>Initializing weights randomly</li>
</ol>
<p>You can get weights like this (Python):</p>
<pre><code>w =np.random.randn(layer_size[l],layer_size[l-1])
</code></pre><p>The weighs follows standard normal distribution while it can potentially lead to two issues: vanishing gradients and exploding gradients.<br>下面的情况是很容易发生，因为网络中特征足够多（网络结构足够宽），所以 random 得到数值有足够的 coverage，所以就会出现 weights too small or too large 这种情况。</p>
<p>If the weights start too small, then the signal shrinks as it passes through each layer until it’s too small to be useful.</p>
<p>If the weights start too large, then the signal grows as it passes through each layer until it’s too massive to be useful (big value in sigmoid function).</p>
<p>Thus there are two necessary conditions to consider:</p>
<ul>
<li><p>The values of each activation layer won’t be zero</p>
</li>
<li><p>The values of each activation layer won’t go into the area of saturation</p>
</li>
</ul>
<ol start="3">
<li>Xavier/Glorot Initialization</li>
</ol>
<p>For deep networks, we can use a heuristic to initialize the weights depending on the non-linear activation function. This applies to Xavier and He initialization.</p>
<p>Xavier/Glorot Initialization initializes the weights in your network by drawing them from a distribution with zero mean and a specific variance.<br>$$ { var } ( w _ { i } ) = \frac { 1 } { layer_{l-1}}$$</p>
<pre><code>w=np.random.randn(layer_size[l],layer_size[l-1])*np.sqrt(1/layer_size[l-1])
</code></pre><p>In practice, it works better for layers with sigmoid or tanh function. </p>
<ol start="4">
<li>He Initialization</li>
</ol>
<p>Using RELU or Leaky RELU is relatively robust to the vanishing/ exploding gradient issues compared with sigmoid function especially for networks that are not too deep. And it the case of Leaky RELU, it never has zero gradients. For RELU, we multiply the randomly generated values of $w$ by: </p>
<p>$$\sqrt { \frac { 2 } { layer _ { [ l - 1 ] } } }$$</p>
<pre><code>w=np.random.randn(layer_size[l],layer_size[l-1])*np.sqrt(2/layer_size[l-1])
</code></pre><p>Sometimes, we combine the idea of Xavier initialization and He initializaiton so the variance becomes the following:</p>
<p>$$\sqrt { \frac { 2 } { layer _ { [ l - 1 ] } + \operatorname { layer } _ { [ l ] } } }$$</p>
<pre><code>w=np.random.randn(layer_size[l],layer_size[l-1])*np.sqrt(2/(layer_size[l-1]+layer_size[l]))
</code></pre><p>The idea behind this is that we set the weights neither too much bigger than 1 nor too much less than 1 so the gradients do not vanish or explode too quickly.</p>
<h2 id="Takeoff"><a href="#Takeoff" class="headerlink" title="Takeoff"></a>Takeoff</h2><p>In summary, the main difference in machine learning is the following:</p>
<ul>
<li><p>He initialization works better for layers with ReLu(s) activation.</p>
</li>
<li><p>Xavier initialization works better for layers with sigmoid activation.</p>
</li>
</ul>
<h2 id="Referrence"><a href="#Referrence" class="headerlink" title="Referrence:"></a>Referrence:</h2><p><a href="https://arxiv.org/abs/1502.01852" target="_blank" rel="noopener">He initialization</a></p>
<p><a href="http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf" target="_blank" rel="noopener">Xavier initialization</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/04/09/cyclegan-stylegan/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jijeng Jia">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jijeng's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/04/09/cyclegan-stylegan/" itemprop="url">CycleGAN & StyleGAN</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-04-09T18:48:30+08:00">
                2019-04-09
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Deep-learning/" itemprop="url" rel="index">
                    <span itemprop="name">Deep learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/04/09/cyclegan-stylegan/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2019/04/09/cyclegan-stylegan/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>In the past few yeas, GANs have been used in lots of different applications such as generating synthetic data, style transfer, super-resolution and text2image generation. But we donn’t aim to give a overview of what GANs are made for. There are many great and detailed blogs for your understanding.</p>
<p><strong>What this post is about</strong></p>
<ul>
<li>Main ideas of CycleGAN</li>
<li>Keypoints in StyleGAN</li>
</ul>
<h1 id="A-Gentle-Introduction-of-GANs"><a href="#A-Gentle-Introduction-of-GANs" class="headerlink" title="A Gentle Introduction of GANs"></a>A Gentle Introduction of GANs</h1><p>We assume the reader has some prior experience with neural networks. In addition, if you are familiar with GANs you can <a href="#jump">skip</a> this section.</p>
<p>The famous minimax objective function can be formulated as following:<br>$$<br>\min _ { \theta _ { g } } \max _ { \theta _ { d } } \left[ \mathbb { E } _ { x \sim p _ { d a t a } } \log D _ { \theta _ { d } } ( x ) + \mathbb { E } _ { z \sim p ( z ) } \log \left( 1 - D _ { \theta _ { d } } \left( G _ { \theta _ { g } } ( z ) \right) \right) \right)<br>$$<br>But in practical, the loss function cannot work very well. So we have alternative objective function:</p>
<ol>
<li>Gradient ascent on discriminator</li>
</ol>
<p>$$<br>\max _ { \theta _ { d } } \left[ \mathbb { E } _ { x \sim p _ { d a t a } } \log D _ { \theta _ { d } } ( x ) + \mathbb { E } _ { z \sim p ( z ) } \log \left( 1 - D _ { \theta _ { d } } \left( G _ { \theta _ { g } } ( z ) \right) \right) \right]<br>$$</p>
<ol start="2">
<li>Gradient ascent on generator<br>$$<br>\max _ { \theta _ { g } } \mathbb { E } _ { z \sim p ( z ) } \log \left( D _ { \theta _ { d } } \left( G _ { \theta _ { g } } ( z ) \right) \right)<br>$$</li>
</ol>
<p>The reasoning behind this can be found in original <a href="https://arxiv.org/abs/1406.2661" target="_blank" rel="noopener">paper</a>. Simplily speaking, we can get higher gradient signal for bad samples, which works much better in practice.<br>bad case 的时候，使用原来的min-max function会使得学习率不够，使用 gradient ascent 就会好一些。</p>
<p><img src="http://ww1.sinaimg.cn/large/e9a223b5ly1g1w9einh3uj20sk0dkn1c.jpg" alt=""></p>
<center><span id="jump">From Stanford CS231 Lecture 13 — Generative Models </span></center>

<h1 id="Main-ideas-of-CycleGAN"><a href="#Main-ideas-of-CycleGAN" class="headerlink" title="Main ideas of CycleGAN"></a>Main ideas of CycleGAN</h1><p>CycleGAN was introduced in 2017 out of Berkeley, <a href="https://arxiv.org/abs/1703.10593" target="_blank" rel="noopener">Unpaired Image-to-Image Translation Using Cycle-Coonsistent Adversarial Networks</a>.  This task is performed on unpaired data. Recent methods such as Pix2Pix depend on the availability of training examples where the samee data is availabel in both domains. However, CycleGAN is able to learning such pair information without one-to-one mapping between training data in source and target domains.<br><img src="http://ww1.sinaimg.cn/large/e9a223b5ly1g1wb7eq678j20vg0kidpn.jpg" alt=""></p>
<h2 id="Network-Architecture"><a href="#Network-Architecture" class="headerlink" title="Network Architecture"></a>Network Architecture</h2><p>We build three networks.</p>
<ul>
<li>A generator $F$ to convert image $y$ to image $ \hat{x}$</li>
<li>A generator $G$ to convert image $\hat{x}$ to image $ \hat{y}$</li>
<li>A discriminator $D$ to idenfify real image or generated picture<br>Simplified version of CycleGAN architecture can be showed in the following.<br><img src="http://ww1.sinaimg.cn/large/e9a223b5ly1g1wbibdfe1j20i60c3wf0.jpg" alt=""><br>The function $F$ and $G$ are generator network, which consists of encoder, transformer and decoder. Encoder is extracting the features from an image which is done by convolution networks. Each convolution layer leads to extraction of progressively higher level features. We would like to transform the feature emebdding of an image from domain $X$ to that of domain $Y$. So for this, authors have used 6 layers of resnet blocks. Resnet block is a neural network layer which consists of two convolutiona layers when a residue of input ia added to the output. This is done to ensure peoperties of input of previous layers are available for later layers as well.Resnet block can be summarized in following image<br><img src="http://ww1.sinaimg.cn/large/e9a223b5ly1g1wet610p7j20km0ceabs.jpg" alt=""><br>The decoder transfer embedding from $y$ back to original embedding $x$.</li>
</ul>
<h2 id="Loss-function"><a href="#Loss-function" class="headerlink" title="Loss function"></a>Loss function</h2><p>There are two types of losses in CycleGAN. Besides adversarial loss, we have another loss named reconstruction cost.<br>Adversarial loss is similary to original GAN.<br>$$<br>\operatorname { Loss } _ { a d v } \left( F , D _ { x } , Y \right) = \frac { 1 } { m } \sum _ { i = 1 } ^ { m } \left( 1 - D _ { x } \left( F \left( y _ { i } \right) \right) \right) ^ { 2 }<br>$$<br>$$<br>\operatorname { Loss } _ { a d v } \left( G , D _ { y } , X \right) = \frac { 1 } { m } \sum _ { i = 1 } ^ { m } \left( 1 - D _ { y } \left( G \left( x _ { i } \right) \right) \right) ^ { 2 }<br>$$<br>However, the adversarial loss alone is not sufficient to produce good looking images, which can not enfore that the input and output are recognizably the same. The cycle consistency loss addresses this issue. It relies on the expectation that if you convert an image to the other domain and back again, and then you should get back something similar to what you put in. It enforces that $F ( G ( x ) ) \approx x$ and $G ( F ( y ) ) \approx y$.<br>$$<br>\operatorname { Loss } _ { c y c } ( G , F , X , Y ) = \frac { 1 } { m } \sum _ { i = 1 } ^ { m } \left[ F \left( G \left( x _ { i } \right) \right) - x _ { i } \right] + \left[ G \left( F \left( y _ { i } \right) \right) - y _ { i } \right]<br>$$<br>We can get the full objective function by putting these two together.<br>$$<br>\mathcal { L } \left( G , F , D _ { x} , D _ { y } \right) = \mathcal { L } _ { \text { GAN } } \left( G , D _ { y } , X , Y \right) + \mathcal { L } _ { \text { GAN } } \left( F , D _ { x } , Y , X \right) + \lambda \mathcal { L } _ { \text { cyc } } ( G , F )<br>$$</p>
<h1 id="Keypoints-of-StyleGAN"><a href="#Keypoints-of-StyleGAN" class="headerlink" title="Keypoints of StyleGAN"></a>Keypoints of StyleGAN</h1><p>The StyleGAN offeras an upgrade version of ProGAN’s image generator, with a focus on the generator. </p>
<p>ProGAN generates high-quality images but, in most models, its ability to control specific features of the generated image is very limited. In other word, the features are entangled and therefore attempting to tweak the input, even a bit, usually affects multiple features at the same time. A good illustrations would be following pictures.<br><img src="http://ww1.sinaimg.cn/large/e9a223b5ly1g1wfimzvg2g20ps0en4qq.gif" alt=""><br>Compared with first version (ProGAN), the new generator includes several additions to ProGAN’s generators.</p>
<h2 id="Mapping-Network"><a href="#Mapping-Network" class="headerlink" title="Mapping Network"></a>Mapping Network</h2><p>The mapping network’s goal in to encode the input vector into an intermediate vector whose different elements control different visual features, which consists of 8 fully connected layers and its output $w$ is of the same size as the input.<br><img src="http://ww1.sinaimg.cn/large/e9a223b5ly1g1wfvtdeh3j20nn0b9gma.jpg" alt=""></p>
<h2 id="Style-Modules-AdaIN"><a href="#Style-Modules-AdaIN" class="headerlink" title="Style Modules (AdaIN)"></a>Style Modules (AdaIN)</h2><p>The AdaIn (Adaptive Instance Normalization) module transfers the encoded information $w$, created by the mapping network, into the generated image.<br><img src="http://ww1.sinaimg.cn/large/e9a223b5ly1g1wg3c5awfj20ca09q3z6.jpg" alt=""></p>
<h2 id="Removing-traditional-input"><a href="#Removing-traditional-input" class="headerlink" title="Removing traditional input"></a>Removing traditional input</h2><p>Since the encoded information $w$ from mapping network was used into generator image, the traditional random input can be omitted and replaced by constant values.</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/04/09/python-for-beginners/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jijeng Jia">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jijeng's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/04/09/python-for-beginners/" itemprop="url">Python from Beginner to Master</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-04-09T18:47:46+08:00">
                2019-04-09
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Python/" itemprop="url" rel="index">
                    <span itemprop="name">Python</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/04/09/python-for-beginners/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2019/04/09/python-for-beginners/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="Basic-Skills"><a href="#Basic-Skills" class="headerlink" title="Basic Skills"></a>Basic Skills</h2><h3 id="module"><a href="#module" class="headerlink" title="module"></a>module</h3><p>python 文件可以当做主文件进行运行或者当做函数的集合进行调用。如果是前者一般是需要包含”__name__ ==”__main__”。对于后者就是在其他的python文件中进行调用。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> my_module <span class="comment"># python文件</span></span><br><span class="line"><span class="keyword">from</span> my_module <span class="keyword">import</span> my_object</span><br></pre></td></tr></table></figure></p>
<h3 id="packages"><a href="#packages" class="headerlink" title="packages"></a>packages</h3><pre><code>from packageroot.packagefolder.mod import my_object
</code></pre><p>Note: Ensure each directory within your package import contains a file __init__.py</p>
<h3 id="python-path"><a href="#python-path" class="headerlink" title="python-path"></a>python-path</h3><p>python2 和python3 使用不同的解释器，导致在一些函数命名和计算上有一些差别，最好在文件的开头标明使用的解释器。</p>
<h3 id="while-or-for"><a href="#while-or-for" class="headerlink" title="while or for"></a>while or for</h3><p>while : provide a condition and run the loop until the condition is not met.</p>
<p>for: loop for a number of specific times; loop over items or characters of a string.</p>
<p>examples:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[Variable] AggregateFunction([Value] <span class="keyword">for</span> [item] <span class="keyword">in</span> [collection])</span><br><span class="line">x =[<span class="number">1</span>, <span class="number">2</span>,<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]</span><br><span class="line">y =[ <span class="number">2</span>*a <span class="keyword">for</span> a <span class="keyword">in</span> x <span class="keyword">if</span> a%<span class="number">2</span> ==<span class="number">0</span>]</span><br><span class="line">y &gt;&gt; [<span class="number">4</span>, <span class="number">8</span>]</span><br></pre></td></tr></table></figure></p>
<p>或者可以使用这样更加简洁的语句：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">    <span class="keyword">lambda</span> arguments : expression</span><br><span class="line">    fun1 = <span class="keyword">lambda</span> a,b,c : a+b+c</span><br><span class="line">    print(fun1(<span class="number">5</span>,<span class="number">6</span>,<span class="number">2</span>))</span><br><span class="line">```    </span><br><span class="line"></span><br><span class="line">来个比较复杂的例子:</span><br><span class="line"></span><br><span class="line">```python</span><br><span class="line">    nums =[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>]</span><br><span class="line">    letters =[<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>,<span class="string">'d'</span>,<span class="string">'e'</span>]</span><br><span class="line">    <span class="comment"># 两个for 循环也是要熟练</span></span><br><span class="line">    nums_letters =[[n, l] <span class="keyword">for</span> n <span class="keyword">in</span> nums <span class="keyword">for</span> l <span class="keyword">in</span> letters ]</span><br><span class="line">    nums_letters</span><br></pre></td></tr></table></figure></p>
<h3 id="break-continue-or-pass"><a href="#break-continue-or-pass" class="headerlink" title="break, continue, or pass"></a>break, continue, or pass</h3><p>The break, continue, and pass statements in Python will allow you to use for loops and while loops more effectively in your code.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">number = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> number <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">   number = number + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">   <span class="keyword">if</span> number == <span class="number">5</span>:</span><br><span class="line">      <span class="keyword">pass</span>    <span class="comment"># pass here</span></span><br><span class="line"></span><br><span class="line">   print(<span class="string">'Number is '</span> + str(number))</span><br><span class="line"></span><br><span class="line">print(<span class="string">'Out of loop'</span>)</span><br></pre></td></tr></table></figure></p>
<p>The pass statement occurring after the if conditional statement is telling the program to continue to run the loop and ignore the fact that the variable number evaluates as equivalent to 5 during one of its iterations.</p>
<p>yield 可以用用作新的 if的测试, return results without termination</p>
<p>The pass statement can create minimal classes, or act as a placeholder when working on new code and thinking on an algorithmic level before hammering out details.<br>pass 的存在就是占坑，否则这个地方就是报错（IndentationError）。用于想要扩展的地方，但是现在还没有扩展。比如在某个method 下面或者某个 if 条件下。</p>
<h3 id="yield-or-return"><a href="#yield-or-return" class="headerlink" title="yield or return"></a>yield or return</h3><p>经常被用来作为生成器。</p>
<p>when you call a normal function with a return statement the function is terminated whenever it encounters a return statement. In a function with a yield statement the state of the function is ‘saved’ from the last call and can be picked up the next time you call a generator function.</p>
<p>for examples<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">gen_exp =(x **<span class="number">2</span> <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">10</span>) <span class="keyword">if</span> x %<span class="number">2</span> ==<span class="number">0</span>)</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> gen_exp:</span><br><span class="line">  print(x)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_gen</span><span class="params">()</span>:</span></span><br><span class="line">  <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">    <span class="keyword">yield</span> x</span><br><span class="line"></span><br><span class="line">gen1 =my_gen()</span><br><span class="line">next(gen1)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_generator1</span><span class="params">()</span>:</span></span><br><span class="line">  <span class="keyword">yield</span> <span class="number">1</span></span><br><span class="line">  <span class="keyword">yield</span> <span class="number">2</span></span><br><span class="line">  <span class="keyword">yield</span> <span class="number">3</span></span><br><span class="line">  </span><br><span class="line">my_gen =my_generator1()</span><br><span class="line"><span class="comment"># 使用 next() 进行调用下一个</span></span><br><span class="line">next(my_gen)</span><br></pre></td></tr></table></figure></p>
<h3 id="recursion"><a href="#recursion" class="headerlink" title="recursion"></a>recursion</h3><p>A function calling itself is known as recursion.</p>
<h3 id="list-tuples-or-dictionary"><a href="#list-tuples-or-dictionary" class="headerlink" title="list, tuples, or dictionary"></a>list, tuples, or dictionary</h3><p>在python 中是使用频繁的data structure，这个是属于 collection 类别，里面放的是element.</p>
<p>list: to add/update/ delete an item of a collection </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">my_list.append(<span class="string">'C'</span>) <span class="comment">#adds at the end</span></span><br><span class="line">my_list[<span class="number">1</span>] = <span class="string">'D'</span> <span class="comment">#update</span></span><br><span class="line">my_list.pop(<span class="number">1</span>) <span class="comment"># removes</span></span><br><span class="line"></span><br><span class="line">mylist.pop() <span class="comment"># 默认就是类似 栈的结构，就是pop 出来最后一个</span></span><br><span class="line">mylist.pop(<span class="number">0</span>) <span class="comment"># 当然也可以根据index 指定特定的 pop(delete) 的element</span></span><br></pre></td></tr></table></figure>
<pre><code>or
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">del</span> mylist[<span class="number">1</span>:<span class="number">2</span>] <span class="comment"># 通过指定 index range 然后进行del</span></span><br><span class="line">mylist.sort() <span class="comment"># 支持 sorting 然后是从小到大, 这个sort是一种操作，inplace 的操作</span></span><br></pre></td></tr></table></figure>
<p>tuples: tuples store a sequence of objects, the object can be of any type. Tuples are faster than lists.</p>
<p>dictionary: It stores key/value pair objects.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">    my_dict =dict()</span><br><span class="line">    my_dict[<span class="string">'key'</span>] =<span class="string">'value'</span></span><br><span class="line">    <span class="keyword">or</span> </span><br><span class="line">    my_dict =&#123;<span class="string">'key'</span>: <span class="string">'value'</span>, ...&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> my_dict:</span><br><span class="line">    <span class="comment"># do something</span></span><br><span class="line">    <span class="keyword">if</span> <span class="string">'some key'</span> <span class="keyword">in</span> my_dict:</span><br><span class="line">    <span class="comment"># do something</span></span><br><span class="line">```    </span><br><span class="line"></span><br><span class="line"><span class="comment">### Iterators###</span></span><br><span class="line"></span><br><span class="line">```python</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">yrange</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, n)</span>:</span></span><br><span class="line">        self.i = <span class="number">0</span></span><br><span class="line">        self.n = n</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 这个表明是一个 iterator，make an object iterable</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__iter__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 这个next 函数就被当做是 class的属性，可以被外部调用的，</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">next</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> self.i &lt; self.n:</span><br><span class="line">            i = self.i</span><br><span class="line">            self.i += <span class="number">1</span></span><br><span class="line">            <span class="keyword">return</span> i</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> StopIteration()</span><br></pre></td></tr></table></figure>
<h3 id="shallow-vs-deep-copy"><a href="#shallow-vs-deep-copy" class="headerlink" title="shallow vs deep copy"></a>shallow vs deep copy</h3><p>python3 中：<br>对于简单的数据类型，像int ，string，这种 copy() 和copy.deepcopy() 这两者都是相同的，copy 都是一种映射，都是相当于”值“ 上的引用；<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">aa =<span class="number">2</span></span><br><span class="line">bb =aa</span><br><span class="line">print(id(aa), id(bb)) <span class="comment"># 相同</span></span><br><span class="line">bb =<span class="number">3</span></span><br><span class="line">print(id(aa), id(bb)) <span class="comment"># 不同，因为把3 这个值重新复制给了变量bb</span></span><br></pre></td></tr></table></figure></p>
<p>对于复杂的数据类型，使用deepcopy() 的时候，本来就是会重新拷贝一份到内存中。在python3 中copy() 和deepcopy() 这个是没有什么区别的。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">list1 =[<span class="string">'a'</span>, <span class="string">'b'</span>]</span><br><span class="line">list2 =list1 <span class="comment"># 这个是引用，所以和list1 是相同的</span></span><br><span class="line">list3 =copy.copy(list1)  <span class="comment"># 这个id 和list1 不同</span></span><br><span class="line">list4 =copy.deepcopy(list1)<span class="comment"># 这个id 和list1 不同 </span></span><br><span class="line">print(id(list1), id(list2), id(list3), id(list4))</span><br></pre></td></tr></table></figure></p>
<h3 id="object-oriented-design"><a href="#object-oriented-design" class="headerlink" title="object oriented design"></a>object oriented design</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">ParentClass</span>:</span></span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">my_function</span><span class="params">(self)</span>:</span></span><br><span class="line">       <span class="keyword">print</span> <span class="string">'I am here'</span></span><br><span class="line">    </span><br><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">SubClass1</span><span class="params">(ParentClass)</span>:</span> </span><br><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">SubClass2</span><span class="params">(ParentClass)</span>:</span> </span><br><span class="line">```    </span><br><span class="line"></span><br><span class="line">对于多继承的支持 （接口）</span><br><span class="line"></span><br><span class="line">```python</span><br><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">A</span><span class="params">(B,C)</span>:</span>  <span class="comment">#A implments B and C</span></span><br><span class="line">```    </span><br><span class="line"></span><br><span class="line">如果想要call parent <span class="class"><span class="keyword">class</span> <span class="title">function</span> <span class="title">then</span> <span class="title">you</span> <span class="title">can</span> <span class="title">dp</span>:</span></span><br><span class="line"></span><br><span class="line">```python</span><br><span class="line">    super(A, self).funcion_name()</span><br><span class="line">```    </span><br><span class="line">    </span><br><span class="line"><span class="comment">### garbage collection###</span></span><br><span class="line"></span><br><span class="line">all the objects  <span class="keyword">in</span> python are stored <span class="keyword">in</span> a heap space. Python has an <span class="keyword">in</span>-built garbage collection mechanism. Memory management <span class="keyword">in</span> Python involves a private heap containing all Python objects <span class="keyword">and</span> data structures. The management of this private heap <span class="keyword">is</span> ensured internally by the Python memory manager.</span><br><span class="line"></span><br><span class="line">In computer science, a heap <span class="keyword">is</span> a specialized tree-based data structure which <span class="keyword">is</span> essentially an almost complete[<span class="number">1</span>] tree that satisfies the heap property: <span class="keyword">if</span> P <span class="keyword">is</span> a parent node of C, then the key(the value) of P <span class="keyword">is</span> either greater than <span class="keyword">or</span> equal to (<span class="keyword">in</span> a max heap) <span class="keyword">or</span> less than <span class="keyword">or</span> equal to (<span class="keyword">in</span> a min heap) the key of C.</span><br><span class="line"></span><br><span class="line"><span class="comment">### try...catch###</span></span><br><span class="line"></span><br><span class="line">```python</span><br><span class="line">    <span class="comment"># raise exceptions</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">      <span class="keyword">raise</span> TyeError</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">      print(<span class="string">'exception'</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># catching exceptions</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">      do_something()</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">      print(<span class="string">'exception'</span>)</span><br><span class="line">    <span class="comment"># try/ catch /finally</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">      do_something()</span><br><span class="line">    <span class="keyword">except</span> TypeError:</span><br><span class="line">      print(<span class="string">'exception'</span>)</span><br><span class="line">    <span class="keyword">finally</span>:</span><br><span class="line">      close_connections()</span><br></pre></td></tr></table></figure>
<h2 id="Advanced-Features"><a href="#Advanced-Features" class="headerlink" title="Advanced Features"></a>Advanced Features</h2><p>Let’s move on to advanced features.</p>
<h3 id="Lambda-functions"><a href="#Lambda-functions" class="headerlink" title="Lambda functions"></a>Lambda functions</h3><p>A Lambda Function is a small, anonymous function — anonymous in the sense that it doesn’t actually have a name. A lambda function can take any number of arguments, but must always have only one expression:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = <span class="keyword">lambda</span> a, b : a * b</span><br><span class="line">	print(x(<span class="number">5</span>, <span class="number">6</span>)) <span class="comment"># prints '30' #  匿名函数也是函数，调用的时候使用这样的方式</span></span><br><span class="line">	x = <span class="keyword">lambda</span> a : a*<span class="number">3</span> + <span class="number">3</span></span><br><span class="line">	print(x(<span class="number">3</span>)) <span class="comment"># prints '12'</span></span><br></pre></td></tr></table></figure>
<h3 id="Maps"><a href="#Maps" class="headerlink" title="Maps"></a>Maps</h3><p>Map() is a built-in Python function used to apply a function to a sequence of elements like a list or dictionary. It’s a very clean and most importantly readable way to perform such an operation.<br>相对于 lambda, map 使用的频率更少了。 最后返回的是一个list。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">square_it_func</span><span class="params">(a)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> a * a</span><br><span class="line"></span><br><span class="line">x = map(square_it_func, [<span class="number">1</span>, <span class="number">4</span>, <span class="number">7</span>])</span><br><span class="line">print(x)  <span class="comment"># prints '[1, 16, 49]'</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">multiplier_func</span><span class="params">(a, b)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> a * b</span><br><span class="line"></span><br><span class="line">x = map(multiplier_func, [<span class="number">1</span>, <span class="number">4</span>, <span class="number">7</span>], [<span class="number">2</span>, <span class="number">5</span>, <span class="number">8</span>])</span><br><span class="line">print(x)  <span class="comment"># prints '[2, 20, 56]'</span></span><br></pre></td></tr></table></figure>
<h3 id="Filtering"><a href="#Filtering" class="headerlink" title="Filtering"></a>Filtering</h3><p>The Filter built-in function is quite similar to the Map function in that it applies a function to a sequence (list, tuple, dictionary). The key difference is that filter() will only return the elements which the applied function returned as True.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">   <span class="comment"># Our numbers</span></span><br><span class="line">numbers = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>]</span><br><span class="line"><span class="comment"># Function that filters out all numbers which are odd</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">filter_odd_numbers</span><span class="params">(num)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> num % <span class="number">2</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">True</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">False</span></span><br><span class="line">filtered_numbers = filter(filter_odd_numbers, numbers)</span><br><span class="line">print(filtered_numbers)</span><br><span class="line"><span class="comment"># filtered_numbers = [2, 4, 6, 8, 10, 12, 14]</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> itertools <span class="keyword">import</span> *</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">check_for_drop</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">print</span> (<span class="string">'Checking: '</span>, x)</span><br><span class="line">    <span class="keyword">return</span> (x &gt; <span class="number">5</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> dropwhile(check_for_drop, [<span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">8</span>, <span class="number">10</span>, <span class="number">12</span>]):</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">'Result: '</span>, i)</span><br></pre></td></tr></table></figure>
<h3 id="Itertools"><a href="#Itertools" class="headerlink" title="Itertools"></a>Itertools</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> itertools <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line"><span class="comment"># zip 就是一块访问的那种形式，返回的是一个tuple 数据类型</span></span><br><span class="line"><span class="comment"># zip ,joing two lists into a list of tuples</span></span><br><span class="line"><span class="comment"># Easy joining of two lists into a list of tuples</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> zip([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>]):</span><br><span class="line">    <span class="keyword">print</span> (i)</span><br><span class="line"><span class="comment"># ('a', 1)</span></span><br><span class="line"><span class="comment"># ('b', 2)</span></span><br><span class="line"><span class="comment"># ('c', 3)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 就是一个count() 计数功能</span></span><br><span class="line"><span class="comment"># The count() function returns an interator that </span></span><br><span class="line"><span class="comment"># produces consecutive integers, forever. This </span></span><br><span class="line"><span class="comment"># one is great for adding indices next to your list </span></span><br><span class="line"><span class="comment"># elements for readability and convenience</span></span><br><span class="line"><span class="comment"># in python3, no need to import izip, use zip directly</span></span><br><span class="line"><span class="comment"># 这个 count() 只有在这里才有意义，如果只是单独调用，没有感觉有多大的意义</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> zip(count(<span class="number">1</span>), [<span class="string">'Bob'</span>, <span class="string">'Emily'</span>, <span class="string">'Joe'</span>]):</span><br><span class="line">    <span class="keyword">print</span> (i)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># (1, 'Bob')</span></span><br><span class="line"><span class="comment"># (2, 'Emily')</span></span><br><span class="line"><span class="comment"># (3, 'Joe')    </span></span><br><span class="line"></span><br><span class="line"><span class="comment"># check ， becomes false for the first time 这个条件很关键, 可以理解成只是找到第一个false 的条件，然后就不再执行该函数</span></span><br><span class="line"><span class="comment"># The dropwhile() function returns an iterator that returns </span></span><br><span class="line"><span class="comment"># all the elements of the input which come after a certain </span></span><br><span class="line"><span class="comment"># condition becomes false for the first time. </span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">check_for_drop</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">print</span> <span class="string">'Checking: '</span>, x</span><br><span class="line">    <span class="keyword">return</span> (x &gt; <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> dropwhile(check_for_drop, [<span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">8</span>, <span class="number">10</span>, <span class="number">12</span>]):</span><br><span class="line">    <span class="keyword">print</span> <span class="string">'Result: '</span>, i</span><br><span class="line"><span class="comment"># 注意理解这个输出</span></span><br><span class="line"><span class="comment"># Checking: 2</span></span><br><span class="line"><span class="comment"># Result: 2</span></span><br><span class="line"><span class="comment"># Result: 4</span></span><br><span class="line"><span class="comment"># Result: 6</span></span><br><span class="line"><span class="comment"># Result: 8</span></span><br><span class="line"><span class="comment"># Result: 10</span></span><br><span class="line"><span class="comment"># Result: 12</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 我的理解这个 groupby  就和数据库中的groupby 是相同的效果</span></span><br><span class="line"><span class="comment"># The groupby() function is great for retrieving bunches</span></span><br><span class="line"><span class="comment"># of iterator elements which are the same or have similar </span></span><br><span class="line"><span class="comment"># properties</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> itertools <span class="keyword">import</span> groupby</span><br><span class="line">things = [(<span class="string">"animal"</span>, <span class="string">"bear"</span>), (<span class="string">"animal"</span>, <span class="string">"duck"</span>), (<span class="string">"plant"</span>, <span class="string">"cactus"</span>), (<span class="string">"vehicle"</span>, <span class="string">"speed boat"</span>), (<span class="string">"vehicle"</span>, <span class="string">"school bus"</span>)]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> key, group <span class="keyword">in</span> groupby(things, <span class="keyword">lambda</span> x: x[<span class="number">0</span>]):</span><br><span class="line">    <span class="keyword">for</span> thing <span class="keyword">in</span> group:</span><br><span class="line">        <span class="keyword">print</span> (<span class="string">"A %s is a %s."</span> % (thing[<span class="number">1</span>], key))</span><br><span class="line"></span><br><span class="line"><span class="comment">#A bear is a animal.</span></span><br><span class="line"><span class="comment">#A duck is a animal.</span></span><br><span class="line"><span class="comment">#A cactus is a plant.</span></span><br><span class="line"><span class="comment">#A speed boat is a vehicle.</span></span><br><span class="line"><span class="comment">#A school bus is a vehicle.</span></span><br></pre></td></tr></table></figure>
<h3 id="Generator"><a href="#Generator" class="headerlink" title="Generator"></a>Generator</h3><p>Generator functions allow you to declare a function that behaves like an iterator, i.e. it can be used in a for loop. This greatly simplifies your code and is much more memory efficient than a simple for loop.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">numbers = list()</span><br><span class="line"><span class="comment"># range()</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">    numbers.append(i + <span class="number">1</span>)</span><br><span class="line">total = sum(numbers)</span><br><span class="line"><span class="comment"># (2) Using a generator</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_numbers</span><span class="params">(n)</span>:</span></span><br><span class="line">    num = <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> num &lt; n:</span><br><span class="line">        <span class="keyword">yield</span> num</span><br><span class="line">        <span class="comment"># 这个yield 之后，函数并没有结束，不像 return 那种函数</span></span><br><span class="line">        num += <span class="number">1</span></span><br><span class="line">total = sum(generate_numbers(<span class="number">1000</span>))</span><br><span class="line">print(total)</span><br><span class="line">total = sum(range(<span class="number">1000</span> + <span class="number">1</span>))</span><br><span class="line">print(total)</span><br></pre></td></tr></table></figure>
<h2 id="List-Comprehension"><a href="#List-Comprehension" class="headerlink" title="List Comprehension"></a>List Comprehension</h2><p>常见的几种形式：<br><img src="http://ww1.sinaimg.cn/large/e9a223b5ly1g383ooxrfkj20kl01nmwy.jpg" alt=""><br>(An iterable is something you can loop over)<br><img src="http://ww1.sinaimg.cn/large/e9a223b5ly1g383oy3v7zj20l001lq2q.jpg" alt=""></p>
<p>list comprehensions vs loops:</p>
<ul>
<li>list comprehensions are more efficient both computationally and coding space </li>
<li>Every list comprehension can be rewritten as a for loop, but not every for loop can be rewritten as a list comprehension.</li>
</ul>
<p>从优化的角度 list comprehensions是优于 for loop 中的if else 操作的。因为前者是 predicatable pattern 是可以预测的。<br>However, keep in mind that list comprehensions are faster because they are optimized for the Python interpreter to spot a predictable pattern during looping.</p>
<p>a small code demo:在于使用功能 timeit libary 进行函数的计时比较。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> timeit</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">squares</span><span class="params">(size)</span>:</span></span><br><span class="line">    result = []</span><br><span class="line">    <span class="keyword">for</span> number <span class="keyword">in</span> range(size):</span><br><span class="line">        result.append(number * number)</span><br><span class="line">    <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">squares_comprehension</span><span class="params">(size)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> [number * number <span class="keyword">for</span> number <span class="keyword">in</span> range(size)]</span><br><span class="line">    </span><br><span class="line">print(timeit.timeit(<span class="string">"squares(50)"</span>, <span class="string">"from __main__ import squares"</span>, number=<span class="number">1</span>_000_000))</span><br><span class="line">print(timeit.timeit(<span class="string">"squares_comprehension(50)"</span>, <span class="string">"from __main__ import squares_comprehension"</span>, number=<span class="number">1</span>_000_000))</span><br></pre></td></tr></table></figure></p>
<p>more complex list comprehensions:<br><img src="http://ww1.sinaimg.cn/large/e9a223b5ly1g383s8zyjfj20ld01ht8i.jpg" alt=""></p>
<p>这种if 的写法 是两个进行并列的。其实可以写成</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">numbers = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">18</span>, <span class="number">20</span>]</span><br><span class="line">squares = [number <span class="keyword">for</span> number <span class="keyword">in</span> numbers <span class="keyword">if</span> number % <span class="number">2</span> == <span class="number">0</span> <span class="keyword">if</span> number % <span class="number">3</span> == <span class="number">0</span>]</span><br><span class="line"><span class="comment"># or</span></span><br><span class="line">squares = [number <span class="keyword">for</span> number <span class="keyword">in</span> numbers <span class="keyword">if</span> number % <span class="number">2</span> == <span class="number">0</span> <span class="keyword">and</span> number % <span class="number">3</span> == <span class="number">0</span>]</span><br><span class="line">print(squares)</span><br><span class="line"><span class="comment"># output: [6, 18]</span></span><br></pre></td></tr></table></figure>
<p>在 output expression 中，也是可以使用 if else 进行进一步输出筛选。<br><img src="http://ww1.sinaimg.cn/large/e9a223b5ly1g383vngz5jj20ma01m742.jpg" alt=""><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">numbers = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">18</span>, <span class="number">20</span>]</span><br><span class="line">	squares = [<span class="string">"small"</span> <span class="keyword">if</span> number &lt; <span class="number">10</span> <span class="keyword">else</span> <span class="string">"big"</span> <span class="keyword">for</span> number <span class="keyword">in</span> numbers <span class="keyword">if</span> number % <span class="number">2</span> == <span class="number">0</span> <span class="keyword">if</span> number % <span class="number">3</span> == <span class="number">0</span>]</span><br><span class="line">	print(squares)</span><br><span class="line"></span><br><span class="line">ouput: [<span class="string">'small'</span>, <span class="string">'big'</span>]</span><br></pre></td></tr></table></figure></p>
<p>converting nested loops into list comprehension<br>代码功能： 都是把二维的 matrix 转成了一个 list （flattened）<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">matrix = [[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]]</span><br><span class="line">flattened = []</span><br><span class="line"><span class="keyword">for</span> row <span class="keyword">in</span> matrix:</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> row:</span><br><span class="line">        flattened.append(item)</span><br><span class="line">print(flattened)</span><br></pre></td></tr></table></figure></p>
<p>注意这个顺序，先是row in matrix 然后是 item in row.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">	matrix = [[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]]</span><br><span class="line">flattened = [item <span class="keyword">for</span> row <span class="keyword">in</span> matrix <span class="keyword">for</span> item <span class="keyword">in</span> row]   </span><br><span class="line">print(flattened)</span><br></pre></td></tr></table></figure></p>
<p>ouput matric from nested list comprehensions:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">matrix = [[item <span class="keyword">for</span> item <span class="keyword">in</span> range(<span class="number">5</span>)] <span class="keyword">for</span> row <span class="keyword">in</span> range(<span class="number">3</span>)]</span><br><span class="line">print(matrix)</span><br></pre></td></tr></table></figure></p>
<p>对于 dictionary 的支持： 主要是 dict1.items() 和 key, value 的使用<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">prices = &#123;<span class="string">"beer"</span>: <span class="number">2</span>, <span class="string">"fish"</span>: <span class="number">5</span>, <span class="string">"apple"</span>: <span class="number">1</span>&#125;</span><br><span class="line">float_prices = &#123;key:float(value) <span class="keyword">for</span> key, value <span class="keyword">in</span> prices.items()&#125;</span><br><span class="line">print(float_prices)</span><br></pre></td></tr></table></figure></p>
<p>从代码的角度，可以看出，操作和最后的返回的形式是没有很大的关系，上面是 [], 这个是 {}, 分别对应的是 list 和 set 两种不同的格式。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">numbers = [<span class="number">10</span>, <span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span>, <span class="number">12</span>, <span class="number">-20</span>, <span class="number">0</span>, <span class="number">1</span>]</span><br><span class="line">unique_squares = &#123;number**<span class="number">2</span> <span class="keyword">for</span> number <span class="keyword">in</span> numbers&#125;</span><br><span class="line">print(unique_squares)</span><br></pre></td></tr></table></figure></p>
<h2 id="Working-with-CSV-Json-and-XML"><a href="#Working-with-CSV-Json-and-XML" class="headerlink" title="Working with CSV, Json and XML"></a>Working with CSV, Json and XML</h2><p>Over the years, the list of possible formats that you can store your data in has grown significantly. But, there are 3 that dominate in their everyday usage: CSV, JSON, and XML. In this article, I’m going to share with you the easiest ways to work with these 3 popular data formats in Python!</p>
<p>有两种方式去读写 csv file：一种是 pd.read_csv() ，一种是built-in 的library 中的库函数<br>之前一直使用的pd.read_csv(), 现在才发现python 有built-in 的library。<br>We can do both read and write of a CSV using the built-in Python csv library. Usually, we’ll read the data into a list of lists.</p>
<p>python in-built function.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> csv </span><br><span class="line">filename = <span class="string">"my_data.csv"</span></span><br><span class="line">fields = [] </span><br><span class="line">rows = [] </span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(filename, <span class="string">'r'</span>) <span class="keyword">as</span> csvfile: </span><br><span class="line">    csvreader = csv.reader(csvfile) </span><br><span class="line">    <span class="comment"># 如果单单是这个for，那么内存是消耗比较大的</span></span><br><span class="line">    <span class="comment"># fields = csvreader.next() </span></span><br><span class="line">    <span class="keyword">for</span> row <span class="keyword">in</span> csvreader: </span><br><span class="line">        rows.append(row)</span><br><span class="line"><span class="keyword">for</span> row <span class="keyword">in</span> rows[:<span class="number">5</span>]: </span><br><span class="line">    print(row)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Writing to csv file </span></span><br><span class="line"><span class="keyword">with</span> open(filename, <span class="string">'w+'</span>) <span class="keyword">as</span> csvfile: </span><br><span class="line">    csvwriter = csv.writer(csvfile) </span><br><span class="line">    csvwriter.writerow(fields) </span><br><span class="line">    csvwriter.writerows(rows)</span><br></pre></td></tr></table></figure></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> dicttoxml <span class="keyword">import</span> dicttoxml</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="comment"># Building our dataframe</span></span><br><span class="line">data = &#123;<span class="string">'Name'</span>: [<span class="string">'Emily'</span>, <span class="string">'Katie'</span>, <span class="string">'John'</span>, <span class="string">'Mike'</span>],</span><br><span class="line">        <span class="string">'Goals'</span>: [<span class="number">12</span>, <span class="number">8</span>, <span class="number">16</span>, <span class="number">3</span>],</span><br><span class="line">        <span class="string">'Assists'</span>: [<span class="number">18</span>, <span class="number">24</span>, <span class="number">9</span>, <span class="number">14</span>],</span><br><span class="line">        <span class="string">'Shots'</span>: [<span class="number">112</span>, <span class="number">96</span>, <span class="number">101</span>, <span class="number">82</span>]</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">df = pd.DataFrame(data, columns=data.keys())</span><br><span class="line"><span class="comment"># Converting the dataframe to a dictionary</span></span><br><span class="line"><span class="comment"># Then save it to file</span></span><br><span class="line">data_dict = df.to_dict(orient=<span class="string">"records"</span>)</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'output.json'</span>, <span class="string">"w+"</span>) <span class="keyword">as</span> f:</span><br><span class="line">    json.dump(data_dict, f, indent=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Converting the dataframe to XML</span></span><br><span class="line"><span class="comment"># Then save it to file</span></span><br><span class="line">xml_data = dicttoxml(data_dict).decode()</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">"output.xml"</span>, <span class="string">"w+"</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(xml_data)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="comment"># Read the data from file</span></span><br><span class="line"><span class="comment"># We now have a Python dictionary</span></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'data.json'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    data_listofdict = json.load(f)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># We can do the same thing with pandas</span></span><br><span class="line">data_df = pd.read_json(<span class="string">'data.json'</span>, orient=<span class="string">'records'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># We can write a dictionary to JSON like so</span></span><br><span class="line"><span class="comment"># Use 'indent' and 'sort_keys' to make the JSON</span></span><br><span class="line"><span class="comment"># file look nice</span></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'new_data.json'</span>, <span class="string">'w+'</span>) <span class="keyword">as</span> json_file:</span><br><span class="line">    json.dump(data_listofdict, json_file, indent=<span class="number">4</span>, sort_keys=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># And again the same thing with pandas</span></span><br><span class="line">export = data_df.to_json(<span class="string">'new_data.json'</span>, orient=<span class="string">'records'</span>)</span><br></pre></td></tr></table></figure>
<p>参考资料：<br><a href="https://towardsdatascience.com/the-easy-way-to-work-with-csv-json-and-xml-in-python-5056f9325ca9" target="_blank" rel="noopener">https://towardsdatascience.com/the-easy-way-to-work-with-csv-json-and-xml-in-python-5056f9325ca9</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/04/06/The-evaluation-of-sentence-similarity/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jijeng Jia">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jijeng's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/04/06/The-evaluation-of-sentence-similarity/" itemprop="url">The Evaluation of Sentence Similarity</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-04-06T15:11:01+08:00">
                2019-04-06
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index">
                    <span itemprop="name">NLP</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/04/06/The-evaluation-of-sentence-similarity/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2019/04/06/The-evaluation-of-sentence-similarity/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>I am trying to write my first english blog based on two reasons: First, the data set used in this blog is english; Second, I’d like to expand my reach and attract more audiences, although I should admit that nobody cares.</p>
<h2 id="Data"><a href="#Data" class="headerlink" title="Data"></a>Data</h2><p>Initially I want to use chinese corpus, but I cannot find a proper one. The data should sound like this one:</p>
<blockquote>
<p>word1    word2    similarity score<br>阿拉伯人    阿拉伯    7.2<br>畜产    农业    5.6<br>垂涎    崇敬    3.4<br>次序    秩序    4.7<br>定心丸    药品    4.3<br>房租    价格    5.2<br>翡翠    宝石    6.7<br>高科技    技术    7.5<br>购入    购买    8.5<br>观音    菩萨    8.2<br>归并    合并    7.7</p>
</blockquote>
<!-- come from https://biendata.com/ccf_tcci2018/datasets/tcci_tag/5 -->
<p>not like this:</p>
<blockquote>
<p>为何我无法申请开通花呗信用卡收款    支付宝开通信用卡花呗收款不符合条件怎么回事    1<br>花呗分期付款会影响使用吗    花呗分期有什么影响吗    0<br>为什么我花呗没有临时额度    花呗没有临时额度怎么可以负    0<br>能不能开花呗老兄    花呗逾期了还能开通    0<br>我的怎么开通花呗收钱    这个花呗是个什么啥？我没开通 我怎么有账单    0<br>蚂蚁借呗可以停掉么    蚂蚁借呗为什么给我关掉了    0<br>我想把花呗功能关了    我去饭店吃饭，能用花呗支付吗    0<br>为什么我借呗开通了又关闭了    为什么借呗存在风险    0<br>支付宝被冻了花呗要怎么还    支付功能冻结了，花呗还不了怎么办    1</p>
</blockquote>
<p>If you can find the dataset where ‘similarity score’ is double, please donot hesitate to <a href="mailto:jiajizhengbuaa@gmail.com" target="_blank" rel="noopener">email me.</a></p>
<p>So, the choice has to be enlgish corpus. The dataset used in this experiment are <a href="http://ixa2.si.ehu.es/stswiki/index.php/STSbenchmark" target="_blank" rel="noopener">STSbenchmark</a> and SICK data. The SICK data contains 10,000 sentence paris labeled with semantic relatedness and entailment relation.<br><img src="http://ww1.sinaimg.cn/large/e9a223b5ly1g1sqxx3vl0j219e07yaam.jpg" alt=""></p>
<h2 id="Similarity-Methods"><a href="#Similarity-Methods" class="headerlink" title="Similarity Methods"></a>Similarity Methods</h2><h3 id="Baseline"><a href="#Baseline" class="headerlink" title="Baseline"></a>Baseline</h3><p>As the baseline, we just take the embedding of the words in sentence, and compute the average, weighted by frequency of each word.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_avg_benchmark</span><span class="params">(sentences1, sentences2, model=None, use_stoplist=False, doc_freqs=None)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> doc_freqs <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">        N = doc_freqs[<span class="string">"NUM_DOCS"</span>]</span><br><span class="line"></span><br><span class="line">    sims = []</span><br><span class="line">    <span class="keyword">for</span> (sent1, sent2) <span class="keyword">in</span> zip(sentences1, sentences2):</span><br><span class="line"></span><br><span class="line">        tokens1 = sent1.tokens_without_stop <span class="keyword">if</span> use_stoplist <span class="keyword">else</span> sent1.tokens</span><br><span class="line">        tokens2 = sent2.tokens_without_stop <span class="keyword">if</span> use_stoplist <span class="keyword">else</span> sent2.tokens</span><br><span class="line"></span><br><span class="line">        tokens1 = [token <span class="keyword">for</span> token <span class="keyword">in</span> tokens1 <span class="keyword">if</span> token <span class="keyword">in</span> model]</span><br><span class="line">        tokens2 = [token <span class="keyword">for</span> token <span class="keyword">in</span> tokens2 <span class="keyword">if</span> token <span class="keyword">in</span> model]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> len(tokens1) == <span class="number">0</span> <span class="keyword">or</span> len(tokens2) == <span class="number">0</span>:</span><br><span class="line">            sims.append(<span class="number">0</span>)</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">        tokfreqs1 = Counter(tokens1)</span><br><span class="line">        tokfreqs2 = Counter(tokens2)</span><br><span class="line"></span><br><span class="line">        weights1 = [tokfreqs1[token] * math.log(N / (doc_freqs.get(token, <span class="number">0</span>) + <span class="number">1</span>))</span><br><span class="line">                    <span class="keyword">for</span> token <span class="keyword">in</span> tokfreqs1] <span class="keyword">if</span> doc_freqs <span class="keyword">else</span> <span class="keyword">None</span></span><br><span class="line">        weights2 = [tokfreqs2[token] * math.log(N / (doc_freqs.get(token, <span class="number">0</span>) + <span class="number">1</span>))</span><br><span class="line">                    <span class="keyword">for</span> token <span class="keyword">in</span> tokfreqs2] <span class="keyword">if</span> doc_freqs <span class="keyword">else</span> <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">        embedding1 = np.average([model[token] <span class="keyword">for</span> token <span class="keyword">in</span> tokfreqs1], axis=<span class="number">0</span>, weights=weights1).reshape(<span class="number">1</span>, <span class="number">-1</span>)</span><br><span class="line">        embedding2 = np.average([model[token] <span class="keyword">for</span> token <span class="keyword">in</span> tokfreqs2], axis=<span class="number">0</span>, weights=weights2).reshape(<span class="number">1</span>, <span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">        sim = cosine_similarity(embedding1, embedding2)[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line">        sims.append(sim)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> sims</span><br></pre></td></tr></table></figure>
<h3 id="Smooth-Inverse-Frequency"><a href="#Smooth-Inverse-Frequency" class="headerlink" title="Smooth Inverse Frequency"></a>Smooth Inverse Frequency</h3><p>The baseline, like we did before, is very simple and crude of computing sentence embedding. Word frequency cannot reliably reflect its importance to sentence, semantically speaking. Smooth Inverse Frequency (SIF) tries to solve this problem.</p>
<ul>
<li>SIF is very similar to the weighted average we used before, with the difference that it’s weighted by this formular.</li>
</ul>
<p>$$<br>\operatorname { SIF } ( w ) = \frac { a } { ( a + p ( w ) )}<br>$$<br>where $a$ is a hyper-parameter (set to 0.001 by default) and $ p(w)$ is the estimated word frequency in the corpus. </p>
<ul>
<li>we need to perform common component removal: subtract from the sentence embedding obtained above the first principal component of the matrix. This corrects for the influence of high-frequency words that have syntactic or dicourse function, such as ‘but’, ‘and’, etc. You can find more information from <a href="https://openreview.net/pdf?id=SyK00v5xx" target="_blank" rel="noopener">this paper</a>.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">remove_first_principal_component</span><span class="params">(X)</span>:</span></span><br><span class="line">    svd = TruncatedSVD(n_components=<span class="number">1</span>, n_iter=<span class="number">7</span>, random_state=<span class="number">0</span>)</span><br><span class="line">    svd.fit(X)</span><br><span class="line">    pc = svd.components_</span><br><span class="line">    XX = X - X.dot(pc.transpose()) * pc</span><br><span class="line">    <span class="keyword">return</span> XX</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_sif_benchmark</span><span class="params">(sentences1, sentences2, model, freqs=&#123;&#125;, use_stoplist=False, a=<span class="number">0.001</span>)</span>:</span></span><br><span class="line">    total_freq = sum(freqs.values())</span><br><span class="line"></span><br><span class="line">    embeddings = []</span><br><span class="line"></span><br><span class="line">    <span class="comment"># SIF requires us to first collect all sentence embeddings and then perform</span></span><br><span class="line">    <span class="comment"># common component analysis.</span></span><br><span class="line">    <span class="keyword">for</span> (sent1, sent2) <span class="keyword">in</span> zip(sentences1, sentences2):</span><br><span class="line">        tokens1 = sent1.tokens_without_stop <span class="keyword">if</span> use_stoplist <span class="keyword">else</span> sent1.tokens</span><br><span class="line">        tokens2 = sent2.tokens_without_stop <span class="keyword">if</span> use_stoplist <span class="keyword">else</span> sent2.tokens</span><br><span class="line"></span><br><span class="line">        tokens1 = [token <span class="keyword">for</span> token <span class="keyword">in</span> tokens1 <span class="keyword">if</span> token <span class="keyword">in</span> model]</span><br><span class="line">        tokens2 = [token <span class="keyword">for</span> token <span class="keyword">in</span> tokens2 <span class="keyword">if</span> token <span class="keyword">in</span> model]</span><br><span class="line"></span><br><span class="line">        weights1 = [a / (a + freqs.get(token, <span class="number">0</span>) / total_freq) <span class="keyword">for</span> token <span class="keyword">in</span> tokens1]</span><br><span class="line">        weights2 = [a / (a + freqs.get(token, <span class="number">0</span>) / total_freq) <span class="keyword">for</span> token <span class="keyword">in</span> tokens2]</span><br><span class="line"></span><br><span class="line">        embedding1 = np.average([model[token] <span class="keyword">for</span> token <span class="keyword">in</span> tokens1], axis=<span class="number">0</span>, weights=weights1)</span><br><span class="line">        embedding2 = np.average([model[token] <span class="keyword">for</span> token <span class="keyword">in</span> tokens2], axis=<span class="number">0</span>, weights=weights2)</span><br><span class="line"></span><br><span class="line">        embeddings.append(embedding1)</span><br><span class="line">        embeddings.append(embedding2)</span><br><span class="line"></span><br><span class="line">    embeddings = remove_first_principal_component(np.array(embeddings))</span><br><span class="line">    sims = [cosine_similarity(embeddings[idx * <span class="number">2</span>].reshape(<span class="number">1</span>, <span class="number">-1</span>),</span><br><span class="line">                              embeddings[idx * <span class="number">2</span> + <span class="number">1</span>].reshape(<span class="number">1</span>, <span class="number">-1</span>))[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line">            <span class="keyword">for</span> idx <span class="keyword">in</span> range(int(len(embeddings) / <span class="number">2</span>))]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> sims</span><br></pre></td></tr></table></figure>
<h3 id="Google-Sentence-Encoder"><a href="#Google-Sentence-Encoder" class="headerlink" title="Google Sentence Encoder"></a>Google Sentence Encoder</h3><p><a href="https://github.com/facebookresearch/InferSent" target="_blank" rel="noopener">InferSent</a> is a pre-trained encoder that produces sentence embedding, which opensourced by Facebook. <a href="https://tfhub.dev/google/universal-sentence-encoder/1" target="_blank" rel="noopener">The Google Sentence Encoder</a> is Google’s answer to Facebook’s InferSent. In contrast to InferSent, the Google Sentence Encoder was trained on a combination of unsupervised data and supervised data (SNLI corpus), which tends to give better results.</p>
<p>The codes can be used in <a href="https://colab.research.google.com/notebooks/welcome.ipynb#recent=true" target="_blank" rel="noopener">Google Jupyter Notebook</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow_hub <span class="keyword">as</span> hub</span><br><span class="line"></span><br><span class="line">tf.logging.set_verbosity(tf.logging.ERROR)</span><br><span class="line">embed = hub.Module(<span class="string">"https://tfhub.dev/google/universal-sentence-encoder/1"</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_gse_benchmark</span><span class="params">(sentences1, sentences2)</span>:</span></span><br><span class="line">    sts_input1 = tf.placeholder(tf.string, shape=(<span class="keyword">None</span>))</span><br><span class="line">    sts_input2 = tf.placeholder(tf.string, shape=(<span class="keyword">None</span>))</span><br><span class="line"></span><br><span class="line">    sts_encode1 = tf.nn.l2_normalize(embed(sts_input1))</span><br><span class="line">    sts_encode2 = tf.nn.l2_normalize(embed(sts_input2))</span><br><span class="line"></span><br><span class="line">    sim_scores = tf.reduce_sum(tf.multiply(sts_encode1, sts_encode2), axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> session:</span><br><span class="line">        session.run(tf.global_variables_initializer())</span><br><span class="line">        session.run(tf.tables_initializer())</span><br><span class="line"></span><br><span class="line">        [gse_sims] = session.run(</span><br><span class="line">            [sim_scores],</span><br><span class="line">            feed_dict=&#123;</span><br><span class="line">                sts_input1: [sent1.raw <span class="keyword">for</span> sent1 <span class="keyword">in</span> sentences1],</span><br><span class="line">                sts_input2: [sent2.raw <span class="keyword">for</span> sent2 <span class="keyword">in</span> sentences2]</span><br><span class="line">            &#125;)</span><br><span class="line">    <span class="keyword">return</span> gse_sims</span><br></pre></td></tr></table></figure>
<h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_experiment</span><span class="params">(df, benchmarks)</span>:</span></span><br><span class="line">    sentences1 = [Sentence(s) <span class="keyword">for</span> s <span class="keyword">in</span> df[<span class="string">'sent_1'</span>]]</span><br><span class="line">    sentences2 = [Sentence(s) <span class="keyword">for</span> s <span class="keyword">in</span> df[<span class="string">'sent_2'</span>]]</span><br><span class="line"></span><br><span class="line">    pearson_cors, spearman_cors = [], []</span><br><span class="line">    <span class="keyword">for</span> label, method <span class="keyword">in</span> benchmarks:</span><br><span class="line">        sims = method(sentences1, sentences2)</span><br><span class="line">        pearson_correlation = scipy.stats.pearsonr(sims, df[<span class="string">'sim'</span>])[<span class="number">0</span>]</span><br><span class="line">        print(label, pearson_correlation)</span><br><span class="line">        pearson_cors.append(pearson_correlation)</span><br><span class="line">        spearman_correlation = scipy.stats.spearmanr(sims, df[<span class="string">'sim'</span>])[<span class="number">0</span>]</span><br><span class="line">        spearman_cors.append(spearman_correlation)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> pearson_cors, spearman_cors</span><br></pre></td></tr></table></figure>
<p>Helper function:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> functools <span class="keyword">as</span> ft</span><br><span class="line"></span><br><span class="line">benchmarks = [</span><br><span class="line">    (<span class="string">"AVG-GLOVE"</span>, ft.partial(run_avg_benchmark, model=glove, use_stoplist=<span class="keyword">False</span>)),</span><br><span class="line">    (<span class="string">"AVG-GLOVE-STOP"</span>, ft.partial(run_avg_benchmark, model=glove, use_stoplist=<span class="keyword">True</span>)),</span><br><span class="line">    (<span class="string">"AVG-GLOVE-TFIDF"</span>, ft.partial(run_avg_benchmark, model=glove, use_stoplist=<span class="keyword">False</span>, doc_freqs=doc_frequencies)),</span><br><span class="line">    (<span class="string">"AVG-GLOVE-TFIDF-STOP"</span>, ft.partial(run_avg_benchmark, model=glove, use_stoplist=<span class="keyword">True</span>, doc_freqs=doc_frequencies)),</span><br><span class="line">    (<span class="string">"SIF-W2V"</span>, ft.partial(run_sif_benchmark, freqs=frequencies, model=word2vec, use_stoplist=<span class="keyword">False</span>)),</span><br><span class="line">    (<span class="string">"SIF-GLOVE"</span>, ft.partial(run_sif_benchmark, freqs=frequencies, model=glove, use_stoplist=<span class="keyword">False</span>)),</span><br><span class="line"></span><br><span class="line">]</span><br></pre></td></tr></table></figure>
<h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.rcParams[<span class="string">'figure.figsize'</span>] = (<span class="number">20</span>,<span class="number">13</span>)</span><br><span class="line">spearman[[<span class="string">'AVG-GLOVE'</span>, <span class="string">'AVG-GLOVE-STOP'</span>,<span class="string">'AVG-GLOVE-TFIDF'</span>, <span class="string">'AVG-GLOVE-TFIDF-STOP'</span>,<span class="string">'GSE'</span>]].plot(kind=<span class="string">"bar"</span>).legend(loc=<span class="string">"lower left"</span>)</span><br></pre></td></tr></table></figure>
<p><strong>Take Off</strong></p>
<ul>
<li>Smooth Inverse Frequency methods are better than baseline, no matter with word2vec or Glove embeddings.</li>
<li>Google Sentence Encoder has the similar performance as Smooth Inverse Frequency.</li>
<li>Using tf-idf weights does not help and using a stoplist looks like a reasonable choice.</li>
</ul>
<p>Pearson Correlation<br><img src="http://ww1.sinaimg.cn/large/e9a223b5ly1g1swpqj9d0j20ps0h5t8z.jpg" alt=""><br>Spearman Correlation<br><img src="http://ww1.sinaimg.cn/large/e9a223b5ly1g1swpqj9d0j20ps0h5t8z.jpg" alt=""></p>
<p>Full codes can be found in <a href="https://github.com/jijeng/sentence-similarity" target="_blank" rel="noopener">here</a>.</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/26/深度网络中的碎碎念/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jijeng Jia">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jijeng's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/26/深度网络中的碎碎念/" itemprop="url">Weights Initialization & Activation Function & Optimizer</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-03-26T10:28:14+08:00">
                2019-03-26
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Deep-learning/" itemprop="url" rel="index">
                    <span itemprop="name">Deep learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/03/26/深度网络中的碎碎念/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2019/03/26/深度网络中的碎碎念/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>主要介绍了常见的网络中权重的初始化，激活函数和优化器。</p>
<h2 id="Weights-Initialization"><a href="#Weights-Initialization" class="headerlink" title="Weights Initialization"></a>Weights Initialization</h2><p>weights 的初始化在网络的训练起到重要的作用，初始化的好坏能够直接影响到网络是否可以正常收敛。<br>这里的初始化都是指的是weights初始化。bias 这个变量就是在企图去描述真实的分布，通过引入随机性来表示这个是具有 推广性的。</p>
<blockquote>
<p>Here’s another trick — before squishing our scalar value (called an activation) into the sigmoid function, we can add a little number called the bias, b. There will be two biases, one for each user and one for each movie.</p>
</blockquote>
<h3 id="Random-Initialization"><a href="#Random-Initialization" class="headerlink" title="Random Initialization"></a>Random Initialization</h3><p>最基础的即使 bias 使用 zero initialization ，然后 weights 使用 random initialzation。这种方法的缺陷在于梯度消失。就是你的weights 如果很大或者很小的时候，再加上如果使用了sigmoid 那么很容易出现上述的现象。</p>
<blockquote>
<p>a) If weights are initialized with very high values the term np.dot(W,X)+bbecomes significantly higher and if an activation function like sigmoid() is applied, the function maps its value near to 1 where slope of gradient changes slowly and learning takes a lot of time.<br>b) If weights are initialized with low values it gets mapped to 0, where the case is same as above.</p>
</blockquote>
<h3 id="He-Initialization"><a href="#He-Initialization" class="headerlink" title="He Initialization"></a>He Initialization</h3><p>$$W^{[l]} =np.random.randn(size_l, size_l -1)* np.sqrt(2/size_l -1) $$<br>这个是使用 relu 或者说 leaky relu 配合使用的。</p>
<p>所以上述的初始化从数学的角度去理解： 从(0,1) 标准正太分布 转换成了 (0, np.sqrt(2/ size_l -1)) 这样的分布，就是你网络结构是越宽，那么这个方差就是越小的，最后的结果是越集中的，就越集中的 均值 u 左右。从图像的角度看，方差越大，图像越矮胖；方差越小，图像越瘦高。</p>
<h3 id="Xavier-initialization-ˈzeɪvjər-s"><a href="#Xavier-initialization-ˈzeɪvjər-s" class="headerlink" title="Xavier initialization /ˈzeɪvjər/s"></a>Xavier initialization /ˈzeɪvjər/s</h3><p>这个是使用tanh() 作为 activation function的。<br>$$W^{[l]} =np.random.randn(size_l, size_l -1)* np.sqrt( 1/size_l -1) $$<br>总的思想原则：<br>They set the weights neither too much bigger that 1, nor too much less than 1.<br>就是本来就是在 (0,1) 标准正太分布出来，然后进行了进一步的约束条件。</p>
<h2 id="Activation-function"><a href="#Activation-function" class="headerlink" title="Activation function"></a>Activation function</h2><p>总的说可以分为线性和非线性的激活函数， activation function 的作用 就是对于网络的输出 说yes or no. It maps the resulting values in between 0 to 1 or -1 to 1 etc.</p>
<h3 id="Sigmoid-function-Logistic-Activation"><a href="#Sigmoid-function-Logistic-Activation" class="headerlink" title="Sigmoid function (Logistic Activation)"></a>Sigmoid function (Logistic Activation)</h3><p>the only reason why we use sigmoid is because it exists between 0 to 1. 这个非常有利于 predict probability. 因为自然映射到 0 是不存在 然后1 是存在。而当多分类的时候，使用softmax。</p>
<h3 id="Tanh-function"><a href="#Tanh-function" class="headerlink" title="Tanh function"></a>Tanh function</h3><blockquote>
<p>The advantage is that the negative inputs will be mapped strongly negative and the zero inputs will be mapped near zero in the tanh graph.</p>
</blockquote>
<h3 id="Relu-Rectified-Linear-Unit-Activation"><a href="#Relu-Rectified-Linear-Unit-Activation" class="headerlink" title="Relu (Rectified Linear Unit) Activation"></a>Relu (Rectified Linear Unit) Activation</h3><p>本质上是分段函数。 range: [0, infinity]. The function and its derivative both are monotonic.</p>
<h3 id="Leaky-Relu"><a href="#Leaky-Relu" class="headerlink" title="Leaky Relu"></a>Leaky Relu</h3><p>每当一个伟大的东西产出，总会伴随着比较明显的错误，然后紧跟着就是有一个  rectified(improved) 这种版本。这个相比之前就是修正了 当输入是负数的时候，怎么办的问题。</p>
<h3 id="Softmax"><a href="#Softmax" class="headerlink" title="Softmax"></a>Softmax</h3><p>这个需要从数学的角度去理解，数学公式就是下面。有一个真实的例子： 如果一张”4” 的图片，输出一个网络，最后是softmax 激活函数，那么最后得到的 “4” 的概率是最大的，但是也是有其他的可能性存在的，这个就是softmax 的效果。 最主要的功能是 convert number into probabilities. 这种效果，不像sigmoid 那样有很明确的数学公式。</p>
<p>$$\sigma ( z ) _ { j } = \frac { e ^ { z _ { j } } } { \sum _ { k = 1 } ^ { K } e ^ { z _ { k } } }$$</p>
<blockquote>
<p>Softmax function, a wonderfulactivation function that turns numbers aka logits into probabilities that sum to one.<br>Remember the takeaway is: the essential goal of softmax is to turn numbers into probabilities.</p>
</blockquote>
<h2 id="Optimizer"><a href="#Optimizer" class="headerlink" title="Optimizer"></a>Optimizer</h2><h3 id="Gradient-Descent"><a href="#Gradient-Descent" class="headerlink" title="Gradient Descent"></a>Gradient Descent</h3><p>最基本的应该是 gradient descent，这个算是开山鼻祖。给出了每次梯度变化的方向</p>
<h3 id="Momentum"><a href="#Momentum" class="headerlink" title="Momentum"></a>Momentum</h3><p>个人更喜欢将其翻译成动量，这个gradient 考虑之前的gradient 的方向和大小，有点像动量的的含义。不仅考虑了current step, 而且accumulates gradient of the past steps.</p>
<blockquote>
<p> A very popular technique that is used along with SGD is called Momentum. Instead of using only the gradient of the current step to guide the search, momentum also accumulates the gradient of the past steps to determine the direction to go. </p>
</blockquote>
<p>好处在于:</p>
<blockquote>
<p>most recent is weighted than the less recent ones<br>the weightage of the most recent previous gradients is more than the less recent ones.<br>for example:</p>
</blockquote>
<p><img src="http://ww1.sinaimg.cn/large/e9a223b5ly1g1fyli4wtkj20nk04ldfr.jpg" alt=""></p>
<h3 id="RMSprop"><a href="#RMSprop" class="headerlink" title="RMSprop"></a>RMSprop</h3><p>RMSprop, or Root Mean Square Propogation has an interesting history. RMSProp also tries to dampen the oscillations, but in a different way than momentum. </p>
<p><img src="http://ww1.sinaimg.cn/large/e9a223b5ly1g1fz79596rj20fu0boaad.jpg" alt=""></p>
<h3 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h3><p>这种套路总是一样的，总是有一个中和者，关键是如何整合到一起。</p>
<blockquote>
<p>Adam or Adaptive Moment Optimization  algorithms combines the heuristics of both Momentum and RMSProp.</p>
</blockquote>
<p><img src="http://ww1.sinaimg.cn/large/e9a223b5ly1g1fzc05tnrj20cy08u74e.jpg" alt=""></p>
<p>The hyperparameter beta1 is generally kept around 0.9 while beta_2 is kept at 0.99. Epsilon is chosen to be 1e-10 generally.</p>
<p>对于公式的解释，Eq 1 and Eq 2是come from RMSprop,  Eq 3 comes from Adam。最后一步是 update 操作，条理清楚，没有问题。(公式中的v 表示导数，g 表示导数e 不太清楚)<br>Here, we compute the exponential average of the gradient as well as the squares of the gradient for each parameters (Eq 1, and Eq 2). To decide our learning step, we multiply our learning rate by average of the gradient (as was the case with momentum) and divide it by the root mean square of the exponential average of square of gradients (as was the case with momentum) in equation 3. Then, we add the update.)</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/26/siamese-network/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jijeng Jia">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jijeng's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/26/siamese-network/" itemprop="url">siamese network</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-03-26T10:26:03+08:00">
                2019-03-26
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Deep-learning/" itemprop="url" rel="index">
                    <span itemprop="name">Deep learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/03/26/siamese-network/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2019/03/26/siamese-network/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>主要是介绍自己论文中的网络结构：siamese network。</p>
<p>但凡优化，无非两条路：在好的基础上更快，在快的基础上效果更好。 Siamese network训练速度快，所以只是需要其在训练效果上更好。</p>
<p>先来分析一下为什么训练速度快。那么不得不分析该网络结构。整个网络的输入是 (img1, img2, y) 这样的三元组，img 表示图片，y表示label。图片可以是同一类别的图片，也可以是不同类别的，y表示两张图片之间的相似程度，y的取值 (0,1)，0表示相似（同一类别），1 表示不相似（不同类别）。并且y 是double 类型，属于0-1 区间任意的数字。网路结构最后的输出是 0-1 区间的任意数字。通常是以0.5 作为分界线，如果小于0.5 那么认为两种图片是属于同一类别，或者说更相似；反之也成立。<br>重要的一点是中间的weights 实现了权值共享，这样可以加快网络的训练速度。<br><img src="http://ww1.sinaimg.cn/large/e9a223b5ly1g1fwyu5fnsj20z90cq75v.jpg" alt=""></p>
<h3 id="loss-function"><a href="#loss-function" class="headerlink" title="loss function"></a>loss function</h3><p>这个是属于经典的 contrastive loss function。当y 接近于0的时候，右半部分消失，这个是表示两张图片很是相似，然后就不断使得 欧氏距离减少；当y 接近于1的时候，左半部分消失，这个时候两张图片很不相似，然后右边就是 hinge loss。参数m 作为一种margin 是是可以调节，我的实验中 m 取1.<br>总的思想：就是使得相近的图像距离相近，不想近的图像距离变远。</p>
<p>$L ( W , ( Y , X _ { 1 } , X _ { 2 } ) ) = ( 1 - Y ) \frac { 1 } { 2 } ( D _ { W } ) ^ { 2 } + ( Y ) \frac { 1 } { 2 } { \max ( 0 , m - D _ { W } ) } ^ { 2 }$</p>
<h3 id="Spectral-Normalization"><a href="#Spectral-Normalization" class="headerlink" title="Spectral Normalization"></a>Spectral Normalization</h3><p>图像输入到网络之前使用正则化，然后输入到激活函数之前也是使用正则化，所以这种效果也是扩展到 weights，直接对 weights 进行正则化使其符合  Lipschitz 约束，避免使用大的gradients。在GAN 网络中的 discriminator 或者 generator 都发现了其可以稳定训练的过程。在实验中，我们扩大了了这种使用范围，把其应用到所有的网络的layer上。</p>
<h3 id="self-attention-mechanism"><a href="#self-attention-mechanism" class="headerlink" title="self-attention mechanism"></a>self-attention mechanism</h3><p>Attention 机制自从 <a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank" rel="noopener">“Attention Is All You Need”</a> 开始火爆，并且实验的效果也是很好的，然后在图像领域也开始尝试使用 attention 机制来解决长依赖的问题。应用到图像领域主要是 explore spatial locality information, 说白了就是细节的信息。</p>
<blockquote>
<p>If we look at the DCGAN model, we see that regular GANs are heavily based on convolution operations, which use a local receptive field (convolutional kernel) to learn representations. Simple features like edges and corners are learned in the first few layers. Also, ConvNets are able to use these simple representations to learn more complex ones. However, long-range dependency might be hard to learn. Long-range dependency (long-term dependency) is from RNN, which we can say anything larger than trigram as a long term dependency. Thus, most of the image content does not exhibit elaborated shape such as sky or the ocean looks fine. The task of creating geometrically complex forms, such as four-legged animals, is far more challenging. This is where attention comes into play.</p>
</blockquote>
<p>而 self-attention 中QKV 三个部分是相同的，对于这种处理方法和Res_block 还是有点相似的。<br><img src="http://ww1.sinaimg.cn/large/e9a223b5ly1g1fx354ispj210k0fsaae.jpg" alt=""></p>
<h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><p>训练数据集使用是 Cifar-10，记录了训练过程中 acc 和loss 的变化情况。除了训练的效果比较好外，训练速度也是非常快的，可以清楚的看到model acc 在接近25 epoches的时候就开始收敛。<br><img src="http://ww1.sinaimg.cn/large/e9a223b5ly1g1fx3rncspj20hi0d20t9.jpg" alt=""></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/25/fastText-faiss/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jijeng Jia">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jijeng's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/25/fastText-faiss/" itemprop="url">fastText & faiss</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-03-25T21:13:52+08:00">
                2019-03-25
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index">
                    <span itemprop="name">NLP</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/03/25/fastText-faiss/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2019/03/25/fastText-faiss/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="fastText"><a href="#fastText" class="headerlink" title="fastText"></a>fastText</h2><p>fastText结合了自然语言处理和机器学习中最成功的理念。这些包括了使用词袋以及n-gram袋表征语句，还有使用子字(subword)信息，并通过隐藏表征在类别间共享信息。另外采用了一个softmax层级(利用了类别不均衡分布的优势)来加速运算过程。</p>
<p>fastText 主要是用来解决 word representations和 sentence classification. 有趣的是前者是无监督的学习方式，后者是有监督的学习方式。分别主要来自 ”Enriching Word Vectors with Subword Information“ 和 “Bag of Tricks for Efficient Text Classification” 两篇论文。并且使用的是 shallow neural network 而不是深度网络。</p>
<blockquote>
<p>FastText is a library created by the Facebook Research Team for efficient learning of word representations and sentence classification.</p>
</blockquote>
<p>Take off:<br>fastText 方法包含三部分：模型架构、层次 Softmax 和 N-gram 特征。<br>fasttext 有两个用处： text classification 和 word embedding 。<br>使用场景：大型数据，高效计算</p>
<p>下面进行细说：</p>
<h3 id="模型架构"><a href="#模型架构" class="headerlink" title="模型架构"></a>模型架构</h3><p>这个是总的框架图。<br><img src="http://ww1.sinaimg.cn/large/e9a223b5ly1g1f8ph67dkj21280yuk1n.jpg" alt=""></p>
<p><center><sub><sup>抱歉哈 这个引用找不见了，如果有侵权，please email me..</sup></sub><center><br>分为两个部分介绍这个网络结构：<br>从input -&gt; hidden:<br>输入层输入的是一个已经分词后短文本。短文本中每个词的词向量是由该短文本的one-hot矩阵乘以一个初始化的矩阵w得到的。（原理图：下图是fasttext 运行的时候，这个分词是再处理成单词和n-gram 组成的特征，这个是不需要我们进行显性的操作的）<br><img src="http://ww1.sinaimg.cn/large/e9a223b5ly1g1f8x600o5j21440lsjz0.jpg" alt=""><br>从 hidden -&gt; output：<br><img src="http://ww1.sinaimg.cn/large/e9a223b5ly1g1f8xzkf3yj213y0lsti8.jpg" alt=""><br>插播一句，我们经常使用的预训练模型中的weights 是从input-&gt; hidden。</center></center></p>
<h3 id="Hierarchical-Softmax"><a href="#Hierarchical-Softmax" class="headerlink" title="Hierarchical Softmax"></a>Hierarchical Softmax</h3><p>从名字上就知道这个是基于softmax的改进版本，主要是运算上的改进。具体说来：利用了类别（class）不均衡这个事实（一些类别出现次数比其他的更多），通过使用 Huffman 算法建立用于表征类别的树形结构。fastText 模型使用了层次 Softmax 技巧。层次 Softmax 技巧建立在Huffman的基础上，对标签进行编码，能够极大地缩小模型预测目标的数量。</p>
<p>这个是softmax 的原始的计算公式：<br><img src="http://ww1.sinaimg.cn/large/e9a223b5ly1g1fbkxph28j20ia0560sm.jpg" alt=""><br>采用二叉树的结构之后，时间上优化不少。$O ( N) \rightarrow O \left( \log _ { 2 } N \right)$。见下图。<br><img src="http://ww1.sinaimg.cn/large/e9a223b5ly1g1fboz6w71j20n20bedh7.jpg" alt=""></p>
<p><center><sub><sup>抱歉哈 这个引用找不见了，如果有侵权，please email me..</sup></sub><center><br>和之前的神经网络模型相比，这里的huffmax树的所有内部节点就类似之前神经网络隐藏层的神经元。其中，根节点的词向量对应我们的投影后的词向量，而所有的叶子节点就类似于之前神经网络softmax输出层的神经元。叶子节点的个数就是词汇表的大小. </center></center></p>
<p>和之前的相比，从隐藏层到输出层的softmax映射不是一下就完成的，而是沿着 huffman树一步步完成的，因此这种 softmax取名为”Hierarchical softmax”. </p>
<h3 id="N-gram-特征"><a href="#N-gram-特征" class="headerlink" title="N-gram 特征"></a>N-gram 特征</h3><p>N-gram是基于这样的思想：某个词的出现依赖于其他若干个词；我们获得的信息越多，预测越准确。我想说，我们每个人的大脑中都有一个N-gram模型，而且是在不断完善和训练的。我们的见识与经历，都在丰富着我们的阅历，增强着我们的联想能力。</p>
<p> N-gram模型是一种语言模型（Language Model，LM），语言模型是一个基于概率的判别模型，它的输入是一句话（单词的顺序序列），输出是这句话的概率，即这些单词的联合概率（joint probability）。 </p>
<p>这样的作用，使用N-gram来给文本添加额外的特征获得关于局部词顺序的部分信息。<br>举个栗子：对于句子：“我 喜欢 喝 咖啡”, 如果不考虑顺序，那么就是每个词，“我”，“喜欢”，“喝”，“咖啡”这五个单词的word embedding求平均。如果考虑2-gram, 那么除了以上五个词，还有“我喜欢”，“喜欢喝”，“喝咖啡”等词。“我喜欢”，“喜欢喝”，“喝咖啡”这三个词就作为这个句子的文本特征。<br>我们经常见到的场景：输入法的预选词汇。就是可以通过这种方式实现的。</p>
<p>当然使用了更多的特征意味着造成了效率下降，于是该作者提出了两种解决方法：<br>过滤掉低词频；使用词粒度代替字粒度。<br>比如说海慧寺使用上面那个句子”我喜欢喝咖啡“，如果使用子粒度的2-gram，那么产生的特征是“我喜”，“喜欢”，“欢喝”，“喝咖”，“咖啡”。如果使用词粒度为2-gram，那么产生的特征是“我喜欢”，“喜欢喝”，“喝咖啡”。</p>
<p>补充一句，subwords就是一个词的character-level的n-gram。比如单词”hello”，长度至少为3的char-level的ngram有”hel”,”ell”,”llo”,”hell”,”ello”以及本身”hello”。<br>n-gram 是一种思想，可以是针对words 之间的，也是可以针对一个word 内部的。前者就是候选词，哪些词语容易组合出现，后者主要是对于单词本身的伸展，可以把没有在dict 中的单词，使用字词（subword） 进行表示，扩充了模型的表达能力。</p>
<h3 id="Negative-Sampling"><a href="#Negative-Sampling" class="headerlink" title="Negative Sampling"></a>Negative Sampling</h3><p>主要是减轻计算量的角度考虑的，每次让一个训练样本仅仅更新一部分的权重参数，<br>这个技术不是 fastText 首创的，但是本着总结知识点的，也就放在这里了。<br>CBOW / Skip-gram模型 （这个论文中）提出了两种方法，一种是Hierarchical Softmax，另一种是Negative Sampling。论文中提出的两种方法都是用来提高计算效率的，下面说一下负采样。<br>在训练神经网络时，每当接受一个训练样本，然后调整所有神经单元权重参数，来使神经网络预测更加准确。换句话说，每个训练样本都将会调整所有神经网络中的参数。而 Negative Sampling 每次让一个训练样本仅仅更新一小部分的权重参数，从而降低梯度下降过程中的计算量。如果 vocabulary 大小为1万时， 当输入样本 ( “fox”, “quick”) 到神经网络时， “ fox” 经过 one-hot 编码，在输出层我们期望对应 “quick” 单词的那个神经元结点输出 1，其余 9999 个都应该输出0。在这里，这9999个我们期望输出为0的神经元结点所对应的单词我们称为 negative word，随机选择一小部分的 negative words，比如选 10个 negative words 来更新对应的权重参数。<br>解决的问题，在最后一层 softmax 的计算量太大，相当于每一次word 都是需要整个dict 量的级别的更新。然后选择 k 个negative words，只是计算这些softmax 的值。</p>
<blockquote>
<p>Training a neural network means taking a training example and adjusting all of the neuron weights slightly so that it predicts that training sample more accurately. In other words, each training sample will tweak all of the weights in the neural network.<br>As we discussed above, the size of our word vocabulary means that our skip-gram neural network has a tremendous number of weights, all of which would be updated slightly by every one of our billions of training samples!<br>Negative sampling addresses this by having each training sample only modify a small percentage of the weights, rather than all of them. Here’s how it works.<br>When training the network on the word pair (“fox”, “quick”), recall that the “label” or “correct output” of the network is a one-hot vector. That is, for the output neuron corresponding to “quick” to output a 1, and for all of the other thousands of output neurons to output a 0.<br>With negative sampling, we are instead going to randomly select just a small number of “negative” words (let’s say 5) to update the weights for. (In this context, a “negative” word is one for which we want the network to output a 0 for). We will also still update the weights for our “positive” word (which is the word “quick” in our current example).<br>The paper says that selecting 5-20 words works well for smaller datasets, and you can get away with only 2-5 words for large datasets.<br>Recall that the output layer of our model has a weight matrix that’s 300 x 10,000. So we will just be updating the weights for our positive word (“quick”), plus the weights for 5 other words that we want to output 0. That’s a total of 6 output neurons, and 1,800 weight values total. That’s only 0.06% of the 3M weights in the output layer!<br>In the hidden layer, only the weights for the input word are updated (this is true whether you’re using Negative Sampling or not).</p>
</blockquote>
<h3 id="Positive-samples-and-Negative-samples"><a href="#Positive-samples-and-Negative-samples" class="headerlink" title="Positive samples and  Negative samples"></a>Positive samples and  Negative samples</h3><p>One little detail that’s missing from the description above is how do we select the negative samples.<br>The negative samples are chosen using the unigram distribution. Essentially, the probability of selecting a word as a negative sample is related to its frequency, with more frequent words being more likely to be selected as negative samples. Instead of using the raw frequency in the original word2vec paper, each word is given a weight that’s equal to it’s frequency (word count) raised to the 3/4 power. The probability for selecting a word is just it’s weight divided by the sum of weights for all words.<br>$$P \left( w _ { i } \right) = \frac { f \left( w _ { i } \right) ^ { 3 / 4 } } { \sum _ { i = 0 } ^ { n } \left( f \left( w _ { j } \right) ^ { 3 / 4 } \right) }$$<br>This decision to raise the frequency to the 3/4 power appears to be empirical; as the author claims it outperformed other functions (e.g. just using unigram distribution).<br>Side note: The way this selection is implemented in the original word2vec C code is interesting. They have a large array with 100M elements (which they refer to as the unigram table). They fill this table with the index of each word in the vocabulary multiple times, and the number of times a word’s index appears in the table is given by </p>
<p>Then, to actually select a negative sample, we just generate a random integer between 0 and 100M, and use the word at that index in the table. Since the higher probability words occur more times in the table, we’re more likely to pick those.</p>
<p>这个也是有讲 任何进行negative sample的选择<br><a href="http://jalammar.github.io/illustrated-word2vec/" target="_blank" rel="noopener">http://jalammar.github.io/illustrated-word2vec/</a><br>一般来说在 word2vec 中context 是会选择到 5，然后这个 positive / negative sample 会是(1/6), 然后 nagative sample 是随机在 dictionary里面选的（所以有可能选到 positive sample）， 这个是这个dictionary 是根据频率，出现次数越多的，被选中的可能性也越大。<br>The number of negative samples is another factor of the training process. The original paper prescribes 5-20 as being a good number of negative samples. It also states that 2-5 seems to be enough when you have a large enough dataset. The Gensim default is 5 negative samples.</p>
<p>To address this, we need to introduce negative samples to our dataset – samples of words that are not neighbors. Our model needs to return 0 for those samples. Now that’s a challenge that the model has to work hard to solve – but still at blazing fast speed.<br>This idea is inspired by Noise-contrastive estimation [pdf]. We are contrasting the actual signal (positive examples of neighboring words) with noise (randomly selected words that are not neighbors). This leads to a great tradeoff of computational and statistical efficiency.</p>
<p>Essentially, the probability for selecting a word as a negative sample is related to its frequency, with more frequent words being more likely to be selected as negative samples.</p>
<p>They fill this table with the index of each word in the vocabulary multiple times, and the number of times a word’s index appears in the table is given byP(wi)P(wi)* table_size. Then, to actually select a negative sample, you just generate a random integer between 0 and 100M, and use the word at that index in the table. Since the higher probability words occur more times in the table, you’re more likely to pick those.</p>
<p>It’s now time to build out our skip-gram generator which will give us pair of words and their relevance</p>
<ul>
<li>(word, word in the same window), with label 1 (positive samples).</li>
<li>(word, random word from the vocabulary), with label 0 (negative samples).</li>
</ul>
<h3 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h3><p>第一个应用场景：词向量。<br>fastText作为训练词向量认为可以有两种模式，一种是根据周围词语预测中心词汇的CBOW （continuous bag-of-words）模型，另一种是根据中心词汇预测上下文的 skip-gram 模型。</p>
<blockquote>
<p>./fasttext – It is used to invoke the FastText library.<br> skipgram/cbow – It is where you specify whether skipgram or cbow is to be used to create the word representations.<br> -input – This is the name of the parameter which specifies the following word to be used as the name of the file used for training. This argument should be used as is.<br> data.txt – a sample text file over which we wish to train the skipgram or cbow model. Change this name to the name of the text file you have.<br> -output – This is the name of the parameter which specifies the following word to be used as the name of the model being created. This argument is to be used as is.<br> model – This is the name of the model created.<br>Running the above command will create two files named model.bin and model.vec. model.bin contains the model parameters, dictionary and the hyperparameters and can be used to compute word vectors. model.vec is a text file that contains the word vectors for one word per line.</p>
</blockquote>
<p>最后生成有两个文件，一个 xxx.bin 文件，一个是 xxx.vec 文件，前者是预训练模型，后者是词向量。<br>这两个可能是最重要的格式了。</p>
<blockquote>
<p>The most important parameters of the model are its dimension and the range of size for the subwords. </p>
</blockquote>
<p>常见的代码格式：</p>
<pre><code>./fasttext skipgram -input data/fil9 -output result/fil9 -minn 2 -maxn 5 -dim 300
</code></pre><p>跑偏一下说一下shell的小技巧。<br>使用echo 或者 &lt; 这样进行单个词或者多个词的词向量的查询。</p>
<blockquote>
<p>./fasttext print-word-vectors model.bin &lt; queries.txt<br>echo “word” | ./fasttext print-word-vectors model.bin</p>
</blockquote>
<p>Finding simialr words:</p>
<pre><code>./fasttext nn model.bin
</code></pre><p>第二个应用场景：文本分类。</p>
<blockquote>
<p>Sentiment analysis and email classification are classic examples of text classification</p>
</blockquote>
<p>在训练数据集中label 默认是使用 “__label__” 进行表示的，当然也是可以进行自定义的。</p>
<pre><code>./fasttext supervised -input train.ft.txt -output model_kaggle -label  __label__ -lr 0.5
就是进行predict的时候，有时候并不是很能想起来只是predict top 3 这样的东西。
# Predicting on the test dataset
./fasttext predict model_kaggle.bin test.ft.txt
# Predicting the top 3 labels
./fasttext predict model_kaggle.bin test.ft.txt 3
</code></pre><h2 id="faiss"><a href="#faiss" class="headerlink" title="faiss"></a>faiss</h2><p>用途：相似度检测和稠密向量的聚类。</p>
<blockquote>
<p>Faiss is a library for efficient similarity search and clustering of dense vectors.</p>
</blockquote>
<p>之前的实习经历主要是用faiss 处理文本的representation，但是这个是有偏差的，凡是能够打成词向量，都是可以使用faiss 进行计算的，当然这词向量需要满足：相近内容在相近的空间。</p>
<blockquote>
<p>Once the vectors are extracted by learning machinery (from images, videos, text documents, and elsewhere), they’re ready to feed into the similarity search library.</p>
</blockquote>
<h3 id="faiss的实现过程"><a href="#faiss的实现过程" class="headerlink" title="faiss的实现过程"></a>faiss的实现过程</h3><p>首先使用 index对于向量进行预处理，然后选择不同的模式。</p>
<p>牺牲了一些精确性来使得运行速度更快。</p>
<blockquote>
<p>Similarity search can be made orders of magnitude faster if we’re willing to trade some accuracy; that is, deviate a bit from the reference result. For example, it may not matter much if the first and second results of an image similarity search are swapped, since they’re probably both correct results for a given query. Accelerating the search involves some pre-processing of the data set, an operation that we call indexing.</p>
</blockquote>
<p>向量的比较有两种metric：一种是L2 一种是基于consine (点乘)进行检索。前者是求解最小的值，后者是通过inner——product 求解maximum. 并且是支持GPU的，在原来CPU上建立的index，然后很好的迁移到 GPU上。</p>
<p>在涉及index使用考虑速度，acc和内存大小三个不同的维度。然后不同的index 是有不同的侧重的。<br><img src="http://ww1.sinaimg.cn/large/e9a223b5ly1g1faf2twaaj20ft0e3ae0.jpg" alt=""></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/5/">5</a><a class="extend next" rel="next" href="/page/3/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/avatar.jpeg"
                alt="Jijeng Jia" />
            
              <p class="site-author-name" itemprop="name">Jijeng Jia</p>
              <p class="site-description motion-element" itemprop="description">Stay Hungry, Stay Foolish</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">43</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">7</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">28</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/jijeng" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:jia1509309698@163.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Jijeng Jia</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>

<div>
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
Total  <span id="busuanzi_value_site_pv"></span> views
You got  <span id="busuanzi_value_site_uv"></span> visitors
</div>




        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      
    </span>
  
</div>








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  

    
      <script id="dsq-count-scr" src="https://https-jijeng-github-io.disqus.com/count.js" async></script>
    

    

  




	





  














  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
