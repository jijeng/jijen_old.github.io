<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="Stay Hungry, Stay Foolish">
<meta property="og:type" content="website">
<meta property="og:title" content="Jijeng&#39;s blog">
<meta property="og:url" content="http://yoursite.com/page/2/index.html">
<meta property="og:site_name" content="Jijeng&#39;s blog">
<meta property="og:description" content="Stay Hungry, Stay Foolish">
<meta property="og:locale" content="default">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Jijeng&#39;s blog">
<meta name="twitter:description" content="Stay Hungry, Stay Foolish">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/page/2/"/>





  <title>Jijeng's blog</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Jijeng's blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="Searching..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/08/23/文本相似度比较基本知识/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jijeng Jia">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jijeng's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/08/23/文本相似度比较基本知识/" itemprop="url">文本相似度比较基本知识</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-08-23T20:19:58+08:00">
                2018-08-23
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>本文服务于<a href="https://jijeng.github.io/2018/08/23/基于simhash的文本相似度比较/" target="_blank" rel="noopener">该篇博客</a>,主要进行名词解释。</p>
<h2 id="simhash"><a href="#simhash" class="headerlink" title="simhash"></a>simhash</h2><h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><p>simhash 也是一种hash，一般的hash 函数映射规则只需要满足以下两个条件：</p>
<ul>
<li>对很多不同的特征来说，它们对所对应的向量是均匀随机分布的</li>
<li>相同的特征来说对应的向量是唯一<br>简单来说普通的hash映射需要满足随机分布和唯一性两个条件。simhash想要实现的是，如果原来的文本的特征是相似，那么映射之后的编码也是相似。这里使用 hamming distance 进行比较simhash映射之后的距离。根据经验值，对64位的 SimHash值，海明距离在3以内的可认为相似度比较高。编码之后的表示在英文中是 fingerprint(指纹)。<br>simhash最初被google 用于网页去重，当时使用的fingerprint 是64,所以这里沿用了这个传统。64位的签名可以表示多达264个象限，因此只保存所在象限的信息也足够表征一个文档了。<br>更进一步，表示的文档的数字最多是多少？这个应该可以准确计算特征的个数应为如果用三位(01) 表示，那么有8种，那么2^64 这么多种特征，所以16*10^18 这么多。<h3 id="算法步骤"><a href="#算法步骤" class="headerlink" title="算法步骤"></a>算法步骤</h3>第一步：文本预处理得到分词（去重，去除的了stop words）,然后weight权重可以使用分词的frequency 或者tfidf 进行得到<br>第二步：进行hash 映射（可以使用md5这种传统的映射方式，基本的要求就是均匀映射到一个空间，这种映射并不能反映原始样本的相关性）<br>第三步：hash 映射值和weight 进行相乘，如果原来是1 则乘以1，如果是0 则乘以-1，<br>第四步： 列向相加，得到summing weights,进行降维如果是正数那么为1，如果是负数那么为-1<br>然后这个simhash就出来了.<br>有图有真相<br><img src="http://p9fyz2xe9.bkt.clouddn.com/18-8-23/22402623.jpg" alt=""><br><img src="http://p9fyz2xe9.bkt.clouddn.com/18-8-23/11071129.jpg" alt=""><h3 id="simhash的局限性："><a href="#simhash的局限性：" class="headerlink" title="simhash的局限性："></a>simhash的局限性：</h3>只考虑到文章存在哪些词，没有考虑到词的顺序。不过相应的优点是，可以实现海量文章相似度计算。文章相似度计算忽略词的顺序之后效果更好。所以在处理大文本时候，simhash是有效的，但是在处理小文本，这种效果往往不能被保证。直观上理解，在一片段文章或者段落中，词语出现的顺序还是比较重要的。<h2 id="minhash"><a href="#minhash" class="headerlink" title="minhash"></a>minhash</h2>可以参考<a href="https://www.youtube.com/watch?v=96WOGPUgMfw" target="_blank" rel="noopener">该视频</a>和<a href="http://www.cnblogs.com/maybe2030/p/4953039.html" target="_blank" rel="noopener">这篇文章</a>。<h2 id="Locality-Sensitive-Hashing"><a href="#Locality-Sensitive-Hashing" class="headerlink" title="Locality Sensitive Hashing"></a>Locality Sensitive Hashing</h2>Locality Sensitive Hashing(局部敏感哈希)作用就是从海量的数据中挖掘出相似的数据，可以具体应用到文本相似度检测、网页搜索等领域。上面的simhah和minhash 就是该思想的实现。<h2 id="距离函数"><a href="#距离函数" class="headerlink" title="距离函数"></a>距离函数</h2>这里的距离函数都是用来文本相似度。<h3 id="Jaccard相似度"><a href="#Jaccard相似度" class="headerlink" title="Jaccard相似度"></a>Jaccard相似度</h3>简单来说交集除以并集。这个集合中存放的是文章或者段落的关键词。<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">def JaccardSim(str_a, str_b):</span><br><span class="line">    &apos;&apos;&apos;</span><br><span class="line">    Jaccard相似性系数</span><br><span class="line">    计算sa和sb的相似度 len（sa &amp; sb）/ len（sa | sb）</span><br><span class="line">    &apos;&apos;&apos;</span><br><span class="line">    seta = splitWords(str_a)[1]</span><br><span class="line">    setb = splitWords(str_b)[1]</span><br><span class="line"></span><br><span class="line">    sa_sb = 1.0 * len(seta &amp; setb) / len(seta | setb)</span><br><span class="line"></span><br><span class="line">    return sa_sb</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>可以看到核心代码很简单，经过分词之后，就是seta 和setb 进行的操作。</p>
<h3 id="cosine"><a href="#cosine" class="headerlink" title="cosine"></a>cosine</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">def cos_sim(a, b):</span><br><span class="line">    a = np.array(a)</span><br><span class="line">    b = np.array(b)</span><br><span class="line">    # return &#123;&quot;文本的余弦相似度:&quot;:np.sum(a*b) / (np.sqrt(np.sum(a ** 2)) * np.sqrt(np.sum(b ** 2)))&#125;</span><br><span class="line">    return np.sum(a * b) / (np.sqrt(np.sum(a ** 2)) * np.sqrt(np.sum(b ** 2)))</span><br></pre></td></tr></table></figure>
<p>将文本的关键词映射成某种高维函数，然后在高维空间中计算两者的相似度。</p>
<h3 id="tf-idf"><a href="#tf-idf" class="headerlink" title="tf-idf"></a>tf-idf</h3><p>在simhash 中使用 tf-idf作为我们的比较函数。TF-IDF的主要思想就是：如果某个词在一篇文档中出现的频率高，也即TF高；并且在语料库中其他文档中很少出现，即DF的低，也即IDF高，则认为这个词具有很好的类别区分能力。<br>词频(term frequency)有两种计算方式,后者考虑了相对的情况。<br><img src="http://p9fyz2xe9.bkt.clouddn.com/18-8-23/52502045.jpg" alt=""><br><img src="http://p9fyz2xe9.bkt.clouddn.com/18-8-23/28700629.jpg" alt=""><br>计算idf(inverse document frequency):<br><img src="http://p9fyz2xe9.bkt.clouddn.com/18-8-23/29062439.jpg" alt=""><br>TF-IDF 优点是简单快速，比较符合实际。缺点，无法体现词的位置信息，所有的位置都是被认为重要性相同，但是开头结尾，段落的开头和段落的结尾，therefore，so，but这些词语都是没有体现的。</p>
<h3 id="hamming-distance"><a href="#hamming-distance" class="headerlink" title="hamming distance"></a>hamming distance</h3><p>hamming distance就是比较01串的不同，按照位进行比较。算法：异或时，只有在两个比较的位不同时其结果是1 ，否则结果为0，两个二进制“异或”后得到1的个数即为海明距离的大小。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">hashbits =64 # 使用64位进行编码</span><br><span class="line">def simhash_function(tokens, weights_dict):</span><br><span class="line">    v = [0] * hashbits</span><br><span class="line">    # 这种 &#123;key: value&#125;.item() 的操作也是没有了谁了</span><br><span class="line">    for key, t in &#123;x: _string_hash(x) for x in tokens&#125;.items():</span><br><span class="line">        for i in range(hashbits):</span><br><span class="line">            bitmask = 1 &lt;&lt; i</span><br><span class="line">            if t &amp; bitmask:</span><br><span class="line">                v[i] += weights_dict[key]</span><br><span class="line">            else:</span><br><span class="line">                v[i] -= weights_dict[key]</span><br><span class="line"></span><br><span class="line">    fingerprint = 0</span><br><span class="line">    for i in range(hashbits):</span><br><span class="line">        if v[i] &gt;= 0:</span><br><span class="line">            fingerprint += 1 &lt;&lt; i</span><br><span class="line">    return fingerprint</span><br><span class="line">fingerprint = simhash_function(tokens, weights)</span><br></pre></td></tr></table></figure></p>
<h3 id="min-edit-distance"><a href="#min-edit-distance" class="headerlink" title="min edit distance"></a>min edit distance</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"># 最小编辑距离</span><br><span class="line">def min_edit_distance(str1, str2):</span><br><span class="line">    rows  =len(str2) +1</span><br><span class="line">    cols =len(str1) +1</span><br><span class="line">    arr =[[0 for _ in range(cols)] for _ in range(rows)]</span><br><span class="line">    # 这种简洁的代码也是牛逼</span><br><span class="line">    for j in range(cols):</span><br><span class="line">        arr[0][j] =j</span><br><span class="line">    for i in range(rows):</span><br><span class="line">        arr[i][0] =i</span><br><span class="line">    for i in range(1, rows):</span><br><span class="line">        for j in range(1, cols):</span><br><span class="line">            # 因为string 是从0 ，len(str) -1的</span><br><span class="line">            if str2[i-1] ==str1[j-1]:</span><br><span class="line">                arr[i][j] =arr[i-1][j-1]</span><br><span class="line">            else:</span><br><span class="line">                # 以后见到这样的式子，就要想到这个二维的数组，因为这个是可以帮助记忆的</span><br><span class="line">                arr[i][j] =1 +min(arr[i-1][j-1], arr[i-1][j], arr[i][j-1])</span><br><span class="line">    # 右下角就是距离</span><br><span class="line">    return arr[rows-1][cols-1]</span><br><span class="line">str_a =&quot;abcdef&quot;</span><br><span class="line">str_b =&quot;azced&quot;</span><br><span class="line">result =min_edit_distance(str_a, str_b)</span><br><span class="line">result</span><br></pre></td></tr></table></figure>
<p>具体可以参看<a href="https://www.youtube.com/user/tusharroy2525/featured" target="_blank" rel="noopener">该视频</a>讲解。(ps. 如果刷leetcode,也可以参看该视频)</p>
<h2 id="分词"><a href="#分词" class="headerlink" title="分词"></a>分词</h2><p>在英文中存在天然的空格可以进行分词操作，但是中文的分词就比较复杂了。常用的中文分词开源工具有 jieba和<a href="https://github.com/hankcs/HanLP" target="_blank" rel="noopener">HanLP</a><br>前者简单易行，容易上手；后者在自然语言处理作为汉语言处理包，可以用于词性标注，命名实体识别等一系列功能。常用的英文分词 <a href="https://stanfordnlp.github.io/CoreNLP/" target="_blank" rel="noopener">corenlp</a></p>
<h2 id="倒排索引"><a href="#倒排索引" class="headerlink" title="倒排索引"></a>倒排索引</h2><p>倒排索引使用python在实现上就是一个dictionary 嵌套一个 set(). 一般的索引都是数字或者英文字母映射内容，具体在放到simhash的情景下就是使用文章的序列号对应提取出来的关键词。但是倒排索引就是关键词对应文章的序列号，类似与原来的”值”对应这”键”，所以称之为倒排索引。可以参考<a href="https://blog.csdn.net/u011239443/article/details/60604017" target="_blank" rel="noopener">这篇文章</a>。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/07/21/differences-between-l1-and-l2-as-loss-function-and-regularization/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jijeng Jia">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jijeng's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/07/21/differences-between-l1-and-l2-as-loss-function-and-regularization/" itemprop="url">differences-between-l1-and-l2-as-loss-function-and-regularization</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-07-21T14:22:27+08:00">
                2018-07-21
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>如果补交作业也算作业的话，那么这篇博文就算做作业。<br>L1 和L2 作为Loss function和 regularization，个人感觉是一个经常容易混淆的概念。但是如果读者觉得很清楚，那么就可以跳过了。<br>本文大量借鉴于<a href="http://www.chioka.in/differences-between-l1-and-l2-as-loss-function-and-regularization/" target="_blank" rel="noopener">该博客</a>，原文是英文，如果读者英文能够handle，建议读原文。</p>
<h2 id="As-loss-function"><a href="#As-loss-function" class="headerlink" title="As loss function"></a>As loss function</h2><p>loss function or error function 是用来衡量真实y 和生成的f(x) 之间差距的函数。在模型训练中我们一般情况下不断训练模型使得loss function不断下降（如果task要求loss function是增大，这时候一般加上符号或者转换成 1- loss fucntion，最后实现的还是loss function下降）。好的回到L1 loss function和L2 loss function. </p>
<p>L1-norm loss function is also known as least absolute deviations(LAD), least absolute errors. It is basically minimizing the sum of the absolute differences between the target value Y and the estimated values (f(x)).<br><img src="http://p9fyz2xe9.bkt.clouddn.com/18-7-19/35322217.jpg" alt="avatar"><br>L2-norm loss function is also known as least squares error(LSE). It is basically minimizing the sum of the square of the differences between the target value Y and the estimated values (f(x)).<br><img src="http://p9fyz2xe9.bkt.clouddn.com/18-7-19/37430433.jpg" alt="avatar"><br>主要你从一下三个指标去衡量两者的不同： robustness，stability和是否具有唯一解。<br>wiki 中关于<a href="https://en.wikipedia.org/wiki/Overfitting" target="_blank" rel="noopener">robust</a> 中是这样定义： A learning algorithm that can reduce the chance of fitting noise is called robust。具有更好的泛化性能，不去过度拟合noise. 关于<a href="https://en.wikipedia.org/wiki/Stability_(learning_theory" target="_blank" rel="noopener">stability</a>) wiki 是这样定义：Stability, also known as algorithmic stability, is a notion in computational learning theory of how a machine learning algorithm is perturbed by small changes to its inputs.</p>
<p>我对于前两个指标的理解：robustness 是对于原始的train 样本中离群点的态度，如果某个模型是robustness的，那么对于该数据集中的离群点是能够抗干扰的。反之则是不具有robustness的。stability是对于原始train 数据的轻微的平移的反应，如果对于某个原始数据的轻微平移，最后的结果没有产生很大的波动，那么该模型就是具有stability, 反之，则不具有stability.</p>
<p>比较L1-norm 和L2-norm在前两个评价指标中的表现：<br><img src="http://p9fyz2xe9.bkt.clouddn.com/18-7-19/80680309.jpg" alt="avatar"><br>对于第三点，我想在下面进行介绍。因为这点和后面和下面的solution uniqueness是相同的。</p>
<h2 id="As-regularization"><a href="#As-regularization" class="headerlink" title="As regularization"></a>As regularization</h2><p>从<a href="http://xgboost.readthedocs.io/en/latest/model.html" target="_blank" rel="noopener">XGBoost调参指南</a>中我们知道objective function = loss funcion + regularization. 而我们大多数情况下提及的都是loss function,常常忽略了regularization 的作用。所以正确的姿态应该是这样的：<br><img src="http://p9fyz2xe9.bkt.clouddn.com/18-7-19/73111516.jpg" alt="avatar"><br>The regularization term controls the complexity of the model, which helps us to avoid overfitting.<br>对于模型训练，一开始的想法是尽量的overfitting, 因为就现在不成熟的经验而言，对于overfitting这个问题有很多处理方法，比如卷积深度神经网络中的dropout, LightGBM中的early stop 和随机采样的思想。 这些方法都是可以缓解overfitting，所以可以出现overfitting。相反，如果你的模型是underfitting，那么你就微显尴尬了。好，收回到L1 and L2。</p>
<p>先上公式<br>L1 regularization on least squares:<br><img src="http://p9fyz2xe9.bkt.clouddn.com/18-7-19/75850249.jpg" alt="avatar"><br>L2 regularization on least squares:<br><img src="http://p9fyz2xe9.bkt.clouddn.com/18-7-19/78934923.jpg" alt="avatar"><br>The difference between their properties can be promptly summarized as follows:<br><img src="http://p9fyz2xe9.bkt.clouddn.com/18-7-19/88209822.jpg" alt="avatar"></p>
<p>对于第一点computational efficient的理解：平方比绝对值更容易计算，平方可以求导直接求最值，但是绝对值就无法求导。并且L1 regularization在 non-sparse cases中是 computational inefficient，但是在 sparse(0比较多) cases中是有相应的稀疏算法来进行优化的，所以是computational efficient.<br>对于第二点是否具有sparse solution可以从几何意义的角度解读：<br><img src="http://p9fyz2xe9.bkt.clouddn.com/18-7-19/82495076.jpg" alt="avatar"><br>The green line(L2-norm) is the unique shortest path, while the red, blue yellow(L1-norm) are all same length for the same route.</p>
<p>Built-in feature selection is frequently mentioned as a useful property of the L1-norm, which the L2-norm does not. This is actually a result of the L1-norm, which tends to produces sparse coefficients (explained below). Suppose the model have 100 coefficients but only 10 of them have non-zero coefficients, this is effectively saying that “the other 90 predictors are useless in predicting the target values”. L2-norm produces non-sparse coefficients, so does not have this property.</p>
<p>所以表格中第三点也是顺理成章的了。<br>至此，我们区分了L1-norm vs L2-norm loss function 和L1-regularization vs L2-regularization。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/07/21/LightGBM和XGBoost及其调参/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jijeng Jia">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jijeng's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/07/21/LightGBM和XGBoost及其调参/" itemprop="url">LightGBM和XGBoost及其调参</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-07-21T14:17:35+08:00">
                2018-07-21
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="lightGBM调参-常用参数"><a href="#lightGBM调参-常用参数" class="headerlink" title="lightGBM调参(常用参数)"></a>lightGBM调参(常用参数)</h3><p>Since lightGBM is based on decision tree algorithms, it splits the tree with the best fit whereas boosting algorithms split the tree depth wise or level wise rather than leaf-wise. So when growing on the same leaf in Light GBM, the leaf-wise algorithm can reduce more loss than the level-wise algorithm and hence results in much better accuracy which can rarely be achieved by any of the existing boosting algorithms. Also, it is surprisingly very fast, hence the word ‘Light’.<br><img src="http://p9fyz2xe9.bkt.clouddn.com/18-7-19/3852489.jpg" alt="avatar"><br><img src="http://p9fyz2xe9.bkt.clouddn.com/18-7-19/21729887.jpg" alt="avatar"><br>Leaf wise splits lead to increase in complexity and may lead to overfitting and it can be overcome by specifying another parameter max_depth parameter.</p>
<h3 id="Advantages-of-LightGBM"><a href="#Advantages-of-LightGBM" class="headerlink" title="Advantages of LightGBM"></a>Advantages of LightGBM</h3><ul>
<li>faster training speed and higher efficiency<br>Light GBM use histogram based algorithm i.e it buckets continuous feature values into discrete bins which fasten the training procedure.</li>
<li>lower memory usage<br>Replaces continuous values to discrete bins which result in lower memory usage.</li>
<li>better accuracy than any other boosting algorithm<br>It produces much more complex trees by following leaf wise split approach rather than a level-wise approach which is the main factor in achieving higher accuracy. </li>
<li>compatibility with large datasets<br>It is capable of performing equally good with large datasets with a significant reduction in training time as compared to XGBOOST.</li>
<li>parallel learning supported</li>
</ul>
<h3 id="lightGBM调参-常用参数-1"><a href="#lightGBM调参-常用参数-1" class="headerlink" title="lightGBM调参(常用参数)"></a>lightGBM调参(常用参数)</h3><ul>
<li><strong>task</strong><br>default= train, option: train, prediction</li>
<li>application<br>default= regression, option: regression, binary, multiclass, lambdarank(lambdarank application)</li>
<li>data<br>training data, 这个比较诡异，你需要创建一个lightGBM类型的data</li>
<li>num_iterations<br>default =100, 可以设置为的大一些，然后使用early_stopping进行调节。</li>
<li>early_stopping_round<br>default =0, will stop training if one metric of one validation data doesn’t improve in last early_stopping_round rounds.</li>
<li>num_leaves<br>default =31, number of leaves in a tree</li>
<li>device<br>default =cpu, options: gpu, cpu, choose gpu for faster training.</li>
<li>max_depth<br>specify the max depth to which tree will grow, which is very important.</li>
<li>feature_fraction<br>default =1, specifies the fraction of features to be taken for each iteration.</li>
<li>bagging_fraction<br>default =1, spefifies the fraction of data to be used for each iteration and it generally used to speed up the training and avoid overfitting.</li>
<li>max_bin<br>max number of bins to bucket the feature values.因为模型是基于bin训练的，如果bin 数量越多，得到better accuracy,同时更加容易 overfitting.</li>
<li>num_threads</li>
<li>label<br>specify the label columns.</li>
<li>categorical_feature<br>specify the categorical features</li>
<li>num_class<br>default =1, used only for multi-class classification</li>
</ul>
<p><img src="http://p9fyz2xe9.bkt.clouddn.com/18-7-19/50917702.jpg" alt="avatar"></p>
<h3 id="referrence"><a href="#referrence" class="headerlink" title="referrence"></a>referrence</h3><p><a href="https://www.analyticsvidhya.com/blog/2017/06/which-algorithm-takes-the-crown-light-gbm-vs-xgboost/" target="_blank" rel="noopener">which-algorithm-takes-the-crown-light-gbm-vs-xgboost</a><br><a href="https://blog.csdn.net/aliceyangxi1987/article/details/80711014" target="_blank" rel="noopener">LightGBM 如何调参</a><br><a href="http://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html" target="_blank" rel="noopener">官方文档param_tuning</a><br><a href="http://lightgbm.readthedocs.io/en/latest/Parameters.html" target="_blank" rel="noopener">官方文档parameter</a></p>
<h3 id="XGBoost调参"><a href="#XGBoost调参" class="headerlink" title="XGBoost调参"></a>XGBoost调参</h3><h3 id="Advantage-of-XGBoost"><a href="#Advantage-of-XGBoost" class="headerlink" title="Advantage of XGBoost"></a>Advantage of XGBoost</h3><ul>
<li>regularization<br>standard GBM implementation has no regularization, in fact, XGBoost is also known as ‘regularized boosting’ technique.</li>
<li>parallel processing<br>we know that boosting is sequential process so how can it be parallelized? <a href="http://zhanpengfang.github.io/418home.html" target="_blank" rel="noopener">this link</a> to explore further.</li>
<li>high flexibility<br>XGBoost allow users to define <strong>custom optimization objectives and evaluation criteria</strong></li>
<li>handling missing values<br>very useful property. XGBoost has an in-built routine to handle missing values. User is required to supply a different value than other observations and pass that as a parameter. XGBoost tries different things as it encounter a missing value on each node and learns which path to take for missing values in future.</li>
<li>Tree pruning<br>A GBM would stop splitting a node when it encounters a negative loss in the split. Thus it is more of a greedy algorithm. XGBoost on the other hand make splits upon the max_depth specified and then start pruning the tree backwards and remove splits beyond which there is no positive gain.</li>
<li>built-in cross-validation<br>This is unlike GBM where we have to run a grid-search and only a limited values can be tested. </li>
<li>continue on existing model</li>
</ul>
<h3 id="XGBoost-Parameters"><a href="#XGBoost-Parameters" class="headerlink" title="XGBoost Parameters"></a>XGBoost Parameters</h3><h4 id="general-parameters"><a href="#general-parameters" class="headerlink" title="general parameters"></a>general parameters</h4><p>General Parameters: Guide the overall functioning</p>
<ul>
<li>booster:<br>default =gbtree, can be gbtree, gblinear or dart. 一般使用gbtree.</li>
<li>silent:<br>default =0, silent mode is activated if set to 1(no running messages will be printed)</li>
<li>nthread:<br>default to maximum of threads.</li>
</ul>
<h4 id="booster-parameters"><a href="#booster-parameters" class="headerlink" title="booster parameters"></a>booster parameters</h4><p>Booster Parameters: Guide the individual booster (tree/regression) at each step</p>
<ul>
<li>eta(learning rate):<br>default=0.3, typical final values to be used: 0.01-0.2, using CV to tune</li>
<li>min_child_weight:<br>minimum sum of instance weight (hessian) needed in a child. If the tree partition step results in a leaf node with the sum of instance weight less than min_child_weight, then the building process will give up further partitioning. In linear regression mode, this simply corresponds to minimum number of instances needed to be in each node. The larger, the more conservative the algorithm will be.<br>default =1,too high values can lead to under-fitting, it should be tuned using CV. 数值越小越容易过拟合，越大越容易 under-fitting.</li>
<li>max_depth:<br>default =6, typical values: 3-10, should be tuned using CV.</li>
<li>gamma:<br>default =0, Gamma specifies the minimum loss reduction required to make a split.如果在分裂过程中小于该值，那么就不会继续分裂。</li>
<li>subsample:<br>default =1, typical values: 0.5-1. Denotes the fraction of observations to be randomly samples for each tree.</li>
<li>colsample_bytree:<br>default =1, typical values: 0.5-1. colsample_bytree和subsample不同点：colsample_by是特征的随机fraction, subsample是rows的随机fraction。</li>
<li>lambda:<br>default =1, L2 regularization term on weights(analogous to Ridge regression). Though many data scientists don’t use it often, it should be explored to reduce overfitting.</li>
<li>alpha:<br>default =0, L1 regularization term on weight (analogous to Lasso regression)</li>
<li>scale_pos_weight:<br>default =1,  a value greater than 0 should be used in case of high class imbalance as it helps in faster convergence.</li>
</ul>
<h4 id="learning-task-parameters"><a href="#learning-task-parameters" class="headerlink" title="learning task parameters"></a>learning task parameters</h4><p>Learning Task Parameters: Guide the optimization performed</p>
<ul>
<li><strong>objective</strong><br>binary: logistic- returns predicated probability(not class)<br>multi: softmax- returns predicated class(not probabilities)<br>multi: softprob- returns predicated probability of each data point belonging to each class.</li>
<li>eval_metirc<br>default according to objective(rmse for regression and error for classification), used for validation data.<br>typical values: rmse(root mean square error), mse(mean absolute error), logloss(negative log-likelihood), error(binary classification error rate, 0.5 threshold), auc(area under the curve)</li>
<li>seed<br>default =0, used for reproducible results and also for <strong>parameter tuning</strong>.</li>
</ul>
<h4 id="Control-Overfitting"><a href="#Control-Overfitting" class="headerlink" title="Control Overfitting"></a>Control Overfitting</h4><p>There are in general two ways that you can control overfitting in xgboost. </p>
<ul>
<li>The first way is to directly control model complexity.</li>
<li>The second way is to add regularization parameters</li>
</ul>
<h3 id="Referrence"><a href="#Referrence" class="headerlink" title="Referrence"></a>Referrence</h3><p><a href="https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/" target="_blank" rel="noopener">complete guide parameter tuning xgboost with codes python</a><br><a href="http://xgboost.readthedocs.io/en/latest/how_to/param_tuning.html" target="_blank" rel="noopener">官方文档 param_tuning</a><br><a href="http://xgboost.readthedocs.io/en/latest/parameter.html" target="_blank" rel="noopener">官方文档 parameter</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/07/21/那些年的算法题目（二）/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jijeng Jia">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jijeng's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/07/21/那些年的算法题目（二）/" itemprop="url">那些年的算法题目（二）</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-07-21T14:13:54+08:00">
                2018-07-21
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/算法/" itemprop="url" rel="index">
                    <span itemprop="name">算法</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="最长01相同子串"><a href="#最长01相同子串" class="headerlink" title="最长01相同子串"></a>最长01相同子串</h2><p>已知一个长度为N的字符串，只由0和1组成， 求一个最长的子串，要求该子串出0和1出现的次数相等。<br>思路：<br>最简单的方式是先生成字串，然后判断每个字串是否满足0的个数和1的个数相同。这种暴力求解时间复杂度O(n^3),明显是不合理的。<br>下面说一下简单的做法：<br>定义一个数组B[N]，B[i]表示从A[0…i]中 num_of_0 - num_of_1，0的个数与1的个数的差 。那么如果A[i] ~ A[j]是符合条件的子串，一定有 B[i] == B[j]，因为中间的部分0、1个数相等，相减等于0。</p>
<p>代码实现<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">def lengest01SubStr(s):</span><br><span class="line">    &apos;&apos;&apos;</span><br><span class="line">    最长0,1 相等的子串长度</span><br><span class="line">    &apos;&apos;&apos;</span><br><span class="line">    count =[0, 0]</span><br><span class="line">    B =[0]*len(s)</span><br><span class="line">    dic =&#123;&#125;</span><br><span class="line">    lengest =0</span><br><span class="line"></span><br><span class="line">    for i in range(len(s)):</span><br><span class="line">        count[int(s[i])]  +=1</span><br><span class="line">        B[i]  =count[0] - count[1]</span><br><span class="line">        if B[i] ==0:</span><br><span class="line">            lengest +=1</span><br><span class="line">            continue</span><br><span class="line">        if B[i] in dic:</span><br><span class="line">            lengest =max(lengest, i- dic[B[i]])</span><br><span class="line">        else:</span><br><span class="line">            dic[B[i]] =i</span><br><span class="line">    return lengest</span><br><span class="line"></span><br><span class="line">a =&apos;1011010&apos;</span><br><span class="line">b =&apos;10110100&apos;</span><br><span class="line">print(lengest01SubStr(a))</span><br><span class="line">print(lengest01SubStr(b))</span><br></pre></td></tr></table></figure></p>
<h2 id="顺时针打印矩阵"><a href="#顺时针打印矩阵" class="headerlink" title="顺时针打印矩阵"></a>顺时针打印矩阵</h2><p>输入一个矩阵，按照从外向里以顺时针的顺序依次扫印出每一个数字。<br>思路：发现网上有很多使用递归的，但是使用四个循环就可以解决这个问题。找到每次开始的起点，然后按照最上面一行，最右面一列，最小面一行和最左面一行这样的顺序进行打印即可。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">def printMatrix(matrix):</span><br><span class="line">    if matrix ==[[]]:</span><br><span class="line">        return</span><br><span class="line">        # 第一次见这样判断空的matrix</span><br><span class="line">    row =len(matrix)</span><br><span class="line">    column =len(matrix[0])</span><br><span class="line">    # 这里的left, right, up, down 都是真实能够access到数据的</span><br><span class="line"></span><br><span class="line">    left =0</span><br><span class="line">    right =column -1</span><br><span class="line">    up =0</span><br><span class="line">    down =row -1</span><br><span class="line">    res =[]</span><br><span class="line">    while right &gt;left and up &lt;down:</span><br><span class="line">        # from left to right</span><br><span class="line">        for i in range(left, right+1):</span><br><span class="line">            res.append(matrix[up][i])</span><br><span class="line">        # from up to down</span><br><span class="line">        for i in range(up+1, down+1):</span><br><span class="line">            res.append(matrix[i][right])</span><br><span class="line">        # from right to left</span><br><span class="line">        for i in range(right-1, left-1, -1):</span><br><span class="line">            res.append(matrix[down][i])</span><br><span class="line">        for i in range(down-1, up, -1):</span><br><span class="line">            res.append(matrix[i][left])</span><br><span class="line">        left +=1</span><br><span class="line">        right -=1</span><br><span class="line">        up +=1</span><br><span class="line">        down -=1</span><br><span class="line">    # 最后对于这种特殊情况的处理是容易忘记的</span><br><span class="line">    # left one row 这种情况很特殊，只是从左往右遍历</span><br><span class="line">    if up ==down and left &lt;right:</span><br><span class="line">        for i in range(left, right+1):</span><br><span class="line">            res.append(matrix[up][i])</span><br><span class="line">    # left one column 只有可能是从上往下遍历</span><br><span class="line">    if left ==right and up &lt;down:</span><br><span class="line">        for i in range(up, down+1):</span><br><span class="line">            res.append(matrix[i][left])</span><br><span class="line">    if up ==down and left ==right:</span><br><span class="line">        res.append(matrix[left][up])</span><br><span class="line">    return res</span><br><span class="line">print(printMatrix(matrix))</span><br></pre></td></tr></table></figure></p>
<p>下面这个版本并没有运行成功，但是中间有个语法点是可以学习的。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">def printMatrix(matrix):</span><br><span class="line">    res =[]</span><br><span class="line">    # 第一个坐标表示行数，第二个坐标表示列数</span><br><span class="line">    # m 表示行数，n 表示列数</span><br><span class="line">    m =len(matrix)</span><br><span class="line">    n = len(matrix[0])</span><br><span class="line">    if m ==1 and n ==1:</span><br><span class="line">        res =[matrix[0][0]]</span><br><span class="line">        return res</span><br><span class="line">    else:</span><br><span class="line">        for o in range(int((min(m,n)+1)/2)):</span><br><span class="line">            # 不加这个[] 会有语法错误</span><br><span class="line">            [res.append(matrix[o][i]) for i in range(o, n-o)]</span><br><span class="line">            [res.append(matrix[j][n-o-1]) for j in range(o, m-o) if matrix[j][n-o-1] not in res ]</span><br><span class="line">            # 感觉这个最后的not in 是不合理的，万一这两个数字就是相同呢，试一下</span><br><span class="line">            [res.append(matrix[m-o-1][k]) for k in range(n-1, o-1, -1) if matrix[m-o-1][k] not in res]</span><br><span class="line">            [res.append(matrix[l][o]) for l in range(m-1-o, o-1,-1) if matrix[l][o] not in res]</span><br><span class="line">            # 这种写法还是少用，可以使用append,就少用这种，可读性不强，容易错</span><br><span class="line">        return resdef printMatrix(matrix):</span><br><span class="line">    res =[]</span><br><span class="line">    # 第一个坐标表示行数，第二个坐标表示列数</span><br><span class="line">    # m 表示行数，n 表示列数</span><br><span class="line">    m =len(matrix)</span><br><span class="line">    n = len(matrix[0])</span><br><span class="line">    if m ==1 and n ==1:</span><br><span class="line">        res =[matrix[0][0]]</span><br><span class="line">        return res</span><br><span class="line">    else:</span><br><span class="line">        for o in range(int((min(m,n)+1)/2)):</span><br><span class="line">            # 不加这个[] 会有语法错误</span><br><span class="line">            [res.append(matrix[o][i]) for i in range(o, n-o)]</span><br><span class="line">            [res.append(matrix[j][n-o-1]) for j in range(o, m-o) if matrix[j][n-o-1] not in res ]</span><br><span class="line">            # 感觉这个最后的not in 是不合理的，万一这两个数字就是相同呢，试一下</span><br><span class="line">            [res.append(matrix[m-o-1][k]) for k in range(n-1, o-1, -1) if matrix[m-o-1][k] not in res]</span><br><span class="line">            [res.append(matrix[l][o]) for l in range(m-1-o, o-1,-1) if matrix[l][o] not in res]</span><br><span class="line">            # 这种写法还是少用，可以使用append,就少用这种，可读性不强，容易错</span><br><span class="line">        return res</span><br></pre></td></tr></table></figure></p>
<h2 id="最短摘要生成"><a href="#最短摘要生成" class="headerlink" title="最短摘要生成"></a>最短摘要生成</h2><p>思路：<br>两个指针begin和end, 首先将end 指针向后移动至包含所有的字符集q的位置；然后将begin向后移动直至再移动一次将不再包含q，记录该位置长度。然后将begin向后移动一位，end继续向后移动直至包含所有q，周而复始。比较位置长度，然后得出最短距离。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line">#include &quot;stdio.h&quot;</span><br><span class="line">#include &quot;string.h&quot;</span><br><span class="line">#include &quot;assert.h&quot;</span><br><span class="line">#define MAX 1024</span><br><span class="line">int isMatchAll(const char *str,const char *key,int begin,int end)</span><br><span class="line">&#123;</span><br><span class="line">	int ret =0;</span><br><span class="line">	char hash[256];</span><br><span class="line">	int i =0;</span><br><span class="line">	int lenK = strlen(key);</span><br><span class="line">	memset(hash,0,sizeof(hash));</span><br><span class="line">	for(i=begin;i&lt;=end;i++)</span><br><span class="line">	&#123;</span><br><span class="line">		hash[str[i]]=1;</span><br><span class="line">	&#125;</span><br><span class="line">	// 标记string之后，然后再遍历key中的字符是否存在 </span><br><span class="line">	for(i=0;i&lt;lenK;i++)</span><br><span class="line">	&#123;</span><br><span class="line">		if(hash[key[i]]==0)</span><br><span class="line">			break;	</span><br><span class="line">	&#125;</span><br><span class="line">	if(i == lenK )</span><br><span class="line">		ret =1;</span><br><span class="line">		</span><br><span class="line">	return ret;</span><br><span class="line">&#125;</span><br><span class="line">void find(const char *str,const char *key)</span><br><span class="line">&#123;</span><br><span class="line"> </span><br><span class="line">	int lenS = strlen(str);</span><br><span class="line">	int lenK = strlen(key);</span><br><span class="line">	</span><br><span class="line">	int begin = 0;</span><br><span class="line">	int end = 0;</span><br><span class="line">	int minLength = 0x7FFFFFFF;</span><br><span class="line">	int mstart = 0;</span><br><span class="line">	int mend =0;</span><br><span class="line">	assert(str&amp;&amp;key);</span><br><span class="line">	for(;;)</span><br><span class="line">	&#123;</span><br><span class="line">		while(!isMatchAll(str,key,begin,end)&amp;&amp;end&lt;lenS)</span><br><span class="line">		&#123;</span><br><span class="line">			end++;		</span><br><span class="line">		&#125;</span><br><span class="line">		while(isMatchAll(str,key,begin,end))</span><br><span class="line">		&#123;</span><br><span class="line">			if(end-begin+1 &lt;minLength)</span><br><span class="line">			&#123;</span><br><span class="line">				minLength = end-begin+1;</span><br><span class="line">				mstart =begin;</span><br><span class="line">				mend =end;</span><br><span class="line">			&#125;</span><br><span class="line">			begin++;</span><br><span class="line">		&#125;</span><br><span class="line">		if(end&gt;=lenS)</span><br><span class="line">			break;</span><br><span class="line">	&#125;</span><br><span class="line">	printf(&quot;%d\n&quot;,minLength);</span><br><span class="line">	for(;mstart&lt;=mend;mstart++)</span><br><span class="line">		printf(&quot;%c&quot;,str[mstart]);</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line">/**</span><br><span class="line"></span><br><span class="line">	char *str =&quot;hello  are  you bottom of do the is bot doke astring&quot;;</span><br><span class="line">	char *key =&quot;abde&quot;</span><br><span class="line">	</span><br><span class="line">*/</span><br><span class="line">	char str[MAX];</span><br><span class="line">	char key[MAX];</span><br><span class="line">	</span><br><span class="line">	gets(str);</span><br><span class="line">	gets(key);</span><br><span class="line">	find(str,key);</span><br><span class="line">	return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/07/07/Loss-Activation-and-Optimisation-Function/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jijeng Jia">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jijeng's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/07/07/Loss-Activation-and-Optimisation-Function/" itemprop="url">Loss-Activation-and-Optimisation-Function</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-07-07T13:37:05+08:00">
                2018-07-07
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>用图说话…</p>
<h2 id="Loss-Function-Error-Function"><a href="#Loss-Function-Error-Function" class="headerlink" title="Loss Function(Error Function)"></a>Loss Function(Error Function)</h2><p>For accurate predictions, one needs to minimize the calculated error. In a neural network, this is done using back propagation.<br>损失函数是用来估量你模型的预测值f(x)与真实值Y的不一致程度，它是一个非负实值函数,通常使用L(Y,f(x))来表示，损失函数越小，模型的鲁棒性就越好。<br>按照函数种类可以划分一下主要的几个类别。</p>
<h3 id="log损失函数"><a href="#log损失函数" class="headerlink" title="log损失函数"></a>log损失函数</h3><p><img src="http://p9fyz2xe9.bkt.clouddn.com/18-7-7/49835120.jpg" alt="avatar"><br>常用于逻辑回归中。</p>
<h3 id="平方损失函数"><a href="#平方损失函数" class="headerlink" title="平方损失函数"></a>平方损失函数</h3><p>又称为最小二乘法，最优拟合直线应该是使各点到回归直线的距离和最小的直线，即平方和最小。<br><img src="http://p9fyz2xe9.bkt.clouddn.com/18-7-7/72402157.jpg" alt="avatar"><br>Y-f(X) 表示残差，残差在数理统计中是指实际观察值与估计值（拟合值）之间的差。整个式子表示的是残差的平方和。<br><img src="http://p9fyz2xe9.bkt.clouddn.com/18-7-7/25515918.jpg" alt="avatar"></p>
<h3 id="指数损失函数"><a href="#指数损失函数" class="headerlink" title="指数损失函数"></a>指数损失函数</h3><p>Adaboost的目标式子就是指数损失，在给定n个样本的情况下，Adaboost的损失函数为：<br><img src="http://p9fyz2xe9.bkt.clouddn.com/18-7-7/36556640.jpg" alt="avatar"></p>
<h3 id="Hinge损失函数"><a href="#Hinge损失函数" class="headerlink" title="Hinge损失函数"></a>Hinge损失函数</h3><p>听着名字怪怪的，但是SVM（support vector machine）的损失函数就是这个。<br><img src="http://p9fyz2xe9.bkt.clouddn.com/18-7-7/54006907.jpg" alt="avatar"><br>从目标函数看来，lr 采用的logistic loss 和 svm 采用的 hinge loss function 思想都是增加对分类影响较大点的权重，减少那些与分类相关不大点的权重。但是两个方法处理的方法不同: LR采用一个sigmod的映射函数，通过这样的非线性映射，大大降低了离分类平面点远的权重 ; SVM 采用的是一个hinge loss function，通过上图可以看到，对于那些离超平面比较远的点，直接设为0了，也就是说直接忽视，只考虑那些对分类平面有影响的点，这些点就是我们经常听到的支持向量。另一方面，支持向量机只考虑局部的边界线附近的点，而逻辑回归考虑全局（远离的点对边界线的确定也起作用，虽然作用会相对小一些）。</p>
<p>按照应用领域可以划分为:<br><strong>Regressive loss functions</strong>:<br>They are used in case of regressive problems, that is when the target variable is continuous. Most widely used regressive loss function is Mean Square Error.Other loss functions are:Absolute error — measures the mean absolute value of the element-wise difference between input.<br><strong>Classification loss functions</strong>:<br>The output variable in classification problem is usually a probability value f(x), called the score for the input x. Generally, the magnitude of the score represents the confidence of our prediction. The target variable y, is a binary variable, 1 for true and -1 for false.<br>On an example (x,y), the margin is defined as yf(x). The margin is a measure of how correct we are. Most classification losses mainly aim to maximize the margin. Some classification algorithms are: Binary Cross Entropy,Negative Log Likelihood,Margin Classifier and Soft Margin Classifier.<br><strong>Embedding loss functions</strong>:<br>It deals with problems where we have to measure whether two inputs are similar or dissimilar. Some examples are:</p>
<ul>
<li>L1 Hinge Error- Calculates the L1 distance between two inputs.</li>
<li>Cosine Error- Cosine distance between two inputs.</li>
</ul>
<p><strong>Visualising Loss Functions</strong>:<br>We performed the task to reconstruct an image using a type of neural network called Autoencoders. Different results were obtained for the same task by using different Loss Functions, while everything else in the neural network architecture remained constant. Thus, the difference in result represents the properties of the different loss functions employed. A very simple data set, MNIST data set was used for this purpose. Three loss functions were used to reconstruct images.</p>
<h2 id="Activation-Function"><a href="#Activation-Function" class="headerlink" title="Activation Function"></a>Activation Function</h2><p>What?<br>It’s just a thing (node) that you add to the output end of any neural network. It is also known as <strong>Transfer Function</strong>. It can also be attached in between two Neural Networks.</p>
<h3 id="Sigmoid-or-Logistic-Activation-Function"><a href="#Sigmoid-or-Logistic-Activation-Function" class="headerlink" title="Sigmoid or Logistic Activation Function"></a>Sigmoid or Logistic Activation Function</h3><p><img src="http://p9fyz2xe9.bkt.clouddn.com/18-7-7/23910535.jpg" alt="avatar"><br>导数比较有特点：<br><img src="http://p9fyz2xe9.bkt.clouddn.com/18-7-7/81953292.jpg" alt="avatar"></p>
<h3 id="Softmax-Function"><a href="#Softmax-Function" class="headerlink" title="Softmax Function"></a>Softmax Function</h3><p>The softmax function is a more generalized logistic activation function which is used for <strong>multiclass classification</strong>.<br><img src="http://p9fyz2xe9.bkt.clouddn.com/18-7-7/70983345.jpg" alt="avatar"><br>使用softmax和多个logistic的多分类的区别：softmax回归进行的多分类，类与类之间是互斥的，即一个输入只能被归为一类；多个logistic回归进行多分类，输出的类别并不是互斥的，即”苹果”这个词语既属于”水果”类也属于”3C”类别。</p>
<h3 id="Tanh"><a href="#Tanh" class="headerlink" title="Tanh"></a>Tanh</h3><p><img src="http://p9fyz2xe9.bkt.clouddn.com/18-7-7/80881786.jpg" alt="avatar"></p>
<h3 id="ReLU"><a href="#ReLU" class="headerlink" title="ReLU"></a>ReLU</h3><p>ReLU (Rectified Linear Unit) Activation Function<br>it is used in almost all the convolutional neural networks or deep learning.在卷积网络和深度网络中经常看到。<br><img src="http://p9fyz2xe9.bkt.clouddn.com/18-7-7/78295605.jpg" alt="avatar"></p>
<h3 id="Leaky-ReLU"><a href="#Leaky-ReLU" class="headerlink" title="Leaky ReLU"></a>Leaky ReLU</h3><p>明显的发现，Leaky ReLU是对于在 X&lt;0时候的改进。<br><img src="http://p9fyz2xe9.bkt.clouddn.com/18-7-7/87669246.jpg" alt="avatar"><br>Both Leaky and Randomized ReLU functions are monotonic in nature. Also, their derivatives also monotonic in nature.</p>
<h2 id="Optimisation-Algorithms"><a href="#Optimisation-Algorithms" class="headerlink" title="Optimisation Algorithms"></a>Optimisation Algorithms</h2><p>Optimisation Algoritms are used to update weights and biases i.e. the internal parameters of a model to reduce the error. They can be divided into two categories:<br>Back Propogation and Optimisation Function: Error J(w) is a function of internal parameters of model i.e weights and bias. For accurate predictions, one needs to minimize the calculated error. In a neural network, this is done using back propagation. The current error is typically propagated backwards to a previous layer, where it is used to modify the weights and bias in such a way that the error is minimized. The weights are modified using a function called Optimization Function.<br><img src="http://p9fyz2xe9.bkt.clouddn.com/18-7-7/71294470.jpg" alt="avatar"><br>Optimisation functions usually calculate the gradient i.e. the partial derivative of loss function with respect to weights, and the weights are modified in the opposite direction of the calculated gradient. This cycle is repeated until we reach the minima of loss function.<br><img src="http://p9fyz2xe9.bkt.clouddn.com/18-7-7/90062053.jpg" alt="avatar"></p>
<h4 id="Constant-Learning-Rate-Algorithms"><a href="#Constant-Learning-Rate-Algorithms" class="headerlink" title="Constant Learning Rate Algorithms"></a>Constant Learning Rate Algorithms</h4><p><img src="http://p9fyz2xe9.bkt.clouddn.com/18-7-7/25023504.jpg" alt="avatar"><br>Here η is called as learning rate which is a hyperparameter that has to be tuned. Choosing a proper learning rate can be difficult.<br>选的小训练速度慢， While a learning rate that is too large can hinder convergence and cause the loss function to fluctuate around the minimum or even to diverge<br>A similar hyperparameter is momentum(动量), which determines the velocity with which learning rate has to be increased as we approach the minima.</p>
<h4 id="Adaptive-Learning-Algorithms-自适应"><a href="#Adaptive-Learning-Algorithms-自适应" class="headerlink" title="Adaptive Learning Algorithms(自适应)"></a>Adaptive Learning Algorithms(自适应)</h4><p>Adaptive gradient descent algorithms such as Adagrad, Adadelta, RMSprop, Adam, provide an alternative to classical SGD. They have per-paramter learning rate methods, which provide heuristic approach without requiring expensive work in tuning hyperparameters for the learning rate schedule manually.</p>
<p>The challenge of using gradient descent is that their hyper parameters have to be defined in advance and they depend heavily on the type of model and problem. Another problem is that the same learning rate is applied to all parameter updates. If we have sparse data, we may want to update the parameters in different extent instead.(假装翻译:gradient descent 的艰难之处在于需要提前 define这种hyper parameters, 并且不同数据的learning rate 应该是不同（学习率应该根据数据的稀疏与否变化))</p>
<p>说到这我们就多说一些关于自适应算法的内容。<br>We used three first order optimisation functions and studied their effect-Stochastic Gradient Decent, Adagrad and Adam.</p>
<p>Gradient Descent calcultes gradient for the whole dataset and updates values in direction opposite to the gradients until we find a local minima. Stochastic Gradient Descent performs a parameter update for each training example unlike normal Gradient Descent which performs only one update. Thus it is much faster. Gradient Decent algorithms can further be improved by tuning important parametes like momentum, learning rate etc.</p>
<p>Adagrad is more preferrable for a sparse data set as it makes big updates for infrequent parameters and small updates for frequent parameters. It uses a different learning Rate for every parameter θ at a time step based on the past gradients which were computed for that parameter. Thus we do not need to manually tune the learning rate.</p>
<p>Adam stands for Adaptive Moment Estimation. It also calculates different learning rate. Adam works well in practice, is faster, and outperforms other techniques.</p>
<p>Stochastic Gradient Decent was much faster than the other algorithms but the results produced were far from optimum. Both, Adagrad and Adam produced better results that SGD, but they were computationally extensive. Adam was slightly faster than Adagrad. Thus, while using a particular optimization function, one has to make a trade off between more computation power and more optimum results.</p>
<p>SGD 和 GDD的区别：<br>正如上所说，在∂E/∂wi=∑（h(x)-y）<em>(xi) 的时候∑耗费了大量的时间，特别是在训练集庞大的时候。所以肯定有人会猜想，如果把求和去掉如何，即变为∂E/∂wi=（h(x)-y）</em>(xi)。（只是专注于当前训练训练集合，当前的数据）对于步长η的取值，标准梯度下降的η比随机梯度下降的大。因为标准梯度下降的是使用准确的梯度，理直气壮地走，随机梯度下降使用的是近似的梯度，就得小心翼翼地走，怕一不小心误入歧途南辕北辙了。</p>
<p>事实证明：中文和英文混在一起，排版是难看的。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/29/特征工程相关概念/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jijeng Jia">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jijeng's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/06/29/特征工程相关概念/" itemprop="url">特征工程相关概念</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-06-29T22:28:25+08:00">
                2018-06-29
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="特征离散化？"><a href="#特征离散化？" class="headerlink" title="特征离散化？"></a>特征离散化？</h2><p>连续化特征就是一些不可枚举的有理数。那么什么是离散化特征呢？ 离散化特征就是可枚举的特征。离散化的作用是把数据变成可计算状态。而特征工程就是从原始字段中根据业务提取出对模型有效的特征出来。</p>
<p>在线性模型下(w.x)，w已经确定的情况下，x的某个特征的值是20，或者30，w.x的值相差是很大的，但是往往20岁的人跟30岁的人对同一个广告的兴趣差距不会那么大。连续特征离散化的基本假设，是默认连续特征不同区间的取值对结果的贡献是不一样的。离散化和连续化最大的区别是，对一个字段做连续化后的结果就还只是一个特征，而离散化后的这一列有多少个key(字段可能的值)就会抽取出多少个特征。当经过离散化之后，特征各有各的权重，彼此之间就没有关系了。</p>
<p>模型是使用离散特征还是连续特征,其实是一个“海量离散特征+简单模型” 同 “少量连续特征+复杂模型”的权衡。既可以离散化用线性模型，也可以用连续特征加深度学习。</p>
<p>常用的选取离散点的方法：等距离离散，等样本离散、画图观察趋势和决策树模型(天生就可以对连续特征分段)。</p>
<p>在工业界，很少直接将连续值作为特征喂给逻辑回归模型，而是将连续特征离散化为一系列0、1特征交给逻辑回归模型，这样做的优势有以下几点：</p>
<ul>
<li>单变量离散化为N个后，每个变量有单独的权重，在激活函数的作用下相当于为模型增加了非线性，能够提升模型表达能力，加大拟合。</li>
<li>离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄&gt;30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰，因为特征值的异常会导致权重也就是w的值也会异常。</li>
<li>一定有同学担心特征过多会导致运算缓慢，但是LR是线性模型，我们在内部计算的时候是向量化计算，而不是循环迭代。稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展。</li>
</ul>
<p>所以不用担心像GBDT算法那样，特征多了就跑不动了(我们都说GBDT不能用离散特征不是因为它处理不了离散特征，而是因为离散化特征后会产生特别多的特征，决策树的叶子节点过多，遍历的时候太慢了)。 所以海量离散特征＋LR是业内常见的一个做法。而少量连续特征+复杂模型是另外一种做法，例如GBDT。</p>
<p>当然也可以通过深度学习来进行特征选择：目前这种手段正在随着深度学习的流行而成为一种手段，尤其是在计算机视觉领域，原因是深度学习具有自动学习特征的能力，这也是深度学习又叫unsupervised feature learning的原因。从深度学习模型中选择某一神经层的特征后就可以用来进行最终目标模型的训练了。</p>
<p>参考文献:<br><a href="https://blog.csdn.net/lujiandong1/article/details/52412123" target="_blank" rel="noopener">https://blog.csdn.net/lujiandong1/article/details/52412123</a></p>
<h2 id="组合特征"><a href="#组合特征" class="headerlink" title="组合特征"></a>组合特征</h2><p>先是离散化，然后是特征组合。<br>交叉从理论上而言是为了引入特征之间的交互，也即为了引入非线性。<br>LR(逻辑回归）分类算法:因为线性函数的表达能力有限，所以我们引入激活函数就是给LR增加非线性关系。能让一条直线变成曲线。这样可以拟合出更好的效果。（也由此才后了后来说的过拟合问题而引入了正则化超参数）<br><img src="http://p9fyz2xe9.bkt.clouddn.com/18-6-29/72457357.jpg" alt="avatar"><br>LR模型之所以很受欢迎，主要是因为LR模型本质是对数线性模型，实现简单，易于并行，大规模扩展方便，迭代速度快，同时使用的特征比较好解释，预测输出在0与1之间契合概率模型。（模型的可解释性举例，比如A-B的权重比较大，A代表用户，B代表物品，那么可以认为A是对B比较感兴趣的）但是，线性模型对于非线性关系缺乏准确刻画，特征组合正好可以加入非线性表达，增强模型的表达能力。另外，广告LR中，基本特征可以认为是用于全局建模，组合特征更加精细，是个性化建模，因为在这种大规模离散LR中，单对全局建模会对部分用户有偏，对每一用户建模又数据不足易过拟合同时带来模型数量爆炸，所以基本特征+组合特征兼顾了全局和个性化。</p>
<p>从统计的角度解释，基本特征仅仅是真实特征分布在低维的映射，不足以描述真实的分布，加入特征在高维空间拟合真实分布，使得预测更加准确。</p>
<h2 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h2><p>真正测试一个模型的不是简单与否，更重要在于它在预测新的情况时表现如何。小权重意味着网络的行为不会因为我们随意更改了一些输入而改变太多。<br><img src="http://p9fyz2xe9.bkt.clouddn.com/18-6-29/38543547.jpg" alt="avatar"><br>这是我们加了正则化之后的成本函数，可以看我们后面加入了正则化 λ 的表达式来完善成本函数。为什么加入λ能够减轻过拟合呢？直观一点的解释是设置的λ值越大，那么参数w的值就会被压缩的越小(在梯度下降中, 每次迭代的步长，也就是这个公式w=w - 学习率*成本函数对w的导数， 现在由于成本函数增加了正则项，使得J和w变得数值相关了)。 假设λ设置的足够大，那么w会无限的趋近于0. 把多隐藏层的单元的权重设置为0以后，那么基本上就是消除掉了这些单元的作用，而使得网络模型得到简化，就像下面的图一样。由于正则化的设置，消除了一些隐藏单元的作用。而使得整个模型越来越接近于线性化，也就是从下图中的过拟合往欠拟合偏转。当然我们有一个适合的λ的值，能让我们的拟合状态达到最佳。所以我们在训练模型的时候，往往都会有一个Ｌ２正则项的超参数需要我们设置。<br><img src="http://p9fyz2xe9.bkt.clouddn.com/18-6-29/26401076.jpg" alt="avatar"><br>这是我们的tanh激活函数， 可以看到当z的值越大时，整个函数的非线性就越大，而z的值越小(图中红色加粗部分),函数就越是呈现出线性分布。 所以当我们增加λ的值， w得值就越小，相应的z的值也就越小。因为z = wx + b。 而我们第一次说激活函数的时候就说过神经网络中基本上是不使用线性函数作为激活函数的，因为不论有多少层，多少个单元，线性激活函数会使得所有单元所计算的都呈现线性状态。</p>
<h2 id="杂货铺"><a href="#杂货铺" class="headerlink" title="杂货铺"></a>杂货铺</h2><p>特征工程可以分为特征处理、Feature Selection（特征选择）、Feature Extraction（特征提取）和Feature construction（特征构造）等阶段。<br>归一化（去中心，方差归一）是属于特征(预)处理:把特征值压缩成0~1的区间。</p>
<p>One-hot（也叫One-of-k）的方法把每个无序特征转化为一个数值向量。比如一个无序特征color有三种取值：red，green，blue。那么可以用一个长度为3的向量来表示它，向量中的各个值分别对应于red，green，blue。如：这种方法在NLP里用的很多，就是所谓的词向量模型。变换后的向量长度对于词典长度，每个词对应于向量中的一个元素。</p>
<p>对于欠拟合: 增加神经网络复杂度，出现欠拟合的原因之一是由于函数的非线性不足，所以用更复杂的网络模型进行训练来加深拟合。<br>对于过拟合：增加数据规模， 出现过拟合的原因之一是数据规模不足而造成的数据分布不均，扩展数据规模能比较好的解决这个问题。当然另一个做法是正则化，我们采取使用正则化来解决过拟合问题，常用的是L2正则，其他的还有L1和 Dropout正则。</p>
<p>很多人会想着既然线性分类器搞不定，那就直接找个非线性的好了，比如高斯核的SVM。我们确实可以通过这种简单换算法的方式解决这个简单的问题。但对于很多实际问题（如广告点击率预测），往往特征非常多，这时候时间约束通常不允许我们使用很复杂的非线性分类器。这也是为什么算法发展这么多年，广告点击率预测最常用的方法还是Logistic Regression (LogisticReg)。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/29/常见的排序算法总结/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jijeng Jia">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jijeng's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/06/29/常见的排序算法总结/" itemprop="url">常见的排序算法总结</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-06-29T22:27:13+08:00">
                2018-06-29
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/算法/" itemprop="url" rel="index">
                    <span itemprop="name">算法</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="分类和总结"><a href="#分类和总结" class="headerlink" title="分类和总结"></a>分类和总结</h3><ul>
<li>根据待排序的数据大小不同，使得排序过程中所涉及的存储器不同，可分为内部排序和外部排序。</li>
<li>排序关键字可能出现重复，根据重复关键字的排序情况可分为稳定排序和不稳定排序。</li>
<li>对于内部排序，依据不同的排序原则，可分为插入排序、交换(快速)排序、选择排序、归并排序和计数排序。</li>
<li>针对内部排序所需的工作量划分，可分为:简单排序 O(n^2)、先进排序 O(nlogn)和基数排序 O(d*n)。<br>常见算法的性质总结：<br><img src="http://p9fyz2xe9.bkt.clouddn.com/18-6-29/52393195.jpg" alt="avatar"></li>
</ul>
<p>排序算法实现默认都是升序…</p>
<h3 id="插入排序-Insert-Sort"><a href="#插入排序-Insert-Sort" class="headerlink" title="插入排序(Insert Sort)"></a>插入排序(Insert Sort)</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">def insert_sort(lists):</span><br><span class="line">    count = len(lists)</span><br><span class="line">    for i in range(1, count):</span><br><span class="line">        key =lists[i]</span><br><span class="line">        j =i-1</span><br><span class="line">        while j &gt;= 0:</span><br><span class="line">            if lists[j] &gt;key:</span><br><span class="line">                lists[j+1] =lists[j]</span><br><span class="line">                lists[j] =key</span><br><span class="line">            j -= 1</span><br><span class="line">    return lists</span><br><span class="line"></span><br><span class="line"># test</span><br><span class="line">lists =[1,2,-3, 90,34]</span><br><span class="line">print(insert_sort(lists))</span><br></pre></td></tr></table></figure>
<h3 id="选择排序-Select-Sort"><a href="#选择排序-Select-Sort" class="headerlink" title="选择排序(Select Sort)"></a>选择排序(Select Sort)</h3><p>为每个位置选择当前元素最小的。选择最小的元素和a[0]交换，选择次最小的和a[1]交换，以此类推。<br><img src="http://p9fyz2xe9.bkt.clouddn.com/18-6-29/15755472.jpg" alt="avatar"><br>代码实现:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">def select_sort(lists):</span><br><span class="line">    count =len(lists)</span><br><span class="line">    for i in range(0, count):</span><br><span class="line">        min =i</span><br><span class="line">        for j in range(i+1, count):</span><br><span class="line">            if lists[min] &gt;lists[j]:</span><br><span class="line">                min =j</span><br><span class="line">        lists[min], lists[i] =lists[i], lists[min]</span><br><span class="line">    return lists</span><br><span class="line"></span><br><span class="line"># test</span><br><span class="line">lists =[1, 23,45, 0,-1]</span><br><span class="line">print(select_sort(lists))</span><br></pre></td></tr></table></figure></p>
<h3 id="冒泡排序-Bubble-Sort"><a href="#冒泡排序-Bubble-Sort" class="headerlink" title="冒泡排序(Bubble Sort)"></a>冒泡排序(Bubble Sort)</h3><p>重复遍历要排序的数列，一次比较两个元素，如果顺序错误就交换位置。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">def bubble_sort(lists):</span><br><span class="line">    count =len(lists)</span><br><span class="line">    for i in range(0, count):</span><br><span class="line">        for j in range(i+1, count):</span><br><span class="line">            if lists[i]&gt; lists[j]:</span><br><span class="line">                lists[i], lists[j] =lists[j], lists[i]</span><br><span class="line">    return lists</span><br><span class="line"></span><br><span class="line"># test</span><br><span class="line">lists =[1,34,45,0,89]</span><br><span class="line">print(bubble_sort(lists))</span><br></pre></td></tr></table></figure></p>
<h3 id="归并排序-Merge-Sort"><a href="#归并排序-Merge-Sort" class="headerlink" title="归并排序(Merge Sort)"></a>归并排序(Merge Sort)</h3><p>归并排序，应该是我第一个接触的排序算法。然后在在某次重要面试时候，使用该算法救急。该算法是采用采用分治法(Divide and Conquer)的思想:先使得子序列有序，然后合并子序列。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">def merge(left, right):</span><br><span class="line">    i, j =0,0</span><br><span class="line">    result =[]</span><br><span class="line">    while i&lt;len(left) and j &lt;len(right):</span><br><span class="line">        if left[i] &lt;= right[j]:</span><br><span class="line">            result.append(left[i])</span><br><span class="line">            i +=1</span><br><span class="line">        else:</span><br><span class="line">            result.append(right[j])</span><br><span class="line">            j +=1</span><br><span class="line">    result += left[i:]</span><br><span class="line">    result += right[j:]</span><br><span class="line">    return result</span><br><span class="line"></span><br><span class="line">def merge_sort(lists):</span><br><span class="line">    if len(lists) &lt;=1:</span><br><span class="line">        return  lists</span><br><span class="line">    num =int(len(lists)/2)</span><br><span class="line">    left =merge_sort(lists[:num])</span><br><span class="line">    right = merge_sort(lists[num:])</span><br><span class="line">    return merge(left, right)</span><br><span class="line"># merge_sort 是先切分，然后再整合，quick sort 是两个指针</span><br><span class="line"></span><br><span class="line"># test</span><br><span class="line">lists =[1, 34, 23,45,0,9]</span><br><span class="line">print(merge_sort(lists))</span><br></pre></td></tr></table></figure></p>
<h3 id="快速排序-Quick-Sort"><a href="#快速排序-Quick-Sort" class="headerlink" title="快速排序(Quick Sort)"></a>快速排序(Quick Sort)</h3><p>快速排序的思想：任意选择一个key(通常选择a[0])，将比他小的数据放在它的前面，比他大的数字放在它的后面。递归进行。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">def quick_sort(lists, left, right):</span><br><span class="line">    if left &gt;= right:</span><br><span class="line">        return lists</span><br><span class="line">    key =lists[left]</span><br><span class="line">    low =left</span><br><span class="line">    high  =right</span><br><span class="line">    while left &lt; right:</span><br><span class="line">        # 因为你最初key 取得是left，然后从右边找到一个比key小的，然后替换left 的位置</span><br><span class="line">        while left &lt;right and lists[right]&gt;=key:</span><br><span class="line">            right  -= 1</span><br><span class="line">        lists[left] =lists[right]</span><br><span class="line">        while left &lt; right and lists[left] &lt;= key:</span><br><span class="line">            left +=1</span><br><span class="line">        lists[right] =lists[left]</span><br><span class="line">    lists[left] =key</span><br><span class="line">    # 这里的left 和right 都是可以的，因为从while 中出来之后两者是相同的</span><br><span class="line">    # 这个步伐是1,所以只能是一个个变化</span><br><span class="line">    quick_sort(lists, low, left-1)</span><br><span class="line">    quick_sort(lists, left+1, high)</span><br><span class="line">    # 因为left的位置已经被占了，所以只是划分左边一块，右边一块就是可以的</span><br><span class="line">    return  lists</span><br><span class="line"># test</span><br><span class="line">lists =[3,2,45, 100,1,56,56]</span><br><span class="line">#lists =[1,2,3,2,2,2,5,4,2]</span><br><span class="line">print(quick_sort(lists, 0, len(lists)-1))</span><br></pre></td></tr></table></figure></p>
<h3 id="堆排序"><a href="#堆排序" class="headerlink" title="堆排序"></a>堆排序</h3><p>参看另一篇<a href="https://jijeng.github.io/2018/06/22/%E9%82%A3%E4%BA%9B%E5%B9%B4%E7%9A%84%E9%9D%A2%E8%AF%95%E9%A2%98%E7%9B%AE/" target="_blank" rel="noopener">博客</a>中堆排序。</p>
<h3 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h3><p><a href="http://ictar.github.io/2015/12/01/%E4%B9%9D%E5%A4%A7%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%E5%8F%8A%E5%85%B6Python%E5%AE%9E%E7%8E%B0%E4%B9%8B%E7%AE%80%E5%8D%95%E6%8E%92%E5%BA%8F/" target="_blank" rel="noopener">算法动图效果</a><br><a href="http://python.jobbole.com/82270/" target="_blank" rel="noopener">排序算法分类</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/22/那些年的算法题目（一）/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jijeng Jia">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jijeng's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/06/22/那些年的算法题目（一）/" itemprop="url">那些年的算法题目（一）</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-06-22T22:58:40+08:00">
                2018-06-22
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/算法/" itemprop="url" rel="index">
                    <span itemprop="name">算法</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>交作业了…</p>
<h2 id="完全二叉树插入"><a href="#完全二叉树插入" class="headerlink" title="完全二叉树插入"></a>完全二叉树插入</h2><h3 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h3><p>已知一个完全二叉树的结构，现在需要将一个节点插入到这颗完全二叉树的最后，使得它还是一个完全二叉树。<br>第一种解法：如果该树为满二叉树或者左子树不为满二叉树，那么就进入左子树，否则进入右子树，递归进行。</p>
<h3 id="二叉树-Binary-Tree"><a href="#二叉树-Binary-Tree" class="headerlink" title="二叉树(Binary Tree)"></a>二叉树(Binary Tree)</h3><p>强行补充一下关于二叉树概念的知识。<br>完全二叉树(Complete Binary Tree):若设二叉树的深度为h，除第h层外，其它各层(1～h-1)的结点数都达到最大个数，第h层所有的结点都连续集中在最左边，这就是完全二叉树。<br>满二叉树:树中除了叶子节点，每个节点都有两个子节点。满二叉树是一种特殊的完全二叉树。<br>二叉搜索树(binary search tree):所有非叶子结点至多拥有两个儿子（Left和Right）；所有结点存储一个关键字,非叶子结点的左指针指向小于其关键字的子树，右指针指向大于其关键字的子树。<br>平衡二叉树(AVL树)：它是一颗空树或它的左右两个子树的高度差的绝对值不超过1。<br>哈夫曼树：带权路径长度达到最小的二叉树，也叫做最优二叉树。<br>树的深度和高度：深度是从上往下数；高度是从下往上数<br><img src="http://p9fyz2xe9.bkt.clouddn.com/18-6-20/23050585.jpg" alt="avatar"></p>
<h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><p>平滑过渡到本问题的代码实现。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">#include&lt;iostream&gt;</span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">typedef struct Node&#123;</span><br><span class="line">	int value;</span><br><span class="line">	struct Node *lchild, *rchild;</span><br><span class="line">&#125;Tree;</span><br><span class="line"></span><br><span class="line">int GetLeftDepth(Tree* root)&#123;</span><br><span class="line">	Tree* pNode =root-&gt;lchild ;</span><br><span class="line">	int depth =0;</span><br><span class="line">	while(pNode != NULL)</span><br><span class="line">	&#123;</span><br><span class="line">		depth ++;</span><br><span class="line">		pNode =pNode-&gt;lchild;</span><br><span class="line">	&#125;</span><br><span class="line">	return depth;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">int GetRightDepth(Tree* root)&#123;</span><br><span class="line">	Tree* pNode =root-&gt;rchild;</span><br><span class="line">	int depth =0 ;</span><br><span class="line">	while(pNode != NULL)</span><br><span class="line">	&#123;</span><br><span class="line">		depth ++ ;</span><br><span class="line">		pNode =pNode-&gt;rchild ;</span><br><span class="line">	&#125;</span><br><span class="line">	return depth;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">bool IsFullBinaryTree(Tree* root)</span><br><span class="line">&#123;</span><br><span class="line">	return GetLeftDepth(root) == GetRightDepth(root) ;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">void insert(Tree* root, Tree * node)&#123;</span><br><span class="line"></span><br><span class="line">	if (IsFullBinaryTree(root) || !IsFullBinaryTree(root-&gt;lchild))&#123;</span><br><span class="line">		insert(root-&gt;lchild, node);</span><br><span class="line">		return ;</span><br><span class="line">	&#125;</span><br><span class="line">	if (root-&gt;rchild ==NULL)&#123;</span><br><span class="line">		root-&gt;rchild =node ;</span><br><span class="line">		return ;</span><br><span class="line">	&#125;</span><br><span class="line">	insert(root-&gt;rchild, node) ;</span><br><span class="line">&#125;</span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line">	Node* a = new Node();</span><br><span class="line">	a-&gt;value =1;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>第二种思路，如果已知之前树的个数，那么可以使用前序遍历的方式，得到将要插入的的节点的位置，然后插入。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">int insert(Tree *t, int n, struct Node *node);</span><br><span class="line"># n表示原来二叉树节点的个数</span><br><span class="line"># 前序遍历 </span><br><span class="line">void printTree(Tree* root)&#123;</span><br><span class="line">    if(root ==NULL)</span><br><span class="line">    &#123;</span><br><span class="line">    return ;</span><br><span class="line">    &#125;</span><br><span class="line">    print(root-&gt;value)</span><br><span class="line">    printTree(root-&gt;lchild)</span><br><span class="line">    printTree(root-&gt;rchild)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>第三种思路：<br>因为是完全二叉树，那么插入的点只能在最下一层。于是我们可以去找最下一层的中间点，找到根的左子树的最右下的节点，如果这个点存在，那么说明，最下一层的左边已经填满，递归右子树，否则递归左子树。</p>
<p>参考文献<br><a href="http://www.voidcn.com/article/p-kbxsvnyq-yq.html" target="_blank" rel="noopener">http://www.voidcn.com/article/p-kbxsvnyq-yq.html</a><br><a href="https://www.toutiao.com/i6192546626911126017/" target="_blank" rel="noopener">https://www.toutiao.com/i6192546626911126017/</a><br><a href="https://blog.csdn.net/psc0606/article/details/48742239" target="_blank" rel="noopener">https://blog.csdn.net/psc0606/article/details/48742239</a></p>
<h2 id="inplace-去除连续的-0"><a href="#inplace-去除连续的-0" class="headerlink" title="inplace 去除连续的 0"></a>inplace 去除连续的 0</h2><p>给定一个一维整数数组，不使用额外的空间，本地去掉数组中连续的0。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">#include&lt;iostream&gt;</span><br><span class="line">using namespace std;</span><br><span class="line">int RemoveDuplicates(int* sortBuffer,int length)</span><br><span class="line">&#123;</span><br><span class="line"></span><br><span class="line">    if(sortBuffer == NULL || length == 0)</span><br><span class="line">    &#123;</span><br><span class="line">        return false;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    int count = 0;</span><br><span class="line">    for(int i = 1; i &lt; length; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        if(sortBuffer[i] ==0 &amp;&amp; 0 == sortBuffer[i-1])</span><br><span class="line">        &#123;</span><br><span class="line">            continue;</span><br><span class="line">        &#125;</span><br><span class="line">        else</span><br><span class="line">        &#123;</span><br><span class="line">            sortBuffer[count]=sortBuffer[i];</span><br><span class="line">            count++;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    return count;    </span><br><span class="line">&#125;</span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line">    int length =sizeof(array)/sizeof(int);</span><br><span class="line">    </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h2 id="最大连续子数组和"><a href="#最大连续子数组和" class="headerlink" title="最大连续子数组和"></a>最大连续子数组和</h2><p>已知一个整数二维数组，求最大的子数组和(子数组的定义从左上角(x0,y0) 到右下角(x1,y1)的数组)<br>先考虑一维整数数组的情况。<br>最大连续子序列的DP动态转移方程为<br><img src="http://p9fyz2xe9.bkt.clouddn.com/18-6-21/32657894.jpg" alt="avatar"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">#include&lt;iostream&gt;</span><br><span class="line">using namespace std;</span><br><span class="line">int Max(int a, int b)&#123;</span><br><span class="line">	return a&gt;b ?a:b;</span><br><span class="line">&#125;</span><br><span class="line">int FindGreatestSubarray(int *arr, int n)&#123;</span><br><span class="line">	int sum =arr[0];</span><br><span class="line">	int max =arr[0];</span><br><span class="line">	for(int i =1; i&lt;n; i++)&#123;</span><br><span class="line">		sum =Max(sum+arr[i], arr[i]);</span><br><span class="line">        max =Max(sum, max)</span><br><span class="line">        /**</span><br><span class="line">		if(sum &gt;=max)&#123;</span><br><span class="line">			max =sum;</span><br><span class="line">		&#125; </span><br><span class="line">        */</span><br><span class="line">	&#125;</span><br><span class="line">	return max;</span><br><span class="line">&#125;</span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line">	return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>本题目的要求是从二位的数组中求解最大的子矩阵。我们可以将其转化成一维数组的问题。如果是二维数组可以压缩为一维数组（我当时也是不懂这里）。如果最大子矩阵和原矩阵等高，就可以这样压缩。<br><img src="http://p9fyz2xe9.bkt.clouddn.com/18-6-21/83888372.jpg" alt="avatar"><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">#include&lt;stdio.h&gt;</span><br><span class="line">#include&lt;iostream&gt;</span><br><span class="line">#include&lt;vector&gt;</span><br><span class="line">using namespace std;</span><br><span class="line">#define inf 0x3f3f3f3f</span><br><span class="line">int Max(int a, int b)&#123;</span><br><span class="line">    return a&gt;b? a:b;</span><br><span class="line">&#125;</span><br><span class="line">// 求解一维数组的最大连续子数列</span><br><span class="line">int FindGreatestSubarray(int *arr, int n)&#123;</span><br><span class="line">    int sum =arr[0];</span><br><span class="line">    int max =arr[0];</span><br><span class="line">    for(int i =1;i&lt;n;i++)&#123;</span><br><span class="line">        sum =Max(sum+arr[i], arr[i])</span><br><span class="line">        if(sum &gt;=max)&#123;</span><br><span class="line">            max =sum;</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">    &#125;</span><br><span class="line">    return max;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">int GreatestMatrix(int[][] arr, int rows, int cols)&#123;</span><br><span class="line">    int maxVal =- inf</span><br><span class="line">    </span><br><span class="line">    for(int i =0 ; i &lt;rows; i++)&#123;</span><br><span class="line">        vector&lt;int&gt; temp(arr[i]);</span><br><span class="line">        maxVal =Max(maxVal, FindGreatestSubarray(temp));</span><br><span class="line">        // 得到第一行的最大和</span><br><span class="line">        // 将行的n个元素加到上一行，然后计算最大和</span><br><span class="line">        for(int j =i+1; j&lt;rows; j++)&#123;</span><br><span class="line">            for(int k =0;k&lt;cols ;k++)&#123;</span><br><span class="line">                temp[k] =arr[j][k];</span><br><span class="line">            &#125;</span><br><span class="line">            // 依次0~k行的最大和</span><br><span class="line">            maxVal =Max(maxVal, FindGreatestSubarray(temp))</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">int main()&#123;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h2 id="聚类-clustering"><a href="#聚类-clustering" class="headerlink" title="聚类(clustering )"></a>聚类(clustering )</h2><p>聚类是一种无监督学习(Unsupervised Learning)，该算法基于数据内部的特征寻找样本的自然族群(集群)。通常使用数据可视化来评价结果。应用场景：新闻聚类，文章推荐，细分客户。</p>
<p>最常见的聚类算法就是K均值(K-Means)：以空间中k个点为中心进行聚类，对最靠近他们的对象归类，通过迭代的方法，逐次更新各聚类中心的值，直到得到最好的聚类结果。最后希望达到的目的：聚类中的对象相似度较高，聚类之间的相似度比较低。</p>
<p>相似度计算：不同的算法需要的”相似度”是不一样的，对于空间中的点，我们一般选取欧式距离来进行衡量，认为距离越近，数据之间越相似。</p>
<h2 id="堆排序"><a href="#堆排序" class="headerlink" title="堆排序"></a>堆排序</h2><p>堆是一种<strong>完全二叉树</strong>，堆排序是一种树形选择排序，其时间复杂度为O(nlogn)，空间复杂度:对于记录较少的文件不推荐使用，对于较大的文件还是有效的.堆分为大根堆和小根堆。大根堆的要求是每个节点的值都不大于其父节点的值，即A[PARENT[i]] &gt;= A[i]。小根堆的要求是每个节点的值都不小于其父节点的值，即A[PARENT[i]] &lt;= A[i]。</p>
<p>堆栈是计算机的两种最基本的数据结构。堆的特点就是FIFO(first in first out)先进先出，栈的特性正好与堆相反，是属于FILO(first in/last out)先进后出的类型。<br>堆的每次调整交换堆顶和最后一个元素，然后只是调整堆顶和堆顶的左右孩子树的关系。<br><img src="http://p9fyz2xe9.bkt.clouddn.com/18-6-21/19691875.jpg" alt="avatar"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"># heap modify</span><br><span class="line">def MAX_Heapify(heap, HeapSize, root):</span><br><span class="line">    left =2* root+1</span><br><span class="line">    right = left +1</span><br><span class="line">    larger =root</span><br><span class="line">    if left &lt;HeapSize and heap[larger] &lt;heap[left]:</span><br><span class="line">        larger =left</span><br><span class="line">    if right &lt; HeapSize and heap[larger] &lt;heap[right]:</span><br><span class="line">        larger =right</span><br><span class="line">    # if modify the larger then exchange it</span><br><span class="line">    if larger != root:</span><br><span class="line">        heap[larger], heap[root] =heap[root], heap[larger]</span><br><span class="line">        MAX_Heapify(heap, HeapSize, larger)</span><br><span class="line"></span><br><span class="line"># Build the heap</span><br><span class="line">def Build_MAX_Heap(heap):</span><br><span class="line">    HeapSize =len(heap)</span><br><span class="line">    # from the end to the begin</span><br><span class="line">    for i in range((HeapSize -2)//2, -1,-1):</span><br><span class="line">        MAX_Heapify(heap, HeapSize, i)</span><br><span class="line"></span><br><span class="line"># sort after building the heap</span><br><span class="line">def HeapSort(heap):</span><br><span class="line">    Build_MAX_Heap(heap)</span><br><span class="line">    for i in range(len(heap)-1, -1, -1):</span><br><span class="line">        heap[0], heap[i] =heap[i], heap[0]</span><br><span class="line">        MAX_Heapify(heap, i, 0)</span><br><span class="line">    return heap</span><br><span class="line"></span><br><span class="line">if __name__ ==&quot;__main__&quot;:</span><br><span class="line">    a =[30, 50, 57, 77, 62, 78, 94, 80, 84]</span><br><span class="line">    print(a)</span><br><span class="line">    print(&quot;without sort but with build heap&quot;)</span><br><span class="line">    Build_MAX_Heap(a)</span><br><span class="line">    print(a)</span><br></pre></td></tr></table></figure>
<p>参考文献<br><a href="https://blog.csdn.net/minxihou/article/details/51850001" target="_blank" rel="noopener">https://blog.csdn.net/minxihou/article/details/51850001</a><br><a href="https://blog.csdn.net/chibangyuxun/article/details/53018294" target="_blank" rel="noopener">https://blog.csdn.net/chibangyuxun/article/details/53018294</a></p>
<h2 id="KMP-字符串高效查找"><a href="#KMP-字符串高效查找" class="headerlink" title="KMP(字符串高效查找)"></a>KMP(字符串高效查找)</h2><p>假设两个字符串的长度分别是m，n(m&gt;n)，在长度为m中的字符串查找长度为n的字符串，最常见的方式是暴力求解，但是这个常规解法的时间复杂度是O(nm)。KMP通过一个O(n)的预处理，可以使得时间复杂度降为O(n+m).<br>代码实现(<a href="https://www.youtube.com/watch?v=GTJr8OvyEVQ" target="_blank" rel="noopener">视频讲解</a>(科学上网))<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">def kmp_match(s, p):</span><br><span class="line">    m, n =len(s) ,len(p)</span><br><span class="line">    cur =0</span><br><span class="line">    table = partial_table(p)</span><br><span class="line">    while cur &lt;= m-n:</span><br><span class="line">        for i in range(n):</span><br><span class="line">            if s[i+cur] != p[i]:</span><br><span class="line">                cur += max(i -table[i-1], 1)</span><br><span class="line">                break</span><br><span class="line">            else:</span><br><span class="line">                return True</span><br><span class="line">    return False</span><br><span class="line"></span><br><span class="line">def partial_table(p):</span><br><span class="line">    prefix =set()</span><br><span class="line">    postfix =set()</span><br><span class="line">    ret =[0]</span><br><span class="line">    for i in range(1, len(p)):</span><br><span class="line">        prefix.add(p[:i])</span><br><span class="line">        postfix =&#123; p[j:i+1] for j in range(1, i+1)&#125;</span><br><span class="line">        ret.append(len((prefix &amp; postfix or &#123;&apos;&apos;&#125;).pop())) # &amp;两个set求交集</span><br><span class="line">    return ret</span><br><span class="line">print(partial_table(&apos;ABCDABD&apos;))</span><br><span class="line">print(kmp_match(&quot;BBC ABCDAB ABCDABCDABDE&quot;, &quot;ABCDABD&quot;))</span><br></pre></td></tr></table></figure></p>
<p>参考文献<br><a href="https://www.cnblogs.com/fanguangdexiaoyuer/p/8270332.html" target="_blank" rel="noopener">https://www.cnblogs.com/fanguangdexiaoyuer/p/8270332.html</a></p>
<h2 id="二叉树的遍历"><a href="#二叉树的遍历" class="headerlink" title="二叉树的遍历"></a>二叉树的遍历</h2><p>在python中二叉树的结构:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">class BinNode():</span><br><span class="line">    def __init__(self, val):</span><br><span class="line">        self.value =val</span><br><span class="line">        self.lchild =None</span><br><span class="line">        self.rchild =None</span><br></pre></td></tr></table></figure></p>
<h3 id="先序遍历-preOrder"><a href="#先序遍历-preOrder" class="headerlink" title="先序遍历(preOrder)"></a>先序遍历(preOrder)</h3><p>第一种思路是递归实现，第二种思路借助栈的结构来实现。栈的大小空间为O(h)，h为二叉树高度；时间复杂度为O(n)，n是树的节点的个数。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"># 递归</span><br><span class="line">def preOrder(self, root):</span><br><span class="line">    if root == None:</span><br><span class="line">        return</span><br><span class="line">    print(root.val)</span><br><span class="line">    self.preOrder(root.lchild)</span><br><span class="line">    self.preOrder(root.rchild)</span><br><span class="line"></span><br><span class="line"># 借助栈结构</span><br><span class="line">def preOrder(self, root):</span><br><span class="line">    if root == None:</span><br><span class="line">        return</span><br><span class="line">    myStack =[]</span><br><span class="line">    node =root</span><br><span class="line">    while node or myStack:</span><br><span class="line">        while node:</span><br><span class="line">            print(node.val)</span><br><span class="line">            myStack.append(node)</span><br><span class="line">            node =node.lchild</span><br><span class="line">        node =myStack.pop()</span><br><span class="line">        node =node.rchild</span><br></pre></td></tr></table></figure></p>
<h3 id="中序遍历-inOrder"><a href="#中序遍历-inOrder" class="headerlink" title="中序遍历(inOrder)"></a>中序遍历(inOrder)</h3><p>递归和非递归两种实现思路。入栈的顺序是一样的，只是改变的遍历(print())的顺序.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"># 递归</span><br><span class="line">def inOrder(self, root):</span><br><span class="line">    if root ==None:</span><br><span class="line">        return</span><br><span class="line">    self.inOrder(root.lchild)</span><br><span class="line">    print(root.val)</span><br><span class="line">    self.inOrder(root.rchild)</span><br><span class="line"></span><br><span class="line"># 借助栈结构</span><br><span class="line">def inOrder(self, root):</span><br><span class="line">    if root ==None:</span><br><span class="line">        return</span><br><span class="line">    myStack =[]</span><br><span class="line">    node =root</span><br><span class="line">    while node or myStack:</span><br><span class="line">        while node:</span><br><span class="line">            myStack.append(node)</span><br><span class="line">            node =node.lchild</span><br><span class="line">        node = myStack.pop()</span><br><span class="line">        print(node.val)</span><br><span class="line">        node =node.rchild</span><br></pre></td></tr></table></figure></p>
<h3 id="后序遍历-post-order"><a href="#后序遍历-post-order" class="headerlink" title="后序遍历(post order)"></a>后序遍历(post order)</h3><p>仍然是递归和非递归版本，非递归中使用两个stack,两个stack的后进先出等于一个先进先出。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"># 递归</span><br><span class="line">def postOrder(self, root):</span><br><span class="line">    if root == None:</span><br><span class="line">        return</span><br><span class="line">    self.postOrder(root.lchild)</span><br><span class="line">    self.postOrder(root.rchild)</span><br><span class="line">    print(root.val)</span><br><span class="line"></span><br><span class="line"># 借助栈结构</span><br><span class="line">def postOrder(self, root):</span><br><span class="line">    if root ==None:</span><br><span class="line">        return</span><br><span class="line">    myStack1 =[]</span><br><span class="line">    myStack2 =[]</span><br><span class="line">    node =root</span><br><span class="line">    myStack1.append(node)</span><br><span class="line">    while myStack1:</span><br><span class="line">        node =myStack1.pop()</span><br><span class="line">        if node.lchild:</span><br><span class="line">            myStack1.append(node.lchild)</span><br><span class="line">        if node.rchild:</span><br><span class="line">            myStack1.append(node.rchild)</span><br><span class="line">        myStack2.append(node)</span><br><span class="line">    while myStack2:</span><br><span class="line">        print(myStack2.pop().val)</span><br></pre></td></tr></table></figure></p>
<h3 id="层序遍历"><a href="#层序遍历" class="headerlink" title="层序遍历"></a>层序遍历</h3><p>使用到了队列的思想，先进先出。实际上，用的是Python中list.pop(0).注意默认是list.pop(-1),也就是默认弹出的是最后一个元素。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">def levelOrder(self, root):</span><br><span class="line">    if root ==None:</span><br><span class="line">        return</span><br><span class="line">    myQueue =[]</span><br><span class="line">    node =root</span><br><span class="line">    myQueue.append(node)</span><br><span class="line">    while myQueue:</span><br><span class="line">        # remove and return item at index (default last)</span><br><span class="line">        node =myQueue.pop(0) </span><br><span class="line">        print(node.val)</span><br><span class="line">        if node.lchild != None:</span><br><span class="line">            myQueue.append(node.lchild)</span><br><span class="line">        if node.rchild != None:</span><br><span class="line">            myQueue.append(node.rchild)</span><br></pre></td></tr></table></figure></p>
<p>参考文献<br><a href="https://blog.yangx.site/2016/07/22/Python-binary-tree-traverse/" target="_blank" rel="noopener">https://blog.yangx.site/2016/07/22/Python-binary-tree-traverse/</a></p>
<h2 id="旋转数组找最小-大-值"><a href="#旋转数组找最小-大-值" class="headerlink" title="旋转数组找最小(大)值"></a>旋转数组找最小(大)值</h2><p>把一个数组最开始的若干个元素搬到数组的末尾，我们称之为数组的旋转。 输入一个非递减排序的数组的一个旋转，输出旋转数组的最小元素。 例如数组{3,4,5,1,2}为{1,2,3,4,5}的一个旋转，该数组的最小值为1。 NOTE：给出的所有元素都大于0，若数组大小为0，请返回0。</p>
<h3 id="暴力求解"><a href="#暴力求解" class="headerlink" title="暴力求解"></a>暴力求解</h3><p>大多数的问题都是可以暴力求解的–鲁迅。(In China 凡是不知道是谁说，都可以说是鲁迅说的; In US，凡是不知道谁说的，as said by Albert Einstein)<br>因为原来的数组假设是增序，所以如果出现了的某一个元素比上一个元素小，该元素就是这个序列中的最小值。(这个情况具有唯一性吧).时间复杂度O(N)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">def minNumberInRotateArray(rotateArray):</span><br><span class="line">    arr =rotateArray # just because of laziness</span><br><span class="line">    if not arr:</span><br><span class="line">        return 0</span><br><span class="line">    if len(arr) ==2:</span><br><span class="line">        return arr[1]</span><br><span class="line">    num =arr[0]</span><br><span class="line">    for i in range(1, len(arr)):</span><br><span class="line">        if arr[i] &gt;= num:</span><br><span class="line">            num =arr[i]</span><br><span class="line">        else:</span><br><span class="line">            return arr[i]</span><br></pre></td></tr></table></figure></p>
<h3 id="递归版本"><a href="#递归版本" class="headerlink" title="递归版本"></a>递归版本</h3><p>旋转数组也是一种有序数组，时间复杂度O(N)…，面试官说改进吧…使用二分法降到O(logN)。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">def minNumberInRotateArray(rotateArray):</span><br><span class="line">    arr = rotateArray</span><br><span class="line">    if not arr:</span><br><span class="line">        return 0</span><br><span class="line">    if len(arr) ==2:</span><br><span class="line">        return arr[1]</span><br><span class="line">    mid =int(len(arr) /2)</span><br><span class="line">    if arr[mid] &gt; arr[0]:</span><br><span class="line">        return minNumberInRotateArray(arr[mid:])</span><br><span class="line">    elif arr[mid] &lt;arr[0]:</span><br><span class="line">        return minNumberInRotateArray(arr[:mid+1])</span><br><span class="line">    else:</span><br><span class="line">        return minNumberInRotateArray(arr[1:])</span><br></pre></td></tr></table></figure></p>
<h3 id="非递归版本"><a href="#非递归版本" class="headerlink" title="非递归版本"></a>非递归版本</h3><p>递归版本占的内存比较多，改进吧..于是非递归的版本就出来了。需要注意的是该版本的判断比较条件(其中一点是和 arr[right]进行比较)一定要小心，都是小坑…<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">def minNumberInRotateArray(rotateArray):</span><br><span class="line">    arr =rotateArray</span><br><span class="line">    left =0</span><br><span class="line">    right =len(arr) -1</span><br><span class="line">    while left &lt; right:</span><br><span class="line">        mid = int((left+ right)/2)</span><br><span class="line">        if arr[mid] &gt;arr[right]:</span><br><span class="line">            left =mid+1</span><br><span class="line">        elif arr[mid] &lt;arr[right]:</span><br><span class="line">            right =mid</span><br><span class="line">        else:</span><br><span class="line">            right -=1</span><br><span class="line">    return arr[left]</span><br></pre></td></tr></table></figure></p>
<p>文章的小标题是求解最小(大)值，上述讲述的都是最小值。如果求解最大值，稍微修改一下特殊情况的判断条件，将返回的index-1 即可。因为最小值的位置是”某一个元素比上一个元素小”，那么 index-1 之后这个元素就是该数组序列中最大的。</p>
<p>参考文献<br><a href="https://blog.csdn.net/u010005281/article/details/79823154" target="_blank" rel="noopener">https://blog.csdn.net/u010005281/article/details/79823154</a></p>
<h2 id="单链表反转"><a href="#单链表反转" class="headerlink" title="单链表反转"></a>单链表反转</h2><p>单链表的反转有循环迭代和递归两种方法。<br>单链表节点<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">class Node(object):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        self.value =None</span><br><span class="line">        self.next =None</span><br><span class="line">    def __str__(self):</span><br><span class="line">        return str(self.value)</span><br></pre></td></tr></table></figure></p>
<h3 id="循环迭代"><a href="#循环迭代" class="headerlink" title="循环迭代"></a>循环迭代</h3><p>循环迭代需要维持三个变量：pre, head, next。pre是head的pre，next是head的next.(废话)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">def reverse_Linkedlist(head):</span><br><span class="line">    if not head or not head.next :</span><br><span class="line">        return head</span><br><span class="line">    pre =None</span><br><span class="line">    while head:</span><br><span class="line">        next = head.next</span><br><span class="line">        head.next =pre</span><br><span class="line">        pre = head</span><br><span class="line">        head =next</span><br><span class="line">        # 最后一次循环迭代 Head==None，而pre指向了头结点</span><br><span class="line">    return pre</span><br></pre></td></tr></table></figure></p>
<h3 id="递归"><a href="#递归" class="headerlink" title="递归"></a>递归</h3><p>一开始正常情况下不会执行if判断，利用递归走到链表的末端，new_head的值没有发生改变，为链表的最后一个节点，反转之后就成为了新链表的head。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">def reverse_Linkedlist(head):</span><br><span class="line">    if not head or not head.next:</span><br><span class="line">        return head</span><br><span class="line">    new_head = reverse_Linkedlist(head.next)</span><br><span class="line">    # 将当前节点设置为后面节点的后续节点</span><br><span class="line">    head.next.next =head</span><br><span class="line">    head.next =None</span><br><span class="line">    return new_head</span><br></pre></td></tr></table></figure></p>
<p>测试<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">if __name__ == &quot;__main__&quot;:</span><br><span class="line">    three = Node()</span><br><span class="line">    three.value =3</span><br><span class="line"></span><br><span class="line">    two =Node()</span><br><span class="line">    two.value =2</span><br><span class="line">    two.next =three</span><br><span class="line"></span><br><span class="line">    one =Node()</span><br><span class="line">    one.value =1</span><br><span class="line">    one.next =two</span><br><span class="line"></span><br><span class="line">    head =Node()</span><br><span class="line">    head.value =0</span><br><span class="line">    head.next =one</span><br><span class="line">    &quot;&quot;&quot;    </span><br><span class="line">    while head:</span><br><span class="line">        print(head.value, )</span><br><span class="line">        head =head.next</span><br><span class="line">    print(&quot;******&quot;)</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    newhead = reverse_Linkedlist(head)</span><br><span class="line">    while newhead:</span><br><span class="line">        print(newhead.value)</span><br><span class="line">        newhead =newhead.next</span><br></pre></td></tr></table></figure></p>
<p>参考文献<br><a href="https://foofish.net/linklist-reverse.html" target="_blank" rel="noopener">https://foofish.net/linklist-reverse.html</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/13/Introduction-to-Ensemble/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jijeng Jia">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jijeng's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/06/13/Introduction-to-Ensemble/" itemprop="url">Introduction_to_Ensemble</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-06-13T10:35:17+08:00">
                2018-06-13
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>虽然在<a href="https://jijeng.github.io/2018/06/05/Titanic-Challenge/" target="_blank" rel="noopener">Titanic Challenge</a>博客中介绍了模型融合(Ensemble)，但是其是以特征提取为主旋律，所以这篇博客将结合实战从最基本的Ensemble到比较复杂的Ensemble,最后安利一个用于模型融合的package.</p>
<p>在文中可能出现一些概念，如果不太熟悉，可以先去文末找找看，如果我没有补充，那么请自行Google吧。为了不影响阅读的流畅性，我可能在其中不会涉及很多概念解释，比如说ROC曲线和AUC值。<br>好，我们进入正文。</p>
<h2 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h2><p>本文书写过程中沿用参考博客(<a href="https://www.dataquest.io/blog/introduction-to-ensembles/" target="_blank" rel="noopener">Introduction to Python Ensembles</a>)的数据集。可以去<a href="https://github.com/fivethirtyeight/data/tree/master/science-giving" target="_blank" rel="noopener">这里</a>下载，当然推荐使用原作者处理之后的数据集，you can find <a href="https://www.dataquest.io/blog/large_files/gen_data.py" target="_blank" rel="noopener">here</a>。</p>
<p>简单介绍一下这个数据集：Federal Election Commission这个组织收集了2007到2016年的donations记录，最后得出 <a href="https://fivethirtyeight.com/features/when-scientists-donate-to-politicians-its-usually-to-democrats/" target="_blank" rel="noopener">When Scientists Donate To Politicians, It’s Usually To Democrats</a>这样的结论。</p>
<p>好了，我不想在数据集这里花太多时间，即使你不太明白数据集的具体含义，完全不影响下文的阅读，因为你很快就会发现下文并没有进行很多和原数据集相关的内容，更多的是模型融合。当然你如果能够看懂，可以感受一下上述结论的有趣之处。</p>
<p>Give me codes:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import pandas as pd</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"># import data</span><br><span class="line"># Always good to set a seed for reproducibility</span><br><span class="line">SEED = 222</span><br><span class="line">np.random.seed(SEED)</span><br><span class="line">df = pd.read_csv(&apos;input.csv&apos;)</span><br><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line">from sklearn.metrics import roc_auc_score</span><br><span class="line"></span><br><span class="line">def get_train_test(test_size= 0.95):</span><br><span class="line">    y =1*(df.cand_pty_affiliation ==&apos;REP&apos;)    </span><br><span class="line">    X = df.drop([&quot;cand_pty_affiliation&quot;], axis=1)</span><br><span class="line">    X = pd.get_dummies(X, sparse=True)</span><br><span class="line">    X.drop(X.columns[X.std() == 0], axis=1, inplace=True)</span><br><span class="line">    return train_test_split(X, y, test_size=test_size, random_state=SEED)</span><br><span class="line">xtrain, xtest, ytrain, ytest = get_train_test()</span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure></p>
<p><img src="http://p9fyz2xe9.bkt.clouddn.com/18-6-12/47340483.jpg" alt="avatar"><br>简单看一下数据长什么样子，虽然有人可能不太懂。</p>
<h2 id="Begin-with-ensemble"><a href="#Begin-with-ensemble" class="headerlink" title="Begin with ensemble"></a>Begin with ensemble</h2><p>之前的<a href="https://jijeng.github.io/2018/06/05/%E6%A8%A1%E5%9E%8B%E8%9E%8D%E5%90%88/" target="_blank" rel="noopener">博客</a>主要从ensemble分类的角度阐述，现在从概念的角度阐述。Ensemble: combining predictions from several models averages out idiosyncratic(怪异的) errors and yield better overall predictions.(有时候我觉得英文说得很清楚，所以就不翻译成中文了，求不被打。)</p>
<p>简单的说，就是可以防止过拟合。当过拟合时，边界曲线就回去过分考虑某一个或某一些少数的点，这时候 ensemble通过某种combine机制，然后得到一个泛化性能比较好的边界曲线，也就是比较好的模型。</p>
<h3 id="Decision-Tree"><a href="#Decision-Tree" class="headerlink" title="Decision Tree"></a>Decision Tree</h3><p>首先我们从 decision tree开始。<br>A decision tree, which is a tree of <strong>if-then</strong> rules. The deeper the tree, the more complex the patterns it can capture, but the more prone to overfitting it will be. Because of this, we will need an alternative way of building complex models of decision trees, and an ensemble of different decision trees is one such way.<br>我们先使用 depth =1<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">from IPython.display import Image</span><br><span class="line">from sklearn.metrics import roc_auc_score</span><br><span class="line">from sklearn.tree import DecisionTreeClassifier</span><br><span class="line"></span><br><span class="line">t1 = DecisionTreeClassifier(max_depth=1, random_state=SEED)</span><br><span class="line">t1.fit(xtrain, ytrain)</span><br><span class="line">p = t1.predict_proba(xtest)[:, 1]</span><br><span class="line">print(&quot;Decision tree ROC-AUC score: %.3f&quot; % roc_auc_score(ytest, p))</span><br></pre></td></tr></table></figure></p>
<p>Decision tree ROC-AUC score: 0.672</p>
<p>发现结果不太理想，加深depth.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">t2 =DecisionTreeClassifier(max_depth=3, random_state=SEED)</span><br><span class="line">t2.fit(xtrain, ytrain)</span><br><span class="line">p =t2.predict_proba(xtest)[:, 1]</span><br><span class="line">print(&quot;Decision tree ROC-AUC score: %.3f&quot; % roc_auc_score(ytest, p))</span><br></pre></td></tr></table></figure></p>
<p>Decision tree ROC-AUC score: 0.751</p>
<p>由于我们最后要ensemble，而这要求我们要构造有差异但每个不是那么差的模型。首先我们考虑到使用不同的数据集。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">xtrain_slim =xtrain.drop([&apos;transaction_amt&apos;], axis=1)</span><br><span class="line">xtest_slim =xtest.drop([&apos;transaction_amt&apos;], axis=1)</span><br><span class="line"></span><br><span class="line">t3=DecisionTreeClassifier(max_depth=3, random_state=SEED)</span><br><span class="line">t3.fit(xtrain_slim, ytrain)</span><br><span class="line">p =t3.predict_proba(xtest_slim)[:,1]</span><br><span class="line">print(&apos;Decision tree ROC-AUC score:&#123;&#125;&apos;.format(roc_auc_score(ytest, p)))</span><br></pre></td></tr></table></figure></p>
<p>Decision tree ROC-AUC score:0.7403182587884118<br>通过corr()来检验两者的相关性(差异性)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">p1 =t2.predict_proba(xtest)[:,1]</span><br><span class="line">p2 =t3.predict_proba(xtest_slim)[:,1]</span><br><span class="line">pd.DataFrame(&#123;&apos;full_data&apos;:p1, &apos;red_data&apos;:p2&#125;).corr()</span><br></pre></td></tr></table></figure></p>
<p><img src="http://p9fyz2xe9.bkt.clouddn.com/18-6-12/93565225.jpg" alt="avatar"><br>发现有一定的相关性，但是还是可以容忍的。于是开始融合。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">p1 = t2.predict_proba(xtest)[:, 1]</span><br><span class="line">p2 = t3.predict_proba(xtest_slim)[:, 1]</span><br><span class="line">p = np.mean([p1, p2], axis=0)</span><br><span class="line">print(&quot;Average of decision tree ROC-AUC score: %.3f&quot; % roc_auc_score(ytest, p))</span><br></pre></td></tr></table></figure></p>
<p>Average of decision tree ROC-AUC score: 0.783<br>我们发现：两个旗鼓相当（0.74 0.73）的可以得到一个更好的结果，可以减少决策失误  的平均是0.78。</p>
<p>需要注意的是我们在构造第二个模型时候通过drop()丢掉一个feature，反而得到一个更好的模型。于是乎，我们想通过使用不同的子集（不同的特征）构造不同的模型，是不是能得到更好的模型？</p>
<h3 id="Random-Forest-Bagging"><a href="#Random-Forest-Bagging" class="headerlink" title="Random Forest(Bagging)"></a>Random Forest(Bagging)</h3><p>A fast approach that works well in practice is to randomly select a subset of features, fit one decision tree on each draw and average their predictions.This process is known as bootstrapped averaging (often abbreviated bagging), and when applied to decision trees, the resultant model is a Random Forest. (我觉得原作者比我说的清楚，借用了)<br>我的理解，在上面小节中我们使用的是Decision Tree,在实际应用中发现有差异的多个子树的效果要更好一些。而实现这个途径快速的方法就是 Random Forest。Random 在这里表示任意几个子树(特征)，然后这些Tree组成了Forest。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.ensemble import RandomForestClassifier</span><br><span class="line">rf =RandomForestClassifier(</span><br><span class="line">    n_estimators=10,</span><br><span class="line">    max_features= 3,</span><br><span class="line">    random_state=SEED</span><br><span class="line">)</span><br><span class="line">rf.fit(xtrain, ytrain)</span><br><span class="line">p =rf.predict_proba(xtest)[:, 1]</span><br><span class="line">print(&apos;Average of decision tree ROC-AUC score:&#123;&#125;&apos;.format(roc_auc_score(ytest, p)))</span><br></pre></td></tr></table></figure></p>
<p>Average of decision tree ROC-AUC score:0.844018408542404<br>这就是叫做”质的飞跃”从那个0.783-&gt; 0.844(将近6个百分点，好吧，有点神经质了…)<br>From nobody to somebody, we are on something..</p>
<h3 id="Ensemble-of-various-models"><a href="#Ensemble-of-various-models" class="headerlink" title="Ensemble of various models"></a>Ensemble of various models</h3><p>可以看出上述模型中，最后的模型（Random Forest）的子模型(Decision Tree)。但是子模型并不是局限树这一种结构，我们更多的选择：linear models, kernel-based models, non-parametric models, neural networks or even other ensembles!</p>
<p>为了避免代码的冗余构造了以下的helper function.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"># A host of Scikit-learn models</span><br><span class="line">from sklearn.svm import SVC, LinearSVC</span><br><span class="line">from sklearn.naive_bayes import GaussianNB</span><br><span class="line">from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier</span><br><span class="line">from sklearn.linear_model import LogisticRegression</span><br><span class="line">from sklearn.neighbors import KNeighborsClassifier</span><br><span class="line">from sklearn.neural_network import MLPClassifier</span><br><span class="line">from sklearn.kernel_approximation import Nystroem</span><br><span class="line">from sklearn.kernel_approximation import RBFSampler</span><br><span class="line">from sklearn.pipeline import make_pipeline</span><br><span class="line"></span><br><span class="line">def get_models():</span><br><span class="line">    &quot;&quot;&quot;Generate a library of base learners.&quot;&quot;&quot;</span><br><span class="line">    nb = GaussianNB()</span><br><span class="line">    svc = SVC(C=100, probability=True)</span><br><span class="line">    # C越大边界越复杂，会导致过拟合</span><br><span class="line">    knn = KNeighborsClassifier(n_neighbors=3)</span><br><span class="line">    # KNN算法寻找训练数据中的K个最近的数据，它使用指向最多的那个类别来作为预测的输出。</span><br><span class="line">    lr = LogisticRegression(C=100, random_state=SEED)</span><br><span class="line">    # 对于这个 c 能知道的就是正则化系数 smaller values specify stronger regularization.</span><br><span class="line">    nn = MLPClassifier((80, 10), early_stopping=False, random_state=SEED)</span><br><span class="line">    gb = GradientBoostingClassifier(n_estimators=100, random_state=SEED)</span><br><span class="line">    # 子模型的数量，默认是100, gbc通常 robust to over-fitting, so a large number results in better performance</span><br><span class="line">    rf = RandomForestClassifier(n_estimators=10, max_features=3, random_state=SEED)</span><br><span class="line">    models = &#123;&apos;svm&apos;: svc,</span><br><span class="line">              &apos;knn&apos;: knn,</span><br><span class="line">              &apos;naive bayes&apos;: nb,</span><br><span class="line">              &apos;mlp-nn&apos;: nn,</span><br><span class="line">              &apos;random forest&apos;: rf,</span><br><span class="line">              &apos;gbm&apos;: gb,</span><br><span class="line">              &apos;logistic&apos;: lr,</span><br><span class="line">              &#125;</span><br><span class="line"></span><br><span class="line">    return models</span><br><span class="line">    </span><br><span class="line">def train_predict(model_list):</span><br><span class="line">    P =np.zeros((ytest.shape[0], len(model_list)))</span><br><span class="line">    P =pd.DataFrame(P)</span><br><span class="line">    print(&apos;Fitting models&apos;)</span><br><span class="line">    cols =list()</span><br><span class="line">    for i, (name, m) in enumerate(models.items()):</span><br><span class="line">        print(&quot;%s...&quot; % name, end=&quot; &quot;, flush=False)</span><br><span class="line">        m.fit(xtrain, ytrain)</span><br><span class="line">        P.iloc[:, i] =m.predict_proba(xtest)[:, 1]</span><br><span class="line">        cols.append(name)</span><br><span class="line">        print(&apos;Done&apos;)</span><br><span class="line">    P.columns =cols</span><br><span class="line">    print(&apos;Done.\n&apos;)</span><br><span class="line">    return P</span><br><span class="line">    </span><br><span class="line">def score_models(P, y):</span><br><span class="line">    &quot;&quot;&quot;Score model in predication DF&quot;&quot;&quot;</span><br><span class="line">    print(&apos;Scoring models&apos;)</span><br><span class="line">    for m in P.columns:</span><br><span class="line">        score =roc_auc_score(y, P.loc[:, m])</span><br><span class="line">        print(&quot;%-26s: %.3f&quot; % (m, score))</span><br><span class="line">    print(&apos;Done, \n&apos;)</span><br></pre></td></tr></table></figure></p>
<p>let’s go…<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">models =get_models()</span><br><span class="line">P =train_predict(models)</span><br><span class="line">score_models(P, ytest)</span><br></pre></td></tr></table></figure></p>
<p><img src="http://p9fyz2xe9.bkt.clouddn.com/18-6-13/12164157.jpg" alt="avatar"><br>This is our base line.<br>Gradient Boosting Machine(GBM) 果然名不虚传, does best<br>我们来分析一下模型之间的相关性，原作者使用的mlens package(You can install it with: pip install mlens)，我这里用的是seaborn(install it with: pip install seaborn).<br>在检查相关性(Pearson相关性:衡量两个数据集合的线性相关性)时候，我们使用的是经过处理的相关性。具体说来可以称之为 error correlation,详细见代码。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># look at error correlations  is more promising, errors are significantly correlated </span><br><span class="line">import seaborn as sns</span><br><span class="line">plt.subplots(figsize=(10,8))  </span><br><span class="line">corrmat = P.apply(lambda pred: 1*(pred &gt;= 0.5) - ytest.values).corr()</span><br><span class="line">sns.set(font_scale=1.5)  </span><br><span class="line">hm = sns.heatmap(corrmat, cbar=True, annot=True, square=True, fmt=&apos;.2f&apos;, annot_kws=&#123;&apos;size&apos;: 10&#125;)  </span><br><span class="line">plt.title(&apos;Pearson Correlation of Features&apos;, y=1.05, size=15)  </span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://p9fyz2xe9.bkt.clouddn.com/18-6-13/15404991.jpg" alt="avatar"><br>预测值和真实值之间的差异称之为error，查看error的 Pearson correlation，效果更加明显。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(&quot;Ensemble ROC-AUC score: %.3f&quot; % roc_auc_score(ytest, P.mean(axis=1)))</span><br></pre></td></tr></table></figure></p>
<p>Ensemble ROC-AUC score: 0.884<br>结果高于每一个单独的模型，但是不是那么明显。</p>
<h3 id="Visualize-Curve-ROC-helper-function"><a href="#Visualize-Curve-ROC-helper-function" class="headerlink" title="Visualize Curve ROC(helper function)"></a>Visualize Curve ROC(helper function)</h3><p>我们注意到之前使用的所有的结果的评价标准都是 roc_auc_score，但是并没有提及这是什么。当然在正文中也不打算解释，如果不是很清楚，可以查看本文最后<strong>补充概念</strong>.简单来说AUC可以用来衡量”二分问题”的泛化能力，是一种评价指标。我们这里想说的是 visualize Curve ROC,可视化。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"># a helper function for roc_curve</span><br><span class="line">from sklearn.metrics import roc_curve</span><br><span class="line">def plot_roc_curve(ytest, P_base_learners, P_ensemble, labels, ens_label):</span><br><span class="line">    &quot;&quot;&quot;Plot the roc curve for base learners and ensemble.&quot;&quot;&quot;</span><br><span class="line">    plt.figure(figsize=(10, 8))</span><br><span class="line">    plt.plot([0, 1], [0, 1], &apos;k--&apos;)</span><br><span class="line">    cm = [plt.cm.rainbow(i) for i in np.linspace(0, 1.0, P_base_learners.shape[1] + 1)]</span><br><span class="line">      </span><br><span class="line">    for i in range(P_base_learners.shape[1]):</span><br><span class="line">        p = P_base_learners[:, i]</span><br><span class="line">        fpr, tpr, _ = roc_curve(ytest, p)</span><br><span class="line">        plt.plot(fpr, tpr, label=labels[i], c=cm[i + 1])</span><br><span class="line"></span><br><span class="line">    fpr, tpr, _ = roc_curve(ytest, P_ensemble)</span><br><span class="line">    plt.plot(fpr, tpr, label=ens_label, c=cm[0])</span><br><span class="line">        </span><br><span class="line">    plt.xlabel(&apos;False positive rate&apos;)</span><br><span class="line">    plt.ylabel(&apos;True positive rate&apos;)</span><br><span class="line">    plt.title(&apos;ROC curve&apos;)</span><br><span class="line">    plt.legend(frameon=False)</span><br><span class="line">    plt.show()</span><br><span class="line">plot_roc_curve(ytest, P.values, P.mean(axis=1), list(P.columns), &quot;ensemble&quot;)</span><br></pre></td></tr></table></figure></p>
<p><img src="http://p9fyz2xe9.bkt.clouddn.com/18-6-12/52483658.jpg" alt="avatar"><br>我们顺便把刚才的模型可视化，发现ensemble的AUC是最大的，意味这泛化性能是最好，这也是符合我们的认知的 。</p>
<h3 id="Beyond-ensembles-as-a-simple-average"><a href="#Beyond-ensembles-as-a-simple-average" class="headerlink" title="Beyond ensembles as a simple average"></a>Beyond ensembles as a simple average</h3><p>我们回到主线上，在上一个模型中我们提及最后的 ensemble的结果是好于每个单独的模型，但是没有那么突出。根据ROC曲线我们也可以看出，有的模型(KNN)在这里表现的没有那么好，我们在想是不是由于因为这个而拉底了最后的结果，当然这只是猜测，于是我们找到了 try的方向。<br>可能第一直觉去掉这个模型再进行融合，在这个实验表明该策略最后的结果0.883，相比与0.884，你懂得，并没有变好。<br>我们还有一种策略:learn a sensible set of weights to use when averaging predictions.让模型自己去学习如何调整各个模型之间的比例。</p>
<h3 id="Learning-to-combine-predications"><a href="#Learning-to-combine-predications" class="headerlink" title="Learning to combine predications"></a>Learning to combine predications</h3><p>为了让模型自学习各个之间的预测比例，我们引入了 meta learner(meta是元，理解为最基础的) to learn how to best combine these predictions.</p>
<p>除此之外，我们将使用不同的数据集，像Random Forest使用不同的数据子集(不同的特征组成的数据集)。于是我们需要a method for splitting the training data between the base learners and the meta learner.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">base_learners =get_models()</span><br><span class="line">meta_learner = GradientBoostingClassifier(</span><br><span class="line">    n_estimators=1000,</span><br><span class="line">    loss=&quot;exponential&quot;,</span><br><span class="line">    max_features=4,</span><br><span class="line">    max_depth=3,</span><br><span class="line">    subsample=0.5,</span><br><span class="line">    learning_rate=0.005,     </span><br><span class="line">    random_state=SEED</span><br><span class="line">)</span><br></pre></td></tr></table></figure></p>
<p>使用最强模型GBM作为 meta learner并定义好 base_learners.<br>To keep things simple, we split the full training set into a training and prediction set of the base learners. This method is sometimes referred to as <strong>Blending</strong>. Unfortunately, the terminology differs between communities, so it’s not always easy to know what type of cross-validation the ensemble is using.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># sefine a procedure for generating train and test sets</span><br><span class="line">xtrain_base , xpred_base, ytrain_base, ypred_base =train_test_split(xtrain, ytrain, test_size=0.5, random_state=SEED)</span><br><span class="line"></span><br><span class="line">def train_base_learners(base_learners, inp, out, verbose =True):</span><br><span class="line">    if verbose: print(&apos;Fitting models&apos;)</span><br><span class="line">    for i, (name, m) in enumerate(base_learners.items()):</span><br><span class="line">        if verbose: print(&quot;%s...&quot; % name, end=&quot; &quot;, flush=False)</span><br><span class="line">        m.fit(inp, out)</span><br><span class="line">        if verbose: print(&apos;Done.&apos;)</span><br><span class="line">train_base_learners(base_learners, xtrain_base, ytrain_base)</span><br></pre></td></tr></table></figure></p>
<p>(注意我们只是使用了50%的data去train, test_size =0.5)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">def predict_base_learners(pred_base_learners, inp, verbose=True):</span><br><span class="line">    &quot;&quot;&quot;Generate a prediction matrix.&quot;&quot;&quot;</span><br><span class="line">    P = np.zeros((inp.shape[0], len(pred_base_learners)))</span><br><span class="line"></span><br><span class="line">    if verbose: print(&quot;Generating base learner predictions.&quot;)</span><br><span class="line">    for i, (name, m) in enumerate(pred_base_learners.items()):</span><br><span class="line">        if verbose: print(&quot;%s...&quot; % name, end=&quot; &quot;, flush=False)</span><br><span class="line">        p = m.predict_proba(inp)</span><br><span class="line">        # With two classes, need only predictions for one class</span><br><span class="line">        P[:, i] = p[:, 1]</span><br><span class="line">        if verbose: print(&quot;done&quot;)</span><br><span class="line"></span><br><span class="line">    return P</span><br><span class="line">P_base = predict_base_learners(base_learners, xpred_base)</span><br></pre></td></tr></table></figure></p>
<p>现在我们得到了base_learners的predications，接下来我们应该使用的是这个流程，<strong>在base learners的基础上</strong>（类似两层结构了 meta learner 学习的如何搭配这些base learner使得最后的结果 predications最大）进行训练，而不是训练原来的数据集。<br><img src="http://p9fyz2xe9.bkt.clouddn.com/18-6-13/82384876.jpg" alt="avatar"><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">meta_learner.fit(P_base, ypred_base)</span><br></pre></td></tr></table></figure></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">#meta_learner.fit(P_base, ypred_base)</span><br><span class="line">def ensemble_predict(base_learners, meta_learner, inp, verbose=True):</span><br><span class="line">    &quot;&quot;&quot;Generate predictions from the ensemble.&quot;&quot;&quot;</span><br><span class="line">    P_pred = predict_base_learners(base_learners, inp, verbose=verbose)</span><br><span class="line">    return P_pred, meta_learner.predict_proba(P_pred)[:, 1]</span><br><span class="line">P_pred, p = ensemble_predict(base_learners, meta_learner, xtest)</span><br><span class="line">print(&quot;\nEnsemble ROC-AUC score: %.3f&quot; % roc_auc_score(ytest, p))</span><br></pre></td></tr></table></figure>
<p><img src="http://p9fyz2xe9.bkt.clouddn.com/18-6-13/11434494.jpg" alt="avatar"><br>最后的结果是0.881 相比与之前最好的0.884（使用相同的数据集，没有进行 meta_learner的操作）<br>这是因为我们在划分数据集的使用只是使用了0.5的数据集，而前者的模型使用了全部的train sets。<br>有人不免疑问：为什么不使用全部的data？我的理解是划分xtrain_base , xpred_base, ytrain_base, ypred_base使用的是 train_test_split()，总是需要设定一个数值的，即使train_size =0.01，也是没有用到全部的datas.</p>
<h3 id="Training-with-cross-validation"><a href="#Training-with-cross-validation" class="headerlink" title="Training with cross-validation"></a>Training with cross-validation</h3><p>我们使用cross-validation 来缓解上面那个问题。<br>During cross-validated training of the base learners, a copy of each base learner is fitted on K−1 folds, and predict the left-out fold. This process is iterated until every fold has been predicted. The more folds we specify, the less data is being left out in each training pass. This makes cross-validated predictions less noisy and a better reflection of performance during test time. The cost is significantly increased training time. Fitting an <strong>ensemble</strong> with <strong>cross-validation</strong> is often referred to as <strong>stacking</strong>, while the ensemble itself is known as the Super Learner.</p>
<p>To understand how cross-validation works, we can think of it as an outer loop over our previous ensemble. The outer loop iterates over K distinct test folds, with the remaining data used for training. The inner loop trains the base learners and generate predictions for the held-out data. Here’s a simple stacking implementation:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.base import clone</span><br><span class="line"></span><br><span class="line">def stacking(base_learners, meta_learner, X, y, generator):</span><br><span class="line">    &quot;&quot;&quot;Simple training routine for stacking.&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    # Train final base learners for test time</span><br><span class="line">    print(&quot;Fitting final base learners...&quot;, end=&quot;&quot;)</span><br><span class="line">    train_base_learners(base_learners, X, y, verbose=False)</span><br><span class="line">    print(&quot;done&quot;)</span><br><span class="line"></span><br><span class="line">    # Generate predictions for training meta learners</span><br><span class="line">    # Outer loop:</span><br><span class="line">    print(&quot;Generating cross-validated predictions...&quot;)</span><br><span class="line">    cv_preds, cv_y = [], []</span><br><span class="line">    for i, (train_idx, test_idx) in enumerate(generator.split(X)):</span><br><span class="line"></span><br><span class="line">        fold_xtrain, fold_ytrain = X[train_idx, :], y[train_idx]</span><br><span class="line">        fold_xtest, fold_ytest = X[test_idx, :], y[test_idx]</span><br><span class="line"></span><br><span class="line">        # Inner loop: step 4 and 5</span><br><span class="line">        fold_base_learners = &#123;name: clone(model)</span><br><span class="line">                              for name, model in base_learners.items()&#125;</span><br><span class="line">        train_base_learners(</span><br><span class="line">            fold_base_learners, fold_xtrain, fold_ytrain, verbose=False)</span><br><span class="line"></span><br><span class="line">        fold_P_base = predict_base_learners(</span><br><span class="line">            fold_base_learners, fold_xtest, verbose=False)</span><br><span class="line"></span><br><span class="line">        cv_preds.append(fold_P_base)</span><br><span class="line">        cv_y.append(fold_ytest)</span><br><span class="line">        print(&quot;Fold %i done&quot; % (i + 1))</span><br><span class="line"></span><br><span class="line">    print(&quot;CV-predictions done&quot;)</span><br><span class="line">    </span><br><span class="line">    # Be careful to get rows in the right order</span><br><span class="line">    cv_preds = np.vstack(cv_preds)</span><br><span class="line">    cv_y = np.hstack(cv_y)</span><br><span class="line"></span><br><span class="line">    # Train meta learner</span><br><span class="line">    print(&quot;Fitting meta learner...&quot;, end=&quot;&quot;)</span><br><span class="line">    meta_learner.fit(cv_preds, cv_y)</span><br><span class="line">    print(&quot;done&quot;)</span><br><span class="line">    return base_learners, meta_learner</span><br></pre></td></tr></table></figure></p>
<p>尤其在cv_preds和cv_y的维度问题上，注意小心。<br>The basic difference between blending and stacking is therefore that stacking allows both base learners and the meta learner to train on the full data set. Using 2-fold cross-validation, we can measure the difference this makes in our case:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.model_selection import KFold</span><br><span class="line"># Train with stacking</span><br><span class="line">cv_base_learners, cv_meta_learner = stacking(</span><br><span class="line">    get_models(), clone(meta_learner), xtrain.values, ytrain.values, KFold(2))</span><br><span class="line"></span><br><span class="line">P_pred, p = ensemble_predict(cv_base_learners, cv_meta_learner, xtest, verbose=False)</span><br><span class="line">print(&quot;\nEnsemble ROC-AUC score: %.3f&quot; % roc_auc_score(ytest, p))</span><br></pre></td></tr></table></figure></p>
<p>Ensemble ROC-AUC score: 0.889<br>这是目前为止最好的结果了。<br>Stacking yields a sizeable increase in performance: in fact, it gives us our best score so far. This outcome is typical for small and medium-sized data sets, where the effect of blending can be severe. As the data set size increases, blending and stacking performs similarly.</p>
<h2 id="Use-packages"><a href="#Use-packages" class="headerlink" title="Use packages"></a>Use packages</h2><p>快要接近尾声了，在文章的开始，我们提及要安利一个resemble好用的package. So, it’s now.<br>Here, we use Ensemble and build our previous generalized ensemble, but now using 10-fold cross-validation:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">from mlens.ensemble import SuperLearner</span><br><span class="line"></span><br><span class="line"># Instantiate the ensemble with 10 folds</span><br><span class="line">sl = SuperLearner(</span><br><span class="line">    folds=10,</span><br><span class="line">    random_state=SEED,</span><br><span class="line">    verbose=2,</span><br><span class="line">    backend=&quot;multiprocessing&quot;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"># Add the base learners and the meta learner</span><br><span class="line">sl.add(list(base_learners.values()), proba=True) </span><br><span class="line">sl.add_meta(meta_learner, proba=True)</span><br><span class="line"></span><br><span class="line"># Train the ensemble</span><br><span class="line">sl.fit(xtrain, ytrain)</span><br><span class="line"></span><br><span class="line"># Predict the test set</span><br><span class="line">p_sl = sl.predict_proba(xtest)</span><br><span class="line"></span><br><span class="line">print(&quot;\nSuper Learner ROC-AUC score: %.3f&quot; % roc_auc_score(ytest, p_sl[:, 1]))</span><br></pre></td></tr></table></figure></p>
<p><img src="http://p9fyz2xe9.bkt.clouddn.com/18-6-13/78451107.jpg" alt="avatar"><br>So simple!<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_roc_curve(ytest, p.reshape(-1, 1), P.mean(axis=1), [&quot;Simple average&quot;], &quot;Super Learner&quot;)</span><br></pre></td></tr></table></figure></p>
<p><img src="http://p9fyz2xe9.bkt.clouddn.com/18-6-13/52534893.jpg" alt="avatar"></p>
<p>发现super learner(meta learner)和 basic learner的mean的结果已经不相上下了。super learner得到了很好的训练。</p>
<h2 id="补充概念"><a href="#补充概念" class="headerlink" title="补充概念"></a>补充概念</h2><ul>
<li><p>ROC曲线和AUC值<br>ROC全称是“受试者工作特征”（Receiver Operating Characteristic）。ROC曲线的面积就是AUC（Area Under the Curve）。AUC用于衡量“二分类问题”机器学习算法性能（泛化能力）。说到这里不得不提及就是经常使用的符号，TP(True Positive), FP(False, Positive), TN(True Negative),FN(False Negative)。他们是根据原来真实数据和预测类别进行的排列组合（当然这是针对二分问题）。好丑的图片啊…<br><img src="http://p9fyz2xe9.bkt.clouddn.com/18-6-12/79026872.jpg" alt="avatar"><br>接着我们计算TRP(True Positive Radio)，FRP(False Positive Ratio)用于描述ROC曲线，分别表示该曲线的Y轴，X轴。<br>TPR=TP/(TP+FN)<br>FPR=FP/(FP+TN)<br>最后就形成了类似这样的图像(来源于上述的训练模型)<br><img src="http://p9fyz2xe9.bkt.clouddn.com/18-6-12/52483658.jpg" alt="avatar"><br>我们希望的结果是TRU越大（接近1），FRU越小（接近0）。AUC的值是ROC所覆盖的面积，当AUC越大时候，分类器的效果越好。从图中可以看出模型(ensemble)的面积是最大的，分类效果也是最好的。<br>关于该图像还有一点，如果你的曲线拟合对角线（图中虚线），那么相当于随机猜测。<br>在机器学习中关于这方面经常涉及到的还有precision(查准率), recall(查全率)两个概念，下面是计算公式。<br><img src="http://p9fyz2xe9.bkt.clouddn.com/18-6-12/56890358.jpg" alt="avatar"><br>其中n= TP+FP<br><img src="http://p9fyz2xe9.bkt.clouddn.com/18-6-12/29323029.jpg" alt="avatar"><br>总结：图中的x，y轴的计算和 precision, recall不是一个概念，虽然 recall 和y 轴在计算上是相同的。precision 是针对于预测数据(predication结果)来说的，而x轴，y轴(recall)的计算某种意义上是针对原来真实数据而言的。所以我们在训练模型过程可以追求 precision 和 recall的双高(即图像的左上角)。这时候引入了F1-measure(F1 =(2<em>P</em>R)/(R+P)).(P: precision, R: recall)</p>
</li>
<li><p>majority/ soft/ hard voting<br>an ensemble that averages classifier predictions is known as a <strong>majority voting</strong> classifier. When an ensembles averages based on probabilities (as above), we refer to it as <strong>soft voting</strong>, averaging final class label predictions is known as <strong>hard voting</strong>.</p>
</li>
<li><p>Pearson相关性<br><img src="http://p9fyz2xe9.bkt.clouddn.com/18-6-13/73757528.jpg" alt="avatar"><br>协方差除以各自的标准差</p>
</li>
<li><p>GBC参数<br>这些参数中，类似于Adaboost，我们把重要参数分为两类，第一类是Boosting框架的重要参数，第二类是弱学习器即CART回归树的重要参数。<br>n_estimators: 也就是弱学习器的最大迭代次数，或者说最大的弱学习器的个数。<br>learning_rate: 即每个弱学习器的权重缩减系数ν，也称作步长<br>对于分类模型，有对数似然损失函数”deviance”和指数损失函数”exponential”两者输入选择。默认是对数似然损失函数”deviance”。一般来说，推荐使用默认的”deviance”。它对二元分离和多元分类各自都有比较好的优化。而指数损失函数等于把我们带到了Adaboost算法。<br>对于回归模型，有均方差”ls”, 绝对损失”lad”, Huber损失”huber”和分位数损失“quantile”。默认是均方差”ls”。一般来说，如果数据的噪音点不多，用默认的均方差”ls”比较好。如果是噪音点较多，则推荐用抗噪音的损失函数”huber”。而如果我们需要对训练集进行分段预测的时候，则采用“quantile”。<br>max_features:可以使用很多种类型的值，默认是”None”,意味着划分时考虑所有的特征数.<br>subsample: 选择小于1的比例可以减少方差，即防止过拟合，但是会增加样本拟合的偏差，因此取值不能太低。推荐在[0.5, 0.8]之间，默认是1.0，即不使用子采样。</p>
</li>
</ul>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://www.cnblogs.com/pinard/p/6143927.html" target="_blank" rel="noopener">GBC参数设置</a><br><a href="https://www.cnblogs.com/gatherstars/p/6084696.html" target="_blank" rel="noopener">ROC曲线和AUC值</a><br><a href="https://www.dataquest.io/blog/introduction-to-ensembles/" target="_blank" rel="noopener">Introduction to Python Ensembles</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/05/对抗性生成网络/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jijeng Jia">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jijeng's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/06/05/对抗性生成网络/" itemprop="url">对抗性生成网络实验对比</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-06-05T22:27:25+08:00">
                2018-06-05
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Deep-learning/" itemprop="url" rel="index">
                    <span itemprop="name">Deep learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>GAN模型是典型的隐式无监督生成模型，建模过程中没有利用到数据的语义标签。但在实际应用中生成模型的可控性至关重要，根据标签生成对可控有样本更具实际应用价值。图像生成模型被广泛应用于数据增强、风格转换和数据补全领域，需要可控且语义完备的生成模型。<br>条件GAN模型在GAN模型建模思路的基础上，将语义标签加入了建模过程，将无监督生成模型转变为有监督的条件生成模型。条件GAN模型具体包括Conditional GAN模型、Semi-GAN模型和AC-GAN模型。</p>
<h3 id="实验模型介绍"><a href="#实验模型介绍" class="headerlink" title="实验模型介绍"></a>实验模型介绍</h3><p>（1）Conditional GAN模型<br>GAN模型中判别器D对输入的数据样本的来源进行判别，是典型的判别模型流程。如果在GAN框架中加入有监督信息来辅助训练，如图像的类别信息来辅助判别器D进行判别，则会帮助生成更加真实的样本。其中最初的尝试方式是Conditional GAN模型，结构如图所示。<br><img src="http://p9fyz2xe9.bkt.clouddn.com/18-6-1/89027900.jpg" alt="avatar"><br>Conditional GAN模型在生成器G和判别器D输入中都加入了标签信息，试图让生成器G学习到从数据标签y到样本x的映射，让判别器D学习对样本x和标签y的组合进行判别。<br>与GAN模型相比，Conditional GAN模型增加了标签信息的输入将模型转变为条件生成模型，在一定程度上提高了模型的稳定性。Conditional GAN模型训练过程中，判别器D对样本x和标签y的类别组合进行训练，并没有输入样本x和标签y的类别错误组合进行训练，因此模型并没有学习样本x和标签y的联合分布。<br>（2）Semi-GAN模型<br>Conditional GAN模型利用了标签信息进行建模，但没有对标签语义的信息进行表征，导致模型能够学到的信息有限。在生成模型过程中，如果判别器D能够明确指出生成样本的类别错误，则可为生成器G提供更加精确的梯度信息，最终能生成更加真实的样本。<br>Semi-GAN基于此思路进行改进，具体结构如图9所示。<br><img src="http://p9fyz2xe9.bkt.clouddn.com/18-6-1/89762825.jpg" alt="avatar"><br>Semi-GAN模型在Conditional GAN模型的基础上，对判别器D的分类输出进行细化加入了半监督学习过程。<br>（3）AC-GAN模型<br>与Conditional GAN模型相比，Semi-GAN模型中判别器D能够判别真实样本的来源，增强了判别器D的判别能力。但研究表明过强的判别信息会影响生产样本的质量，具体原因为Semi-GAN模型的建模过程为半监督分类过程，目标优化函数为无监督分类和有监督分类目标函数之和。若判别器D的监督分类信息过强，则会削弱判别器D对样本来源的判别能力。<br>Conditional GAN模型能够生成指定类别的样本，Semi-GAN模型能够判别样本的类别信息。AC-GAN模型将以上两个模型思路进行整合，得到够进行条件生成的生成器G，和能够判别样本类别和来源信息的判别器D。AC-GAN模型的结构如图10所示。<br><img src="http://p9fyz2xe9.bkt.clouddn.com/18-6-1/23586830.jpg" alt="avatar"><br>AC-GAN模型在Conditional GAN的基础上，让判别器D在判别样本来源的同时，让样本进行分类。此时的判别器D的输出分为样本来源信息LS和样本分类LC信息。</p>
<h3 id="不同模型比较"><a href="#不同模型比较" class="headerlink" title="不同模型比较"></a>不同模型比较</h3><p>下面用表格的方式对比在实验中使用的模型的目标函数(ps,图画比较丑，之后再修改)</p>
<table>
<thead>
<tr>
<th style="text-align:center"><em>Name</em></th>
<th style="text-align:center"><em>Paper Link</em></th>
<th style="text-align:left"><em>Value Function</em></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><strong>GAN</strong></td>
<td style="text-align:center"><a href="https://arxiv.org/abs/1406.2661" target="_blank" rel="noopener">Arxiv</a></td>
<td style="text-align:left"><img src="http://p9fyz2xe9.bkt.clouddn.com/18-6-1/55901553.jpg" alt="avatar"></td>
</tr>
<tr>
<td style="text-align:center"><strong>DCGAN</strong></td>
<td style="text-align:center"><a href="https://arxiv.org/abs/1511.06434" target="_blank" rel="noopener">Arxiv</a></td>
<td style="text-align:left"><img src="http://p9fyz2xe9.bkt.clouddn.com/18-6-1/90451211.jpg" alt="avatar"></td>
</tr>
<tr>
<td style="text-align:center"><strong>Semi-GAN</strong></td>
<td style="text-align:center"><a href="https://arxiv.org/abs/1606.01583" target="_blank" rel="noopener">Arxiv</a></td>
<td style="text-align:left">和GAN 模型相同</td>
</tr>
<tr>
<td style="text-align:center"><strong>CGAN</strong></td>
<td style="text-align:center"><a href="https://arxiv.org/abs/1411.1784" target="_blank" rel="noopener">Arxiv</a></td>
<td style="text-align:left"><img src="http://p9fyz2xe9.bkt.clouddn.com/18-6-1/78212709.jpg" alt="avatar"></td>
</tr>
<tr>
<td style="text-align:center"><strong>ACGAN</strong></td>
<td style="text-align:center"><a href="https://arxiv.org/abs/1610.09585" target="_blank" rel="noopener">Arxiv</a></td>
<td style="text-align:left"><img src="http://p9fyz2xe9.bkt.clouddn.com/18-6-1/84966180.jpg" alt="avatar"></td>
</tr>
<tr>
<td style="text-align:center"><strong>our model</strong></td>
<td style="text-align:center"><a href="https://arxiv.org/abs/1705.07215" target="_blank" rel="noopener">Arxiv</a></td>
<td style="text-align:left"><img src="http://p9fyz2xe9.bkt.clouddn.com/18-6-1/48822906.jpg" alt="avatar"></td>
</tr>
</tbody>
</table>
<h3 id="数据集介绍"><a href="#数据集介绍" class="headerlink" title="数据集介绍"></a>数据集介绍</h3><p><img src="http://p9fyz2xe9.bkt.clouddn.com/18-6-1/70807095.jpg" alt="avatar"><br>在常用于图像生成的图像数据集中，大部分数据的标签类型为离散类型。其中MNIST和Fashion-MNIST为常用的灰度图像数据，每类样本分布较为独立，常用于进行图像分类和样本生成的实验；SVHN和CIFAR10为彩色数据集图像像素分布较为复杂，其中CIFAR10常用来检验分类网络性能的评价数据集；CelebA为大规模的人脸识别和属性分类数据集，每幅人脸图像包括40个属性标签；ImageNet为图像分类和识别数据集，数据集类别分布比较复杂具体包括自然图像和人为图像。UnityEyes为人眼视觉合成数据集，数据集标签包括瞳孔标签和视觉方向标签，其中视觉方向标签为连续的语言标签。<br>常见的离散标签图像数据集的样例:<br><img src="http://p9fyz2xe9.bkt.clouddn.com/18-6-1/73164393.jpg" alt="avatar"></p>
<h3 id="我们的模型"><a href="#我们的模型" class="headerlink" title="我们的模型"></a>我们的模型</h3><p>在原始GAN模型中，目标函数定义为生成器G和判别器D的博弈过程，定义V(G;D)为模型的目标函数，由生成器G和判别器D组成。语义匹配目标函数FMloss。基于语义匹配的条件生成网络模型的生成器G和判别器D的目标函数分别为：<br><img src="http://p9fyz2xe9.bkt.clouddn.com/18-6-1/48822906.jpg" alt="avatar"><br>如上式，LS为样本来源，LC为分类结果。判别器D目标为最大化LC+ LS + FMloss，其试图对输入样本进行分类，并通过来源和语义匹配区分生成样本和原始样本。生成器G的目的是最大化LC − LS −FMloss，其试图通过样本分类结果、样本来源和语义匹配结果来欺骗判别器D。LS来源损失与原始GAN模型相同，LC为语义标签分类损失，在类别分类中使用交叉信息熵，在数值回归中则使用均方差回归。</p>
<h3 id="生成结果对比"><a href="#生成结果对比" class="headerlink" title="生成结果对比"></a>生成结果对比</h3><p>MNIST数据集生成结果<br><img src="http://p9fyz2xe9.bkt.clouddn.com/18-6-1/60239019.jpg" alt="avatar"><br>Fashion-MNIST数据集生成结果<br><img src="http://p9fyz2xe9.bkt.clouddn.com/18-6-1/25827392.jpg" alt="avatar"><br>SVHN数据集生成结果<br><img src="http://p9fyz2xe9.bkt.clouddn.com/18-6-1/12463336.jpg" alt="avatar"><br>CIFAR10数据集生成结果<br><img src="http://p9fyz2xe9.bkt.clouddn.com/18-6-1/62008706.jpg" alt="avatar"><br>CelebA数据集生成结果<br><img src="http://p9fyz2xe9.bkt.clouddn.com/18-6-1/12335202.jpg" alt="avatar"><br>UnityEyes数据集生成结果<br><img src="http://p9fyz2xe9.bkt.clouddn.com/18-6-1/34168567.jpg" alt="avatar"></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/3/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/avatar.jpeg"
                alt="Jijeng Jia" />
            
              <p class="site-author-name" itemprop="name">Jijeng Jia</p>
              <p class="site-description motion-element" itemprop="description">Stay Hungry, Stay Foolish</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">26</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">7</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">26</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/jijeng" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:jia1509309698@163.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Jijeng Jia</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
