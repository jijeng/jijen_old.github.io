<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="super resolution 项目的调研和 实践过程记录。">
<meta property="og:type" content="article">
<meta property="og:title" content="关于 super resolution 的相关论文">
<meta property="og:url" content="http://yoursite.com/2020/04/18/super_resolution/index.html">
<meta property="og:site_name" content="Jijeng&#39;s blog">
<meta property="og:description" content="super resolution 项目的调研和 实践过程记录。">
<meta property="article:published_time" content="2020-04-18T12:05:45.000Z">
<meta property="article:modified_time" content="2020-07-15T01:50:10.868Z">
<meta property="article:author" content="Jijeng Jia">
<meta name="twitter:card" content="summary">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2020/04/18/super_resolution/"/>







<script>
	(function(){
		if(''){
			if (prompt('请输入文章密码','') !== ''){
				alert('密码错误！');
				history.back();
			}
		}
	})();
</script>

  <title>关于 super resolution 的相关论文 | Jijeng's blog</title>
  








<meta name="generator" content="Hexo 4.2.0"></head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Jijeng's blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/%20" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/%20" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/%20" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/%20" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="Searching..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/04/18/super_resolution/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jijeng Jia">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jijeng's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">关于 super resolution 的相关论文</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-04-18T20:05:45+08:00">
                2020-04-18
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">Post modified&#58;</span>
              
              <time title="Post modified" itemprop="dateModified" datetime="2020-07-15T09:50:10+08:00">
                2020-07-15
              </time>
            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2020/04/18/super_resolution/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2020/04/18/super_resolution/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="page-pv"><i class="fa fa-file-o"></i>
            <span class="busuanzi-value" id="busuanzi_value_page_pv" ></span>
            </span>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>super resolution 项目的调研和 实践过程记录。</p>
<a id="more"></a>



<h2 id="调研部分"><a href="#调研部分" class="headerlink" title="调研部分"></a>调研部分</h2><ol>
<li>Residual Dense Network for Image Super-Resolution</li>
</ol>
<p>残差学习有效缓解了随着网络深度增加引发的梯度消失的现象。使得提高网络深度，还能保持很好性能与效率。</p>
<p>残差学习很适合做图像复原，因为低质图像与高质图像之间相似度很高，而他们的残差其实很稀疏，简单理解残差学习另网络需要学习的东西变少了。</p>
<p>单幅图像超分辨率（SISR）旨在于低分辨率（LR）测量的基础上生成视觉良好的高分辨率（HR）图像。SISR 用于各种计算机视觉任务，如安全和监视成像 [38]、医学成像 [22] 和图像生成 [9]。图像超分辨率是一个不适定（ill-posed）逆过程，因为对于任何 LR 输入都存在多种解决方案。为了解决这个逆问题，研究者们已经提出了大量的图像 SR 算法，包括基于插值、基于重建和基于学习的方法 [27, 28, 19, 2, 20, 8, 10, 30]。</p>
<ol start="2">
<li>Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network</li>
</ol>
<p>SRGAN目标从一个低分辨率的图片中生成它的高分辨率版本。</p>
<p>所谓超分辨率重建就是将低分辨率图像恢复成对应的高分辨率图像。但是由于低分辨率图像信息的缺失，这是一个病态的求逆问题，尤其是在恢复的倍数较高的时候。</p>
<p>SRGAN (Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network, arxiv, 21 Nov, 2016)将生成式对抗网络（GAN)用于SR问题。</p>
<p>提出了SRGAN网络，该网络结构根据对抗网络网络结构提出了一种新的视觉损失函数(perceptual loss),利用VGG的网络特征作为内容损失函数(content loss),代替了之前的MSE损失函数。</p>
<p>之前的很多研究以最小化平方方差(MSE)作为损失函数，该方法能得到较好的信噪比，但图像会缺失高频信息导致图像的视觉效果差。</p>
<p>所谓超分辨率重建就是将低分辨率图像恢复成对应的高分辨率图像。但是由于低分辨率图像信息的缺失，这是一个病态的求逆问题，尤其是在恢复的倍数较高的时候。</p>
<p>Content Loss </p>
<p>像素级 MSE Loss 是最经常使用的优化目标。但是，这种方式当取得较高的 PSNR的同时，MSE 优化问题导致缺乏 high-frequency content，这就会使得结果太过于平滑（overly smooth solutions）。</p>
<p>SRGAN是single image super-resolution领域里一项有创造力的工作，可生成更加真实的纹理，使得看起来更加真实舒服。但是，使用该方法生成的图片有时在细节处会伴随令人不愉快的伪影。为了改进这部分工作，作者提出从3个方面进行：</p>
<ul>
<li>network architecture（网络结构）</li>
<li>adversarial loss（对抗损失)</li>
<li>perceptual loss（感知损失）</li>
</ul>
<p>GANs 可以创造新的图像的能力。</p>
<ol start="3">
<li>ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks</li>
</ol>
<p>“ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks”发表于ECCV 2018 workshop，是对“The Super-Resolution Generative Adversarial Network”(SRGAN)的改进工作，在PIRM2018-SR挑战赛上，在region 3区域获得第一名并且获得最好的perceptual index。</p>
<p>ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks 是18年港中文的工作</p>
<ol start="4">
<li><a href="https://github.com/idealo/image-super-resolution/tree/master/ISR" target="_blank" rel="noopener">https://github.com/idealo/image-super-resolution/tree/master/ISR</a> </li>
</ol>
<p>开源代码需要大概过一遍</p>
<p>两个开源代码都是需要过一遍的。</p>
<h2 id="在用的开源项目"><a href="#在用的开源项目" class="headerlink" title="在用的开源项目"></a><a href="https://github.com/idealo/image-super-resolution" target="_blank" rel="noopener">在用的开源项目</a></h2><ol>
<li>PSNR（Peak Signal to Noise Ratio）峰值信噪比</li>
</ol>
<p>给定一个大小为 $m \times n$ 的干净图像 $ I$和噪声图像 $K$，均方误差$MSE$定义为：</p>
<p>\begin{equation}<br>M S E=\frac{1}{m n} \sum_{i=0}^{m-1} \sum_{j=0}^{n-1}[I(i, j)-K(i, j)]^{2}<br>\end{equation}</p>
<p>然后 PSN 就定义</p>
<p>\begin{equation}<br>PSNR=10 \cdot \log_{10}\left(\frac{M A X_{I}^{2}}{M S E}\right)<br>\end{equation}</p>
<p>其中  $(M A X_{I}^{2})$ 为图片可能的最大像素值。如果每个像素都由 8 位二进制来表示，那么就为 255。</p>
<p>psnr是“Peak Signal to Noise Ratio”的缩写，即峰值信噪比，是一种评价图像的客观标准。为了衡量经过处理后的影像品质，我们通常会参考PSNR值来衡量某个处理程序能否令人满意。PSNR的单位是dB，数值越大表示失真越小。n为每像素的比特数，一般的灰度图像取8，即像素灰阶数为256。它是原图像与被处理图像之间的均方误差相对于(2n-1)2的对数值(信号最大值的平方，n是每个采样值的比特数)，所以PSNR值越大，就代表失真越少，图像和原图越接近。</p>
<p>PSNR基于对应像素点间的误差，是使用最广泛的一种图像客观评价指标，但未考虑人眼的视觉识别感知特性，常出现评价结果和人主观感觉不一样的情况。这是因为人眼的视觉对于误差的敏感度并不是绝对的，其感知结果会受到许多因素的影响而产生变化（例如：人眼对空间频率较低的对比差异敏感度较高，人眼对亮度对比差异的敏感度较色度高，人眼对一个区域的感知结果会受到其周围邻近区域的影响）。</p>
<p>所以MSE越小，则PSNR越大；所以PSNR越大，代表着图像质量越好。</p>
<p>在开源项目中的实现： <a href="https://github.com/idealo/image-super-resolution/blob/master/ISR/utils/metrics.py" target="_blank" rel="noopener">metrics</a> </p>
<ol start="2">
<li>SSIM (Structural SIMilarity) 结构相似性</li>
</ol>
<p>捕捉图像之间的结构上的相似性，基本上是符合人眼的主观判断。</p>
<p>SSIM的基本思路是，通过一下三个方面来对两幅图像的相似性进行评估</p>
<ol>
<li>luminance，亮度</li>
<li>contrast，对比度</li>
<li>structure，结构</li>
</ol>
<ol start="3">
<li>实际上是可以不构建 validation dataset 的，如果仅仅是为了测试的话。设置 <code>n_validation_samples</code> 为0 即可。</li>
</ol>
<h3 id="issues-整理"><a href="#issues-整理" class="headerlink" title="issues 整理"></a>issues 整理</h3><ol>
<li>predict 减少内存的使用</li>
</ol>
<ul>
<li>in the ‘dev’ branch there is an option in the predict function that allows for large image inference:<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">m &#x3D; R(R)DN(...) m.predict(image_array, by_patch_of_size&#x3D;some_number</span><br></pre></td></tr></table></figure></li>
<li>use a lighter model. If you are not interested in noise reduction, use the model you can find, again in the dev branch (Ive been terrible at maintaining recently, will soon be moved to master), under weights/sample_weights/rrdsn-C4-D3-G64-G064-T10-x4/Perceptual/: this is a much lighter RRDN model, trained with GANs. It’s my model of choice 90% of the times.</li>
</ul>
<ul>
<li>如果是 keras 的代码，那么可以考虑 <code>tf.keras.backend.clear_session</code></li>
</ul>
<blockquote>
<p>If that is NOT the case, you could try to use tf.keras.backend.clear_session between each prediction (I guess you’d have to load the model again).</p>
</blockquote>
<p>1.1 predict 的时候 input size </p>
<blockquote>
<p>you are getting this error because you’re using Tensorflow’s predict function. You are calling rrdn.model.predict instead of rrdn.predict. The second is the predict function from ISR’s ImageModel, which takes care of that. If you want to use TF’s predict then yes, you have to prepare your array: look at process_array under ISR/utils/image_processing.py.</p>
</blockquote>
<p><a href="https://github.com/idealo/image-super-resolution/issues/111" target="_blank" rel="noopener">https://github.com/idealo/image-super-resolution/issues/111</a></p>
<blockquote>
<p>When doing inference you should not specify an input size when creating a model instance. For example<br>model = RRDN(arch_params={‘C’:4, ‘D’:3, ‘G’:32, ‘G0’:32, ‘T’:10, ‘x’:2})</p>
</blockquote>
<p><a href="https://github.com/idealo/image-super-resolution/issues/27" target="_blank" rel="noopener">https://github.com/idealo/image-super-resolution/issues/27</a></p>
<p>1.2  predict 时候的模型选择</p>
<p>需要手动选择，否则的话是人工check</p>
<blockquote>
<p>If using GANs you need to manually check the models, otherwise normally the last model is the best one.</p>
</blockquote>
<ol start="2">
<li>GPU 的使用</li>
</ol>
<p>uninstalling previous tensorflow and installing tensorflow-gpu (需要注意的是，tensorflow 的安装是需要明确指明安装的版本)</p>
<ol start="3">
<li>提高图像生成质量的参数</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Thanks @cfrancesco for the your work. by_patch_of_size did helped me out with and I also increased padding_size because I was having seam-lines. Above all I can now use the same model with pre-trained weights on GPU.</span><br></pre></td></tr></table></figure>
<blockquote>
<p>提高 padding_size 可以解决 seam-lines？ （线缝）</p>
</blockquote>
<p>PSNR is not the perfect metric, so I’d sugget not to make strong decisions based on it.</p>
<ol start="4">
<li>sample weights</li>
</ol>
<p>这个是 sample weights 的dowload link：</p>
<p><a href="https://drive.google.com/drive/folders/1cJLPgGfEuFAQzBKbXQtSGXxLXssw1D9f" target="_blank" rel="noopener">https://drive.google.com/drive/folders/1cJLPgGfEuFAQzBKbXQtSGXxLXssw1D9f</a></p>
<ol start="5">
<li>SRGAN training 的时候</li>
</ol>
<p>使用官方的script 的时候，注意使用以下的 monitored_metrics。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">monitored_metrics&#x3D;&#123;&#39;val_generator_PSNR_Y&#39;: &#39;max&#39;&#125;</span><br></pre></td></tr></table></figure>

<blockquote>
<p>Keras will internally change the name of the metric if there is more than one model, so it will be ‘val_generator_PSNR_Y’ if using GANS, other just ‘val_PSNR_Y’, disregarding the model name.<br>The colab notebook def need to be updated.</p>
</blockquote>
<p>查看开源代码</p>
<p>基于 keras 的实现（该版本keras 是基于 tensorflow 的实现）</p>
<p><a href="https://github.com/idealo/image-super-resolution/blob/master/ISR/models/imagemodel.py" target="_blank" rel="noopener">imagemodel</a></p>
<p>一般对于kernel_size 而言， 一般来说是 3，5 这样的奇数，很少有 偶数这样的出现。但是可以调整的是 filter 的数量。</p>
<p>针对某一个 weight，是特定的 name.hdf5 文件。</p>
<p><a href="https://github.com/idealo/image-super-resolution/blob/master/ISR/models/cut_vgg19.py" target="_blank" rel="noopener">cut_vgg19</a> 在train 的时候使用。</p>
<p>这个文件中的 vgg19 是非常常见的模型，常常用于 feature extractor， trained on imagenet dataset。</p>
<h3 id="代码阅读"><a href="#代码阅读" class="headerlink" title="代码阅读"></a>代码阅读</h3><ol>
<li>该开源项目实现的算法</li>
</ol>
<p>项目中的一些术语</p>
<ul>
<li><a href="https://arxiv.org/abs/1802.08797" target="_blank" rel="noopener">Residual Dense Network for Image Super-Resolution</a></li>
</ul>
<p>该论文主要的观点</p>
<p>Specifically, we propose residual dense block (RDB) to extract abundant local features via dense connected convolutional layers</p>
<blockquote>
<p>提出了一个 RDB 的模型的基本结构</p>
</blockquote>
<p>the number of RDB (denote as D for short), the number of Conv layers per RDB (denote as C for short), and the growth rate (denote as G for short). </p>
<p>在该框架下实现的三个算法</p>
<p><a href="https://github.com/idealo/image-super-resolution/blob/master/ISR/models/rdn.py" target="_blank" rel="noopener">https://github.com/idealo/image-super-resolution/blob/master/ISR/models/rdn.py</a></p>
<p>1). psnr-large</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#39;arch_params&#39;: &#123;&#39;C&#39;: 6, &#39;D&#39;: 20, &#39;G&#39;: 64, &#39;G0&#39;: 64, &#39;x&#39;: 2&#125;</span><br></pre></td></tr></table></figure>

<p>2). psnr-small</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#39;arch_params&#39;: &#123;&#39;C&#39;: 3, &#39;D&#39;: 10, &#39;G&#39;: 64, &#39;G0&#39;: 64, &#39;x&#39;: 2&#125;</span><br></pre></td></tr></table></figure>
<p>上述两种网络结构都是一样的，只不过前者的网络参数比后者的网络参数更大。</p>
<p>3). noise-cancel</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#39;arch_params&#39;: &#123;&#39;C&#39;: 6, &#39;D&#39;: 20, &#39;G&#39;: 64, &#39;G0&#39;: 64, &#39;x&#39;: 2&#125;</span><br></pre></td></tr></table></figure>



<p>实现的功能：</p>
<p>输入是 (x, x) 这样的原图像，输出是 (2x, 2x) 这样的生成图像。</p>
<ul>
<li><a href="http://cn.arxiv.org/abs/1809.00219" target="_blank" rel="noopener">ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks</a></li>
</ul>
<p>对应的代码是</p>
<p>所以这个开源项目是没有实现 最新的 super resolution方面的研究</p>
<p>ESRGAN+ - Further Improving Enhanced Super-Resolution Generative Adversarial Network</p>
<p>ESRGAN- Enhanced Super-Resolution Generative Adversarial Networks</p>
<p>对应的开源项目是：</p>
<p><a href="https://github.com/open-mmlab/mmsr" target="_blank" rel="noopener">Open MMLab Image and Video Super-Resolution Toolbox, , including SRResNet, SRGAN, ESRGAN, EDVR, etc.</a></p>
<p>loss 的方面</p>
<ol>
<li>After computing the squared distance between the inputs, the mean value over the last dimension is returned.</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss &#x3D; mean(square(y_true - y_pred), axis&#x3D;-1)</span><br></pre></td></tr></table></figure>


<ol start="2">
<li>MeanSquaredError</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss &#x3D; square(y_true - y_pred)</span><br></pre></td></tr></table></figure>


<p>train 的loss的源码是这个：<br><a href="https://github.com/idealo/image-super-resolution/blob/master/ISR/train/trainer.py" target="_blank" rel="noopener">https://github.com/idealo/image-super-resolution/blob/master/ISR/train/trainer.py</a></p>
<p>（知道是由 3 个loss 组成，没有完全理解）</p>
<h3 id="改进的方向"><a href="#改进的方向" class="headerlink" title="改进的方向"></a>改进的方向</h3><p>解决的问题</p>
<ul>
<li>evaluation data 上的效果没有 test data 上的效果差？ （多整理一下， 说清楚；）</li>
<li>出现一些无规律的斑点（像素点） -&gt; 网络结构</li>
<li>冷暖色调的问题，原图是暖色调的图像，super resolution 之后是冷色调的图像</li>
</ul>
<p>改进的理论基础</p>
<p>However, these PSNR-oriented approaches tend to output over-smoothed results without sufficient high-frequency details, since the PSNR metric fundamentally disagrees with the subjective eval- uation of human observers [1].</p>
<p>the number of RDB (denote as D for short), the number of Conv layers per RDB (denote as C for short), and the growth rate (denote as G for short). We use the performance of SRCNN [3] as a reference.<br>larger D or C would lead to higher performance.</p>
<p>PSNR-oriented methods, which tend to generate blurry results,<br>GAN-based methods, whose textures are unnatural and contain unpleasing noise.</p>
<p>balancing perceptual quality and PSNR.</p>
<p>具体调整的方向</p>
<ul>
<li>kernel size（3<em>3 -&gt; 4</em>4， 但是可以从 3<em>3 -&gt; 5</em>5） 保持这个奇数</li>
<li>试一下 noise-cancel 模型</li>
<li>网络结构的调整（RDB层的个数， 子网络的数量，一般来说是加大网络的深度和广度）</li>
</ul>
<h2 id="mmsr开源代码"><a href="#mmsr开源代码" class="headerlink" title="mmsr开源代码"></a>mmsr开源代码</h2><p><a href="https://github.com/idealo/image-super-resolution/issues/14" target="_blank" rel="noopener">Inference of large images</a></p>
<blockquote>
<p>Actually, my concern was on ability to run inference on images larger then GPU memory, I am not expecting a speed gain, but ability to use GPU accelerated functionality on very large images (40-50 megapixels)</p>
</blockquote>
<p><a href="https://github.com/idealo/image-super-resolution/issues/16" target="_blank" rel="noopener">Questions on images used in SR training</a><br>需要了解一下这个指标是什么PSNR ，主要cover 的是哪些方面</p>
<blockquote>
<p>It is of course crucial that the two images, LR and HR, are representing exactly the same object under the exact same conditions, especially for a PSNR-driven training.</p>
</blockquote>
<p>比较专业的角度去区分 resize 和 crop 两者的区别？</p>
<blockquote>
<p>Resizing should never be done. Rather, crops of the images should be taken, but this is done automatically in ISR (the ‘patch_size’ parameter in config.yml (under [‘session’][‘training’]) determines the size of the crops).</p>
</blockquote>
<p>给出了一个baseline：基本的数量是 800 ，要求的数量不是很多</p>
<blockquote>
<p>As for the number, the more the better, but for instance the dataset I used for training, the DIV2K dataset, has only 800 images in it.<br>Ideally the different resolutions you use should lead to image sizes of nxn, 2nx2n, 3nx3n, 4nx4n.</p>
</blockquote>
<p><a href="https://github.com/idealo/image-super-resolution/issues/27" target="_blank" rel="noopener">Prediction input dimension error </a><br>这个现在还是 open 的状态，是关于 prediction时候 input dimension 方面的</p>
<p><a href="https://github.com/idealo/image-super-resolution/issues/31" target="_blank" rel="noopener">while training this error raised</a></p>
<blockquote>
<p>Please check the image sizes, make sure that all the LR images are 32x32 and all HR images are 128x128.<br>确保是一样的 resolution</p>
</blockquote>
<p><a href="https://github.com/idealo/image-super-resolution/issues/53" target="_blank" rel="noopener">Parameters for more subtle output effect</a></p>
<p>提升效果的方式（这个是非常有价值的尝试，通过两个不同model的结果进行融合，那么最后的效果是否可以提升，并且这两个融合的 model 是需要有差别的，需要handle 不同的方面）</p>
<p>This is an effect of the noise cancelling model we provide, there is a few things you could do:</p>
<ul>
<li>re-train the current model, with some data that is similar to the images you want to upscale (maybe the images themselves?)</li>
<li>take two or more model that do different things (eg one w/ and one w/out noise canc) and merge the weights (seems like a feature to add asap)</li>
<li>or just use the gans-RRDN model if you don’t need to remove jpeg artifacts</li>
</ul>
<blockquote>
<p>Yeah I ran it through a combination of the noise cancelling and non noise cancelling models which gave a better result, would be cool to merge the weights!<br>一般来说，是非常cool to merge the weights？<br>但是具体 weights 是在哪里merge呢？需要有code 最好是有code 方面的指导</p>
</blockquote>
<p>其中有四个model，那么这四个model 各有什么特点，需要有一个比较详细的了解，然后才能排列组合。</p>
<p><a href="https://github.com/idealo/image-super-resolution/issues/102" target="_blank" rel="noopener">Unable to save trained model</a><br>使用 docker 解决了 trained model；软玉docker 的使用 （重点是可以学习一下了）</p>
<blockquote>
<p>sudo docker run –gpus all -it -v /home/image-super-resolution/:/home/isr/ isr_image -c config.yml</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1. create docker image by the file Dockerfile.gpu, let say define the name as isr_image (you can define any name you want)</span><br><span class="line">Execute docker container by the following command</span><br><span class="line">2. sudo docker run --gpus all -it -v &#123;path of project of host&#125;:&#123;path of project of docker container&#125; &#123;docker image name&#125; -c config.yml</span><br><span class="line"></span><br><span class="line">Example</span><br><span class="line">sudo docker run --gpus all -it -v &#x2F;home&#x2F;image-super-resolution&#x2F;:&#x2F;home&#x2F;isr&#x2F; isr_image -c config.yml</span><br></pre></td></tr></table></figure>


<p><a href="https://github.com/idealo/image-super-resolution/issues/14" target="_blank" rel="noopener">Inference of large images</a></p>
<blockquote>
<p>这个是对于图片大于显存之后 仍然想要predict 进行的操作。</p>
</blockquote>
<p><a href="https://github.com/idealo/image-super-resolution/issues/61" target="_blank" rel="noopener">can not get my train model </a><br>是找不见pretrained model 之后的事情。</p>
<p><a href="https://github.com/idealo/image-super-resolution/issues/62" target="_blank" rel="noopener">Parameter setting for training the RDN ArtefactCancelling models?</a><br>一种training 的方式</p>
<blockquote>
<p>the first few epochs (~20) you might want to train the model only with the MAE instead than adversarial loss (maybe without adding noise?).<br>There is not automatic switch yet though, so you have to train and then train again loading the pre-trained weights you obtained in the first part<br>就是当使用一种 模式 training epoch之后，然后使用另一种模式training，这种是需要一种理解吧。为什么可以采用这种方式training？</p>
</blockquote>
<p>如何得到任意大小 size 的图像输出</p>
<blockquote>
<p>先是得到一个大size 的图像，然后任意resize 成想要的大小</p>
</blockquote>
<p>如何提升 inference 的速度： 并行化</p>
<blockquote>
<p>you could try to parallelize the inference of different images to speed up,</p>
</blockquote>
<p>这种训练方式很 trick，开始的epoch 是训练一部分，后面的训练是一部分。</p>
<blockquote>
<p>the first few epochs (~20) you might want to train the model only with the MAE instead than adversarial loss (maybe without adding noise?).<br>There is not automatic switch yet though, so you have to train and then train again loading the pre-trained weights you obtained in the first part</p>
</blockquote>
<p>模型的大小差别不是很大，推荐使用和原作者相似的模型框架。</p>
<blockquote>
<p>If you have an adequate dataset, I would use a 4x upscaling.<br>For the rest, your configuration looks fine. In my experiment I observed no to very little qualitative difference between the larger and smaller models.<br>I would however recommend to use the RRDN model, with a similar configuration to what we provide already: you could for instance retrain the provided RRDN GANS model, with proper preprocessing have it learn noise (et al.) cancelling and share here your results ;)<br>Also, experiment with the layers you extract from the vgg network.</p>
</blockquote>
<p><a href="https://github.com/idealo/image-super-resolution/issues/66" target="_blank" rel="noopener">training fails with non-RGB images</a></p>
<blockquote>
<p>之后有需要是可以看得， non-RGB 方面的事情</p>
</blockquote>
<p><a href="https://github.com/idealo/image-super-resolution/issues/16" target="_blank" rel="noopener">Questions on images used in SR training</a></p>
<blockquote>
<p>It is of course crucial that the two images, LR and HR, are representing exactly the same object under the exact same conditions, especially for a PSNR-driven training.</p>
</blockquote>
<p>注意这种命名的方式，其中的大小写表示的是参数。</p>
<blockquote>
<p>rrdn = RRDN(arch_params={‘C’:3, ‘D’:4, ‘G’:64, ‘G0’:64, ‘T’:9, ‘x’:scale}, patch_size=lr_train_patch_size)<br>To load weight file ‘rdn-C6-D20-G64-G064-x2_ArtefactCancelling_epoch219.hdf5’</p>
</blockquote>
<p><a href="https://github.com/idealo/image-super-resolution/issues/18" target="_blank" rel="noopener">monitored_metrics error</a><br>需要注意的是是训练数据集一般在 2-3w 数据量就是 ok的，所以需要注意数据的质量 而非数量。</p>
<blockquote>
<p>25000 images is a very large dataset for ISR. Beware that quality, rather than quantity, is key. The model we provide was trained on the DIV2K dataset, which contains 800 training images. The main reason is that from each image thousands of different patches can be extracted. Please close the issue if your problem was solved.</p>
</blockquote>
<p><a href="https://github.com/idealo/image-super-resolution/issues/37" target="_blank" rel="noopener">What changes should be made to run on GPU machine</a></p>
<p>有两种方式使用GPU，一种是docker 一种是tensorflow-gpu。</p>
<blockquote>
<p>so basically if you use the dockerized version of the package, you should be able to use GPU directly. Otherwise you need to make that you have tensorflow-gpu installed and that the non-gpu version isn’t overriding it.<br>If everything is setup correctly, there should be no need for extra lines of code to use the GPU, in my experience.</p>
</blockquote>
<p>如果出现内存方面报错，那么可以从两方面考虑</p>
<ul>
<li>减少一次 predict的图像数量</li>
<li>use a lighter model （使用更加轻量级的model）</li>
</ul>
<p>这里是一些训练好的模型，可以参考使用。<a href="https://drive.google.com/drive/folders/1cJLPgGfEuFAQzBKbXQtSGXxLXssw1D9f" target="_blank" rel="noopener">https://drive.google.com/drive/folders/1cJLPgGfEuFAQzBKbXQtSGXxLXssw1D9f</a></p>
<p><a href="https://github.com/idealo/image-super-resolution/issues/69" target="_blank" rel="noopener">Training State-of-the-art super-resolution model for ISR </a></p>
<p>从理论上说 ESRGAN 是理论上更好的模型；在开源实现的角度来说，基于PSNR 训练的更加容易实现。</p>
<blockquote>
<p>I have seen that PSNR of RDN is less than that of ESRGANs which is state-of-the-art in 2018 and I think there are some other models which have been newly researched. I am interested to contribute to the project. I think that model change will make a significant impact on performance.</p>
</blockquote>
<blockquote>
<p>Yes, relativistic GANs might be a better choice and it will be one of the next steps as soon as the project is active again.<br>Feel free to PR any model you should train.<br>Btw, PSNR is not the perfect metric, so I’d sugget not to make strong decisions based on it.</p>
</blockquote>
<p><a href="https://github.com/idealo/image-super-resolution/issues/74" target="_blank" rel="noopener">Get Strange results when training a X3 upsacle ESRGAN model</a></p>
<p>为了消除GAN 网络中的一些 artifact ，这里给出了一个非常trick 的技巧，先是使 MAE 进行训练，然后使用 gan 进行训练。</p>
<blockquote>
<p>I mean, I could easily be wrong and that could also be an issue with the pixel shuffling operation. You should try to experiment with strides and kernel sizes. Let me know how it goes.<br>Another suggestion is to start with a purely MAE driven learning. This should avoid those very evident artefacts. Then you continue with gans</p>
</blockquote>
<p>如果有 <code>setup.py</code> 文件，那么是可以直接修改该文件，就可以达到安装不同软件版本的目的。比如说<br>from</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">install_requires&#x3D;[&#39;imageio&#39;, &#39;Keras&#x3D;&#x3D;2.2.4&#39;, &#39;numpy&#x3D;&#x3D;1.16.2&#39;, &#39;tensorflow&#x3D;&#x3D;1.13.1&#39;, &#39;tqdm&#39;],</span><br></pre></td></tr></table></figure>
<p>to</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">install_requires&#x3D;[&#39;imageio&#39;, &#39;Keras&#x3D;&#x3D;2.2.4&#39;, &#39;numpy&#x3D;&#x3D;1.16.2&#39;, &#39;tensorflow-gpu&#x3D;&#x3D;1.13.1&#39;, &#39;tqdm&#39;],</span><br></pre></td></tr></table></figure>
<p>然后再次运行 <code>python setup.py</code> 文件。</p>
<p>tile of image: 表示一些家具材质的图片。<br>stitching image 图像缝补（图像拼接），这种方式真的是非常神奇</p>
<blockquote>
<p>Images Warping 图像变形<br>Seam Estimation 接缝估计</p>
</blockquote>
<p>Git LFS 是 Github 开发的一个 Git 的扩展，用于实现 Git 对大文件的支持</p>
<p><a href="https://zzz.buzz/zh/2016/04/19/the-guide-to-git-lfs/" target="_blank" rel="noopener">Git LFS 操作指南</a></p>
<h2 id="新的调研"><a href="#新的调研" class="headerlink" title="新的调研"></a>新的调研</h2><p>看起来比较好用的super resolution 项目</p>
<ol>
<li><a href="https://github.com/krasserm/super-resolution" target="_blank" rel="noopener">super-resolution</a></li>
</ol>
<p>实现了以下三个算法</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">- Enhanced Deep Residual Networks for Single Image Super-Resolution (EDSR), winner of the NTIRE 2017 super-resolution challenge.</span><br><span class="line">- Wide Activation for Efficient and Accurate Image Super-Resolution (WDSR), winner of the NTIRE 2018 super-resolution challenge (realistic tracks).</span><br><span class="line">- Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network (SRGAN).</span><br></pre></td></tr></table></figure>



<p>使用的是 div2k 数据集训练的。</p>
<p>特点</p>
<ul>
<li>整体感觉思路比较清晰，比较简单的一个demo。 </li>
<li>貌似没有提供自定义的 data 的方式，只有一个 DIV2K 的方式</li>
<li>从展示的效果上看，比较令人满意</li>
</ul>
<ol start="2">
<li><a href="https://github.com/idealo/image-super-resolution" target="_blank" rel="noopener">image-super-resolution</a></li>
</ol>
<p>这个开源项目是在公司正在使用的算法</p>
<p>实现了以下四种model</p>
<p>Currently 4 models are available:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">RDN: psnr-large, psnr-small, noise-cancel</span><br><span class="line">RRDN: gans</span><br></pre></td></tr></table></figure>
<p>对应的论文是以下四个：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">The super-scaling Residual Dense Network described in Residual Dense Network for Image Super-Resolution (Zhang et al. 2018)</span><br><span class="line">The super-scaling Residual in Residual Dense Network described in ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks (Wang et al. 2018)</span><br><span class="line">A multi-output version of the Keras VGG19 network for deep features extraction used in the perceptual loss</span><br><span class="line">A custom discriminator network based on the one described in Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network (SRGANS, Ledig et al. 2017)</span><br></pre></td></tr></table></figure>



<ol start="3">
<li><a href="https://github.com/open-mmlab/mmediting" target="_blank" rel="noopener">mmediting</a></li>
</ol>
<p>这个是曹凯迪推荐的，虽然还没有来得及 run，这个是大概率可行的。最新的成果。</p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2020/04/10/%E5%85%B3%E4%BA%8E%E6%9C%AC%E4%B8%93%E4%B8%9A%E6%96%B9%E9%9D%A2%E7%9A%84%E9%97%B2%E8%AE%B0/" rel="next" title="闲记">
                <i class="fa fa-chevron-left"></i> 闲记
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2020/04/18/cluster_algorithm/" rel="prev" title="聚类算法的总结">
                聚类算法的总结 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="https://disqus.com/?ref_noscript" target="_blank" rel="noopener">comments powered by Disqus.</a>
        </noscript>
      </div>
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/avatar.jpeg"
                alt="Jijeng Jia" />
            
              <p class="site-author-name" itemprop="name">Jijeng Jia</p>
              <p class="site-description motion-element" itemprop="description">Solving Problems by Coding</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/%20%7C%7C%20archive">
              
                  <span class="site-state-item-count">168</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">7</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">83</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/jijeng" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:jia1509309698@163.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          

          <script type="text/javascript" src="//rf.revolvermaps.com/0/0/5.js?i=5kk0u0mm50w&amp;m=0&amp;c=54ff00&amp;cr1=ff0000" async="async"></script>


        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#调研部分"><span class="nav-number">1.</span> <span class="nav-text">调研部分</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#在用的开源项目"><span class="nav-number">2.</span> <span class="nav-text">在用的开源项目</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#issues-整理"><span class="nav-number">2.1.</span> <span class="nav-text">issues 整理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#代码阅读"><span class="nav-number">2.2.</span> <span class="nav-text">代码阅读</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#改进的方向"><span class="nav-number">2.3.</span> <span class="nav-text">改进的方向</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#mmsr开源代码"><span class="nav-number">3.</span> <span class="nav-text">mmsr开源代码</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#新的调研"><span class="nav-number">4.</span> <span class="nav-text">新的调研</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Jijeng Jia</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>

<div>
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
Total  <span id="busuanzi_value_site_pv"></span> views
You got  <span id="busuanzi_value_site_uv"></span> visitors
</div>




        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      
    </span>
  
</div>








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  

    
      <script id="dsq-count-scr" src="https://https-jijeng-github-io.disqus.com/count.js" async></script>
    

    
      <script type="text/javascript">
        var disqus_config = function () {
          this.page.url = 'http://yoursite.com/2020/04/18/super_resolution/';
          this.page.identifier = '2020/04/18/super_resolution/';
          this.page.title = '关于 super resolution 的相关论文';
        };
        var d = document, s = d.createElement('script');
        s.src = 'https://https-jijeng-github-io.disqus.com/embed.js';
        s.setAttribute('data-timestamp', '' + +new Date());
        (d.head || d.body).appendChild(s);
      </script>
    

  




	





  















  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('10');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
