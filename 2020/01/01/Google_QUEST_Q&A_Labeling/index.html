<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang>
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">










<meta name="description" content="Google QUEST Q&amp;amp;A Labeling 项目记录（ongoing）">
<meta property="og:type" content="article">
<meta property="og:title" content="Google QUEST Q&amp;A Labeling">
<meta property="og:url" content="http://yoursite.com/2020/01/01/Google_QUEST_Q&A_Labeling/index.html">
<meta property="og:site_name" content="Jijeng&#39;s blog">
<meta property="og:description" content="Google QUEST Q&amp;amp;A Labeling 项目记录（ongoing）">
<meta property="og:locale" content="default">
<meta property="og:image" content="https://s2.ax1x.com/2020/01/08/l2HkGt.jpg">
<meta property="og:image" content="https://i.niupic.com/images/2020/01/08/6fG8.png">
<meta property="og:image" content="https://i.loli.net/2020/01/08/dNqbf29WzDtBEZ7.png">
<meta property="og:image" content="https://upload.cc/i1/2020/01/08/c6oMa2.png">
<meta property="og:image" content="https://ftp.bmp.ovh/imgs/2020/01/1b38f3f92dd09f53.jpg">
<meta property="og:image" content="https://i.loli.net/2020/01/07/QaPLBKTvshwAcly.jpg">
<meta property="og:updated_time" content="2020-02-22T13:11:34.433Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Google QUEST Q&amp;A Labeling">
<meta name="twitter:description" content="Google QUEST Q&amp;amp;A Labeling 项目记录（ongoing）">
<meta name="twitter:image" content="https://s2.ax1x.com/2020/01/08/l2HkGt.jpg">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2020/01/01/Google_QUEST_Q&A_Labeling/">







<script>
	(function(){
		if(''){
			if (prompt('请输入文章密码','') !== ''){
				alert('密码错误！');
				history.back();
			}
		}
	})();
</script>

  <title>Google QUEST Q&A Labeling | Jijeng's blog</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Jijeng's blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/01/01/Google_QUEST_Q&A_Labeling/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jijeng Jia">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jijeng's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Google QUEST Q&A Labeling</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-01-01T22:20:24+08:00">
                2020-01-01
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">Post modified&#58;</span>
              
              <time title="Post modified" itemprop="dateModified" datetime="2020-02-22T21:11:34+08:00">
                2020-02-22
              </time>
            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2020/01/01/Google_QUEST_Q&A_Labeling/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2020/01/01/Google_QUEST_Q&A_Labeling/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="page-pv"><i class="fa fa-file-o"></i>
            <span class="busuanzi-value" id="busuanzi_value_page_pv"></span>
            </span>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>Google QUEST Q&amp;A Labeling 项目记录（ongoing）</p>
<a id="more"></a>




<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><blockquote>
<p>Computers are really good at answering questions with single, verifiable answers. But, humans are often still better at answering questions about opinions, recommendations, or personal experiences.<br>Humans are better at addressing subjective questions that require a deeper, multidimensional understanding of context - something computers aren’t trained to do well…yet.. Questions can take many forms - some have multi-sentence elaborations, others may be simple curiosity or a fully developed problem. They can have multiple intents, or seek advice and opinions. Some may be helpful and others interesting. Some are simple right or wrong.<br>机器擅长回答确切单一的问题。人类相对比较擅长关于观点，建议和个人经历类的问题。并且每个问题的期待的答案是不一样的，有的仅仅是好奇，有的是询问建议。所以这个比赛的目的就是这样的。</p>
</blockquote>
<blockquote>
<p>In this competition, you’re challenged to use this new dataset to build predictive algorithms for different subjective aspects of question-answering. The question-answer pairs were gathered from nearly 70 different websites, in a “common-sense” fashion. Our raters received minimal guidance and training, and relied largely on their subjective interpretation of the prompts. As such, each prompt was crafted in the most intuitive fashion so that raters could simply use their common-sense to complete the task. By lessening our dependency on complicated and opaque rating guidelines, we hope to increase the re-use value of this data set. What you see is what you get!<br>数据来自70多个网站中的问答语句。</p>
</blockquote>
<blockquote>
<p>Submissions to this competition must be made through Notebooks. Your Notebook will re-run automatically against an unseen test set, and needs to output a file named submission.csv. You can still train a model offline, upload it as a dataset, and use the notebook exclusively to perform inference.<br>特别说明：可以线下训练好一个模型，但是最后的predict 是需要线上在两个小时内跑完。</p>
</blockquote>
<p>（1）Spearman correlation coefficient（斯皮尔曼相关性系数）</p>
<p>最好的是接近于1， 最坏的是接近-1。就是这么简单。</p>
<blockquote>
<p>The Spearman correlation between two variables is equal to the Pearson correlation between the rank values of those two variables; while Pearson’s correlation assesses linear relationships, Spearman’s correlation assesses monotonic relationships (whether linear or not). If there are no repeated data values, a perfect Spearman correlation of +1 or −1 occurs when each of the variables is a perfect monotone function of the other.<br>Intuitively, the Spearman correlation between two variables will be high when observations have a similar (or identical for a correlation of 1) rank (i.e. relative position label of the observations within the variable: 1st, 2nd, 3rd, etc.) between the two variables, and low when observations have a dissimilar (or fully opposed for a correlation of −1) rank between the two variables.</p>
</blockquote>
<p>关于为什么使用Spearman，可以参考这篇<a href="http://jijeng.github.io/2018/06/13/evaluation_metrics/#more" target="_blank" rel="noopener">机器学习中的常见的评价指标</a>和<a href="https://www.kaggle.com/carlolepelaars/understanding-the-metric-spearman-s-rho" target="_blank" rel="noopener">Understanding The Metric: Spearman’s Rho</a>。</p>
<h2 id="数据"><a href="#数据" class="headerlink" title="数据"></a>数据</h2><p>（1）数据本身的信息</p>
<p>size 大小</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">train size: 6079, test size: 476 , sample size: 476</span><br><span class="line">Number of train columns</span><br><span class="line">41</span><br></pre></td></tr></table></figure>

<p>上述的test size 是 public LB，占比总的 test data 是 13%（总的大概有 3600左右）。如果使用 5-fold split，那么就是使用了 around 1200 样本，所以直觉上知道 5-fold 貌似是更加准确的选择，相对于 public LB 上的结果。</p>
<p>缺省值情况missing values？</p>
<p>没有missing values </p>
<p>数据网站来源：只有6个王章<br>…<br>askubuntu’, ‘superuser’, ‘mathoverflow’, ‘stackexchange’, ‘serverfault’, ‘stackoverflow’}<br>…</p>
<p>train数据有10列</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">question_title</span><br><span class="line">question_body</span><br><span class="line">question_user_name</span><br><span class="line">question_user_page</span><br><span class="line">answer</span><br><span class="line">answer_user_name</span><br><span class="line">answer_user_page</span><br><span class="line">url</span><br><span class="line">category (LIFE_ARTS, CULTURE, SCIENCE)</span><br><span class="line">host: 网站 pg.stackexchange.com</span><br></pre></td></tr></table></figure>

<h3 id="训练语料"><a href="#训练语料" class="headerlink" title="训练语料"></a>训练语料</h3><p>question_title，question_body 和answer 是训练数据集。 在answer 的子数上进行了截取，否则的话，长尾效应太明显。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">train[<span class="string">'answer_n_chars'</span>].clip(<span class="number">0</span>, <span class="number">5000</span>, inplace=<span class="keyword">True</span>)</span><br><span class="line">test[<span class="string">'answer_n_chars'</span>].clip(<span class="number">0</span>, <span class="number">5000</span>, inplace=<span class="keyword">True</span>)</span><br><span class="line">train[<span class="string">'answer_n_words'</span>].clip(<span class="number">0</span>, <span class="number">1000</span>, inplace=<span class="keyword">True</span>)</span><br><span class="line">test[<span class="string">'answer_n_words'</span>].clip(<span class="number">0</span>, <span class="number">1000</span>, inplace=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">fig, axes = plt.subplots(<span class="number">1</span>, <span class="number">2</span>, figsize=(<span class="number">12</span>, <span class="number">6</span>))</span><br><span class="line">sns.distplot(train[<span class="string">'answer_n_chars'</span>], label=<span class="string">'train'</span>, ax=axes[<span class="number">0</span>])</span><br><span class="line">sns.distplot(test[<span class="string">'answer_n_chars'</span>], label=<span class="string">'test'</span>, ax=axes[<span class="number">0</span>])</span><br><span class="line">axes[<span class="number">0</span>].legend()</span><br><span class="line">sns.distplot(train[<span class="string">'answer_n_words'</span>], label=<span class="string">'train'</span>, ax=axes[<span class="number">1</span>])</span><br><span class="line">sns.distplot(test[<span class="string">'answer_n_words'</span>], label=<span class="string">'test'</span>, ax=axes[<span class="number">1</span>])</span><br><span class="line">axes[<span class="number">1</span>].legend()</span><br></pre></td></tr></table></figure>

<img src="https://s2.ax1x.com/2020/01/08/l2HkGt.jpg" width="80%" height="80%">


<p>如果从机器学习的角度说，那么句子上能够提供的信息是单词的个数。从相似度的角度分析，可以得到以下的相关性。</p>
<p>We can see following relationship:</p>
<ul>
<li>length of answer is correlated with “answer_level_of_information”.</li>
<li>length of question_title is correlated with “question_body_critical” and length of question body is anticorrelated with it.</li>
<li>length of question_body is anticorrelated with “question_well_written”</li>
</ul>
<img src="https://i.niupic.com/images/2020/01/08/6fG8.png" width="80%" height="80%">


<p>EDA方面：</p>
<p>主要学习数据分析（如何画出比较优美的图像）</p>
<p><del><a href="https://www.kaggle.com/codename007/start-from-here-quest-complete-eda-fe" target="_blank" rel="noopener">Lathwalmaster tier gold medalStart From Here : QUEST Complete EDA + FE ✓✓</a></del></p>
<p><del><a href="https://www.kaggle.com/mobassir/jigsaw-google-q-a-eda" target="_blank" rel="noopener">Jigsaw Google Q&amp;A EDA</a></del></p>
<p>重点看： 好好总结，对问题本身的理解。</p>
<p><del><a href="https://www.kaggle.com/corochann/google-quest-first-data-introduction" target="_blank" rel="noopener">Google QUEST: First data introduction</a></del><br><del><a href="https://www.kaggle.com/hamditarek/get-started-with-nlp-lda-lsa" target="_blank" rel="noopener">Get Started with NLP: LDA/LSA</a></del></p>
<h3 id="target数据"><a href="#target数据" class="headerlink" title="target数据"></a>target数据</h3><p>我的理解是： 有的target 是可以根据 question 和answer 本身搞定的，但是有的target 是不能。比如说问具体问题的时候，这个很容易得到clear 的大胆，所以 fact seaking 和opinion seeking 是两类不同的要求，一般来说前者更加容易评判一些。</p>
<p>需要预测的有30列的信息，可以理解为有30个回归问题需要做。30 target labels consist of 21 question related labels and 9 answer related labels.</p>
<blockquote>
<p>This is not a binary prediction challenge. Target labels are aggregated from multiple raters, and can have continuous values in the range [0,1]. Therefore, predictions must also be in that range.<br>最后的分数是有多个个rater 取平均得到的<br>When you access to the URL, you can understand that multiple answer to the single question is given in the page. But only one answer is sampled in the dataset. Also this answer may not be the most popular answer. We can find the answer of this data in the relatively bottom part of the homepage.<br>样本中的问题和答案并不一定是最好的配对</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">question_asker_intent_understanding 0.30927212305370644</span><br><span class="line">question_body_critical 0.34848463545192004</span><br><span class="line">question_conversational 0.36225613345099045</span><br><span class="line">question_expect_short_answer 0.19924713581463108</span><br><span class="line">question_fact_seeking 0.23402362603601937</span><br><span class="line">question_has_commonly_accepted_answer 0.3270647472500346</span><br><span class="line">question_interestingness_others 0.35865338882656406</span><br><span class="line">question_interestingness_self 0.487661610856663</span><br><span class="line">question_multi_intent 0.23687996328539443</span><br><span class="line">question_not_really_a_question 0.08674931206262017</span><br><span class="line">question_opinion_seeking 0.28771690488749013</span><br><span class="line">question_type_choice 0.2523967744393584</span><br><span class="line">question_type_compare 0.21108026157259407</span><br><span class="line">question_type_consequence 0.14593132381029747</span><br><span class="line">question_type_definition 0.31426570161463124</span><br><span class="line">question_type_entity 0.24858633694975918</span><br><span class="line">question_type_instructions 0.595091290867614</span><br><span class="line">question_type_procedure 0.21292310957231805</span><br><span class="line">question_type_reason_explanation 0.3246562485652123</span><br><span class="line">question_type_spelling 0.1832668832258104</span><br><span class="line">question_well_written 0.3481170025229528</span><br><span class="line">cat_label 1.0</span><br><span class="line">answer_helpful 0.12060418111422218</span><br><span class="line">answer_level_of_information 0.16451754792380313</span><br><span class="line">answer_plausible 0.10178566580122785</span><br><span class="line">answer_relevance 0.10755932163431739</span><br><span class="line">answer_satisfaction 0.1374023010915725</span><br><span class="line">answer_type_instructions 0.5943622624796011</span><br><span class="line">answer_type_procedure 0.1740017131077821</span><br><span class="line">answer_type_reason_explanation 0.38073306076344576</span><br><span class="line">answer_well_written 0.167064377343516</span><br></pre></td></tr></table></figure>

<p>target的类型： 问题本身描述（是否矛盾，是否具有批判性，是否有趣，是不是一个问题）、问题的意图（寻求意见，比较多选，问定义、拼写、写作），是否有了已知解。</p>
<p>（1）target label distribution</p>
<p>可以查看一下数据的分布，<a href="https://www.kaggle.com/phoenix9032/get-started-with-your-questions-eda-model-nn" target="_blank" rel="noopener">参考这里</a>，每一个值大概都是什么位置。<br>代码：</p>
<div><div class="fold_hider"><div class="close hider_title">开/合</div></div><div class="fold">
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## lets see some distributions of questions targets</span></span><br><span class="line">plt.figure(figsize=(<span class="number">20</span>, <span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">sns.distplot(train[question_target_cols[<span class="number">0</span>]], hist= <span class="keyword">False</span> , rug= <span class="keyword">False</span> ,kde=<span class="keyword">True</span>, label =question_target_cols[<span class="number">0</span>],axlabel =<span class="keyword">False</span> )</span><br><span class="line">sns.distplot(train[question_target_cols[<span class="number">1</span>]], hist= <span class="keyword">False</span> , rug= <span class="keyword">False</span>,label =question_target_cols[<span class="number">1</span>],axlabel =<span class="keyword">False</span>)</span><br><span class="line">sns.distplot(train[question_target_cols[<span class="number">2</span>]], hist= <span class="keyword">False</span> , rug= <span class="keyword">False</span>,label =question_target_cols[<span class="number">2</span>],axlabel =<span class="keyword">False</span>)</span><br><span class="line">sns.distplot(train[question_target_cols[<span class="number">3</span>]], hist= <span class="keyword">False</span> , rug= <span class="keyword">False</span>,label =question_target_cols[<span class="number">3</span>],axlabel =<span class="keyword">False</span>)</span><br><span class="line">sns.distplot(train[question_target_cols[<span class="number">4</span>]], hist= <span class="keyword">False</span> , rug= <span class="keyword">False</span>,label =question_target_cols[<span class="number">4</span>],axlabel =<span class="keyword">False</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">target_cols = [<span class="string">'question_asker_intent_understanding'</span>,</span><br><span class="line">       <span class="string">'question_body_critical'</span>, <span class="string">'question_conversational'</span>,</span><br><span class="line">       <span class="string">'question_expect_short_answer'</span>, <span class="string">'question_fact_seeking'</span>,</span><br><span class="line">       <span class="string">'question_has_commonly_accepted_answer'</span>,</span><br><span class="line">       <span class="string">'question_interestingness_others'</span>, <span class="string">'question_interestingness_self'</span>,</span><br><span class="line">       <span class="string">'question_multi_intent'</span>, <span class="string">'question_not_really_a_question'</span>,</span><br><span class="line">       <span class="string">'question_opinion_seeking'</span>, <span class="string">'question_type_choice'</span>,</span><br><span class="line">       <span class="string">'question_type_compare'</span>, <span class="string">'question_type_consequence'</span>,</span><br><span class="line">       <span class="string">'question_type_definition'</span>, <span class="string">'question_type_entity'</span>,</span><br><span class="line">       <span class="string">'question_type_instructions'</span>, <span class="string">'question_type_procedure'</span>,</span><br><span class="line">       <span class="string">'question_type_reason_explanation'</span>, <span class="string">'question_type_spelling'</span>,</span><br><span class="line">       <span class="string">'question_well_written'</span>, <span class="string">'answer_helpful'</span>,</span><br><span class="line">       <span class="string">'answer_level_of_information'</span>, <span class="string">'answer_plausible'</span>, <span class="string">'answer_relevance'</span>,</span><br><span class="line">       <span class="string">'answer_satisfaction'</span>, <span class="string">'answer_type_instructions'</span>,</span><br><span class="line">       <span class="string">'answer_type_procedure'</span>, <span class="string">'answer_type_reason_explanation'</span>,</span><br><span class="line">       <span class="string">'answer_well_written'</span>]</span><br><span class="line"></span><br><span class="line">fig, axes = plt.subplots(<span class="number">6</span>, <span class="number">5</span>, figsize=(<span class="number">18</span>, <span class="number">15</span>))</span><br><span class="line">axes = axes.ravel()</span><br><span class="line">bins = np.linspace(<span class="number">0</span>, <span class="number">1</span>, <span class="number">20</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i, col <span class="keyword">in</span> enumerate(target_cols):</span><br><span class="line">    ax = axes[i]</span><br><span class="line">    sns.distplot(train[col], label=col, kde=<span class="keyword">False</span>, bins=bins, ax=ax)</span><br><span class="line">    <span class="comment"># ax.set_title(col)</span></span><br><span class="line">    ax.set_xlim([<span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">    ax.set_ylim([<span class="number">0</span>, <span class="number">6079</span>])</span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br><span class="line">plt.close()</span><br></pre></td></tr></table></figure>

</div></div>


<img src="https://i.loli.net/2020/01/08/dNqbf29WzDtBEZ7.png" width="80%" height="80%">

<blockquote>
<p>It seems some of the labels are quite imbalanced. For example “question_not_really_a_question” is almost always 0, which means most of the question in the data is not a noisy data but an “actual question”.</p>
</blockquote>
<p>（2）Nan values<br>There is no nan values in the data.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">test.isna().sum()</span><br></pre></td></tr></table></figure>

<p>（3）Category</p>
<blockquote>
<p>The dataset consists of 5 categories: “Technology”, “Stackoverflow”, “Culture”, “Science”, “Life arts”. Train/Test distribution is almost same.</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">train_category = train[<span class="string">'category'</span>].value_counts()</span><br><span class="line">test_category = test[<span class="string">'category'</span>].value_counts()</span><br><span class="line"></span><br><span class="line">fig, axes = plt.subplots(<span class="number">1</span>, <span class="number">2</span>, figsize=(<span class="number">12</span>, <span class="number">6</span>))</span><br><span class="line">train_category.plot(kind=<span class="string">'bar'</span>, ax=axes[<span class="number">0</span>])</span><br><span class="line">axes[<span class="number">0</span>].set_title(<span class="string">'Train'</span>)</span><br><span class="line">test_category.plot(kind=<span class="string">'bar'</span>, ax=axes[<span class="number">1</span>])</span><br><span class="line">axes[<span class="number">1</span>].set_title(<span class="string">'Test'</span>)</span><br><span class="line">print(<span class="string">'Train/Test category distribution'</span>)</span><br></pre></td></tr></table></figure>

<img src="https://upload.cc/i1/2020/01/08/c6oMa2.png" width="80%" height="80%">

<p>可以发现 <code>Technology</code> 和 <code>Stackoverflow</code> 类型的数据是比较多的。</p>
<p>It’s clear that the most questions are technical and IT questions.</p>
<p>Let’s check and count hosts catecories</p>
<p>It’s clear that most questions are about graphic design, more than 1200. <a href="https://www.kaggle.com/hamditarek/get-started-with-nlp-lda-lsa" target="_blank" rel="noopener">数据来源</a></p>
<p>（4）Word Cloud visualization</p>
<p>这个只是说好看，但是对于实际的分析是没有什么用的</p>
<p>（5）Correlation in target labels</p>
<p>I could find following 3 pairs are correlated:</p>
<p>“question_type_instructions” &amp; “answer_type_instructions”<br>“question_type_procedure” &amp; “answer_type_procedure”<br>“question_type_reason_explanation” &amp; “answer_type_reason_explanation”<br>This is reasonable that same evaluation on both question &amp; answer are correlated.</p>
<p>On the other hand, Anticorrelation pattern can be found on following pairs:</p>
<p>“question_fact_seeking” &amp; “question_opinion_seeking”<br>“answer_type_instruction” &amp; “answer_type_reason_explanation”<br>I think this is also reasonable that question that asks fact &amp; opinion conflicts.<br>And answer which shows instruction or reason explanation also conflicts.</p>
<p>是可以根据颜色分辨出来的，一般偏向浅色，表示正相关；偏向黑色，表示负相关。</p>
<p>（6）User check</p>
<blockquote>
<p>The dataset contains question user and answer user information. This may be because user attribution is impotant, same user tend to answer same kind of question and same answer user tends to answer in similar quality.<br>Let’s check if how the user are distributed, and the user are duplicated in train/test or not.<br>做某个实验的基本依据： 同一个user 可能回答同一个相同类型的问题，同一个user 回答问题的质量相近。除此之外，还需要看一下原始的数据中对此的支持力度有多大，train 和test 数据集中有多少</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">train_question_user = train[<span class="string">'question_user_name'</span>].unique()</span><br><span class="line">test_question_user = test[<span class="string">'question_user_name'</span>].unique()</span><br><span class="line"></span><br><span class="line">print(<span class="string">'Number of unique question user in train: '</span>, len(train_question_user))</span><br><span class="line">print(<span class="string">'Number of unique question user in test : '</span>, len(test_question_user))</span><br><span class="line">print(<span class="string">'Number of unique question user in both train &amp; test : '</span>, len(set(train_question_user) &amp; set(test_question_user)))</span><br></pre></td></tr></table></figure>

<p>想说的是下面的数据并不一定准确，因为只有统一个网站中的一个名字对应的多个问题或者回答才应该被认为是同一个用户，然后这种属性才可以被扩展；上面代码中的同一个user 可能是不同的</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Number of unique question user <span class="keyword">in</span> train:  <span class="number">3215</span></span><br><span class="line">Number of unique question user <span class="keyword">in</span> test :  <span class="number">467</span></span><br><span class="line">Number of unique question user <span class="keyword">in</span> both train &amp; test :  <span class="number">37</span></span><br><span class="line"></span><br><span class="line">Number of unique answer user <span class="keyword">in</span> train:  <span class="number">4114</span></span><br><span class="line">Number of unique answer user <span class="keyword">in</span> test :  <span class="number">363</span></span><br><span class="line">Number of unique answer user <span class="keyword">in</span> both train &amp; test :  <span class="number">29</span></span><br></pre></td></tr></table></figure>

<p>应该根据网站site 和用户名来确定是否是同一个用户，上述的代码实现逻辑上是有问题的，因为可能存在：不同网站上的同一个姓名 表示是不同的人，所以该人的回答和问题不具有传递性。</p>
<p>（7）数据和target的之间的相关性</p>
<p>数据统计发现：问题的句子个数和单词的数量和 <code>question clear</code> 是没有什么关系的，</p>
<p>Lets list down the inferences that I have drawn from here</p>
<ul>
<li>Question Interestingness self and others are very correlated</li>
<li>Its is also correlated that short answer seeking questions are commonly accepted</li>
<li>Question is well written and It is interesting to self are correlated</li>
<li>Ofcourse the option seeking questions are more conversational</li>
<li>It is surprising to see that fact seeking and opinion seeking questions are strongly inversely correlated . Because I thought Fact+Bias = Opinion</li>
</ul>
<p>（8）hard targets </p>
<p>hard targets 在某些地方的定义是非0即1的数列，如[0, 1, 0] ，相对应的是 soft target [0.1, 0.9, 0.1] 这样。在这里更加偏向于target 值过度偏向于某个固定的数值，比如说0，比如说1。比如question_not_really_a_question，question_type_compare等 label 都是倾向于0。可以也是可以看做是样本不均衡的情况。</p>
<p>（9）数据中的二义性</p>
<p>同一个相同的问题，有两个不同的label。这个怎么处理？</p>
<h3 id="数据清洗部分"><a href="#数据清洗部分" class="headerlink" title="数据清洗部分"></a>数据清洗部分</h3><p>该代码从三个部分进行数据清洗。主要体现了对数字进行替换，实际上使用特殊符号可能更好，一定要特殊处理。参考<a href="https://www.kaggle.com/phoenix9032/get-started-with-your-questions-eda-model-nn" target="_blank" rel="noopener">这里</a></p>
<div><div class="fold_hider"><div class="close hider_title">开/合</div></div><div class="fold">
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">train = pd.read_csv(ROOT+<span class="string">'train.csv'</span>)</span><br><span class="line">test = pd.read_csv(ROOT+<span class="string">'test.csv'</span>)</span><br><span class="line">sub = pd.read_csv(ROOT+<span class="string">'sample_submission.csv'</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">decontract</span><span class="params">(text)</span>:</span></span><br><span class="line">    text = re.sub(<span class="string">r"(W|w)on(\'|\’)t "</span>, <span class="string">"will not "</span>, text)</span><br><span class="line">    text = re.sub(<span class="string">r"(C|c)an(\'|\’)t "</span>, <span class="string">"can not "</span>, text)</span><br><span class="line">    text = re.sub(<span class="string">r"(Y|y)(\'|\’)all "</span>, <span class="string">"you all "</span>, text)</span><br><span class="line">    text = re.sub(<span class="string">r"(Y|y)a(\'|\’)ll "</span>, <span class="string">"you all "</span>, text)</span><br><span class="line">    text = re.sub(<span class="string">r"(I|i)(\'|\’)m "</span>, <span class="string">"i am "</span>, text)</span><br><span class="line">    text = re.sub(<span class="string">r"(A|a)isn(\'|\’)t "</span>, <span class="string">"is not "</span>, text)</span><br><span class="line">    text = re.sub(<span class="string">r"n(\'|\’)t "</span>, <span class="string">" not "</span>, text)</span><br><span class="line">    text = re.sub(<span class="string">r"(\'|\’)re "</span>, <span class="string">" are "</span>, text)</span><br><span class="line">    text = re.sub(<span class="string">r"(\'|\’)d "</span>, <span class="string">" would "</span>, text)</span><br><span class="line">    text = re.sub(<span class="string">r"(\'|\’)ll "</span>, <span class="string">" will "</span>, text)</span><br><span class="line">    text = re.sub(<span class="string">r"(\'|\’)t "</span>, <span class="string">" not "</span>, text)</span><br><span class="line">    text = re.sub(<span class="string">r"(\'|\’)ve "</span>, <span class="string">" have "</span>, text)</span><br><span class="line">    <span class="keyword">return</span> text</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">clean_text</span><span class="params">(x)</span>:</span></span><br><span class="line"></span><br><span class="line">    x = str(x)</span><br><span class="line">    <span class="keyword">for</span> punct <span class="keyword">in</span> <span class="string">"/-'"</span>:</span><br><span class="line">        x = x.replace(punct, <span class="string">' '</span>)</span><br><span class="line">    <span class="keyword">for</span> punct <span class="keyword">in</span> <span class="string">'&amp;'</span>:</span><br><span class="line">        x = x.replace(punct, <span class="string">f' <span class="subst">&#123;punct&#125;</span> '</span>)</span><br><span class="line">    <span class="keyword">for</span> punct <span class="keyword">in</span> <span class="string">'?!.,"#$%\'()*+-/:;&lt;=&gt;@[\\]^_`&#123;|&#125;~'</span> + <span class="string">'“”’'</span>:</span><br><span class="line">        x = x.replace(punct, <span class="string">''</span>)</span><br><span class="line">    <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">clean_numbers</span><span class="params">(x)</span>:</span></span><br><span class="line"></span><br><span class="line">    x = re.sub(<span class="string">'[0-9]&#123;5,&#125;'</span>, <span class="string">'12345'</span>, x)</span><br><span class="line">    x = re.sub(<span class="string">'[0-9]&#123;4&#125;'</span>, <span class="string">'1234'</span>, x)</span><br><span class="line">    x = re.sub(<span class="string">'[0-9]&#123;3&#125;'</span>, <span class="string">'123'</span>, x)</span><br><span class="line">    x = re.sub(<span class="string">'[0-9]&#123;2&#125;'</span>, <span class="string">'12'</span>, x)</span><br><span class="line">    <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess</span><span class="params">(x)</span>:</span></span><br><span class="line">    x= decontract(x)</span><br><span class="line">    x=clean_text(x)</span><br><span class="line">    x=clean_numbers(x)</span><br><span class="line">    <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">train[<span class="string">'question_body'</span>] = train[<span class="string">'question_body'</span>].progress_map(<span class="keyword">lambda</span> q: preprocess(q))</span><br><span class="line">train[<span class="string">'answer'</span>] = train[<span class="string">'answer'</span>].progress_map(<span class="keyword">lambda</span> q: preprocess(q))</span><br><span class="line">train[<span class="string">'question_title'</span>] = train[<span class="string">'question_title'</span>].progress_map(<span class="keyword">lambda</span> q: preprocess(q))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">test[<span class="string">'question_body'</span>] = test[<span class="string">'question_body'</span>].progress_map(<span class="keyword">lambda</span> q: preprocess(q))</span><br><span class="line">test[<span class="string">'answer'</span>] = test[<span class="string">'answer'</span>].progress_map(<span class="keyword">lambda</span> q: preprocess(q))</span><br><span class="line">test[<span class="string">'question_title'</span>] = test[<span class="string">'question_title'</span>].progress_map(<span class="keyword">lambda</span> q: preprocess(q))</span><br></pre></td></tr></table></figure>

</div></div>





<p>（2）<a href="https://www.kaggle.com/phoenix9032/quest-preprocessing-data-for-embedding" target="_blank" rel="noopener">QUEST -Preprocessing Data for Embedding</a></p>
<p>用数据说明：对文本进行预处理，修正，那么是能够提高 embedding的成功率的。其中对于 oov 进行count 统计，这种策略还是挺好的。</p>
<h3 id="数据增强"><a href="#数据增强" class="headerlink" title="数据增强"></a>数据增强</h3><p>使用了预先处理的数据增强</p>
<p><a href="https://www.kaggle.com/ykojima1989/nlpaug-bert-use-nn-en-ensemble-on-sklearn-pipeline" target="_blank" rel="noopener">https://www.kaggle.com/ykojima1989/nlpaug-bert-use-nn-en-ensemble-on-sklearn-pipeline</a></p>
<p><a href="https://www.kaggle.com/c/google-quest-challenge/discussion/123269" target="_blank" rel="noopener">https://www.kaggle.com/c/google-quest-challenge/discussion/123269</a></p>
<h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><h3 id="Pytorch版本"><a href="#Pytorch版本" class="headerlink" title="Pytorch版本"></a>Pytorch版本</h3><p><del>（2）<a href="https://www.kaggle.com/axel81/pytorch-bert-baseline" target="_blank" rel="noopener">PyTorch BERT Baseline</a> 线上的分数不行 0.034</del></p>
<p><del>（4）<a href="https://www.kaggle.com/arvissu/simple-bert-pytorch-ver/notebook" target="_blank" rel="noopener">Simple_BERT_Pytorch.Ver</a> 0.356，线上不通，参考思路</del></p>
<p>这个的理解： 实际上是多个回归问题，然后预测的时候就是一列一列地进行预测，这个在思路上是没有问题的。不是多分类，是多个回归问题，只不过这个回归问题拼接到了一块。</p>
<p><del>（5）<a href="https://www.kaggle.com/pednoi/googlequest-bert-baseline" target="_blank" rel="noopener">[GoogleQuest] BERT baseline</a> 没有最后的结果，也没有引用量</del></p>
<p>基于transformers的模型，然后可以选择的预训练模型也是比较多的，比如说 stilbert 等模型。这个是很好的基于pytorch 深度学习的框架，包含了常见的loss， optimizer ， scheduler 和writer 等操作，最后直接写到loss 里面。</p>
<p>非常规范的random 函数的书写。 这个从头到尾 给了一个基本的的框架出来，所以这个可以按照这个去改进一下： 因为这个里面是没有对数据进行清洗之类的。但是从 dataloader 到train 到predict ，这个写的十分规范。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">SEED = <span class="number">2019</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">seed_everything</span><span class="params">(seed)</span>:</span></span><br><span class="line">    random.seed(seed)</span><br><span class="line">    np.random.seed(seed)</span><br><span class="line">    os.environ[<span class="string">'PYTHONHASHSEED'</span>] = str(seed)</span><br><span class="line">    </span><br><span class="line">    torch.manual_seed(seed)</span><br><span class="line">    torch.cuda.manual_seed_all(seed)</span><br><span class="line">    torch.backends.cudnn.deterministic = <span class="keyword">True</span></span><br><span class="line">    </span><br><span class="line">seed_everything(SEED)</span><br></pre></td></tr></table></figure>

<p><del>（9）<a href="https://www.kaggle.com/sakami/pytorch-base-model" target="_blank" rel="noopener">PyTorch base model</a>  代码不是该任务的</del></p>
<p>所以只能是借鉴思路；代码的结构是非常好，作为备选修改代码之一。 这个分享的代码并不是一个任务，所以是可以照着这个格式去写，但是不能原封不动的 copy 过来。需要重新写。</p>
<p> （1）<a href="https://www.kaggle.com/artgor/pytorch-approach/data" target="_blank" rel="noopener">Pytorch approach</a>  0.324</p>
<p>可以作为备选方案</p>
<p>（3） <a href="https://www.kaggle.com/abhishek/bert-s-the-word-distillbert/notebook" target="_blank" rel="noopener">BERT’s the word: DistillBERT!!!</a> 0.302</p>
<p>（6）<a href="https://www.kaggle.com/adityaecdrid/pytorch-bert-end-to-end-with-cv" target="_blank" rel="noopener">Pytorch Bert end-to-end-with-CV</a> 这个也没有运行的结果</p>
<p>这个值得好好看一下, pytorch bert +CV</p>
<p>（7）<a href="https://www.kaggle.com/abhishek/distilbert-use-features-oof/notebook" target="_blank" rel="noopener">DistilBERT + USE + Features + OOF</a> 0.367</p>
<p>可以作为备选方案</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Most of the code in this kernel comes directly from:</span><br><span class="line">https://www.kaggle.com/abazdyrev/use-features-oof</span><br></pre></td></tr></table></figure>

<p>至少是比较诚实的，在外面混，注意写上内容来源。</p>
<p>基于keras 的实现，重要是 SpearmanRhoCallback 是可以基于 pytorch 进行实现的吧。 这个是比较全的了</p>
<p>相比于上者，加上了DistilBERT 的embedding，最后拼接成一个训练数据集。</p>
<p>（10）<a href="https://www.kaggle.com/abazdyrev/use-features-oof" target="_blank" rel="noopener">USE + Features + OOF</a> 0.321</p>
<p>可以作为备选方案</p>
<p>（）transformer 中如何下载模型的问题</p>
<p><a href="https://github.com/huggingface/transformers/issues/136" target="_blank" rel="noopener">It’s possible to avoid download the pretrained model?</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model = BertModel.from_pretrained(&apos;bert-base-uncased&apos;) </span><br><span class="line">just download the model you need by the url and unzip it, then you will get bert_config.json and pytorch_model.bin. You can put them in a folder X.</span><br><span class="line">Now, you can use model = BertModel.from_pretrained(&apos;THE-PATH-OF-X&apos;)</span><br></pre></td></tr></table></figure>

<p>如果是其他的模型，那么去<a href="https://github.com/huggingface/transformers/tree/ce50305e5b8c8748b81b0c8f5539a337b6a995b9/src/transformers" target="_blank" rel="noopener">transformers/src/transformers/</a> 找相应的实现代码，然后里面必然有一个保存的地址。</p>
<h3 id="tensorflow版本"><a href="#tensorflow版本" class="headerlink" title="tensorflow版本"></a>tensorflow版本</h3><p>（1）<del><a href="https://www.kaggle.com/phoenix9032/get-started-with-your-questions-eda-model-nn" target="_blank" rel="noopener">keras + tensorflow +nltk： 机器学习（特征工程） + 深度模型</a></del></p>
<p>（2）<del><a href="https://www.kaggle.com/akensert/bert-base-tf2-0-minimalistic" target="_blank" rel="noopener">Bert-base TF2.0 (minimalistic)</a></del></p>
<p>（3）<a href="https://www.kaggle.com/abazdyrev/use-features-oof" target="_blank" rel="noopener">USE + Features + OOF</a></p>
<p>使用了 universal sentence embedding + 简单的features，自实现了 SpearmanRhoCallback，然后在训练的过程中，是可以优化最后的系数的</p>
<blockquote>
<p>OOF simply stands for “Out-of-fold” and refers to a step in the learning process when using k-fold validation in which the predictions from each set of folds are grouped together into one group of 1000 predictions. These predictions are now “out-of-the-folds” and thus error can be calculated on these to get a good measure of how good your model is.</p>
</blockquote>
<p>模型上的建议：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">A lot of experiments can be made with the title + body + answer sequence. Feel free to look into e.g. </span><br><span class="line"></span><br><span class="line">(1) inventing new tokens (add it to &apos;../input/path-to-bert-folder/assets/vocab.txt&apos;), </span><br><span class="line"></span><br><span class="line">(2) keeping [SEP] between title and body but modify _get_segments(), </span><br><span class="line">(3) using the [PAD] token, or </span><br><span class="line">(4) merging title and body without any kind of separation. In this commit I&apos;m doing (2). I also tried (3) offline, and they both perform better than in commit 8, in terms of validation rho.</span><br></pre></td></tr></table></figure>

<p>（3）<del><a href="https://www.kaggle.com/khoongweihao/bert-base-tf2-0-minimalistic-iii" target="_blank" rel="noopener">Bert-base TF2.0 (minimalistic) III</a></del></p>
<p>思路非常清晰，分成 6大步骤。</p>
<p>1). Read data and tokenizer<br>Read tokenizer and data, as well as defining the maximum sequence length that will be used for the input to Bert (maximum is usually 512 tokens)</p>
<p>2). Preprocessing functions<br>These are some functions that will be used to preprocess the raw text data into useable Bert inputs.</p>
<p>3). Create model<br>compute_spearmanr() is used to compute the competition metric for the validation set</p>
<p>CustomCallback() is a class which inherits from tf.keras.callbacks.Callback and will compute and append validation score and validation/test predictions respectively, after each epoch.</p>
<p>bert_model() contains the actual architecture that will be used to finetune BERT to our dataset. It’s simple, just taking the sequence_output of the bert_layer and pass it to an AveragePooling layer and finally to an output layer of 30 units (30 classes that we have to predict)</p>
<p>train_and_predict() this function will be run to train and obtain predictions</p>
<p>4). Obtain inputs and targets, as well as the indices of the train/validation splits</p>
<p>5). Training, validation and testing<br>Loops over the folds in gkf and trains each fold for 5 epochs — with a learning rate of 1e-5 and batch_size of 8. A simple binary crossentropy is used as the objective-/loss-function.</p>
<p>6). Process and submit test predictions<br>First the test predictions are read from the list of lists of histories. Then each test prediction list (in lists) is averaged. Then a mean of the averages is computed to get a single prediction for each data point. Finally, this is saved to submission.csv</p>
<p>使用 DistillBERT 的预选连模型得到 question，answer的 embedding。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">train_question_body_dense = fetch_vectors(df_train.question_body.values)</span><br><span class="line">train_answer_dense = fetch_vectors(df_train.answer.values)</span><br><span class="line"></span><br><span class="line">test_question_body_dense = fetch_vectors(df_test.question_body.values)</span><br><span class="line">test_answer_dense = fetch_vectors(df_test.answer.values)</span><br><span class="line"></span><br><span class="line">xtrain = np.hstack((train_question_body_dense, train_answer_dense))</span><br><span class="line">xtest = np.hstack((test_question_body_dense, test_answer_dense))</span><br></pre></td></tr></table></figure>

<p>然后使用 <code>ctb.CatBoostRegressor</code> 模型进行分分类预测。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> tc <span class="keyword">in</span> target_cols:</span><br><span class="line">    print(tc)</span><br><span class="line">    clf = ctb.CatBoostRegressor(task_type=<span class="string">"GPU"</span>)</span><br><span class="line">    clf.fit(xtrain, df_train[tc].values)</span><br><span class="line">    preds = [sigmoid(x) <span class="keyword">for</span> x <span class="keyword">in</span> clf.predict(xtest)]</span><br><span class="line">    sample[tc] = preds</span><br></pre></td></tr></table></figure>

<p>其中label <code>df_train[tc].values</code> 需要再重新看一下，不懂这个是二分类还是多分类，还是回归问题。</p>
<p>（4）训练的时候用到了question 和answer的 embedding和 句子本身</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y_pred = model(question, answer, title, category, host, use_emb_q, use_emb_a, use_emb_t, dist_feature)</span><br></pre></td></tr></table></figure>

<p>（5）<a href="https://www.kaggle.com/mobassir/jigsaw-google-q-a-eda/data" target="_blank" rel="noopener">Jigsaw Google Q&amp;A EDA</a> 0.385</p>
<p>分成两部分：数据分析和 transfer learning。工作量主要是体现在数据分析阶段，最后的提升比较大，说明这方面的工作很强的。</p>
<p>数据分析部分（EDA）</p>
<p>针对target 任务的分布去划分不同的任务，如果分布比较均匀的，那么使用模型预测；如果分布不均匀的，那么是不是可以使用其他的方式进行计算。</p>
<ul>
<li>将原来的文本转换成 unicode 格式</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">convert_to_unicode</span><span class="params">(text)</span>:</span></span><br><span class="line">    <span class="string">"""Converts `text` to Unicode (if it's not already), assuming utf-8 input."""</span></span><br><span class="line">    <span class="keyword">if</span> six.PY3:</span><br><span class="line">        <span class="keyword">if</span> isinstance(text, str):</span><br><span class="line">            <span class="keyword">return</span> text</span><br><span class="line">        <span class="keyword">elif</span> isinstance(text, bytes):</span><br><span class="line">            <span class="keyword">return</span> text.decode(<span class="string">"utf-8"</span>, <span class="string">"ignore"</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">"Unsupported string type: %s"</span> % (type(text)))</span><br><span class="line">    <span class="keyword">elif</span> six.PY2:</span><br><span class="line">        <span class="keyword">if</span> isinstance(text, str):</span><br><span class="line">            <span class="keyword">return</span> text.decode(<span class="string">"utf-8"</span>, <span class="string">"ignore"</span>)</span><br><span class="line">        <span class="keyword">elif</span> isinstance(text, unicode):</span><br><span class="line">            <span class="keyword">return</span> text</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">"Unsupported string type: %s"</span> % (type(text)))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">"Not running on Python2 or Python 3?"</span>)</span><br><span class="line">``` </span><br><span class="line"></span><br><span class="line">- 对于chinese word 的判断</span><br><span class="line"></span><br><span class="line">```python</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_is_chinese_char</span><span class="params">(self, cp)</span>:</span></span><br><span class="line">        <span class="string">"""Checks whether CP is the codepoint of a CJK character."""</span></span><br><span class="line">        <span class="comment"># This defines a "chinese character" as anything in the CJK Unicode block:</span></span><br><span class="line">        <span class="comment">#   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)</span></span><br><span class="line">        <span class="comment">#</span></span><br><span class="line">        <span class="comment"># Note that the CJK Unicode block is NOT all Japanese and Korean characters,</span></span><br><span class="line">        <span class="comment"># despite its name. The modern Korean Hangul alphabet is a different block,</span></span><br><span class="line">        <span class="comment"># as is Japanese Hiragana and Katakana. Those alphabets are used to write</span></span><br><span class="line">        <span class="comment"># space-separated words, so they are not treated specially and handled</span></span><br><span class="line">        <span class="comment"># like the all of the other languages.</span></span><br><span class="line">        <span class="keyword">if</span> ((cp &gt;= <span class="number">0x4E00</span> <span class="keyword">and</span> cp &lt;= <span class="number">0x9FFF</span>) <span class="keyword">or</span>  <span class="comment">#</span></span><br><span class="line">                (cp &gt;= <span class="number">0x3400</span> <span class="keyword">and</span> cp &lt;= <span class="number">0x4DBF</span>) <span class="keyword">or</span>  <span class="comment">#</span></span><br><span class="line">                (cp &gt;= <span class="number">0x20000</span> <span class="keyword">and</span> cp &lt;= <span class="number">0x2A6DF</span>) <span class="keyword">or</span>  <span class="comment">#</span></span><br><span class="line">                (cp &gt;= <span class="number">0x2A700</span> <span class="keyword">and</span> cp &lt;= <span class="number">0x2B73F</span>) <span class="keyword">or</span>  <span class="comment">#</span></span><br><span class="line">                (cp &gt;= <span class="number">0x2B740</span> <span class="keyword">and</span> cp &lt;= <span class="number">0x2B81F</span>) <span class="keyword">or</span>  <span class="comment">#</span></span><br><span class="line">                (cp &gt;= <span class="number">0x2B820</span> <span class="keyword">and</span> cp &lt;= <span class="number">0x2CEAF</span>) <span class="keyword">or</span></span><br><span class="line">                (cp &gt;= <span class="number">0xF900</span> <span class="keyword">and</span> cp &lt;= <span class="number">0xFAFF</span>) <span class="keyword">or</span>  <span class="comment">#</span></span><br><span class="line">                (cp &gt;= <span class="number">0x2F800</span> <span class="keyword">and</span> cp &lt;= <span class="number">0x2FA1F</span>)):  <span class="comment">#</span></span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">True</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">False</span></span><br></pre></td></tr></table></figure>

<ul>
<li>对于 accents 和punc 的支持</li>
</ul>
<div><div class="fold_hider"><div class="close hider_title">开/合</div></div><div class="fold">
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_run_strip_accents</span><span class="params">(self, text)</span>:</span></span><br><span class="line">        <span class="string">"""Strips accents from a piece of text."""</span></span><br><span class="line">        text = unicodedata.normalize(<span class="string">"NFD"</span>, text)</span><br><span class="line">        output = []</span><br><span class="line">        <span class="keyword">for</span> char <span class="keyword">in</span> text:</span><br><span class="line">            cat = unicodedata.category(char)</span><br><span class="line">            <span class="keyword">if</span> cat == <span class="string">"Mn"</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            output.append(char)</span><br><span class="line">        <span class="keyword">return</span> <span class="string">""</span>.join(output)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_run_split_on_punc</span><span class="params">(self, text)</span>:</span></span><br><span class="line">        <span class="string">"""Splits punctuation on a piece of text."""</span></span><br><span class="line">        chars = list(text)</span><br><span class="line">        i = <span class="number">0</span></span><br><span class="line">        start_new_word = <span class="keyword">True</span></span><br><span class="line">        output = []</span><br><span class="line">        <span class="keyword">while</span> i &lt; len(chars):</span><br><span class="line">            char = chars[i]</span><br><span class="line">            <span class="keyword">if</span> _is_punctuation(char):</span><br><span class="line">                output.append([char])</span><br><span class="line">                start_new_word = <span class="keyword">True</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">if</span> start_new_word:</span><br><span class="line">                    output.append([])</span><br><span class="line">                start_new_word = <span class="keyword">False</span></span><br><span class="line">                output[<span class="number">-1</span>].append(char)</span><br><span class="line">            i += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> [<span class="string">""</span>.join(x) <span class="keyword">for</span> x <span class="keyword">in</span> output]</span><br><span class="line">        </span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">_is_whitespace</span><span class="params">(char)</span>:</span></span><br><span class="line">    <span class="string">"""Checks whether `chars` is a whitespace character."""</span></span><br><span class="line">    <span class="comment"># \t, \n, and \r are technically contorl characters but we treat them</span></span><br><span class="line">    <span class="comment"># as whitespace since they are generally considered as such.</span></span><br><span class="line">    <span class="keyword">if</span> char == <span class="string">" "</span> <span class="keyword">or</span> char == <span class="string">"\t"</span> <span class="keyword">or</span> char == <span class="string">"\n"</span> <span class="keyword">or</span> char == <span class="string">"\r"</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">True</span></span><br><span class="line">    cat = unicodedata.category(char)</span><br><span class="line">    <span class="keyword">if</span> cat == <span class="string">"Zs"</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">True</span></span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_is_control</span><span class="params">(char)</span>:</span></span><br><span class="line">    <span class="string">"""Checks whether `chars` is a control character."""</span></span><br><span class="line">    <span class="comment"># These are technically control characters but we count them as whitespace</span></span><br><span class="line">    <span class="comment"># characters.</span></span><br><span class="line">    <span class="keyword">if</span> char == <span class="string">"\t"</span> <span class="keyword">or</span> char == <span class="string">"\n"</span> <span class="keyword">or</span> char == <span class="string">"\r"</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">False</span></span><br><span class="line">    cat = unicodedata.category(char)</span><br><span class="line">    <span class="keyword">if</span> cat <span class="keyword">in</span> (<span class="string">"Cc"</span>, <span class="string">"Cf"</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">True</span></span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_is_punctuation</span><span class="params">(char)</span>:</span></span><br><span class="line">    <span class="string">"""Checks whether `chars` is a punctuation character."""</span></span><br><span class="line">    cp = ord(char)</span><br><span class="line">    <span class="comment"># We treat all non-letter/number ASCII as punctuation.</span></span><br><span class="line">    <span class="comment"># Characters such as "^", "$", and "`" are not in the Unicode</span></span><br><span class="line">    <span class="comment"># Punctuation class but we treat them as punctuation anyways, for</span></span><br><span class="line">    <span class="comment"># consistency.</span></span><br><span class="line">    <span class="keyword">if</span> ((cp &gt;= <span class="number">33</span> <span class="keyword">and</span> cp &lt;= <span class="number">47</span>) <span class="keyword">or</span> (cp &gt;= <span class="number">58</span> <span class="keyword">and</span> cp &lt;= <span class="number">64</span>) <span class="keyword">or</span></span><br><span class="line">            (cp &gt;= <span class="number">91</span> <span class="keyword">and</span> cp &lt;= <span class="number">96</span>) <span class="keyword">or</span> (cp &gt;= <span class="number">123</span> <span class="keyword">and</span> cp &lt;= <span class="number">126</span>)):</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">True</span></span><br><span class="line">    cat = unicodedata.category(char)</span><br><span class="line">    <span class="keyword">if</span> cat.startswith(<span class="string">"P"</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">True</span></span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">False</span></span><br></pre></td></tr></table></figure>

</div></div>



<ul>
<li>word 的表示还是非常nice的</li>
</ul>
<p>对于一个word 来说使用 “un”, “##un” 这样形式进行表示。</p>
<div><div class="fold_hider"><div class="close hider_title">开/合</div></div><div class="fold">
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">WordpieceTokenizer</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""Runs WordPiece tokenziation."""</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab, unk_token=<span class="string">"[UNK]"</span>, max_input_chars_per_word=<span class="number">200</span>)</span>:</span></span><br><span class="line">        self.vocab = vocab</span><br><span class="line">        self.unk_token = unk_token</span><br><span class="line">        self.max_input_chars_per_word = max_input_chars_per_word</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">tokenize</span><span class="params">(self, text)</span>:</span></span><br><span class="line">        <span class="string">"""Tokenizes a piece of text into its word pieces.</span></span><br><span class="line"><span class="string">        This uses a greedy longest-match-first algorithm to perform tokenization</span></span><br><span class="line"><span class="string">        using the given vocabulary.</span></span><br><span class="line"><span class="string">        For example:</span></span><br><span class="line"><span class="string">          input = "unaffable"</span></span><br><span class="line"><span class="string">          output = ["un", "##aff", "##able"]</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">          text: A single token or whitespace separated tokens. This should have</span></span><br><span class="line"><span class="string">            already been passed through `BasicTokenizer.</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">          A list of wordpiece tokens.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line"></span><br><span class="line">        text = convert_to_unicode(text)</span><br><span class="line"></span><br><span class="line">        output_tokens = []</span><br><span class="line">        <span class="keyword">for</span> token <span class="keyword">in</span> whitespace_tokenize(text):</span><br><span class="line">            chars = list(token)</span><br><span class="line">            <span class="keyword">if</span> len(chars) &gt; self.max_input_chars_per_word:</span><br><span class="line">                output_tokens.append(self.unk_token)</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">            is_bad = <span class="keyword">False</span></span><br><span class="line">            start = <span class="number">0</span></span><br><span class="line">            sub_tokens = []</span><br><span class="line">            <span class="keyword">while</span> start &lt; len(chars):</span><br><span class="line">                end = len(chars)</span><br><span class="line">                cur_substr = <span class="keyword">None</span></span><br><span class="line">                <span class="keyword">while</span> start &lt; end:</span><br><span class="line">                    substr = <span class="string">""</span>.join(chars[start:end])</span><br><span class="line">                    <span class="keyword">if</span> start &gt; <span class="number">0</span>:</span><br><span class="line">                        substr = <span class="string">"##"</span> + substr</span><br><span class="line">                    <span class="keyword">if</span> substr <span class="keyword">in</span> self.vocab:</span><br><span class="line">                        cur_substr = substr</span><br><span class="line">                        <span class="keyword">break</span></span><br><span class="line">                    end -= <span class="number">1</span></span><br><span class="line">                <span class="keyword">if</span> cur_substr <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">                    is_bad = <span class="keyword">True</span></span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">                sub_tokens.append(cur_substr)</span><br><span class="line">                start = end</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> is_bad:</span><br><span class="line">                output_tokens.append(self.unk_token)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                output_tokens.extend(sub_tokens)</span><br><span class="line">        <span class="keyword">return</span> output_tokens</span><br></pre></td></tr></table></figure>

</div></div>



<p>模型方面：</p>
<p>模型相对做的比较简单，使用一个bert 模型进行 finetune，相对来说比较简单。</p>
<p>该模型是有比较多的预训练模型的</p>
<ul>
<li>bert 是预训练的模型</li>
<li>然后有 5个 tune的模型，应该是使用 5-fold 数据集进行训练的</li>
</ul>
<h3 id="机器学习模型"><a href="#机器学习模型" class="headerlink" title="机器学习模型"></a>机器学习模型</h3><p>（1）<a href="https://www.kaggle.com/saivarunk/basic-modelling-for-quest-q-a-using-xgboost" target="_blank" rel="noopener">Basic modelling for QUEST Q&amp;A using XGBoost</a></p>
<p>一个问题： 为什么 <code>xgb.XGBRegressor</code> 中使用 <code>binary:logistic</code> 二分类的损失函数。</p>
<blockquote>
<p>Should XGBClassifier and XGBRegressor always be used for classification and regression respectively?<br>Basically yes, but some would argue that logistic regression is in fact a regression problem, not classification, where we predict probabilities. You can call predicting probabilities “soft classification”, but this is about a naming convention.<br>Is “explained variance” the best metric for regression model evaluation? or perhaps RMSE?<br>There is no such a thing as “the best metric”, if there was, we would be using it for all the problems and didn’t have multiple metrics. Metric is problem specific.</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_xbg_model</span><span class="params">(target_feature)</span>:</span></span><br><span class="line">    xgb_model = xgb.XGBRegressor(learning_rate = <span class="number">0.1</span>, n_estimators=<span class="number">1000</span>,</span><br><span class="line">                           max_depth=<span class="number">5</span>, min_child_weight=<span class="number">1</span>,</span><br><span class="line">                           gamma=<span class="number">0</span>, subsample=<span class="number">0.8</span>,</span><br><span class="line">                           colsample_bytree=<span class="number">0.8</span>, objective= <span class="string">"binary:logistic"</span>,  </span><br><span class="line">                           nthread=<span class="number">-1</span>, scale_pos_weight=<span class="number">1</span>, random_state=<span class="number">2019</span>, seed=<span class="number">2019</span>)</span><br><span class="line">    xgb_model.fit(X_train, train[target_feature])</span><br><span class="line">    y_pred = xgb_model.predict(X_test)</span><br><span class="line">    <span class="keyword">return</span> y_pred</span><br></pre></td></tr></table></figure>

<h3 id="KFold训练方法"><a href="#KFold训练方法" class="headerlink" title="KFold训练方法"></a>KFold训练方法</h3><p>区分这三种训练方式：</p>
<ul>
<li>KFold</li>
<li>GroupKFold</li>
<li>MultilabelStratifiedKFold</li>
</ul>
<p>Should we simply do K-folds split?</p>
<p>Problem A. There can be leakage between train and validation¶<br>This nice EDA kernel shows that there are some questions that appear many times (with different answers each time). For example the most frequent question is</p>
<blockquote>
<p>What is the best introductory Bayesian statistics?</p>
</blockquote>
<p>which appears 12 times in the training set. Therefore, this means that if we split the data naively, this same question can be in both train and valid sets, and since many labels are related to “questions”, there will be leakage between train and valid sets.</p>
<blockquote>
<p>So naive KFold can give you an over-estimated CV<br>简单说，如果使用 naive k-folds，那么有可能训练集和验证集中出现相同的数据，这个时候发生了 data leakage。</p>
</blockquote>
<p>GroupKFold</p>
<blockquote>
<p>In contrast, this top kernel from my friend @akensert smartly avoid this problem by using <code>sklearn.model_selection.GroupKFold</code> instead! By using <code>GroupKFold</code> it is possible to guarantee that all data with the same question, will always stay in the same fold. Therefore, the above leakage will not happen, and CV is more likely to be more accurate compared to KFold.<br>groupkfold 是可以保证相同的数据集在一个fold 中，所以上述问题解决了。</p>
</blockquote>
<p>最后的metric 选用的是spearman，所以产生了一个新的问题，可能结果是 <code>nan</code></p>
<p>Problem B. Some labels having very few non-zero values, possibly causes <code>spearmanr = nan</code> Concretely, if you take a look at the label question_type_spelling, there will be only 11 / 6079 non-zero values!!! That’s are very few! And if we are not careful it will make CV score nan (explained below)</p>
<blockquote>
<p>如果某个batch 中的所有数值为0，那么计算的 spearmanr 就是 nan</p>
</blockquote>
<blockquote>
<p>GroupKFold has higher chance than KFold to get nan especially when increasing the NUM_FOLDS number<br>因此， <code>GroupKFold</code>有更高的可能性得到 nan值，当增加folds 的个数的时候。</p>
</blockquote>
<blockquote>
<p>Multi-label Stratified-KFold<br>Unlike the above two methods, MultilabelStratifiedKFold is designed directly for multi-labels problem. It will try to make each fold have similar number of examples for each label. Therefore, MultilabelStratifiedKFold is a method with the least chance to have nan problem. Also, it should give the most stable estimation since the label distributions are similar (less biased toward extreme) to all folds.<br>使用 Stratified-KFold 可以在一个batch 中得到相近个数的样本数（对于每个类别而言）。</p>
</blockquote>
<p>所以在 k-fold 议题上，选择的是 <code>Multi-label Stratified-KFold</code>。</p>
<h2 id="其他总结"><a href="#其他总结" class="headerlink" title="其他总结"></a>其他总结</h2><p>python在科学计算领域有三个非常受欢迎库，numpy、SciPy、matplotlib。numpy是一个高性能的多维数组的计算库，SciPy是构建在numpy的基础之上的，它提供了许多的操作numpy的数组的函数。SciPy是一款方便、易于使用、专为科学和工程设计的python工具包，它包括了统计、优化、整合以及线性代数模块</p>
<p>SSD就是固态硬盘，它的优点是速度快，日常的读写比机械硬盘快几十倍上百倍。缺点是单位成本高，不适合做大容量存储。HDD就是机械硬盘，它的优点是单位成本低，适合做大容量存储，但速度远不如SSD。</p>
<p>一些有用的库函数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> OneHotEncoder</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset, DataLoader</span><br><span class="line"><span class="keyword">from</span> torch.nn.utils.rnn <span class="keyword">import</span> pack_padded_sequence, pad_packed_sequence</span><br></pre></td></tr></table></figure>

<p>预训练模型使用和离线下载安装的问题</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">!pip install ../input/sacremoses/sacremoses-master/ &gt; /dev/null</span><br><span class="line">sys.path.insert(<span class="number">0</span>, <span class="string">"../input/transformers/transformers-master/"</span>)</span><br><span class="line"><span class="keyword">import</span> transformers</span><br></pre></td></tr></table></figure>

<p>（1）Text processing 主要做了三件事情</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">clean_data</span><span class="params">(df, columns: list)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> col <span class="keyword">in</span> columns:</span><br><span class="line">        df[col] = df[col].apply(<span class="keyword">lambda</span> x: clean_numbers(x))</span><br><span class="line">        df[col] = df[col].apply(<span class="keyword">lambda</span> x: clean_text(x.lower()))</span><br><span class="line">        df[col] = df[col].apply(<span class="keyword">lambda</span> x: replace_typical_misspell(x))</span><br><span class="line">    <span class="keyword">return</span> df</span><br></pre></td></tr></table></figure>

<p>clean_numbers 对于数字进行了统一的处理，只要能预测出是数字即可，替换成 <code>#</code>，不需要预测出具体的值；<br>clean_text 对于特殊的字符，替换成<code>[punck]</code>特殊字符，不是直接去掉<br>replace_typical_misspell 将原来的缩写形式改写成 正常的形式。</p>
<p>Python3.6新的字符串格式化语法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">'My name is &#123;name&#125;.'</span>.format(name = name))</span><br><span class="line"><span class="comment"># 即便是简化的版本</span></span><br><span class="line">print(<span class="string">'My name is &#123;&#125;.'</span>.format(name))</span><br><span class="line"></span><br><span class="line"><span class="comment"># f表达式</span></span><br><span class="line">name = <span class="string">"Tom"</span></span><br><span class="line">age =<span class="number">3</span></span><br><span class="line"><span class="string">f"His name is <span class="subst">&#123;name&#125;</span>, he's <span class="subst">&#123;age&#125;</span> years old."</span></span><br><span class="line"><span class="comment"># "His name is Tom, he's 3 years old."</span></span><br></pre></td></tr></table></figure>

<p>其他的字符串的形式</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">r"\n\t\n” # 声明后面的字符串是普通字符串，相对的</span></span><br><span class="line"><span class="string">u"</span>我是含有中文字符组成的字符串。<span class="string">" # u表示使用Unicode 编码，一般用在中文字符串前面，防止因为源码储存格式问题，导致再次使用时出现乱码。</span></span><br><span class="line"><span class="string">b“我(python 3)的str是 bytes”  # py2.x的str是 bytes类, python3.x里默认的str是unicode类,</span></span><br></pre></td></tr></table></figure>

<p>（2）关于路径的写法</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># Paths for easy data access</span><br><span class="line">BASE_PATH = &quot;../input/google-quest-challenge/&quot;</span><br><span class="line">TRAIN_PATH = BASE_PATH + &quot;train.csv&quot;</span><br><span class="line">TEST_PATH = BASE_PATH + &quot;test.csv&quot;</span><br><span class="line">SUB_PATH = BASE_PATH + &quot;sample_submission.csv&quot;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">data_dir = &apos;../input/google-quest-challenge/&apos;</span><br><span class="line">train = pd.read_csv(path_join(data_dir, &apos;train.csv&apos;))</span><br><span class="line">test = pd.read_csv(path_join(data_dir, &apos;test.csv&apos;))</span><br><span class="line">print(train.shape, test.shape)</span><br><span class="line">train.head()</span><br></pre></td></tr></table></figure>

<p>（3）使用python中程序实现词云功能（WordCloud）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## Lets see the words of first question</span></span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">20</span>, <span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">text = train.question_body[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create and generate a word cloud image:</span></span><br><span class="line">wordcloud = WordCloud().generate(text)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Display the generated image:</span></span><br><span class="line">plt.imshow(wordcloud, interpolation=<span class="string">'bilinear'</span>)</span><br><span class="line">plt.axis(<span class="string">"off"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>




<p>（4）回调函数</p>
<blockquote>
<p>A callback function is a function that is passed as an argument to another function, to be “called back” at a later time. A function that accepts other functions as arguments is called a higher-order function, which contains the logic for when the callback function gets executed. It’s the combination of these two that allow us to extend our functionality.</p>
</blockquote>
<p>回调函数是一个很有用，也很重要的概念。当发生某种事件时，系统或其他函数将会自动调用你定义的一段函数。</p>
<p>回调函数通常和应用处于同一抽象层（因为传入什么样的回调函数是在应用级别决定的）。而回调就成了一个高层调用底层，底层再回过头来调用高层的过程。（我认为）这应该是回调最早的应用之处，也是其得名如此的原因。</p>
<p><img src="https://ftp.bmp.ovh/imgs/2020/01/1b38f3f92dd09f53.jpg" alt></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">function greeting(name) &#123;</span><br><span class="line">  alert(&apos;Hello &apos; + name);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">function processUserInput(callback) &#123;</span><br><span class="line">  var name = prompt(&apos;Please enter your name.&apos;);</span><br><span class="line">  callback(name);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">processUserInput(greeting);</span><br></pre></td></tr></table></figure>

<p>keras 中的回调函数 【Tips】虽然我们称之为回调“函数”，但事实上Keras的回调函数是一个类。其目的不仅仅在于训练的某个时刻进行调用，而是使用在early stop中，如果某个指标不再变好，那么就没有必要继续训练下去。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.callbacks.EarlyStopping(monitor=<span class="string">'val_loss'</span>, patience=<span class="number">0</span>, verbose=<span class="number">0</span>, mode=<span class="string">'auto'</span>)</span><br></pre></td></tr></table></figure>

<p>常见的参数：</p>
<ul>
<li>monitor：需要监视的量</li>
<li>patience：当early stop被激活（如发现loss相比上一个epoch训练没有下降），则经过patience个epoch后停止训练。</li>
<li>verbose：信息展示模式</li>
<li>mode：‘auto’，‘min’，‘max’之一，在min模式下，如果检测值停止下降则中止训练。在max模式下，当检测值不再上升则停止训练。</li>
</ul>
<p>（5）Sklearn中Pipeline</p>
<p>Pipeline（管道机制）实现了对全部步骤的流式化封装和管理（streaming workflows with pipelines），管道机制更像是编程技巧的创新，而非算法的创新。Pipeline可以将许多算法模型串联起来，比如将特征提取、归一化、分类组织在一起形成一个典型的机器学习问题工作流。主要带来两点好处：</p>
<ul>
<li>直接调用fit和predict方法来对pipeline中的所有算法模型进行训练和预测。</li>
<li>可以结合grid search对参数进行选择。</li>
</ul>
<p>机器学习的训练通常包括以下几个步骤：</p>
<ul>
<li>源数据ETL</li>
<li>数据预处理</li>
<li>特征选取</li>
<li>模型训练与验证</li>
</ul>
<p>利用pipeline我们可以方便的减少代码量同时让机器学习的流程变得直观，如图所示。</p>
<p><img src="https://i.loli.net/2020/01/07/QaPLBKTvshwAcly.jpg" alt="162d6d3f6b4fcbc2.png"></p>
<p>（5）sacremoses</p>
<blockquote>
<p>Python port of Moses tokenizer, truecaser and normalizer<br>是做文本的 token，truecase 和normalization 的工具</p>
</blockquote>
<p>对于truecase 的定义：规范的大小写</p>
<blockquote>
<p>Truecasing is the problem in natural language processing (NLP) of determining the proper capitalization of words where such information is unavailable. This commonly comes up due to the standard practice (in English and many other languages) of automatically capitalizing the first word of a sentence. It can also arise in badly cased or noncased text (for example, all-lowercase or all-uppercase text messages).</p>
</blockquote>
<p>normalization 值得是对文本进行“normalization预处理”，其中包含对中文字符转换成英文字符，数字转换成拉丁文数字(u”０”, u”0”)，简单来说就是完成了统一化。</p>
<h2 id="未解决的问题"><a href="#未解决的问题" class="headerlink" title="未解决的问题"></a>未解决的问题</h2><p>回调函数 和python 中的修饰器（闭包）注意区别？ 和嵌套函数，从概念上去分析理解。</p>
<p>Exploratory Data Analysis,简称EDA</p>
<p>LDA 和 Latent Dirichilet Allocation (LDA)的区别？</p>
<p>如果是大量相同，那么一定要在最开始显眼的位置写上： 这个是对于知识产权的尊重把</p>
<p>Acknowledgements<br>Oiginal kernel: <a href="https://www.kaggle.com/akensert/bert-base-tf2-0-minimalistic" target="_blank" rel="noopener">https://www.kaggle.com/akensert/bert-base-tf2-0-minimalistic</a></p>
<h2 id="想法"><a href="#想法" class="headerlink" title="想法"></a>想法</h2><p>很重要的一个想法，对于相关性比较强的target 和变量，使用机器学习模型进行预测，而对于相关性比较差的模型，使用文本挖掘的模型进行预测。这个是超级nice的想法，很自然的想法。</p>
<p>这个是需要和最后的nlp 任务， 如果说最后的某个target 预测的不好，并且这个target 是和语义不相关，那么是可以直接尝试使用机器学习的模型进行预测的。</p>
<p>这个是大神的比较排名很高，一定要在这个思路上修改一下，</p>
<ul>
<li>加上 cv 之类</li>
<li>换成 stillbert（镇流）</li>
<li>数据除了清洗之外，然后还是数据增强，这个方向是一个比较大的方向了</li>
</ul>
<h2 id="新的资料待总结"><a href="#新的资料待总结" class="headerlink" title="新的资料待总结"></a>新的资料待总结</h2><h3 id="想法-1"><a href="#想法-1" class="headerlink" title="想法"></a>想法</h3><p>主要侧重 overview的想法，这里的基本要求是大牛对于问题的看法。</p>
<p>（1）快速生成词向量</p>
<p>HuggingFace has released a new open-source library for ultra-fast and versatile tokenization for NLP<br>Enjoy <a href="https://github.com/huggingface/tokenizers/tree/master/tokenizers" target="_blank" rel="noopener">https://github.com/huggingface/tokenizers/tree/master/tokenizers</a><br>Pyhton Installation pip install tokenizers</p>
<p>（2）Colab 中安装版本 tensorflow 版本</p>
<p> the only real change necessary is updating the tensorflow version - Colab has 1.15 by defaults,</p>
<p><code>pip install tensorflow-gpu==2.1.0-rc0</code></p>
<p>（3）关于bert 的相关资料的汇总</p>
<p><a href="https://www.kaggle.com/c/google-quest-challenge/discussion/128420" target="_blank" rel="noopener">Everything you always wanted to know about BERT (but were afraid to ask)</a></p>
<h2 id="处理的基本框架"><a href="#处理的基本框架" class="headerlink" title="处理的基本框架"></a>处理的基本框架</h2><h3 id="Pre-Processing"><a href="#Pre-Processing" class="headerlink" title="Pre-Processing"></a>Pre-Processing</h3><p>（1）unidecode and html unescape （主要发生在 json 和html 文件中）</p>
<p>常见的 unescape（转义字符） 符号：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">\&quot;</span><br><span class="line">\\</span><br><span class="line">\/</span><br><span class="line">\b</span><br><span class="line">\f</span><br><span class="line">\n</span><br><span class="line">\r</span><br><span class="line">\t</span><br><span class="line">\u followed by four-hex-digits</span><br></pre></td></tr></table></figure>

<p>（2）various text truncation</p>
<ul>
<li>pre-truncate （截断 截取）</li>
<li>post-truncate</li>
<li>head + tail tokens (下面的论文说明了对于句子，首尾包含重点的信息的概率大)</li>
<li>assign longer max_len to answer</li>
</ul>
<p>sentances longer than maximum length was trimmed at the head and tail parts</p>
<p><a href="https://arxiv.org/abs/1905.05583" target="_blank" rel="noopener">How to Fine-Tune BERT for Text Classification?</a>) 主要思想：</p>
<div><div class="fold_hider"><div class="close hider_title">开/合</div></div><div class="fold">
<ul>
<li>微调策略</li>
</ul>
<p>对于长文本，尝试了（1）取头部510 tokens，（2）尾部510 tokens，（3）头部128 tokens+尾部382 tokens，（4）分片并进行最大池化、平均池化、attention，发现方法（3）最好。因为文章的关键信息一般在开头和结尾。</p>
<p>灾难性遗忘：在下游finetune可能会遗忘预训练的知识。需要设置较小的学习率，如2e-5. 在训练集做Pretraining时，作者指出，训练的步数太少达不到效果，太多会造成灾难性遗忘，选择100K作为一个训练步数是合理的。</p>
<ul>
<li>继续预训练</li>
</ul>
<p>任务内（within-task） 和同领域（in-domain）的继续预训练可以大大提高准确率。In-domain比within-task要好。</p>
<ul>
<li>多任务微调</li>
</ul>
<p>在单任务微调之前的多任务微调有帮助，但是提升效果小于Further pretraining。</p>
<ul>
<li>小数据集</li>
</ul>
<p>BERT对小数据集提升很大，这个大家都知道的。Further pretraining对小数据集也有帮助。</p>

</div></div>

<p>（3） data augmentation 在这个任务中，效果貌似不太明显。<a href="https://www.kaggle.com/c/google-quest-challenge/discussion/129896" target="_blank" rel="noopener">这里</a> <code>It was just about +0.004-5 on LB</code></p>
<p>Data augmentation 常用的手段：</p>
<ul>
<li>inverse translation (<a href="https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/48038" target="_blank" rel="noopener">https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/48038</a>)</li>
<li>manifold mixup （重点是后面的 mixup）</li>
<li>up-sample minor labels</li>
</ul>
<p>inverse translation  就是将英文翻译成德文，然后德文返回回来成英文。上下采样是比较常规的做法。manifold mixup 是针对不同类别之间建模实现数据增强：比如说从训练样本中随机抽取两个样本进行简单的随机加权求和。同样样本的标签页对应加权求和。</p>
<h3 id="Modeling"><a href="#Modeling" class="headerlink" title="Modeling"></a>Modeling</h3><p>（1） 使用 use features </p>
<p>（2） answers 和questions 单独建模，根据target 进行分开</p>
<p>（3）可以尝试使用 Roberta，而不是最初的bert 模型</p>
<p><a href="https://arxiv.org/pdf/1907.11692.pdf" target="_blank" rel="noopener">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a> 详情点开下面的开关</p>
<div><div class="fold_hider"><div class="close hider_title">开/合</div></div><div class="fold">
<p>RoBERTa: A Robustly Optimized BERT Pretraining Approach 重复了BERT 模型的训练过程。</p>
<p>机构： Facebook &amp; 华盛顿大学</p>
<p>本文并没有对 BERT 模型框架有原创性的改动，而是将立足点在如何更好地通过参数调整挖掘 BERT 模型的潜能。</p>
<p>它和BERT 的主要区别如下</p>
<ul>
<li>更大的模型参数量，使用 1024块 V100 GPU 训练了一天的时间</li>
<li>对更多的输数据进行更大批（batch）处理（bert 是16G 文本数据，而Robert 是160 G 数据），研究人员尝试了从 256 到8000 不等的batch size。</li>
<li>删除“next sentence prediction” 任务</li>
<li>动态更改应用于训练数据的masking 模式</li>
</ul>
<p>数据包括：</p>
<ul>
<li>BOOKCORPUS 和英文维基百科：原始 BERT 的训练集，大小 16GB </li>
<li>BOOKCORPUS 和英文维基百科：原始 BERT 的训练集，大小 16GB </li>
<li>OPENWEBTEXT：从 Reddit 上共享的 URL （至少3个点赞）中提取的网页内容，大小 38 GB </li>
<li>STORIES：CommonCrawl 数据集的一个子集，包含 Winograd 模式的故事风格，大小 31GB</li>
</ul>
<p>静态掩码 v.s. 动态掩码</p>
<p>版bert的预训练只会在训练开始随机选择15%的词进行遮掩，但是一经选定，就会固定下来，不论训练多少个epoch。而动态遮词，则是会将数据进行副本备份，然后不同的副本随机遮词。所以 Robert 是不是有点像是通过遮掩来做数据增强呢。</p>
<p>选择大的batch，配合稍微大的学习率，可以在一定程度上提高 perplexity，以及下游任务的准确率。</p>
<p>虽然没有中文预训练模型的效果比较，但 RoBERT 的作者对别了 RoBERTA （large）、BERT（large） 和XLNET 在不同任务上的表现。</p>
<p>同时国内开发者 brightmart 开源了最新的 基于RoBERTa 中文预训练语言模型。项目地址：<a href="https://github.com/brightmart/roberta_zh" target="_blank" rel="noopener">https://github.com/brightmart/roberta_zh</a></p>

</div></div>





<p>（4）huggingface </p>
<p>目前调用预训练模型最主要的项目是 huggingface 的pytorch-transformers，几乎包含了所有主流的预训练语言模型，使用十分方便。从Tokenize，转换为字符 ID 到最终计算隐藏向量表征，提供了整套的API， 可以快读嵌入到各种 NLP 系统中。</p>
<p>值得注意的是pytorch-transformers 同时支持导入 tensorflow预训练模型和pytorch 预训练模型。</p>
<p>关于该 huggingface 开源工具详细的解释。</p>
<div><div class="fold_hider"><div class="close hider_title">开/合</div></div><div class="fold">
<p>（1）bert 预训练模型的安装使用</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install pytorch-pretrained-bert</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tokenizer = BertTokenizer.from_pretrained(<span class="string">'bert-base-uncased'</span>)</span><br><span class="line"> model = BertModel.from_pretrained(<span class="string">'bert-base-uncased'</span>)</span><br></pre></td></tr></table></figure>

<p>预训练模型默认会下载在 <code>~/.pytorch_pretrained_bert/</code> （建议是提前下载好，放到指定的位置）</p>
<p>（2）pytorch-transformers 的安装使用</p>
<p>该库中包含了以下的模型的pytorch 实现</p>
<p>-BERT：用于语言理解的深度双向 Transformer 的预训练 <a href="https://arxiv.org/abs/1810.04805" target="_blank" rel="noopener">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a>，来自Google</p>
<ul>
<li>GPT 通过生成式预训练提高语言理解能力 <a href="https://blog.openai.com/language-unsupervised/" target="_blank" rel="noopener">Improving Language Understanding by Generative Pre-Training </a>， 来自 OpenAI</li>
<li>GPT2 语言模型是无监督的多任务学习器 <a href="https://blog.openai.com/better-language-models/" target="_blank" rel="noopener">Language Models are Unsupervised Multitask Learners </a>，来自 OpenAI</li>
<li>Transformer-XL Transformer-XL：超长上下文关系的注意力语言模型  <a href="https://arxiv.org/abs/1901.02860" target="_blank" rel="noopener"> Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context</a> 来自Google/ CMU</li>
<li>XLNet 用于语言理解的广义自回归预训练 <a href="https://arxiv.org/abs/1906.08237" target="_blank" rel="noopener">​XLNet: Generalized Autoregressive Pretraining for Language Understanding </a></li>
<li>XLM  跨语言的语言模型预训练 <a href="https://arxiv.org/abs/1901.07291" target="_blank" rel="noopener"> Cross-lingual Language Model Pretraining </a> 来自Facebook</li>
</ul>
<p>Transformer 网络受到固定长度上下文的限制，基于此，Transormer-XL 设定了一个比较长的 context来缓解上述问题。</p>
<p>从经验上讲，XLNet 在20个任务上的表现都比 BERT 好，而且是占据很大的优势。包括在问答、自然语言推理、情感分析和文档排序等方面。</p>
<p>在这个开源项目中可以找到 6 transformer architectures and 27 pretrained weights.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">MODELS = [(BertModel,       BertTokenizer,      <span class="string">'bert-base-uncased'</span>),</span><br><span class="line">          (OpenAIGPTModel,  OpenAIGPTTokenizer, <span class="string">'openai-gpt'</span>),</span><br><span class="line">          (GPT2Model,       GPT2Tokenizer,      <span class="string">'gpt2'</span>),</span><br><span class="line">          (TransfoXLModel,  TransfoXLTokenizer, <span class="string">'transfo-xl-wt103'</span>),</span><br><span class="line">          (XLNetModel,      XLNetTokenizer,     <span class="string">'xlnet-base-cased'</span>),</span><br><span class="line">          (XLMModel,        XLMTokenizer,       <span class="string">'xlm-mlm-enfr-1024'</span>)]</span><br></pre></td></tr></table></figure>

<p>预训练模型自然是越大越好。本文使用GPT和GPT-2。GPT和GPT-2是两个非常类似的、基于Transformer的语言模型。这些模型称为解码器或因果模型，这意味着它们使用上下文来预测下一个单词。</p>
<p><code>完形填空</code>  和 <code>预测下一个词语</code> 这两个任务是比较相似的。</p>
<p>使用 GPT-2，Transformer-XL 和XLNet 生成自然语言。</p>
<p>tensorflow的兼容性： 可以将tensorflow checkpoints 作为pytorch 中的模型导入。</p>

</div></div>



<p>（5）模型融合</p>
<h3 id="Post-Processing"><a href="#Post-Processing" class="headerlink" title="Post-Processing"></a>Post-Processing</h3><p>这部分应该是比较有趣，之前的处理从来没有说过得到结果之后，后序还需要修改结果。这特殊在最后的Spearman 是识别double 类型的数据，实践上有用。</p>
<p>首先对  Spearman 相关系数做一下解释。当两个变量不是符合正太分布（或者是不是线性相关的时候），那么可以考虑 spearman，因为该指标不是以数据本身进行计算，而是根据数据的排序进行计算，所以是不依赖于数据本身服从的分布。和其比较相关的是 pearson 相关系数。</p>
<p>本文的数据确实不太符合正太分布（有的label 变量的变化是非常极端的），所以当出来结果之后，使用post -processsing 减少原来预测值的变化，是一种可以尝试的方式。这样最后的数据看起来比较规整。</p>
<p> <a href="https://www.kaggle.com/c/google-quest-challenge/discussion/129901" target="_blank" rel="noopener">该链接</a> 也给出了为什么使用 post-processing 进行处理</p>
<h3 id="试一下的项目"><a href="#试一下的项目" class="headerlink" title="试一下的项目"></a>试一下的项目</h3><p>要求有讲解教程有相应的代码实现。</p>
<p><a href="https://www.kaggle.com/c/google-quest-challenge/discussion/129840" target="_blank" rel="noopener">1st place solution with code 教程</a><br><a href="https://github.com/oleg-yaroshevskiy/quest_qa_labeling" target="_blank" rel="noopener">1st place solution with code 代码</a><br>代码既有training 的部分，也有predict 的部分。</p>
<p><a href="https://www.kaggle.com/c/google-quest-challenge/discussion/130243" target="_blank" rel="noopener">6th place solution 讲解教程</a><br><a href="https://github.com/robinniesert/kaggle-google-quest" target="_blank" rel="noopener">6th place solution  代码</a></p>
<p>这个既有训练部分的代码，也有predict 部分的代码，至少从语言上认为是比较好的tutorial。</p>
<p><a href="https://www.kaggle.com/c/google-quest-challenge/discussion/129904" target="_blank" rel="noopener">23th place solution 教程</a><br><a href="https://www.kaggle.com/shuheigoda/23th-place-solusion/notebook" target="_blank" rel="noopener">23th place solusion 代码</a></p>
<p>该solution 中数据预处理包括</p>
<ul>
<li>html.unescape</li>
<li>trim()  ，该步骤也很重要</li>
<li>word2id, id2tokens  文本转换成了数字表示</li>
</ul>
<p>模型是基于bert 进行 fine-tune 得到的 </p>
<p><a href="https://www.kaggle.com/c/google-quest-challenge/discussion/129859" target="_blank" rel="noopener">15th solution with codes 讲解</a><br><a href="https://github.com/jionie/Google-Quest-Answer" target="_blank" rel="noopener">15th solution with codes 代码</a></p>
<p>代码既有training 的部分，也有predict 的部分。</p>
<p><a href="https://www.kaggle.com/c/google-quest-challenge/discussion/129835" target="_blank" rel="noopener">silver solution (44th)</a></p>
<p>该 solution 有相应的 Inference 的代码实现，对于 sentence embedding 是有一点疑惑的？ 然后可以再看看。</p>
<p>只有 inference 的代码</p>
<p><a href="https://www.kaggle.com/sakami/google-quest-single-lstm/data" target="_blank" rel="noopener">Google QUEST Single LSTM</a><br>Private Score<br>0.37731<br>Public Score<br>0.40992</p>
<p><a href="https://www.kaggle.com/sakami/google-quest-single-lstm/data" target="_blank" rel="noopener">models_without_optimization-v3</a><br>Private Score<br>0.39481<br>Public Score<br>0.42529</p>
<p><a href="https://www.kaggle.com/rashmibanthia/googlequest-inference" target="_blank" rel="noopener">GoogleQuest_Inference</a><br>Private Score<br>0.40760<br>Public Score<br>0.44128</p>
<p><a href="https://www.kaggle.com/zzyucoding/fork-of-roberta-large-featue-lstm-inference-9a3d44" target="_blank" rel="noopener">Fork of roberta-large-featue-lstm-inference 9a3d44</a><br>Private Score<br>0.40958<br>Public Score<br>0.44016</p>
<p><a href="https://www.kaggle.com/guchio3/refactored-bert2-robert1-xlnet1-blend-pseudo1" target="_blank" rel="noopener">refactored-bert2-robert1-xlnet1-blend-pseudo1</a><br>Private Score<br>0.41812<br>Public Score<br>0.45777</p>
<p><a href="https://www.kaggle.com/markpeng/ensemble-5models-v4-v7-magic" target="_blank" rel="noopener">ensemble-5models-v4-v7-magic</a><br>Private Score<br>0.41208<br>Public Score<br>0.44238</p>
<p><a href="https://www.kaggle.com/shuheigoda/23th-place-solusion" target="_blank" rel="noopener">23th place solusion</a><br>Private Score<br>0.41228<br>Public Score<br>0.45763</p>
<p>To give everything without effort doesn’t contribute for anyone grow.</p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2020/01/01/memory_management/" rel="next" title="Python 和C++ 中的内存管理">
                <i class="fa fa-chevron-left"></i> Python 和C++ 中的内存管理
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2020/01/01/leetcode-array2/" rel="prev" title="LeetCode- Array">
                LeetCode- Array <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
      </div>
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/avatar.jpeg" alt="Jijeng Jia">
            
              <p class="site-author-name" itemprop="name">Jijeng Jia</p>
              <p class="site-description motion-element" itemprop="description">Solving Problems by Coding</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">126</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">7</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">85</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/jijeng" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:jia1509309698@163.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          

          <script type="text/javascript" src="//rf.revolvermaps.com/0/0/5.js?i=5kk0u0mm50w&amp;m=0&amp;c=54ff00&amp;cr1=ff0000" async="async"></script>


        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#背景"><span class="nav-number">1.</span> <span class="nav-text">背景</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#数据"><span class="nav-number">2.</span> <span class="nav-text">数据</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#训练语料"><span class="nav-number">2.1.</span> <span class="nav-text">训练语料</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#target数据"><span class="nav-number">2.2.</span> <span class="nav-text">target数据</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#数据清洗部分"><span class="nav-number">2.3.</span> <span class="nav-text">数据清洗部分</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#数据增强"><span class="nav-number">2.4.</span> <span class="nav-text">数据增强</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#模型"><span class="nav-number">3.</span> <span class="nav-text">模型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Pytorch版本"><span class="nav-number">3.1.</span> <span class="nav-text">Pytorch版本</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tensorflow版本"><span class="nav-number">3.2.</span> <span class="nav-text">tensorflow版本</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#机器学习模型"><span class="nav-number">3.3.</span> <span class="nav-text">机器学习模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#KFold训练方法"><span class="nav-number">3.4.</span> <span class="nav-text">KFold训练方法</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#其他总结"><span class="nav-number">4.</span> <span class="nav-text">其他总结</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#未解决的问题"><span class="nav-number">5.</span> <span class="nav-text">未解决的问题</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#想法"><span class="nav-number">6.</span> <span class="nav-text">想法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#新的资料待总结"><span class="nav-number">7.</span> <span class="nav-text">新的资料待总结</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#想法-1"><span class="nav-number">7.1.</span> <span class="nav-text">想法</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#处理的基本框架"><span class="nav-number">8.</span> <span class="nav-text">处理的基本框架</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Pre-Processing"><span class="nav-number">8.1.</span> <span class="nav-text">Pre-Processing</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Modeling"><span class="nav-number">8.2.</span> <span class="nav-text">Modeling</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Post-Processing"><span class="nav-number">8.3.</span> <span class="nav-text">Post-Processing</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#试一下的项目"><span class="nav-number">8.4.</span> <span class="nav-text">试一下的项目</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Jijeng Jia</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>

<div>
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
Total  <span id="busuanzi_value_site_pv"></span> views
You got  <span id="busuanzi_value_site_uv"></span> visitors
</div>




        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      
    </span>
  
</div>








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  

    
      <script id="dsq-count-scr" src="https://https-jijeng-github-io.disqus.com/count.js" async></script>
    

    
      <script type="text/javascript">
        var disqus_config = function () {
          this.page.url = 'http://yoursite.com/2020/01/01/Google_QUEST_Q&A_Labeling/';
          this.page.identifier = '2020/01/01/Google_QUEST_Q&A_Labeling/';
          this.page.title = 'Google QUEST Q&A Labeling';
        };
        var d = document, s = d.createElement('script');
        s.src = 'https://https-jijeng-github-io.disqus.com/embed.js';
        s.setAttribute('data-timestamp', '' + +new Date());
        (d.head || d.body).appendChild(s);
      </script>
    

  




	





  















  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
